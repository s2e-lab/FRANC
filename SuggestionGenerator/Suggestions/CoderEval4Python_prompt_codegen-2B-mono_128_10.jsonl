{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif not tz:\n\t\t\ttz = dt.fromutc(datetime.utcnow())\n\t\telif isinstance(tz, dt.tzinfo):\n\t\t\ttz = dt.timezone(tz)\n\t\tt = dt.fromtimestamp(nanoseconds)\n\t\tdt = datetime.combine(t, dt.time())\n\t\treturn int((dt.hour * 3600 + dt.minute) * 1000 + dt.second, tz) \n\n# ------------------------------------------------------------------------------\n#  time methods\n# ------------------------------------------------------------------------------\n\n@register_db_schema\nclass DateTime(Model):\n\t__table_args__ = {'extend"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = common.local_timezone()\n\t\tif isinstance(nanoseconds, int):\n\t\t\tnanoseconds = time.mktime(nanoseconds)\n\t\treturn nanoseconds + time.timezone\n\treturn hydrate_time\n\n\n#===================================================================================================\n# Utils\n#===================================================================================================\n# Helpers\n#===================================================================================================\n\ndef add_to_list(s, elem):\t\t# adds element to list if not yet in list\n\tif elem not in list_all:\n\t\tlist_all.append(elem)\n\treturn list"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\ttime = Time(nanoseconds)\n\t\treturn time\n\n\n# A number of seconds.\n# \n# The following example:\n# \n# >>> t = hydrate_time(15.058472E-6, timezone.convert_t)\n# >>> print(t)\n# 615.058472 UTC\n# >>> print(time.iso)\n\n# Note that hydrate_time is the local time at which you want to convert the time to a\n# DateTime object. However, the time may not reflect time. However, it is not UTC"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\treturn Time(((nanoseconds + TimeDelta.SECONDS)\n                    ).time()\n                    ).replace(tzinfo=None)\n\n\n# ------------------------------------------------------------------------------\n\nclass Date(DateTime):\n\t\"\"\"\n\tDate field wrapper. Data is a `DateTime` with a `localtime` element.\n\tAlso supports the date format as a string.\n\n\tExamples:\n\n\t    >>> d=Data(1, 1, 2, 3)\n\t\t>>> d=Data(date(2017, 4, 18), date(2017, 10, 16))\n\n\tDefault format for DateTime is ``'YYYY-mm-dd\\'\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\n\treturn time.time() + nanoseconds / 3600\n\n#!/usr/bin/env python\r\n#\r\n# A library that provides a Python interface to the Telegram Bot API\r\n# Copyright (C) 2016-2017\r\n# Leandro Toledo de Souza <devs@python-telegram-bot.org>\r\n#\r\n# This program is free software: you can redistribute it and/or modify\r\n# it under the terms of the GNU Lesser Public License as published by\r\n# the Free Software Foundation, either version 3 of the License, or"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz:\n\t\t\tnanoseconds = datetime.datetime.combine(nanoseconds, datetime.time())\n\t\telse:\n\t\t\tnanoseconds = datetime.datetime.fromtimestamp(float('nanoseconds'))\n\n\t\treturn nanoseconds + timedelta(seconds=nanoseconds.second % nanoseconds.second)\n\telse:\n\t\treturn str(float('nanoseconds')) +'seconds'\n\nclass TimeDeltaField(Field):\n\t\"\"\"\n\tClass for describing different `TimeDelta` fields.\n\t\"\"\"\n\tdef __init__(self, name, fieldType):"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz is None:\n\t\t\tif np.any(nanoseconds.values < 0):\n\t\t\t\treturn np.nan\n\t\t\telse:\n\t\t\t\treturn nanoseconds\n\t\treturn timedelta(hours=nanoseconds.values/1000000000)\n\n\nclass DateTime(datetime.tzinfo):\n\t\"\"\"\n\tUTC timezone.\n\n\tTimezone is timezone-aware, and has the usual form of UTC with its \n\ttimezone component added.\n\tThis allows this class to work in the future.\n\t\"\"\"\n\tdef utcoffset(self, dt):\n\t\treturn timedelta(0)\n\n\tdef"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\treturn nanoseconds.days * 86400 + nanoseconds.seconds\n\ndef time_to_datetime(time):\n\t\"\"\"\n\tConverts a `time` object to a `datetime`.\n\n:param time: Datetime object\n\t@return: Datetime Object\n\t\"\"\"\n\treturn time.replace(tzinfo=pytz.utc).astimezone(pytz.timezone(\"CET\"))\n\ndef date_to_time(date):\n\t\"\"\"\n\tConverts a `date` object to a `time.time` object.\n\n\t:param date: Date object\n\t@return: Time Object"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz:\n\t\t\treturn  timezone.now().astimezone(timezone.get_current_timezone()) - datetime.now(tz)\n\t\telse:\n\t\t\treturn  datetime.now()\n\n\treturn datetime.now()\n\n\n\ndef to_json(params, output_ext = '.json'):\n\t\"\"\"\n\tConvert `params` to JSON\n\n\tParameters\n\t----------\n\tparams : `dict`\n\t\tTo convert\n\n\tOutput\n\t-------\n\tdict\n\t\t- `Result`: `str`\n\n\t\t\"\"\"\n\n\t\tresult = {}\n\t\treturn {'result' : result }"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz:\n\t\t\treturn Timestamp(nanoseconds) - TimeDelta(nanoseconds, tz=tz)\n\t\treturn Timestamp(nanoseconds)\n\n\ndef date_range(start, end=None, periods=None, freq=None):\n\t\"\"\"\n\tFind some dates.\n\n\t:param start:\n\t:param end:\n\t:param periods:\n\t:param freq:\n\t\"\"\"\n\tif start and end and periods:\n\t\treturn _date_range(start, end, periods)\n\n\tif freq and not end and periods:\n\t\treturn _"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\ndef dehydrate_string_to_timedelta(value):\t\"\"\"\t\n\tDehydrator for `string` values.\n\n:param value:\n:type value: str\n:return:\t\"\"\"\n\t\ndef dehydrate_interval(value):\t\"\"\"\n\tDehydrator for `interval` values.\n\n:param value:\n:type value: int\n:return:\t\"\"\"\n\t\ndef dehydrate_str(value):\t\"\"\"\n\tDehydrator for `string` values.\n\n:param value:\n:type value"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(seconds=value)\n\ndef dehydrate_int(value):\n\t\"\"\"\n\tDehydrator for `int` values.\n\n:param value:\n:type value: int\n:return:\t\"\"\"\n\t\treturn int(value)\n\ndef dehydrate_enum(value):\n\t\"\"\"\n\tDehydrator for `enum` values.\n\n:param value:\n\t:type value: str\n\t:return:\t\"\"\"\n\t\treturn str(value)\n\ndef dehydrate_enum_list(value):"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t#!/usr/bin/env python\n\nfrom __future__ import print_function\nimport sys\nimport subprocess\nimport multiprocessing\n\n# A very hacked version of this is so we can use multiprocessing version\n# 2 as opposed to 2.6/2.7 on Windows\ntry:\n\tfrom multiprocessing import forking\nexcept ImportError:\n\tforking = None\n\nfrom argparse import ArgumentParser\nparser = ArgumentParser(description=__doc__)\n\nparser.add_argument(\"-D\", dest=\"dir\", nargs=1, required=True)\nparser.add_argument(\"-T\", dest=\"task"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(seconds=value)\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Sep 15 15:56:51 2016\n\n@author: custo\n\"\"\"\n\nimport numpy as np\nfrom pandas import DataFrame \n\nimport time\nimport re\nimport codecs\n\nfrom sklearn.decomposition import TruncatedSVD\n\n\"\"\"\nthis script gets the list of documents and then turns them into a\nnew dataframe\n\"\"\"\n    \ndef removeAccents(x):\n  x = x[:, [len(x)/10:] + 10]\n  x = x."}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\n\n\treturn datetime.timedelta(seconds=(int(value.total_seconds()) + \n\t\t\t\t                   int(value.seconds) * 10**6))\n\ndef _create_date_range(date_range_start, date_range_end, time_zone=None):\n\t\"\"\"\n\tCreate a `DateRange` object for the given date range.\n\n\t:param date_range_start: (datetime) starting date range\n\t:type date_range_start: tuple\n\t:param date_range_end: (datetime) ending date range\n\t:type date_range"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(seconds=float(value))\n\ndef get_next_execution_rate(workflow_name):\n\t\"\"\"\n\tThe method gets the next execution rate in seconds.\n\n\t:param workflow_name:\n\t:return:\n\t:rtype: int\n\t\"\"\"\n\tnext_execution_rate=get_workflow_execution_rate(workflow_name)\n\tnext_frequency=get_workflow_frequency(workflow_name)\n\tif next_execution_rate > 0:\n\t\tnext_runings=get_runings_for_workflow_type(workflow"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(seconds=value)\n\telse:\n\t\traise ValueError(\"value should be timedelta\")\n\ndef dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n\t:param value:\n\t:type value: datetime.time\n\t:return:\t\"\"\"\n\t\treturn get_time_in_seconds(value / 60)\n\ndef _get_datetime(dt, fmt=\"%Y-%m-%d %H:%M:%S\"):\n    \"\"\"\n\tGet datetime instance based on format and interval.\n\n\t:param dt:"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(seconds = value)\n\t\n\tif value is None: return timedelta_None()\n\telif isinstance(value, timedelta):\n\t\treturn value\n\n\telif isinstance(value, (int, float)):\t# No need to do anything in that case\n\t\treturn timedelta(seconds = value)\n\telif hasattr(value, \"__iter__\"): # len() is not available Python 3\n\t\t# Check inefficient slicing support of Python < 2.7\n\t\ttry:\n\t\t    i = iter(value)\n\t\t    if i:\n\t\t    while True:\n\t\t        yield i.next()\n\t\texcept StopIteration:"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\n\n\tdays = (value.days, value.seconds)\n\tseconds = int(value.seconds or 0)\n\tdiv, rem = divmod(days, 86400)\n\thours = rem / div\n\tminutes = rem % div\n\tseconds -= (hours * 3600 * 24)\n\treturn datetime(int(days), int(seconds), int(minutes))\n#\n#\n# This file is part of beets.\n# Copyright 2013, Adrian Sampson.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(hours=int(value.total_seconds() / 3600.0 + value.seconds() / 60.0 + 10))\n\n\n@functools.total_ordering\nclass FileOperator(object):\n    def __init__(self, *, name=None, format=_(\"%(path)s\"):\n        self._name = name\n        self._format = format\n        self._storage = None\n        self._default_location = kwargs.pop('location')\n\n    @property\n    def name(self):\n        if self._name is None:\n            _, self._name = os.path.split(self.location)"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn float(time.mktime(value.timetuple()))\n\t# else:\n\t# \treturn int(time.time())\n\ndef is_number(value):\n\t\"\"\"\n\tDehydrating for usage. The value `value` is considered\n\ta decimal value.\n\n\t:param value: the value to check\n\t:type value: Python ``float``\n\t:return:\tboolean True if the value is a number, else false\n\t:rtype: bool\n\t\"\"\"\n\ttry:\n\t\tfloat(value)\n\texcept ValueError:\n\t\treturn False"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn int(time.time())\n\ndef jsonable(obj):\n\tif isinstance(obj, datetime.date):\n\t\treturn obj.isoformat()\n\telif isinstance(obj, datetime.time):\n\t\treturn obj.isoformat()\n\telse:\n\t\treturn obj.__repr__()\n\ndef get_timestamp(date):\n\treturn \"%s/%s/%s,%s\"%(date.year, date.month, date.day, date.hour, date.minute)\n\ndef formatdate(date_time):\n\n\tif date_time:\n\t\treturn strftime(\"%Y-%"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn int(ceil(float(value.total_seconds() / 60)))\n\ndef dehydrate_date(value):\n\t\"\"\"\n\tDehydrator for `date` values.\n\n:param value:\n:type value: Date\n:return:\t\"\"\"\n\t\treturn int(now().date().isoformat())\n\n# ---------------------------\n\n\nclass Cached(object):\n\t\"\"\"\n\tAbstract class, used by `LazyValueMixin` to cache cache entries for the current object.\n\n\t:param key: The key to be fetched.\n\t:type key:"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tassert(value > 0. or value < 1.)\n\treturn value * 60. + value * 60.\n\n\ndef check_for_nan(value):\n\t\"\"\"\n\tCheck if `value` is a valid date or not:\n\n\tIf `value` is a valid date (or `datetime` instance), `None`\n\twill be returned.\n\n\tParameters\n\t----------\n\tvalue:\n\t\t`datetime` instance or datetime instance with number\n\t:return:\n\t\t`value` instance or datetime instance with number\n\n\tReturns\n\t-------\n\tboolean or None (if"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn str(time.mktime((datetime.utcfromtimestamp(int(value) * 1000)).timetuple()))\n\treturn \"\"\n\ndef get_config_from_app_settings(data_document, config):\n\n\tif config:\n\t\t# app settings\n\t\tfor key in (\"APP_TITLE\", \"APP_VERSION\", \"APP_WEBSITE\"):\n\t\t\tapp_key = key.replace(\"_\", \" \")[::-1]\n\n\t\t\t# make sure we have an existing config object for this app\n\t\t\tapp_data = data_document.get(key)\n\t\t\tif not app_data"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn value/1000.0\n\t\t\n\n############################\ndef dehydrate_length(value):\t\"\"\"\n\tDehydrator for `length` values.\n\n:param value:\n:type value: int\n:return:\t\"\"\"\n\t\treturn (value/10.0)\n\t\n\n############################\ndef dehydrate_temp_difference(value, temperature):\n    \"\"\"\n    Dehydrate for `temperature` values.\n    \n    :param value: \n    :type value: float\n    :param temperature: \n    :type temperature: float\n    :return:\n    \"\"\"\n        return temperature -"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\t# convert to seconds\n\t\tvalue = long(value.total_seconds())\n\t# time is a float in seconds\n\treturn value.total_seconds()\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n#\n#       Copyright (C) 2013 Martin Owens\n#\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option)"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn value\n\n\nclass TimeConverter(float, int):\n\t\"\"\"\n\tConvert values to float seconds\n\t\"\"\"\n\tdef parse(self, text):\n\t\t\"\"\"\n\t\tParses a string according to the regular expression\n\t\t:type text: str\n\t\t:param text: string to parse\n\t\t:return:\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvalue = string.split(text, ':')\n\t\t\tvalue = (1 if value else 0) * 60 + int(value[0])\n\t\t\tvalue = int(value)\n\t\t\treturn value\n\t\texcept Exception:\n\t\t\tlogging"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn Time(value).dehydrate()\n\n\ndef gps_in_ms(self, gps):\n\t\"\"\"\n\tDehydrator for `gps` in milliseconds.\n\n\t:param gps:\n\t:type gps: int\n\t:return:\t\"\"\"\n\t\treturn time.strftime(\"%M:%S\", gps)\n\n\ndef miles_per_hour(self, miles, hours):\n\t\"\"\"\n\tDehydrator for ``miles`` per hour.\n\n\t:type gps: float\n\t:type miles: float"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t result = value.hour\n\t\tif value.minute >= 0:\n\t\t\tresult += 3600 * value.minute\n\t\treturn result\n\n\nclass SecondsWithinUTC(BaseType):\n\t\"\"\"\n\tRepresents the seconds of when a date has a date in the past.\n\n\t:param default:\n\t:type default: bool\n\t:param default_utc:\n\t:param nullable:\n\t:type nullable: bool\n\t\"\"\"\n\tdefault = False\n\tdefault_utc = False\n\n\tdef __str__(self):\n\t\tvalue = (str(self.default) == '"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn repr(value)\n\t#def dehydrate_point(point):\n\t#\t\"\"\"\n\t#\tDehydrate for Point data.\n\t#\n\t#\tArguments:\n\t#\tvalue(str):\n\t#\ttype(str):\t\n\t#\treturn:\t\"\"\"\n\t#\treturn str(point)\n\tpass\n\n\nclass MapData(object):\n\tdef __init__(self, file, **options):\n\t\tsuper(MapData, self).__init__()\n\t\tself.points = []  # type: list\n#"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tif isinstance(value, Point):\n\t\t\treturn value.get_x()\n\n\t\telif isinstance(value, float):\n\t\t\treturn value\n\n\t##\n\t##\tdehydrator for Point data\n\tdef distance_point(self, point=None):\n\t\t\"\"\"\n\t\tFind the x and y from the coordinate system.\n\n\t\t:param point:\n\t\t\tIf you select the coordinates with a point not its points,\n\t\t\tthe coordinates will be rounded to the nearest 10th decimal place\n\t\t\tand the actual x & y will be rounded to the closest point. \n\t\t:type point:"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tpass\n\nclass PolyDataType(DataType):\n\t\n\tdef __str__(self):\n\t\treturn \"Poly Data Type\"\n\t\t\n\tdef to_string(self):\n\t\t\"\"\"\n\t\tReturns string version of this type.\n\t\t@return:\t\"\"\"\n\t\treturn \"Poly Data Type\"\n\t\n\nimport re\nimport os\nimport glob\nimport subprocess\nfrom BCBio import GFF\nfrom collections import namedtuple\n\ndef clean(out_directory, clean_dir):\n    files = []\n    if os.path.isdir(out_directory) == True:\n        for dirpath, dirnames,"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\n    if type(value) == Point:\n        return value.set_color_hsl((value.x, value.y, value.z))\n    elif type(value) == Plane:\n        return value.x, value.y, value.z\n\n# -----------------------------------------------------------------\n# Copyright 2015 Cisco Systems, Inc.  All rights reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#        "}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn Point(value, value)\n\n\t@staticmethod\n\tdef get_random_point_pair():\n\t\t# random_point_pair = tuple()\n\t\treturn Point(random.randint(-180, 180), random.randint(-90, 90))\n\n\t@staticmethod\n\tdef get_point_of_in_polygon(point, polygon):\n\t\t# point = polygon_to_point_pair (point)\n\t\treturn Point((point.x, point.y))\n\n\t@staticmethod\n\tdef get_point_of_polygon(point, polygon):\n\t\treturn (polygon"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\n\t\t\n\tif not isinstance(value, Point):\n\t\traise Error(\"Expecting Point (got: {}\".format(type(value)))\n\t\t\n\ttry:\n\t\tvalue.check_point([Decimal(val) for val in value.split(',')])\n\texcept ValueError as e:\n\t\traise Error(\"Invalid literal for Point {} as value '{}'\".format(value, e.message))\n\t\t\n\treturn value\n\n\ndef dehydrate_float(value):\t\"\"\"\n\tDehydrator for FloatData.\n\n:param value:\n:type value: Data\n:return:\t\"\"\"\n\t\n\t\t\n"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value.points[0].distance(value.location)\t\t\n\t\n\n\ndef is_outside_boundary(point):\n\t\"\"\"Returns True/False/None whether point is outside the boundary\n:param point:\n:type point: Point\n:return: \tTrue if point is in the boundary. False, None else\n\"\"\"\n\treturn get_point_on_segment(point, segment_length(point)) <= \\\n\t\tsegment_length - 1\n\n\ndef boundary_points(point_list):\n\t\"\"\"\n\tFind all points of an input polygon which represent in order"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tif value is None or str(value) == \"\" or value[1] == 0:\n\t\t\treturn None\n\n\t\tsplit_point = split(value)\n\t\tif len(split_point) == 2:\n\t\t\treturn split_point[1] + split_point[0]\n\t\telse: # if string\n\t\t\treturn split_point[0] + split_point[1]\n\n\treturn None\nimport os\nfrom django.conf import settings\nfrom django.db.models.loading import get_models\nfrom django.utils.functional import empty\n\n\ndef get_model"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t return Point((value.x, value.y))\n\ndef get_line(x1, y1, x2, y2):\n\tif x1 < 0 or x2 <0 or y1 < 0 or y2 < 0:\n\t\treturn None\n\treturn Line((x1, y1), (x2, y2))\n\n# TODO:\n# - Fix \"Blind Data\"\n# - Fix a bug with the \"Blind Curve\" algorithm in Rhino.\n_DEFAULT_ALPHA      = 1\n_DEFAULT_INTENSITY      = 1\n"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn Point(0, value)\n\t\n\tdef dehydrate_line(value):\t\"\"\"\n\tDehydrator for Line data.\n\n:param value:\n:type value: Line\n:return:\t\"\"\"\n\t\treturn Line(0, value)\n\nclass DataObject:\n\tdef __init__(self, line_points, line_normals):\n\t\tself.line_points = line_points\n\t\tself.line_normals = line_normals\n\n\t@classmethod\n\tdef create(cls, name, color):\n\t\tobj = cls(name)\n\t\t"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.keys_raw.keys()\n\n\tdef keys_raw(self, rec):\n\t\treturn self.keys_raw[str(rec)]\n\n\tdef __getitem__(self, keys):\n\t\tif keys not in self.keys_raw:\n\t\t\traise KeyError(\"Key not found: \" + keys)\n\n\t\treturn self.keys_raw[keys]\n\n\tdef __iter__(self):\n\t\treturn self.keys_raw.__iter__()\n\n\tdef iteritems(self):\n\t\treturn self.keys_raw.iteritems()\n\n\tdef __str__(self):\n\t\tstring"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\tkeys = self.conn.getKeyInfo()\n\t\treturn keys\nimport unittest\n\nfrom nose.plugins.attrib import attr\nfrom mock import patch\n\nfrom savory.aggregator.config import Aggregate\nfrom savory.aggregator.tests.factories import _DEFAULT_QUEUE\nfrom savory.utils import parse_url_to_queue\n\n\nclass TestAggregators(unittest.TestCase):\n    def setUp(self):\n        self.url = ('http://test.com/api/v1/queue?'\n                    'key=value')\n        self.queue = _DEFAULT_QUEUE"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.list_store.keys()\n\n\tdef count(self, field_name):\n\t\t\"\"\"\n\tCount the number of fields for 'field_name'.\n\tNote, the count is case sensitive.\n\t\"\"\"\n\t\treturn self.list_store.count(field_name)\n\n\tdef index(self, field_name):\n\t\t\"\"\"\n\tGet the index of record on the record list for the field_name.\n\tThe index can be used with \"list_store[\"field_name\"]\" notation. Return\n\tthe actual index.\n        \"\"\"\n\t\treturn"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn copy.deepcopy(self._data[\"keys\"])\n\n\tdef get_subfields_and_values(self):\n\t\t\"\"\"\n\t\tReturn a dict of subfields and values.\n\t\t\"\"\"\n\t\tsubfields = copy.deepcopy(self._data[\"field_order\"])\n\t\tvalue_map = {\"type\": \"text\", \"size\": \"integer\"}\n\t\tfor f in self._data[\"field_order\"]:\n\t\t\tsubfields.append(value_map[f])\n\t\tvalue_map[\"subfields\"] = copy.deepcopy(subfields)\n\t\treturn value_map\n\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.keys\n\nclass RecordList(list):\n\t\"\"\"\n\tA list of record dictionaries (can be accessed like a standard\n\tlist of records or the result of iteration).\n\n\tThe constructor accepts an optional dictionary which maps field names\n\tto field types (including the name of the field in question) and\n\tallows access as xform attributes to access the contents of the record. The\n\tfield types are then converted using the function *as_dict* or\n\t*as_ordered* dictionary functions. The dictionary functions may also\n\tbe imported from the same source file, or"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.__keys\n\n\tdef __str__(self):\t\"\"\"\n\tget the title\n\t\t\"\"\"\n\t\treturn self.__title\n\n\tdef __hash__(self):\t\t\t\"\"\"\n\tget the hash\n\t\t\"\"\"\n\t\treturn self.__id\n\n\tdef __bool__(self): \n\t\t\"\"\"\n\tget a boolean\n\t\t\"\"\" \n\t\tif self.__value is None\t: return False\n\t\telse: return self.__value == 'true'\n\nclass ITEM_TAG(object):\n\t\"\"\"\n\tClass to convert items in a file to a tag\n\t\"\"\"\n\t\n\tdef"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\tnames = list(self.keys_).copy()\n\t\tif self.keys_doctype:\n\t\t\tfor f in self.fields_doctype:\n\t\t\t\tfor key in names:\n\t\t\t\t\tif f == key:\n\t\t\t\t\t\tnames.remove(key)\n\t\treturn names\n\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\tUpdate multiple fields.\n\n:param *args: Dictionary or Field names whose values should be updated\n\t:param fieldnames: List of fields to be updated.\n\t:param **kwargs: If name is in `fieldnames`, update multiple fields"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\tkeys = list()\n\t\tfor i in self.records:\n\t\t\tkeys.append(i.name())\n\t\treturn keys\n\nclass PatientRecord:\n\tdef __init__(self,name,record):\n\t\t\"\"\"\n\t\tRecord object to be used as a new patient record with a name. Also creates\n\t\tunique identification (e.g. '0010101', '011111', etc). You can create a record\n\t\tusing either the dict from a patient record or the database info.\n\t\t\"\"\"\n\t\tself.name = name\n\t\tself.first_name = record['first"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t      \n\tlist = []\n\tfor d in self.records :\n\t\tfor k in d.keys() :\n\t\t\tlist.append(d[k].name)\n\t\t\n\treturn list\n#----------------------------------------------------------------------------------------------------------------------\n#--- public  --------------------------------------------------------------------------------------------\n#----------------------------------------------------------------------------------------------------------------------\n\n#------------------- private methods -----------------------\n\nimport copy\n\n#----------------------------------------------------------------------------------------------------------------------\n#--- class  ---------------------------------------------------------------------------\n\nclass Record:\n\t\"\"\"\n\tRecord of a L{NiftiImage} instance.\n\t\"\"\"\n\n\tdef __init__(self, name = None):\n\t\t\"\"\"\n\t\t@param name: name"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn [k for k in self.header if k.find(\":\")>0][7:]\n\n\tdef keys_by_name(self,name):\n\t\t\"\"\"\n\t\tReturn the keys of the specified field of the record.\n\t\t\"\"\"\n\t\treturn [k.strip().replace(\":\",\"\") for k in self.header][1:]\n\n\n\tdef iterrows(self):\n\t\t\"\"\"\n\t\tReturn a iterable over the records of the specified type.\n\n\t\t:returns: iterator over the records of the specified type\n\t\t:raises: :StopIteration: if no row is found.\n\t\t\"\"\"\n\t"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\t\n'''\n@author: Marsi Pelletier\n\nModule that is responsible for analyzing a FEM as a dictionary.\n\nThe file containing the FEM result is returned in a list of dicts. Each\nitem in the list is a dictionary. Its keys are'message' and 'id'. The\ndictionary values are the respective entries of a FEM result, the key is the\nfew_id of that few_id in the message dictionary, while the value is the associated\nfew_id's value as per FEM dict.\n\n\"\"\"\nimport sys\n\nfrom"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\traise TypeError('protocol_version %s does not support protocol versioning' % protocol_version.__class__.__name__)\n\n\t# TODO: may not be required, but we'd rather not need the more efficient way\n\t# of getting version info from here.\n\n\thandlers = dict.fromkeys(base_handlers(protocol_version))[:]\n\tfor version in protocol_version:\n\t\thandlers[version] = version\n\treturn handlers\n# Copyright 2012 OpenStack Foundation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t# get the highest protocol handler compatible with this version \n\n\t\tif protocol_version[0] is None:\n\t\t\t# we are not using the requested protocol version, give up our own\n\t\t\t# version information\n\t\t\treturn cls.available_versions[0]\n\n\t\telif isinstance(protocol_version, (str, unicode)):\n\t\t\t# we are using the request version, get the highest protocol handler compatible with this one\n\t\t\tversion = protocol_version\n\t\t\thandler_version = protocol_version\n\t\telif protocol_version is None:\n\t\t\t# we are not providing any version to handler for, so"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\thandler_map = {}\n\tfor version, handlers in cls._protocol_version_handlers.items():\n\t\thandlers = handlers[protocol_version]\n\t\tif protocol_version == version:\n\t\t\t# No protocol handlers at all\n\t\t\tbreak\n\t\telse:\n\t\t\thandler_map[version] = handlers\n\n\t# Return all registered bolts compatible version handlers\n\thandler_list = [v for k, v in handler_map.items() if handler_map[v]]\n\t#"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\traise TypeError(\"protocol_version should be an tuple\")\n\tif protocol_version:\n\t\tif version_support(protocol_version):\n\t\t\treturn dict\n\treturn dict(\n\t\tversion=protocol_version,\n\t\thandlers={}\n\t)\n\n# ------------------------------ Protocol handler lookup ------------------------\n\ndef protocol_list(namespace=None, protocol_version=None):\t\n\t\"\"\"\n\tReturn a list of available bolt protocol handlers found at the\n\tnamespace and protocol version.\n\t\"\"\"\n\tif namespace is None:\n\t\tnamespace = \"\"\n\tif not namespace:\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tassert protocol_version\n\t\t## check to see if we are dealing with an incorrect protocol\n\t\tall_handlers = {}\n\t\tdef _make_handler(version_tuple):\n\t\t\tversion = tuple(map(int, version_tuple.split(\".\")))\n\t\t\tif protocol_version > version:\n\t\t\t\tversion_tuple = \"%s-%s\" % version\n\t\t## create a handler for the specified protocol version\t\t\n\t\t# check to see if version_tuple can be detected by the currently\n\t\t# detected protocol version\n\t\tfor handler in all_handlers.values():\n\t\t\tif (protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t# no protocol version, try to guess protocol version\n\tif protocol_version is None:\n\t\t# we try to guess it in the dict\n\t\tm_versions = protocol_version\n\telse:\n\t\t# we try to guess if the protocol version matches the active\n\t\t# protocol version\n\n\t\t# TODO: this is not always reliable, but could improve validation\n\t\t# because version numbers are not supported\n\t\ttry:\n\t\t\tm_versions_str = int(protocol_version[0])\n\t\texcept (IndexError, ValueError):\n\t\t\tm_versions_str = -1\n\t\t\t#"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    handlers = {}\n\n    if protocol_version is None:\n        for proto in __implements__:\n            if not hasattr(proto, '_VERSION'):\n                version = []\n            else:\n                if len(proto._VERSION) > 0:\n                    version = proto._VERSION\n                elif len(proto._VERSION) == 1:\n                    version = proto._VERSION[0]\n            handlers[version] = parser.Version\n        if len(protocol_version) > 0:\n            handlers[protocol_version[-1]] = version\n\n    return handlers\n\n_known_versions = {}\n\ndef register_protocol(protocol, version):\n    \"\"\""}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tglobal protocols\n\n\t\tif protocol_version == (3, 5):\n\t\t\tversions = protocols # will raise AttributeError\n\n\t\telse:\n\t\t\tversions = protocol_versions # this will raise TypeError\n\n\t\tresult = {\"http\": (\"11\", 9),\n\t\t\t\t\t\"https\": (\"11\", 10),\n\t\t\t\t\t\"ws\": (\"10\", 11),\n\t\t\t\t\t\"wss\": (\"11\", 11),\n\t\t\t\t\t\"wsf\": (\"11\", 11),\n\t\t\t\t\t\"wsl\": (\"11\", 11)}\n\n\t\tif isinstance(protocol_version, version):\n\t\t\tresult[\"http\"][0] = protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\n\t# We do this since we need to use protocol version detection and\n\t# version detection does not require protocol version detection. Any\n\t# version detection takes place in the dictionary.\n\t#\n\t# The default dict for the protocol handlers is not used since\n\t# version detection may need to be explicitly configured prior to\n\t# default. This function ensures that version detection does not\n\t# need to be explicitly configured for all bolts (even if the\n\t# version detection is available).\n\n\tif not protocol_version:\n\t\tprotocol_version = ProtocolVersion.NONE\n\n\tif"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t        \n\t        self._timeout_ = config['DEFAULT']['transaction.timeout'] if 'transaction.timeout' in config else None\n\t\t\n\t        self._data_size_ = config['DEFAULT']['transaction.size'] if 'transaction.size' in config else None\n\n\n\tself.metadata = metadata\n\n\tdef __bool__(self):\n\t\t\"\"\"Return boolean for `True` result of `unit_of_work`.\"\"\"\n\t\treturn bool(self.get_success())\n\n\t@property\n\tdef data_size(self):\n\t\t\"\"\"The size of the data query/transaction request, or `None`"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tcontext = dict(context or {})\n\t\tfor key in \"timeout\", \"context\":\n\t\t\tif key in metadata:\n\t\t\t\tvalue = metadata[key].strip().lower()\n\t\t\t\tif value.lower() == \"none\":\n\t\t\t\t\tdel metadata[key]\n\t\t\t\telse:\n\t\t\t\t\tmetadata[key] = {\"timeout\": timeout}\n\t\t\t\n\t@unit_of_work(timeout=10)\t\n\tdef runTransaction(query, transaction, metadata=None, timeout=None):\n\t\t\"\"\"\n\t\tRun a single query.  Return results as a dictionary with the total number of records updated"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\t# NOTE: database is very low resolution, if it's too painful, you shouldn't need that. Investigate using the parameter 'timeout' in code below.\n\tif timeout is None:\n\t\tduration_ms = 1000\n\telse:\n\t\tduration_ms = 1000 * timeout\n\t\tif (DurationInSeconds is not None\n\t\t\tand isinstance(DurationInSeconds,int)\n\t\t):\n\t\t\tduration_in_seconds = DurationInSeconds\n\t\telse:\n\t\t\tduration_in_seconds = DurationInSeconds.total_seconds()\n\t\tduration_ms = duration_in"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tcontext = {}\n\t\tcontext['options'] = {}\n\n\t\tcontext['options']['dbms'] = config.dbms\n\t\tcontext['options']['dbms.transaction.timeout'] = timeout\n\t\tdel context['options']['dbms']\n\t\t#del context['options']['dbms.transaction.timeout']\n\t\ttime.sleep(int(timeout))\n\t\tcontext['transaction'] = dbms_transaction.transaction.Transaction(dbms_schema.Transaction_Transaction())\n\n\t\tcontext['transaction'].get_history()\n\t\tcontext['transaction'].get_status()\n\t\tif context['transaction']."}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\t\n\t\n\tdef transaction(query_id, params={}):\n\t\t\"\"\"\n\t\tTransaction wrapper.\n\n\t\t:param query_id: query id that was created\n\t\t:type query_id: integer\n\t\t:param params: query parameters\n\t\t:type params: arbitrary keyword arguments\n\t\t\"\"\"\n\t\n\t\tt0 = time.time()\n\t\tresult = dbms.transaction.query(query_id, params)\n\t\t\n\t\tt1 = time.time()\n\t\tlogger.info(\"Transaction returned %s %.3fs\" % (query_id, time.time() - t1))\n\t\treturn"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n\toptions = dict(timeout=timeout)\n\tif \"quiet\" in options:\n\t\toptions[\"quiet\"] = True\n\t\n\ttxn_info   = dbms.transaction.get_tx_metadata(**options)\n\t#dbms.transaction.get_tx_metadata_returns_transactions(txn_info)\n\tfor i in range(MAX_TRANSACTION_RETRIES):\n                try:\n\t\t\ttxn_result = dbms.transaction.get_tx_metadata(**options)\n\t\texcept Database.DatabaseError:\n\t            return None\n\n\t\tif not isinstance(txn_result, Transaction):"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n        def _timeout_wrapper(func):\n        \t@functools.wraps(func)\n        \tdef wrapper(*args):\n            ts = time.time()\n            result = func(timeout, timeout, [item['type'] for item in args])\n            # record result in dbms.transaction.timeout\n            if time.time() - ts >= _timeout_timeout:\n                result = None\n                logger.info(\"Transaction timeout reached, query/transaction completed\")\n            return result\n\n        return wrapper\n\treturn _timeout_wrapper\n\n@unit_of_work()\ndef get_transactions(metadata=None, timeout="}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tdef decorator(f):\n\t\t\t@wraps(f)\n\t\t\tdef inner(self, *args, **kwargs):\n\t\t\t\tresult = f(self, *args, **kwargs)\n\t\t\t\tself.log.debug(\"executed: %s\" % self.executed)\n\t\t\t\treturn result\n\t\t\tinner.__doc__ = f.__doc__\n\t\t\treturn inner\n\n\t\treturn decorator\n\n\treturn decorator\n__author__ = 'wenzhiquan'\n\nfrom random import randint\n\n# \u8fd9\u4e2a\u65b9\u5f0f\u90fd\u4e0d\u9700\u8981\u505a\u4e00\u7c7b\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n        def committer(cursor):\n            query.execute_sql(dbms.transaction.timeout, query.log)\n\n        if not timeout:\n            committer(None)\n        else:\n            committer = failure(timeout)  # re-raise if timeout is not set\n        for i in range(timeout):\n            committer(None)\n        return cursor.__exit__()\n\n\n@unit_of_work()\nclass QueryLogger(object):\n    \"\"\"\n    A logger that records transaction, TransactionRunner and TransactionMetadata.\n    This class is designed for using the standard Python logging facilities, specifically:\n\n    1) use"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n    def transaction_with_timeout(method):\n        \"\"\"\n        Decorator which makes a transaction call with default timeout.\n        \"\"\"\n        @wraps(method)\n        def wrapped(self, *args, **kwargs):\n            logger.debug(\"starting transaction with timeout\")\n            res = method(self, *args, **kwargs)\n            logger.debug(\"waiting: \"+ method.__name__)\n            if res is None:\n                raise TimeoutError(\"timeout has elapsed\")\n            logger.debug(\"waiting: \"+ method.__name__)\n            if res.status == 0:\n                logger.info(\"Transaction passed. ID=%s Waited %.6f"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tpass\n\n\t@staticmethod\n\tdef fromarray(data, index):\n\t\t'''\n\t\tCreate an index given a 2D list.\n\t\tCreates a single value or several values (the array may be of this form\n\t\tand is just an array of arrays!) using index().\n\t\t'''\n\t\t#assert isinstance(data, [int, str, list, tuple])\n\t\t#assert all([isinstance(a, collections.Sequence) for a in data]), \"Data must consist only of lists (not lists).\"\n\t\tif data and isinstance(data[0], (list, tuple)):\n\t\t\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.root[key].index\n\n\tdef select(self, key=None):\t\"\"\"\n\tReturn the selected item or None\n\n:param key: optional key\n:return: the selected item if the index given\n:rtype: object | None\n\n:since: version 1.1\n\t\"\"\"\n\t\tfor k in self._keys:\n\t\t\tif (key is None or k <= key) and (key < len(self._data) - 1):\n\t\t\t\treturn self._data[key].index\n\t\treturn None\n\n\tdef pop(self, idx):\n\t\t\"\"\""}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t return len(self.items) - 1\n\n\tdef values(self):\treturn self.items\n\n\tdef remove_key(self, key):\n\t\tnew_len = len(self.items)-1\n\t\tif new_len <= key:\n\t\t\tself.items = self.items[:-1]\n\t\t\t#print \"remove_key new_len:\", new_len,\"len:\", len(self.items)-1\n\t\telse:\n\t\t\tself.items = self.items[1:]\n\t\t\tself.items += [None]\n\t\t\tindex = -1\n\t\tself.items ="}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t #print \"index \", key\n\t\tif isinstance(self[key], list):\n\t\t\treturn self[key].index(key)\n\t\telif isinstance(self[key], tuple):\n\t\t\treturn self[key]\n\n\tdef keys(self):\n\t\t\"\"\"\n\t\tReturn all keys in the order they were added.\n\t\t\"\"\"\n\t\tif isinstance(self[0], list):\n\t\t\treturn self[0]\n\t\telse:\n\t\t\tfor k in self.keys():\n\t\t\t\tyield k\n\n\tdef values(self):\n\t\t\"\"\"\n\t\tReturn all values in the order they were added."}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\traise NotImplemented\n\n\n\tdef has(self, key):\n\t\t\"\"\"\n\tReturn true if this item has the given property. Otherwise false.\n\n\t:param key: a key\n\t:return: bool\n\t:rtype: bool\t\"\"\"\n\t\traise NotImplemented\n\n\n\tdef get(self, key):\n\t\t\"\"\"\n\tReturn a list of the property names.\n\n\t:param key: a key\n\t:return: a list of strings\n\t:rtype: list of string\t\"\"\"\n\t\traise NotImplemented\n\n\n\tdef get_by_key(self, key):\n\t\t\"\"\"\n\tReturn"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tif key == \"name\":\n\t\t\treturn self.name_id\n\t\telif key == \"version\":\n\t\t\treturn self.version\n\t\telif key == \"path\":\n\t\t\treturn self.path_id\n\t\telif key == \"type\":\n\t\t\treturn self.type_id\n\t\telif key == \"size\":\n\t\t\treturn self.size\n\t\telif key == \"pathtype\":\n\t\t\treturn self.pathtype_id\n\t\telif key == \"mtime\":\n\t\t\treturn self.mtime\n\t\telif key == \"atime\":\n\t\t\treturn self.atime\n\t\telif key == \"n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tif type(key)!= str or type(key) is unicode:\n\t\t\treturn None\n\n\t\tif len(key) < 1 or key[0]!= '#':\n\t\t\treturn None\n\n\t\t# TODO: Implement search functionality to find out the right items\n\t\t# with the same key and return the result\n\t\t# and the given item\n\t\titem = self.get_item(key)\n\t\tassert item!= None\n\t\treturn item.index\n\n\tdef has_key(self, key):\n\t\t\"\"\"\n\tCheck you have a dictionary with the given key.\n\t\n\t:param key:"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\t# it seems faster if you set this after read\n\t\tkeylen = len(key)\n\t\ti = 0\n\t\twhile keylen:\n\t\t\tj = ord(key[i]) + (ord(key[i+1]) << 8)\n\t\t\ti += 1\n\t\t\tkeylen = len(key)\n\n\t\treturn i\n\n\tdef find_by(self, name):\n\t\t\"\"\"\n\tReturns the number of entries with the given name within the cache.\n\t\"\"\"\n\t\tresult = [p.index for p in cache.get_all_pages() if p.key =="}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._items.index(key)\n\n\tdef last(self):\n\t\t\"\"\"\n\t\tReturn last element in the list.\n\t\t\"\"\"\n\t\tif self: return self[-1]\n\t\telse: return None\n\n\tdef items(self): return self._items.copy()\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('crm', '0006_auto_20160821_1945'),\n    ]\n\n    operations = [\n        migrations"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._list.index(key.element)\n\n\t@Property\n\tdef text(self): return self._list.text\n\t@Property\n\tdef parent(self): return self._list.parent\n\t@Property\n\tdef previous(self): return self._list.previous\n\t@Property\n\tdef next(self): return self._list.next\n\t@Property\n\tdef previousIndex(self): return self._list.previousIndex\n\t@Property\n\tdef nextIndex(self): return self._list.nextIndex\n\t@Property\n\tdef indexInParent"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif keys is None:\n\t\t\tindices = list(range(len(self)))\n\t\telse:\n\t\t\tindices = []\n\t\t\tfor key in keys:\n\t\t\t\tindices.append(self.values(key))\n\t\treturn [self.__class__(self[index] if index < len(self) else self[index] + \"t\", self._values[index]) for index in indices]\n\n\tdef insert(self, index, key, value, case_sensitive = True, make_list = False):\n\t\t\"\"\"Insert new key with optional value. If case_sensitive is False,\n\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn self.mget(keys, self.defaults)\n\n\tdef pop(self, *keys):\n\t\t\"\"\"\n\t\tRemove and return a pair of the values at the given index(s) from\n\t\tthe data list. If no record exists there is no error. Else if multiple\n\t\tvalues matched the filter is undefined, the entire list of values\n\t\twill be returned.\n\n\t\tExample:\n\t\treturn items.pop(index1, 1) == values1 [1]\n\t\treturns items, including the second one for which there is more than one\n\t\tvalue with the matching keys ("}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tvalues = []\n\t\tfor k in keys:\n\t\t\tif k in self._fields:\n\t\t\t\tif len(values) == 1:\n\t\t\t\t\t# no need to count a list\n\t\t\t\t\treturn values\n\t\t\t\tvalues = []\n\t\t\t\tvalues.extend(filter(lambda (x,y): y._hasIndex(k), values))\n\t\t\telse:\n\t\t\t\tvalues = values + filter(lambda k: k in self._fields, values)\n\t\treturn values\n\nclass RecordListField(_ListStorageField):\n\t\"\"\"\n\tAn Field subclass that stores a list of items, where each item\n"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif keys:\n\t\t\t# apply indexes to filter lists of indexes\n\t\t\treturn [item for item in self._values\n\t\t\t\t    if item['key'] in keys]\n\t\telse:\n\t\t\treturn list(self._values)\n\t\n\tdef contains(self, key, value):\n\t\t\"\"\"\n\tCheck if the records contain the specified key or value\n\t(case insensitive).\n\n\t:param key: the name of the field to match\n\t:param value: the value to test for\n\t\t:return: True/False of this field value exists,\n\t\t\texcept otherwise throws an InvenioWebRequestOn"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tassert_single(keys, \"value must be a list of index-keys\")\n\t\tassert_single(not isinstance(keys, (tuple, list)),\n\t\t\t\"keys must be a sequence\")\n\n\t\tdef flatten(dict): # {{{1: 0, 2: 1}} => {0: 1, 1: 2}\n\t\t\treturn dict.items() if isinstance(dict, dict) else dict\n\n\t\treturn list(map(flatten, keys))\n\nclass Evolver(object):\n\t\"\"\"\n\tA class that can be used to execute a single query of\n\tall documents"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif len(keys) == 0:\n\t\t\tvalues = self.values_at(self.record, 0)\n\t\telse:\n\t\t\tvalues = []\n\t\t\tfor key in keys:\n\t\t\t\tif key not in self.indices:\n\t\t\t\t\tmask = self.record.fields.get(key, None)\n\t\t\t\t\tif mask:\n\t\t\t\t\t\tmask = mask.lower()\n\t\t\t\t\telse:\n\t\t\t\t\t\tmask = None\n\n\t\t\t\t\tindices = sorted(self.indices[key].items())\n\t\t\t\t\tfor index in indices:\n\t\t\t\t\t\tif mask:\n\t\t\t\t\t\t\tmask = mask.lower()\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif keys:\n\t\t\treturn map(lambda x:getattr(self,x),self.values(*keys))\n\t\telse:return self.values()\n\n\tdef iteritems(self):\n\t\t\"\"\"\n\t\tReturn an iterator over (key-value pairs) for\n\t\ta dictionary of the given record or list of key-value\n\t\tpairs. Does not currently cache the result.\n\t\t\"\"\"\n\t\tif not self._dirty:\n\t\t\tself.flush()\n\t\treturn iter(self.index.iteritems())\n\n\tdef itervaluerefs(self):\n\t\t\"\"\"\n\t\tReturn an iterator over object"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn self._filter(lambda x: getattr(self, key)(x), keys)\n\t#\n\t# def key(self, name):\n\t# \treturn self._filter(lambda x: getattr(self, name).key)\n\t#\n\t# def __len__(self):\n\t# \treturn len(self._filter(lambda x: getattr(self, x).key))\n\t#\n\t# def __iter__(self):\n\t# \t'''\n\t# \tReturn an iterator over all records as tuples\n\t# \t(row, keys"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\titems = self._db.select(self._key, *keys)\n\t\tif items and len(items) > 0: return items\n\t\traise IndexNotFoundError\n\n\tdef items(self, **keys):\n\t\t\"\"\"Return a record as dictionary or None if the item\n        do not exist for any key in the `keys` parameter.\"\"\"\n\t\titems = self._db.select(self._key, **keys)\n\t\tif items: return items\n\t\treturn None\n\n\tdef get_keys(self):\n\t\t\"\"\"Return a list of keys, optionally empty if nothing is passed\"\"\"\n\t\tkeys"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif keys is None:\n\t\t\tkeys = self.getIndexKeys()\n\t\tkeys = [str(key).strip() for key in keys] # make sure list contains only valid index keys\n\t\t# prepare query\n\t\tquery = [self._getFilteredQuery()]\n\t\t# get all unique index values by index\n\t\tfilteredQuery = []\n\t\tfor indexName in keys:\n\t\t\tquery.append(self._getSortedQuery(indexName, query))\n\t\t# add filtered values to query\n\t\tquery = self._query_from_filtered(query, filteredQuery)\n\t\t# replace"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif keys == []:\n\t\t\tquery = '''SELECT %s FROM %s_rec WHERE %(key)s='%(key)s';'''\n\t\telse:\n\t\t\tquery = '''SELECT %s FROM %s where key IN (NULL,?) ORDER BY %(key)s'''\n\n\t\tif len(keys) == 1:\n\t\t\tidx = keys[0]\n\t\t\tif idx == None:\n\t\t\t    key = 'id'\n\t\t\telif not hasattr(idx, 'keys'):\n\t\t\t    raise IndexError( \"\"\"No %s fields in table %s\"\"\" % \n\t\t\t\t\t\t\t\t (key,self.tablename) )\n\t\t    else"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif len(keys) == 0:\n\t\t\t# Return immediately if no key is provided\n\t\t\treturn { key: None for key in self.keys() }\n\t\ttry:\n\t\t\treturn { key : _unpack( val ) for key, val in keys }\n\t\texcept IndexError:\n\t\t\traise ValueError(\"'{}' not found in {} list\".format( keys, self.name.string()))\n\n\tdef __getitem__( self, keys ):\n\t\t\"\"\"\n\t\tReturn items from the index specified by *keys*,\n\t\treturning :const:`None`\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttry:"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tresults = {}\n\t\tfor key in keys:\n\t\t\tif key not in self:\n\t\t\t\tlog.debug(\"key %s not present in record\", key)\n\t\t\t\tcontinue\n\t\t\tv = self.get(key, ())\n\t\t\tif len(v) == 0:\n\t\t\t\tcontinue\n\t\t\tif v[0].startswith('{') and v[-1].startswith('>'):\n\t\t\t\tv = v[1:-1]\n\t\t\tif v!= None:\n\t\t\t\tresults[key] = v\n\n\t\treturn results\n\n\tdef __iter__(self):\t\t# (key, value) tuple\n\t\t"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\trows = []\n\t\tfor k in keys:\n\t\t\tif not self.out_of_bounds(k):\n\t\t\t\traise IndexError(\"Out of bounds indices\")\n\t\t\tvalue = getattr(self, k)\n\t\t\tif value is None:\n\t\t\t\tfor row in rows:\n\t\t\t\t\trows.append(row)\n\t\t\telse:\n\t\t\t\trows.append(value)\n\t\treturn rows\n\n\t# for compatibility with old versions,\n\t# see also documentation\n\tdef out_of_bounds(self, key):\n\t\t\"\"\"\n\t\tSearches for values to be included in the specified index\n\t"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif keys is None:\n\t\t\titems=[]\n\t\t\tfor item in self.items:\n\t\t\t\titems= [x[:item[0]]+[\"...\"]+_v for x in item[1:]]\n\t\t\treturn items\n\t\telse:\n\t\t\titems= [x[:][x[0]]+[\"...\"]+_v for x in keys]+[\"...\"]+_v for x in keys]\n\t\t\treturn dict(items)\n\n\t@classmethod\n\tdef load(cls, f, keys=None, **kwargs):\n\t\titems = cPickle.load(f)\n\t\tif keys is None:"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\treturn self.get(keys)\n\n\tdef iteritems(self):\n\t\t\"\"\"items iterator in the fields\"\"\"\n\t\tfor k in self._index:\n\t\t\ttry:\n\t\t\t\tyield self[k]\n\t\t\texcept TypeError:\n\t\t\t\tcontinue\n\n\tdef values(self):\n\t\t\"\"\"items iterator in order\"\"\"\n\t\tfor k in self._index:\n\t\t\ttry:\n\t\t\t\tyield self[k]\n\t\t\texcept TypeError:\n\t\t\t\tcontinue\n\n\tdef iteritems_with_key(self, key):\n\t\t\"\"\"iterating over key-value mappings, with optional index\"\"\"\n\t\tfor k in"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif keys:\n\t\t\treturn dict(zip(self._fields.keys(), keys))\n\n\t\treturn dict(zip(self._fields.keys(), xrange(len(self._fields))))\n\n\tdef insert(self, rec):\n\t\t\"\"\"\n\tInsert a new record into this record set and return\n\tthe new record.\n\t\"\"\"\n\t\tif issequence(rec):\n\t\t\tif isinstance(rec, dict):\n\t\t\t\tif len(rec) == 1:\n\t\t\t\t\trec = seq_union\n\t\t\telif len(rec) > 0:\n\t\t\t\traise ValueError(\"First record (%s) of"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif self.minpos is None or self.maxpos is None:\n\t\t\traise ValueError(\"Both'minpos' and'maxpos' required\")\n\t\t\n\t\tif len(keys) == 0:\n\t\t\tif self.key is None:\n\t\t\t\treturn self.defaultValue, self.defaultValue\n\t\t\telse:\n\t\t\t\t# TODO: check if all index keys are present in each field; \n\t\t\t\t# if so, raise an error.\n\t\t\t\traise IndexError\n\t\tif len(keys) == 1:\n\t\t\tif keys[0] == \"*\":\n\t\t\t\treturn self.defaultValue, self.defaultValue\n\t\t\t"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\t# get the list of the fields in the index and the values\n\t\tfields = [x[0] for x in keys]\n\t\t# iterate over all fields and assign it a tuple containing\n\t\t# the index and the value\n\t\tdef get_f(idx):\n\t\t\t# index is the key value\n\t\t\t# return values are of type string: integer, float...\n\t\t\tif isinstance(idx[0], basestring):\n\t\t\t\t# value is the item itself, try converting to float\n\t\t\t\ttry:\n\t\t\t\t\t# try converting from a string\n\t\t\t\t\titem = self.by_name(idx["}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif len(keys) == 0:\n\t\t\traise IndexError('Keys of all records (%d) were null' % len(keys))\n\n\t\ttry:\n\t\t\tv = getattrs(self.record, '%s_values' % self.field_name)\n\t\t\treturn OrderedDict((record[v] if v in record else None)\n\t\t\t  for record in zip(*getattrs(self.record, '%s_values' % self.field_name)))\n\t\texcept AttributeError:\n\t\t\traise KeyError('Record no longer exists for: %r' % self.field_name)\n\n\tdef __"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tif self.mode == \"NF\":\n\t\t\tself.f.seek(8,\">\")\n\t\telse:\n\t\t\ti = 0\n\t\t\twhile i < len(self.f)-3:\n\t\t\t\tself.f.read(1)\n\t\t\tself.f.seek(10+i,0)\n\t\t\ti+=3\n\t\t\t\nclass CNC_Int8(CCN):\n\t\"\"\"\n\tDecode ASCII as signed byte \n\t\"\"\"\n\t# 2-bit signed values\n\tunsigned_32_le = 8\n\tunsigned_96_le = 56\n\tunsigned_128_"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tret = self.read_u16()\n\t\treturn ret\n\n\tdef pop_u32(self):\t\"\"\"\n\tReturn the next 4 bytes of character data, as a single long long\n\tstring.\n\t\"\"\"\n\t\tret = self.read_u32()\n\t\treturn ret\n\n\tdef pop_byte(self):\t\t\t\t\treturn self.read_byte()\n\n\tdef pop_u16le(self):\t\t\t\t\treturn self.read_u16le()\n\n\tdef pop_u32le(self):\t\t\t\t\t\treturn self.read_u32le()\n\n\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn self.__heap.pop_u16()\n\n\tdef pop_u32(self):\t\"\"\"\n\tTake a big-endian 32-bit unsigned integer and return the integer.\n\t\"\"\"\n\t\treturn self.__heap.pop_u32()\n\t\n\tdef xadd(self, elem):\n\t\t\"\"\"\n\t\tGet the sum of the two values stored in element.\n\t\t\"\"\"\n\t\tif sys.version_info < (3,):\n\t\t\telem = elem.encode()\n\t\treturn self.__heap.xadd(elem)\n\n\tdef xradd(self, a):\n\t\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tdata = self.pop()\n\t\tif (data[-2] == 0x7f):\t# big-endian\n\t\t\treturn struct.unpack('<h', data)[0]\n\t\telse:\n\t\t\treturn struct.unpack('<h', data)[0]\t# no big-endian 16-bit\n\n\tdef peek(self, length):\n\t\t\"\"\"\n\t\tThe method returns a 4-byte string representing the given length.\n\t\t\"\"\"\n\t\tdata = self.pop()\n\t\tif (len(data) < length):\t\t# quick-fix for odd length? this is not a"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tnbuf= self._nbuf\n\t\taddr= self._addr_i\n\t\tfor i in xrange(nbuf):\n\t\t\ts=addr+i*8\n\t\t\te=self._buf[s:]\n\t\t\tself._addr+=s\n\t\t\ts=s>>8\n\t\t\te=self._buf[e:]\n\t\t\ts=0\n\t\t\tr=0xF\n\t\treturn r\n# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd.\n# MIT License. See license.txt \n\nfrom __future__ import unicode_literals\nimport webnotes\nfrom webnotes import msg"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tret = self.u16.pop()\n\t\tret += self.n * struct.unpack(\">H\", ret)[0]\n\t\tif type(self.version) == int:\n\t\t\treturn struct.unpack(\">H\", ret)[0]\n\t\telse:\n\t\t\treturn ret\n\tdef pop_u32(self):\t\t\"\"\"\n\tReplace the low 32 bits of the data, returning 0x00.\n\t \"\"\"\n\t\tret = self.u32.pop()\n\t\tret = ret + chr(self.version)\n\t\treturn ret\n\tdef pop_u64(self"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tn = self._fp.read(4)\n\t\tdata = self._fp.read(n)\n\t\tret = struct.unpack('>I', data)[0]\n\t\treturn ret\n\n\tdef _read_num(self):\n\t\t\"\"\"\n        Read next num bytes, returning a Python long int.\n\t\t\"\"\"\n\t\treturn self._fp.read(4)\n\n\tdef _read_line(self):\n\t\t\"\"\"\n        Read next line, returning a list of 3 bytes length (as integer)\n\t\t\"\"\"\n\t\tsize, = struct.unpack('>I', self._fp.read"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tc = self._read(4) # read 4 byte header\n\t\tl = struct.unpack('B', c)[0]\n\t\treturn l\n\n\tdef _decode(self, s):\n\t\tn = 1\n\t\tb = ''\n\t\twhile n < len(s):\n\t\t\tif s[n] == 0:\n\t\t\t\traise ValueError(\"Invalid end of string.\")\n\t\t\tb = s[n]\n\t\t\tn += 1\n\t\tif not b == ';':\n\t\t\traise ValueError(\"String ended unexpectedly.\")\n\t\treturn b\n\n\tdef _encode(self, s):\n\t\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tm_str = self._readregs(REGS_U16)\n\t\treturn b2x(m_str[1], 16)\n\t\n\tdef pop_u32(self):\t\"\"\"\n\tRemove the two bytes of data, returning them as a big-endian 32-bit\ninteger.\t\"\"\"\n\t\tm_str = self._readregs(REGS_U32)\n\t\treturn b2x(m_str[0], 32)\n\t\n\tdef _readregs(self, reglist):\n\t\tregs = []\n\t\tfor reg in reglist:\n\t\t\tregval"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tself.check_byte(0x1e, 'ushort')\n\t\tself.check_word(0x23, 'ushort')\n\t\tself._flush_char(0x27)\n\t\treturn self._u[0x27:0x14].tobytes()\n\n\tdef pop_ushort(self):\n\t\t\"\"\"\n\t\tRemove the last two bytes of data, returning them as a big-endian\n\t 16-bit unsigned integer.\n\t\t\"\"\"\n\t\tself.check_byte(0x1e, 'ushort')\n\t\tself.check_word(0x23, 'ushort"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tpass\n\n\t#def reset_handlers(self, handlers={}):\n\n\t#\tself.handler_funcs = {}\n\t#\tself.handler_funcs.clear()\n\n\t#\tfor n in self.packet_list:\n\t#\t\tself.handler_funcs[n]=self.default_handler\n\n\tdef add_handler(self, handler, prio=10):\n\t\t\"\"\"\n\t\tAdd a handler function handler. Can be overridden with a specific prio.\n\t\tCan be called multiple times.\n\n\t\t:param handler: handler function\n\t\t:param prio: no effect\n\n\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n\n\traise NotImplementedError\n\n\t# Default error handler.\n\tdef default_error_handler(self, err, req, args):\n\t\traise Exception(self.default_error)\t\n\t\n\t# Default handler.\n\tdef default_error_rh(_self):\n\t\tpass\n\t\n\t# Error handlers.\n\tdef default_handler_default(self, ctx, req, args):\n\t\thandler = self.default_handler(ctx.err.handler)\n\t\tif not handler:\n\t\t\traise Exception(self.default_error)\n\t\telse:\n\t\t\thandler(req.content)"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\traise NotImplemented(\"Handlers are not valid for non-dehydration messages\")\n\t#\n\t# we ignore all of the callbacks, it doesn't add them to anything, this is done on purpose.\n\t#\n\t# we should only keep the callbacks which are not callable.\n\t#\n\tif functions is None: functions = collections.defaultdict(list)\n\tif actions is None: actions = collections.defaultdict(list)\n\n\tdef wrapper(msg):\n\t\tdata = msg.pack_content()\n\t\tif n == -1:\n\t\t\tif \"message\" in data"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\traise Exception (\"Not supported\")\n\t\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = dict()\n\n\tdef wrapper (obj, handler, *args, **kwargs):\n\t\t# handler function name: dispatcher\n\t\tif handler in dehydration_hooks:\n\t\t\thandler = dehydration_hooks[handler]\n\t\t\n\t\t# handler arguments: dispatcher\n\t\tif args or kwargs:\n\t\t\thandler(obj, *args, **kwargs)\n\t\t\n\t\t# handler return value:\n\t\trv = _Handler(obj, handler(d=obj, p="}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\treturn super(OFPHelloReply, self).discard(n, qid, dehydration_hooks, handlers)\n\n\tcls = OFPHelloReply\n\ttypes = {}\n\n\tdef __init__(self, datapath, buffer_id=None, total_len=None):\n\t\tsuper(OFPHelloReply, self).__init__(datapath)\n\t\tself.buffer_id = buffer_id\n\t\tself.total_len = total_len\n\n\t@classmethod\n\tdef parser(cls, datapath, version, msg_type, msg_len, xid, buf):\n\t\toffset = ofproto"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n\t\t\n\t\toutqueue = self._outqueue\n\t\tself._outqueue = {}\n\t\tself._qid = qid\n\t\tself.__dehydration_hooks = dehydration_hooks\n\t\tself.__hydration_hooks = set()\n\t\tself.__handlers = handlers\n\t\toutqueue.put((n,qid,dehydration_hooks,handlers,self.__handler_lock,self._handler_cache))\n\t\n\tdef __del__(self):\n\t\tpass # to be implemented by subclass   \n\t\n\t\n\tdef _handler_dehydrate(self, packet"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n# ----------------- MAIN ----------------- #\n\ndef main():\n\timport logging\n\tlogFormatter = logging.Formatter(\"%(levelname)s: %(message)s\")\n\tlogHandler = logging.StreamHandler()\n\tlogHandler.setFormatter( logFormatter)\n\tloghandler.setLevel(logging.DEBUG)\n\tlogHandler.addFilter(logging.Filter(\".*\"))\n\n\ttry:\n\t\topts, args = getopt.getopt(sys.argv[1:], \"n:dq:g:r:m:m:c:Hhi\")\n\texcept getopt.GetoptError:\n\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t \n\t\n\n\tdef __enter__(self):\n\t\tself.recv_thread = threading.Thread(target = self.recv)\n\t\tself.recv_thread.start()\n\n\t\n\tdef __exit__(self, type, value, traceback):\n\t\tself.recv_thread.join()\n\t\tself.sock.close()\t\n\n\n\t#\n\t#\t[END pubsubnub]:\n\n\tdef send(self, type, **data):\n\t\t\"\"\"\nThis method does nothing, just call send_response(), except when you call close()\nor when you want to. Also when you"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t  \n\tif True:\n                logging.info(\"Dropping packets from the queue: {}\".format(id))\n\t\tpackets = [self.remove_packet(pack2bin(b)) for b in packets]\n\n\tif False:\n                logging.info(\"Dropping all packets from the queue: {}\".format(id))\n                self.remove_packets(packets)\n\n\tself.push_packets(packets)\n# -*- coding: utf-8 -*-\n\n# Copyright (C) 2012-2019 Red Hat, Inc.\n#\n# This copyrighted material is made available to anyone wishing"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\traise NotImplementedError(\"Should be implemented by subclasses\")\n\tif dehydration_hooks:\n\t\tself._dehydrate_types(dehydration_hooks)\n\t\tself.outq.append(n)\n\n\tif self.outq and self.outq[0][0] == qid:\n\t\tif self.outq[0][1]!= qid:\n\t\t\traise AssertionError(\"outq(%s, %s) == %d %s\"\n\t\t\t\t\t\t % (qid, n, self.outq[0][0], n))\n\t\tn +="}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif not settings.USE_AMQP: #pragma: no cover\n\t\t\traise Exception(\"AGQL requires a non-amqp connection. install ambqp or impala[amqp].\")\n\t\tmode = mode or settings.AMQP_MODE\n\t\tbookmarks = bookmarks or []\n\t\ttry:\n                        #check if the mode is in read-write\n\t\t\t\t\t\t#and mode is write-only and the bookmarks is ok\n\t\t\tif mode:\n                    if settings.USE_AMQP == True:\n                        if settings.CREATE_DB == True:\n                            raise Exception(\"AMQP - Only one argument of create_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif not self.connected:\n\t\t\t\traise BoltException('Connection not established')\n\n\t\t# create request packet\n\t\trequest = build_create_request(mode, bookmarks, metadata, timeout, db, dehydration_hooks)\n\n\t\t# send request\n\t\tif not self.started:\n\t\t\tself.send_connection_packet(request)\n\n\t\t# check request status\n\t\tif request.state!= REQUEST_STATE_START:\n\t\t\traise BoltException('Request is not in START state')\n\n\t\tyield 'BEGIN'\n\t\t# send response\n\t\tself.send_response_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\ttry:\n\t\t\tif self.mode == Mode.READ:\n\t\t\tself.__connection.begin( mode=mode, bookmarks=bookmarks, metadata=metadata\n\t\t\t, timeout=timeout, db=db, imp_user=imp_user\n\t\t\t)\n\n\t\t\telif self.mode == Mode.INSERT:\n\t\t\tself.__connection.begin( mode=self.mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db)\n\n\t\t\telif self.mode == Mode.UPDATE:\n\t\t\tself.__connection.begin( mode=self.mode, bookmarks=bookmarks, metadata=metadata,"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tpass\n\n\tdef __repr__(self):\n\t\treturn \"begin: mode = %r, bookmarks = %r, metadata = %r, timeout = %r, db = \"%s\" % \\\n\t\t\t(self.mode, bookmarks, self.metadata, self.timeout, self.db)\n# -*- coding: utf-8 -*-\n#\n# Copyright 2017 Ricequant, Inc\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\t\"\"\"Send START message\"\"\"\n\t\timport sys\n\t\timport warnings\n\t\twarnings.warn(\"begin function deprecated\", DeprecationWarning, 2)\n\t\treturn Response(\"BEGIN\",\"OK\",\n\t\t\t_assemble_begin(str(self)),\n\t\t\t_encode_msg,\n\t\t\tmode=mode,\n\t\t\tbookmarks=bookmarks,\n\t\t\tmetadata=metadata,\n\t\t\ttimeout=timeout,\n\t\t\tdb=db,\n\t\t\timp_user=imp_user,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks,\n\t\t)"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tassert(mode in ['READ', 'WRITE', 'ADMIN'])\n\t\tassert(bookmarks is None or bookmarks == [])\n\t\tself.request = method\n\t\treturn self\n\n\tif mode == 'ADMIN':\n\t\tself.mode = 'ADMIN'\n\t\tself.bookmark.append({\n\t\t\t'params': { 'db': db }, 'timeout': (timeout, timeout),\n\t\t\t'hooks': dehydration_hooks })\n\t\tself.bookmarks = bookmarks\n\t\tself.metadata = metadata if metadata else {}\n\t\tself.request['extensions-enabled'] = True\n\n\t\tif mode == '"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\n\n\tif mode:\n\t\tmessage = \"begin(mode={}, bookmarks={}, metadata={}, timeout={}, db={}, imp_user={}, dehydration_hooks={}, handlers={})\".format(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, handlers)\n\telse:\n\t\tmessage = \"begin(bookmarks={}, metadata={}, timeout={}, db={}, imp_user={}, dehydration_hooks={}, handlers={})\".format(bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, handlers)\n\n\treturn self._request_builder(self"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\traise BoltException(\"begin is not implemented\")\n\n\trequest = IRequest(self.__version, mode, bookmarks, metadata)\n\trequest.type = IType.BeginTransaction\n\trequest.max_length = timeout\n\n\tif mode in [\"READ\", \"WRITE\"]:\n\t\trequest.flags = IRequest.ReadWrite\n\telif mode == \"GET\":\n\t\trequest.flags = IRequest.Read & ~IRequest.Write\n\telif mode == \"INSERT\":\n\t\trequest.flags = IRequest.Write\n\treturn self._request(request)\n\n\tdef _request(self"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif isinstance(callback, BoltCollection):\n\t\t\tfor obj_type, obj in self.to_obj.items():\n\t\t\t\tself.obj_wrapper[obj_type] = obj\n\t\t\t\tif isinstance(obj, BoltError):\n\t\t\t\t\traise self.obj_wrapper[obj_type](obj, *args, **kwargs)\n\t\t\t\telif inspect.isclass(obj) and issubclass(obj, BoltError):\n\t\t\t\t\traise self.obj_wrapper[obj_type](obj, *args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tfor hook in dehydration_hooks:"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\t(o,a) = self._unpack(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, handlers)\n\t\treturn OutputQueueResponse(self, o, a)\n\n\tdef begin_transaction(self, transaction=None, timeout=None, db=None, imp_user=None, dehydration_hooks=None, handlers=None):\n\t\t\"\"\"\n\t\tBegins the transaction on the specified connection.\n\n\t\t:param transaction: an instance of :class:`Transaction`\n\t\t:param timeout: timeout for transaction execution (seconds)"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n### Constants ####\n\nNOISE_ONE = \"NOISE ONE\"\n\"\"\" Noise 1: TIP (TIP to TIP)\nTIP: TIP to IEEE-754\nIEEE-754: (IEEE 802.15.4)\n\n:return:\n\"\"\"\n\nNOISE_HALF = \"NOISE HALF\"\n\"\"\" Noise 1: HALF (DOUBLE TO TIP, TIP to IEEE-754 format)\nDOUBLE TO TIP: (IEEE 802.15.4)\nIEEE-754: (IEEE 802.15.4)\n\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t     return round((n+1)/2)\n\n\nif __name__ == \"__main__\":    \n\n    print(round_half_to_even(1))\n    print(round_half_to_even(2))\n    print(round_half_to_even(4))\n    print(round_half_to_even(5))\n    print(round_half_to_even(5.2))\nimport operator\nfrom itertools import chain\n\n\n# Evaulation of the number of primes (count of primes) is given by\n# the following equation:\n#   n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\tn_float = int(float(n))\n\t\trounded = round_half_to_even(n_float)\n\t\trounded = int(rounded)\n\t\treturn rounded\n\n\telse:\n\t\treturn n\n\n\nif __name__ == '__main__':\n\tprint (\"1.7\")\n\tprint (\"2\")\n\tprint (\".3\")import numpy as np\nimport numpy.random as rnd\nimport pytest\n\nimport pandas as pd\n\nimport ibis\nimport ibis.expr.datatypes as dt\nimport ibis.expr.types as ir\n\npytest"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\n\tif n < 1:\n\t\traise Exception(f'N has to be >= 1')\n\telif n == 0:\n\t\treturn 1\n\telif abs(n) < 5:\n\t\treturn round_half_to_odd(n)\n\telse:\n\t\treturn int(round_half_to_even(int(math.floor(n))))\n\ndef round_half_to_odd(n): \n\t\"\"\"\n\t>>> round_half_to_odd(3)\n\t3\n\t>>> round_half_to_odd(3)\n\t3"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\t# Round half to even.\n\t\tif n % 1 == 0:\n\t\t\tn += 1\n\n\t\t# We can't be more than 1 and have the following case:\n\t\t# 1 + -0.5 = 1, but because when we divide by three and we have\n\t\t# above 2, we cannot see out of order:\n\t\t# 1/1.2 = 0.5\n\t\t# 1/1.5 = 0.5\n\t\t# 1/2 = 0.5\n\t\t# 1/3 = 0.5\n\t\t# 1/4 = 0.5\n\t\t"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\treturn n\n\n'''\nConvert decimal number to integer with exponent:\n'''\ndef DecimalToInt(f):\n\timport decimal\n\treturn decimal.Decimal(str(f))\t\n\n'''\nFormat given decimal number to decimal number using format \"%g\".\n'''\ndef FormatNumber(n):\n\timport decimal\n\treturn \"%.*g\" % DecimalToInt((n))\n\n'''\nFormat float number to float decimal number and return\na float.\n'''\ndef FloatToFloat(f):\n\timport float\n\treturn str( round_half_to_even("}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\tif n < 100:\n\t\t\treturn n\n\t\telse:\n\t\t\treturn round(n) / 2.0\n\ndef calculate_percent_to_even(n):\t\t\"\"\"\n    >>> calculate_percent_to_even(-25)\n    -4\n    >>> calculate_percent_to_even(-35)\n    4\n    >>> calculate_percent_to_even(-25)\n    4\n    >>> calculate_percent_to_even(-15)\n    3\n    >>> calculate_percent_to_even(-10)\n    4\n    >>> calculate_percent_to_even(+1)\n    3\n    >>>"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\tif n < 0:\n\t\t\treturn str(int((10 + math.floor(math.log10(abs(0.125)))) * n))\n\t\telse:\n\t\t\treturn str(int((10 + math.floor(math.log10(abs(n)) % 1)) * n))\n\nif __name__ == '__main__':\n\tmain()\n__author__ ='maciej'\nfrom django.template import TemplateDoesNotExist, TemplateSyntaxError\nfrom django.core.urlresolvers import reverse\nfrom django.conf import settings\nfrom django.template.defaultfilters import stringfilter\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\tn = float(n)\n\t\td = int(round(n))\n\t\treturn n + d * (1 / d)\n\n        \ndef mod(n):\t\treturn math.floor(n * math.ceil(math.log(n)))\n\n\t\t\ndef mod_x(n):\t\treturn math.floor(math.log(n)) \n\n\n\ndef add_y(x, y):\t\treturn x + y\n\n\ndef add_z(z):\treturn z + 360\n\n\ndef add(x1, y1, x2, y2):\t\t"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t  \n\treturn n * math.log(1.0 - n)\n\ndef round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(10000001.0)\n\t0.0021324414745951\n\t\"\"\"\n\treturn round_half_to_even(n * 0.25)\n\ndef round_half_to_even_decimals(n):\n\t\"\"\"\n\t>>> round_half_to_even_decimals(12.5)\n\t0.0021324414745951"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tname_parts = []\n\t\tif isinstance(srid_map, basestring):\n\t\t\tparts = srid_map.split(\";\")\n\t\t\tfor part in parts:\n\t\t\t\tname_parts.append(part.strip())\n\t\telse:\n\t\t\tname_parts = srid_map.split(\";\")\n\t\tFieldsClass = get_adapted_class('Point')\n\t\treturn type(\"Point\", (NameParts,), {\n\t\t\t\"name\": name,\n\t\t\t\"fields\": FieldsClass(fields, extra_types={'name': 'point'}),\n\t\t}, {\n\t\t\t\"name\": \"Point3"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif srid_map and type(srid_map)!= dict:\n\t\t\tsrid_map = srid_map.copy() # make a copy so we're modifying\n\t\t\tcrs_dict = srid_map.pop('crs') # clear out class parameter\n\t\t\tsrid = srs_dict.get('type', 4326)\n\t\t\tif srid: # create instance of SRID, return as type\n\t\t\t\tdtype = getattr(settings, type(srid))\n\t\t\telse: # create geometry from WKT\n\t\t\t\tdtype = geos.linestring.get_type(name)\n\t\telse: # probably an integer field\n\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\t# Copy the coordinate sequence\n        fields = list(fields)\n        fields[0] = Point(points=[], srid=srid_map[name].get_srid())\n        fields.insert(1, MultiPoint(points=[Point(0, 0), Point(1, 0)]))\n        # Reference the SpatialReference object to create a LinearRing\n        # and check into Area models.\n        geom = Point(data=Point(4, 23), srid=srid_map[name].get_srid())\n        tol = 0.00001 # Check for tolerance argument.\n        for name, geo in fields:\n                tol = tol +"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\treturn Point(srid=srid_map[fields['name_field']], instance=Point(srid=srid_map[fields['srid_field']], **fields['geometry_field']))\n\nclass MultiPoint(GeometryBase):\n\t\"\"\"\n\tA MultiPoint is just a combination of MultiPoints (for example,\n\ta Polygon).\n\t\"\"\"\n\tpoints = MultiPointManager()\n\tmpoly_ring = models.PolygonField()\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself.points = kwargs.pop('points', [])\n\t\tsuper(MultiPoint,"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tpoint = Point(srid_map[name])\n\t\tpoint.srid = srid_map[srid_map[name].geom_type]\t\n\t\tpoint.fields = fields\t\n\t\treturn point\n\t\n\t@staticmethod\n\tdef centroid(geometry, srid_map):\t\t\"\"\"\n\tReturns a Point class with the given coordinates.\t\"\"\"\t\t\t\n\t\tpts = Point()\n\t\tfor name, info in six.iteritems(Geometry._centroid_map):\n\t\t\tif info[\"type\"] == 'Point':\n\t\t\t\tpts = Point(info)\n\t\t\t\t# Make sure create_point returns a"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tn = Point(name, srid=srid_map[name])\n\t\tn.x = fields['x']\n\t\tn.y = fields['y']\n\t\tn.srid = srid_map[n.name]\n\t\tn.save()\n\t\treturn n\n\n\tdef test_point_construction(self):\n\t\t\"\"\"\n\t\tPoint fields should be created with the correct shape.\n\t\t\"\"\"\n\t\tp = Point(1, 2, srid=4326)\n\t\tself.assertEqual(p.geom_type, 'Point')\n\t\tself.assertEqual(p.x, 1"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\treturn Point(srid=int(srid_map[name]), gtype=GeometryCollection(PointGeometry, srid=srs_code))\n\tdef LineString(x, y):\treturn tuple(x), tuple(y)\n\tdef MultiPoint(point_list):\t#nasty, but do special case with one Point.\n\t\tassert len(point_list) == 1\n\t\treturn Point(point_list[0]), Point(point_list[0])\n\tfields = dict([\n\t\tfield(name=str(field))\n\t\tfor field in fields  # note: no"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\timport psycopg2.extras\n\t\tpoint_types = [('POINT', 'POINT')]\n\t\tsupports_mapping = True\n\tconversions = {}\n\tfor f in fields:\n\t\tconversions.setdefault(f, []).append(name)\n\n\tconversions = [PointField(name, type=point_types[i]) \\\n\t\tfor i, name in enumerate(fields)] + \\\n\t\t[PointField(name, type=point_types[i], size=srid_map[f]) \\\n\t\tfor i, name in enumerate(fields)]\n\n\tfor i, t in"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\t# Getting the Type\n        try:\n\t\t\treturn Point(srid=srid_map[name])\n\t\texcept KeyError:\n\t\t\traise ValueError(\"\"\"\n\t\t\tCouldn't find a point type for '%s'.\t\"\"\" % name)\n\n\tdef line_type(self, name, fields, srid_map):\t\"\"\"\n\tDynamically create a LineString subclass.\t\"\"\"\n\t\ttry:\n\t\t\treturn LineString(srid=srid_map[name])\n\t\texcept KeyError:\n\t\t\traise ValueError(\"\"\"\n\t\t\tCouldn't find a line type for '%s'.\t\"\"\" % name)\n\n\tdef geometry_type"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t \n\tpoint_type = type(name, (Point,), {\"srid\": srid_map})\n\tpoint_type.__doc__ =  \"\"\"\n\t\nPoint types that represent Point types.\n\nThe `get_srid_info` function returns a tuple of the `PostGISInfo`\ninstance and the appropriate SRID name for the geometry field.\n\"\"\"\n\n\n#!/usr/bin/env python\n# -*- coding: latin-1; py-indent-offset:4 -*-\n################################################################################\n# \n# This file is part of Bfplusplus\n#\n# Bfplusplus is a graph"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n    def foo(x):\n        \"\"\"\n\tThe foo() decorator.\n\n\t:param x:\n\t\tA function to decorate\n\n\t:rtype:\n\t\tfunction\n\t\t\"\"\"\n        return x\n\n\n\nFOO = 'FOO'\n\ndef deprecated_class_function():\t\"\"\"\n\tDecorator for overriding methods and properties.\n\n::\n\n    class New(object):\t\tdef __init__(self, x):\n\t\tself.x = 2\n\tnew = New()\n\tnew.x = 3\n\tprint(new.x)\t\t# 42\n\"\"\"\n\ndef deprecated_get_"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\n\ndef main(argv):\n\n    \"\"\"\n    Main function\n\n    \"\"\"\n\n\n\n\n    # Create argument parser\n    parser = argparse.ArgumentParser(prog='avro', description='AVRO')\n\n    # Set common options\n    parser.add_argument('-h', '--host', dest='host', required=True, type=str, help='Hostname or IP of avro')\n    parser.add_argument('-w', '--password', dest='password', required=True, type=str, help='Password of avro')\n    parser.add_argument('-c', '--critical',"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n    def bar(x=x):\n        \"\"\"\n        :param x:\n        :return:\n        \"\"\"\n        pass\n\t\t\n\n# TODO: change documentation\n# TODO: make deprecation\n# TODO: add docstrings that will be replaced on API functions\n# TODO: make doc strings optional\n#     to be removed\n# TODO: improve the code when it becomes needed\ndef deprecated_method(f):\n    \"\"\"Decorated version of api if it exists.\"\"\"\n    name=\"deprecated_%s\" % f.__name__\n    name=name.replace(\"_\", \" \")\n    @functools.wraps(f)\n    def wrapper(*"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef _is_deprecated(message):\n    \"\"\"\n    Returns true if the message is being deprecated in favour of\n    'bar'.\n    \"\"\"\n    return not message.__doc__ is None\n\n\ndef _is_deprecated_deprecated(name):\n    \"\"\"\n    Returns True if the given name is deprecated to avoid cropping the\n    deprecation warning when used from another module.\n    \"\"\"\n    \n    if name in _DEPRECATED_WARNING_NAMES:\n        return True\n\ndef deprecated_name(name):\n    \"\"\"\n    A decorator which can be added to a function:\n    @deprecated_name(\"foo\")\n    def foo():\n       "}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\n        # XXX better nice way to do this\n        # assert_true(x)\n\n    try:\n        # First, decorate with @deprecated(message)\n        if hasattr(np.abc, 'deprecated'):\n\t\t\tdeprecated('Deprecated alias:'+ message)\n\t\treturn\n    except:\n\t    with warnings.catch_warnings(record=True) as w:\n\t\twarnings.simplefilter(\"always\")\n\t\tclass Foo(object):\n\t\t    pass\n\n\t\tdef foo(foo1, foo2, foo3):\n\n\t        if foo1 == foo2:\n\t\t        raise Exception, \"foo1 was deprecated\"\n\t\n\t\t       "}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\treturn x\n\n\tdecorate = deprecated(\"foo\")\n\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef wrapped_wrapper(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapped_wrapper\n\treturn wrapper\n\"\"\"\n__all__ = [\n\t\"deprecated\",\n]\n\nimport time, threading, re, sys\n\n_registered = dict()\n\n# For python2.5 compatibility\ntry:\n\tfrom thread import get_ident as get_thread_ident\n\t_get_ident = get_thread_ident"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\t#return func\n\treturn lambda x: \"foo(%r) has been deprecated in favour of 'bar'\"%x\n\nReturns\n-------\nA decorator, which can be used to mark functions\nas unsafe, or used wherever it is or was introduced.\n\"\"\"\n\ndef _makeCallable(ty=None):\n\t\"\"\"\n\tCreate a callable object for this decorator.\n\t\"\"\"\n\n\tif ty is None:\n\t\tdef function(f, out, *args):\n\t\t\t\"\"\"\n\t\t\tReturn a callable object that implements a\n\t\t\t(guarded) callable interface.\n\t\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef isTest():\n\treturn True\n\n#----------------------------------------------------------------------------\n\n\n\"\"\"\nThis config file extends the test environment configuration\nso that we can run the lettuce acceptance tests.\nThat we include the default settings file.\n\"\"\"\n\n# We intentionally define lots of variables that aren't used, and\n# want to import all variables from base settings files\n# pylint: disable=wildcard-import, unused-wildcard-import\n\nfrom.test import *\nfrom openedx.config import TEST_ROOT\n\n# You need to start the server in debug mode,\n# otherwise the browser will not render"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\n\tdef on_call(self, func, args, kwargs):\n\t\tif self.no_args and len(args) > 0:\t\t\n\t\t\treturn func(self, *args, **kwargs)\n\t\telse:\n\t\t\treturn func(self, *args)\n\n\n\nclass FavourTest(unittest.TestCase):\n\n\t# @deprecated(\"Fairly-favour doesn't work\")\n\tdef test_memoize(self):\n\t\t\"\"\"\n\t\tThis test ensures that Favour does something sensible at this\n\t\tlevel.\n\t\t\"\"\"\n\t\t\t\n\t\t# TODO -"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\tpass\n\t\t\n\t@deprecated(\"'foo' has gone away\")\n\tdef foo(x):\n\t\tpass\n\t\t\n\t@deprecated(\"'foo' has gone away (unspecified \" \\\n\t\t\t\"argument)\")\n\tdef foo(x):\n\t\tpass\n\n\t@deprecated(\"'foo' has gone away (unspecified argument)\")\n\t@deprecated(\"'foo' has gone away (unspecified argument \" \\\n\t\t\t\"multiple times)\")\n\tdef foo(x):\n\t\tpass\n\t\t\n\t@deprecated(\"'foo' is gone (unspecified argument \" \\\n\t\t\t\"multiple times)\")\n\tdef foo(x):"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn code and \"\\n\\n\" + code\n\treturn \"\"\t\n\t\t\ndef _inline_r_parse(code: str) -> str:\n\treturn \"_\".join(r.content.strip() for r in code.split(\"\\n\") if len(r.content.strip()) > 0)\n\ndef _inline_r_run(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R starts executing within R. The first time we read the\n\tcontent in as much as possible.\t\"\"\"\n\treturn _parse(_inline_r_parse(code"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn code + \"\\r\"\n\n\ndef _inline_p_setup():\n\t@line_init_comment\n\tdef _inline_p_setup_block_name():\n\t\treturn \"block \"\n\n\tdef _line_gen():\n\t\t# for every line in the input file\n\t\tfor i, line in enumerate(sys.stdin):\n\t\t\tif line.rstrip()!= \"\\n\":\n\t\t\t\tline = line.rstrip()\n\t\t\t\tyield (i, line)\n\n\tline_gen = _line_gen()\n\n\tdef _sub_init():\n\t\treturn \"sub init"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn _R_ENABLED and \"R\" or \"F\"\n\n\telse:\n\t\t_logger.info(Fore.green + \"Setting up python-rb commands...\"\t + Style.RESET_ALL)\n\n\t\t# Add all R settings to R's global namespace.\n\t\tfor var in sorted(config.all_configs()):\n\t\t\tif var.lower() == \"all\":\n\t\t\t\tfor n in global_env:\n\t\t\t\t\tif n.startswith(\"R\"):\n\t\t\t\t\t\t_logger.info(Fore.BLUE + \"  - {} is already configured!\".format(var))\n\t\t\t\t\telif config"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn code\n\ndef _get_runtime_executable() -> str:\n\t\"\"\"\n\tA helper for getting the path of the runtime executable. It\n\tsets the environment variable SYNC_EXECUTABLE.\n\t\"\"\"\n\tpath = os.environ.get('SYNC_EXECUTABLE')\n\tif path:\n\t\treturn path\n\telse:\n\t\tif 'VIRTUAL_ENV' in os.environ:\n\t\t\tprefix = '/usr/local/bin/virtualenv-2.2'\n\t\telif os.name == 'posix':\n\t\t\tprefix = os.path.join('/usr/share"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\t# Disable code on r-options.py because R uses it with R disabled.\n\t\tos.environ[\"R_NO_AUTOESCAPE\"] = \"1\"\n\t\tc = shlex.split(code)\n\t\tc += [\"--r\"]\n\t\treturn c\n\n\t@conf.commands.register\n\tdef r_run(self, ctx, args):\n\t\t\"\"\"\n\t\tRuns r-code in the main process.\n\n\t\tArguments:\n\t\t- ctx: The command's arguments.\n\t\t- args: The command line args.\n\t\t\"\"\"\n\t\tprint(\"Running r-code in the"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tr_opt_val:\t\tdict = dict()\n\t\tfor opt_name in R_OPTS:\n\t\t\tr_opt_val[opt_name] = None\n\t\tr_opt_val.update(os.environ.get(RC_OPT, '').split(':'))\n\t\treturn r_opt_val\n\treturn _inline_r_setup\n\ndef _print_stdout_stderr_msg(r_func: str, stream: IO[str], prefix: str, message: str):\n\tstream.write(\"\\033[91m[{}]:{}\\033[0"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\traise RuntimeError('R does not support inline R')\n\n\t@staticmethod\n\tdef load() -> 'Code':\n\t\treturn Code()\n\n\t@staticmethod\n\tdef run() -> 'Code':\n\t\tif _settings.global_variables.R:\n\t\t\treturn Code(code='R is still in sync')\n\t\telse:\n\t\t\traise RuntimeError('R is not configured yet, so cannot run this command')\n\n\t@staticmethod\n\tdef run_source(source: str) -> 'Code':\n\t\t# Try to load from.env variable and make sure it exists and does not throw an exception"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn '# inline_r is set, please use r_command_line_args().'\n\telse:\n\t\treturn ''\n\t#TODO(g): refactor this and put this in common code\n\n#TODO(g): merge config, get_config\ndef _inline_c_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of c needs to be configured via env variables, and can\nonly be configured via R options once C has started. These are set here.\t\"\"\"\n\t\treturn '# inline_c is set, please use r_command_line_args"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tdef __builtin_function(x_):\n\t\t\tif env.get(x_).get(\"_originalfunc\", None) is not None:\n\t\t\t\treturn env.get(x_).get(\"_originalfunc\", x_).__globals__[x_]\n\t\t\treturn getattr(x_, \"__doc__\", None) if x_.__doc__ is not None else \"\"\n\t\treturn __builtin_function\n\treturn _inline_r_setup\n\n_style_env_vars = {\"title\", \"titleurl\", \"titleref\", \"pageref\", \"pages\"}\n\n\ndef _style_r_template(*"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tsettings = code_string = \"\"\n\t\tif code.startswith(\"env_var.\"):\n\t\t\timport os\n\t\t\tif os.environ.get(\"NOSE_RELOAD\", False):\n\t\t\t\tsettings = os.environ[\"NOSE_RELOAD\"] + \" && \" + \" \".join(map(lambda name: name.lower(), os.environ.items(\"NOSE_RELOAD\")))\n\t\t\t\tcode_string = \"export DJANGO_SETTINGS_MODULE=\"+settings\n\t\tsettings = code_string = _rewrite_inline_settings(\n\t\t\tcode, code, settings, code_string,"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\tpass\n\n\tdef xenv(self):\n\t\t\"\"\"\n\t\tRun xenv in chroot using chroot_setup()\n\t\t\"\"\"\n\t\tpass\n\nclass TestLinux(Platform):\n\t\"\"\"A platform that is built on linux systems.\n\t\n\tThe goal is that this class has methods for creating, configuring, \n\tand managing the kernel (e.g. x86-64-slab, x86-64-kmem, powerpc64le, \n\tppc64le-slab, and darwin-i686-smp)\n\n\tIt also creates a fake chroot.\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\ndef xcmd(cmd, cmd_prefix = ''):\n\tfrom os import path\n\tcmd = '%s %s' % (cmd_prefix, cmd)\n\treturn str(xargs(sys.executable, cmd))\n\ndef ycmd(cmd, cmd_prefix = ''):\n\tfrom os import path\n\tcmd = '%s %s' % (cmd_prefix, cmd)\n\treturn str(xargs(sys.executable, cmd))\n\ndef get_terminal():\n\n\t\"\"\"\n\tFinds terminal details through xterm.\n\t\"\"\"\n\n\timport platform\n\n\tif"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn [\n\t\t\t\"run -d {source_dir}/build/{testname}/xargs.sh\".format(source_dir=os.path.realpath(source_dir),\n\t\t\t\t\t target_dir=os.path.realpath(target_dir)\n\t\t\t\t\t),\n\n\t\t\t\"xargs_pipetool_params=--xargs_pipetool_params=' '.join(\n\t\t\t\t\t(' -x'+ x for x in SHELL_PROCS for x in x.split(' '))])\n\n\t\t]\n\n\ndef xargs_pipetool_params(options):\n\t# Make /"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\ndef yargs(\t\"\"\"\n\tA simplified implementation of yargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\ndef test_options():\n\t\"\"\"\n\tMakes sure the options for the executable can access things by\n\t\tgiving them in a convenient format.\n\t\"\"\"\n\toptions = []\n\toptions.append((\"-t\", \"\", \"--targetfile\", False))\n\n\toptions.append((\"-v\", \"\", \"--x-args-command\"))\n\t\n\treturn options\n\t\n\ndef extract_"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\ndef xargs_filter_list(args):\n\t# filter to use when running xargs\n\tfilter= []\n\n\t# If a target doesn't support xargs then we want outputting to stdout\n\tif args.target_concurrency is not None:\n\t\tfor target_con in args.target_concurrency:\n\t\t\tif args.target_concurrency == 'default':\n\t\t\t\tcontinue\n\t\t\tcmd=args.target_command+\" -c \\\n\t\t\tgrep -v '^\\(-\\d+\\.|-.\\w+\\):/\\\\-.*$/\"' \"+args.target_file+"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn \"--color=%s,%s,0\" % (PROMPT, target_concurrency)\n\nxargs.items = [\"dev\"]\nxargs.key_value_pairs[\"dev\"] = \"\"\"\\\n#dev=sda\nhost1:1\nhost2:1\n\"\"\"\n\n\nmain()\"\"\"\n@package mi.core.instrument.chunker\n@file marine-integrations/mi/core/instrument/chunker.py\n@author Emily Hahn\n@brief Common functionality for stream instruments\nRelease notes:\n\nInitial Release\n\"\"\"\n\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\n\n\tdef __init__(self, color=False, target_concurrency=0):\n\n                # Set color variable and initialize\n                self.color = color\n                self.target_concurrency = target_concurrency\n\n        def to_str(x):  # pragma: no cover\n            return \"{path}\", target_concurrency={target_concurrency}\".format(path=quote(x), target_concurrency=self.target_concurrency)\n\n\n\n    class Terminal(object):\n        \"\"\"\n        A terminating child in the running container, i.e. runs the command line utility a command with sudo.\n\n        :param target:  String representing which"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\tif not hasattr(os, 'getppid'):\n\t\t\timport socket\t# Python 3\n\t\tif sys.hexversion < 0x3000000:\n\t\t\tdef targets(self):\n\t\t\t\treturn [self._execvpe(arg) for arg in list(os.environ.items()) if arg.startswith(\"C:\")]\n\t\t\tself._execvpe = self._spawnvpe = self._select_target = self._spawn\n\t\telse:\n\t\t\tself._execvpe = execvpe_spawn\n\t\tif os.uname()[0] == \"SunOS\":\n\t\t\tself"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn ['--color=' + colors['linux'][platform]] + _parse_args()\n\ndef xargs_from_env():\n\t\"\"\"xargs_from_env: A dict of keys, and values to be passed on to\n\t\tsubprocess.Popen() after connecting to the pty.\n\t\n\tReturns a dictionary of args and options from the environment's\n\tenvironment.\"\"\"\n\treturn {\n\t\t'XAUTHORITY':       'xauthority',\n\t\t'XMIT':            'x_close',\n\t\t'SHELL':          'SHELL' if _os_is_win32 else 'bash', "}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\nparser = argparse.ArgumentParser(\n\tdescription = \"A comprised version of xargs with a custom\" + \\\n\t\t\t\" template\",\n\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\tprog='xargs',\n\tepilog=\"A simplified xargs program which is meant to be run\" + \\\n\t\t\t\" with the template file as a basis of a shell\" + \\\n\t\t\t\"script. It supports bash syntax, bash scripts, java\" + \\\n\t\t\t\"\\n\\n\"\n\t\t\"This script is designed as a template for simple shell script\" + \\\n\t\t\t\"\\"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif type(seq) is list:\n\t\t\treturn [_shuffled(s) for s in seq]\n\t\telse:\n\t\t\treturn seq\n\n\ndef _random_token_generator(length: int, chars: List[str]) -> str:\n\t\"\"\"\n\tGenerates a random token in the form of ``token_<token>``.\n\n\tExample:\n\t\t>>> f = open_tone_dream.read_words('.')\n\t\t>>> f.next_word()\n\t\t'next'\n\n\tTODO Implementing this method requires python3\n\t\"\"\"\n\treturn ''.join("}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif len(seq) <= 1:\n\t\t\treturn seq\n\t\telif len(seq) == 2 and seq[0] <= seq[1]:\n\t\t\treturn seq + [seq[0]]\n\t\telse:\n\t\t\treturn [seq[i] for i in range((i for _ in range(len(seq)) if _ == 1))]\n\n\ndef isin(seq: Sequence[str]) -> bool:\n\t\"\"\"\n\tReturns True if given array is contained in it.\n\t\n\tArgs:\n\t\tseq: Array to test if it should be searched for in.\n\t\n\tReturns:\n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tret: list[str] = list()\n\tn = len(seq)\n\tcnt: int\n\twhile cnt > 0:\t# Use [0] count\n\t\tlast = len(seq) - cnt\n\t\tif last < 0:  # last == 0: skip first element(a)\n\t\t\tcontinue\n\t\tfor _ in range(n):  # Try until n * n_len_b < n_iter\n\t\t\tif seq[last] < seq[last_n_len + 1] and random.random() < seq[last_n_len + 1]:\n\t\t\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tassert len(seq) > 0\n\t\tassert len(seq) == 0\n\t\treturn map(next, seq)\n\n\tdef _shuffle_impl(sequence: List[str], num_shuffles: Optional[int] = None, shuffle_order: Optional[int] = -1) -> deque:\n\t\t\"\"\"shuffle iterator\"\"\"\n\t\treturn deque((seed[s] for s in sequence), maxlen=(num_shuffles or 0))\n\n\tdef __reduce__(self) -> float:\n\t\t\"\"\"reduce\"\"\"\n\t\tif isinstance(self.next(), float):\n\t\t\treturn"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tseq = reversed(seq)\n\t\trandom.shuffle(seq)\n\t\tresult = [seq[i] for i in range(len(seq)) for seq in seq]\n\t\tresult = ''.join(result)\n\t\tresult = result[::-1]\n\t\treturn result\n\ndef _decode_seq(seq_string: str) -> list[str]:\n\t\"\"\"\n\tDecode string to a sequence of integers\n\t\"\"\"\n\n\tseq_string = seq_string.replace('.', '')\n\tseq_string = ''.join(itertools.islice(itertools.cycle(string.punctuation),"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn map(str.strip, seq)\n\tshuffle = shuf_as_shuffled\n\tshuf_as_shuffled = shuf # compat alias\n\ndef shuffle_iterlines(seq: Sequence[str], limit: typing.Sequence[int]=None) -> Iterator[tuple]:\n\t\"\"\"\n\tYield successive n-tuples in sequence\n\n\t>>> list(shuffle_iterlines(\"ababababbccccdd\"))\n\t[(0, 0), (1, 1), (2, 2), (0, 0), (1, 0)]\n\t\"\"\"\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tseq_copy = copy.copy(seq)\n\t\tseq_copy[0] = random.choice(seq[0].lower())\n\t\tfor item in seq_copy:\n\t\t\tif len([c for c in item + seq_copy if c in item]) <= len(seq_copy) / 2:\t# don't shuffle if less than two unique words to be merged\n\t\t\t\tif item not in seq:\n\t\t\t\t\tbreak\n\t\t\telif len(seq[0]) <= len(seq_copy)/2:\t# shuffle if less than two unique words to not be merged\n\t\t\t\t_"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\trearranged = random.sample(seq, 1)\n\t\tfor i in xrange(0, len(rearranged), 10) + range(0, len(rearranged), 10):\n\t\t\tstart = i + rearranged[i:i + 10]\n\t\t\tend = start + rearranged[i:i + 21]\n\t\t\tyield start, end\n\treturn _shuffled(seq)\n\ndef _shuffle_data(n: int) -> List[List[str]]:\n\t\"\"\"\n\tShuffle the input data in a random order"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn jigsaw(shuffle(seq))\n\ndef _shuffle_list_of_pairs(seqs: Sequence[str], key: Callable[[str, str], Tuple[str, str]]) -> List[Tuple[str, str]]:\n\t\"\"\"\n\tRandomly shuffle\t\"\"\"\n\treturn shuffle(list(izip(*seqs.tolist()) for i, seq in enumerate(seqs)))\n\n@contextmanager       \ndef temporarily_iterable_iterable(x: Iterable['str']) -> Iterator[Iterable[str]]:\n\t\"\"\"\n\tContext manager for a generator that returns items of `"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tfrom string import ascii_lowercase as lowercase\n\n\tdef helper(seq: Sequence[str]) -> list[str]:\n\t\tif not is_all_in_iterable(seq):\n\t\t\treturn []\n\t\tseen = set()\n\t\tseen_add = seen.add\n\t\treturn [x for x in seq if x not in seen and not seen_add(x)]\n\n\tseqs = (x.strip(newline) for x in seq)\n\t\n\treturn helper(seqs)\n\n\ndef strip(s: str) -> str:\n\treturn \"\".join([c for c in chain(\n\t"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\ti = s.find(\" \")\n\t\tif i!= -1:\n\t\t\tif int(s[i-1])!= int(version):\n\t\t\t\treturn (0, i-1)\n\t\t\telse:\n\t\t\t\treturn (int(s[:i]), 0)\n\t\treturn (int(s), 0)\n\t\t\n\n\telse:\n\t\tif str(version) == \"r\" \\\n\t\t\telse\t\\\n\t\t\t\"unstable\" \t\t:\t# emacs\n\t\t\treturn (0, 0, 0)\n\n\n\ndef split_version(s: str) -> tuple[int"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = VERSION_REGEX.match(s)\n\t\tif not match: return (True, 0)\n\n\t\tgroups = match.groupdict() # type: Dict[str, int]\n\n\t\tgroups['major'] = int(groups['major'])\n\t\tgroups['minor'] = int(groups['minor'])\n\t\tgroups['patch'] = int(groups['patch'])\n\t\tgroups['prerelease'] = int(groups['prerelease'])\n\t\tgroups['version'] = int(groups['major']) * 10000 + groups['minor'] + \\\n\t\t\t\t\t\t\t\t\tint(groups['patch']) * 100 + int"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t return tuple(map(int, s.split('.')[:2]))\n\ndef is_pre_release(dev_version: str, release_type: str) -> bool:\n\tpre_release = get_pre_dev_version(dev_version)\n\tif (pre_release is None) or (pre_release == UNKNOWN) or (pre_release > UNKNOWN):\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef get_post_dev_version(dev_version) -> Tuple[int,...]:\n\tif version_str(dev_version).startswith"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tm = re.match(r\"^\\d\\.\\d/\\d\\.\\d\\.\\d[abc]+$\", s)\n\t\tif m:\n\t\t\treturn int(m.groups()[0]), int(m.groups()[1])\n\t\tm = re.match(r\"^\\d\\.\\d\\.?(\\d)\\.\\d\\.\\d\\.\\d[abc]+$\", s)\n\t\tif m:\n\t\t\treturn int(m.groups()[0]), int(m.groups()[2])\n\t\tm = re.match(r\"(^\\d\\.\\d\\.\\d"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tresult = tuple(int(q) for _, q in regex.findall(s).groups(1))\n\t\treturn result[0], result[1]\n\n\treturn parse_version(s)\n\n\ndef _parse_requirements_line(name: str) -> list[str]:\t\t\t\"\"\"\n\tparse one requirements line of pip installation requirements.txt\n\t\"\"\"\n\tline: str = f'pip=={get_requirements_line(name)}'\n\tif line:\n\t\treturn line.split(':')[1].strip().split(' ', 1)\n\treturn []\n\n\ndef"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tif \"v\" in s.lower():\n\t\t\ts = s[1:]\n\t\t\tver = [int(s[0]), int(s[1])]\n\t\telif \"r\" in s.lower():\n\t\t\t# Porem ipsum dolor sit amet, consectetur adipiscing elit.\n\t\t\ts = s[2:]\n\t\t\tver = [int(s[0]), int(s[1])]\n\t\telse:\n\t\t\tver = [int(re.sub(r'[^0-9.]', '', s))]\n\t\treturn ver\n\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\ti = 0\n\t\ttry:\n\t\t\ti += len(s)\n\t\texcept TypeError:\n\t\t\ti += 1\n\t\tif (i - s.find(\".0.0\") > i):\n\t\t\ts = s.replace('.0.0', '')\n\t\twhile i < 14:\n\t\t\tif s.find(\".0.0\") > i:\n\t\t\t\ts = s.replace('.0.0', '')\n\t\t\t\tbreak\n\t\t\ti += 1\n\t\tif (i - s.find(\".0.0\") > i):\n\t\t\ts = s.replace('.0.0', '')"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = VERSION_PAT.match(s)\n\t\tif match:\n\t\t\ts = match.group('v')\n\t\t\tversion = tuple(map(int, s.split('.'))) if s else tuple()\n\telse:\n\t\traise ValueError('{} cannot be parsed as a valid version string'.format(s))\n\n\treturn (match.group(1), int(version[0]), version[1])\n\ndef parse_filename():\n\t\"\"\"\n\t:rtype: str\n\t:return: filename with path (if the path is local directory, in case of a local directory, or"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = VERSION_VERSION_RE.match(s)\n\n\t\tif match is None:\n\t\t\traise ValueError('cannot parse {}'.format(s))\n\n\t\tmajor, minor, build = match.groups()\n\t\tif build:\n\t\t\tbuild = int(build)\n\t\telse:\n\t\t\tbuild = 0\n\t\tversion = tuple(map(int, [major, minor, build]))\n\t\treturn version\n\t# XXX: return empty string if version could not be determined\n\treturn\n\n\ndef normalize_version(version: str) -> str:\n\t\"\"\"\n\tNormalize a version string\n\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmo = re.match(r'^version\\s*([\\d.]+)', s.replace('\\n', ''), re.M)\n\t\tif mo:\n\t\t\treturn (int(mo.group(1)), int(mo.group(2)))\n\t\traise ValueError('invalid version: %s' % s)\n\n\tversion = get_git_version()\n\tif version!= '0.0.1':\n\t\traise ValueError('invalid version: %s' % s)\n\n\t# parse the version:\n\tv = parse_version(version)\n\n\tassert v[0] <"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn reduce(lambda a, b: b if len(a) >= len(cmd) else a + [cmd], cmd, [])\n\n\nclass _Sherlock:\n\t_cache = []  # type: _Cache\n\t_lock = threading.RLock()\n\n\t@staticmethod\n\tdef cache_add(*items):\n\t\tret = _Sherlock._cache.append(_Sherlock.cache_add, items)\n\t\treturn ret\n\n\t@staticmethod\n\tdef cache_get(*items):\n\t\tret = _Sherlock._cache.pop(_Sherlock."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn cmd[1:] if cmd.startswith('s')] else cmd\n\ndef process_command(cmd: tuple[str,...]) -> Tuple[str,...]:\n\t''' Takes a command (cmd), performs the necessary action based on the args provided '''\n\tfor cmd in cmd:\n\t\tcmd = cmd.lower()\n\n\t\tif cmd in ['help','version', 'help']:\n\n\t\t\tif len(cmd) > 1 and (cmd[0].isupper() or cmd[0] == '--'):\n\t\t\t\tcmd = cmd[1:]\n\n\t\t\tif cmd in ['start', '"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\n\tfor p in cmd:\n\t\tif '..' not in p or'' not in p:\n\t\t\t break\n\t\t\n\t\tif'' in p:\n\t\t\tp = '\"' + p + '\"'\n\t\t\n\t\treturn p\n\n\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Author',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\t# Strip shell variables in the command.\n\t\tcmd = re.sub(r\"([\\w#])\", r\"\\1\\2\", cmd)\n\n\t\t# Convert Windows paths to Windows paths.\n\t\tcmd = _windows_conversion(cmd)\n\n\t# Apply custom shell escaping commands.\n\treturn _convert_special_string_win32_command(cmd)\n\ndef cmd_unix(command: str) -> str:\n\t\"\"\"\n\tReplace windows/dos commands with Unix-style command characters\n\t\"\"\"\n\t_windows_conversion(command)\n\n\t_replacements.append"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn [s.replace('\\\\', '/').replace(':', '\\.').replace('*','').replace('.', '_').replace('/', '_').replace('(', '_') for s in os.path.split(cmd, posix_path)]\n\n\ndef add_to_cmd(*args):\n\tcmd = normalize_cmd(*args)\n\tif os.name!= 'nt':\n\t\tadd_to_path(os.path.dirname(cmd[0]), cmd[1])\n\tif os.name!= 'nt':\n\t\tcmd[0] = os.path.join(os.path.basename(cmd"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\t\n\t\n\tif sys.platform == \"win32\":\n\t\t# we have to replace double backslashes with single quotes - we have to replace them\n\t\tcmd = cmd.replace(\"\\\\\\\\\", os.path.abspath(cmd))\n\treturn cmd\n\ndef _load_scripts(main_dir: str) -> None:\n    \"\"\"Load scripts and return a dictionary with the following info\"\"\"\n\n    scripts_path = os.path.join(main_dir, \"scripts\")\n    if os.path.exists(scripts_path):\n        scripts = load_scripts_from_file(scripts_path)\n   "}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\n\t# convert all windows to lowercase\n\tcmd = map(lambda e: e.lower(), cmd)\n\n\t# handle the case where an empty space is passed\n\tif not cmd.isspace():\n\t\tpass\n\n\t# find out if we have a windows, then prepend a space\n\t# this removes the first occurence of the first word not already in the result\n\tif cmd[0].isspace():\n\t\tcmd ='' + cmd[1:]\n\n\n\treturn cmd\n\n\n@cmd(['i'], ['target', 'url'], ['targetdir'])\ndef check_target_exists"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\t# The best way is to convert a shebangs file to the c++ extension for this\n\t\t# use case: we want `make` to have the full path\n\t\t# The best way can to turn the shebangs file path into the cpp extensions\t\n\n\t\tcfile = cmd[0]\n\t\tif len(cfile) > 0:\n\t\t\t# use full path of file in c++ if not an shebangsfile\n\t\t\t# otherwise use os.path.splitext to figure out which one is an shebangs file\n\t\t\tcfile = os.path.splitext(cfile"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\ndef sanitize(text: str) -> str:\n\t# TODO replace by regex expressions or something like 'A...B..' (bug 79454)\n\t# https://bugs.python.org/issue38804\n\ts = re.sub(\"(^|[^a-zA-Z0-9_])\", r' \\w+', text)\n\tif len(s) > 100:\n\t\ts = '_' + s[0:97] + '-'    # to accommodate all non-ascii chars\n\treturn s.lower().strip()\n\ndef get_text_"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n    \tif len(cmd) > 0:\n\t\tcmd = tuple(cmd)\n\t\treturn cmd\n\n\tcmd = fix_cmd(_remove_comments(cmd))\n\tcmd = _remove_comments(cmd)\n\tcmd = _remove_comments(cmd)\n\n\treturn cmd\n\n# This file is part of Copernicus\n# http://www.copernicus-computing.org/\n# \n# Copyright (C) 2011, Sander Pronk, Iman Pouya, Erik Lindahl, and others.\n#\n#"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tclass Memoize:\n\t\t\tdef __init__(self):\n\t\t\t\tself.value = None\n\t\t\t\tself.time = time.time\n\t\t\t\tself.accessed = []\n\t\t\t\tself.cache = cache(maxsize=2**20)\n\n\t\t\tdef __call__(self, *args): pass\n\n\t\t\tdef __get__(self, obj, type=None):\n\t\t\t\tif obj is None:\n\t\t\t\t\t# print 'Calling cache with nothing'\n\t\t\t\t\treturn self\n\t\t\t\tvalue = obj.value\n\t\t\t\tdef expire():\n\t\t\t\t\tself.time = time.time() - self.accessed[0]\n\t\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef cached_func(obj, *args, **kwargs):\n\t\t\tif isinstance(obj, types.MethodType):\n\t\t\t\tget = functools.partial(cached, obj)\n\t\t\telse:\n\t\t\t\tget = cache.get(key)\n\t\t\t\tif not get:\n\t\t\t\t\tval = func(*args)\n\t\t\t\t\tif val is _NOTHING:\n\t\t\t\t\t\tcache.set(key, val, get)\n\t\t\t\treturn get()\n\t\t\treturn func(obj, *args, **kwargs)\n\n\t\treturn cached_func\n\treturn cached\n\n\n# TODO"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n\n\t# The wrapped key is the number of times this cache\n\t# was called. The cache key is created in that tuple.\n\t_cache_key = _make_cache_key(callable, key)\n\n\t# Return the function object\n\treturn partial(cache, _cache_key)from __future__ import (absolute_import, division, print_function)\nimport unittest\nimport testutil\n\nfrom mantid.kernel import (CompositeSource, Detector, FloatX, FloatSaccade)\nfrom mantid.api import (FunctionFactory, PropertyManager, IMD"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n    def decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef func(*args, **kw):\n\t\t\tkw[key] = cache.get(key, None) or f(*args, **kw)\n\t\t\treturn f(*args, **kw)\n\t\tcache[key] = property(func)\n\t\treturn func\n\t\n\treturn decorator\n\n\nclass Job(object):\n\t\n\t__slots__ = ('name','status', 'progress', 'priority', 'timestamp','result', 'error', 'runtime_storage')\n\tdef __init__(self):\n\t\t# the"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@wraps(cache)\n\t\tdef wrapper(*args, **kw):\n\t\t\tif key not in cache:\n\t\t\t\tvalue = cache[key] = cache.get(key, get_undecorated, _nofloat)\n\t\t\t\tif value is _nofloat:\n\t\t\t\t\tcache[key] = default\n\t\t\telse:\n\t\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn cached\n\nclass Timer(object):\n\tdef __init__(self):\n\t\tself._data = OrderedDict()\n\t\tself._stop = False\n\n\tdef start(self):\n\t\tif self._stop:"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n#  def log(self, line):\n# \t\t\"\"\"log() returns the result of the function\"\"\"\n# \t\n# \t\twith lock:\n# \t\t\treturn None\n\t\n# \t\tretval = self.cache.get(key)\n# \t\tself.cache.set(key, retval, timeout=self.timeout)\n# \t\n# # \t\tself.log(\"Read cache: %s (from %s)\" % (cache.cachekey, retval))\n# \t\treturn retval\n\n# # \t\treturn self.cache.get(key,"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t \n\tclass Decorated(object):\n\t\t\"\"\" Decorator that memoizes results of the wrapped function.\n\t\t\n\t\t\tNote that when memoized results return data that is cached,\n\t\t\tthe wrapped function is never memoized again.\n\t\t\t\"\"\"\n\t@wraps(decorated)\n\tdef wrapper(func, *args, **kwargs):\n\t\tif not hasattr(cache, key):\n\t\t\tcache[key] = func(*args, **kwargs)\n\t\treturn cache[key]\n\treturn wrapper\n\n\t@cached(key=hashkey, lock=lock)\n\tdef key(func"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\ndef cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that returns\nresults in a cache.\t\"\"\"\n\ndef wrap(function):\n\t\"\"\"\n\tWrapper in a function with a cache wrapping it.  This allows\n\tthe code to pass the caching function around a function\n\t\"\"\"\n\tdef inner(*args):\n\t\tcache = False\n\t\tif cache:\n\t\t\tresult = cache.get(key)\n\t\t\tif result:\n\t\t\t\tcache = result\n\t\t\t\treturn result\n\t\t\tresult = function(*"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\ndef clear_cache():\n\t\"\"\"\n\tWraps a function so that it doesn't clear the cache of all values.\n\t\"\"\"\n\tcache = {}\n\tdef _clear(self, *args, **kwargs):\n\t\tif kwargs.get('clear', True):\n\t\t\tkey = str(args) + str(kwargs)\n\t\t\tif self.debug is True:\n\t\t\t\tprint('%s is clear' % key)\n\t\t\treturn cache.pop(key, None)\n\n\t\ttry:\n\t\t\treturn cache[args]\n\t\texcept KeyError:\n\t\t\tresult = self._cache."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef memoizer(*args, **kwargs):\n\t\t\tif key not in cache:\n\t\t\t\tkey = repr(key)\n\t\t\t\tif cache._expire:\n\t\t\t\t\tctime = cache._expiretimes[key]\n\t\t\t\t\tcache._expire[key] = ctime + PERSIST_TTL\n\t\t\t\treturn cache._expire[key]\n\t\t\tresult = cache[__value] = func(*args, **kwargs)\n\t\t\taccess_info = result._info_cache\n\t\t\tcache._info_cache = cache._info\n\t\t\treturn result\n\t\tcache = cache_wrapper"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\tdef memoized_func(mtime, key, *args, **kwargs):\n\t\t\ttimer_elapsed = time.monotonic() - mtime\n\t\t\thkey = key(lru_cache_key(mtime, key))\n\t\t\t#print \"lru_cache_key: {}, hkey: {}\".format(heapq.heappop(hkey), repr(hkey))\n\t\t\t#print \"lru_cache_key: {}, timeout: {}\".format(mb.heappop(hkey), timer_elapsed)\n\t\t\t#print \"lru"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\t\n\tdef wrap(func):\n\t\tcache_data = TTL_CACHE_KEY % (func.__name__, func_name)\n\t\t\n\t\t# if __debug__ is True, the cache is only time-to-live\n\t\tif __debug__:\n\t\t\tlog('wrap function \"%s\" with (%f seconds)' % (func_name, cache_data), verbosity)\n\t\t\n\t\treturn cache_data is not None\n\t# return decorator if there is no caching\n\tif not hasattr(func, '_cache_data'):\n\t\twrap.func_data = func_data_required\n\n\t# wrap"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\nclass Lru(object):\n    def __init__(self, data, timeout=None, maxtime=2000000000, fast_expire=True):\n        self.data = data\n        self.index = min(len(data), maxtime)\n        self.slow = int(time.time() > slow_expire)\n        self.key = self.bisect(data, self.index + self.slow)\n        self.freed = freed = struct.unpack(\"QQ\", data[self.index:self.index + self.slow])[0] & 0x7fffffff\n        if not fast"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\t\t\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator for caching decorated functions, to help avoid\n\t\tpickling when calling functions that use cached results.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef decorated_func_wrapper(self, *args):\n\t\t\t# The wrapped Function is wrapped by this decorator.\n\t\t\t## Retrieve arguments to cache for future access.\n\t\t\targs = list(args)\n\t\t\tkey = str(args) + repr([counter]) + self.key\n\t\t\n\t\t\t## Perform argument caching according to the given settings.\n\t\t\tkey_to_cache = args"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tcache = {}\n\t\tdef memoizer(fxn):\n\t\t\t## If the function being decorated wraps a function, cache its result\n\t\t\tif typed:\n\t\t\t\tname = fxn.__name__\n\t\t\t\tif name not in cache:\n\t\t\t\t\tresult = fxn(name)\n\t\t\t\t\tcache[name] = result\n\t\t\t\telse:\n\t\t\t\t\tresult = cache[name]\n\t\t\t\t#print name, type(result)\n\t\t\t\treturn result\n\t\t\telse:\n\t\t\t\treturn fxn(fxn.__name__)\n\t\treturn memoizer\n\t\n\t@ttl_cache(maxsize=500, ttl=100,"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t# Store in cache.\n\t\t\tcache = cache_enabled()\n\n\t\t\t# Check for maximum (and then cache expiration).\n\t\t\tif len(cache) >= maxsize:\n\t\t\t\t# LRU expired for longer than maxsize, do not store anything.\n\t\t\t\treturn func\n\n\t\t\t# Check for the LRU interval of the cache.\n\t\t\tcheck = [True]\n\t\t\tfor ii in range((1 << maxsize*2) - 1):\n\t\t\t\tcheck.append(cache.pop(ii))\n\t\t\t# Do not exceed number of items.\n\t\t\tif check and cache:"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\t\n\tdef decorator(f):\n\t\t\"\"\"Decorate a function to cache results from LRU\"\"\"\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"Cache result and cache the time-to-live\"\"\"\n\t\t\t\n\t\t\tresult = cache.get(key(args, kwargs))\n\t\t\tif result is None:\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tcache.set(key(args, kwargs), result, time.monotonic() + ttl)\t\t\t\t\n\t\t\treturn result\n\t\t\n\t\treturn wrapper\n\t\t\n\treturn decorator\n\ndef log_cache(ttl"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef decorator(function):\n\t\t\tdef wrapper(*args, **kw):\n\t\t\t\tvalue = (lru_cache(maxsize) * [function(), args, kw] +\n                        tuple(positional + [timer() for i in xrange(maxsize)]))\n\t\t\t\t_debug(\"Caching item: %s, value: %s\" %\n\t\t\t\t\t  (args, kw))\n\t\t\t\treturn value\n\n\t\t\treturn cast(*wrapper(*args, **kw))\n\t\treturn decorator\n\n\n\treturn ttl_cache\n\n\n\ndef make_cached_client_cachekey(*args, **kw):\n\t\"\"\"\n\tCreates a key from"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tmax_cache_args = dict(args=args, kwds=kwargs)\n\t\t\tif maxsize > 0:\n\t\t\t\tcache_key = (func.__name__,) + tuple(cPickle.dumps(max_cache_args.items()))\n\t\t\t\tif not typed:\n\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\t\tvalue = cache.get(cache_key)\n\t\t\t\tif value is None or len(value) < maxsize:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache.set("}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\t\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef memoized_f_with_ttl_value(self, *args, **kwargs):\n\t\t\tkey = hash(frozenset(args))\n\t\t\tvalue = args[0]\n\t\t\tif not cache.redis_client.exists(key):\n\t\t\t\t# This key doesn't exist - create one.  First, create the object.\n\t\t\t\tcache.redis_client.set(key, value, ttl)\n\t\t\t# This key is the function/method the value was stored in, and the TTL value\n\t\t\t#"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\t@wraps(fun)\n\tdef mru(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tReturn the result of the given function call with memoization wrapped in\n\t\tthe cache.\n\t\t\"\"\"\n\t\tkey = str(uuid.uuid4())\n\t\tval = self.get(key)\n\t\tif val is not None:\n\t\t\treturn val\n\t\tval = fun(self, *args, **kwargs)\n\t\tself.set(key, val, timeout=maxsize)\n\t\treturn val\n\t\n\treturn mru\nimport unittest\n\nfrom django.contrib."}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\ndef undecorate(m):\n\t\"\"\"\n\tDecorator to undecorate a function after it is called.\n\tWill raise a NotImplementedError if unable to undecorate\n\tanything.\n\t\"\"\"\n\tpass\nimport pytest\nfrom dodo_commands.shell import Command\nfrom dodo_commands.dodo_commands import (\n    get_new_command, get_help, add_help, set_help, run, run_cmd, print, show,\n    add_help_options,\n    get_help_context,\n)\nimport dodo.commands\n\n\n@"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef decorated_func(*args, **kwargs):\n\t\t\tkey = str(args) + str(kwargs)\n\t\t\tif limit and not key in limited_cache:\n\t\t\t\t# NOTE: Don't run this once we get the same results again\n\t\t\t\treturn None\n\t\t\tif typed!= cache:\n\t\t\t\treturn value(key)\n\t\t\tresult = cache.get(key)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(key, result, timeout=None)\n\t\t\t\treturn result\n\t\t\tif typed:"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorated(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# If the wrapped method is called directly, then it will\n\t\t\t# use the cache so far to save the function object.\n\t\t\tif hasattr(caching, '__call__'):\n\t\t\t\treturn caching.__getattribute__(f.__name__)(f)(*args, **kwargs)\n\n\t\t\twrapped_f = cache.wrap(f, wrapped, maxsize)\n\t\t\tcache_results += 1\n\n\t\t\treturn f(*args, **kwargs)\n\n\t\twrapper._cache_results = f.__name__ + '"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t# cache decorator\n\t\tcache_key = (\"%s:%s:memoized:%s\" % (func.__module__, func.__name__, func.__name__))\n\t\tdef memoize(key=cache_key):\n\t\t\ttry:\n\t\t\t\tresult = cache.get(key)\n\t\t\texcept AttributeError:\n\t\t\t\tresult = func(key)\n\t\t\tif result is None:\n\t\t\t\tresult = func(key)\n\t\t\tcache.set(key, result, timeout=maxsize)\n\t\t\treturn result\n\t\treturn memoize\n\tcache = mru_cache()"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\n\t@wraps(fun)\n\tdef _mru_wrapper(*args, **kwargs):\n\n\t\t\"\"\"\n\t\tWrap a function, caching its results. Only calls `_get_cache` to\n\t\tuse this decorator.\n\t\t\n\t\tReturns decorated version or decorator if nothing happened.\n\t\t\n\t\t\"\"\"\n\t\t\n\t\tkey = args[0]\n\t\targ = args[1]\n\n\t\tt = cache.get(key)\n\t\tif typed:\n\t\t\tif __debug__:\n\t\t\t\t_mutual_logging(key)\n\t\telse:\n\t\t\t_mutual_logging(key)\n\t\td = fun"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef mru_cache_decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t@cached(maxsize=maxsize, typed=typed)\n\t\t\tdef func_wrapper(**kwargs):\n\t\t\t\tkey=args[0]\n\t\t\t\tval=args[1]\n\t\t\t\timport time\n\t\t\t\t@wraps(f)\n\t\t\t\tdef memoized_func():\n\t\t\t\t\treturn f(key,val)\n\t\t\t\t\ttime.sleep(1)\n\t\t\t\t\treturn memoized_func\n\t\t\t\treturn func_wrapper\n\t\t\n\t\treturn wrapper\n\n\treturn mru_cache_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n    def cache_decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kw):\n            key = hash(args) + str(id(args))\n            cache, cache_key = cls._cache_key(key, **kw)\n            obj = cache.get(cache_key)\n            if obj is None:\n                # We need to cache this. Note that this has the\n                # overhead of not caching a \"default\" function in the decorator\n                return f(*args, **kw)\n            else:\n                with cls._lock:\n                    return f(*args, **kw)\n        return"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef mru_cache_wrapper(f):\n\t\tdef _mru(*args, **kwds):\n\t\t\tkey = (args, kwds) + ((\"lru\", maxsize, typed),)\n\t\t\treturn f(*key)  # this returns a function bound to a key\n\t\t \n\t\t_mru.__name__ = f.__name__\n\t\treturn _mru\n\t\n\tmru_cache_wrapper.__name__ = \"MRU\"\n\n\t# mru caching (in seconds) is not threadsafe.\n\t@staticmethod\n\tdef mru_cache(maxsize=128, typed=False"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tn = random.randint(1, maxsize)\n\t\ttry: n = min(n, maxsize)\n\t\texcept ValueError: return lambda func, maxsize=maxint, typed=typed : func\n\t\tdef memoizing_function(*args):\n\t\t\tcache = {}\n\t\t\tresults = [ cache.get(key) for key in args ]\n\t\t\ttry: results.sort()\n\t\t\texcept TypeError: pass\n\t\t\tresults.reverse()\n\t\t\tself.cache = results\n\t\t\treturn results\n\t\treturn memoizing_function\n\ndef reset_cache(): global _cache\ndef cache(min_compress_"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tdef wrapper(user_function):\n\t\t\tcache = LRUCache(maxsize, typed)\n\t\t\tuser_function = functools.lru_cache(maxsize+1)(user_function)\n\t\t\treturn cache.user_function(user_function)\n\t\treturn wrapper\n\n\t@lru_cache()\n\tdef cached(*args, **kw): return wrapped\n\n\tdef __getattr__(self, name):\n\t\tif callable(name):\n\t\t\treturn getattr(self.wrapped, name)(*args, **kw)\n\t\treturn getattr(self, name)\n\treturn cached\n\nclass LRUCache"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tdef wrapper(f):\n\t\t\t@wraps(f)\n\t\t\tdef memoizer(*args, **kwargs):\n\t\t\t\tkey = args\n\t\t\t\tif typed:\n\t\t\t\t\tkey += (maxsize, key, f.__name__)\n\t\t\t\ttry:\n\t\t\t\t\tres = f(*args, **kwargs)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tres = f(*args)\n\t\t\t\t\tkey[1] += 1\n\t\t\t\t\tf.__name__ = f.__name__ + str(args)\n\t\t\t\t\tkey = (key[0], key, f.__name__)\n\t\t\t\t\tres = res.key\n\t\t\t\texcept TypeError:\n\t\t\t\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\t@functools.wraps(func)\n\tdef cache_key(argname):\n\t\t\"\"\"\n\t\tWrapped function to cache results.\n\n\t\t:param argname:\n\t\t\tA least reciever used function name.\n\t\t:return:\n\t\t\tThe wrapped function.\n\t\t\"\"\"\n\t\tif argname in memoize_cache:\n\t\t\treturn memoize_cache[argname]\n\t\telse:\n\t\t\tmemoize_cache[argname] = result = func(argname)\n\t\t\tmemoize_cache.save(argname, result)\n\t\t\treturn result\n\n\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\tmemoize = _memoize\n\twrap = memoize\n\n\tdef memoize(func):\n\t\tdef wrapper(*args, **kwds):\n\t\t\t\"\"\"\n\t\t\tCache return values for a given length only,\n\t\t\tusing the standard LRU.\n\t\t\t\t\"\"\"\n\t\t\tmaxsize = kwds.pop('maxsize', self.maxsize)\n\t\t\tstart_key = args[0].key()\n\n\t\t\tif start_key not in lru:\n\t\t\t\tlru[start_key] = LRU(maxsize=maxsize)\n\n\t\t\ttry:\n\t\t\t\tif typed:\n\t\t\t\t\tvalue = func(*args,"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\treturn memoized(func)\n\t\n\tdef decorator_with_cache(*args, **kwargs):\n\t\t\"\"\"\n\t\tdecorator_with_cache decorates the function with memoizing\n\t\tit's return value.\n\t\t\"\"\" \n\t\tcache = LRUCache(maxsize=maxsize)\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcache.clear()\n\t\t\t\n\t\t\treturn cache.get(*args, **kwargs)\n\t\treturn wraps(func)(wrapper)\n\t\n\tif typed:\n\t\treturn decorator_with_cache\n\telse:\n\t\treturn decorator_with"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\t@functools.wraps(fun)\n\tdef memoizer(*args, **kwargs):\n\t\tmaxsize = get_maxsize(args, kwargs)\n\t\tif maxsize <= 0:\n\t\t\treturn fun(*args, **kwargs)\n\t\telse:\n\t\t\tkey = make_key(maxsize)\n\t\t\tfun = cache(maxsize=maxsize)(fun)\n\t\t\treturn cache(maxsize=maxsize)(fun, *args, **kwargs)\n\treturn memoizer\n\t\ndef make_key(key):\n\t\"\"\"\n\tConvert string to an integer\n\t\"\"\"\n\treturn int(key.replace(\","}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\t@typecheck(maxsize=int, typed=bool)\n\tdef f_memoized(func):\n\t\t\"\"\"\n\t\tDecorator to make a function as a memoizing callable\n\t\tthat saves in-memory to `maxsize` results based on a\n\t\tLeast Recently Used (LRU) algorithm.\t\t\t\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapped_func(*args, **kwargs):\n\t\t\tcache = {}\n\t\t\tif'maxsize' in kwargs:\n\t\t\t\tif len(args) >= maxsize:\n\t\t\t\t\tcache['maxsize'] = args[0]\n\t\t\t\t\tdel"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorator(fun):\t\t\t\t# @UnusedVariable\n\t\t\"\"\"\n\t\tDecorator to make a function memoizing callable.\n\n\t\t>>> from __future__ import print_function, division, absolute_import, unicode_literals\n\t\t>>> from functools import lru_cache\n\t\t>>> @lru_cache(maxsize=128)\n\t\t... def f(x):\n\t\t... \treturn x\n\n\t\t>>> f(5)\n\t\t5\n\t\t>>> print(f.__doc__)\n\t\t\t15\n\t\t>>> f(15)\n\t\t15\n\t\t>>> @lru_cache()\t\t\t\t#"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorated(*args, **kwargs):\n\t\tcache = [dict(args[0], **kwargs),dict(args[1], **kwargs)]\n\t\t\n\t\tif typed:\n\t\t\tcache_l = cache.keys()[:maxsize]\n\n\t\telse:\n\t\t\tcache_l = cache.keys()[maxsize:]\n\t\t\n\t\tif len(cache_l) > maxsize:\n\t\t\tkwargs[\"_cache\"] = [lru_cache(cache_l, typed=True)]\n\t\t\n\t\treturn update_wrapper(wrapped, decorator(*args, **kwargs))\n\ndef cached_property(*args, **kwargs):\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\t# def cached(f):\n\t#   \"\"\"\n\t#   Decorator that returns a cacheable function\n\t#   decorated with `@lru_cache` as a decorating function.\n\t#   \"\"\"\n\t#   f.memoized = True\n\t#   def wrapper(*args, **kwargs):\n\t#\n\t#   Cache decorator\n\t#\n\t#   @lru_cache(maxsize, typed)\n\t#   def cachewrapper(orig):\n\t#       def decorated_func(*fargs, **fkwargs):\n\t#           lru_cache."}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tdef wrapper(function):\n\t\t\t@wraps(function)\t# the wrapped function\n\t\t\tdef _inner(*args, **kwargs):\n\t\t\t\tcache = {}\n\t\t\t\tkey = str(uuid.uuid1())\n\t\t\t\tcache[\"key\"] = key\n\t\t\t\tcache[\"value\"] = function(*args, **kwargs)\n\t\t\t\treturn cache[\"value\"]\n\t\t\treturn _inner\n\n\t\tif(typed):\n\t\t\tcache = {\"value\": wrapper}\n\t\telse:\n\t\t\tcache = lock_function(wrapper)\n\t\tcache['value'] = wrapper\n\t\treturn cache\n\n\n\tdef __init__(self,"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\t@wraps(fun.outer)\n\tdef memoized(*args, **kwargs):\n\t\tcache = LFU_cache(maxsize=maxsize, typed=typed)\n\t\tdef wrap(*a, **kw):\n\t\t\tresult, cache.clear()\n\t\t\treturn result\n\t\treturn wrap\t\t\t\n\n\n\treturn memoized\n\n\ndef memoized(*args, **kwargs):\n\t\"\"\"\n\tO(L) memoization decorator.\n\n\tUsage:\n\n\t@memoized()\n\tdef add_to_cache(self):\n\t\tself._memoizer.add_to_cache(self"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\nimport hashlib\nfrom itertools import repeat\n\ndef lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (lru)\nalgorithm.\t\"\"\"\n\t\nfrom peewee import * \n\nlru_cache0_10 = lru_cache(maxsize=10, typed=True)\nlru_cache0_100 = lru_cache(maxsize=100, typed=True)\nlru_cache0_1000"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorate(f):\n\n\t\tdef llf(self, *args, **kwargs):\n\t\t\t# print('Cache-length=%i, typed=%s, maxsize=%i' % (len(args), typed, maxsize))\n\t\t\tcache_key = '%s_cache_%s_%s' % (f.__name__, f.__qualname__, maxsize)\n\t\t\tttl = self._cache.setdefault(cache_key, [])\n\t\t\tif limit_after_hit >= 0:\n\t\t\t\tlru_time = self._lru_cache.setdefault(cache_key, time()) + \\\n\t\t\t\t\t "}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\t\t\n\t\tdef f(_id):\n\t\t\t\"\"\"\n\t\t@frk_cache\n\t\t\"\"\"\n\t\t\tdef _f(f):\n\t\t\t\tself.cache_results.append( (max(0,f.id()), True) )\n\t\t\t\n\t\t\treturn _f\n\t\t\n\tcache = lfu_cache(maxsize)\n\t\nclass FrequentlyUsedCache(object):\n\t\"\"\"\n\tDecorator.\n\t\"\"\"\n\t\n\tdef __init__(self, f=None):\n\t\tif f == None:\n\t\t\tself.f = { }\n\t\n\tdef f(self, _id):\n\t\t\"\"\""}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef wrap(f):\tpass\n\treturn wrap\n\n#--- STRUCTURES ---#\ndef lemma(code, **kwargs):\n\tdef newFunc(self, *args, **kwargs):\n\t\treturn self.cached_lemmas.get(code, 0)\n\treturn lemma\n\n#--- SAMPLING ---#\ndef padleft(x, *args):\n\t'''\n\tPad the left side of the function with the given x value.\n\n\tArgs:\n\t    x(int): input\n\t'''\n\treturn [x & 0xff"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\t\t@functools.wraps(fun, updated = fun)\n\t\tdef func_wrapper(self, *args):\n\t\t\tmaxsize -= 1\n\t\t\tkwd_args = (kwd for idx, kwd in enumerate(args)\n\t\t\t\t\t  if idx % maxsize == 0 and kwd is True)(kwd_args)\n\t\t\tif not kwd_args: break\n\t\t\tif len(args) > maxsize: maxsize = len(args)\n\n\t\t\tcache = self.cache_class(maxsize=maxsize)\n\t\t\tif typed:\n\t\t\t\tkey = (fun, (args, kwd_args"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n# TODO: Use a decorator to cache LFU datastructures within a\n# (possibly larger) level.\ndef lfu_cache_decorator(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\tdef decorator(f):\n\t\tdef cache_func(*args, **kwargs):\n\t\t\t# TODO: Make this a cache.\n\t\t\tcache = LFUCache(**kwargs)\n\t\t\tcache.maxsize ="}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef wrapper(method):\n\t\t\"\"\"\n\t\tA caching decorator, that converts an exception\n\t\tinto a function that is cacheable.\n\t\t\"\"\"\n\t\t@functools.wraps(method)\n\t\tdef wrapper_wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tA wrapper function to be used by the decorator above.\n\t\t\t\"\"\"\n\t\t\tif not typed:\n\t\t\t\treturn memoize(method(*args, **kwargs))\n\t\t\telse:\n\t\t\t\treturn method(*args, **kwargs)\n\t\t\twrapper.cacheable = True\n\t\t\treturn wrapper.cacheable\n\t\treturn wrapper_wrapper\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\ncache_manager = LFUCacheManager(maxsize=maxsize, typed=typed)\n\ncache = cache_manager.cache # shortcut\n\ndef cache_decorator_factory(cache_manager = None):\n\t\"\"\"Cache decorator factory\n\n\tDecorator that converts functions into memoized versions of the\n\tfunction so they can be cached at the time of cache calls.\n\n\tThe cache decorator will automatically use the LFUCacheManager in the\n\tsame way that the cache decorator will work with a single invocation.\n\t\"\"\"\n\tdef cache_decorator(func):\n\t\tdef decorator(*args):\n\t\t\t"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif len(self.stack):\n\t\t\treturn self.stack.pop()\n\t\treturn None\n\n\ndef insert_into_queue(queue, item):\n\t\"\"\"\n\tInsert item into queue.  If queue is a Queue-like object, then queue\n\tit.  Otherwise, an item is added iff queue=someitem.  Return\n\tFalse.\n\t\"\"\"\n\treturn self.queue.insert(len(self), (item, ))\n\n\ndef has_queue(queue):\n\t\"\"\"\n\tTell if the queue is a Queue-like object already being tested?"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn self.data\n\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemove and return `(key, value)` pair (may be an indexable object).\n\t\t\"\"\"\n\t\ttry:\n\t\t\tkey = self.pop()\n\t\texcept IndexError:\n\t\t\traise IndexError # [invalid index]\n\t\t\t# [too small]\n\t\tdel(self.data[key])\n\t\treturn self.pop()\n\n\tdef remove(self, value):\n\t\t\"\"\"\n\t\tRemove the first `value` from the list.\n\t\tIf an error occurs, the last item is removed.\n\t\t\"\"\"\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\ttry: self.keymap.pop(self.keymap.index(ANCHOR))\n\t\texcept LookupError: pass\n\t\tself.keymap.insert(self.keymap.index(ANCHOR), (KEY, self.pop(-1)))\n\t\t\n\tdef poplist(self):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value)` pair as a 2-tuple: (key value, index).\n\t\t\"\"\"\n\t\ttry: self.keymap.popall()\n\t\texcept LookupError: pass\n\t\treturn self.keymap.index(ANCHOR), self.pop()\n\t\t\n\tdef resetkeymap(self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\ttry:\n\t\t\tkv = self.dic.popitem()\n\t\t\tkv.pop(0)\n\t\texcept KeyError:\n\t\t\treturn (0, None)\n\t\ttry:\n\t\t\tv, k = kv.popitem()\n\t\t\tself.dic[k] = self.dic.pop(k)\n\t\texcept KeyError:\n\t\t\tpass\n\t\treturn v\n\n\tdef remove(self, key):\n\t\t\"\"\"\n\t\tRemove and return the (key, value) pair identified by `key`.\tThe\n\t\tvalue returned is either the key (or value) of the value in the dict,\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif not self.populated: return self.inorder(self.db.cursor())\n\t\tvalue = self[self.key_to_value(self.key)]\n\t\tself.inorderby(self.key, value)\n\t\tself.populated = True\n\t\treturn value\n\n\tdef last_insert(self):\t\treturn self.insert(1)[0]\n\n\tdef insert(self, key, value):\n\t\t#FIXME update\n\t\told_key, old_value = self.key, self.value\n\t\tself.parent.value += old_value\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn self[:]\n\n\tdef setdefault(self, key, default=None):\n\t\tif default is None:\n\t\t\tdefault = self[key]\n\t\ttry:\n\t\t\tself[key] = default\n\t\texcept KeyError:\n\t\t\treturn default\n\nclass Config(UserDict.IterableUserDict):\n\t\"\"\"\n\tA simple multi-value configuration holder that also supports\n\tdictionary keys as well as values when appending an item. Can be used\n\tfor configuration options such as the CLI --with-config command line option.\n\n\tFor example:\n\n\t.. code-block::\n\t   env.MY"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif len(self) > 0:\n\t\t\tif self[0] in self[1]:\n\t\t\t\tdel self[1][self[0]]\n\t\t\tdel self[0]\n\t\treturn self.get(0, [None])[0]\n\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tEmulates setdefault to raise a KeyError if the key\n\t\tdoes not exist.\n\t\t\n\t\tdefault has the same meaning as setdefault unless key is set.\n\t\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self.setdefault("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\t(key, value) = self.popitem()\n\t\tself.discard(key)\n\t\treturn (key, value)\n\n\tdef poplist(self):\t\"\"\"\n\tRemove and return a list beginning with and including the item at the front of the list.\n\t\"\"\"\n\t(value_list, value) = self.pop(0)\t\t# (Gives the first occurance of 'value')\n\treturn (value_list, value)\n\n\tdef popitemlist(self):\t\t# (Gives the first occurance of 'value')\n\t\treturn self."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tkey, value = max(self.pop(0)), self.pop(0)\n\t\tdel self[key]\n\t\treturn key, value\n\n\tdef setdefault(self, path, with_expires=False):\n\t\t\"\"\"\n\t\tOverwrite the cached value associated with the given path.\n\t\tOnly removes it if value is None.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef update(self, other):\t# pragma: no cover\n\t\t\"\"\"\n\t\tLike :meth:`dict.update` but overwrites values instead of creating a new\n\t\t\"\"\"\n\t\traise NotImplementedError\n\nclass Null"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tv, start, count = self._pos()\n\t\treturn self._list[v][start:start+count]\n\t\t\n\tdef peek(self, key):\n\t\t\"\"\"\n\t\tReturns the next item from the list associated with `key`\n\t\tand removes it from the list if found.\n\t\t\"\"\"\n\t\tv, start, count = self._pos()\n\t\tif key == v: return (\"list\", v), start, count\n\t\ti = start\n\t\tif i < len(self._list):\n\t\t\tif self._list[i]:\n\t\t\t\tself._list[i-1"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\treturn self.setdefault_list(key, default)[-1]\n\n\tdef __getitem__(self, key):\n\t\tif key in self: return self.get(key)\n\t\treturn dict.__getitem__(self, key)\n\n\tdef __setitem__(self, key, value):\n\t\tself.set_list(key, [value])\n\n\tdef set_list(self, key, values):\n\t\t\"\"\"\n\t\tD.set_list(key, [values]) -> None.\n\t\t\n\t\t\"\"\"\n\t\tif not isinstance(values, list):\n\t\t\tvalues = [values]\n\t\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n\n\tdef __iter__(self):\n\t\treturn iter(self._store)\n\n\tdef __len__(self):\n\t\treturn len(self._store)\t\n\tdef __repr__(self):\n\t\treturn repr(list(iter(self._store)))\n\tdef __str__(self):\n\t\treturn '\\n'.join(self._store)\n\n\tdef copy(self):\n\t\traise TypeError(\"DictView is not mutable\")\n\n\t@staticmethod\n\tdef __sorted__(keys="}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry: self.__getitem__(key)\n\t\texcept KeyError: self.__setitem__(key, default)\n\t\t\n\tdef keys(self): return self.__keys[:]\n\t\t\n\tdef itervalues(self): return iter(self);\n\t\n\tdef iteritems(self): return iter(self);\n\n\tdef keys2(self): return self.__keys2[:]\n\t\t\n\tdef values(self): return map(lambda x: x[1], self.__values)\n\t\t\n\tdef items(self): return map(lambda (k,v): (k,v), self.__items)\n\t\t\n\tdef clear(self): self.__"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self.dict:\n\t\t\treturn self.dict[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\treturn default\n\n\tdef __setitem__(self, key, val):\n\t\tif key in self.dict:\n\t\t\tdict.__delitem__(self, key)\n\t\tself.dict[key] = val\n\n\tdef update(self, E, **F):\n\t\t\"\"\"\n\t\tE.update(F) -> None.  Update D from E: for k in E: D[k] = E[k]\n\t\tIf E has a.keys"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\treturn default\n\ndef eval(expression):\n\t\"\"\"\n\tEvaluates an expression in NFA\n\t\"\"\"\n\tresult = evaluate(expression)\n\treturn result\n\ndef evaluate(expression):\n\t\"\"\"\n\tEvaluates an NFA expression\n\t\"\"\"\n\texpression = re.sub(r\"\\'s\", \"'s\", expression)\\\n                .replace(' ', \" \") \\\n                .replace('  \",\" ') \\\n                .replace('[','')"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\treturn default\n\n\tdef setDict(self, d, *args):\n\t\t\"\"\"\n\t\tSet D with elements from 'd' to 'd', overwriting existing values.\n\t\tThe values can be 1- or 2-tuples of the same length.\n\t\t>>> x = D(1, (2, 3), (4, 5, 6))\n\t\t>>> y = x.setDict({'x':10, 'y':20})\n\t\t>>> print y\n\t\t10"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n\n\tdef __getattr__(self, key):\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\traise AttributeError, key\n\n\tdef get(self, key, defaultValue=None):\t#we must override this method\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn defaultValue\n\n\nclass _dict_type(object):\n\t\"\"\"Simple wrapper to translate a dict to a sequence of key-value entries.\n\n\tThe seq_to"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\treturn default\n\tdef setValues(self, values):\n\t\t'''\n\t\tD.setValues([D1,D2,...,Dn]) -> None.  Remove from D[keys]=d if\n\t\tD has a key not in D \n\t\tD.update({k:v}) -> None\n\t\t'''\n\t\tfor k in self:\n\t\t\tself[k] = values\n\tdef copy(self): # D.copy() implements D.__class__, so D"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\treturn default\n\n\tdef setdValue(self, key, value):\n\t\t\"\"\"\n\t\tD.__setitem__(k,d) <==> D[k]=d if k not in D, otherwise set D[k]=d.\n\t\tAllows you to do:\n\t\t\tif key not in D: D[key] = D[k]\n\t\t\telse:\n\t\t\t\tdict.__setitem__(d, key, D[key])\n\n\tdef set(self, key, value):\n\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\nclass Sheet(object):\n\tdef __init__(self, sheetName):\n\t\tself.name = sheetName\n\n\n_sheetNames = Sheet.sheetNames\n\"\"\"The list of sheet names in the book.\"\"\"\n\t\n\t\nclass SheetName:\n\t\"\"\"Provides a name for a sheet. It keeps its internal representation for future use.\"\"\"\n\tdef __init__(self, sheetName):\n\t\tself.name = sheetName\n\t\t\n\tdef getDefault(self, key"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.dict.get(key, default)\n\n\tdef has_key(self, key):\n\t\treturn key in self.dict\n\n\t__contains__ = has_key\n\n\tdef items(self):\n\t\treturn list(self.dict.items())\n\n\tdef keys(self):\n\t\treturn list(self.dict.keys())\n\n\tdef values(self):\n\t\treturn list(self.dict.values())\n\n\tdef __cmp__(self, other):\n\t\traise TypeError(\"Don't compare\")\n\n\tdef __eq__(self, other):\n\t\traise TypeError(\""}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\tdef __getitem__(self, key): return self.get(key, None)\n\tdef __setitem__(self, key, value): self.set(key, value)\n\tdef __contains__(self, key): return self.has_key(key)\n\tdef getlist(self, key): return self.getlist(key)\n\tdef keys(self): return self.iterkeys()\n\tdef itervalues(self): return (self.get(k) for k in self.keys())\n\tdef iteritems"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\t\n\n\tdef iterkeys(self):\n\t\tfor k in self.keys(): yield k.keys()\n\n\tdef __iter__(self):\n\t\treturn self.iterkeys()\n\n\tdef itervalues(self):\n\t\tfor k in self.keys():\n\t\t\tyield self[k]\n\n\tdef iteritems(self):\n\t\tfor k in self.keys():\n\t\t\tyield (k, self[k])\n\ndef _merge_config(base_config, override_config):\n\t\"\"\"\n\tReturns a"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self.dict[key][0]\n\t\texcept (KeyError, IndexError): return default\t\n\tdef dict(self):\n\t\treturn self.dict\n\tdef has_key(self, key):\n\t\treturn key in self.dict\n\tdef __contains__(self, key): return key in self.dict\n\tdef has_key_list(self, key): return key in self.dict\n\tdef update(self, d):\n\t\t\"\"\"\n\t\tD.update(E) -> None.\n\t\tAdds the keys from 'd' to the collection D.\n\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\t# -- backwards compat\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tD.__setitem__(i, y) <==> del D[y]\n\t\t\"\"\"\n\t\t# Note:  other _get() methods may not override __delitem__, so we\n\t\t# need a copy to burn.  But since it turns out, Python will clean up\n\t\t# the circular references that _get() knows about.\n\t\tif getattr(self, '_get') is not key:\n\t\t\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\tdef update(self, dict=None, **kwargs):\n\t\t\"\"\"\n\t\tupdate(mapping) -- update D from mapping WLD values\n\t\tD.update(E) -> None.\n\t\tUpdates all existing items with key/value (which are not in the WLD sense) and values in E\n\t\t\"\"\"\n\t\t# FILL FLUSHES\n\t\tself._dirty()\n\t\tif dict is None: dict = {}\n\t\tif kwargs: self.__dict__.update(kwargs)\n\t\tif len(kwargs):"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__database[key]()\n\t\texcept KeyError:\n\t\t\treturn default\n\t\n\tdef __getitem__(self, key):\n\t\tif key in self.__database:\n\t\t\treturn self.__database[key]\n\t\telse:\n\t\t\traise KeyError\n\t\t\n\tdef __setitem__(self, key, val):\n\t\tif key not in self.__database:\n\t\t\tself.__database[key] = {}\n\t\tself.__database[key][val] = True\n\n\tdef __delitem__(self, key):\n\t\tif key in self.__database:\n\t\t\tself.__database[key]."}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self.__getitem__(key)\t\t\t\n\t\texcept KeyError: return default\n\n\tdef keys(self): return [ key for key in self.__dict__ if not key.startswith('_') ]\n\n\tdef __iter__(self): return self.keys().__iter__()\t\n\tdef __contains__(self, item): return item in self.__dict__\n\nclass Struct(dict):\n    \"\"\"\n    Subclass of dict, with extra functions to support indexing.\n\n    >>> D = Struct([('foo', int), ('bar', ['baz', 'qux'])]) \n    >>> a, b = D.items()\n    >>>"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\tdef _has(self, key):\n\t\tif self.has_key(key):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\nfrom __future__ import unicode_literals, division, absolute_import\nfrom builtins import *  # noqa pylint: disable=unused-import, redefined-builtin\n\nimport logging\n\nimport math\nfrom datetime import datetime\nfrom sqlalchemy import DateTime\nfrom sqlalchemy import Integer\nfrom sqlalchemy import Unicode\nfrom sqlalchemy import select\nfrom sqlalchemy import String\n\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef new(self, values = None):\n\t\t\"\"\"\n\t\tReturn a new D object. D defaults to None which indicates that\n\t\tno key has been added with default (defaults to None).\n\t\t\"\"\"\n\t\tif values == None:\n\t\t\tvalues = {}\n\t\t_d = self\n\t\tfor key in self.keys:\n\t\t\td = _d.get(key)\n\t\t\tif d: break\n\t\telse:\n\t\t\tif not _d: _d = D({})\n\t\t"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef wrapper(self, *args, **kwargs):\t\t\t# @ReservedAssignment\n\t\t\tif args not in cache:\n\t\t\t\tif lock is None:\n\t\t\t\t\tlock = Condition(True)\n\t\t\t\tcache[args] = cache.get(args, 0) + 1\n\t\t\t\treturn cache[args]\n\t\t\treturn wrapper(self, *args, **kwargs)\n\t\treturn wrapper\n\n\n##\t================================================================################\n##\t Internals\n##\t================================================================################\n\nCacheInfo = namedtuple(\"CacheInfo\", ['hits','misses','maxsize', 'currsize'])\n\nclass LRUCache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef get(self):\n\t\t\treturn cache.get(key, self)\n\tdef clear(self):\n\t\tcache.clear()\n\tdef clearCache(self):\n\t\tcache.clear()\n\treturn cachedmethod\n\t\ndef cacheMethodDecorator(cache, key, lock):\n\t\t\"\"\"\n\tDecorator to wrap a class or instance method with a new memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\t@functools.wraps(cacheMethodDecorator)\n\t\tdef get(self):\n\t\t\treturn cache.get(key, self, lock)\n\t\t@functools.wraps"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef caching_callable_(cache, key=hashkey, lock=lock):\n\t\t\tdef new_method(*args, **kwargs):\n\t\t\t\tkey = normalize_cache_key(key, cache)\n\t\t\t\tresult = cache[key] = cache.get(key, {})\n\t\t\t\tresult.update(kwargs)\n\t\t\t\treturn result\n\t\t\tcached_method.cache = cache\n\t\t\tcaching_callable_.lock = lock\n\t\t\treturn new_method\n\t\treturn caching_callable_\n\n\t# def cached(cache_func, key=hashkey, lock=None):\n\t# \t\"\"\"Decorator"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tcache_key = make_cache_key(key, lock=lock)\n\t\ttype.__call__ = cached(cache_key)\n\t\treturn cache_key\n\treturn cachemethod\n# Copyright 2012 OpenStack Foundation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef dec(func):\n\t\t\tkey2 = hashkey(func)\n\t\t\tif key2 not in cache:\n\t\t\t\tcache[key2] = func(key)\n\t\t\treturn cache[key2]\n\t\treturn dec\n\tcache = {}\n\tfor func in methods:\n\t\tfunc_cache = cachedmethod(cache, func)\n\t\tif func_cache is not func:\n\t\t\tcached_results[func] = func_cache\n\t\telif hasattr(func_cache, 'cache'):\n\t\t\tcache[func] = func_cache.cache\n\t\telse:\n\t\t\tcached_"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tclass cachedmethodmethod(object):\n\t\t\tdef __init__(self, method):\n\t\t\t\tself.method = method\n\t\t\t\tself.cache = cache\n\t\t\t\tself.__name__ = method.__name__\n\t\t\t\t\n\t\t\t\tdef __call__(self, *args, **kwargs):\n\t\t\t\t\tif not hasattr(self.cache, '__call__') or not self.cache.__call__(self.method, *args, **kwargs):\n\t\t\t\t\t\treturn self.cache(self.__name__, *args, **kwargs)\n\t\t\t\t\treturn self.cache.__call__(self.method, *args, **kwargs)\n\t\t\t\t\n\t\t\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef deco(func):\n\t\t\tclass memodict(dict):\n\t\t\t\t# hashkey = uuid4() -> {id(o): value}\n\t\t\t\tclass_dict = dict_copy(class_dict, cache)\n\t\t\t\tmethod_name = \"__\" + key + \"__\" + str(func)\n\t\t\t\tmemodict[method_name] = memodict\n\t\t\t\treturn class_dict\n\t\treturn deco\n\treturn cachedmethod\n\"\"\"Support for Buienradar.\"\"\"\nimport json\nimport logging\n\nimport requests.exceptions\n\nfrom buienradar import Bui"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\n\n\tdef __init__(self, func, *args, **kwargs):\n\t\tself._key = hashkey(func.__name__, args, kwargs)\n\t\tself.cache, self.lock = cache, {}\n\t\tself.args = args\n\t\tself.kwargs = kwargs\n\t\tself.result = None\n\t\tself.result_lock = threading.Lock()\n\t\tself.cached_results = []\n\t\tself.lock = self.lock_for_update\n\t\t\n\tdef __call__(self, *args, **kwargs):\n\t\ttry:\n\t\t\tif not self.cache:\n\t\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\n#\tdef _memoize_method(target):\n#\t\t@wraps(target)\n#\t\tdef method(*args, **kwargs):\n#\t\t\tif not hasattr(target, 'cache'):\n#\t\t\t\tmemoize_required()\n#\t\t\t\ttarget.cache = {}\n#\t\t\tmemoize_required.memoize = cache\n#\t\t\telse:\n#\t\t\t\ttarget.cache = cache\n#\t\t\treturn target(target, *args, **kwargs)\n#\t\treturn method\n\t\ndef memoize_required(callable):\n\t\"\"\"\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tcache_dict = cache.get(key,None)\n\t\tif cache_dict is None:\n\t\t\tvalue = make_cache(key, lock)\n\t\t\tcache[key] = value\n\t\telse:\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\treturn cache_dict[key]\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\treturn cachedmethod\n\n\ndef memoized(timeout=None):\n\tdef wrapped(f):\n\t\t@functools.wraps(f)\n\t\tdef memoized(self):\n\t\t\tif timeout is None:\n\t\t\t\tdef timeout_func(*"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\treturn super(Exception, cls).__str__() \\\n\t\t\t+ \" ({0})\".format(str(e)) + \": \" + str(e) + \" (level \" + \\\n\t\t\t\tstr(max_level) + \", path \" + str(max_path_level)\n\ndef get_exception_message(e):\n    \"\"\"\n    Return full message suitable for logging and exception formatting (as generated by the \n    :mod:`logging` module).\n    \"\"\"\n    if isinstance(e, Exception):\n        return \"Exception: {0}\".format(str(e))\n    if isinstance(e, SyntaxError):"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif not issubclass(e.__class__, IOError):\n\t\t\traise ValueError(\"Invalid file object\")\n\t\t\n\t\treturn '{0}: {1}'.format(e.__class__, e.args[0])\n\n\nclass DaemonError(Exception):\n\tdef __init__(self, message, status=None):\n\t\tself.message = message\n\t\tself.status = status\n\n\tdef __str__(self):\n\t\treturn 'daemonError: {0}:{1}'.format(self.status, self.message)\n\n\nclass DaemonStartError(DaemonError):\n\tdef __str__(self):\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\treturn \"{:f}: {e}\".format(str(e), str(e).replace(\".\",\"\"), str(e).replace(\" \",\"\")).replace(\"\\\\\", \"\\\\\\\\\").replace(\"%\", \"%%\")\n\n\ndef excepthook(type, value, tback):\n\t\"\"\"\n\tCallback which logs the excepthook and close the app.\n\n\t:param type: Error type\n\t:param value: Error message\n\t:type type: str\n\t:param tback: Tell application that error occurred\n\t:type type: :py:mod:`Exception`\n\t:param"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif e[0] is None:\n\t\t\treturn \"0x\" if e[1] is None else \"%08x\" % e[1]\n\t\telif e[0] in (1, 2, 3, 4):\n\t\t\tstars = [e[0]]\n\t\t\tif ( len(e[2]) == 2 ) & len(e[3]) > 0:\n\t\t\t\tstars.append(e[2][1])\n\t\t\t\tstars.append(e[2][3])\n\t\t\tretstr = \"\"\n\t\t\tif len(stars) > 0:\n\t\t\t\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\texc_cls, exc_desc, exc_tb = sys.exc_info()\n\t\tmax_level = max(max_level, e.max_level)\n\t\tmax_path_level = max(max_path_level, e.max_path_level)\n\t\t\t\t\n\t\ttype = '%s : %s\\n\\n%s' % (exc_cls.__name__, exc_desc, max_level_str(max_level))\n\t\te.set_traceback(exc_tb)\n\n\t\tif type == 'e.CommandNotFound' and cls.__missing_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tname = getattr(e, 'name', None) or str(getattr(e, 'name', None))\n\t\tmsg = ('%s: %s/%s' % (e.__class__.__name__, e.message, e.error if e.error else ''))\n\telse:\n\t\tname = getattr(e, '__name__', None) or str(getattr(e, 'name', None))\n\t\tmsg = '%s: %s' % (e.__class__.__name__, e)\n\n\tif hasattr(e,'stack_level'):\n\t\ts = str(e.stack_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\t_etype = type(e)\n\t\t\n\t\t_str = \"\"\n\t\tif _etype is BaseException:\n\t\t\t_str = traceback.format_exc()\n\t\t\n\t\tif max_level == 35: \t# Max Call Level\n\t\t\tmsg = \"This exception is a non-critical error (maximum calling stacks reached).\"\n\n\t\telif max_level == 7: \t# Max Path Level\n\t\t\tmsg = \"This exception is a critical error (maximum path nesting problems).\"\n\t\t\n\t\telif max_level == 50: \t# Max path level\n\t\t\tmsg = \"This exception is a critical error (maximum recursion"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n\t\n\te_type = getattr(e, 'type', None)\n\t# FIXME(JF) : This is a hack to make tracebacks consistent\n\t#  in Python 2/3 compatible way\n\t#  we use this constant for a given parameter (Python 3 only)\n\n\tif not e_type:\n\t\treturn str()\n\n\tif max_path_level:\n\t\tif e_type < E_PATH_MAX_LEN: return str()\n\t\telif e_type < E_PATH_MAX_LEN * 8: return str(e)\n\telse:\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif not isinstance(e, EException): return str(e)\n\t\te ='at'+ e\n\n\t\tfor line in str(e).splitlines(True):\n\t\t\tlevel, value, location = line.partition(':')\n\t\t\tvalue = value.strip()\n\t\t\tif not value: continue\n\t\t\tlevel = int(level)\n\t\t\t\n\t\t\tif max_level > level: max_level = level\n\t\t\tif max_path_level > level: max_path_level = level\n\n\t\t\tpath = str(e)\n\t\t\tif len(path) > max_path_level: max_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n    str_class = basestring\n\n# \tif isinstance(e, Exception):\n# \t\treturn str(e)\n#\n# # \tif isinstance(e, SystemExit):\n# # \t\treturn \"Exiting...\"\n# # \tif isinstance(e, Exception):\n# # \t\tif isinstance(e, ValueError):\n# # \t\t\treturn str(e)\n# # \telse:\n# # \t\tif hasattr(e, \"args\"):\n# # \t\t\te = e.args[0]\n\n\n\n#     return str_class"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\ttry:\n\t\t\tif overwrite:\n\t\t\t\ttext_buffer = open(file_name, 'w', encoding=encoding)\n\t\t\t\twith open(file_name, 'a') as file:\n\t\t\t\t\tfile.write(text_buffer)\n\t\t\telse:\n\t\t\t\tfile = open(file_name, 'w', encoding=encoding)\n\t\texcept Exception as error:\n\t\t\tprint(\"The error occured: {}.\".format(error))\n\t\t\treturn None\n\n\t\treturn file_name\n\n\nclass Logger():\n\tdef __init__(self, **kwargs):\n\t\tself.verbose = kwargs."}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\t\n\tif not is_file_supported(file_name):\n\t\traise Exception(\"File type \" + str(type(file_name)) + \" not supported!\")\n\t\t\n\tf = open(file_name, 'w')\n\tf.write(text_buffer)\n\tf.close()\n\t\n\ttry:\n\t\tsize = os.path.getsize(file_name)\n\t\t# Note:  Currently the above checks for the existence of the\n\t\t# contents of the file before appending the content, and there\n\t\t# are some issues when reading a file that contained characters\n\t\t#"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfh = open(file_name, \"a\")\n\t\tif overwrite:\n\t\t\tfh.write(text_buffer)\n\t\telse:\n\t\t\tfh.write(text_buffer)\n\t\tfh.close()\n\t\treturn os.stat(file_name).st_size\n\telse:\n\t\treturn os.stat(file_name).st_size\n\ndef format_bytes(byte_size):\n    \"\"\"\n    Format bytes by number of bytes.\n\n    :param byte_size: Bytes to format into bytes.\n    :type byte_size: int\n    :return: Bytes or more"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\n\tcontent = file_name.read()\n\tif encoding is None:\t# Default encoding is utf-8\n\t\tencoding = 'utf-8'\n\n\tfile_len = len(content)\n\ttext_len = len(text_buffer)\n\n\tfile_handle = open(file_name, write_mode)\n\tif overwrite == True:\n\t\tfile_handle.write(bytes(content))\n\t\tfile_handle.write(encoding)\n\t\tfile_handle.write(text_buffer)\n\telse:\n\t\t# Open a file handle and truncate.\n\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif overwrite:\n\t\t\tnew_text = \"\".join([text_buffer, str(encoding)])\n\t\t\tnew_file_obj = open(file_name, \"w\")\n\t\t\tnew_file_obj.write(new_text)\n\t\t\tnew_file_obj.close()\n\t\telse:\n\t\t\ttemp_file = open(file_name, \"a\")\n\t\t\tencoding = \"UTF-8\"\n\t\n\t\t\tif isinstance(text_buffer, str):\n\t\t\t\ttemp_file.write(\"\\\\\" + text_buffer.encode(encoding) + \"\\\\\\n\")\n\t\t\telse"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\ttext_file = codecs.open(file_name, encoding, errors)\n\t\tcount = 0\n\t\twhile count <= 0:\n\t\t\tnewline = text_file.readline()\n\t\t\tif newline == '':\n\t\t\t\tbreak\n\t\t\tcount += 1\n\t\t\tif newline.count(text_buffer) == 0:\n\t\t\t\ttry:\n\t\t\t\t\tnewline += text_buffer\n\t\t\t\t\ttext_file.close()\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\telif exists(file_name):\n\t\ttext_file = open_with_delete_if_not_exists(file_name)\n\telse:"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfile_obj = TextFile(file_name, encoding).open(text_buffer, encoding)\n\t\tcontents_offset = file_obj.size()\n\t\tfile_obj.close()\n\t\tread_only = bytes_written = 0\n\n\t\tif not overwrite:\n\t\t\ttry:\n\t\t\t\twith codecs.open(file_name, \"wb\", encoding) as f:\t# Create the file in binary mode.\n\t\t\t\t\tf.write(text_buffer) # Copy the buffer to the file.\n\t\t\t\t\t\n\t\t\t\t\t# Open the new file in binary mode.\n\t\t\t\t\tos.fsync("}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tnum_bytes = 0\n\t\ttry:\n\t\t\tbytes = codecs.EncodedFile(text_buffer, encoding,'replace')\n\t\texcept UnicodeDecodeError as excp:\n\t\t\tbytes = codecs.decode(text_buffer,'replace')\n\t\tnum_bytes = len(bytes)\n\t\tif num_bytes == 0:\n\t\t\treturn True, 'Error'\n\t\ttry:\t\n\t\t\tnum_lines = 2 * num_bytes / (num_bytes + num_lines + 1)\n\t\t\tos.makedirs(os.path.split(file_name)[0], exist_ok="}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\t\n\tif not os.path.exists(file_name):\n\t\treturn 0\n\t\n\tif os.path.getsize(file_name) > 0:\n\t\tcontent = \"\"\n\n\t\twith open(file_name, \"rb\") as file_open:\n\t\t\tcontent = file_open.read().replace(encoding, '').replace(\"\\xce\\xa9\", \"'\")\n\n\t\tif sys.version_info >= (3, 0):\n\t\t\tcontent = content.replace(\"u'\",'')\n\t\t\tcontent = content.replace(\"u'\",'')\n\n\t\tcontent = content.replace(b\"\\"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\twith open(file_name, 'wb') as f:\n\t\t\tbytes_written = 0\n\t\t\ttotal_size = len(text_buffer)\n\t\t\tstart = 0\n\t\t\tbytes_to_write = int_or_none(0)\n\t\t\tfor page in range(self._page_size):\n\t\t\t\tf.write(b'%05d'%page)\n\t\t\t\tif page < self._page_size - 1:\n\t\t\t\t\tbytes_to_write += 4 # Include space at the end of the filename.\n\t\t\t\tfor _ in range_helper(page, self.size, end"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\tstream = open(file_name, \"r\")\n\n\t\ttry:\n\t\t\treader = codecs.getreader(encoding.lower() if encoding else encoding.lower())\n\t\t\tstream_text = reader.read()\n\t\texcept UnicodeDecodeError:\n\t\t\tstream_text = \"\"\n\t\texcept IOError:\n\t\t\tstream_text = \"\"\n\n\t\tif stream_text:\n\t\t\treturn stream_text\n\t\telse:\n\t\t\traise FileNotFoundException(\"File could not be located at: {}\".format(file_name))\n\"\"\"\nSupport for Broadlink RTC covers.\n\nFor more details about this platform"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n    if not file_name:\n\tif encoding:\n\t    file_name = \"%s.txt\" % (file_name, )\n\telse:\n\t    file_name = \"%s.txt\" % (file_name, )\n    try:\n      file = open(file_name, \"r\")\n    except IOError, e:\n      return \"Unable to open file %s for reading.\" % (file_name, )\n        \n\n    lines = file.readlines()\n    file.close()\n\n    try:\n\tif sys.version_info.major == 2:\n\t    for line in lines:\n\t\tif"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttry:\n\t\t\ttext_buffer = codecs.open(file_name, encoding=encoding)\n\t\t\ttext_buffer.read()\n\t\texcept (IOError, OSError) as e:\n\t\t\tprint(\"ERROR: Unable to read the file: {0}.\\n\".format(file_name), file=sys.stderr)\n\t\t\tprint(e, file=sys.stderr)\n\t\treturn text_buffer\n\t\n\t@staticmethod\n\tdef convert_file_to_textbuffer(file_name, file_encoding):\n\t\t\"\"\"\n\t\tTry to read the file as text (UTF-8"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\n\t# Check file exists\n\ttry:\n\t\t# Check file existance\n\t\tfile_lock = core.os_fs_file_lock(file_name)\n\t\tcheck_file_existance(file_lock)\n\t\t# Create text buffer using specified encoding\n\t\ttext_buffer = core.text_buffer(file_name, encoding)\n\n\texcept core.FileExistsError:\n\t\treturn None\n\t# Return buffer or None\n\treturn text_buffer\n\n#-------------------------------------------------------------------------------\n\ndef create_dir(sub_dir):\n  \"\"\"\n  Create or move \"dir1"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\t\n\t# Open a file for reading\n\tif FILE_TYPE.startswith(\"text\"):\n\t\tif not file_name:\n\t\t\traise IOError(\"File name not specified.\")\n\t\tfilename = file_name\n\telse:\n\t\ttry:\n\t\t\tfilename = codecs.open(file_name, \"r\", encoding)\n\t\texcept IOError:\n\t\t\treturn None\n\t\n\t# Check for new line\n\tif not filename.newlines and not filename.newlines[0].endswith(\"\\r\"):\n\t\traise IOError(\"Invalid file or line not begin with a newline character.\")\n\t\n\t# Read file line by line"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\tfor line in getfile(file_name):\n\t\t\tnew_line = []\n\t\t\tfor line in encoding:\n\t\t\t\t\tif not line.startswith(line):\n\t\t\t\t\t\tnew_line.append(line)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tnew_line.append(str(line, encoding))\n\t\t\tline = new_line\n\t\t\tyield line\n\n\t\ndef write_unicode_file(file_name, content, encoding):\t\t\t\"\"\"\n\tWrite this file (e.g. from myfile.py) the content of textfile.\n\tUnicode is not yet supported.\n\t:param file"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ndef load_textbuffer_from_file(filename):\n\t\"\"\"\n\tLoad a file from the disk. When using STREAMING a text buffer, it reads the whole file and uses standard codecs to read it, and if the file cannot be loaded it must be read directly.\n\t:param filename: str \t Filename where the text buffer must be loaded.\n\t:type filename: str\n\t:return: File content, if successful\n\t:rtype bytes\t\n\t\"\"\"\n\t\t\n\tfile_content = None\n\ttry:\n\t\tfile_content = open(filename, 'r', encoding"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\traise NotImplementedError(\"\")\n\n\tif not (encoding == \"utf-8\" or encoding.upper() == \"UCS-2\"):\n\t\treturn None\n\n\ttry:\n\t\ttry:\n\t\t\tfile = open(file_name, \"r\", encoding)\n\t\t\treturn file.read() #read in the file or raise an exception\n\t\t\tfile.close()\n\t\texcept UnicodeDecodeError:\n\t\t\treturn None\n\texcept IOError as ex:\n\t\traise ParseError(\"Could not parse file: \" + str(ex)) #TODO raise an error with generic error codes.\n\ndef is_text_file(file"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttry:\n\t\t\tlines = get_file_as_lines(file_name)\n\t\texcept IOError as e:\n\t\t\tprint(\"[Error: could not open file %s] | %s\" % (file_name, e))\n\t\texcept UnicodeDecodeError as e:\n\t\t\tprint(\"[Error: Problem reading file %s] | %s\" % (file_name, e))\n\t\treturn lines\n\n\t\n# =============================== #\n#    CLASS: O_TEXTBUF         #\n# =============================== #\nclass O_TEXTBUF:\n\t\n\tdef __init__(self, buffer_size=1024"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttry:\n\t\t\tif isinstance(file_name, unicode):\n\t\t\t\tfile_name = file_name.encode(encoding)\n\t\texcept UnicodeEncodeError as error:\n\t\t\t\tgui_utilities.show_message('Could not decode file name {0:s} using a non-unicode encoding'\n\t\t\t\t\t\t\t.format(file_name),\n\t\t\t\t\t\t\t\ttitle='Error',\n\t\t\t\t\t\t\t\twait_for_response=True)\n\t\t\t\tgui_utilities.show_message('Error',\n\t\t\t\t\t\t\t\t\t\ttitle='The encoding of {0!a} in your file is {1:s}. Please enter the encoding '\n\t\t\t\t\t\t\t\t\t"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tf_exists = os.path.exists(file_name)\n\t\treturn file_exists and os.path.isfile(file_name)\n\tdef open_file_read(filename):\t# Open file in read mode for reading.\n\t\ttry:\n\t\t\tfh = open(filename,'r')\n\t\texcept IOError:\n\t\t\tprint 'Cannot open file',filename\n\t\t\t#self.error_msg=True\n\t\t\tself.error_msg= \"Can not open the file '%s'. %s\" % (filename,strerror(errno.ENOENT))\n\t\t\tself.__show_message"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ttry:\n\t\t\twith open(file_name, 'r') as file: # open file\n\t\t\t\tfor line in file: # read lines\n\t\t\t\t\titem_content = line.split()\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif item_content[0]!= '' and item_content[1]!= '':\n\t\t\t\t\t\t\tfull_file_name = os.path.join(path, file_name)\n\t\t\t\t\t\t\tif is_file_exist(full_file_name):\n\t\t\t\t\t\t\t\treturn False\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn False\n\t\t\t\tif len(item_content) > 0:\n\t\t\t\t\treturn True"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\n\t\n\twith open(file_name, 'r') as f:\n\t\tfor line in f:\n\t\t\tif (line == '{'):\n\t\t\t\treturn True\n\t\t\tif (line == '}'): # end of file\n\t\t\tif (line == ''): # end of file\n\t\t\t\treturn False;\n\t\t\treturn True;\n\t\n\ndef get_filename_size(file_path):\n\t\"\"\"\n\tGet file size.\n:param file_path: File path.\n:type file_path: str\n:return file size in bytes.\n:rtype tuple\t\"\"\"\n\tp = stat(file"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t       \n\tfile_exists = os.path.exists(file_name)\n\n\tif file_exists == False:\n\t\terror(file_name + \" file not found.\")\n\t\treturn False\n\telse:\n\t\treturn True\n\n\n\n\n#get the size of the file\ndef get_file_size(file_name):\n\tfile_size = 0\n\tfile_size = os.path.getsize(file_name)\n\treturn file_size\n\n\n################################################################################\n################################################################################\n#Main\n################################################################################\n\ndef main():\n\t\n    print \"-------------------------------------------------------------------------------------------------\"\n    print \"                                 "}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tresult = False\n\t\ttry:\n\t\t\tfp = None\n\t\t\tfp = open(file_name, 'r')\n\t\texcept Exception as inst:\n\t\t\tprint(\n\t\t\t\t'[!] Error [{0}]: {1}'.format(\n\t\t\t\t\tinst, type(inst)))\n\t\t\tresult = False\n\t\telse:\n\t\t\tfp = fp.read()\n\t\t\tfp.close()\n\t\treturn result\n\n\t@staticmethod\n\tdef is_dir_exist(dir_name):\t\"\"\"\n\tCheck if dir name exist.\n:param dir_name: Dir name.\n:type dir"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ttry:\n\t\t\tfile_stat = os.stat(file_name)\n\t\t\tif (int(file_stat.st_size * (1024 * 1024)) > int(conf['log_size'])):\n\t\t\t\tif (os.path.isfile(file_name)):\n\t\t\t\t\tfile_content = read_file(file_name)\n\t\t\t\telse:\n\t\t\t\t\terr_msg = 'Error, the log file \\'%s\\' not exist! Check path and name' % file_name\n\t\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\tfile_content = read_file(file_name)\n\t\t"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tf = open(file_name, \"r\")\n\t\tfile_content = f.read()\n\t\tf.close()\n\t\treturn validate_file(file_content)\n\n\tdef remove_dir_p(self, file_dir):\n\t\t\"\"\"\n\t\tRemove the directory with a specific file_dir\n\t\t:type file_dir: str\n\t\t:param file_dir: File directory.\n\t\t:return: Return true (remove dir)\n\t\t\"\"\"\n\t\ttry:\n\t\t\tshutil.rmtree(file_dir)\n\t\texcept Exception as e:\n\t\t\tlogging.error"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif os.path.isfile(file_name):\n\t\t\treturn True\n\t\telse:\n\t\t\tprint('[+] Error: File name not found')\n\t\t\tsys.exit(0)\n\ndef is_new_file():\t\"\"\"\n\tDetermine if file is new.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (new), false(exist), file name not equal to old file name\n:rtype bool\t\t\t\n\"\"\"\n\tfile_exists = False\n\tline_number = 0\n\tif os.path.isfile"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tf_name = os.path.join(config_path, file_name)\n\t\t\n\t\t# Check if the file exists\n\t\treturn os.path.isfile(f_name)\n\t\n\t# Error\n\treturn False\n\ndef get_file_list(config_path, files=[]):\n\t\n\t\"\"\"\n\tRecursively return files list (and sub-list) from file. File list can be filtered by user input. \n\tFunction will be called by user as argument to `file_name`.\n\tUsage : python get_file_list(config_path=[\"/etc/nginx/sites"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tfile_exists = os.path.isfile(file_name)\n\t\treturn file_exists\n\t\n\t\n\tdef get_root_path_dir(mypath):\n\t\t\"\"\"\n\t\tGet the root path directory.\n\t\t:param mypath: Current directory path.\n\t\t:type mypath: str\t\t\n\t\t\"\"\"\n\t\troot_dir = os.path.abspath(os.path.dirname(mypath))\n\t\troot_dir = os.path.join(root_dir, os.pardir)\n\n\t\tif not os.path.exists(root_dir):\n\t\t\ttry"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tif hasattr(cls, '_logger'):\n\t\t\treturn\n\n\t\ttqdm.tqdm(cls.LOGGING_LEVEL)\n\t\t# logger = logging.getLogger()\n\t\t# logger.setLevel(logging.DEBUG)\n\t\t# logger.propagate = False\n\t\tcls._logger = None\n\n\tdef _set_logging_level(self, level):\n\t\t\"\"\"\n\t\tset loglevel in class\n\t\t\"\"\"\n\t\tif not isinstance(level, int):\n\t\t\traise TypeError(\n\t\t\t\t\"{0} of log levels must be a int type\"\n\t\t\t\t.format(self.__class__.__"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tfor n, f in cls.loggers.iteritems():\n\t\t\tif f is not None:\n\t\t\t\tcls.loggers[n].close()\n\n@_pytest.fixture(scope='session')\ndef _logging_fixture(request):\n\t@contextmanager\n\tdef _logged_user(request):\n\t\twith _LoggingSession(request):\n\t\t\tyield\n\n\trequest.addfinalizer(_reset_logging)\n\t\n\treturn _logged_user\n\nclass TestLoggingLog(object):\n\tdef test_logging_logged_success(self):\n\t\t\"\"\"\n\t\tShould get a successful log (True"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls._log = []\n\t\treturn\n\n\t@classmethod\n\tdef set_logger(cls, log_obj):\n\t\tcls._log.append(log_obj)\n\t\treturn\n\n\t@classmethod\n\tdef log(cls, msg):\n\t\t\"Log a message at the end or start\"\n\t\ttry:\n\t\t\t_log = msg.strip().split(\"\\n\")\n\t\t\tif 'logger' in _log[-1]:\n\t\t\t\tlog_obj = _log\n\t\t\telse:\n\t\t\t\t_log = log\n\t\t\t\n\t\t\t#Log a single line\n\t\t\tif len(_log)>1"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls._logger = logging.getLogger(__name__)\n\n\"\"\"\n===================================================\nSimpleCV\n===================================================\n\nThis little module implements a simple CV method for simple data classification. It\noffers the interface with the OpenCV library, and implements no event detection,\nand is pretty similar.\n\"\"\"\nimport SimpleCV, cv, numpy as np, os, sys\nfrom collections import namedtuple\nfrom sklearn.datasets import make_classification\n\nclass CV_Class(object):\n    def __init__(self, name, args=None, max_iter=None, tol=1e-4"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.logger.handlers = []\n\t\tcls.formatter = Formatter()\n\nlogging.configure(CONFIG)\n# encoding: utf-8\nimport datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        \n        # Adding field 'Image.size'\n        db.add_column('lizard_wms_image','size', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True), keep_default=False)\n\n        #"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.debug_levels = [logging.CRITICAL,\n                             logging.ERROR,\n                             logging.WARNING,]\n\tcls.loggers = []\n\tcls.loggers_disabled = True\n\tcls.logger_handlers = collections.OrderedDict()\n\tcls.__logger__ = logging.getLogger(\"__logger__\")\n\t# @loggable\n\t@classmethod\n\tdef _remove_debug_levels(cls):\n\t\tfor h in cls.debug_levels:\n\t\t\tif h in cls.loggers.keys():\n\t\t\t\tcls.loggers.pop(h)\n\t\t\t\tcls.loggers_disabled = False\n\t"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls._logging = 0\n\t\tcls._flushed = 0\n\t\t\n\tdef _flush(cls):\t\n\t\tcls._flushed = 0\n\t\tcls._logging = cls._logging * 2\n\t\t\nclass File(object):\n\t\"\"\"\n\tAn object representing a path to a text file or other object.\n\t\n\tFile objects support iteration (for iteration purposes).\n\tThis has been tested against: \n\t\thttp://mail.python.org/pipermail/python-list/2009-October/368301.html\n\t\tAll strings in a row are of type unicode.\n\t"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tif len(cls.log) == 0:\n\t\t\tcls.log = ['']\n\t\telse:\n\t\t\tcls.log = cls.log[-1]\n\t\tcls.log.append('Log')\t\n\t\tcls.log.append('[DEBUG] %s' % time.asctime())\n\t\t\n\t@classmethod\n\tdef reset_all(cls):\n\t\t\"\"\"\n\t\tReset all\n\t\t\"\"\"\n\t\twhile cls.log == [''] or cls.log[0] == u\"\":\n\t\t\tif cls._log is None:\n\t\t\t\treturn\n\t\t\ttry:\n\t\t\t\tcls._log = [item"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\t# reset all previous logs\n\t\tfor (root, dirnames, filenames) in os.walk(cls.project_folder):\n\t\t\tif '.svn' in dirnames:\n\t\t\t\tpass\n\t\tfiles = [\n\t\t\t'gogogo/logs/log_{0}.log'.format(now),\n\t\t\t'log_{0}.log'.format(now),\n\t\t]\n\t\tfor file in files:\n\t\t\t#log_file = 'log_{0}.log'.format(now)\n\t\t\tos.system('rm -f {0}'.format(file))\n\t\ttry:\n\t\t\tos"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\n\n\tdef __init__(self):\n\t\n\t\tself._logger = { }\n\t\t\n\t\n\n# -*- coding: utf-8 -*-\n#\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.__class__\n\ndef _createModuleObj(modname):\n\t\"\"\"\n\tImport a module and return its name.   This can be used to import\n\tmodules at the right place.\t\"\"\"\n\t#import it from the source if the target is \"Py\"\n\tif modname.endswith(\".py\"):\n\t\tfrom imp import new_module\n\t\treturn new_module(modname[:-3])\n\treturn modname+'.'+modname\n\n\n###############################################################\n### the main test code\n\nfrom Ft.Lib.Subsets import Subset\nfrom Ft.Lib.Compiler import"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.__class__.__name__[:-2], self.interface\n\n\t#--Helpers-----------------------------------------------------------------\n\t@property\n\tdef isMainThread(self):\n\t\t\"\"\"Is this self-thread or not?\n\t\t\"\"\"\n\t\treturn self.thread_id is None\n\n\t##~~ Settings-----------------------------------------------------------------------\n\n\tdef getSetting(self, name):\n\t\t\"\"\"Get the settings for this plugin\n\n\t\tIf a setting is not set by the plugin the default is returned.\n\n\t\t@param name: Plugin setting name\n\t\t@type name: str\n\t\t@return: Settings instance\n\t\t@rtype"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tc = self.klass\n\t\tif isinstance(c, type):\n\t\t\tname = c.__name__\n\t\t\tif name is not None:\n\t\t\t\tif self.suffix is None and name.endswith(self.suffix):\n\t\t\t\t\tname = name[:-len(self.suffix)]\n\t\t\t\telse:\n\t\t\t\t\tname = name + self.suffix\n\t\t\tif name in self.classes:\n\t\t\t\treturn self.classes[name]\n\t\t\telse:\n\t\t\t\treturn None\n\t\telse:\n\t\t\treturn None\n\n\tdef __call__(self, request, *args, **kwargs):\n\t\ttry:\n\t\t"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tsuf = self.suf\n\t\tname = self.name\n\t\tif self.scope == 'GLOBAL' and name[:2] == '__' and name[-3:] == '__':\n\t\t\ttry:\n\t\t\t\tself.scope = self.scope[2:]\n\t\t\texcept IndexError:\n\t\t\t\ts = suf.suf[name] = suf.newScope()\n\t\t\t\ts.setParent(s)\n\t\t\t\tself.update({name: s})\n\t\t\t\tif name == '.':\n\t\t\t\t\treturn self\n\t\t\t\telse:\n\t\t\t\t\treturn self\n\t\treturn self\n\n\tdef _findSelf"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.__metaclass__[:3]\n\n\n\t# For Python <= 2.5:\n\tif 'Py' in self.names:\n\t\treturn object\n\telif 'Fallback' in self.names:\n\t\treturn SafeRebindable\n\traise ImportError(\"This code is built using Python >= 2.5 not found.\")\n\n\n\tdef __repr__ (self):\n\t\treturn \"PyoObject( %s)\" % repr(pyoArgs(self.pyobj, self.pyobj.In()))\n\n\n\tdef getPy_(self):\n\t\t\"\"\"\n\t\tReturn the PyClass that the object represents"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn _pyCode\n\tdef _getTargetClassVersion(self):\t# for backward compatibility\n\t\treturn sys.hexversion\n\npycode = _getTargetClass()\nPyCode_GetVersion = PyCodeCodeGenerator().generate(\n\t\tpycode.__dict__, 'PyCode_GetVersion')\ndel _getTargetClass\nfrom PyQt4 import QtGui, QtCore\nfrom PyQt4.QtCore import pyqtSignal as Signal\n\nimport numpy as np\n\n#from matplotlib.patches import Polygon\n##import matplotlib.collections as mcoll\n\nclass Slider(QtGui.QWidget):\n\n    value"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self._target.__class__\n\nclass Method(Py):\n\tdef __new__(klass, mod):\n\t\t\"\"\"\n\t\tApply the methods specified to the target.\n\t\t\"\"\"\n\t\tassert inspect.isfunction(mod), \"Can only instantiate classes\"\n\t\tklass._controller = klass\n\t\tklass.__name__ = mod.__name__\n\t\treturn klass\n\t\n\tdef __init__(self, target, name, description, **keywords):\n\t\tPy.__init__(self)\n\t\tname = str(name)\n\t\tself.method = Method(target, name, **keywords)\n\t"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn _target_class\n\n\nclass BasePluginLoader(object):\n\n\tdef __init__(self):\n\t\tself.name = 'base'\n\n\tdef loadPlugins(self, testLoad):\n\t\t'''\n\t\tLoads loaded plugins given the plugin name, and uses the plugins\n\t\t'pluginLoader'.\n\n\t\t@param testLoad: Boolean indicating if the testLoad parameter is\n\t\t\tenabled.\n\t\t'''\n\t\tself.name = testLoad\n\n\tdef loadPluginsForCategory(self, categoryName):\n\t\t'''\n\t\tLoads all plugins and sub-modules of the specified category\n\t\tand"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.__class__\n\n# Some constants related to text.\nCursorPosition = namedtuple('CursorPosition', ['characterIndex'])\nTextLength = namedtuple('TextLength', ['characterIndex'])\nTextFormat = namedtuple('TextFormat', ['characterIndex'])\n\n# Concrete numeric types.  Each numeric type has a different set of\n# byte-length strings.\n#\n# I would prefer to use the C types here, but for performance, these are\n# currently unimplemented, as we haven't broke anything.  However, there\n# wouldn't be an ideal way to do this"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn \"Pype\"\n\n\tdef _getClassSuffix(self):\t\t\"\"\"\n\tA string with a suffix, to be appended before a string with\n\tthe name in case of a Py target string.\n\t@rtype: String\n\t@return: @code\n\t\"\"\"\n\t\treturn \"_target\"\n\n\tdef _getClassName(self, i):\n\t\t\"\"\"\n\t@rtype: String\n\t@return: @code\n\t\"\"\"\n\t\treturn str(_getClassName(self._target) + \"_\" + str(i))\n\n\tdef _getClass(self):\t"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\ndef _doMergeOrderings(a, b, orderings):\t\n\tif a[0] == b[0]:\t\t\t# merge orderings in common order\n\t\treturn orderings\n\telif (a[0] == b[0] and a[1] == b[1]):\t# merge orderings in common; merge in reverse order\n\t\t# use ordered permutations order instead as in the ordered permutations\n\t\treturn _doSortCombinations(a, b, orderings)\n\telse:\n\t\t# use unsorting, reverse order so that sorting matches both of\n\t\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\tif not _py2 and not hasattr(orderedList,'reverse'):\t# python < 3\n\t\torderedList = orderedList.reverse()\t\n\n\tif not orderedList:\n\t\treturn []\n\ttry:\n\t\treturn orderedList.pop()\n\texcept IndexError:\n\t\tpass\n\n\tresult = []\n\tfor o in orderings:\n\t\tif hasattr(o, \"__getitem__\") and not isinstance(o, tuple):\n\t\t\tresult.extend(_legacy_mergeOrderings([\n\t\t\t\to[0]\n\t\t\t]))\n\t\tresult.append(o)\n\tresult."}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\t# Keep track of the common prefix for each method.\n\t\tprefixA, prefixB = {}, None\n\n\t\t# All the names used in this function (including this one) should be\n\t\t# prepended with the suffix applied to each method.\n\t\tprefixedA, prefixedB = [], {}\n\n\t\tfor prefix, method in orderings:\n\t\t\tif prefix in prefixedA:\n\t\t\t\traise ValueError(\"Prefix %r conflicts with method %r\" % (prefix, method))\n\t\t\tprefixA[prefix] = method\n\n\t\t\t# All the names used in this function (including this one) should be\n\t\t\t#"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\t\n\tout_reorder = []\n\tfor i in range(len(orderings)):\n\t\tfor j in range(i + 1, len(orderings)):\n\t\t\tif orderings[i] < orderings[j]:\n\t\t\t\tout_reorder.append(orderings[j])\n\t\t\telse:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tout_reorder.append(orderings[i])\n\t\n\tmerge = [x for x in out_reorder if x in orderings]\n\treturn merge\n\ndef findCommonPrefixes(data, prefix, objId, maxPrefixes=100"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n_mergeOrderings([\n... ['x'],\n... ['q', 'z'],\n... ['x', 'q'],\n... ['q', 'z'],\n... ['x', 'y'],\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... ['x', 'y'],\n... ['q'],\n... []\n... ])\n['x', 'y', 'z', 'q']\t\"\"\"\n\t\n_mergeOrderings([\n... ['b', 'd', 'c'],\n... ['b', 'c'],\n... ['b', '"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\ndef _merge_orderings(orderings):\t# Merge multiple orderings so that within-ordering order\n\t# is preserved - and their orderings need to be fully sorted\n\t# so that they can be rearranged if equilibrium \n\tres = []\n\tfor order in orderings:\n\t\tif order == None:\n\t\t\tcontinue\n\t\tassert 0 <= order[0] <= len(orderings), \"Order should have at least len(p-q) orders\"\n\t\tfor left, right in zip(order[1:], order[0:]):\n\t\t\tleft"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\tif len(orderings) <= 1:\n\t\treturn orderings\n\t\n\tif not isinstance(orderings, (list, tuple)):\n\t\torderings = (orderings,)\n\n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\n\treturn''.join(_legacy_mergeOrderings(a) for a in orderings)\n\ndef _legacy_mergeOrderings(objects):\n\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\tif orderings and all(len(o) == 2 for o in orderings):\n\t\tyield _nestedSortedList_toList_( orderings );\n\telse:\n\t\twhile True:\n\t\t\tfor o in reversed(orderings):\n\t\t\t\tyield o;\n\tyield _nestedSortedList_toList_( orderings);\n\n\n# Helper for unit tests below\ndef _splitTestList( testList ):\n\t\"\"\"\n\tSplit the testList so that we don't have to check more than\n\tone line.\n\n\tEach line must start with the whitespace characters.\n\n\tReturns"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\ndef _legacy_merge(right, left, left_name, right_name):\n\tif len(left)!= len(right):\n\t\treturn False\n\tresult = []\n\tfor i, (l, r) in zip(left, right):\n\t\tif l == r:\n\t\t\tcontinue\n\t\tif r[i] == 'i':\n\t\t\tresult.append((l, i))\n\t\telif r[i] == 'j':\n\t\t\tresult.append((i, jn[i]))\n\t\telif r[i] == 'k':\n\t\t\tresult.append"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tn = len(orderings)\n\n\t\t# Keep track of whether first order needs to be merged\n\t\tnewOrderings = []\n\n\t\t# Prepare for the rest of the orderings in the sequence\n\t\t# for emerging orderings (no need to check for equality\n\t\t# of different sorting orders).\n\t\tfor i in range(n):\n\t\t\t# Pick the first element of the two orderings\n\t\t\tt1 = orderings[i][0]\n\n\t\t\t# Check if the orderings can be merged into the existing order\n\t\t\t# while we still have the desired sort order.\n\t\t\t"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\t# this is a copy of obj.__dict__, but we want our own\n\t\t# informations with their fields stripped from the class variables\n\t\treturn getattr(obj, '__dinfo__', getattr(obj,'_dinfo__', None))\n\t# This is probably not safe, but we do want to pass the string to\n\t# __repr__() like we do for repr():\n\treturn repr(object)\n\ndef isInterface(obj): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn True if the object is a class, and False otherwise.\n\n\tIf we're dealing with an"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn None\n\tif isinstance(object, _ast.Interface) and IDeclaration.providedBy(object):\t\t\treturn object\n\tfor member in object.declarations():\t\t\tyield member\n\ndef resolveExistential(object):\t\t\t\"\"\"\n\tReturn the interfaces directly provided by the given obj.\n\n\tIDeclaration objects accept the interfaces provided by the given object.\n\tThis includes interfaces that are interfaces directly provided by the given object.\n\t\"\"\"\n\tif isinstance(object, _ast.Interface) and IDeclaration.providedBy(object) and object.extends: return object\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn _iface_cache.get(object)\n\ndef interfaceProvides(object, interfaces, version): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object::\n\n\t\t# The specification requires specifying IDeclaration interface\n\t\t# implemented by the interface.  The other interfaces in\n\t\t# the specifications return None.  The interfaces interfaces\n\t\t# are declared for the given object as if they were part of some\n\t\t# interface.\n\t\t#\n\t\t# The specifications may specify interfaces themselves\n\t\t# not included here as `for x in interfaceProvides(x"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\traise NotImplementedError\n\nclass IDeclaration(Interface): # pylint:disable=redefined-builtin\t\"\"\"\n\tImplementation of an IDL `interface <http://zope.interface.org/spec/idl.html#idl-definitions>`_\n\tinterface.\n\t\n\tExample of code, in this example interface:\n\t\n\t>>> from zope.interface._compat import IDeclaration\n\t>>> from zope.interface._compat import IReference\n\t>>> class IFoo(Interface):\n\t...\n\t...     implements(IFoo)\n\t\n\t>>> class IBaz(Interface):\n\t...\timplements(IFoo)\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tfrom pygen.api import PythonInterface\n\t\tif isinstance(object, (int, long)):\n\t\t\treturn PythonInterface(proxy=object, boundname=objc_name(object))\n\t\tif isinstance(object, (str, unicode)):\n\t\t\tclassname = objc_name(object)\n\t\t\tif classname == \"__main__\":\n\t\t\t\treturn PythonInterface(proxy=object, boundname=objc_name(object))\n\t\t\treturn AnonymousInterface(proxy=object, boundname=objc_name(object))\n\telse: # pragma: no cover\n\t\treturn AnonymousInterface(proxy=object,"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\nclass _DescriptorInterface(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tA :py:class:`.IDeclaration` to be used in a descriptor implementation.\n\t\"\"\"\n\t\n\t# TODO: test whether this can still be used (for instances or classes)\n\t# NOTE: also, implement `__get__` to see if the object implements a given interface\n\tdef __get__(self, owner): # pylint:disable=redefined-builtin\t\"\"\"\n\t\tGet the interfaces for the given ``owner``\n\n\t\t:rtype: sequence of :py:class:`._DescriptorInterface`\n\t\t:"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\t# we get called from _lookup\n\t\ttry:\n\t\t\tobject.__iface_methods__\n\t\texcept AttributeError:\n\t\t\traise AttributeError(\"'This interface' object has no interface methods\")\n\t\ttry:\n\t\t\tobject.__iface_object__\n\t\texcept AttributeError:\n\t\t\traise AttributeError(\"'This object' object has no __iface_object__ set\")\n\t\t\n\t\tfor i in object.__iface_methods__:\n\t\t\ttry:\n\t\t\t\t__provides__ = object.__provides__[i]\n\t\t\texcept KeyError:\n\t\t\t\tif i in object.__provides__:\n\t\t\t\t\t# we don't want to depend on this,"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tiface = declarations.getattr(object, 'zopeInterface', None)\n\t\tif iface is not None:\n\t\t\treturn iface._toHelps()\n\ndef getInterfaces(*interfaces):\n\t\"\"\"\n\tGiven a sequence of `IDeclaration` *interfaces*, an iterable of one or more interfaces,\n\treturn one or more Interface instances.\n\t\"\"\"\n\treturn filter(lambda i: IDeclaration in i, interfaces)\t\n\nclass ObjectSpecification(Interface):\n\ttype = ObjectSpecifier\n\n\tdef _getInterfaces(self):\n\t\tyield self['type'].getInterfaces()"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\n@zope.interface.implementer.ISpecificationAdapter\nclass SpecificationAdapter(object):\n\t\"\"\"\n\tBase class for all specification implementations\n\t\"\"\"\n\tinterface = ISpecificationImplementation\n\n@zope.interface.implementer.interfaces.ISpecificationAdapter\nclass FactorySpecificationAdapter(SpecificationAdapter):\n\t\"\"\"\n\tInterface for factories which implement :class:`spec.operations.ISpecification.implants.ISpecificationAdapter`\n\t\"\"\"\n\tinterface = ISpecificationAdapter\n\"\"\"\n\tFactory for factories which provide :class:`zope.interface.interfaces.ISpecification."}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tpass\n\t#\n\t# Python 3 uses some of the function definitions, which are just methods\n\t# in the global namespace because the value being defined as the\n\t# declaration was declared for you.\n\tdef sameForInterface(*interfaces):\n\t\t\"\"\"\n\t\tReturn true if the specified interfaces were provided \n\t\tby the given interfaces, else return false.\n\n\t\t:param interfaces: one or more interfaces to compare\n\t\t\"\"\"\n\t\tresult = True\n\t\tfor i in interfaces:\n\t\t\tif i.providedBy(object):\n\t\t\t\tresult = result and i.providedBy(object.context"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\n##\t\tprint(\"----> Building minified classes -----\")\n\t\t\n\tminimums = min(len(classes), key = lambda klass: len(set(classes[klass])))\n\t\t\n\tfor klass in classes:\n\t\tfor s in klass.SibSpans:\t\n\t\t\tfor o in s.Bases:\n\t\t\t\tif klass.isMinimizable(o):\n\t\t\t\t\tminimums |= list(s.minimizable(o))\n\t\t\n\t\tfor l in klass.Leaves:\n\t\t\tfor m in l.Lessons:\n\t\t\t\tif klass.isMinimizable(m):"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tnewMin = {}\n\t\tnewMax = {}\n\t\tfor base in classes:\n\t\t\tfor i in range(len(base.classes)):\n\t\t\t\tnewMin[base.classes[i]] = min(base.classes[i], newMin.get( base.names[i] ) + base.classes[i])\n\t\t\t\tnewMax[base.classes[i]] = max(base.classes[i], newMax.get( base.names[i] ) + base.classes[i])\n\t\treturn newMin, newMax\n\t\n\t@staticmethod\t\n\tdef minimalRemain"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tminBases=[]\n\t\t#sort the clasions\n\t\tfor claz in classes:\n\t\t\tif(claz.hasMinAbs()):\n\t\t\t\tminBases.append(claz)\n\t\t\t\n\t\treturn minBases\n\t\t\n\tdef findCapsules(classes, minBases):\n\t\t'''\n\t\tTakes a list of base classes in a form\n\t\t\t(minMinRel, maxSizeAbs, maxSizeRel)\n\n\t\t\tReturns:\n\t\t\t\ta list of sorted base clasions\n\t\t'''\n\t\t#sort base clasions\n\t\tminBases=sorted(set(minBases)"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tbase_classes = reduce (\n\t\t\t\t\t\t\tlambda c1,c2: (c1.size()-c2.size())/2,\n\t\t\t\t\t\t\tclasses,\n\t\t\t\t\t\t\t1 )\n\t\t\t\n\t\tif len (base_classes) < 1 :\n\t\t\treturn []\n\n\t\tbase_ordered_min = list(set(itertools.chain([c[0] for c in base_classes],[1,-2,4*len(base_classes)]),1))\n\t\tbase_ordered = dict(set(itertools.chain([c[1] for c in base_classes],[1,-2,4*len"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tresult = {}\n\t\tfor i in range(len(classes)):\n\t\t\tif classes[i] > classes[i-1]:\n\t\t\t\tresult[i] = 1\n\t\t\telse:\n\t\t\t\tresult[i] = classes[i]\n\t\treturn result\n\n\tdef minimize(base):\t\t\"\"\"\n\t\tCalculate minimal equivalent (a and b) using a greedy algorithm\n\t\t@param base: list of items for which base (A,B) are the base items\n\t\t@return: one of two length lists. The lowest minimal level and the second level should be equal.\n\t\t\"\"\""}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\treturn reduce(lambda s,m: bases[1] * m, bases)\n\t#return reduce(lambda s,m: s[0]*m[0], bases[1:] + bases[0])\n\treturn reduce(lambda s,m: s[1]*m[1]*m[1] + bases[0], bases)\n\nclass Node:\t#Class to simplify all operations, such as simplifying a tree of a tree\n\tdef __init__(self,typeCode,name,parent,children,parents):\n\t\tself.typeCode = typeCode\n\t\tself"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tclassList = {}\n\t\tbases = list(classes)\n\t\ts = set(bases)\n\t\twhile bases:\n\t\t\ti = s\n\t\t\twhile i in bases and len(bases) > 0:\n\t\t\t\tj = bases.pop(i)\n\t\t\t\tif j not in classList:\n\t\t\t\t\tclassList[j] = len(bases) + 1\n\t\t\t\telse:\n\t\t\t\t\tclassList[j] = 1\n\t\treturn classList\n\t\n\tnewMaxCouplings = 0\n\t\t\t\t\n\tdef rebalance(classList):\t\n\t\t\n\t\trebalance = 0\n\t\t\t\n\t\twhile len("}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\n\n\tclass MinimalBaseValidator:\n\t\tdef __init__(self, classes):\n\t\t\tself.classes = classes \n\t\tdef isValid(self, obj):\n\t\t\tfor a in objsSet:\n\t\t\t\tfor l in a.bases:\t\t# Filter the bases\n\t\t\t\t\tif(l not in self.classes):\t\t# Filter disabled base\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif(a.isIn(l)):\t\t\t\t# The class is in not in the set\n\t\t\t\t\t\t# Make a copy of the base so that any missing bases are removed\n\t\t\t\t\t\tself.classes.add(l)\n\t\t\t\t\t\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tbases = tuple(t.__bases__() for t in classes if not t is object)\n\t\tif bases:\n\t\t\treturn bases\n\n\tsubclasses = class_bases(class_bases(object))\n\n\tnew = type(str(\"Meta\" + \", \".join(c.__name__ for c in subclasses)) +'s', baseClasses(classes))\n\n\tif new not in bases:\n\t\tclasses.append(new)\n\n\treturn bases\n\ndef all_bases(classes, baseClasses, allClasses):\n\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\n\n\tdef __cmp__(a, b):\n\t\treturn cmp(a[1], b[1]) == 0\n\n\n\tdef baseMinMax(c):\t\n\t\treturn sorted(c, key=natural_sort_key)\n\n# \n# The Python Imaging Library\n# $Id$\n#\n# GSGUI 2.5/3.0/PEP/GSGUI-2.5.0/\n#\n# D. Eppstein, July 2002\n#\n# Copyright (c) 2000-2002 by Secret Labs AB\n# Copyright (c"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn self._namesAndDescriptions_(all).values()\n\n\tdef namesAndDescriptions_( self, all=False ): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by a class.\t\"\"\"\n\t\treturn self._namesAndDescriptions_(all).items()\n\n\t# ----------------------------------------\n\t#\n\t# XML attribute parsing support\n\t#\n\n\tdef hasAttribute(self, name):\n\t\t\"\"\"\n\t\tCheck if a pycopia attribute exists.\n\n\t\tReturns a string indicating if attribute exists.\n\t\t\"\"\"\n\t\treturn hasattr(self, name"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn [attr for key, attr in self.__dict__.items() if not key.startswith('_')]\n\tdef get(self, name, iface=None): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn an attribute value.\n\tIf iface is not given, then the attribute is requested.\n\t\"\"\"\n\t\ttry:\n\t\t\tret = self.__dict__[name]\n\t\texcept KeyError:\n\t\t\tif self.__proxy is not None:\n\t\t\t\tmsg = '{0} has no attribute {1!r} -> {2!r}'.format(name, iface or self.__proxy"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn getattr(self._attributes, MNS + \"namesAndDescriptions\", []) + filter(lambda x:x.startswith(\"CF_\"), dir(self))\n\t\n\tdef getDescription(self): # pylint:disable=redefined-builtin\t\"\"\"\n\t\tReturn description of the interface, for error messages.\n\t\t@rtype: string\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self._descriptions[MS + \"description\"]\n\t\texcept KeyError:\n\t\t\treturn \"\"\n\t\t\n\tdef __dir__(self):\n\t\t\"Get a list of attributes defined in the interface itself.\"\n\t\treturn map(lambda x"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn self.getNamesAndDescriptions(all=all)\n\n\tdef lookupInterface(self, ifaces=all): # pylint:disable=unused-argument\n\t\t\"\"\"\n\t\tReturn the interface by interface name. If there is none, then return None\n\t\tso that InterfaceNameResolver will not be called for some interfaces.\n\n\t\t@param ifaces: a sequence of strings (for example, ['eth1', 'wlan0', 'eth1dev',..])\n\t\t@type  ifaces: sequence of strings\n\t\t@return: result of L{InterfaceNameResolver."}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn dict(self.__dict__).keys()\n\t# pylint:enable=redefined-builtin\n\t# pylint:enable=redefined-builtin\t# not really used anywhere\n\tdef __len__(self): \n\t\treturn len(self.keys())\n\tdef __iter__(self):\n\t\treturn iter(self.keys())\n\tdef keys(self): \n\t\treturn list(self).__iter__()\n\tdef __repr__(self): \n\t\treturn repr(self.__dict__)\n\t\t#raise AttributeError, 'len() failed'\n\tdef update(self, dict):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tret = self.__getattr__('namesAndDescriptions', [])\n\t\tret += self.__getattr__('namesAndDescriptionsAndDocstrings')\n\t\tret = self.__getattr__('namesAndDescriptionsAndSummaryStrings')\n\t\tfor name, desc in ret:\n\t\t\tret.update(name=name, desc=desc)\n\t\tret.update(self.__getattr__('allNamesAndDescriptions'))\n\t\treturn ret\n\n\tdef showNames(self):\n\t\tnamesAndDescriptions = self.__getattr__('namesAndDescriptionsAndSummaryStrings')\n\t\tif self.__descname:\n\t\t\tnamesAndDescription"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tresult = []\n\t\tdescs = self.descriptor.interfaces\n\t\tfor i in range(len(descs)):\n\t\t\tinterface = descs[i]\n\n\t\t\tresult.append(interface.identifier.name)\n\t\t\tresult.append(interface.description)\n\n\t\t\tif interface.hasDescendants():\n\t\t\t\tdesc = {\n\t\t\t\t\t'arguments': [],\n\t\t\t\t}\n\n\t\t\t\tfor d in descs[i].declarators:\n\t\t\t\t\tif d.isMethod():\n\t\t\t\t\t\tparam = self.createParameter('parameter', method=d.identifier.name)\n\t\t\t\t\telse:\n\t\t\t\t\t\tparam"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn map(\n\t\t\tlambda name: self.get(name, self.getAttribute(name)), \n\t\t\tself._AttributeNamesAndDescriptions(all=all)\n\t\t)\n\n\t@classmethod\n\tdef _AttributeNamesAndDescriptions(cls, all=False):\n\t\tresult = {}\n\t\tfor attrib, value in cls.__dict__.iteritems():\n\t\t\tresult[attrib] = value or \"\"\n\t\treturn result\n\n\tdef _Set(self, xml):\n\t\tself.__xml__ = xml\n\n\tdef _Create(self, xml):\n\t\telem = xml.createElement(self.__"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tret = list()\n\t\tif not all:\n\t\t\tfor d in self.typedefs():\n\t\t\t\tret.append(d.type)\n\n\t\tret.sort()\n\t\tret.reverse()\n\t\treturn ret\n\n\tdef output(self, prn=None):\n\t\tself.init_child_types()\n\n\t\t# Output type properties\n\t\tfor t in self.typedefs():\n\t\t\tt.handle_properties(self)\n\n\tdef typedefs(self):\n\t\t\"\"\"\n\t\tReturn list of top-level typedefs contained in this type object.\n\t\t\"\"\"\n\n\t\tret"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\toutput  = [] # [ str name, [ str description] ]\n\tnames = self.names()\n\tnamesStr = ';'.join(sorted(names))\n\tnamesList, descriptionsList = [], []\n\tfor name, description in names:\n\t\tnamesStr +='' + name # [ str name, [ str description ] ]\n\t\tdescriptionsList.append(description) # [ str description ]\n\t\tnamesList.extend(description.names()) # [ str name, [ str description ] ]\n\t# [ str description, [ [ [ name, description] ]...\n\tif all"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tif not all: return self.__names.keys()\n\t\treturn self.__names.values()\n\n\tdef names_and_values(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names and values defined by the interface. \t\n\t@return {str, list, enum.Enum, int, str}\n\n\t\"\"\"\n\tif not all: return self.__names.items()\n\t#\tpprint(dict(self.__names.items()), indent=2, width=60)\n\treturn [str(name) for name in self.__"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tif all: self.all_attributes\n\t\treturn self.attributes.keys()\n\tdef __getitem__(self, name):\n\t\t\"\"\"\n\t\tRetrieve an attribute value.\n\t\t:returns: The attribute value.\n\t\t:raises KeyError: Does not exist.\n\t\t\"\"\"\n\t\tif name in self.attributes:\n\t\t\treturn self.attributes[name]\n\t\treturn None\n\tdef __delitem__(self, name):\n\t\t\"\"\"\n\t\tRemove an attribute.\n\t\t:raises KeyError: Does not exist.\n\t\t\"\"\"\n\t\tself.attributes[name] = None\n\tdef __"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn _get(self.tk, 'names')\n\n\t# Tkinter 2 compatibility\n\tTk = name = names\n\nclass Frame(OldTcl):\n\t\"\"\"The Frame widget can be used to display text; it is\n\tconsidered a text widget when it is scrolled with a\n\tsmall window.  All data is treated as raw unicode.\n\t\"\"\"\n\n\t# XXX this has nothing to do with Frame, and is handled in the\n\t# synchronous Tcl command!\n\t# We need to cache this as IDLE handed off to CacheCtrl.SetWindow.\n\tdef __"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tif not all:\n\t\t\treturn tuple(self.__dict__.keys())\n\t\telse:\n\t\t\tret = []\n\t\t\tfor item in self:\n\t\t\t\t# Copy the value set.\n\t\t\t\tthis = item.__dict__.copy()\n\t\t\t\tthis[self.attribname] = True\n\t\t\t\tif hasattr(this[item], 'name') and this[item].name in ret:\n\t\t\t\t\tcontinue\n\t\t\t\tret.append(this[item].name)\n\t\t\t# Return a copy that can be modified to prevent changes in the\n\t\t\t# class' default getter, which is used to prevent automatic\n\t\t\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self.__dict__.__dict__.keys()\n\n\tdef is_compatible(self, other): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn whether the name is compatible with another interface.\n    This is needed, for example, to return True to add more complexity.\n\n    This can be done by replacing the previous_names member with\n    the extended_attributes list.\n    \n    Args:\n      other: interfaces, Interface\n\n    Returns:\n      boolean\n\n    \"\"\"\n    return type(self) == type(other) and \\\n           self.extended_attributes() == other.extended_attributes()\n#!/usr/"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn list(_accessors(self._object))\n\n\tdef __iter__(self): # pylint:disable=protected-access\n\t\t\"\"\"\n\tReturn an iterator providing the interface's attributes as a sequence. The\n\treturned sequence can be accessed using ``list.get(key)`` to retrieve the\n\tattribute's attributes using ``list`` directly.\n\t\n\tReturns: an iterator\n\t\"\"\"\n\t\treturn iter(self._object)\n\n\t__iter__ = names\n\n\tdef __reversed__(self): # pylint:disable=protected-access\n\t\t\"\"\"\n\tReturn an iterator providing the interface's attributes"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tif all:\n\t\t\treturn dir(self)\n\t\treturn dir(self.__class__)\n\n\tdef __dir__(self):\n\t\t\"Return a list of attribute names.\"\n\t\tresult=[]\n\t\tfor key in dir(self):\n\t\t\tif not key.startswith('__') and \\\n\t\t\t\tnot self.is_public(key):\n\t\t\t\tresult.append(key)\n\t\tresult.sort()\n\t\treturn result\n\t# some of our interfaces (e.g. DialogForm)\n\t# are exercised after the class is defined.\n\tdef __getitem__(self,"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn list(self.keys()) if all else list(self)\n\n\tdef values(self):\n\t\t\"\"\"The attribute values\"\"\"\n\t\treturn self.attributes()\n\n\tdef items(self):\n\t\t\"\"\"The attribute key-value pairs\"\"\"\n\t\treturn [(k, self[k]) for k in self.keys()]\n\n\tdef getNamedItem(self, name):\n\t\t\"\"\"\n\t\tRetrieve a named (symbol, class or function) item.\n\n\t\t:param name: The name of the item to retrieve (symbol, class, \n\t\t\tfunction or lambda)\n\t\t:type name:"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\ttry:\n\t\t\treturn tuple(\n\t\t\t\t\t(n, getattr(self, n))\n\t\t\t\t\tfor n in dir(self) if not n.startswith('_') and not n.startswith(\"__\")\n\t\t\t\t\t\tif not n.startswith(\"_\")\n\t\t\t\t\t)\n\t\t\t)\n\t\texcept Exception as e:\n\t\t\traise e\n\n\tdef __iter__(self):\n\t\treturn iter(self.__attrs__)\n\n\tdef __getattr__(self, name):\n\t\tif name == \"_data\":\n\t\t\treturn self._data\n\t\traise AttributeError(\"Access to data not implemented\")\n\n\tdef __setattr__(self, name"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\ttry:\n\t\t\tresult = self._name\n\t\texcept COMError:\n\t\t\tif all:\n\t\t\t\traise\n\t\t\tresult = []\n\t\treturn result\n\t#---- end of name accessors\n\n\tdef get_argument(self, name, direction=None):\n\t\ttry:\n\t\t\tdoc = self._element.queryDescriptionFull(direction=direction)\n\t\t\tdoc = doc.getNextElement()\n\t\t\targs = doc.getArguments()\n\t\t\tif direction:\n\t\t\t\targs = args.cast(type(arguments['direction']), DocumentElement)\n\t\t\targuments = doc.listArgs()\n\t\t"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tassert isinstance(sequence, _seqtypes),\trepr(sequence)\n\t\tif output is not None:\n\t\t\tassert output.startswith(\"<declarations>\")\t\t\t\t\t# \"implements\" keyword\n\t\t\toutput = output[8:]\n\t\t\toutput = output.replace('<', '&lt;').replace('>', '&gt;')\n\t\t\toutput = output.replace(' ', '&nbsp;&#x2c;').replace('\\n', '<br />')\n\t\toutput = output.replace('>', '&gt;')\n\t\targs = []\n\t\tfor arg in sequence:\n\t\t\tfor impl in arg:\n\t\t\t\tif impl is None:"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t \n\tret = []\n\t# First deal with the various 'args'\n\tfor item in sequence:\n\t\tif hasattr(item, 'args'): ret.extend(item.args) # copy over all entries\n\t\telse: ret.append(item)\n\n\t# Now get the number of arguments used if we are the first one\n\tif len(ret) == 0: return len(sequence)\n\n\tif len(ret) == 1: return ret[0]\n\telse:\t\t return tuple(ret)\n\n\ndef resolvedependencies(deps, dependencies): # FIXME: merge with deps"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\t    return output\n\nif sys.platform == 'java':\n\n    #-------------------------------------------------------------------------\n    #\n    #   Globals, classes, functions\n    #\n    #-------------------------------------------------------------------------\nelse:\n\t#------------------------------------------------------------------------------\n\n    #-------------------------------------------------------------------------\n    #\n    #  Functions, methods\n    #\n    #-------------------------------------------------------------------------\n\n    #-------------------------------------------------------------------------\n    #\n    #  Constructor, used only by subclasses that do not register\n    #  with the main() function\n\n#=======================================================================\n\n#\n#  Class, method, and instance creation\n#\n#-----------------------------------------------------------------------\n\nclass Class:\n    \"\"\" This is the base class for all python classes\"\"\"\n    if __debug__:\n\t\"\"\" Define"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n_default_value = object()\ndef _tostr(name, value):\treturn _default_value\n\ndef _toset(type):\t\"Make sure input parameters are strings\"\n\tglobal _value\n\t_value = type\n\n\nclass BaseType(BaseType):\n\tdef setvalue(self, val):\n\t\traise ValueError(\"this name must be string\")\n\tdef _get_maxval(self):\n\t\treturn None\n\tdef _get_minval(self):\n\t\treturn None\n\tdef coerce(self, val):\n\t\traise ValueError(\"this"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\ndef _flatten_list(sequence):\t\t# For dictionaries\n\t\"\"\"Recursive flatten nested lists for subsequences\n\nFlattens all sub-sequences to a single element, keeping the lists that\nare supposed to have been used, and recursing into sub-sequence elements when\nthe dictionary item is encountered. \"\"\"\n\toutput = []\n\tfor element in _flatten_list(sequence):\n\t\toutput.append(_flatten(element))\n\treturn output\n\ndef flatten(*sequence):\n\t\"\"\"\n\tFlatten nested lists and strings into single elements.\n\n\tLike flatten, but also takes a list"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n    from collections import defaultdict\n\ttry:\n\t\treturn sequence[:]\n\texcept AttributeError:\n\t\tpass\n\t\n\ttry:\n\t\tseq = list(sequence)\n\t\tseq[0] = None\n\t\treturn seq\n\texcept TypeError:\n\t\ttry:\n\t\t\titems = enumerate(sequence)\n\t\t\titems.append(sequence[0])\n\t\t\treturn tuple(items) \n\t\texcept TypeError:\n\t\t\traise TypeError(\"Can't flatten sequence\")\n\t\t\ndef _parseargs(args, output=None):\t\n\t\"\"\"\n\tParse declaration arguments\n\nThe declaration arguments have to be formatted to:"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\t\n\toutput.write(declsarser.deindent(sequence))\n\n\tfor declsard in sequence:\n\t\targs = []\n\t\tfor x, n in declsard.getspecs():\n\t\t\tspec, v = x\n\t\t\tif v!= None: args.append(spec + '/' + n)\n\t\toutput.write(declsarser.expand(args))\n\t\toutput.write(declsarser.deindent(declsard.defaultspec))\n\n\treturn output\nfrom django.contrib.auth.models import User\nfrom.models import *\n\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n        if output is None:\n                output = sys.stdout\n            \t\t\t\t\t\t\t\t\t\t\t\t\t\n        if not hasattr(sequence, \"__len__\"):\n                # no len() function defined so return the input sequence unchanged\n                return sequence\n        elif not sequence:\n                # scalar sequence, i.e. an untyped sequence\n                return sequence\n\n        # sequence already expanded, replace if one-off.\n            if \"extend\" in sequence.__dict__:\n                return replace(sequence, output)\n                      \n            if type(__builtins__).__name__ in _knownmods:\n                return sequence \n\n            sequences = []\n            for member in sequence:  # iterate each member ("}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\n\ti = 0\n\tfor substr, args in sequence:\n\t\tif not isinstance(args, tuple):\n\t\t\traise Exception(\"invalid normalize declaration for %s: %s\" % (repr(substr), repr(args)))\n\t\t\n\t\tif len(args) > 1:\n\t\t\ti = i + 1\n\t\t\targs = _normalizeargs(args, output)\n\t\t\t\n\t\t\tif len(args) == 1: pass\n\t\t\telse:\n\t\t\t\targs = _normalizeargs(args[0], output)\n\t\t\t\tif not args: raise Exception(\"invalid normalize declaration for %s: no input or"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tif isinstance(sequence, Declarions):\n\t\t\tsequence.reverse()\n\t\telif isinstance(sequence, dict):\n\t\t\t_normalizeargs(sequence.items(), output)\n\t\telif isinstance(sequence, tuple):\n\t\t\t_normalizeargs(sequence, output)\n\t\telif isinstance(sequence, basestring):\n\t\t\t_normalizeargs(sequence, output)\n\t\telse:\n\t\t\tassert False, \"Unhandled parameter: \" + str(sequence)\n\telse:\n\t\tassert False, \"Unhandled parameter: \" + repr(sequence)\n\t\t#raise TypeError(\"Unhandled parameter, \" +"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn None\n\n\ndef _have(x):\n\t\"\"\"\n\tGiven x being acceptable.\n\t\n\tX may be None or of type x.\n\t\"\"\"\n\tif type(x) == type(''):\n\t\treturn x.lower()\n\treturn None\n\n# NOTE: this must NOT evaluate to be too costly if it is used before\n# running the optimizers. If the optimizer is running from the command line\n# then this can generate an unpredictable situation.\n\n\ndef _get_optimizer(optimize=optimize, randomize=random, x0=None, *args"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t       \n\toptimizations=[]\n\tif _c_optimizations_available.unavailable:\n\t\t# TODO: check for unavailable flags\n\t\tif 0 and optimize.and_return(lambda x: _c_optimizations_available()[0](x))==impossible:\n\t\t\traise ImportError(\"Couldn't find the optimizations module of C\");\n\t\telif optimize.and_return(lambda x: _c_optimizations_available()[1](x))!=impossible:\n\t\t\traise ImportError(\"Couldn't find the optimizations module of C\");\n\n\tfor mod in _get_optimizations_modules"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\t# XXX: is this inefficient\n        return True # C optimizations are always required, this is safe\n    \n    def _use_c_optimizations():\t\t#C optimizations is always required if they\n        return True\n\ndef _optimize_available():\n    \"\"\"\n    Return true if optimization modules are being loaded.\n\nWARNING: only available with the gmpe_c_optimized\nalgorithm. It should be called by gmpe.py.\n\"\"\"\n    global _optimizations\n    if not _optimizations_available():\t\t# gmpe_c_optimized not loaded yet\n        raise ImportError,"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\tif not _pycbc_available:\n\t\t\traise ImportError('In order to use the C version of the optimizer '\n\t\t\t\t\t'from scipy.optimize.optimize._c_optimizations',\n\t\t\t\t\t'Do you have TensorFlow installed?')\n\t\tfor name in PyCBC.__global__._optimizer_names:\n\t\t\tif _pycbc_available:\n\t\t\t\traise ImportError('We can not find the %r optimizer' % name)\n\treturn PyCBC.__global__._optimizer_names\n\n\ndef set_verbosity_and_verbosity(*args):\n\t\"\"\"\n\tParse or set verbosity and verbosity flags to"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\tif 'optimizations' in sys.modules:\n\t\t\treturn\n\telse:\n\t\tfor module in sys.modules['pyopsim']:\n\t\t\tif module.find('optimizations') > -1:\n\t\t\t\treturn True\n\t\t\t\t\ndef test():\n\t\"\"\"Test with PyOpSim's optimizations.\"\"\"\n\t# set up default module values\n\tif _c_optimizations_available():\n\t\treturn\n\n\tsim = PyOpSim()\n\tfor name in sim.list_optimizations():\n\t\tprint name\n\t\tsim.register_opt(pyop.OptimizationObjective(name,"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn False\n\treturn get_software_root('c2pycc') is not None\n\n\ndef c_optimizations_needed():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n        return False\n\tassert get_software_root('c2pycc') is not None\n\treturn True\n\n_optimizations_available = False\n_c_optimizations_available = False\t#"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\ttry:\n\t\t\treturn _optimizations_available.index(os.extsep + 'optimizers') + 1\n\t\texcept ValueError:\n\t\t\t_optimizations_available.append(sys.implementation.name)\n\t\t\treturn False\n\ndef set_optimizations_available(optimizations, warning=\"\"):\n\t\"\"\"Sets the list of available optimizations.\"\"\"\n\n\tglobal _optimizations_available\n\tif optimizations == None:\n\t\treturn False\n\n\tnew_optimizers = []\n\n\tfor optimizations_available in optimizations:\n\t\tif optimizations_available <= 0:\n\t\t\tcontinue"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n_optimizations = []\n\ndef optimize():\n    global _optimizations\n    if not _optimizations:\n        global _c\n        _optimizations.append([\"_c\", None])\n    else:\t\n        import cProfile\n\t\n        from. import profiler\n\n        try:\n            profiler.enable()\n            cProfile.run('result = OptimizeResult(a,b,optimize)', globals(), {\"a\" : 1,'b' : None})\n            output = _c.Optimize(True)\n            print(output[1])\n        except Exception as e:\n            import ipdb;ipdb.set_trace()\n       "}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn sys.flags.optimize\n\ndef _check_path_limit(path):\n\tif len(path) > 100:\n\t\t# Path is too long, truncate it.\n\t\tpath = path[:50] + \"...\"\n\treturn path\n\t\ndef _find_exe(program_name, searchpaths=None):\n\timport os\n\timport re\n\timport subprocess\n\n\texts = ['.exe', '.bat']\n\tif searchpaths:\n\t\tfor path in searchpaths:\n\t\t\tfor ext in exts:\n\t\t\t\texe_name = \"%s%s%s\""}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n# XXX backward compatibility\nfrom numpy.testing import (assert_equal, run_module_suite,\n        with_seed)\n\ntry:\n    from pyamg.util.linalg import dtrtrs, dsolve, max_step_size\nexcept ImportError:\n    print \"pyamg is not available, no DTRTRSLS, and dtrtrs or dtrtrs_fft for these tests.\"\n    import sys\n    sys.exit(1)\n\n\n# FIXME: the module is not yet ready\n# This is a list of the available optimizers.\nfrom theano."}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tassert not is_pypy:\n\t\t\treturn _c_optimizations\n\t\treturn False\n\n\tif _is_pypy:\n\t\t# The _get_optimizations function would cause this for a PyPy\n\t\t# if using a C C API that already has at least been explicitly\n\t\t# enabled.  This would allow us to use the same logic, regardless of\n\t\t# how its set of optimizations are used.\n\t\t# To support the `PYPY_CPYTHON` environment variable or the\n\t\t# corresponding python executable to be used, we check whether\n\t\t# `build_in"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tc_optimizations = 'PYPY' in os.environ\n\t\n\tc_optimizations |= os.environ.get(C.PYPY_MAJOR_VERSION, None)!= None\n\n\treturn c_optimizations\n\ndef _use_c_impl():\n\t\"\"\"\n\tIf we're on PyPy and we're calling a Python implementation of a\n\tC extension, use that. This has been deprecated to be removed and replaced\n\twith a specific version of the pure C extension.\n\t\"\"\"\n\timplementation = C.PURE_PYTHON\n\t\n\ttry:\n\t\timplementation = implementation"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn os.environ.get('PURE_PYTHON', '') == '1'\n\n\tenv_vars = {\n\t\t'__PURE_PYTHON': _is_pypy_in_env(\"2\"),\n\t\t'PURE_PYTHON': sys.executable,\n\t\t# For coverage, do not provide the __file__ attribute for the case,\n\t\t# use the C implementation instead.\n\t\t'PYTHONVERSION': '2',\n\t}\n\n\treturn _use_c_impl\n\n\ndef _skip_if_file_is_skipped():\n\tfor filename_name in os.listdir"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn _use_c_impl == 'Pyc' # optimization is on\n\n\treturn _use_c_impl\n\ndef _have_c_runtime_libs():\n\t\"\"\"\n\tReturn if we have C runtime libs or not, i.e. that we've checked already.\n\n\tThis checks when we're on CPython, as the check above is the\n\tmaximum that this does _not_ allow compiled code to detect MSVC.\n\t\"\"\"\n\tenv = os.environ.copy()\n\n\tif env.get('CC', '').find('clang') >= 0:\n\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t                                                       \t#\n\tif _is_pypy:\n\t\tif _is_jython:\n\t\t\t# CPython-specific\n\t\t\treturn True\n\t\telse:\n\t\t\t# Jython-specific\n\t\t\treturn False\n\telif not _is_jython:\n\t\treturn False\t\t\t# PYPY\n\telif sys.platform[:6] == 'java' and not _is_jython:\n\t\treturn False\t\t# Jython\n\telif _is_pypy:\n\t\t# CPython-specific...\n\t\treturn sys.executable.lower()=='pypy'\n\telse:\n\t\tif sys.platform[:"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tpython_value = os.environ.get(_PYTHON)\n\t\tif python_value and os.environ.get(_PY3):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\treturn _should_attempt_c_optimizations\n\ndef _find_cpython_path():\n\t\"\"\"\n\tFind C++, F77 or AVL for the system path currently running on the\n\tmachine. Returns C, F, P etc. \n\t\"\"\"\n\timport commands\n\tgcc = commands.getoutput('/usr/bin/gcc')\n\t# For compatibility with 2."}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\t# XXX: we might want to pass this through the configuraiton; for now we\n\t\t# need env_c_var outside of the normal configuration, or maybe we could.\n\t\treturn bool(_use_c_impl)\n\n\tif six.PY2:\n\t\t# python 2 returns a string but we need to encode it ourselves.\n\t\tp_str = six.binary_type(sys.executable, sys.getfilesystemencoding(),'replace')\n\t\treturn _c_optimizations_str()\n\n\tc_supported_python = False\n\tuse_c_impl = not"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tfrom distutils.sysconfig import get_config_var\n\t\treturn get_config_var(\"PURE_PYTHON\")\n\treturn _use_c_impl is not None\n\ndef _use_c_impl(debuglevel=None):\n\tassert _possible_ctypes\n\tdebuglevel = debuglevel or 6\n\tif _possible_ctypes is None:\n\t\tdebuglevel = 7\n\telse:\n\t\tdebuglevel = int(debuglevel)\n\tif debuglevel == 7:\n\t\tdebugmode = True\n\telif debuglevel == 8:\n\t\tdebugmode = True\n\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif os.environ.get(\"PURE_PYTHON\") is not None:\n\t\t\treturn \"pypy\"\n\t\tif config.safe_env:\n\t\t\treturn \"__pypy__\"\n\t\treturn (\"python\" if config.safe_env else \"pypy\")\n\t\t\"\"\"\n\n\tfrom os import (\n\t\tname as _name,\n\t\tlink as _link,\n\t\tlink_tree as _link_tree,\n\t\tO_WRONLY,\n\t\tO_RDWR,\n\t)\n\n\tvalid_modes = (\n\t\tW_NO_COMPRESSION,\n\t\tW_C_NO_"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif not is_py3k and _py3k:\n\t\t\treturn True\n\n\t\treturn _should_do_c_optimizations()\n\n\treturn _should_attempt_c_optimizations() or \\\n\t\tconfig.env.HAVE_PYTHON_SETUP and \\\n\t\tnot config.options.no_site_packages\n\ndef _should_use_c_optimizations():\n\tif not _have_run_2to3_and_dont_warned_about_error():\n\t\twarn(\"The _should_use_c_optimizations() step for the Python installation"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tif _g_optimizations_ignored._is_C_noC():\n\t\t\t_g_optimizations_ignored()\t# The default behavior: do nothing\n\n\ttry:\n\t\t_g_optimizations_required.remove(_k_f_c)\n\t\t_g_optimizations_exclude.remove(_k_g_c)\n\texcept ValueError:\n\t\tpass\n\ndef _c_optimizations_available(e):\n\n\tm = \"unknown\"\n\tif hasattr(e, \"__path__\"): m=e.__path__\n\tif m==\"\": pass\n"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tpass\n\n\ndef _get_global_optimizations():\n\treturn _c_optimizations[0]\n\n\ndef _get_function_optimization():\n\n\tdef function_optimization(g_x):\n\t\t\"\"\"\n\t\tThis function takes the x vector and returns a Python function\n\t\tto be used in the optimization.\n\n\t\tParameters\n\t\t----------\n\t\tg_x : vector\n\t\t\t1-d vector\n\n\t\tReturns\n\t\t-------\n\t\ta function\n\t\t\"\"\"\n\t\ta = 2*a\n\n\t\t#TODO: remove this when y_min is 0\n\t\treturn lambda x:"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\traise NotImplementedError ()\n\treturn _c_optimizations_required()\n\n\ndef _c_optimizer_for(op):\n\t\"\"\"\n\tDecorator to create a function for an op. For now, just create a function\n\tfor the op, then do a few more.\n\n\tIn the case of a subclass to store the op, we can override this as needed.\n\n\tWe need to do the decorator again.\n\n\tThis allows us to do the decorator at least once because the op is also a function\n\twith or without arguments, but this can be useful in situations that might not\n\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\n\tdef execute(self):\n\t\t\"\"\"\n\t\tA sub-class should return.  Override for sub-class support.\n\t\t\"\"\"\n\t\treturn False\n\n\tdef get_name(self):\n\t\t\"\"\"\n\t\tOverridable implementation of the 'name' property to ensure backwards compatibility.\n\t\t\"\"\"\n\t\treturn str(self.name)\n\n\tdef make_inputs(self):\n\n\t\traise Exception, str, \"Subclasses should implement this\"\n\n\tdef add_install_data(self):\n\t\t\"\"\"\n\t\tThis function will be called to prior to installing a new"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn -1\n\n\t# ---------------------------------------------------------------------------\n\t# Get the available optimizations for which any action has been\n\t# overwritten.\n\tdef _c_optimizations_changed(self):\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn -1\n\n\t# ---------------------------------------------------------------------------\n\t# Get the available optimizations for which any action has been modified.\n\tdef _c_optimizations_added(self, count=1):\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn -1\n\n\t# ---------------------------------------------------------------------------"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn getattr(self, '_c_optimizations')\n\n\tdef _c_optimized_methods_allowed(self):\t\"\"\"\n\tMethod for optimizations for optimizations returned by :meth:`solve` and :meth:`simplify`.  \n\n\tThe default method (`self.fmin`) will not have the option `fopt`, and may be overwritten by the \n\tmethods in a derived class.\n\n\t.. note::\n\t\tIt is set to ``False`` before :meth:`optimize` and :meth:`simplify` can be called \n\t\tmanually and not directly.\n\t\t\"\"\"\n\t\treturn"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\traise NotImplementedError()\n\n\n# =============================================================================\n# Standard __repr__, and long doc-strings\n# =============================================================================\n\ndef __repr__():\n\t\"\"\"\n\tA concise textual representation of the object. \n\t\n\tUsage::\n\n\t\tdef _c_optimizations_only():\n\t\t\tprint 'Hello World!'\n\t\t\treturn 'Hello'\n\t\t\tprint\n\n\t\t_v_ optimizations = get_config().get(_c_optimizations_only())\n\t\tpprint(list(_v_ optimizations.items()))\n\t\"\"\"\n\trep = []\n\tif not hasattr(PrebuiltDatabase, '_"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tif not _is_empty_container(self.__container):\n\t\t\treturn\n\n\t\tfor opt in self.__optimizations:\n\t\t\tif opt not in self.__container.optimizations or opt!= self:\n\t\t\t\tself.__container.optimizations[opt] = self.__optimizations[opt]\n\n\tdef optimizations(self):\n\t\t\"\"\"\n\t\tRetrieves the optimizations contained within this instance, or\n\t\t`None` if there are no optimizations to retrieve. Note that such\n\t\toperations do not affect the values contained in the result, but only that\n\t\tthe optimizations of the current instance"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tyield\n\t_c_optimizations_required()\n\ndef _optimize_for(ops, verbose=False):\n\t\"\"\"\n\tOptimize the program. Returns (next_state, symbol) triple\n\ttuples, where the first item is a dictionary of\n\t:py:class:tuple giving symbols that may or may not be updated.\n\t\n\tAfter a successful optimization, we return a dict of\n\tsymbol:op\n\t\"\"\"\n\top = ops[0]\n\tsymbols = op.symbols if verbose else []\n\treturn {\n\t\t'model': op,\n\t\t'"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tfor i in range(3):\n\t\t\tyield i, i \t\t\t\n\t\t\tyield i, i\n\n\tdef _c_optimizations_provided_by_previous_evaluation_step():\n\t\traise ValueError, \"this call should not be called by previous evaluation step\"\n\n\tdef _c_optimizations_requested_by_next_evaluation_step():\n\t\traise TypeError, \"this call should not be called by next evaluation step\"\n\n\tdef _c_optimizations_provided_by_first_evaluation_step():\n\t\traise TypeError, \"this call should not be called by first evaluation"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn not sys.flags.optimize # i.e.: skip on numpy.distutils\n\n# The following code is written for python 2.7.\n\n# Copyright (c) 2001-2004 Twisted Matrix Laboratories.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\ts = set(inspect.signature(x[0]).parameters)\n\t\tc_methods = ['__lt__', '__gt__', '__le__', '__ge__']\n\t\tc_builtin = [x for x in s if x.startswith('__')]\n\t\tc_builtin_method = [x for x in s if x not in c_methods]\n\t\tif len(c_builtin) and not c_builtin_method:\n\t\t\treturn bool\n\t\treturn c_builtin\n\telse:\n\t\treturn True\n\ndef _get_c_flags_from_flake8():\n\t\"\"\""}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn conf.env.C_OPTIMIZATIONS_REQUIRED == \"yes\"\n\n# Whether to skip the warning about certain functions.\n_c_warn_once_outside_functions = ConfigSet(\"config.env.C_WARN_ON_WARN_ON_WRITABLE\",\n\t\t\t\t\t\t\t\t\t\t\"bool\",\n\t\t\t\t\t\t\t\t\t\t\"bool\",\n\t\t\t\t\t\t\t\t\t\t\"env.warnings_flags_on\",\n\t\t\t\t\t\t\t\t\t\tFalse,\n\t\t\t\t\t\t\t\t\t\t\"env.warn_only_current_thread\",\n\t\t\t\t\t\t\t\t\t\t\"env.error_on_warnings_off\",\n\t\t\t\t\t\t\t\t\t\tFalse,\n\t\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tfor p in dir(PURE_PY):\n\t\t\tif p.startswith(\"PURE_\") and p.endswith(\"_opt\"):\n\t\t\t\treturn True\n\n\treturn None\n\nclass CLinker(object):\n\t\"\"\"\n\tLinking a C library to Python\n\tWe use the following wrapper class to modify C linkers::\n\n\t\t_c_functype = CFUNCTYPE(c_int, c_char_p, c_char_p, c_int, c_void_p)\n\n\twe need to define a function object called c_functype_p:\n\n\t\tdef _"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn _c_optimizations_required\n\n##class _C(_Verbose):\n##\t\"\"\"\n##\t\t@brief\tThis object is returned by the ``PURE_DLL`` initialization function.\n##\t\t@note\t\tThis constructor is only for internal use only.\n##\t\t\"\"\"\n##\tpass\n\t#def _cleanup(): pass\n\t#def _exception_warning(self): pass\n\t#def _verbose(*args): pass\n\t#def _raise(*args): pass\n\t#def _info(*args): pass\n\t#def _def_raise"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tif not (get_numargs() >= 2 and PURE_PYTHON):\n\t\t\twarnings.warn('c_optimizations_required has changed from the C compiler version to '\n\t\t\t\t\t'2.0, you should use the optimization from the parent folder.',\n\t\t\t\t\tDeprecationWarning, stacklevel=2)\n\t\tglobal PY3\n\t\tif PY3:\n\t\t\tBUILTIN_MOD = 'builtin'\n\t\t\tPURE_PY = 'PY'\n\t\t\tdef tmp_python_c_optimizations(*pargs, **kargs):\n\t\t\t\ttmp = None\n\t\t\t\tfor arg in pargs:\n\t\t\t\t\tif"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn \"NO\"\n\nclass Option(RegisterOption):\n\n\tdef compute_result_annotation(self, context, **options):\n\t\t# If C++ was used the user defined options are all parsed and\n\t\t# are parsed using the options. Otherwise we prefer the value of\n\t\t# the C++ default. If options['cpp_optimize'] were specified,\n\t\t# the result values used will be stored in options['cpp_optimize'].\n\t\tpass\n\nclass OptionList(list):\n\n\tdef __init__(self, *args, **kwargs):\n\t\tlist.__init__(self,"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tif callable(c_optimizations):\n\t\t\treturn c_optimizations(\n\t\t\t\t'c' in globals(),\n\t\t\t\t_get_c_optimizations_method_names\n\t\t\t)\n\telse:\n\t\treturn _get_c_optimizations_method_names['c']\n\ndef _c_optimizations_required_str():\n\treturn ','.join(_c_optimizations_required())\n\ndef _get_c_optimizations_method_names(method_name_list):\n\t# Returns a map with the unique method name as the keys and the method\n\t"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn \"0\"\n\n\t# ----------------------------------------------------------------------\n\tdef _use_c_wrapper():\n\t\t\"\"\"\n\t\tUse C API wrapper.\n\n\t\tThe Python wrapper uses the C API that was used by PyDOE (https://pyDOE.sourceforge.net/) to compute optimal values.\n\t\t\"\"\"\n\t\timpl = [\"c_api\"]\n\t\tresult = call(*impl)\n\t\tmsg = \"C API version 5.1.3 is required and only when C was used.\"\n\t\tassert result[0] == msg, msg\n\treturn _use_c_wrapper\n\n################\n##"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn getattr(Options.options, 'C_OPTIMIZE_REQUIRED', 0)\n\n@_use_c_impl\n@_c_optimizations_needed()\ndef _use_c_find_in_env(self):\n\tenv = self.env\n\tuse = Options.options.find_program(env, self.path)\n\tfound_include_dirs = []\n\tif Options.options.find_include_dir:\n\t\tfound_include_dirs.extend(Options.options.find_include_dir)\n\tif Options.options.use_listing"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t    \n\t    \n\t# set initial values, if this is the first time initializing\n\tif self.initial_value == None:\n\t\tself.initial_value = numpy.zeros(self.get_shape())\n\t\tfor i in range(self.get_shape()[0]):\n\t\t\tself.initial_value[i] = initial_value[i]\n\t\tself.init_bins = [[numpy.NaN for i in xrange(self.get_shape()[0])] for i in xrange(self.get_shape()[1])]  \n\t\t\nclass histogram_model:\n\t\"\"\"\n"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.set()\n\t\t\n\tdef make_bins(self,bins):\n\t\t\"\"\"\n\t\tCreate a tuple, where bins is 1-D sequences of values from the\n\t\tstandard normal distribution.\n\t\t\"\"\"\n\t\tbins = numpy.asarray(bins)\n\t\tif len(bins.shape) == 1:\n\t\t\tbins_bins = numpy.ones((len(self.bins_values),2))-bins**2\n\t\telse:\n\t\t\tbins_bins = numpy.asarray(bins).flatten()/bins.max()\n\t\tbins_bins.shape = (len(self.bins_"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.__dict__.clear()\n\n\tdef _reinit(self):\n\t\tself.bins = None\n\n\tdef set_bins(self, bins):\n\t\t\"\"\"Set or reset the histogram bins (if given).\"\"\"\n\t\tif bins!= None:\n\t\t\tself.bins = bins\n\t\tself.__dict__.update(self.__dict__)\n\n\tdef set_initial_value(self, initial_value):\t#FIXME\n\t\tif self.initial_value is not None and initial_value!= '':\n\t\t\tself.bins = {'initial_value':initialValue, 'current_value':self."}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.histogram = defaultdict(list)\n\t\tself.weights = defaultdict(list)\n\t\tself.initial_value = 0.0\n\t\tself.make_bins = np.linspace(\n\t\t\t10**0.5,\n\t\t\t10**2.1,\n\t\t\t31\n\t)\n\t\tself.make_bins_from_weights()\n\t\tself.init_weights()\n\tdef initial_value(self):\n\t\treturn self.initial_value\n\t\nclass MRHistogram(object):\n\t\"\"\"The MR histogram, with associated value per edge.\"\"\"\n\tdef __"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.__dict__[\"_bins\"] = BinDict(self._initial_value)\n\ndef run(argv=None):\n\t\"\"\"\n\tUse the main program as a library.\n\t\"\"\"\n\tglobal _options\n\t\n\timport os, tempfile\n\timport commands\n\timport numpy as np\n\t\n\t\n\t# Check for path\n\tif (not argv and len(options.input_files) > 0) or (not argv and len(options.output_files) > 0):\n\t\tprint \"You must provide a directory with the input data!\"\n\t\treturn -1\n\t\n\t# Get"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.bins[-1][1] = 0\n\tself.bins[0][1] = 0\n\tself.__reset_histogram()\t\t\n\tif self.update() == 0:\n\t\tprint(\"Error: can't compute histogram.\")\n\t\treturn\n\n\tif self.finalised == 0:\n\t\tbins = self.bins\n\t\tbin_length = len(bins)\n\t\tfinal_hist = [0.0]*bin_length\n\t\ti = 0\n\t\twhile i < len(bins):\n\t\t\tfinal_hist[i] = bins[i]/bin_length\n\t\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tpass\n\n\tdef __init__(self, *argl, **argd):\n\t\tsuper(Histogram, self).__init__(*argl, **argd)\n\t\tself.initial_value = self._get_vals_from_args(self.argl)\n\t\tself.make_bins()\n\n# =============================================================================\n# =============================================================================\n\nclass Hist(Element):\n    \"\"\"Represents a histogram.\n    >>> hist = Hist()\n    >>> hion = Hist(title='My histogram')  # 'My histogram'\n    >>> zetta = Hist(title='Michael\\'s t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\twith self:\n\t\t\tself._histogram.value = np.zeros_like(self._data.time)\n\t\t\tself._bins = self._make_bins()\n\t\t\t\n\t\n\tdef _make_bins(self):\n\t\t\"\"\"\n\t\tPreliminaries that only a time bin can be initialised.\n\t\t\"\"\"\n\t\tif not self._initialised:\n\t\t\treturn np.zeros_like(self._data.time, dtype=np.int32)\n\t\tif not hasattr(self, '_initialised'):\n\t\t\tif self.verbose: print \"Initialising histogram\"\n\t\t\tself._"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tassert self.state\n\t\tassert self.init\n\t\tif self.bins is None:\n\t\t\tself.bins = []\n\t\telse:\n\t\t\tassert isinstance(self.bins, list)\n\n\tdef to_dict(self):\treturn {key: value for key, value in self.bins}\n\n\tdef __call__(self, state):\t\t# Initialize the history.\n\t\tinit = state.hist.setdefault(self.init, self.init).to_dict()\n\t\tbins = [h * m / self.bins[0].sum() for m in self.bins"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.result[\"hist\"] = dict()\t\t# { <label, min, max> : [[min,max],...],...}\n\t\tself.result[\"label\"] = dict()\n\t\tself.result[\"min\"] = 0\n\t\tself.result[\"max\"] = 0\n\n\t#############\n\t# PROPERTIES #\n\t#############\n\n\t#\n\t# Initialize (in the superclass) or reset the histogram. A new\n\t# histogram is created if necessary, and then reinitializes it if it\n\t# is not.\n\t#\n\tdef _set_"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.graph.to_csv(sep, header, self)\n\n\t\"\"\"\n\t.. autoclass:: Lengtha \n\t:members:\n\t- AbstractLine\n\t\"\"\"\n\t#: The maximum number of nodes\n\tmax_no=None\n\t#: Minimum number of nodes\n\tmin_no=None\n\n\t#: the optional comment string\n\tcomment=\"\"\n\n\t#: the number of nodes\n\tn=0\n\t#: the number of vertices\n\td=0\n\n\t#: the edge list/adjacency matrix\n\te=None\n\n\t#: the value stored in the :attr"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\timport csv\n\t\twriter = csv.writer(self, delimiter=separator)\n\t\tfor i, p in enumerate(self.points):\n\t\t\tif ( (hasattr(p, \"X\") and isinstance(p.X, str)) and\n\t\t\t\tself.X == p.X ): continue\n\t\t\tif ( (hasattr(p, \"Y\") and isinstance(p.Y, str)) and\n\t\t\t\tself.Y == p.Y ): continue\n\t\t\tif i: writer.writerow([i])\n\t\t\tif header: self.header.append(i)\n\t\t\telse: self.rows."}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\t# pylint: disable=W0221\n\n\t\tfor point in self:\n\t\t\tif not isinstance(point.formattedValue, six.string_types) or dimension==\"line\":\n\t\t\t\tpoint.formattedValue = ','.join(str(round(point.formattedValue)))\n\t\t\telse:\n\t\t\t\tpoint.formattedValue = '.'.join(unicode(point.formattedValue).split(\".\")[-2:].split(\"[\")[0:-1])\n\t\t\tself._as_csv(separator, point.formattedValue, header)\n\t\t# TODO check for duplicate line separation\n\n\tdef __iter__(self):\n\t\titerator"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tif isinstance(separator, str):\n\t\t\tseparator = delimiter.split(',')\n\t\tif not isinstance(header, list):\n\t\t\theader = list(header)\n\t\toutput = []\n\t\tfor line in self:\n\t\t\tfor element in line:\n\t\t\t\toutput.append(str(element))\n\t\tdata = '\\n'.join(output)\n\t\treturn data\n# -*- coding: utf-8 -*-\n\n# ####################################################################\n#  Copyright (C) 2005-2013 by the FIFE team\n#  http://www.fifengine.net\n#  This file is part of"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\timport csv\n\t\timport numpy as np\n\n\t\tif not isinstance(separator, str):\n\t\t\traise ValueError(\"Separator must be a string.\")\n\t\tseparator = separator.strip()\n\n\t\tif header is None:\n\t\t\theader = []\n\n\t\tif isinstance(separator, str):\n\t\t\t# Make sure we have a list for some reason\n\t\t\tsep = shlex.split(separator)\n\t\t\theader = sep\n\n\t\tif self is None:\n\t\t\traise ValueError(\"Graph.to_csv. Please enter a filename.\")\n\n\t\t# Create the output file\n\t\twith open(self, 'w', newline='',"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\traise DeprecationWarning(\"the 'to_csv' method has been renamed \"\n\t\t\t\t\t\t\t                 \"to output.ToCSV\")\n\n\treturn to_csv(self, separator, header)\n\n\nclass TGraph(BaseGraph):\n\t\"\"\"\n\t.. versionadded:: 0.7\n\t.. versionchanged:: 0.6.1\n\n\tGraphs are only hashable and can thus be treated as an iterable\n\tthrough the :class:`iter`\n\tfunction instead of over the iterable directly.\n\n\t\"\"\"\n\tdef __init__(self):\n\t\tsuper(TGraph, self).__init__()\n\n\tdef to_"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.to_csv(separator, header)\n\t\n\n\n\tdef to_json(self, **kwargs):\t\t\n\t\t\n\t\t\"\"\"\n       .. deprecated:: 0.8 in Lena 0.8, now returns json.\n\t\"\"\"\n\t\treturn self.to_json(**kwargs)\n\n\n\n\tdef to_xml(self, **kwargs):\n\t\t\n\t\t\"\"\"\n       .. deprecated:: 0.7 in Lena 0.5, now returns xml.\n\t\"\"\"\n\n\t\treturn self.to_xml(**kwargs)\nfrom django.db import models\n# Create your models here.\n\n# In our"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\n#TODO: use csv.reader\nclass Graph:\n\t\"\"\"\n\tThe main graph class for the graph object\n\t\"\"\"\n\n\tdef __init__(self, title=\"Graph\", gtype=\"auto_graph\", color = None):\n\t\t\"\"\"\n\t\t:param title: Title of the graph (default: \"Graph\")\n\t\t:type  title: str\n\t\t:param gtype: Graph gtype; \"auto_graph\" (default), \"graph\", or \"edge_list\",\n\t\t(defaulting to \"auto_graph\").\n\t\t:type  gtype: str\n\t\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\ttry:\n\t\t\tinput = self._to_csv_string(separator).split(separator)\n\t\t\tif header and isinstance(header, str):\n\t\t\t\tinput = [header] + list(input)\n\t\t\toutput = self.output.ToCSV(*input)\n\t\texcept:\n\t\t\tprint (\"Cannot create file %s\" % self.__filename)\n\t\t\tcontinue\n\t\tself._write(output, separator, header)\n\n\tdef write_to_csv(self, path, separator=\",\", header=None, **opts):\n\t\t\"\"\"\n\t.. deprecated:: 0.5 in Lena"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\traise NotImplementedError()\n\nclass NXGraph(NBaseGraph):\n\t\"\"\"\n\t:class:`NXGraph` class\n\t\"\"\"\n\n\tdef __init__( \\\n\t\tself, \\\n\t\tname, \\\n\t\tnxclass=NXGraph, \\\n\t\toutG='nxGraph', \\\n\t\toutT='csv', \\\n\t\tdataX='sep_xy', \\\n\t\tdataY=[ '0.1' ], \\\n\t\tlabel=False):\n\t\t\"\"\"\n\t:param str name: graph name.\n\t:param int nxclass: NX class attribute, used"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tif isinstance(coord_name, str):\t# Must be an integer.\n\t\t\tcoord_name = [coord_name]\n\n\t\t# Get the list of indices corresponding to the\n\t\t# specified coordinate, in the order they will appear.\n\t\tn = len(_coord_name)\n\t\tif self.covar_names is not None:\n\t\t\tindices = []\n\t\t\tfor i in range(n):\n\t\t\t\tif (self._covar_names is not None) and \\\n\t\t\t\t\tcoord_name[i] == self.covar_names[i]:\n\t\t\t\t\tindices.append"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tfor i in range(4):\n\t\t\tch = coord_name + str(i)\n\t\t\tx = self.coord_names[ch]\n\t\t\ty = self.coord_names[ch]\n\t\t\tif x<=np.pi:\n\t\t\t\ty = y[::-1]\n\t\t\tif y>=np.pi:\n\t\t\t\ty = y[::-1]\n\t\t\terr_indices = np.array([x/np.sin(2.*np.pi), y/np.sin(2.*np.pi)])\n\t\t\terr_indices = err_indices + origin_indices\n\t\t\tyield"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord_idx_name = self.coord_system.variables[coord_name]['name']\n\t\tcoord_idx = self.cube.coord(coord_idx_name)\n\n\t\tcoord_err_indices = None\n\t\tif coord_idx in coord_err_by_coord_idx:\n\t\t\tcoord_err_indices = coord_err_by_coord_idx[coord_idx]\n\t\tif coord_idx in coord_err_by_coord_name:\n\t\t\tcoord_err_indices = coord_err_by_coord_name[coord_idx]\n\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn_indices = []\n\t\tfor i,c in enumerate(self.coord_systems):\n\t\t\tif c['coord'] == coord_name:\n\t\t\t\treturn_indices.append( (i,c) )\n\t\treturn return_indices\n\n\tdef _get_err_indices_and_groups(self, coord_name, group_name):\n\t\t\"\"\"\n\t\tGet the indices and groups corresponding to a coordinate.\t\n\t\t\"\"\"\n\t\treturn_indices = self._get_err_indices(coord_name)\n\t\treturn_groups = self.coord_systems[group_"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn np.where( coord_name!= 'longitude', axis=0 )[ 0 ]\n\t\n\tdef _get_coord_var_name(self, coord_var_name):\n\t\t\n\t\t\"\"\"\n\t\tGet the variable name corresponding to the coordinate variable.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tcoord_var_name : string\n\t\t\tName of the coordinate variable.\n\t\t\n\t\tReturns\n\t\t-------\n\t\tcoord_var_name : string\n\t\t\tName of the position variable.\n\t\t\n\t\t\"\"\"\n\t\t\n\t\treturn '{0}_{1}'.format( self.grid.at_coords[ self._index], \n\t\t\t\t\t\t\t\t coord"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord = self[coord_name]\t\n\t\tassert coord.has_bounds()\n\t\treturn self[coord_name][coord_name].data.max()-coord.data.min()\n\tdef idx_for_coord(self, coord_name):\t\n\t\treturn self._get_err_indices(coord_name).item()#Get indices for the given coordinate\n\t\n\tdef add_error(self, dim):\n\t\t\"\"\"\n\t\tAdd and set coordinates of a specific dimension to the dataset.\n\t\t\"\"\"\n\t\tassert dim > 0\t\n\t\tself.update()\t\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tassert self.__coord_name == coord_name, 'coord_name must be same as a name of the coordinate'\n\t\t# get indices\n\t\terr_indices = self.__coord_indices[coord_name]\n\t\t# get indices of all the valid pairs for error_indices. It is unsort of the indices. \n\t\tif self.__invalid_err_indices is not None:\n\t\t\terr_indices = np.array(self.__valid_err_indices)\n\t\t\terr_indices = np.sort(err_indices)\n\t\t# get indices of all error keys. It is"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord_ref = self.coord_map[coord_name]\n\t\tif coord_ref == -1:\n\t\t\treturn [-1] * self.ndim\t# default index is all-0\n\t\telse:\n\t\t\treturn ((self.shape[0] - self.coord_map[-1, coord_ref]) /\n\t\t\t\t\tself.shape[1])\n\n\tdef get_data(self, coord_name, copy=True):\n\t\t\"\"\"Get the data for a particular coordinate.\n\n\t\tParameters\n\t\t---------\n\t\tcoord_name : string\n\t\t\tname of the coordinate\n\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tx = [coord_name]\n\t\ty = [coord_name]\n\t\tx_err = len(x)\n\t\ty_err = len(y)\n\t\te_err = 2*x_err*y_err\n\t\ti1 = _nearest_index(x, y_err)\n\t\ti2 = _nearest_index(x_err, y_err)\n\t\treturn [i1, i2], [i2, i1]\n\n\tdef _get_err_xy(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\terridex = self.coord(coord_name)\n\t\ttry:\n\t\t\treturn erridex.indices\n\t\texcept AttributeError:\n\t\t\tpass\n\t\t\t\t\n\t## Methods for writing out/error files\n\tdef _write_err_file(self, err_file, index, coords_and_dims, **kwargs):\n\t\t\"\"\"\n\t\tWrite out an err file for the given coordinate, containing the given\n\t\tcoordinate's position and coordinate name.\n\t\t\n\t\tKeyword arguments:\n\t\tindex\t\t- name of the coordinate\n\t\tcoords\t\t- a sequence of numbers giving the coordinates or dimensional"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n\n\tfor key in context.dict.keys():\n\t\tself[key] = context[key]\n\n    def __str__(self):\n            \"\"\"\n            Pretty-print out graph properties to make a graph in\n            readable format.\n\n            When unstructured graphs with only one vertex,\n            then each vertex has a name and a color.\n\n            The first index can have a color and a list of x,y,z\n            coordinates.\n            \"\"\"\n            return dumper.represent(self)\n\n        def format_dict(self):\n            \"\"\"\n            Pretty-print out the properties of the graph.\n\n            Each node in the graph is represented"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tself.get_subcontexts = context.get_subcontexts\n\t\tself.post_process = context.post_process\n\t\tif context.post_process:\n\t\t\tself.process_post_proc = context.process_post_proc\n\t\telse:\n\t\t\tself.process_post_proc = False\n\n\t\tself._old_graph = None\n\t\tself._old_graph_element = None\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tIf this graph has a context, return\n\t\tthe properties and the subcontexts. If this context is\n\t\tempty, return a"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\t# TODO: make this so that multiple errors can be found\n\t\t# without a single value\n\t\t# TODO: do we really want to handle this? (use log instead)\n\t\tpass\n\n\tdef __repr__(self):\n\t\treturn pformat(vars(self.iteritems()), indent=4, width=1)\n\n\tdef __str__(self):\n\t\t\"\"\"Render graph information to a string (for IPython, this is\n\t\tso we can render a graph only when the context has a single\n\t\tvalue)\n\t\t\"\"\"\n\n\tdef render(self):\n\t\tctx = self._update_"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tself.context.clear()\n\t\traise NotImplementedError\n\n\tdef __iter__(self):\n\t\tself.index = 0\n\t\treturn self\n\n\tdef __str__(self):\n\t\tself.start_at = 0\n\t\tself.end_at = 0\n\t\tout = []\n\t\tfor element in self:\n\t\t\tout.append(str(element))\n\t\t\tif len(out) > 100000: break\n\t\tout = \"\\n\".join(out)\n\n\t\tif self.debug: print out\n\n\t\treturn out\n\n\nclass Vertex(object):\n\t\"\"\"A class to represent"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n#\t\t# The following line denotes a blank line at the bottom\n#\t# of the file when we print something on the way\n#\t\tprint(\"graph: no new lines\")\n#\t\tpprint(list(context.items()))\n#\t\treturn\n\t\n#\tdef add_new_line(self, node_class, index):\n#\t# Overridden to add a new edge:\n#\t\tprint(\"graph: new line added\")\n#\t\traise NotImplementedError\n#\t\t\t\n#\tdef remove_node(self, node):\n#\t\tdel"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n\n\n\terror = context.value[\"error\"]\n\tvalues = context[\"values\"]\n\tx = context[\"x\"]\n\ty = context[\"y\"]\n\tz = context[\"z\"]\n\n\te = 0\n\tif error:\n\t\te = error\n\n\ttry:\n\t\te_low = values[\"x_low\"][\"e\"]\n\t\te = values[\"x_low\"][\"t\"]\n\t\te_high = values[\"x_low\"][\"n\"]-1\n\t\te_avg = values[\"x_low\"][\"avg\"]\n\t\te_zero = float(values[\"x"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\t# FIXME: how can we do these all?\n\t\t# TODO: allow for custom-defined conversion\n\t\tself.write({'value'       : self._context.value,\n\t\t\t\t\t   });\t\t\t\t\t\t\t# TODO: add more error codes\n\t\t\n\t\t# Store the context, so we can check if this is a\n        # valid, sane way.\n\t\tcontext.value                 = self.create(context.value);\t # TODO: add a more error codes\n\t\tif not self._flow.propagate(context) == False: raise GraphError(\"Error while destruction\")\n\t\n\tdef __enter__("}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t \n\nclass Flow(object):\n\tdef __init__(self, name, graph=None):\n\t\tif graph is None:\n\t\t\tedgeTable = {}\n\t\telse:\n\t\t\tedgeTable = graph.edgeTable\n\t\tself.name = name\n\t\tself.graph = graph\n\t\tself._edges = {}\n\t\tfor e in edgeTable.values():\n\t\t\tself._edges[e] = edgeTable[e]\n\n\tdef __call__(self, **kwargs):\n\t\twith self.graph.push_state():\n\t\t\ttry:\n\t\t\t\tedgeTable = AttributeTable()\n\t\t\t\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t # We could use the flow only once, since that would\n\t # create a circular reference chain for every run (so we could\n\t # have to create an infinite loop).\n\n\"\"\"\n\nimport logging.handlers\nimport os\nimport sys\nimport struct\nimport time\nfrom datetime import datetime\n\nfrom hwt.hdl.types.bits import Bits\nfrom hwt.hdl.types.register import HStruct\nfrom hwt.interfaces.std import VectSignal\nfrom hwt.interfaces.utils import addClkRstn\nfrom hwt.synthesizer.param import Param\nfrom"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tself.__graph = context.parent\n\t\tself.__flow = None\n\t\tif self.__graph is not None:\n\t\t\tself.__flow = self.__graph.flow\n\t\telse:\n\t\t\tself.__flow = None\n\n\tdef reset(self):\n\t\t\"\"\"\n\tReset the graph by creating a fresh instance.\n\t\"\"\"\n\t\tfor attr, value in self.__graph.attrs.items():\n\t\t\tnew_value = getattr(self, attr)\n\t\t\tif new_value is None or new_value!= value:\n\t\t\t\tsetattr(self, attr, value)\n\t\t\t\tassert self.__"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tbins = np.array(bins)\n\t\tedges = np.array(edges)\n\t\twidth = edges / bins.sum()\n\t\tintegral = np.zeros((bins.shape[0], bins.shape[1]*width))\n\t\tfor i in range(len(edges)):\n\t\t\tintegral[i] = 1.0 / (bins[:,i] + edges[:,i])\n\t\treturn integral\n\tdef plot(self, data, bins=10):\t#, labels=None):\n\t\t\"\"\"\n\tPlot a set of data's values onto a graph.\n\n\t*data*"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tbins, edges = np.asarray(bins), np.asarray(edges)\n\t\tif edges.shape!= (len(bins), ): raise ValueError(\"bins and edges must have same length\")\n\t\t# Check that the bins are valid, or raise ValueError\n\t\tbins = np.atleast_2d(bins)\n\t\tif not np.iterable(bins): bins = [bins]\n\t\tif (np.all(np.diff(bins) < 0) or np.all(np.diff(bins) > 1)):\n\t\t\traise ValueError(\"bins must be monotonically increasing values\")"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\treturn (np.exp(edges[1:] + edges[:-1]) / (bins[1] + edges[0])) ** 0.5\ndef mean(bins, edges):\t\"\"\"\n\tCompute mean (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n        res = (1. / (bins.max() + bins.min()) ** 0.5) - 0.5\n        res = np.exp(res)\n        # res = 1."}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\treturn integral.__doc__+'\\n'+\\\n\t\t      (\n\t\t\t 'Integral :\\n'+\n\t\t\t '\tbins is the number of bins in histogram\\n'+\n\t\t\t ''\n\t\t\t '\tedges is the number of entries in an edge list\\n'+\n\t\t\t '\tNote that this returns zero.')\n\t\n\tdef histogram(self, bins, edges):\n\t\t\"\"\"\n\tCompute the integrand of a gaussian.\n\t\n\t*bins* : the number of bins in the histogram.\n\tTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tif self.bins>bins+1:\n\t\t"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\nclass uniform(histogram):\n        \"\"\"\n\tPlot a uniform distribution over the (z-axis) given bins.\n\t\"\"\"\n        def __init__(self, bins, edges):\n                \"\"\"\n\tParameters\n\t----------\n\t\tbins: `float`\n\t\t\tValue(s) for each dimension of the uniform grid.\n\t\tedges: `float`\n\t\t\tValue(s) for each dimension of the uniform grid.\n\t\t\"\"\"\n                super(uniform, self).__init__(bins, edges)\n\n        def tostring(self):\n                \"\"\"\n\t\tReturns a (string) plot of the uniform grid.\n\t\t\n\t\t"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\treturn 1.0/(bins[1]*(edges[1]-edges[0]) \\\n\t\t  +(bins[1]-bins[0])*edges[0])\n\n\nclass _Line:\n\tdef __init__(\n\t\tcls,start,end,length,num,\n\t\tmid,end_mid,start_mid,\n\t\tinterval=0.0):\n\n\t\tself.start_bin = start\n\t\tself.end_bin = end\n\t\tself.length = length\n\t\tself.mid = mid\n\t\tself.end_mid = end_mid\n\t\tself.interval"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\n\t# If we need to calculate the integral, we would be using\n\t# a cubic polynomial to calculate the integral from the bins,\n\t# we would need to multiply by the integral on each bin, and\n\t# divide by the integrand.\n\tif (len(bins)!= len(edges)) or (len(bins) == 0):\n\t\treturn bins\n\tif (len(edges)!= len(bins[0])):\n\t\traise RuntimeError (\"len(edges)!= len(bins[0])\")\n\tintegral = 0\n\tn_bins = len(bins)"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\n\t# For discrete cases\n\tnum_bins = len(bins)\t\t\t\t# size of bins\n\tnum_rows = len(edges)\t\t\t\t\t# number of rows (including last entry)\n\t\n\tif num_rows == 0 and num_bins == 0:\n\t\treturn\n\tif num_rows == 1:\n\t\treturn\n\t\n\t# find the integral over this bin\n\tmin_val = float('inf')\n\tfor i in range(num_rows):\n\t\tedge = list(map(float, edges[i][1:]))\t\t\t# the edge's midpoint\n\t"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tbins, edges = meshgrid(bins, edges)\n\t\th = np.histogramdd(edges, bins)[0]\n\t\treturn np.diff(h)\n\n\nclass _SilentRaster(Raster):\n\t\"\"\"\n\tA raster that sits in raster space around a given radius.\n\t\"\"\"\n\t#: Defining *minimum* value of the density is the\n\t#: energy density value divided by the number of neighboring\n\t#: pixels.  This value varies in number to allow for\n\t#: smooth background.\n\tminimum = None\n\t#: Minimum radius"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\n\tnbins = len(bins)\n\tN = nbins.prod()\n\n\t# make a map of neighbouring nbins to the integral\n\tfor i in xrange(N):\n\t\tb = bins[i]\n\t\tintegral = BreuschCurve(b.values, b.npts, amin, amax)\n\t\tintegral *= scale\n\t    b.values = b.npts - integral.values\n\t\n\treturn integral\n\t\ndef spread(bins, values):\n\t\"\"\"\n\tSpreads the integral curve at points where the given values\n\tare"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfor e in seq:\n\t\t\tif not e.is_source():\n\t\t\t\treturn False\n\t\treturn True\n\ndef is_in_path(element, path, context=None):\n\t\"\"\"Check whether the element is in the path;\n\tIf *element* is not actually part of path then\n\tit can either be a path or a parent context/element.\n\tIn either case, the element must be in the path and\n\tnot in the parent context. Otherwise False is returned.\n\t\"\"\"\n\t# We assume the elements are either a path or a parent\n\t#"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfirst_type = seq.getDataElement(0).getType()\n\t\tif first_type == 'String':\n\t\t\tfirst_type = 'String'\n\t\tfor seq_index in range(1, len(seq) + 1):\n\t\t\t#if not first_type:\n\t\t\tself.assertEqual(\n\t\t\t\tseq[\n\t\t\t\t0],\n\t\t\t\tseq_index == len(seq) + 1\n\t\t\t)\n\t\t\tfirst_index = first_type.find('String')\n\t\t\tself.assertEqual(\n\t\t\t\tfirst_index,\n\t\t\t\tind + len(seq)\n\t\t\t\t)\n\t\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfor elem in seq:\n\t\t\tparent = elem.getparent();\n\t\t\tif \"RefSeq\" not in parent.name:\n\t\t\t\tcontinue\t\t\t\t\t\t\t\t\t\t\t# No repeat-seq\n\t\t\t\tif parent in [\"Empty\",\"Source\"]:\t\t# A repeat-seq with no parent (no reference)\n\t\t\t\t\tif \"Name\" in elem.attrib:\n\t\t\t\t\t\telem[\"Name\"] = \"\"\t\t# Replace name so that its not redefined\n\treturn;\n\t\ndef is_fill( seq, new_seq=None ):\n\t\"\"\"\n\tIs this the same as FillRequestSeq, but this"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\t# Note: Sequence types which are not currently defined are\n\t\t#  \"FILL\", \"FIL\", and \"SPT\" (the ID of the element).\n\t\tif not isinstance(seq, Fill RequestSeq):\n\t\t\treturn False\n\t\t# All elements of a FillRequestSeq should have a Source sequence.\n\t\t# I have no idea what he's supposed to do here.\n\t\treturn True\n\n\treturn True\n\ndef seq_contains_source(seq, source):\n\t\"\"\"\n\tTests if *seq* contains a particular source (e.g. source of a Sequence).\n\t\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif len(seq) < 1:\n\t\t\treturn False\n\t\treturn seq[0].type!= fill_type\n\t#\n\tdef readElementTag(self, name):\n#\t\tbuf = '<root><name>'+(name.toXml()[:-1]+'</name>)'\n#\t\tret = libxml2mod.xmlReadCdata(self._o, None, buf)\n#\t\treturn ret\n\t\n\tdef nameToIdMap(self, name):\n\t\ti = 0\n\t\twhile name[i]!='':'\n\t\t\ti = i+1\n\t\tif"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif not seq or len(seq) == 1:\n\t\t\treturn False\n\t\tfor s in seq:\n\t\t\tif not isinstance(s, FillRequestSeq):\n\t\t\t\treturn False\n\t\treturn True\n\telse:\t\t\t\t\t\t\t\t# None\n\t\treturn None\n\n\n\ndef validate(doc):\n\t\"\"\"\n\tCheck that this element is an instance of a FillRequest\n\tobject.\n\t\"\"\"\n\t#assert isinstance(DocComment, basestring)\n\n\tif not isinstance(doc[\"text\"], basestring):\n\t\treturn False, \"Text to be a string is expected\"\n\n\t# Check that"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn _make_source_seq(seq) in _get_fill_request_seq_ids()\n\ndef extract_fill_request_seq(seq, index, name):\n\tif index >= len(seq):\n\t\traise IndexError(\"Expected index < \" + str(index))\n\treturn seq[index]\n\ndef _get_fill_request_seq_ids(): return 'fill_request_seq_ids'\n\ndef _get_fill_request_seq(): return 'fill_request_seq'\n\ndef _create_fill_request_seq(seq_"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfor s in seq.getElements():\n\t\t\tif s.isFillRequest():\n\t\t\t\treturn True;\n\ndef get_element_from_path(element, path):\n\t\"\"\"\n\tGet the first matching <element> in <path>, where\n\tpath is a element name relative to the document root path.\n\t\"\"\"\n\tfor s in element.getElements():\n\t\tif element.localName == path:\n\t\t\treturn s\n\treturn None\t\t# No element matching\n\n\n\"\"\"\nReturn the element in document with attributes set to 'None'\n\nPass None for element attributes"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif seq.get_line_start() == 1:\n\t\t\treturn True\n\telif len(seq.get_elements()) == 1:\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef is_fill_request_start(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequest start.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif seq.get_line_start() == 0:\n\t\treturn True\n\telif len("}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\ttype = seq.type\n\t\tlen_src_seq = len(seq.src)\n\t\tif len_src_seq < 4:\n\t\t\traise ValueError(\"Invalid FillRequest Sequence\")\n\t\tfor elt in seq.src:\n\t\t\tif type!= elt.type:\n\t\t\t\treturn 0\n\t\treturn 1\n\ndef get_seq_len_for_fill_request(seq):\n\t\"\"\"\n\tGet the length of the FillRequests sequence from the\n\ttype of sequence element. \n\t\"\"\"\n\tseq = seq.seq\n\tif \"FillRequest\" in seq.options:"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn obj.get_command()\n\t# ------------------\n\t# Method 'is_filler' must be called by the Object.\n\t# ------------------\n\tdef is_filler(self):\n\t\t\"\"\"\n\tObj.is_filler(obj)\n\t\tChecks if the file is filler.\n\t\t@rtype bool\n\t\t@return The result of calling 'is_filler' on the object.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t# ------------------\n\t# Interface 'is_filler' must be called by the object.\n\t# --------------------------------\n\tdef"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn not ('r' in obj and 'W' not in obj) and 'w' not in obj\n\tdef is_filler_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'filler' and 'filler'.\t\"\"\"\n\t\treturn not ('P' in obj and 'E' in obj) and 'p' not in obj\n\ndef is_filler(obj):\t\t\n\t\"\"\"\n\tObject contains all of supported fillers. \n\t\"\"\"\n\traise NotImplementedError\n\ndef is_fill_request_el_1(obj):\t\n\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn (obj.type == 'fill') # TODO\n\n\tdef is_fill_request(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and'request', with objects.\n\t\"\"\"\n\t\treturn not(is_fill_request_el(obj))\n\n\ndef print_output(str):\n\tif DEBUG:\n\t\tprint str\n\n\ndef is_null(obj):\n\tif DEBUG:\n\t\tprint 'False'\n\treturn obj is None\n\n\ndef make_output(f):\n\tif isinstance(f, str):\n\t\tf = open(f"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif obj.executable_path.endswith(u'.exe'):\n\t\t\tobj.fill()\n\t\t\treturn True\n\n\tdef is_request(self):\n\t\t\"\"\"\n\t\tObject contains executable methods'request' and'system'.\t\"\"\"\n\t\tif self.is_fill_request_el(u'request'):\n\t\t\treturn True\n\t\treturn False\n\n\tdef is_system(self):\n\t\t\"\"\"\n\t\tObject contains executable methods'system' and'system'.\t\"\"\"\n\t\tif self.is_fill_request_el(u'system'):\n\t\t\treturn True\n\t\treturn False"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tpass\n\t\n\tclass Fill(Drawable):\n\t\t\"\"\"\n\t\tClass representing an <fill> element or an <path> element.\n\t\t\n\t\t:param node:\n\t\t\tThe node to fill.\n\t\t:param path:\n\t\t\tThe path to be filled.\n\t\t:param type_:\n\t\t\tThe type hint.\n\t\t:param pen:\n\t\t\tThe pen for the fill (boolean).\n\t\t:param width:\n\t\t\tThe width of the fill.\n\t\t:param thickness:\n\t\t\tThe thickness (int) for the fill.\n\t\t:param offset:\n\t\t\tThe offset for"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn obj.is_fill_request\n\n\t@staticmethod\n\tdef get_object_name(obj):\t\t\t\"\"\"\n\tObject contains executable methods 'create' and 'post'.\t\"\"\"\n\t\treturn obj.create\n\n\t@staticmethod\n\tdef is_create(obj):\t\t\t\"\"\"\n\tObject contains executable methods 'create' and 'post'.\t\"\"\"\n\t\treturn obj.create_post\n\n\t@staticmethod\n\tdef get_object_permissions(obj):\t\t\t\"\"\"\n\tObject contains executable methods 'create' and 'post'.\t\"\"\"\n\t\treturn obj.create_post"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif isinstance(obj, types.MethodType):\n\t\t\t# In case of bound non-bound method\n\t\t\treturn\n\t\telif isinstance(obj, types.FunctionType):\n\t\t\t# In case of python function\n\t\t\ttry:\n\t\t\t\treturn str(obj.__code__)\n\t\t\texcept AttributeError:\n\t\t\t\treturn str(obj)\n\t\treturn False, ''\n\n\tdef is_get_error_message_el(self):\n\t\t\"\"\"\n\t\tObject contains error message codes.\n\t\t\"\"\"\n\t\treturn 'Get error message'\n\n\tdef is_get_error_report_el(self):\n\t\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn obj.startswith('call ') \\\n\t\t     \tif obj.endswith('()') else True\n\ndef is_unconnected_elt(ip):\t#\n\t\"Tells an instance of socket.socket, for which \"\n\t# is socket.socket.recv_into().\"\n\treturn isinstance(ip, NodeType) \\\n\t      and (ip.port == 9002 or ip.port == 7201) \\\n\t\t     \tand (ip.family!= socket.AF_INET6 or ip.family!= socket.AF_INET ) \\\n\t\t     \tand (ip.type =="}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif isinstance(obj, RV_Fill):\n\t\t\treturn True\n\t\treturn False\n\n\tdef check_fill_result(code):\t\n\t\t\"\"\"\n\t\tCheck for valid fill result codes.\n\n\t\tCode is a boolean, 0 is a false, None is a true string, 0\n\t\totherwise.\n\t\t\"\"\"\n\t\tif code in range(len(fill_codes)) or fill_codes[code]!= 0:\n\t\t\traise DataParseError(\"Illegal fill result code %d.\" % code)\n\n\tdef check_end_of_fill(code):\t\n\t\t\"\"\"\n\t\tCheck"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif get_element_selector(obj, 'div.input__use_default', 'input_type') in [\n\t\t\t'hidden'\n\t\t\t] and obj.get_attribute('disabled') == '':\n\t\t\treturn False\n\n\t\telement = obj.find_element_by_css_selector(get_element_by_css_selector(\n\t\t\tobj, 'div.input__use_default', 'input_type'))\n\t\tif not element:\n\t\t\treturn False\n\t\treturn safe_string(element.get_attribute('value')) == 'hidden'\n\n\tdef is_hidden"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tif obj.__class__.__name__.lower() in ['runas','runas.in']:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\tdef is_pso(obj):\t\"\"\"\n\tObject contains 'pso' method.\n\t\"\"\"\n\tdef is_pso(obj):\n\t\t\"\"\"\n\tObject contains 'pso' method.\n\t\"\"\"\n\tdef is_psf(obj):\n\t\t\"\"\"\n\tObject contains 'pso' method.\n\t\"\"\"\n\treturn obj.get('runas', False) and obj.get('runas"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.runnable or getattr(obj, 'force_run', False)\n\nclass task_gen(object):\n\t\"this is the interface between the class name :py:class:`waflib.Build.BuildContext` and the task generator, see :py:func:`waflib.TaskGen.task_gen`\"\n\nprofile=None\nbuild_fun=task_gen()\ndef set_profile(profile):\n\t\"sets the py:class:`waflib.Configure.ConfigurationSet` instance used by :py:class:`waflib.Configure.ConfigurationContext`"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn (obj.executable and obj.command or \"\").lower() in _commands\n\n\nclass RexCmd(object):\n\t\"\"\"\n\tThis object can execute commands.\n\t\"\"\"\n\tdef __init__(self, script, command, arguments):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tself._script = script\n\t\tself._command = command\n\t\tself._arguments = arguments\n\n\t@property\n\tdef path(self):\n\t\treturn self._script[\"path\"]\n\n\t@property\n\tdef executable(self):\n\t\treturn self._command[\"executable\"]\n\n\t@property\n\tdef"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tpass\n\n#########################################\n# Function decorators\n#########################################\n\ndef init_func_wrapper(func, *args):\n\t\"\"\"\n\tFunction wrapper is wrapped within the :func:`waflib.Context.Context.cmd_and_log`\n\tcontext which takes a function name as argument.\n\n\t:param func: function name as string\n\t:type func: string\n\t:param *args: arguments\n\t:type args: list\n\t:return: function wrapper\n\t:rtype: function\n\t\"\"\"\n\tfunc_def['wrapper'] = func\n\tif isinstance("}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn (\n\t\t\thasattr(obj, 'waf_exec') or\n\t\t\t'__builtins__' in dir(obj)\n\t\t)\n\n\tdef parse(self):\n\t\tname = self.inputs[0].name\n\t\tcode = self.read_file(name)\n\t\tvars = self.environ\n\t\tfor k in self.all_envs:\n\t\t\tif not k in self.environ:\n\t\t\t\tself.undefine(k)\n\t\t\telif k in vars:\n\t\t\t\tself.undefine(k)\n\t\t\t\tself.env.append_value(k + '_env')\n\t\t\t\t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tfor k, v in obj.__dict__.items():\n\t\t\tif 'c' in v:\n\t\t\t\treturn True\n\ndef is_valid_el(obj):\n\t\"\"\"\n\tObject contains class method 'isvalid'.\t\n\tFor a given object, the class attribute 'isvalid' is set to True and the\n\tobject to be validated is returned.\n\t\"\"\"\n\treturn all(x in dir(obj) for x in dir(obj.__class__))\n\nclass task_info(object):\n\t\"\"\"\n\tObject for gather information about each task defined in 'run'."}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn isinstance(obj, Exec) and obj.path_in == obj._dir\n\n\tdef set_run_execve_flags_once(obj):\t\"\"\"\n\tSet any flags used to enable RUNPATH execve_flags.\n\n\tThis is a helper.\n\n\tArguments:\n\t\tobj\t--   The object to use when setting RUNPATH.\n\t\"\"\"\n\t_set_run_execve_flags(obj, _RUNPATH_FLAGS)\n\t_set_run_execve_flags(obj, _RUNPATH_DEFFLAGS)\n\n\tif _run_exe"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj=='run'\n\ndef is_runscript(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n        return obj=='runscript'\n\ndef is_runimage(obj):\t\"\"\"\n\tObject contains executable method 'runimage'.\t\"\"\"\n        return obj=='runimage'\n\ndef is_runscript_in_img(obj):\n\t\"\"\"\n\tObject contains executable method 'runimage_in_img'.\t\"\"\"\n        return obj=='runrunimage_in_img'\n\ndef is_runcommand(obj):\t\"\"\"\n\tObject contains"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tassert(isinstance(obj, Exec), \"must evaluate to an Exec\")\n\t\tif not obj.is_run:\n\t\t    raise ValueError(\"No executable method run function: '%s'\"%obj)\n\t\tobj.add_run_command(self, obj) # for performance reasons\n\t\t\n\tdef _run(self,obj):\n\t\t\"\"\"\n\t\tSubclasses implement this method; should use the Run function that\n\t\thas a return value.\n\t\t\"\"\"\n\t\treturn self._run_for_run(obj)\n\t\n\tdef _run_for_run(self,obj):\n\t\traise NotImplementedError"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.__class__.__name__==\"run\"\n\tdef has_run(self,f):\n\t\t\"\"\"\n\t\tObject contains executable method 'run'.\t\"\"\"\n\t\tret=None\n\t\ttry:\n\t\t\tif self.meths.has_key(f):\n\t\t\t\tif type(self.meths[f])==types.ClassType:\n\t\t\t\t\tret=True\n\t\texcept:\n\t\t\tpass\n\t\treturn ret\n\tdef get_executable_name(self,f):\n\t\t\"\"\"\n\t\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.__class__.__name__"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn hasattr(obj, '__getitem__')\n\telif hasattr(obj, '__iter__'):\t# iterable?\n\t\ttry:\n\t\t\tlen(obj)\n\t\texcept: pass\n\t\telse:\n\t\t\tfor i in obj:\n\t\t\t\tif isinstance(i, Node) and i._is_fill_fn:\n\t\t\t\t\treturn True\n\t\t\treturn False\n\telse:\n\t\treturn False\n\ndef _find_node_at_cursor(doc, node, x, y):\n\tif node == cursor.cursor.cursor:\n\t\treturn node\n\telif node == cursor.connection:\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif obj[0] == 'fill': return True\n\t\treturn False\n\n\tdef fill_compute_el(obj):\t\"\"\"\n\tFill compute from object 'obj' into executable methods 'fill'.\t\"\"\"\n\t\tif obj is None or is_fill_compute_el(obj): return None\n\t\tc = obj[0]\n\t\to = obj[1] if len(obj) > 1 else None\n\n\t\t# create executable method\n\t\tif c in ['exe', 'exe_noseg', 'exe_sep']:\n\n\t\t\t# if executable method is empty: use default executable method"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif not (obj is None or isinstance(obj, ELF)):\n\t\t\tobj = obj.parent()\n\t\telif not hasattr(obj, 'fill_compute'):\n\t\t\treturn True\n\t\telif getattr(obj, 'fill_compute') == 'compute':\n\t\t\treturn True\n\tdef is_fill_sub_el(obj):\treturn obj.is_sub_el()\n\tdef is_fill_slice(obj):\treturn obj.is_slice()\n\t_is_fill_slice = is_fill_slice\n\tis_fill_sub_el = is_"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif is_exe(obj):\traise FuseOSError(\"can't open file: \" + obj)\n\t\treturn obj\n\n\tdef write(self, obj, tofile):\n\t\t\"\"\"\n\t\twrite command object 'obj' to 'tofile'.  Note that write() only\n\t\taccepts keyword arguments.\n\t\t\"\"\"\n\t\tassert is_exe(obj)\n\t\tif not (is_exe(obj) or is_exe(obj.name)): raise FuseOSError(\n\t\t\t\t\t\"write: cannot write to 'tofile'.\" + \" (use 'close' or 'check')\""}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj.has_key(\"fill\") and obj.has_key(\"compute\")\n\n\t# Returns True if the object has filled this function\n\t# (i.e. a function has been \"fill\" or \"compute\")\n\tdef fill_obj_is_fill_compute(self):\n\t\tfor (c, f) in self.core_objs_with_fill.items():\n\t\t\tif \"fill\" in c.signature:\n\t\t\t\treturn True\n\t\treturn False\n\n\t# Returns object is an integer for a given name.\n\tdef add_obj_name (self"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj.type == TYPE_CODE_CLS and (obj.name == b'__fill')\n\n\tdef is_call_call_arg(self):\t\"\"\"\tReturns True if this object is a class method and is a call argument. Returns False otherwise.\n\t\t\"\"\"\n\t\treturn self.meth_type == CALLABLE_OUTGOING_CALL_ARG\n\n\tdef __str__(self):\n\t\treturn \"InstructionDef [%s]\" % (self.name)\n\n\nclass DataRef(object):\n\t\"\"\"\n\tThis class represents a data type with the data and the value."}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif (obj.cmd_arg == cmd_str_obj.cmd_arg_obj.cmd_type) and (obj.args_str[0] == obj.args_str[1]):\t\t return True;\n\t\telse:\t\t\t# we can only fill out, if it's a cmd and there is a 'core', or its own one\n\t\t\treturn (obj.command in ['core', 'fill']) or \\\n\t\t\t\t\t(obj.command in ['fill']) and\\\n\t\t\t\t\tnot (obj.args_str[1] in args_str_obj.argv)\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif IS_PY3:\n\t\t\t# assume the first arguments are the same length:\n\t\t\treturn obj.fill(api.IS_PY3)\n\t\telse:\n\t\t\treturn obj.compute()\n\telif IS_PY3:\n\t\t# assume the first argument is the number of args\n\t\treturn (obj.fill(api.IS_PY3) if hasattr(obj, 'fill') else 1)\n\telse:\n\t\treturn api.py_wrap(obj.get())\n\ndef wrap_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj.prototype == 'compute'\t# return false by default!\n\t# check_call\n\tdef check_call(self, obj):\n\t\tif obj.procedure.name in ['fill', 'compute']:\n\t\t\tcheck(False, \"this procedure should never be called because an 'fill' procedure is a 'compute'(not a 'fill')\")\n\t\tcheck(not obj.prototypes.has_key('compute'), \"the prototype is a 'compute', and is not a 'fill' procedure\")\n\t\tif obj.prototype!= self.prototype:\n\t\t\tcheck(False, \""}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tassert not obj.is_fill(cl.EL_IN)\n\treturn obj.is_fill()\n\n\ndef is_compute_el(obj):\t\"\"\"\n\tIs object is compute-related, it is in use!\n\t\"\"\"\n\treturn isinstance(obj, (cl.EL_IN, cl.EL_INDIRECT))\n\n\ndef is_fill_free_el(obj):\t\"\"\"\n\tIs object is free-related, the result of is_fill, is_alloc_free (if in use)\n\t\"\"\"\n\treturn any(is_fill_"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\n\tresult = {}\n\tfor key in d1:\n\t\tif level & 1 == -1:\t# only return keys of the same level\n\t\t\tresult[key] = d2[key]\n\t\telse:\n\t\t\tif level & 1 < 0:   # if the same value is higher than level\n\t\t\t\t# we may have a key that was not present in dict or\n\t\t\t\t# if the same key is present in the grandchild\n\t\t\t\t# in dictionary, or the grandchild in dictionary\n\t\t\t\t# but has different values\n\t\t\t\tresult[key] = d1[key]\n\t\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tassert level > 0\n\t\tout = dict(copy.deepcopy(self.data))\n\t\tout[0] = d1 if level else d2\n\t\treturn out\n\n\t###\n\t##\n\t## Mixins\n\t###\n\t###\n\t###\n\t## Return a copy of the input data. For any\n\t### non-dictionary object, all items in this dict will\n\t### be replaced by the corresponding value. For\n\t### non-dictionary objects, all items' values will be replaced.\n\t### Equalities are not checked. They can be of any type"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\t\n\tif level > -1:\n\t\traise error, 'invalid *level*: %s' % level\n\t\t\t\n\td1keys = set(d1.keys())\n\td2keys = set(d2.keys())\n\t\n\td1keys = d1keys.intersection(d2keys).difference(set(d1keys))\n\td2keys = d2keys.difference(d1keys).difference(set(d2keys))\n\t\t\n\td1keys = sorted(d1keys)\n\td2keys = sorted(d2keys)\n\t\n\tif level > -"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\ndef intersection(*keys, **kw):\n\t\"\"\"Return a dictionary containing only items with matching keys.\"\"\"\n\td = dict()\n\tfor key in keys:\n\t\td[key] = kw.get(key, -1e9)\n\td.update(kwargs)\n\tif len(keys):\n\t\td = {k:v for k,v in d.items() if v>min(d.values())}\n\treturn d\n\ndef find_unique(lst, lst2):\n\t\"\"\"Return a list containing only unique items from *lst2*.\"\"\"\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\td1.clear()\n\t\td1.update(d2)\n\n\tfor k,v in d1.iteritems():\n\t\tif k in d2:\n\t\t\tif level!= -1 and level!= 1 and (v == d2.get(k,k)):\n\t\t\t\td2[k] = v\n\n\treturn d2\n\n\ndef difference_update(d1, d2):\t\tNone\n\t\"\"\"\n\tUpdate the value of d1 with the *d2* object.\n\n\tWill not update the value at *d1*.\n\t\"\"\"\n\n\tfor k"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\treturn Dictionary(self, d1=d1, d2=d2, level=level + 1)\n\n\tdef difference_dict(self, d1, d2):\n\t\t\"\"\"\n\t\tReturn a dictionary of items from *d1* that have differences in\n\t\t*d2*.  Uses values in ``d1`` and ``d2`` but excludes keys\n\t\tfrom the result if they are different.\n\t\t\"\"\"\n\n\t\tif not isinstance(d1, dict):\n\t\t\traise TypeError(\"Expected dictionary, got %r instead\" % (type(d1),))\n\n\t\tif not"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\n\tkeys = map(lambda x: x, filter(lambda x: not bool(x), [d1, d2], [level]))\n\n\tif len(keys) <= 1: return d1\n\telif len(keys) == 2: return d1.setdefault(keys[0], {})\n\telse:                 return d2.setdefault(keys[0], {})\n\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand\n\nfrom zerver.lib.send_email import send_email\n\nfrom zerver.models import Stream, UserProfile\n\n\nclass Command"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\t(depth, key) = _levels(depth, d1)\n\n\t\tresult = {}\n\t\tchildren = {key: value for key, value in d2.items() if hasattr(value, '__iter__')}\n\n\t\tfor key, value in children.items():\n\t\t\tif value == []: continue\n\t\t\tresult.update(copy.deepcopy(value))\n\n\t\tif level > 0:\n\t\t\tresult.update((key, levels - 1))\n\telse:\n\t\tresult = d2\n\n\treturn result\n\ndef update_level(d1, tree, level, *, set_fn"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\n\tif level == -1:\n\t\tdel d1[:]\n\t\treturn d2\n\n\tif 'd1' in d2:\t\n\t\td1[-1].update(d2)\n\t\treturn d2\n\n\tif 'd2' in d1:\n\t\td2[-1].update(d1)\n\t\treturn d1\n\n\tret = {}\n\tif isinstance(d1, dict) and 'd2' in d1:\n\t\tif 'values' in d1['values'] and 'items' in d1['values']['items']:\n\t\t\tret ="}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\treturn Overlap(\n\t\t\tself.d2, self.d1, self.d2, self.level + [(0, [])]\n\t\t)\n\n\tdef difference_update(self, dict_one, dict_two, new_dict_two = {}):\n\t\t\"\"\"\n\t\tReturn a dictionary with items from *dict_two* not contained in\n\t\t *dict_one*.  Also adds new dictionary and returns the result.\n\t\t\"\"\"\n\t\tassert isinstance(dict_one, dict)\n\t\tassert isinstance(dict_two, dict)\n\n\t\tlevel_ = int(new"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.fillcount -= weight\n\tdef begin(self, npts=256):\n\t\t\"\"\"\n\tStart the histogram, counting the number of points in the\n\tdistribution.\n\t \"\"\"\n\t\t# get the npts\n\t\tnpts = int(math.ceil(npts))\n\t\t# check that npts is valid\n\t\tif npts < 0: raise ValueError # need at least one frame\n\n\t\t# determine how to pad the histogram\n\t\tnsamp = self.nsamp\n\t\t\n\t\t# initialize empty histogram\n\t\tpadding = 0 # nbins = 1\n\t\tfor k in range("}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif coord.size == self._datadimension:\n\t\t\tself.bins = np.linspace(coord.min(), coord.max(), self._datadimensions) # need to normalize\n\t\t\tself._datacount = 0\n\t\t\tself.bins = self._datadimensions*self._numbins\n\t\telse:\n\t\t\traise ValueError('Dimension {} not fully specified'.format(coord.size))\t\n\t\tself.binbounds = np.zeros(self._numbins)\n\t\t#if weight is None then fill each segment with that weight\n\t\tif weight is None:\n\t\t\tsumbins = np.zeros"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif (not coord in self.hists) and (self.hists[coord][\"type\"]==\"histogram\"):\n\t\t\treturn\n\n\t\thist  = self.hists[coord][\"data\"]\n\t\twidth = hist.GetNbinsX()\n\t\theight = hist.GetNbinsY()\n\n\t\tif weight==1:\n\t\t\tfill = 0\n\t\telse:\n\t\t\tfill = ( width-weight*height/2 ) /2\n\t\t\t\t\t\t\n\t\tself.plot_bins(hist, fill=fill, xlab=\"bins\")\n\n\tdef histplot(self, coord, weight=1):\n\t"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.m_nbins = int((self.end-self.start)/self.nbins.value*weight)\n\t\tself.m_nbins = max(self.m_nbins, 1)*2\n\t\tif self.end-self.start >= self.nbins.value*weight:\n\t\t\tfor i in range(int((self.start+1)/self.nbins.value*weight),\n\t\t\t\t\t\t    int((self.nbins.value/2)/weight)):\n\t\t\t\ti += 1\n\t\t\t\tself.m_nbins -= 1\n\t\telse:\n\t\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.coord = coord\n\t\tself.weight = weight\n\tdef __call__(self, color):\n\t\treturn '\\x1b[{};{}m {}'.format(\n\t\t\tself.coord[0]+ (self.weight/6), color, self.coord[-1]+ (self.weight/6))\t\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nCopyright (c) 2013-2014, https://twitter.com/pneumatic/\nMIT License, see LICENSE.txt for details\n\"\"\"\nimport time\nfrom gevent"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\ti = self.findindex(coord, weight)\n\t\tif i >= 0:\n\t\t\tself.fill_hist(i, self.parent.hbins)\n\n\t\t\ti = self.findindex(coord[i], weight)\n\t\t\tif i >= 0:\n\t\t\t\tself.fill_hist(i, self.parent.hhistedges)\n\n\tdef set_linewidth(self, p):\n\t\t\"\"\"\n\t\tSet the size of the histogram's linewidth.  The color of this\n\t\thistogram is set to 1.\n\t\t\"\"\"\n\t\tself.linewidth = p\n\t\tself._"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif weight > 1:\n\t\t\t# Fancy fill method\n\t\t\tpass\n\n\t\tself.fill_start = 0 + coord[0]\n\t\tself.fill_end = self.fill_start + weight\n\n\t\tif self.fill_start > self.fill_end:\n\t\t\tself.fill_start = self.fill_end\n\n\tdef __repr__(self):\n\t\treturn \"Fill(start=\\\"%s\\\", end=\\\"\\\" + str(self.fill_start) + \"\\\", weight=\\\"\\\" + str(self.fill_end) + \"\\\")\"\t\n\n\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.hist.fill(coord, weight)\n\ndef fill2(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight* and return it.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.hist2.fill(coord, weight)\n\ndef fill3(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.hist3.fill(coord, weight)\n\ndef fill4(self"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif isinstance(self._origin, np.ndarray) and len(self._origin) > len(coord):\n\t\t\traise ValueError, \"origin must have lenght <= than the number of bins\"\t\t\t\n\n\n\t\tself._checkbounds(coord)\n\t\tx = np.linspace(coord[0], coord[-1], weight)\n\t\tif np.isscalar(weight):\n\t\t\tweight = np.ones(len(self.bins)) * weight\n\t\t\n\t\tweights = self._weights(x, weight)\n\t\tself._histo.fill(weights, [len(x), self.bins"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself._mark.attributes['hist_edges'] = (False, 0)\n\t\t# Don't use 'length' or 'width' attributes?\n\t\tself._mark.range = None\n\t\tself._mark.attributes['label'] = None\n\t\tself._mark.scale_value = (1, 1, 1)\n\t\tself._handle_style_properties()\n\t\t# Set the text and/or color we want.\n\t\tstyle_properties = {'angle': {'value': coord},\n\t\t\t\t\t\t  'fillColor': {'value': weight}}\n\n\t\tself._mark.scale_source ="}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tpass\n\ndef test_validate_labels_fail():\t\n\t\"\"\"\n\tCheck that the provided keys and keys are strings that must be a dict\n\tcontaining strings (without newlines) specifying a key that was not being\n\tcorrect.\n\n\tArgs:\n    expected_exception (str|int|str_or_regex): The expected exception being raised.\n\n    Raises:\n    ValidationError: if any of the keys or values were missing or malformed\n        from the given error.\n\n    \"\"\"\n\tassert_equal(\"key1\", \"key2\", \"values must be strings or an instance of dicts or strings"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\traise ValidationError(errors.messages)\n\n\t__validate_single_label(labels)\n\t#_validate_optional_label(labels)\n\t\n\tfor key, value in labels.items():\n\t    if not value:\n\t \t__validate_label(key, messages.REQUIRED_MSG)\n\t\t__validate_flag_value(key, value, messages, \"missing one of the keys: {}\".format(labels.keys()))\n\t\tcontinue\n\t    if not re.match(keys[key], value):\n\t\t__validate_label(key, messages.REQUIRED_MSG)\n\t\t__validate_flag_"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\treturn\n\n\ttry:\n\t\timport re\n\t\tres = re.match(\n\t\t\tr'^\\W+(.+)$',\n\t\t\tlabels[\"$$\"],\n\t\t\tre.IGNORECASE\n\t\t)\n\t\tres = bool(res)\n\t\tif not isinstance(res, bool):\n                raise ValidationError({\"$$\":[res]},labels[\"$$\"])\n\t\treturn\n\t\t# This is a bit hacky, but the goal is to remove the key=\"invalid\"\n\t\t# message from dict() by removing the error key=\"valid\" message.\n\t\t# Using \"None\" avoids having to use both `value"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\n        _validate_key_type(labels, list)\n\n\n        for key in labels:\n          assert key in labels_schema.get(\"properties\", {}).get(\"label\", {}), \\\n                    \"Label '{}' is not a valid key in the schema\".format(key)\n        for label in labels.values():\t\t\t\t\t\t\n            _validate_key_type(labels, label)\n\t\t\n            # For each of key/value pairs, check if the value is a valid regular expression\n            for key, value in labels_schema.get(key).items():\n                try:\n                    assert value\n                except AssertionError:\n                    continue\n                else:\n                   "}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t_validate_labels(labels)\n\t\t\n        \"\"\"\n        keys = set()\n        regex = re.compile(r'\\s+')\n\t# Keys should be lowercase for database-specific key names; values\n        # need to start with a letter\n        for k, v in labels.items():\n            if isinstance(v, unicode):\n                    labels[k.lower()] = v.translate(_ASCII_TRANSLATIONS)\n                else:\n                    labels[k.lower()] = re.sub(regex, r'\\g<1>', v)\n\n        # values should be uppercase for database-specific key names; otherwise they're\n        #"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\nclass BaseValidator(object):\n    \"\"\" Base class of all validators.\n\n    You may initialize a validator with a validator.\n    Then call the ``validate`` method with one or several optional arguments.\n\n    You may also use a ``strip`` method that normalizes validation\n    errors.\n    \"\"\"\n    def __init__(\n            self,\n            prefix=None,\n            message=None,\n        ):\n        self.prefix = prefix\n        self.message = message\n        self.messages = []\n\n    def __call__(self, value):\n        for message_text in self.messages:\n            message_text = message_text.format(**self"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\n    # Raise an exception if the keys and values cannot be matched by a regular\n    # expression.\n\ndef validate(value):\n    \"\"\"\n    Validates that the given value is a string. Raises a ValidationError if an\n    exception is raised during the validation process.\n\n    An example check on whether a string is an object which inherits from dict\n    and does not allow any object-oriented checks.\n    \"\"\"\n    if istext(value):\n        if not isinstance(value, str):\n            raise ValidationError((\n                \"'{value}' must be a string: '{type}'\".format(\n                    value=value, type=type("}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\nclass AttributeConfig(Enum):\n    \"\"\"\n    A configuration variable to control the behaviour of a `with` keyword in a\n    configuration file's ``__option_name__`` method.\n\n    There are no restrictions on the values of names in this dict (so for example\n    for setting the \"mandatory\" option to a non-mandatory key). So a\n    configuration variable *has* to be created when we write a configuration file\n    with this name. The values given in the configuration file are the names of\n    the options used when constructing the attribute, or just a list of the\n    different options.\n\n\n    Attributes:\n        mandatory (bool):"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\traise ValidationError(list(labels.keys()), descriptions=labels)\nfrom. import db\n\n\ndef initialize_app(app: 'flask_restful.flask_restful_1_0'):\n\n    class User(db.Model):\n        id = db.Column(db.Integer, primary_key=True)\n        username = db.Column(db.String(64), unique=True)\n        password_hash = db.Column(db.String(64))\n\n    class Post(db.Model):\n        id = db.Column(db.Integer, primary_key=True)\n        title"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\ttry:\t\n\t\t\tfor key, val in labels.iteritems():\n\t\t\t\tassert key in (\"key1\", \"key2\", \"value\"), \"Label keys don't match in\"\n\t\t\tassert \"value\" in labels, \"Label values don't match in: \" + labels\n\t\t\tfor key, value in labels.iteritems():\n\t\t\t\tif value!= config.labels[key]:\n\t\t\t\t\tmsg = \"{} ({})!= {}\".format(key, label_str(key), label_str(value))\n\t\t\t\t\traise ValidationError(msg)\n# (c) 2012-2014, Michael DeHaan <michael."}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn [\n\t\t\tr'(--resources)', '(?=[^/^$])']\n\n\t#--resources--\n\t@staticmethod\n\tdef get_options():\n\t\treturn [\n\t\t\t\tmake_option('-m', '--manager'),\n\t\t\t\tmake_option('-u', '--user', dest='username'),\n\t\t\t\tmake_option('-p', '--password', dest='password', default=None,\n\t\t\t\t\thelp='Krakoport password', action='callback'),\n\t\t\t\tmake_option('--debug', action='store_true', dest='debug',\n\t\t\t\t\thelp='Enable debug output'),\n\t\t\t\tmake"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\t\n\treturn None\n\ndef parse_resource_name(resource_name):\n\t\"\"\"\n\tParse a <resource> from name (e.g., \n\tuser/v2/user/v2).\n\t\n\tIf this is a valid resource name, the corresponding field is \n\tset to the resource name, otherwise the given resource name is \n\tset to `None`.\n\t\n\tEach resource name may also be an integer (e.g., 1) that begins \n\twith a digit or '-' and must end with a letter.\n\t\n\tIf the resource name includes a group number, it is used"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\ndef _get_resource_regex(resource_name):\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\ndef is_pattern(regex):\n    \"\"\"\n\tCheck wether the regex is a valid Raven resource name.\n\n    Arguments:\n    regex -- the regular expression that is to be checked.\t\n\t'''\n    return True if not regex else False\n\ndef is_resource_regex(resource_regex"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\n\treturn re.compile(r'^([^/]+)/(.*)$')\n\n\ndef validate_krake_resources(client, resource_name):\n\t\"\"\"\n\tValidate the named resource name (aka the resource ID) and throw out an\n\texception if the resource is not found.\n\t\n\t\"\"\"\n\tfor resource in _get_resources(client):\n\t\tif resource.resource_id == resource_name:\n\t\t\traise KrakenResourceNotFoundError(resource_name)\t\n\t\telse:\n\t\t\tprint >> sys.stderr, 'WARNING: Resource \"%s\" not found'\n\n\ndef"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\n\tif not PY3K:\n\t\t__from_default__ = builtins\n\n\t\tdef __RegexReplace(match1, match2):\n\t\t\tif not match1: return False\n\t\t\tif not match2: return False\n\n\t\t\t# First handle the regular expression\n\t\t\ttry:\t\t s1 = match1.group(\"value\")\n\t\t\t\ts2 = match2.group(\"value\")\n\t\t\t\tn = string.atoi(s1) + string.atoi(s2)\n\t\t\t\ta1 = \"\"\n\t\t\t\ta2 = \"\"\n\t\t\t\tfor ch in a1 + string.split("}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\t# Use re.escape to escape the name\n\t\t# with re.sub('&', '\\\\\\\\\\\\', name) to\n        # match any characters appearing without a \\:\n\t\tpattern = r\"[uU]?[rR]?\"\n\t# Replace unicode strings with escaped spaces\n\tpattern = re.escape(pattern)\n\treturn pattern\n\nclass KrakeResource(object):\n\t_template_default = \"Krake\"\n\n\t\"\"\"\n\tHelper class to access a krake resource.\n\t\"\"\"\n\tdef __init__(self, resource_name):\n\t\tself."}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t # Note: the regular expressions here are very simple, but\n        # this does not allow the use of this regex to search for a resource's\n        # namespace, e.g., re.compile(\"namespace/(.*)\", re.IGNORECASE).\n    _nsp_regexes = [ re.compile(\"^(.*?)^(\\w+)$\") ]\n\n    def _is_valid_glob(resource_name):\n        \"\"\"\n        Takes an arbitrary string and if it is a regular expression\n        to validate it as well as it is a proper resource name.\n\n        Args:\n            resource_name: the full name of the resource to validate.\n\n        Returns"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\tdef sub(string, variable):\n\t\t\"\"\"\n\t\tCreate a regular expression for a variable value in the string\n\t\tvar (string): the current regular expression variable.\n\n\t\t\"\"\"\n\t\treturn re.compile(r\"%s\" % variable, re.VERBOSE + re.IGNORECASE)\n\n\tdef validate_resource_name(resource_name):\n\t\t\"\"\"\n\t\tValidates that the resource name contains only characters that\n\tare not allowed in the Krakon dataset name, and, if the\n\tname does not appear within a valid regular expression,\n\treturns the string're' where"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn get_full_regex(\n\t\t\t[_get_resource_name_regex(),  _get_resource_name_regex()])\n\n\ndef _get_resource_name(name, resources_key_type, collection_key_type, \n\t\t\t\t\t\t\tcollection_name, unique_key_type):\n\t\"\"\"\n\tGenerate the actual resource name for the Krake resource name.\n\t\n\tNote: This is a utility function that must be added to the user\n\tobject, but must be defined for ALL API endpoints to be able to perform\n\tthe action for this api.\n\n\t\tParameters"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\tpass\t\n\t# Regular expression pattern which will match all captured\n\t# resource references (as specified in the regex above)\n\tres_pat_pattern = (\n\t\tr\"\"\"\n\t\t\t\t^\n\t\t\t\t\t# Regular expression that can detect and disambiguate\n\t\t\t\t\t# and capture the resource name.\n\t\t\t\t\t[ \\t]*\n\t\t\t\t\t# [^@].\t\t\t#\n\t\t\t\t\t# Note that the capture group will capture all captured\n\t\t\t\t\t# capture groups including capturing groups.\n\t\t\t\t\t((?:[a-zA-Z_0-9_]+\\.)*)\n\n\t\t\t\t\t# Name can contain anything.\t"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\tif not _python_version and isinstance(value, unicode):\n\t\tif not all([_match_re(r\"^(?![\\dA-F]{3})[\\w_]+(?<!\\d)\", str(value)) for value in value_list]):\n\t\t\traise ValidationError(_(\"'%s' is not a valid value. Valid values are %s.\") % (value, ', '.join(\"'%s'\" % n for n in value_list)))\n\n\ndef validate_values(values):\n\t\"\"\"\n\tValidate the given values against the corresponding regular expression.\n\n\tAn optional list of valid values are"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    if not value or validate_value_length(value, 16):\t# 0...255\n\traise ValidationError(\"Value should be between 6 and 16\", \"value\")\n\ndef validate_type(value):\t# validate the given value against the corresponding regular expression.\n\t# TODO: validate length of value\n\tfrom frappe.model.document import Document\n\tif (not value or document_has_matches(doc=value, field='document_type')):\n\t\traise ValidationError(\"Invalid document_type '{0}'\".format(value))\n \ndef validate_length(value, fieldname, message="}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\tif not re.match(_regex, value):\n\t\traise ValidationError(\"Not a valid value: %s\" % value)\n\n\ndef check_and_create_unique_email(value, model_class):\n\t\"\"\"\n\tValidate the given value against the unique email address associated with the model's model field.\n\n\tThis function requires that a unique email address be assigned to an instance of the model's model class, and\n\tthe unique email address must be associated with the existing model class.\n\t\"\"\"\n\n\tif value and len(re.findall(_regex, value)) > 0 and"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef is_number(value):\n\t\"\"\"\n\tTest if the given value is a number.\n\t\n\tArgs:\n      value (str): the value to validate\n\t\n\tReturns:\n      (bool) True if the value represents a real number.\t\"\"\"\n\t\n\ttry:\n\t\tfloat(value)\n\texcept ValueError:\n\t\treturn False\n\t\t\n\treturn True\n\ndef is_time(value):\n\t\"\"\"\n\tTest if the given value is a time.\n\t\n\tArgs:\n      value (str): the value to validate\n\t\n\tReturns:\n      (bool) True if the value represents the given"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    if not re.match(pattern_validate_regular_expression, value, re.VERBOSE):\n\t    raise ValidationError(\"Value '%s' does not match regular expression: '%s'\" % (value, pattern_validate_regular_expression))\n\ndef length(number):\n\t\"\"\"\n\tReturns the number of digits represented by the number of. \n\t\"\"\"\n\n\treturn number.rstrip('0').rstrip('.')\n\n\ndef decimal_separator(value, thousands_sep):\n\t\"\"\"\n\tTruncation function to remove thousands separators from value\n\t\n\t:param value: string, float or integer\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef get_valid_filename(value):\n\t\"\"\"\n\tGets the filename component from a given path and validates that it is a valid filename\n\tthat exists.\n\n\t:param value: the path to the file to validate\n\t:return: the the component of the given filename, if valid\n\t:rtype: string\n\t\"\"\"\n\tvalid_filename = False\n\tfor extension in validate_extensions:\n\t\tvalid_filename = validator.validate(value + extension)\n\t\tif valid_filename:\n\t\t\treturn value\n\tif not valid_filename:\n\t\tvalid_filename ="}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    if __debug__:\t\t__M_DBG(\"%s - validate_value(%s)\" % (__name__, value))\n\n    if len(value) < 1:\n        raise ValidationError(\"invalid %s value\" % value)\n\n    if type(value) is not str:\n        raise ValidationError(\"invalid %s value (type=%s)\" % (value, type(value).__name__))\n\n    if re.search(r'[^0-9a-zA-Z_]', value):\n        raise ValidationError(\"invalid %s value (too many characters)\"% value)\n\n    if not re.match(r'^[-\\.\\"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n    # Make sure, python considers None, this will throw this error.\n\tif value is None:\n\t\t# The empty string is not a valid value. So you can't give a traceback.\n\t\traise ValidationError(\"Your value is None.\")\n\ndef validate_email(value):\t\"\"\"\n\tValidate the given email address against the corresponding regular expression.\n\t\n\tArgs:\n       value: the string to validate\n\nRaises:\n    ValidationError\n        If the email address is invalid.\n\t\"\"\"\n\tif not validate_regex(value):\n\t\traise ValidationError(\"Invalid email address\")\n\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\ndef validate_key(key):\n    \"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string representing the key being validated\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\ndef validate_length(length):\n    \"\"\"\n\tValidate the given length against the corresponding regular expression.\n\nArgs:\n    length: the string to validate\n\nRaises:\n    ValidationError: if the given length is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\ndef validate_min_length(value):\n    \"\"\""}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\tif not re.match(r\"^[-]+$\", value):\t# strip leading or trailing whitespace\n\t\treturn False\n\n\ttry:\n\t\tvalidate_value(value)\n\texcept ValidationError:\n\t\t# don't validate anything!\n\t\treturn False\n\telse:\n\t\treturn True\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nAUTHOR = 'Konstantin Lepa\u00ebl'\nSITENAME = '2016Saturday'\nSITEURL = 'http://blog.csdn."}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    if re.match(\".*[a-zA-Z_][a-zA-Z_0-9]*$\", key):\n        raise ValidationError(\"The passed key must contain at least one letter and one digit\")\n\n\ndef create_password_for_file():\n    \"\"\"\n    Validate the 'passwd' file password against the format required by the\n    :class:`pyramid.security.PasswordSet`\n\n    Returns:\n      the string password for the password field\n    \"\"\"\n    passwd = None\n    with open('passwd') as f:\n        if password_re.search(f.read()):\n            passwd = f.read()"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\n\tmatch = re.match(REGEX_KEY_VALIDATION, key)\n\t\n\tif match:\n\t\treturn (True, match.group(1))\n\n\tif len(key) > len(VALID_KEYS):\n\t    return (False, \"\")\n\n\treturn (False, validate_key(key))\t\n\n\ndef main():\n\tprint(\"Validating key\")\n\n\tinvalid_patterns = []\t\n\tfor k, v in VALID_KEYS.items():\n\t    try:\n\t        validate_key(v)\n\t    except ValidationError:\n\t        print(\"Error with key: \" + v +"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\tif not isinstance(key, basestring):\n\t\traise ValidationError(\"The given key does not appear to be a string value\")\n\t\t\n\tkeyPattern = keyPattern.lower()\n\tif not keyPattern in _keyValidators:\n\t\traise ValidationError(\"The given key is not valid for this format: %s\" % key)\t\n\t\n\t_keyValidators[keyPattern](key)\n\ndef validate_key_name(key_name):\n\t\"\"\"\n\tValidate the given key name against the corresponding regular expression.\n\t\n\tArgs:\n    key_name: the string to validate\n\nRaises"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    # validate the key\n    try:\t\t\n       key.rstrip('s')\n    except UnicodeDecodeError:\n        raise ValidationError(\"Invalid key specified: %s\" % key)\n    # validate the pattern\n    key = re.compile(key.replace('$', '\\*').replace('\\\\*','*'))\n    if not all(c in string.hexdigits for c in key.upper()):\n        raise ValidationError(\"Invalid key specified: %s\" % key)\n    return key\n\n\nclass Validator(object):\n    \"\"\"\n    Abstract base class for validators.\n\n    Subclasses must define the following attributes:\n\n        key:      The validation key for"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    if not validate_regex(key):\n        raise ValidationError(\"Invalid key.\")\n    if '/' in key:\n\t\t# If the regex matches an absolute path, then the key has to be at most 3 characters.\n\t\t# See check_match_regex.\n\t\traise ValidationError(\"{} is not in valid format.\".format(key)) \n    try:\t\n\t\tre.compile(key)\n\t\treturn True\n\texcept re.error as e:\t\n\t\traise ValidationError(\"Invalid key: {}\".format(e.args[0]))\n\texcept TypeError:\n\t\treturn False\n\t\n\n\ndef validate_regex"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_item(item):\t\n\t\"\"\"\n\tValidate the given dictionary against the corresponding regular expression.\t\n\nArgs:\n    item: dictionary\n\t\n\t* The dictionary to validate is assumed to contain the following key:\n\t\n\t    * id: the ID of the item\n\t    * value: the value of the item\n\t\n\tReturns: True if the key is valid, False if the item is invalid.\n\n\t    False otherwise.\t\"\"\"\n\t\n\tfor name, regex in dict_pattern.items():\n\t\tname = name.replace('\\\\','')\n\t\tif not re.search(regex, name):\n\t\t\t"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    if not re.fullmatch(KEY_REGEX, key):\n        raise ValidationError(\"The key supplied doesn't seem to be a regex!\")\n\t\n\tif len(key) < 10:\n\t\traise ValidationError(\"The provided key is too short!\")\n\t\n\treturn key\n\ndef is_valid(key_list):\n\t\"\"\"\n\tCheck to see if each key is valid by using the VALID_KEY_REGEX format.\n\t\n\tArgs:\n        key_list: the list of keys to check\n\n\tReturns:\n\t\treturn False if all the conditions are fulfilled,\n\t\t\tTrue if all the values"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t# TODO: Add error checking\n\t\ndef _validate_string_length(string, length):\n\t\"\"\"\n\tCheck if the given string is a valid string.\n\t\n\tArgs:\n\t\tstring: the string to validate\n\t\tlength: the length of the string\n\t\n\tRaises:\n\t\tValidationError: if the string should contain at least two identical whitespace values.\n\t\"\"\"\n\n\tstring_value = string\n\tmatch = _REGEX[LENGTH_REGEXP].match(string_value)\n\tif not match:\n\t\traise ValidationError(\"String '%s' is not valid.\" % string_value"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n# Test whether the string has sufficient characters\ndef validate_string(string):\t\n\tif len(string) < 100:\n\t\treturn False\n\tfor ch in string.split(';'):\n\t\tif not validate_char(ch):\n\t\t\treturn False\n\treturn True\n\t\t\n# Verify the given string contain some characters in it or not\n# that we are expecting.\ndef validate_string_contains_characters(string):\n\tif not check_capitalized(string):\n\t\treturn False\n\tfor ch in string:\n\t\tif not ch in '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ$*+/"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\n\t# TODO: Add more validation tests for boolean regex checkers. TODO: Add more validation tests for other types of patterns (like a string or a list).\n\t# TODO: Add possible validation tests for numeric values (including, fractions, etc).\n\treturn validate_regex.validate(key, key)\n\n\ndef validate_regex(key, key_pattern):\n\t\"\"\"\n\tValidate the given regex against the corresponding regular expression.\n\tRegex is composed of:\n\t\t^[a-z0-9_](?:[-\\\\.A-Za-z0-9]+/)*\n"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\ndef generate_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tTogether with :func:``generate_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n\t the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\tdef update_existing_observer_meta_properties():\n\t\tif not isinstance(self.observer_meta, dict):\n\t\t\traise ValueError('observer_meta must defined as a dictionary')\n\t\tfor field in self.observer_meta:\n\t\t\tif not isinstance(field, dict):\n\t\t\t\traise ValueError('observer_meta key \\'{}\\' is a dictionary'.format(field))\n\t\t# get field values from manifest_dict\n\t\tif 'fields' in manifest_dict:\n\t\t\tfield_dict = manifest_dict.get('fields', {})\n\t\t\t# get all fields values from previous_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t \n\tdef generate_initial_observer_object(observer_obj_name, properties):\n\t\t\"\"\"\n\t\tCreates a new observer_type object to be used during the creation of\n\t\ta bundle.\n\t\t\"\"\"\n\t\n\tdef generate_nested_observer(observer_object):\n\t\t\"\"\"\n\t\tThis function creates a new nested observer with the specified parameters.\n\t\t\n\t\t:param observer_obj_name (str): Name of the resource that contains the observer description.\n\t\t:param properties (dict): Properties associated with the observer\n\t\t\"\"\"\n\t\tname = '%s._%s' % (observer_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tmanifest_lines = manifest_dict['lines'][:]\n\t\tfor line in manifest_lines:\n\t\t\ttry:\n\t\t\t\tline = line.split(':')[0]\n\t\t\t\tif first_level and line.strip().startswith('{'):\n\t\t\t\t\tcontinue\n\t\t\t\tresult, schema = line.split(':', 1)\n\t\t\t\tresult = result.strip()\n\t\t\t\tresult = result.replace('{', '').replace('}', '')\n\t\t\t\tschema = schema.strip()\n\t\t\t\tschema =''.join(filter(None, schema.split(' ')))\n\t\t\t\tif result!= schema:\n\t\t\t\t\tschema = result\n\t\t\t\tresult ="}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\tif first_level:\n\t\tfirst_level = manifest_dict\n\n\tif 'actors' in manifest_dict:\n\t\tactors = manifest_dict['actors']\n\telse:\n\t\tactors = None\n\n\tres_schema = {\n\t    'allOf': [actors],\n\t    'oneOf': [],\n\t    'choiceOf': [],\n\t    'allOfClass': [actors],\n\t    'object': [],\n\t    'array': [],\n\t   'map': [actors],\n\t    'file': [],\n\t   'mapFromManaged': [actors]\n   }\n\n\n\t# If this is a list of a single level dictionary"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\t\n\t# Get metadata (flattened and sorted by the level of nesting)\n\tmetadata = {'created_utc': str(generate_time()),'metadata': {'created': '1970-01-01T00:00:00'} }\n\t# Copy metadata from manifest file\n\tcopy_dict(manifest_dict, metadata, 'observer_schema', 'name')\n\n\t# Preserve metadata from manifest file\n\t# If a previous update is a new resource, then overwrite. Otherwise, replace.\n\tif first_level:\n\t\tif type(metadata)!= dict:\n\t\t\tmetadata = deepcopy"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n        manifest_list_with_no_no_duplicates(manifest_dict)\n\n        dict_key = manifest_list_fields(manifest_dict, first_level=first_level)\n        return generate_default_observer_schema_list(dict_key)\n\t\t\t\ndef generate_default_observer_schema_list(manifest_list):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a list of Kubernetes resource from\n    part of a Kubernetes resource.\n    \"\"\"\n\t\n        manifest_list.sort(key=lambda"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tmanifest_dict = dict()\n\t\tfor field in manifest_dict:\n\t\t\tmanifest_dict[field] = manifest_dict.pop(field)\n\n\t\tif (not first_level) or manifest_dict.get('is_list', False):\n\t\t\treturn dict(\n\t\t\t\tlist_options=dict(),\n\t\t\t\tremovable_fields=bool(manifest_dict.get('removable_fields', False)),\n\t\t\t)\n\n\t\tif ('removable_fields' not in manifest_dict) and'removable_fields' not in manifest_dict:\n\t\t\treturn"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\tdef _merge_dict(base_dict, other_dict, update=False):\n\t\t\"\"\"\n\t\tMerges two dicts into one, recursively merging one level dict. Returns\n\t\t``merged_dict`` for merging two ``other_dict`` into a new dictionary in\n\t\tthe form of ``new_dict``. If the ``update`` flag is True, a new dictionary\n\t\twill be inserted into ``other_dict``. Otherwise, ``other_dict`` will\n\t\tbe updated with the values returned.\n\n\t\tArgs:\n\t\t\tbase_dict (dict): The merged dict\n\t\t\tother_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\tmanifest = {}\n\tif isinstance(manifest_dict, dict):\n\t\t\tif not first_level:\n\t\t\t\tmanifest_dict = OrderedDict(manifest_dict)\n\t\t\tcreate_first_level_observer_schema_dict(manifest, manifest_dict, False)\n\t\telif isinstance(manifest_dict, list):\n\t\t\t\tfor value in manifest_dict:\n\t\t\t\t\tcreate_first_level_observer_schema_dict(manifest, manifest_dict, True)\n\t\t\t\tif not first_level:\n\t\t\t\t\tfor subdict in manifest_dict:\n\t\t\t\t\t\tcreate_first_level_observer_"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\n\t\tfor field in dir(observer_schema):\n\t\t\tif not hasattr(observer_schema, field) or\\\n\t\t\t\tfield.startswith('_') or\\\n\t\t\t\tfield in ['nextBuild', 'previousBuild']:\n\t\t\t\tgetattr(observer_schema, field).update(getattr(observer_schema, field))\t\t\n\treturn last_applied_manifest\n\t\ndef update_last_applied_manifest_dict_from_resp(\n\t\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t, last_applied_manifest)\n\tfor field in [\n\t\t\t'apiresourcelimit', 'apiresourcecount',\n\t\t\t'apiresourcelimitstate', 'apiresourcemaxresponsesize',\n\t\t]:\n\t\tif hasattr(last_applied_manifest_dict, field):\n\t\t\tsetattr(last_applied_manifest_dict, field, getattr(last_applied_manifest, field))\n\ndef update_last_applied_manifest_dict_from_resp(\n\t\"\"\"\n\tLike :func:``update_last_applied_manifest_dict_from_resp``, this function is\ncalled"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n    last_applied_manifest_list = [] # will hold our list here as is\n    for r in observer_schema:\n        current = {}\n        current['id'] = r\n        current['type'] = 'object'\n        current['name'] = r['name']\n        current['spec']['selector']['type'] = 'transform'\n        current['spec']['selector']['field'] ='status'\n    \n        if r['type'] == 'object' and'status' in r:\n            current['status'] = r['status']\n        elif'status' in r:\n            current['status'] = r['status']\n        \n       "}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\n\t# if not observer_schema:\n\t\n\t# \treturn\n\t\n\tupdated_last_applied_manifest = []\n\t\n\tfor d in observer_schema:\n\t\t\n\t\t# if not update last applied manifest's field list...\n\t\tif not 'last_applied_manifest' in d.keys():\n\n\t\t\tupdate_last_applied_manifest_dict_from_resp(d)\n\t\t\t\n\t\t\tupdated_last_applied_manifest.append(d)\n\t\n\t# if no fields found, then return\n\tif len(updated_last_applied_manifest) == 0:\n\t\t\n\t\treturn\n\n\tfor"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\n\tfor observedField in observed_fields:\n\t\ttry:\n\t\t\t# If the observedValue is an object, we want a list of\n\t\t\t# ManagedClusterManifest.\n\t\t\tif issubclass(observedField, ManagedClusterManifest):\n\t\t\t\tprint 'Updating %s' % observedField.name\n\t\t\t# A 'partial' copy of ManagedClusterManifest\n\t\t\telse:\n\t\t\t\t# A 'full' copy of ManagedClusterManifest\n\t\t\t\tnew_last_applied_manifest = copy.deepcopy( \\\n\t\t\t\t\t\t\t\t\t\tobservedField.manifest )\n\t\t\t\tnew_last_applied_manifest.pop"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\treturn None\n\n\t# we have multiple entries, which can be more than one field\n\tfinal_value_list = []\n\t# first, collect all the previous values, that is, values from the last\n\t# commit, followed by values from the last commit\n\n\tprevious_values = last_applied_manifest[0]\n\twhile previous_values!= {}:\n\t\tfor key, value in previous_values.items():\n\t\t\tvalue = get_finalised_json_for_each_field(key, value)\n\t\t\tif key!= 'last_applied_manifest':\n\t\t\t\t#"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\n\tif not was_empty_result(last_applied_manifest):\n\t\treturn\n\tfor entry in last_applied_manifest:\n\t\tdef update_item(key_name):\n\t\t\tif type(entry[key_name]) is int:\n\t\t\t\tentry[key_name] = int(entry[key_name])\n\t\t\telif type(entry[key_name]) is bool:\n\t\t\t\tentry[key_name] = entry[key_name].lower()\n\t\tupdate_item(\"id\")\n\t\tupdate_item(\"created_at\")\n\t\tupdate_item(\"created"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\n\tfor resp in response:\n\t\t# iterate through each key under the current field in the existing\n\t\t# responses\n\t\tfor key in resp:\n\t\t\t# for each key in the current response\n\t\t\tpayload = resp[key]\n\n\t\t\tif not payload['__applied_tag_sets__']['_applied_tags']:\n\t\t\t\t# empty key so continue\n\t\t\t\tcontinue\n\n\t\t\tfor applied_tag_set in payload['__applied_tag_sets__']['_applied_tags']:\n\n\t\t\t\t# do we have a key we currently have\n\t\t\t\tif applied_tag_set['__applied_tag_sets"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\tfor updated_field in last_applied_manifest:\n\t\tfor response_field in updated_field['value']:\n\t\t\tresponse_type = modifiedFunctions.get(response_field, '')\n\t\t\tupdate_field_value(response, updated_field, response_type)\n\t\t\tif (response_type == 'null'): # null\n\t\t\t\tlog.info(\"Found entry '{0}' which has been set to null (null)\".format(updated_field['value']))\n\n\ndef update_last_applied_manifest_dict_from_resp(response_type, response, response_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\treturn\n\n\tdef update_last_applied_manifest(self, data):\n\t\t\"\"\"\n\t\tUpdates the last applied manifest dictionary of the :py:data:`resp` for\n\t\tthe ``last_applied_manifest`` which is found in ``last_applied_manifest_list``\n\t\t\"\"\"\n\t\tfor (k, v) in data[\"resp\"].iteritems():\n\t\t\tif not self.is_valid_json_key(v):\n\t\t\t\tcontinue\n\t\t\tif not self.last_applied_manifest.get(k):\n\t\t\t\tself.last_applied_manifest[k] = v"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t\tfor observed_field in list(list(observer_schema.keys())):\n\t\t\tif observed_field not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[observed_field] = observer_schema[observed_field]\n\tupdate_last_applied_manifest_dict_from_response(response)\n\n\"\"\"\ndef update_last_applied_resource_dict_from_resp(resp):\n\t\"\"\"\n\tUpdates the dictionary with the content of ``resp`` with the value\n\tfrom the Kubernetes response\n\t\"\"\"\n\tfor field in list(resp"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\ndef update_all_observed_manifests_in_resp(last_applied_manifest=None, \n\t\t\t\t\t\t observer_schema=None, response=None):\n\t\"\"\"\n\tGiven a ``last_applied_manifest`` (dict) of ``last_applied_manifests``,\n\tupdates this response with the 'artifacts' field with the new \n\tvalues of all observed manifests in the ``last_applied_manifest`` dict. If \n\t``response`` is None than no changes are made in the response, the 'artifacts'\n\tfield will be filled from the latest response"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t )\n\n\tfor field_definition in filter_dict_changes(resp.get('last_applied_manifest', {}),\n\t\t\t\t\t\t\t\tresponse).get('annotations', []):\n\t\tfield_name = field_definition.get('name')\n\t\tfield_type = field_definition.get('type')\n\t\tfield_value = field_definition.get('value')\n\t\tif field_type == 'int' and field_value == '0':\n\t\t\tfield_value = '0'\n\t\tif field_type == 'bool' and field_value == '1':\n\t\t\tfield_value = '"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t\tfor key in firstkeys(observer_schema):\n\t\t\tsetattr(response, key, getattr(last_applied_manifest, key))\n\t\tfor key in lastkeys(observer_schema):\n\t\t\tgetattr(response, key)\n\ndef add_last_applied_manifest(observer_response):\n\t\"\"\"Adds the last applied manifest to an ``event_type`` response.\n\t\n\tArgs:\n    observer_response (dict): partial health check response from the\n    ``event_type`` API endpoint\n\t\n\tReturns:\n    dict\n\tThe resulting new ``last_applied_manifest`` added"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t\tif \"observer_meta\" in resp:\n\t\t\tpass\n\t\telse:\n\t\t\tfor key, value in resp.items():\n\t\t\t\tif key not in observer_meta:\n\t\t\t\t\tobserver_meta[key] = value\n\n\tret = {}\n\tfor key, value in observer_meta.items():\n\t\tif key in last_applied_manifest:\n\t\t\tif isinstance(value, dict) and value!= last_applied_manifest[key]:\n\t\t\t\tret[key] = value\n\t\t\telse:\n\t\t\t\tret[\"last_applied_manifest\"][key] = value\n\n\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\n\tlast_applied_manifest.update(\n\t\t{\n\t\t\t\"Version\": \"3.0\",\n\t\t\t\"Context\": {\n\t\t\t\t\t\"Pod\": \"{{pod}}{{}}\".format(\n\t\t\t\t\t\tpod=next(iter(\n\t\t\t\t\t\t\tlatest_applied_pods_as_bundle if not prev_applied_pod else (lambda x: x[\"Name\"] == prev_applied_pod),\n\t\t\t\t\t\t\tall_bundle_pods if not prev_applied_manifest else (lambda x: x[\"Name\"]!= prev_applied_manifest),\n\t\t\t\t\t\t\tlast_applied_template_name\n\t\t\t\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t, \n\t\t\"before_after_after\": None, \n\t\t\"before_after\": None, \n\t\t\"after_after\": None,\n\t\t\"after\": [], \n\t\t\"before\": False,\n\t\t\"after\": []\n\t)\n\ndef create_agent_response_by_id(\tid\t\t):\n\t\"\"\"\n\tAdds a ``CREATE_AGENT_RESPONSE`` entry for the\n\t:func:``create_agent_response_by_id`` function.\n\n\t:param id: The deployment id, in which to search the agent response\n\t\t(see :func"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\tfor observed in observer_schema.keys():\n\t\tvalue = observer_schema[observed]\n\t\tif value not in [str, None]:\n\t\t\tval = observer_schema[observed]\n\t\t\tif isinstance(val, dict):\n\t\t\t\tif not (isinstance(val, list) or isinstance(val, tuple)):\n\t\t\t\t\tval = [val]\n\n\t\t\tif not ('observed' in val or 'value' in val):\n\t\t\t\tlast_applied_manifest['observed'].append(observed)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[observed].append(value)\n\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\ndef update_last_applied_manifest_list_from_k8s(\n\t\tlatest_killed_killed_pod,\n\t\tagent_pool,\n\t\tclient,\n\t\tlast_applied_manifest,\n\t):\n\t\"\"\"\n\tUpdates an existing ``last_applied_manifest`` list from kubernetes response to\n\tthe K8s API\n\t\"\"\"\n\tmanifests_list = client.list_namespaced_pod_security_policy_spec(\n\t\tnamespace=namespace,\n\t\tapiVersion=None,\n\t\tlabelSelector='mode=podsecuritystate"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t# update in the response with the current key (this is the only key, and we can skip the rest)\n        # we only need the data in the result for the last_applied_manifest\n        # check if some fields were not present in the current response for\n        # updating\n        if 'last_applied_manifest' in response and 'observer_schema' in response['last_applied_manifest']:  # pylint: disable=E1101\n            response['last_applied_manifest']['observer_schema'] = observer_schema.copy()\n\n        current_value = response.get('last_applied_manifest', None"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    for resource in app.resources.values():\n        \ndef generate_default_event_schema(app):\t\"\"\"\n\tGenerate the default event schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom event schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default event schema\t\"\"\"\n        \n\nif __name__ == '__main__':\n    app = create_app()\n    print app.render('/', 'apps/app_main.html')\n\n\n#!/usr/bin/env python\n#"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tyield kip.data.resources.custom_observer\n\nclass KubernetesResourceHandler:\n\n    def __init__(self, pod, resource_name):\n        self.pod = pod\n        self.name = resource_name\n        self.method_spec = create_method_spec(resource_name)\n        self.method_spec.add((self.name, self.method_spec.method_name))\n\n    def check_create_method(self, model):\n        \"\"\"\n        Return true if the method should be allowed to create this resource.\n\n        Should not be overridden by subclasses.\n\n        Returns:\n            bool\n       "}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    schema = {}\n    schema[constants.V_API_KEY] = app.name\n    schema[constants.DEPLOYMENT_CONFIG_PATH] = os.path.join(os.path.dirname(__file__), 'config')\n    schema[constants.ENVIRONMENT_CONFIG_PATH] = os.path.join(os.path.dirname(__file__), 'config', '')\n\n    for env_key, env in app.krakens.environments.items():\n        if env_key in schema: schema_name = schema[env_key]\n        else: raise Exception('No default observer schema found"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\ttry:\n\t\t\t# If the application for which to generate a default observer schema is a key that\n\t\t# points to an integer, then it is passed in here.\n\t\t\tservice = app.kraken.services[self.app_name.lower()]\n\t\t\ttry:\n\t\t\t\t# If the target for which to generate the default observer schema is a unique (long)\n\t\t\t\t# identifier. We need this to specify the prefix of the node in the generated observer \n\t\t\t\t# to identify the new identifier of the key: the node name, an integer which should not change,\n\t\t\t\t# or an object"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n        # Set up an empty document schema so that it shows the namespace hierarchy\n        document_schema = get_fake_service(\"v1beta1-deploymentmanager.swagger.v1beta2.extensions\", \"v1beta1-deploymentmanager.schemas.DocumentSchema\")\n\n        # Add the appropriate schema to the document schema\n        document_schema.add_definition('object', {\n            'name': {\n                'type':'string',\n               'required': True,\n                'default': \"default\"\n            },\n            'labels': {\n                'type': 'boolean',\n               'required': True,\n                'default':"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    generate_object_schema(app.settings).type(\"KubernetesResourceObserver\", KubeResourceObserver)\n\ndef generate_object_schema(app):\t\"\"\"\n\tGenerate the default Observer API schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n\tIf the schema is not there it will be generated.\n\t\"\"\"\n\n\tobj_schema = generate_object_schema(app.settings)\n\n\tnamespace = \"service.k8s.io/v1alpha1/namespaces/default\"\n\n\tdef make_object"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\t# Create a basic \"custom_observer\" namespace for namespace\n\t\t# parameters used during deployment\n\t\tns = krb_data.ResourceKind(\n\t\t\tnamespace=u\"custom_observer\",\n\t\t\tname=u\"CustomObserver\",\n\t\t\tlabels={\n\t\t\t\t'hostname': u\"fqdn1.my.domain.com\",\n\t\t\t\t'prefix': u\"ns1.my.app.prefix\",\n\t\t\t\t'description': u\"ns1.my.app.description\",\n\t\t\t\t'app': u\"service\",\n\t\t\t\t'uid': u\"0\"\n\t\t\t},\n\t\t\tcreated"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t \n\n    @app.route('/<namespace>/<cluster>/observer')\n    def observer(namespace, cluster):\n\t\n        client = kraken.Client(ns='/'.join([app.services][namespace, cluster]), cert=app.rest_client.crt, key=app.api_client_key)\n        \n        try:\n            r = client.read('/v1/namespaces/'+namespace+'/cluster-default', recurse=False)\n            return r\n        except Exception as e:\n            traceback.print_exc()\n            return str(e)\n\n    @app.route('/<namespace>/<cluster>/observer/<task_id>')"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    # Make the output type always the same as the one in v3\n    if not hasattr(type(app), 'output') or not isinstance(app.output, dict):\n        app.output = {}\n\ndef generate_default_service_schema(app):\n    \"\"\"\n    Generate the default service schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom service schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default service schema\t\"\"\"\n\n\ndef get_yaml_config_string("}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tprint 'generating default observer schema for {0}'.format(app.name)\n\t\tresult += generate_observer_schema(app)\n\n\treturn str(result)\n\t\t\t\n\ndef get_latest_commit_from_spec(version, spec_version):\n\t\"\"\"\n\tGiven a version string, return the tag corresponding to the latest commit\n\tof the current kubelet version.\n\n\tThe version can either be a commit hash or commit message.\n\t\"\"\"\n\thash_pattern = get_spec_version_version_pattern(spec_version)\n\n\trepo = k8s"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tparams = {}\n\t\tfor i in range(0, len(out_params)):\n\t\t\tp = out_params[i]\n\t\t\tparams[i[0]] = p\n\n\t\treturn (tuple(list(zip(SQLQuery.IN_STYLE_SQL, params), ) + SQLQuery.IN_STYLE_SQL), True)\n\n\tdef to_inlinable_sql(self, *sql_params):\n\t\t\"\"\"Return the SQL query, substituting placeholders and escaping SQL commands.\n\t\tThe resulting SQL string is a combination of the placeholder placeholders and escaped\n\t\tSQL placeholders. The"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn (\n\t\t\tself._FORMAT.format(\n\t\t\t\tparam\n\t\t\t)\n\t\t\t+ tuple(\n\t\t\t\tself.items()\n\t\t\t\t+ map(\n\t\t\t\t\tstr.format,\n\t\t\t\t\tmap(\n\t\t\t\t\t\tlambda row: (\n\t\t\t\t\t\t\tNone if row[i] == _DEFAULT_VALUE else row[i],\n\t\t\t\t\t\t\trow[i]\n\t\t\t\t\t\t) for i in range(len(self.items()))\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t)+ sql\n\t\t)\n\n\t@classmethod\n\tdef loads(cls, *param_list, **params):\n\t\t\"\"\"\n\t\tConstruct a new instance"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n\n\tout_style = params is not None\n\tdb = SQLParam('db','sqlite', 'file' or 'query.db')\n\tif out_style is not None:\n\t\tout_style = in_style\n\n\tout_style = out_style if isinstance(out_style, str) else self.out_style\n\n\tconverter = converters.get(db)\n\tif converter:\n\t\tout_style, out_style_params = converter(out_style, db, out_style)\n\telse:\n\t\tout_style = ('out_style',)"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n    return _format(\"sql\", params)\n\n\ndef format_params(params):\n    return _format(\"params\", params)\n\n\ndef _format(template, params):\n    if params:\n        return template % params\n    else:\n        return template\n\ndef set_param_style(paramstyle):\n    if paramstyle:\n        paramstyle = paramstyle.copy()\n        for pname, pvalue in paramstyle.items():\n            if pvalue is not None:\n                prop = pvalue.get(\"paramstyle\")\n                if prop:\n                    paramstyle[pname] = prop\n            else:\n                paramstyle."}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\ndef params(param_spec, in_style=False):\n\t\"\"\"\n\tReturn the SQL query to use the params (including params) in-style\n\tparameters instead of the in-style parameters.\n\n\t.. Note:\n\n\t   Parameters with a default value (i.e.: :class:`None` value or as an\n       empty `dict`) are not supported for SQL params.\n\n\tParameters that should be set include the \"default\" value, and are\n\ttypically set using some pre-compiled rules such as the :meth:`in_`,\n\t:meth:`out`, :meth:`"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\ttbl_params = self._tbl_params\n\n\t\tnew_tbl_params = []\n\n\t\tfor param in params:\n\t\t\ttry:\n\t\t\t\tnew_tbl_params.append(\n\t\t\t\ttbl_params[param]\n\t\t\t\tif param in tbl_params else None\n\t\t\texcept KeyError:\n\t\t\t\tnew_tbl_params.append(param)\n\n\t\tif new_tbl_params:\n\t\t\treturn tuple(new_tbl_params)\n\n\t\treturn None\n\n\t@typecheck\n\tdef bind_params(self, *param_values):\n\t\t\"\"\"Update the parameter_values table with"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tif not isinstance(\n\t\t\t\tparams, collections.abc.Mapping\n\t\t):\n\t\t\traise ValueError('params must be Dictionary')\n\t\tout_style = params.get('out_style', 'dict')\n\t\tparams_out = params.get('params', [])\n\n\t\tif not isinstance(\n\t\t\t\tlist, collections.abc.Sequence\n\t\t):\n\t\t\tparams_out = [params_out]\n\n\t\tif len(params_out)!= len(params):\n\t\t\traise ValueError('params must have same length')\n\t\tfor param, out_param in zip(params_out, params):"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn tuple([ self._to_sql(arg, True) for arg in\n\t\t\t     map(lambda x: x.getquoted(), params.values() ]) )\n# -*- coding: utf8\n# This file is part of PYBOSSA.\n#\n# Copyright (C) 2015 Scifabric LTD.\n#\n# PYBOSSA is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tyield self.__query__\n\t\tyield ', '.join([ _format(x) for x in self.__in_style ] + [\n\t\t\t_format(x) for x in self.__out_style ])\n\t\tif self.__in_style and self.__out_style:\n\t\t\tyield ''.join([ _format(x) for x in\n\t\t\t\t[ _format(x) for x in self.__out_style ] + [\n\t\t\t\t\t_format(x) for x in self.__in_style[-1:] ]\n\t\t\t\t])\n\t\tyield ''.join([\n\t\t\t'INSERT INTO' +"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn self.sqlparams.format(**params)\n\n\t@property\n\tdef sqlparams(self) -> dict:\n\t\treturn self._sqlparams\n\n\t@property\n\tdef params(self) -> dict:\n\t\tresult = {}\n\t\tfor key in self.queryables:\n\t\t\tresult[key] = self.params.get(key, self.params.getdefault(key))\n\t\treturn result\n\n\tdef insert(self, **kwargs: dict):\n\t\t\"\"\"Inserts a new SQL query, returning the resulting SQL query.\n\t\t\"\"\"\n\t\tif self.sqlparams"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn tuple(tuple(\n\t\t\t(field,\n\t\t\t\t_formatone(table[field], value, self, in_style)\n\t\t\tfor (field, value) in params.items()\n\t\t))\n\t\t\tfor table, (value, in_style) in tables.items()\n\t\t)\n\t)\nfrom django import forms\nfrom django.utils.datastructures import SortedDict\n\nfrom django_q.forms import DjangoForm, ModelChoiceField\nfrom floppyforms.fields import (\n    CommaSeparatedUserField, CommaSeparatedUserField_User, DateRangeField,\n    Time"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tif params == {}:\n\t\t\traise ValueError(\"At least one parameter or a sequence parameter must be given\")\n\t\tif isinstance(params, dict):\n\t\t\tparams = tuple(params.iteritems())\n\t\tout_style = {}\n\t\tfor parm in [\"sql\", \"many_params\", \"many_params_iter\"]:\n\t\t\tif parm in params:\n\t\t\t\tkey = re.match(r\"(\\*|_)(%s\\*|_)\" % parm, key, re.I)\n\t\t\t\tdefault = params[parm][1]\n\t\t\t\tif isinstance(default, (list,"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn (\n\t\t\tsuper(SQLQuery, self).format(*(params or ())).encode(\n\t\t\t\t_params_as_sqlite(params))\n                for params in (self.iter_params(),)\n\t\t\t)\n\ndef format_as(\tas_):\n\t\"\"\"\n\tFormat one SQL query into one or more queries in the specified format.\n\n\t *as_ format\n\t\t  The SQL formatted string with parameters in *as*, formatted\n\t\t  for the given *as_ format* with each of the named parameters.\n\n\t *as_ ****\n\t\t *one or more query parameters (as a keyword argument"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\traise NotImplementedError, \"SQLAlchemy dialect cannot handle sequence types\"\n\n\n\t# TODO: the below function should probably be a method in :class:`list` or the corresponding\n\t#       Python :class:`dict` are not implemented yet.\n\t# Note that Python's standard behavior is not realy useful for SQL,\n\t# so it is not possible to just use ``list()`` or ``dict()``, as they could be\n\t# more flexible.\n\n\t# -------------------------------------------------------------------------\n\n\t# -------------------------------------------------------------------------\n\t# Internal SQL dialect methods\n\n\t# -------------------------------------------------------------------------\n\n\t@abc.abstractmethod\n\tdef execute(self, sql"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\ttry :\n\t\t\tres = []\n\t\t\tfor k,v in (params or []):\n\t\t\t\tv = _to_bytes(v)\n\t\t\t\tassert _is_hex(v)\n\t\t\t\tv = _to_unicode(v)\n\t\t\t\tassert _is_bytes(v)\n\t\t\t\tself._encode_param(v)\n\t\t\tdata = cdbapi.compress(_to_bytes(res))\n\t\texcept TypeError as e:\n\t\t\traise TypeError(\"{0}.{1}\".format(self.name,e))\n\t\tif params and isinstance(params, Mapping) and any(map(_to"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\ttry :\n\t\t\toutopts = self.__get_output_opts()\n\t\t\tif self.__params.is_in_style is None :\n\t\t\t\tif outopts[\"in_style\"] is None :\n\t\t\t\t\toutopts[\"in_style\"] = dict( map( lambda x: x._to_opt(options=outopts), outopts.get(\"in_style\", [] ) )\n\t\t\tdata = self.__get_params()\n\t\t\tformatted, outoptsdict = self.__format_raw( data, data_output_args=outopts[\"out_opts\"] )\n\t\texcept :"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn self._convert(*many_params, params=\";\", out_style=False)\n\n\t@property\n\tdef in_style(self) -> bool:\n\t\t\"\"\"Boolean indicating if input is in style.\n\n\t\tSee :attr:`.in_params`.\n\t\t\"\"\"\n\t\treturn self._in_style\n\n\t@in_style.setter\n\tdef in_style(self, value):\n\t\t\"\"\"Set whether ``in-style==True`` (or if it is an ordinal parameter).\n\t\t\"\"\"\n\t\tself.in_style = value\n\n\tdef _convert(self, *"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\ttry:\n\t\t\tresult = self.cursor.executemany(query, params)\n\t\texcept MySQLdb.IntegrityError:\n\t\t\traise ValueError(\"The query already exists.\")\n\t\texcept MySQLdb.ProgrammingError:\n\t\t\traise ValueError(\"The query already exists.\")\n\t\treturn result\n\nclass _BaseMySQLDB(MySQLDB):\n\n\tdef __init__(self, client):\n\t\tMySQLDB.__init__(self, client)\n\t\tself.charset = 'utf8mb4'\n\nclass _basepyMySQLDB(MySQLDB):\n\n\tdef __init__(self, database):\n\t\tMySQLDB.__init"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\n\n\tif not self.config.is_in_slave():\n\t\traise errors.InternalError('Query contains no input queries.')\n\t\n\t# Check that all of these are either valid SQL parameters, or raise\n\t# a :class:`~errors.ValueError` exception if the parameters are invalid.\n\tfor param in all_params:\n\t\tfor val in kwargs.pop(param, []):\n\t\t\tif not val:\n\t\t\t\traise errors.ParameterError(\"'%r' needs to be one of the following forms: %r\" % (param, str(kwargs)))\n\n\t# Convert every parameter supplied to one"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn []\n\n\t#def sqlargs_get_arraysize(self):\n\t#\t# TODO: we don't need to compute this for now.\n\t#\treturn 1\n\n\t#\t# SQL parameters which take an arraysize\n\n\t#\t# TODO: we'll want to support this!\n\t#\t# return ()\n\n\tdef sqlparams_len_to_arraysize(self):\n\t\t\"\"\"\n\t\t:return: the length of the arraysize (in bytes)\n\t\t:rtype: :class:`int`\n\t\t\"\"\"\n\t\treturn 1\n\n\tdef sqlparams"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\ndef make_tree(path):\n\tout = {}\n\tcur = node.ReadTree(path)\n\tfor i in cur:\n\t\tif i.type!= node.Type.OBJTREE:\n\t\t\tcontinue\n\t\tname = node.GetPropString(i, \"name\")\n\t\tdesc = i.GetPropString(i, \"desc\")\n\t\tif name in node and desc not in nchildren:\n\t\t\treturn None\n\t\tout[name] = desc\n\treturn out\n \ndef main(argv):\n\tif len(argv)!= 2 and not argv[1"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tif not path or not os.path.exists(path):\n\t\t\treturn False\n\t\tif not validateFs:\n\t\t\treturn False\n\t\tif not os.access(path, os.R_OK):\n\t\t\treturn False\n\t\treturn True\n\n\tdef convert2unix(self, strpath):\n\t\tif strpath in [ \"C:/\", \"d:\\\\d:\", \"c:\\\\d\" ]:\n\t\t\treturn convertTimes(strpath)\n\n\tdef convert2unixstr(self, strpath, convertTimes = False):\n\t\ttry:\n\t\t\tstr = convertTimes and str(convert"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\twith open(path, \"r+b\") as file:\n\t\t\tfor line in file:\n\t\t\t\tm1, m2, p0, p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, p16, p17, p18, p19, p20, p21, p2, p3p, p4p, p5p, p6p, p7p, p8p, p9p, p10"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\twarnings = False\n\t\tif self.warnings == None:\n\t\t\twarnings = True\n\t\t\tself.warnings = warnings\n\n\t\twith open(path, self.newline) as f:\n\t\t\tlines = f.readlines() # strip newlines\n\t\t\tfor line in lines:\n\t\t\t\tfor regexp in self.re_validate:\n\t\t\t\t\tif regexp.search(line):\n\t\t\t\t\t\twarnings = True\n\t\t\t\t\t\tbreak\n\t\treturn warnings\n\n\tdef write(self, path, contents):\n\t\t\"\"\"\n\tWrite text to path.\n\nParameters:\n path  - string\n contents - bytes\n\nReturns True if"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\ttry:\n\t\t\troot = ET.parse(path)\n\t\texcept ET.ParseError as err:\n\t\t\tself.logger.warning(\"Error parsing file %s: %s\", path, err)\n\t\t\tself.logger.error(\n\t\t\t\t\"There was an error parsing file %s, probably due to poorly formatted configuration file\", path)\n\t\t\treturn False\n\t\treturn True\n\n\tdef _writeConfigFile(self, path, root):\n\t\t\"\"\"\n\t\tWrite XML representation of config file in root XML definition.\n\t\t\tIf an invalid root element, raises OCFLException.\n\t"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tprint \"[Validate]:\", path\n\t\tself.report(\"path is {0}\".format(normpath(path)))\n\t\t\n\t\tif not os.path.isdir(path): return False\n\n\t\t# Ensure we have no readrights attribute from user\n\t\tself.get_nodeattr_nowritable_attribs()\n\t\tif 'READRIGHTS' in os.environ:\n\t\t\twritemsg_level(WARNING, _(\"You have readrights on this server.\"), \\\n\t\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\t\t# Must have read rights on node if required\n\t\tif self.root is"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\twarnings = []\n\t\twarnings_dict = get_warnings_dict(path, None)\n\n\t\tfor warning in warnings_dict:\n\t\t\tif os.path.isdir(path):\n\t\t\t\tpath = os.path.join(path, warning)\n\t\t\tis_valid = True\n\t\t\tif not os.path.isfile(path):\n\t\t\t\tis_valid = False\n\t\t\tif warning in [ 'is invalid', 'invalid-format', 'invalid-index' ] :\n\t\t\t\tis_valid = False\n\t\treturn is_valid\n\n\nclass PseudoSection(object):\n\t\"\"\"A"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tret = True\n\t\tif not os.path.isdir(path):\n\t\t\ttry:\n\t\t\t\tos.makedirs(path)\n\t\t\texcept:\n\t\t\t\tlogging.getLogger('omfg-logger').exception('failed to create dir %s', path)\n\t\tfor f in find_files(path, '.c'):\n\t\t\traw_lines = open(f, \"rb\").read().split(\"\\n\")\n\t\t\tfor line in raw_lines:\n\t\t\t\tif line.strip() == \"\": continue\n\t\t\t\tif re.search(\"^\\s*$\", line.strip()):\n\t\t\t\t\tcontinue\n\t\t\t\tif line"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\ttry:\n\t\t\t# first check if file exists and then cant validate:\n\t\t\to = open(path, 'rb')\n\t\texcept IOError as exc:\n\t\t\treturn False\n\t\ttry:\n\t\t\to.close()\n\t\texcept (OSError, IOError) as exc:\n\t\t\treturn False\n\t\t\n\t\t# try to read, validate, and return\n\t\ttry:\n\t\t\td = ConfigParser.RawConfigParser(allow_no_value=True)\n\t\t\td.readfp(StringIO.StringIO(valid))\n\t\t\treturn True\n\t\texcept ConfigParser.NoSectionError:\n\t\t\t# next try to"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\ttry:\n\t\t\treturn pyfs.validate(path)\n\t\texcept pyfs.fs.errors.FSError:\n\t\t\treturn False\n\tdef validate_file_permissions(self, path):\n\t\tpass\n\tdef validate_share_attributes(self, path):\n\t\traise pyfs.errors.FSError(\"Internal error\")# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn str(self)\t\n\t\t\nif __name__ == '__main__':\n\t# Run the script.\n\targs = parse_args()\n\tif args.subcommand == 'test':\n\t\tfrom tests.base_test import make_test_environment\n\t\tmake_test_environment()\n\telse:\n\t\timport unittest\n\t\ttest = unittest.TestLoader().loadTestsFromTestCase(TestStatus)\n\t\ttest.addTest(unittest.makeSuite(TestStatus)) \n\t\tunittest.TextTestRunner(verbosity=2).run(test)\n\t\t\n\t\t\n# -*- encoding: utf-8 -*-\n#\n#"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tif not self.is_valid_status(self.status):\n\t\t\treturn self.prefix\n\t\tret = self.status\n\t\tif ret is None and self.prefix and ret:\n\t\t\tret += prefix +''\n\t\twhile ret:\n\t\t\ts = ret.pop()\n\t\t\tif s:\n\t\t\t\tret += s\n\t\t\tret +=''\n\t\treturn ret[:len(prefix)]\n\n# vim: se ts=3:\nimport pytest\nimport numpy as np\nimport pandas as pd\n\nfrom..pandas_vb_common import DataFrame\nfrom..exceptions import ("}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn '{}: {}'.format('.'.join(prefix), self.validate())\n\nclass PathList(list):\n\t\"\"\"List of validator paths.\"\"\"\n\tdef validate(self):\n\t\t\"\"\"Checks if all validators are valid.\n\t\tRaises RuntimeError if not all validators are of the wrong form.\"\"\"\n\n\t\tvalid = True\n\t\tfor validator in self:\n\t\t\tif validator.valid():\n\t\t\t\tvalid &= validator.validate()\n\t\treturn valid\n\n\nclass Path(object):\n\t\"\"\"Class for path validators.\"\"\"\n\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"Inits validator.\n\n\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tif self._status:\n\t\t\treturn self._status\n\t\treturn self._prefix + prefix + self._name\n\t\n\t@property\n\tdef path(self):\n\t\treturn getfull(self._status, '/')\n\t\t\t\n\t@property\n\tdef data(self):\n\t\tif not self._data:\n\t\t\tself._data = []\n\t\treturn self._data\n\t\n\tdef add_data(self, data):\n\t\tassert isinstance(data, str), 'data should be str, not: %s' % repr(data)\n\t\tself._data.append(data.decode('UTF"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn''.join(self.VALID_TAGS) + prefix + self.OK_MARKER\n\n\tdef valid_str(self, string, prefix=''):\t\"\"\"\n\t\tReturn valid string of validation status, with optional prefix.\t\"\"\"\n\t\tif self.VALID_STRING.search(string):\n\t\t\tif self.DEBUG:\tprint 'Valid: \"' + string + '\" in \"' + self.VALID_STRING + '\"'\n\t\t\treturn string\n\t\treturn ''\n\n\tdef run_value(self, test_context, value):\t## @@@\n\t\t\"\"\"Execute command with specific value, and return"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tif prefix!= '':\n\t\t\tif prefix[-1:] == ';':\n\t\t\t\tprefix = prefix[:-1]\n\t\tstatus = self.status[0].upper()\n\t\ttry:\n\t\t\tval = Scheme._names[status]\n\t\texcept KeyError:\n\t\t\tval = None\n\t\telse:\n\t\t\tval = val[1:]\n\t\t\ttry:\n\t\t\t\tkey = val[0]\n\t\t\texcept IndexError:\n\t\t\t\tkey = None\n\t\treturn prefix + status + val + Scheme.valid_strs[status]\n\ndef status_msg(self, msg, prefix=''):\t\"\"\"\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\ts = ''\n\t\tfor x in self.status_info:\n\t\t\tif x[0] == prefix: s += x[1] + '\\t'\n\t\tif len(prefix) > 0: s += '\\n'\n\t\treturn s\n\n\tdef isValid(self):\treturn False\n\t\tfor x in self.status_info:\n\t\t\tif not self.status_info[x][0]: return False\n\t\treturn True\n\n#\n# Check if a validator is in a dictionary\n# Note: the code in validate function is complicated\n#\ndef isValidDict("}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\t# pylint: disable=invalid-name\n\t\tstatus = self._status.get(self._status_key, '')\n\t\tif prefix:\n\t\t\tstatus = status + prefix + ': '\n\t\treturn '[' + str(status) + ']'if len(status) > 0 else ''\n\t\n\tdef format(self):\n\t\t\"\"\"\n\t\tReturn string with status as list by status string.\n\t\t\"\"\"\n\t\tstring = ''\n\t\t# status\n\t\tif self._status_key:\n\t\t\tstatus = self._status[self._status_key]\n\t\telse:\n\t\t\tstatus = '"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn \"[ %s | %s ]\" % (self.status, prefix)\n\n\tdef is_valid_root(self):\n\t\t\"\"\"\n\t\tCheck if this plugin is installed or not and if not,\n\t\tif this plugin is running or not.\n\t\t:rtype: boolean\n\t\t\"\"\"\n\t\tif not self.enabled:\n\t\t\treturn _(\"Root plugin is disabled. Please check your installation.\")\n\t\treturn True\n\n\tdef check_enabled(self):\n\t\t\"\"\"\n\t\tCheck for enabled status and return True if enabled.\n\t\tUsed as callback to allow returning false\n\t\t:param"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn prefix +'status'\n\n\tdef end_validator(self, value):\n\t\tif value == 0:\n\t\t   return 'yes'\n\t\telse:\n\t\t   return 'no'\n\n\tdef start_validator (self, value):\n\t\tif value == 0:\n\t\t   return 'off'\n\t\telse:\n\t\t   return 'on'\n\n\tdef end_validator (self, value):\n\t\tif value == 0:\n\t\t   return 'no'\n\t\telse:\n\t\t   return 'off'\n\nclass Input_object(object):\n\t\"\"\"\n\tInput class for storing configuration"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tif self.success and hasattr(self,'success_log'):\n\t\t\treturn '%s %s: ok [%s]' % (prefix,''.join([status.path() for status in self.success_log]), self.success)\n\t\telif self.success:\n\t\t\treturn '%s %s: ok [%s]' % (prefix,''.join([status.path() for status in self.success]), self.success)\n\t\telif self.failure:\n\t\t\treturn '%s %s: failure [%s]' % (prefix,''.join([status.path() for status in"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn self.__str__()\n\ndef get_validation_log_str(validation_log):\n\treturn '[{0}{1}: {2}]'.format(validation_log[0], validation_log[1], summary(validation_log[1]))\n\ndef run_validation_log(log):\n\t\"\"\"\n\tRun validation log and generate JSON that can be returned by parse_output_str().\n\t\"\"\"\n\tlogs, config_paths = [], [], []\n\tfor root, _, filenames in log.items():\n\t\tfor filename in filenames:\n\t\t\twith open(filename"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"validating\" if self.is_valid else \"not validating\"\n\n\t# Validation Logging\n\t#\n\tdef get_log_text(self):\n\t\t\"\"\"\n\t\tReturn validation log as a string.\n\t\t\"\"\"\n\t\treturn \"%s %s\" % (self.get_log_timestamp(self.name), self.status_str(prefix=''))\n\n\tdef get_log_timestamp(self, suffix=''):\n\t\t\"\"\"\n\t\tReturn validation log timestamp (time since a particular time\n\t\tstamp, as returned by strftime).\n\t\t\"\"\"\n\t\treturn str(\"%"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn ('[{prefix}]\\n\\t\\t[{msg}]\\n\\t[{state}]\\n{msg}').format(\n\t\t\trepr(prefix),\n\t\t\trepr(self.state),\n\t\t\t(self.msg, self.state, self.msg))\n\n\t@property\n\tdef message(self): return self._message\n\n\t@message.setter\n\tdef message(self, value): self._message = value\n\n\tdef update(self, **kwargs):\n\t\t\"\"\"\n\t\tMerge state, state, and message using this log object.  The keyword args"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tif getattr(self,'status_count_cache', None):\n\t\t\treturn self.status_count_cache.get(\"%s_count\" % prefix, 0)\n\t\telse:\n\t\t\treturn self.status_count_cache.get(\"%s_count\" % prefix, 0)\n\n\tdef status_error_str(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn string representation of validation errors, with optional prefix.\n\t\t\"\"\"\n\t\tif getattr(self,'status_count_cache', None):\n\t\t\tstatus = self.status_count_cache['Error']\n\t\t\tif"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn '%s%s'%(str(prefix), self._status)\n\tdef print_header(self, prefix=''):\t\"\"\"\n\tPrint informational header in console.\n\t\"\"\"\n\tdef print_params(self, prefix=''):\t\"\"\"\n\tPrint informational parameters for further program (like: check, eval,...).\n\t\"\"\"\n\tdef log(self, prefix=''):\n\t\tglobal logger\n\t\tif logger is not None:\n\t\t\tlogger.log(prefix+':'+self.__repr__(), prefix=prefix)\n\t\tprefix = prefix+'*'\n\t\tself._status = 0\n\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn ''\n\n\tdef __str__(self):\n\t\tprefix = prefix or ''\n\t\toutput = [prefix + prefix + self.__status_str(prefix)]\n\t\toutput.append('----------------')\n\t\tfor (key, val) in sorted(self.__validation.iteritems()):\n\t\t\toutput.append(prefix + '%15s  %6d/%6d'%\\\n\t\t\t\t(key, val[0], len(val[0])))\n\t\toutput.append('')\n\t\treturn '\\n'.join(output)\n\n\t@staticmethod\n\tdef generate_validation_report(self):\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tif not self.success:\n\t\t\tret = ''\n\t\telif prefix!= '':\n\t\t\tret = prefix +'(' + self.success + ')'\n\t\telse:\n\t\t\tret = ''\n\t\treturn ret.__str__()\n\tdef __repr__(self):\n\t\tif self.__str__(True) == '<ValidationLog: (len ='+ str(len(self.success)) + ')' and str(self.success[:len(self.success) - 2]) == '<ValidationLog: (len ='+ str(len(self.success)) + ')'\n\t\telse:\n\t\t\treturn"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn prefix +''.join(map(str, self.errors))\n\n\tdef summary(self):\n\t\t\"\"\"\n\t\tReturn summary of validation log\n\t\t\"\"\"\n\t\tmsg = \"Validation results: \" + str(len(self.errors))\n\t\tlength = len(self.errors)\n\t\tmsg += \" (\" + str(length) + \")\"\n\t\treturn msg\n\nclass NanoValuationChecker(object):\n\t\"\"\"\n\tCheck the validity of the following values in the validation log:\n\n\t0: no error\n\t1: bad type (\"float\")\n\t2:"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tlog_format = \"[%(prefix)s:%(asctime)s %(levelname)-8s|%(name)-12s]: %(message)s\"\n\t\treturn log_format % self.__dict__\n\n\tdef error(self, prefix='', msg=''):\n\t\t\"\"\"Print error string (to standard output)\"\"\"\n\t\tif self.error_log is not None:\n\t\t\tlog_msg = \"\\n\\n[%(prefix)s:%(asctime)s %(levelname)-8s|%(name)-12s]: %(message)s\" % {\n\t\t\t\t'prefix': prefix,\n\t\t\t\t'asctime' : time"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self.VALID_IDENTIFIER_CHARS\n\n\nclass EnchantmentIdCode(_EnhancedIdentityCode):\n\t\"\"\"\n\tContains the id code for the character codes.\n\t\"\"\"\n\n\tdef is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self.VALID_IDENTIFIER_CHARS\n\n\nclass EnchantmentIdCodeCode(CodeBase):\n\t\"\"\"\n\tContains the codebase used to identify an Enchantment ID. Includes the"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self._identifiers\n\n\tdef __getitem__(self, identifier):\n\t\treturn self._identifiers.get(identifier)\n\n\tdef __contains__(self, item):\n\t\treturn item in self._identifiers\n\n\tdef __len__(self):\n\t\treturn len(self._identifiers)\n\n\tdef find(self, identifier, key):\n\t\ttry:\n\t\t\treturn self._identifiers[identifier]\n\t\texcept KeyError:\n\t\t\treturn None\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass IRModule(IRHandlerBase):\n\t\"\"\"\n\tBase class for all"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef get(self):\n\t\t\"\"\"\n\t\tReturn a string representation of this class.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef export_js(self, file_name):\n\t\t\"\"\"\n\t\tReturn string to pass to JavaScript to compute hashes.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef export(self):\n\t\t\"\"\"\n\t\tReturn bytes of compiled JS for this class.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\nclass JSONField(Field):\n\t\"\"\"\n\tA list field that wraps a Python dictionary. This is represented using a ``JSONDecoder`` object, and allows"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self._identifier\n\n\tdef identifier_to_snake(self, ident):\n\t\t\"\"\"\n\t\tConverts an identifier to snake format for a C# identifier.\n\t\tE.g. '1.1.1' to '10.1.1.1'.\n\n\t\t:param identifier: An identifier, as returned by C# identifiers.\n\t\t@type identifier: str\n\t\t\"\"\"\n\t\treturn snakecase(self._identifier)\n\"\"\"\nTest for the split_migrator and split_migrator_with_new_name_works correctly.\n\"\"\"\nimport"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef to_sql(self, schema, not_null=False, boolean=False):\n\t\t\"\"\"\n\t\tTake in a string and convert it into a valid SQL clause.\n\n\t\t:param schema: the source SQL to use to build a query against.\n\t\t:type schema: str\n\t\t:param not_null: whether a NULL value should be treated as NOT NULL. If False, a NULL value will be treated as NONE.\n\t\t:type not_null: bool\n\t\t:param boolean: whether to include a NULL value without a NONE.\n\t\t:type"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef load_configuration(self, conf_file='/etc/gcf-get-user.conf'):\n\t\t\"\"\"\n\t\tRead the configuration file and assign to this instance.\n\n\t\t@raise NotImplementedError\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef find_plugin(self, plugin_type, name=None):\n\t\t\"\"\"\n\t\tReturn a Plugin object if plugin_type is the type of plugin to instantiate.\n\t\t@param plugin_type:\n\t\t@param name:\n\t\t@return:\n\t\tThe requested plugin.\n\t\t@raise NotImportedError\n\t\t\"\"\""}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn False\n\nclass BaseConfigurationTest(TestCase):\n\t\"\"\"\n\tA test case for Base Configuration and ConfigurationDescriptor tests\n\t\"\"\"\n\t# Common test methods\n\n\tdef setUp(self):\n\t\t\"\"\"\n\t\tThis method is invoked automatically in tests, before each test method. \n\t\tSo, override it and set a class attribute named my_config.tests_base\n\t\t\"\"\"\n\t\t# Create a unique username for the username for this test \n\t\tusername = uuid.uuid4().hex[:5]\n\t\tself.username = username.lower()[:5]\n\t\t# set the user object"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn (self.__identifier is None or self.__identifier == identifier) and not self.can_execute()\n\n\t@staticmethod\n\tdef __string():\n\t\treturn \"-\".join(\"{0}:{1}\".format(part[0], part[1]) for part in sys.argv)\n\n\tdef __repr__(self):\n\t\tparts = self.__identifier.split(\"::\")\n\t\tparts = [p.replace(\"_\", \"-\") for p in parts]\n\t\treturn \"{0} ({1})\".format(self.__string(), \":\".join(parts))\n\n\tdef __eq__(self, identifier"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn len(identifier) == 128\n\n\tdef to_bytes(self, identifier):\n\t\t\"\"\"\n\t\tGenerate the string for use as filename and hash it to an absolutely readable text object.\t\n\t\t:param identifier: a string of the format ``<type>=<identifier>` for the id\n\t\t:return: ``str``\n\t\t\"\"\"\n\t\treturn \"\".join(identifier).encode(\"utf-8\")[:self.HASH_SIZE]\n\n\tdef __eq__(self, other):\n\t\t\"\"\"\n\t\tCompare two base string's fields.  If some of them are not equal (e."}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\nclass _XRefTranslator(base.TranslationUnit):\n\t\"\"\"Class that implements the `XRef` class.\"\"\"\n\tdef __init__(self, source):\n\t\t\"\"\"Create the XRefTranslator.\n\n\t\t:param source: Unused argument for this class.\n\t\t:type source: string\n\t\t\"\"\"\n\t\tassert source is not None\n\t\tself.source = source\n\t\tself.lineno = None\n\t\tself.colno = None\n\n\tdef get_lineno(self):\n\t\t\"\"\"Get the line number.\"\"\"\n\t\tassert self.lineno or self.colno"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\twith open(self.spec_version, 'r') as f:\n\t\t\t\tself.spec_version = self._parse_spec_version(f)\n\t\telse:\n\t\t\tself.spec_version = self.inventory.get('version', '')\n\n\nclass SshKey(MutableMapping):\n\t\"\"\"\n\tA `KeyValue` is an object which contains a known SSH\n\tKey, and may have SSH v1 (e.g. PuTTY on CentOS 7).  These are\n\trequired so that SSH can be used easily"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif not self.spec_version:\n\t\t\traise RuntimeError(\"Trying to validate an invalid type without providing version. Use Validate for a description of what it does.\")\n\t\tif self.spec_version[0] == 0:\n\t\t\tif (not extract_spec_version):\n\t\t\t\traise RuntimeError(\"Given data is of not type 'type:type1', not of type {1}.\".format(self.get_identifier(), type(self)))\n\t\t\tself.validate_type_1()\n\t\telif self.spec_version[0] == 1:\n\t\t\tif (not extract_spec_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tv = inventory['_meta']['hostvars'][self.current_host]['vars']\n\t\tp = v.get('ports')\n\t\tif p:\n\t\t\tp = [x.strip() for x in p]\n\t\t\tif not isinstance(p, list):\n\t\t\t\tp = [p]\n\t\t\tif len(p) < 2: return\n\t\tp = [str(x) for x in p]\n\n\t\tif self.spec_version: # spec version\n\t\t\tif not re.match('^.*?(\\d+\\.\\d+)', self.current_host):\n\t\t\t\treturn\n\t\t\tp.append"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\ttry:\n\t\t\tspecversion = self.spec_version\n\t\texcept AttributeError:\n\t\t\treturn\n\n\t\t# Verify that the type is correct\n\t\tcheck = lambda actual_version: checkPathExists(\"/srv/pulp/db/v%s/db/type\" % actual_version, inventory)\n\t\tfor version, count in scanner_scan.scan(check):\n\t\t\tself.assertEqual(specversion[0], version, \"Version %s does not match expected one (%s)\" % (version, count))\n\t\t# Verify that it's well formed\n\t\tfor version, count in scanner"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif (extract_spec_version and self.version) or \\\n\t\t\t(not self.spec_version):\n\t\t\traise Invalid(message=\"No version provided\")\n\t\tself.spec_version = validate_version_arg(self.version, self.spec_version)\n\n\tdef _get_spec_path(self, spec_name):\n\t\treturn os.path.join(self.storage.path, '%s.spec' % (spec_name))\n\n\tdef _get_spec(self, spec_path=None):\n\t\tif spec_path is None:\n\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\traise ValueError(\"'{}' is not a valid inventory version. There \" \\\n\t\t\t\t\"are two mandatory arguments: type, path, url, and \" \\\n\t\t\t\t\"query\".format(self.__class__.__name__))\n\n\tdef get_host_list(self):\n\t\treturn self._host_list\n\n\tdef is_valid_record_type(self, record_type):\n\t\tfor host in self.get_host_list():\n\t\t\tif host.type == record_type:\n\t\t\t\treturn True, True\n\t\treturn False, True\n\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self._version == 'current':\n\t\t\treturn False\n\t\tcurr_spec = self.spec_version or spec_from_line(self.line)\n\t\tnew_spec = normalize_depending_spec(self.release, self.environment, curr_spec)\n\t\tif not self.spec_version and new_spec is None:\n\t\t\tself._version = new_spec[0]\n\t\tif self.release == self.release and self.version:\n\t\t\tclosest_version = self.version_compare(new_spec)\n\t\t\tif closest_version:\n\t\t\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tversion = extract_spec_version or self.version\n\t\t# Make sure it's either a specific version or not if not using an\n\t\t# inventory format\n\t\tif inventory.version and LooseVersion(version) < LooseVersion('0.1'):\n\t\t\traise ValueError('Invalid inventory format %s' % version)\n\t\telif inventory.version:\n\t\t\tver_parts = version.split('.')\n\t\t\tif len(ver_parts)!= 3:\n\t\t\t\traise ValueError(\"Invalid inventory format %s\" % inventory.version)\n\t\t\t# Make sure they are two numbers long\n\t\t\tif ver_parts["}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.spec_version is not None and self.spec_version in ['undefined', 'absent']:\n                        msg = None\n                        if self.spec_version == '1.2':\n                                msg = 'Type \"list\" requires a \"version\" keyword argument with a list of version specifiers:'\n                        else:\n                                msg = 'Type \"dict\" requires a \"version\" keyword argument: \"%s\".' % self.spec_version\n                elif self.spec_version == '2.0':\n                        msg = 'Type \"bool\" requires a \"version\" argument with a boolean value.'\n                else:\n                        msg ="}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.spec_version is None:\n\t\t\treturn True\n\t\t# Don't validate pre_obj_key we already know...\n\t\tif isinstance(self.pre_obj_key, string_types):\n\t\t\tif get_required_version(self.pre_obj_key) is not None and self.spec_version == 'unknown':\n\t\t\t\tif not self.parser.has_section(self.pre_obj_key):\n\t\t\t\t\tself.parser.add_section(self.pre_obj_key)\n\t\t\treturn True\n\t\treturn validate_dependencies(self.cfg"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tself._sanity_check = True\n\n\t\tif not self._check_digests_list_of_lists(manifest_files):\n\t\t\t# We're OK.\n\t\t\tself._save_digest_lists(digests_used)\n\t\t\treturn\n\n\t\ttry:\n\t\t\t# Get the digests that need to be checked.\n\t\t\tmissing_digests = digests_used - self._check_digests_list_of_lists(manifest_files)\n\n\t\t\t# Store the missing digests.\n\t\t\tself._save_missing_digests_to_manifests(missing_digest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\twith open(manifest_files) as manifest_in:\n\t\t\ttry:\n\t\t\t\tmanifest_infos = json.load(manifest_in)\n\t\t\texcept ValueError as e:\n\t\t\t\traise ValueError(\"[ERROR] Error parsing %s manifest file %s, error is %s\" % \\\n\t\t\t\t\t\t(manifest_in, manifest_infos['name'], str(e)))\n\n\t\t\tfor package_name in manifest_infos['packages']:\t\n\t\t\t\t# We have a package named in the manifest but it's not in our\n\t\t\t\t# list for checking purposes and it doesn't have any package\n\t\t\t\t# to add"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tself.error('%s manifests in manifest_files' % (len(manifest_files), ))\n\t\tfor filename in manifest_files:\n\t\t\tfilename_without_digest, length, digest = self._get_file_hashes(filename)\n\t\t\tdigests_used.add(digest)\n\t\tdigests_required = self.digests_required(digests_used)\n\t\ttotal = False\n\t\tfor digest_type, digest in digests_required.items():\n\t\t\t\tdigests, total = self._check_digests(digest, digests_used)\n\t\t\t\tdigest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\thave_digests = False\n\t\thave_archives = False\n\t\thave_archives_in_manifest = False\n\t\tfor manifest_file in manifest_files:\n\t\t\tfilename = manifest_file\n\n\t\t\t# check for the digests\n\t\t\tdigest_exists, digest_used = check_hash(manifest_file, \"SHA1\")\n\t\t\tif digest_exists == digest_used: # if digest exists\n\t\t\t\thave_digests = True\n\t\t\t\tif digest_exists == True: # if digest is used\n\t\t\t\t\thave_archives = True\n\t\t\t\t\thave_archives_in_manifest = True"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tneed_update = False\n\t\tdigests_added = False\n\t\tneeded_digests = {}\n\t\tfor digest in manifest_files:\n\t\t\tif digest == self.digest or self.fingerprint_required(digest):\n\t\t\t\t#print(str(digest))\n\t\t\t\tneeded_digests[digest] = True\n\t\t\t\tif need_update:\n\t\t\t\t\tdigests_added = True\n\t\t\t\tself.log(\"\\tChecking %s (digest %s) against %s\" % (self.fingerprint_required(digest), digest, self.manifest_digests))\n\t\t\t\tdigests_added ="}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tmanifest = get_manifest(manifest_files)\n\t\tmanifest.ensure_digests()\n\t\tdigests_used.append(digests_used)\n\t\tfound = [(name, digest) for name, digest in manifest]\n\t\tif found:\n\t\t\tself.output(\"Not going to check manifests: %s\" % (len(found), ),\n\t\t\t\t\tnoiselevel=-1)\n\t\t\treturn False\n\t\telse:\n\t\t\tself.output(\"Manifest does NOT need to be re-built\")\n\t\t\treturn True\n\ndef main():\n\ttry:\n\t\tmanifest_file ="}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfailed_keys=[]\n\n\t\tfor i,filename in enumerate(manifest_files):\n\t\t\tif filename in manifest_files[i].names:\n\t\t\t\tthis_filename = '/'.join([os.getcwd(),filename])\n\n\t\t\t\tif self.get_digest_key(this_filename)!= None and check_digests_for_new_digest(this_filename, digests_used, this_filename) is True:\n\t\t\t\t\tif this_filename in manifest_files[i].names and len(manifest_files[i].names)==1:\n\t\t\t\t\t\tif this_filename not in failed_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\twith tarfile.open(mode='r:gz', fileobj=manifest_files) as manifest:\n\t\t\tfor manifest_file in manifest:\n\t\t\t\tif manifest_file.name == config.get_manifest_list_prefix():\n\t\t\t\t\tif digests_used == manifest_file.size:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprint(\"\")\n\t\t\t\t\t\t\tprint(\"Digests used: %s  Total size is %s\" % (\n\t\t\t\t\t\t\tmanifest_file.size,\n\t\t\t\t\t\t\tconfig.get_manifest_list_size()))\n\t\t\t\t\t\t\tprint(\"Total size: %s\" %\n\t\t\t\t\t\t\t\tdigests"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tall_need_digests = set()\n\t\tused_need_digests = set()\n\t\tused_digests = set()\n\t\tif manifest_files:\n\t\t\tfor filepath in manifest_files:\n\t\t\t\tdigest = getFileDigest(filepath)\n\t\t\t\ttry:\n\t\t\t\t\tdigests_used.add(digest)\n\t\t\t\texcept:\n\t\t\t\t\tall_need_digests.add(digest)\n\t\t\t\t\tused_need_digests.add(digest)\t\n\n\t\tdef check_no_digests(digests):\n\t\t\tmissing = set(digest.file_digests[\""}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tif not manifest_files:\n\t\t\tprint \"No manifest file.\"\n\t\t\treturn False\n\n\t\tfor current_digest in manifest_files.keys():\n\t\t\tfor digest in digests_used.keys():\n\t\t\t\tif digest not in current_digest:\n\t\t\t\t\tprint \"ERROR: Unknown digest.\"\n\t\t\t\t\tprint \"    Expected digest: \" + digest + \". Expected: \" + current_digest.keys()\n\t\t\t\t\tprint \"    Reason: \" + str(digests_used.keys()).replace('\\n','')\n\t\t\t\t\tself.parser.print_help()\n\t\t\t\t\tsys.exit(-1"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tassert not isinstance(prior, Inventory))\n\n\t\tself._root.get_inventory().validate_version(prior)\n\t\tself.validate_inventory()\n\n\tdef validate_inventory(self):\n\t\t\"\"\"\n\t\tCheck that the passed inventory is a valid inventory object.\n\t\t\"\"\"\n\t\tfor ent in self.required_entries():\n\t\t\tif entry_is_valid_for_type(ent.get(\"group\")):\n\t\t\t\tif ent not in self.required_contents():\n\t\t\t\t\treturn False\n\n\t\t\tfor key, val in ent.iteritems():\n\t\t\t\tif key not in self.required_"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif self.version!= prior.version:\n\t\t\traise Error(\"Version %s does not match Specification %s.\" % (prior.version, prior))\n\tdef has_inventory(self):\n\t\t\"\"\"Is inventory initialized?\"\"\"\n\t\tif not self.inventory:\n\t\t\treturn False\n\t\t# check each member of the Inventory\n\t\tfor item in self.inventory:\n\t\t\tif item.inventory_key in self.__vars_to_check:\n\t\t\t\tself.__vars_to_check[item.inventory_key](item.inventory)\n\t\t\telse:\n\t\t\t\t# this item isn't"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tversion = self.version \n\t\tvalidate_version(version, self.valid_item_level or prior.required_version)\n\n\t\titem_level = prior.item_level\n\n\t\tif item_level is not None and item_level not in self._cache:\n\t\t\tself._cache[item_level] = None\n\n\t\treturn version\n\n\tdef update(self, value):\n\t\t\"\"\"\n\tCheck that every entry in the inventory is a valid prior version of \n\tthe current inventory.\n\t\"\"\"\n\t\tif value.item_level is not None:\n\t\t\tvalidate_inventory_"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\ttry:\n\t\t\tversion = self.get_var_value(\n\t\t\t\t\"REQUIRED\", \n\t\t\t\tself.root_name,\n\t\t\t\t\"REQUIRED\", \n\t\t\t\tallow_wildcard=True)\n\t\t\tif version!= self.version:\n\t\t\t\traise ValueError(\"invalid prior version\",\n\t\t\t\t\t[str(_x) for _x in prior])\n\t\t\t\n\t\texcept ValueError, e:\n\t\t\traise ValueError(\"invalid prior version\", str(e))\n\n\tdef validate_from_file(self, path):\n\t    \"\"\" Validate self to make sure that all essential members are valid and\n\t\tconsistent. \"\"\"\n\t\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t#raise exception if the data structure contains only an 'extra'\n\t\t#key with an 'extra' key\n\t\ttry:\n\t\t\tversion_major, sep, version_minor = prior.split('.')\n\t\texcept ValueError:\n\t\t\treturn None\n\t\t#handle special case where version is not a string. See below for that\n\t\tif type(version_major)!= type(0):\n\t\t\treturn None #this is the same as the prior.split('.')[0].split('+')[0]\n\t\tif type(version_major)!= type(''):\n\t\t\treturn None #this is a string"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif isinstance(prior, InventoryValidator):\n\t\t\tself.obj_id = prior.obj_id\n\t\t\tself.version = prior.version\n\t\t\tself.action_date = datetime.date(2000, 1, 1)\n\t\telif isinstance(prior, ItemValidator):\n\t\t\tself.obj_id = prior.obj_id\n\t\t\tself.from_date = prior.from_date\n\t\t\tself.to_date = prior.to_date\n\t\t\tself.date = prior.date\n\t\t\tself.action_date = datetime.date(2000, 1, 1)\n\t\telif"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\terror = ''\n\n\t\tif type(prior)!= type(self):\n\t\t\terror += \"\\n\" + self.get_report() + \":\\n    \" + type(self).get_report()\n\t\t\n\t\tif isinstance(prior, str):\n\t\t\terror += \"\\n\" + PriorityValidator.get_type(prior).get_report()\n\t\t\n\t\treturn error\n\t\n\tdef validate_version_requirements(self):\n\t\t\"\"\"\n\t\tReturn True if the requirements is met.\n\t\tReturn False if not.\n\t\t\"\"\"\n\t\tif len(self._check_requirements()) == 0:"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t#check if there is already a task and if so return\n\t\tif prior.isTask() or prior.version!= self.initial_version:\n\t\t\tresult = PriorityChecker.hasTask() and PriorityChecker.hasInventories()\n\t\t\tif result:\n\t\t\t\tresult = prior.doTask() and PriorityChecker.isSelfTask() and PriorityChecker.isInventories()\n\t\telse:\n\t\t\t#print 'inconsistent prior version'\n\t\t\tresult = prior.doTask() and PriorityChecker.isSelfTask() and PriorityChecker.hasTask()\n\t\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t# TODO: Currently, Priority checking is done here.\n\t\traise KeyError(\"Invalid input variable prior: %s\" % \n\t\t\t(prior,))\n\n\tdef validate_all_fields(self):\n\t\tfor f in SCHEMA_FIELDS:\n\t\t\tif f not in STANZA_PROPERTIES:\n\t\t\t\traise KeyError(\"Invalid schema field: %s\" % f)\n\t\t# For consistency, all schema field is guaranteed to be valid.\n\t\tfor f in SCHEMA_FIELDS:\n\t\t\tif f not in STANZA_PROPERTIES:\n\t\t\t\traise KeyError(\"Schema field %s"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise Exception(\"Proposed must be of type InventoryValidator, got %s\" % type(prior))\t# (was %s, did %s)'%(type(prior), type(self))\n\t\t# check that there are compatible inventory types and all of them are in the prior\n\t\tif not (isinstance(self, InventoryType) and isinstance(prior, InventoryValidator)):\n\t\t\traise Exception(\"Proposed must be of type InventoryType, got %s and %s\"%(self, prior), type(prior))\n\t\tif (self.level <"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\t    logical_files = os.listdir('/home/ansible/ansible-cli/lib/ansible-tower-cli/') \n\t    logical_paths = [os.path.join(logical_path, file) for file in logical_files if os.path.isfile(os.path.join(logical_path, file))]\n\t    # we want the logical_path_in_disk property of the inventory\n\t    \n\t    logical_paths_to_ind = {}\n\t    for i in range(len(logical_paths)):\n            \tif logical_paths[i][0] not in inventory:\n                \traise Exception"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\n\t\tphysical_path_map = {}\n\t\tf = open(os.path.join(inventory_path, \"path_on_disk\"), 'r')\n\t\tcontent = f.read().split(\"\\n\")\n\t\tf.close()\n\n\t\tfor line in content:\n\t\t\t# not support lines that include tabs. That's fine:\n\t\t\tif re.match(r'\\t', line) \\\n\t\t\tand not re.match(r'\\n', line):\n\t    \t\tcontinue\n\t\t\tif line[0] == \"{\":\n\t\t\t\tfields = line[1:].split(\"\\\""}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n    logical_path_in_files = {}  \t#: {{{1: set(...), 2: set(...),...},...}\n\ndef get_file_lines_for_role(role, line_data):\n    \"\"\"\n\tGet a generator of lines of text for each file on disk for a given role path.\n\nReturns a generator: paths of file content for each of specified role files.\n\nNote: files are not treated specially - you always use relative paths since\nit would usually be very inefficient.  \n\n\"\"\"\n    for line_number, line in enumerate(line"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tif version in inventory['_files_version_cache']:\n\t\t\treturn inventory['_files_version_cache'][version]\n\n\t\timport stat\n\t\tst = os.stat(inventory['_inventory_basedir'] + '/files')\n\t\tlast_modified = stat.st_mtime\n\t\tif last_modified >= st.st_mtime:\n\t\t \tcontent_files = inventory['_parse_manifest_file'](st.st_size)\n\t\telse:\n\t\t \tlast_modified = st.st_mtime\n\t\tinventory['_files_version_cache'][version] = {\n\t\t\t\"content_files\" :"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tinventory_path_map = {}\n\t\ttree = inventory.lookup(version)\n\t\tfor f in tree:\n\t\t\tlogical_path_in_state = f.get_logical_path_in_state()\n\t\t\tinventory_path_map[logical_path_in_state] = set()\n\t\treturn inventory_path_map\n\tinventory_path_map = {k.get_logical_path_in_state(): k.get_content_files() for k in inventory.list}\n\treturn inventory_path_map\n\ndef add_inventory_files(inventory"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\treturn inventory.get('content_files', {})\n\n\ndef update_ansible_version_map(inventory, ansible_version):\n\t\"\"\"\n\tMerge a version number with remote git hashes. The inventory\n\thas keys: \n\tversion - string (ex. 1.10.0-999b2c73-a6c9-0a80-5d0e4f5fb5be1f2)\n\t\"\"\"\n\tansible_version['update_id'] = ansible_version['version']\n\tinventory.update(ansible_version)\n\n\n# Global inventory is"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tdisk_path_map = {}\t\t# {file path: set(content_files) }\n\n\t# Initialize result data structure.\n\tresult_data = {}\t\t# Key  => {logical path: set(logical file hash)}\n\n\t# Parse the inventory\n\tfor directory_name, file_names in inventory.items():\n\t\tfor file_name in file_names:\n\t\t\tlogical_path = get_path(directory_name, file_name)\n\t\t\tif logical_path:\t  # Only save data from root directory?\n\t\t\t\ttry:    # Try to"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tfile_map = {}\n\t\tfor f in inventory[\"files\"]:\n\t\t\tif not \"include\" in f:\n\t\t\t\tcontinue\n\n\t\t\tfile_size = os.stat(f).st_size\n\t\t\ta = open(os.path.join(f))\n\t\t\ttry: a.close()\n\t\t\texcept OSError:\n\t\t\t\treturn\n\t\t\tif file_size > file_map[f][\"size\"] or f in inventory[\"merge list\"]:\n\t\t\t\tfile_map[f] = {\n\t\t\t\t\t\"content_files\": a.read(),\n\t\t\t\t\t\"size\": file_size\n\t\t\t\t}"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t                       \n\tfiles = {}\n\tfor version in inventory.versions():\n\t\t# Use the latest file for the specified version\n\t\tversion_files = inventory._list_latest_files('v'+version)\n\n\t\tfor file in version_files:\n\t\t\t# Get the inventory file for a given version\n\t\t\tinventory_file_version_file = inventory._get_inventory_file_for_version(version_files, file)\n\t\t\t# If the inventory file still exists, return it\n\t\t\tif inventory_file_version_file:\n\t\t\t\tfiles.update({\n\t\t\t\t\t   inventory_file_version"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tpaths = inventory['_meta']['hostvars'][version]['search_paths']\n\t\tlogical_paths = paths_to_files(paths, version)\n\t\treturn dict((k, v) for k, v in logical_paths.items() if v not in inventory['virtuals'].keys())\n\treturn None\n\ndef files_for_version(version, inventory, file_type='any'):\n\thosts = {}\n\n\t# Resolve any symlinks\n\tif'symlinks' in inventory['_meta']['hostvars'][version]:\n\t\trelpaths = inventory['_meta']['hostvars'][version]['symlinks']\n\t\t\n\t\t"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor manifest_file in manifest_files:\n\t\t\tfpath = os.path.join(manifest_source_dir, manifest_file)\n\t\t\tif not os.path.exists(fpath):\n\t\t\t\traise SafeException(\"Manifest file '%s' not found!\" %manifest_file)\n\t\t\tif not self.fixity.is_valid(fpath):\n\t\t\t\traise SafeException(\"Invalid fixed fixity block %s.\" %fpath)\n\nclass FatalError(StandardError):\n\t\"\"\"\n\tRaised when an exception occurs during the application loading code.\n\t\"\"\"\n\tdef __init__("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\t# check for non-empty list items\n\t\tmissing_items = []\n\t\tif not os.path.exists(manifest_files):\n\t\t\tmissing_items.append('EMPTY_LIST')\n\t\t# check for missing manifests\n\t\tmanifest_plist = os.path.join(os.getcwd(), manifest_files)\n\t\twith open(manifest_plist, 'rb') as f:\n\t\t\tmanifest = Manifest(f)\n\t\t\tfor finfo in manifest.files:\n\t\t\t\tif not finfo.flags.has_key(\"MF\") or not finfo.flags[\"MF\"]:"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor f in self.files:\n\t\t\tdata = self._get_data(f)\n\t\t\thash = hashlib.md5(data)\n\t\t\tif manifest_files is not None:\n\t\t\t\tfor manifest_file in manifest_files:\n\t\t\t\t\tif manifest_file.hash == hash:\n\t\t\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tfor file in manifest_files:\n\t\t\t\t\tif file == f:\n\t\t\t\t\t\treturn True\n\n\t\treturn False\n\n\tdef update(self, files):\n\t\tsuper(InventoryPacker, self).update(files)\n\n\t\tfor file in files:\n\t\t\tself"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor file in manifest_files:\n\t\t\tif not check_file_exists(file):\n\t\t\t\traise SafeException(_(\"File is not exists.\"))\n\n\tdef apply(self):\n\t\t\"\"\"\n\t\tHandle the fixity block.\n\t\tFill out any missing files (missing files that do not correspond\n\t\tto any fixity block). Return a list of the errors and the list of\n\t\terrors that occurred during the process.\t\"\"\"\n\t\tvalidation_results = []\n\t\tfixity_paths = self.fixity_paths()\n\t\tfor file in self.files():\n\t\t\ttype = get_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor _, section in fixity.items():\n\t\t\tsections = self.expand_sections(section)\n\t\t\tsections = validate_fixity_files(sections, manifest_files)\n\t\t\tif len(sections)!= 0:\n\t\t\t\tself.fatal(\"One or more sections referenced in %s section\" % section)\n\n###\n### Misc functions\n###\n\n\tdef warning(self, msg, *args):\n\t\tself.fatal(\"WARNING: %s\" % msg, *args)\n\ndef error(self, msg, *args):\n\t\tself.fatal(\"ERROR: %"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tdef walk(node, dirname, fnames):\n\t\t\twhile node:\n\t\t\t\tnode = node.parent\n\t\t\t\tpath = posixpath.join(dirname, node.name)\n\t\t\t\tif os.path.isfile(path):\n\t\t\t\t\tfname = os.path.basename(path)\n\t\t\t\t\tif fname == self.manifest.manifest_file:\n\t\t\t\t\t\treturn node\n\t\t\t\t\telif fname in fnames and \\\n\t\t\t\t\t   manifest_files.get(fname) is None:\n\t\t\t\t\t\tnode = node.child(path)\n\t\t\t\t\t\tbreak\n\t\t\t\tnode = node.parent\n\t\twalk(self.root, manifest"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tvalid_fixity_block = None\n\t\ttry:\n\t\t    if fixity == \"source\":\n\t\t        try:\n\t\t            valid_fixity_block = self.get_asset_fixity_block_definition()\n\t\t    elif fixity == \"asset\":\n\t\t        valid_fixity_block = self.__get_asset_fixity_block(manifest_files)\n\t\t    else:\n\t\t\traise AssetsException(\"Unknown fixity block definition: \" + fixity)\n\t\t    except AssetsException:\n\t\t    \traise NoFixityBlockSetError\n\t\t    return valid_fixity_block\n\t\texcept No"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor file_name in manifest_files:\n\t\t\tif file_name == 'fixity_block_file':\n\t\t\t\tfor file_entry in manifest_files[file_name]['entries'] or []:\n\t\t\t\t\tif file_entry['path'] not in fixity:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif 'digest' not in file_entry['digest']:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif file_entry['digest']['digest']!= fixity[file_name]['digest']:\n\t\t\t\t\t\tprintfile('Error: Archive file \"%s\" does not match the digest of this file (\"%s\").' %\n\t\t\t\t\t                      ("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tblocks = [block for block in self.blockinfo if block in manifest_files]\n\n\t\tif len(blocks)!= 1:\n\t\t\tself.error(\"File contains multiple blocks\")\n\t\t\treturn {}\n\n\t\traise Exception(\"Found one broken fixity block\")\n\n\t\"\"\"\n\tProcess the fixity block\n\n\tReturn the fixed fixity block with relative path\n\t\"\"\"\n\tfrom. import fixity_blocks\n\tfor block in fixity_blocks:\n\t\tblockpath = block.getRelFilename()\n\t\trelativepath = blockpath[len(blockpath.strip(\"/\")) +"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor file_path in manifest_files:\n\t\t\tif file_path.endswith(self.filetypes.file_exts[0]):\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef calculate_fixity(self):\n\t\t\"\"\"\n\t\tCalculate the path of the fixity file.\n\t\tWill return empty string if the file is empty.\n\t\t\"\"\"\n\t\treturn ''\n\n\tdef isfixity(self):\n\t\t\"\"\"\n\t\tCheck if the fixity file contains all of the contents.\n\t\t@return: (bool)\n\t\t\"\"\"\n\t\treturn len(os.listdir(self"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn path.iterdir()\n\n\t# walk\n\tfor dir, subdirs, files in walk(path, topdown=True):\n\t\tfor name in files_list(dir):\n\t\t\tyield name, path.join(dir, name)\n\n\ndef files(path, filters=None, topdown=False):\n\t\"\"\"Returns a list of `File` objects for given path\"\"\"\n\tpath = os.path.join(path)\n\tfiles = []\n\tfor name in files_list(path):\n\t\tif filters:\n\t\t\tfor ffilt in filters:\n\t\t\t\tif"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tresult = []\n\t\tfile_list = os.listdir(path)\n\t\tfile_list.sort()\n\t\tfor f in file_list:\n\t\t\tif os.path.isfile(os.path.join(path, f)):\n\t\t\t\tresult.append(f)\n\t\treturn result\n\t\n\t\"\"\"\n\tReturn files in specified path.\n\t@param[in]   path   \tstring\n\t@return \t\tlist of files in specified path\n\t\"\"\"\n\tresult = files_list(path)\n\n\treturn result \n\t\t\n\ndef sort_substrings(s):\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tfiles = []\n\t\tfor i in os.listdir(path):\n\t\t\tif os.path.isfile(os.path.join(path, i)):\n\t\t\t\tfiles.append(os.path.join(path, i))\n\t\treturn(files)\n\t# END files_list\n\n\ndef main():\n\t# get_files()\n\t# files = get_files()\n\t# print files\n\t# print type(files)\n\t# print files.sort('key')\n\t# sys.exit(0)\n\n\tindex_file = open(\"index2.txt"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tfiles_base = os.path.dirname(path)\n\t\tfiles = []\n\n\t\t# recursively get files\n\t\tfor (dirpath, dirnames, filenames) in os.walk(files_base):\n\t\t\tfiles += [os.path.join(dirpath, f) for f in filenames if f == 'yaml']\n\n\t# filter files by those extensions\n\tfiles = [filename for filename in files if filename[0]!= '.'and (filename[1:] not in valid_extensions) and filename[0:2]!= 'ini.ini']\n\t# sort alphabetically\n\tfiles ="}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [n.split('/')[-1] for n in walktree(path)]\n\n\ndef get_files_in_path(path, files=[]):\n\t\"\"\"\n\tReturns all the files in path\n\t\"\"\"\n\treturn [f for f in walktree(path) if isfile(join(f, f)) and f.endswith(FORMAT_EXT)]\n\n\ndef get_files(path, files=[]):\n\t\"\"\"\n\tReturns all the files in path\n\t\"\"\"\n\treturn [f for f in listdir(path) if isfile(join(f, f)) and f.endswith("}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tif not os.path.exists(path): \n\t\t\treturn []\n\t\treturn filter(lambda x: x.endswith([\".tmp\", \".Trash\"]), os.listdir(path))\n\tfiles = map(lambda path: filter(is_protected, glob(path + \"/*\")), os.listdir(path) ))\n\tfiles += map(lambda x: os.path.join(path, x), filter(is_protected, files))\n\treturn files\n\ndef zip_folder(path, zip_file):\n\t\"\"\"\n\tzip_file => [(prefix, zip_path"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [file.split(\"/\")[-1] for file in listdir(path) if os.path.isfile(file)]\n\ndef fileSize(filename):\n\tfile = open(filename, 'r')\n\tfileSize = path.getsize(filename)\n\tfileContent = file.read()\n\tfile.close()\n\treturn long(fileSize)\n\ndef downloadUrl(url, dest):\n\t\"\"\"\n\tDownload a file a chunk to a given URL.\n\t@param url: url\n\t@type url: string\n\t@param dest: destination path (including"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\ttry:\n\t\t\tfiles = []\n\t\t\tfor entry in os.listdir(path):\n\t\t\t\tfile_path = os.path.join(path, entry)\n\t\t\t\t# no need to look further down\n\t\t\t\tif os.path.isdir(file_path):\n\t\t\t\t\tself.files.extend([file_path])\n\t\t\t\telse:\n\t\t\t\t\tfiles.append(file_path)\n\t\t\treturn files\n\t\texcept OSError:\n\t\t\tprint('Error calling list of files:'+ path, file=sys.stderr)\n\t\t\treturn []\n\t\t\tpass\n\n\tdef load_user_feed("}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [f for f in listdir(path) if isfile(join(path, f))]\n\treturn []\n\nclass filehandler():\n\tdef __init__(self):\n\t\tself.files = []\n\tdef write(self,text,line_break=u'\\n'):\n\t\tself.files.append(text)\n\tdef flush(self):\n\t\tself.files = []\n\tdef close(self):\n\t\tself.files.append(self.backup_filename)\n\tdef make_backup(self):\n\t\tfd,name = tempfile.mkstemp()\n\t\t"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tfrom frappe.utils import cint\n\t\treturn [(file.path, file.file_name) for file in cint(frappe.utils.get_file_items(path))\n\t\t\tif file.file_name and file.file_name.endswith(\".json\") and not file.has_permission(\"read\"))]\n\ndef save_file_by_path(path, file_data, folder=None, is_private=None):\n\tfrom frappe.utils import scrub_urls\n\n\tif not folder: folder = path.split('/private/', 1)[-1]\n\t"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n    def check_package_name(name, source):\n        if not isinstance(source, str):\n            return True\n        if not re.match(r\"^[0-9a-zA-Z_\\.\\-\\+]+$\", name):\n            return True\n\n        for f in files:\t\t\n            if f.name == name: return True\n        return False\n\n    tree = etree.parse(source)\n    root = tree.getroot()\n    root.set('name', check_package_name(source, root_dir))\n\n    packages = root.find('packages')\n    if not packages:\n        return dict()\n\n    for"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t \n\t \n\t\ndef save_package_to_disk(filename):\n    \"\"\"\n    Creates and saves a disk file at filename\n    \"\"\"\n    import os\n    import tarfile\n    import shutil\n    from xml.etree import ElementTree\n    from os.path import isfile\n    import tarfile\n\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    tarfile.open(filename, 'w:gz').extractall(path=filename)\n    \n    \ndef create_template_dict(filename):\n    \"\"\"\n    Creates a dictionary with the keys 'name','version', 'authors' in the XML template\n    \"\"\"\n    import"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n       \n    # Extract data out of data file\n    _data = {} \n\n\n    # Extract data out of files\n    for name in files:\n        _path = os.path.join(source, name)      \n        if os.path.exists(_path):\n            print name\n        print _path\n        _data[name] = open(os.path.basename(_path),'rU').read()\n    \n    return _data\n        \n        \ndef _parse_and_save_packages_txt(path):\n    \"\"\"\n    Parse and save packages txt file\n    \"\"\"       \n    # Open xml files\n    print 'Read XML data file:', path"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tif xmls == None or isinstance(xmls, list):\n\t\t\txmls = [xmls]\n\t\telse:\n\t\t\txmls = xmls.split(',')\n\n\t\t# Get a list of unique elements\n\t\tkeys = [item.keys() for item in xmls]\n\t\tunique_list = [list(i) for i in zip(*keys)]\n\n\t\t# Build a list of output filenames\n\t\tdata = []\n\t\tfor f in files:\n\t\t\t# Skip files that are not.xml.ext\n\t\t\tif f.endswith('.xml'): continue\n\t\t\tif f.endswith"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\t# TODO: Implement better error handling\n\tkeys = ('packages', 'package-version', 'files', 'files-checksum')\n\tdata = {}\n\tfor key in keys:\n\t\tif key not in xmls:\n\t\t\tdata[key] = []\n\t\tif isinstance(xmls[key], list):\n\t\t\tel_xml = open(xmls[key[0]])\n\t\t\tdoc = {}\n\t\t\tel_xml = el_xml.read()\n\t\t\tel_soup = BeautifulSoup.BeautifulSoup(el_xml, \"lxml\")\n\t\t\troot = el_soup.find"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tdata = {}\n\t\tfor item in xmls:\n\t\t\txml_name = item.get('name')\n\t\t\tdata[xml_name] = item.text['filename']\n\t\treturn data\n\ndef _group_files_by_pkg_name(source, files):\n\t\"\"\"\n\tGroup files by their package name\n\nGroups files by their package name and returns data in dict format.\n\nParameters\n----------\npkg_name : str\n    Package name\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: package name\n\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t return {'source':     source,\n\t\t\t'xmls':      xmls,\n\t\t\t'files':     files}\n\ndef _group_files_by_package_filename(source, root, basename, files):\t\"\"\"\n\tGroup files by their package basename. Also returns (extracted) data in dict format.\n\n\tParameters\n\t----------\nxml_filename : str\n        XML filenames\nroot : str\n        parent folder path of XML data in the archive\nbasename : str\n        package basename\nfiles : list\n        list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tfiles_by_ext = []\n\t\tfor x in xmls:\n\t\t\tif not os.path.basename(x):\n\t\t\t\tpass\t\t\t\t\t\t\n\n\t\t\twith zipfile.ZipFile(x, \"r\") as zipobj:\n\t\t\t\tfiles_by_ext.append(str(zipobj.namelist()))\n\t\n\t\tret = dict(files_by_ext)\n\t\treturn ret\n\n\ndef _make_dirs(d, parent_dir):\n\t\"\"\"\n\tAdds sub directories of a directory tree\n\t\"\"\"\n\tfor dirname, subdirs, files in os.walk(parent"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\t# initialize default dictionary 'names'\n\t\t# if package has a 'xml_filenames' key, use that\n\t\t# otherwise use the names of the files\n\tname = \"\"\n\t\n\t# get xml_filename key\n\tif len(xml_filename) > 0:\n\t\tname = xml_filename[0].text\n\t\tname = _get_xml_name_from_extension(name)\n\t\t\n\t\t# group files by basename, name and content; this takes the first\n\t\t# letter of filename and strips off the extension\n\t\tfor group in files:\n\t\t\t# group['xml_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tfilename_pattern = re.compile(filename)\n\t\tfor fxmlfile in xmls:\n\t\t\tif filename_pattern.search(fxmlfile):\n\t\t\t\tdata = get_file_metadata(fxmlfile)\n\t\t\t\tdataset = None\n\t\t\t\tif 'filename' in data:\n\t\t\t\t\tfiles.append(os.path.basename(data['filename']))\n\t\t\t\telse:\n\t\t\t\t\traise BTLEException(\"File '%s' not found in file list %s.\"% (str(fxmlfile), str(files)))\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise BTLEException"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\n\t\ndef match_file_by_pattern(pattern):\t\"\"\"\n\tIdentify if a `file_pattern` belongs to a document package by a given `pattern`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\npattern : str\n    Pattern to match against files\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\t\ndef check_subdirs(subdirs):\n\tfor subdir in subdirs:\n\t\tif not os.path.isdir(subdir):\n\t\t\tcontinue\t\n\t\t\n\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn os.path.isfile(file_path)\n\treturn os.path.isdir(file_path) and os.listdir(file_path) == [file_path]\n\ndef match_file_by_suffix(suffix, file_path = 'content'):\n\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `suffix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nsuffix : str\n    Filename suffix\nfile_path : str\n    File path\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t#TODO: Implement.\n\treturn _is_document(file_path)\n\ndef match_file_by_mime_type(mime_type, package_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `mime_type`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nmime_type : str\n    File basename or file extension\npackage_path : str\n    File path\nReturns\n-------\nbool\n    True - document belongs to the package\t\"\"\""}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\n\tfilename_regex = r'.*_\\d+.js' #TODO make this more robust\n\ttry:\n\t\tjs_compiled = re.compile(filename_regex) #find.js file\n\t\tmatches = js_compiled.findall(file_path)\n\texcept:\n\t\tmatches = []\n\t\n\t\n\tfor match in matches:\n\t\tfile_name = 'docx' + str(match_num) + '_' + file_path.split('/')[-1]\n\t\tif 'o1' in match:\n\t\t\tis_o1 = False\n\t\telse:\n\t\t\t#if ("}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t# Checks if file path is a valid file for the package\n\t\tif os.path.isfile(file_path):\n\t\t\t# Recognizing file\n\t\t\t# Check file extension\n            # Check file name extension\n\t\t\tif file_path[-3:]!= \".pacote\" and file_path[-4:]!= \".pacote\":\n\t\t\t\tfile_extension = \".\" + file_path[-3:]\n\t\t\t\tfile_name_extension = \".\" + file_extension\n\t\t\t\tmatch = re.match(prefix + file_name_extension, file_path)\n\t\t\t\treturn match\n\t\t\telse:"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn bool(re.match(\"[^(]+)\\\\.(js|jpe|png|svg)\", file_path))\n\n\t# Si la existent informativo a pesquisa a aporta!\n\tif not prefix in file_paths:\n\t\treturn False\n\t\n\t# Verifique o tipo da carpeta foto\n\ttry:\n\t\tpath_list = os.listdir(file_path)\n\texcept:\n\t\treturn False\n\n\t# Si no pesquisa encontramos, apenas que o pesquisa dent"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\tpass\n\n\ndef read_package(path, recursive=True, ignore_package_name=False):\n\t\"\"\"\n\tRead a package and return the first record, optionally recursive\n\tparameter.\n\t\n\tParameters\n\t----------\n\tpath: str\n\t String with absolute path on file system containing package\n\t\t\n\trecursive : bool, optional\n\t\tCheck the directory is an (unordered) list of directories within the \\\n\t\tcurrent folder\n\tignore_package_name : bool, optional\n\t\tIf False, skip extracting the package name using the file name; default \\\n\t\tis false (by default it is"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t# check existence of file in database\n\t\tif os.path.isfile(file_path):\n\t\t\treturn True\n\t# check, folder and file exist\n\telif os.path.splitext(file_path)[1].lower() in ['.docx','.txt','*txt','*mdt']:\n\t\treturn True\n\t# check, file or folder exist\n\telse :\n\t\t# check existence of file in doc and docx folder\n\t\tfor document in os.listdir(doc_path):\n\t\t\tif os.path.isfile(os.path.join(doc_path,document"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\timport re\n\t\ttry:\n\t\t\t# Create an absolute file path on the local machine:\n\t\t\t# - the name must start with `prefix` if the file starts with `prefix`\n\t\t\t# - end with `prefix` if it does not\n\t\t\tfrom os import path\n\t\t\tfull_path = os.path.abspath(file_path)\n\t\t\tbase_name = path.basename(full_path)\n\t\t\tfile_path = path.join(prefix, base_name.replace(prefix + \"{\", \"\"))\n\t\t\twith open(file_path, mode='rb') as fp:"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\n\t# Get the total size\n\ttotal_size = 0\n\tfstream = open(file_path, 'r')\n\n\tfor name in fstream.readlines():\n\t\tfstream.seek(0)\n\t\tline = fstream.readline()\n\t\tif name == prefix:\n\t\t\tcontinue\n\t\ttotal_size += int(match_file_by_prefix(name, file_path).st_size)\n\n\tfstream.close()\n\treturn total_size \n\n#########################################\ndef load_packages(dir_path):\n\t\"\"\"\n\tLoad the package into"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n        res = map(lambda p: p.lower(), filter(_is_document_dir, files))\n        return list(set([ f.split('/')[-1] for f in res]) )\n\n\ndef select_document_filename(file):\n    \"\"\"Select filename in a document package.\n\n    Paramters\n    ---------\n    file : str\n        Silent file name\n\n    Returns\n    -------\n    path : str\n        Path to given filename if document package extension is \".doc\".\n    \"\"\"\n    \n        if \".\" not in file:\n            return file\n        if not file.split('.doc')[1] in document_extensions:\n            return file"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\tif len(sys.argv) < 2:\n\t\tprint(\"Usage: python3 select_filenames_by_prefix.py <folder_name> <files_directory>\")\n\t\texit();\n\tfor folder in sys.argv[1:]:\n\t\tfiles = get_files_by_prefix(folder, files)\n\t\n\tfiles = file_find_all(files)\n\t\n\tfor f in files:\n\t\tprint(f.__file__)\n# -*- coding: utf-8 -*-\n# Generated by Django 1.9.4 on 2016"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tfiles_glob = os.path.join(prefix, files)\n\t\tfiles = glob(files_glob)\n\t\tfiles.sort()\n\t\treturn files\n\n\ndef select_filenames_by_file_ext(file_ext, files):\t\"\"\"\n\tGet file names which belong to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `file_ext`\n\nParameters\n----------\nfile_ext: str\n    Filename extension of files\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n\n   "}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\t#recoge a list\n\t\tfiles_list = files\n\t\tfiles_list = [re.sub(r'([\\w_\\.]+)+$', '', f) for f in files_list]\n\t\tfiles_list = [l for l in files_list if re.search(prefix+'_'+l, files)]\n\t\treturn files_list\n\n\nclass File:\n\n\t@staticmethod\n\tdef newfile(in_loc, out_loc, sep=' '):\n\t\t\"\"\"\n\t\tCreates the necessary directories/extensions for a new file.\n\t\tIn case you are using ebook,"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tfor filename in fnmatch.filter(files, '*.py'):\n                yield filename[len(prefix):]\n\ndef select_unique_files_from_list(filenames, ext='txt'):\t\t#filelist=[e.split('.py')[0] for e in ['py', 'txt', 'pyxt']]\n\tlist=[]\n\tfor filename in filenames:\n\t\tif os.path.splitext(filename)[1] in ext:\n\t\t\t\tlist.append(filename)\n\treturn select_filenames_by_prefix(files, list)\n\n\n# Copyright (c) 2012- The Bitcoin Core"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\treturn filter(lambda name: prefix in name, files)\n\t\t\n\nfrom __future__ import unicode_literals\n\nfrom django.db.models.sql import compiler, util\nfrom django.db.models.sql.constants import MULTI, MULTIPOLYGON, SINGLE\nfrom django.db.models.sql.datastructures import Join\nfrom django.db.models.sql.where import AND, OR, Constraint\nfrom django.db.models.sql.where import ORFilter\nfrom django.utils import six\nfrom django.utils.encoding import force_text\nfrom django.utils.tree import"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n        for i_file in xrange(0, len(files)-1):\n                file_name = prefix + '*' + files[i_file] \n\n                if fnmatch.fnmatch(file_name, '*'+files[i_file]+'*'):\n\t\t        return files[i_file], files[i_file+1]\n\n                else:\n                    return files[i_file], files[i_file+1]\n\n\n    @staticmethod#\n\tdef get_filenames_by_prefix(prefix, files):\t\n        return [file for file in glob.glob(prefix + '*'+files) if os."}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tif prefix is None:\n\t\t\tfiles = files\n\t\tfiles = os.sep.join(files)\n\t\tfiles = [f.split(os.sep)[-1] for f in files]\n\tfiles = [ os.path.basename(f) for f in files]  # trim trailing '/'\n\n\tif prefix is None or len(prefix) == 0:\n\t\treturn files\n\telif len(prefix) == 1:\n\t\tif prefix == os.path.basename(files[0]):\n\t\t\tfiles = [os.path.basename(f) for f in files]\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\n\t# Obtenemos todas as lista pelo usuarios\n\tfiles = _file_list(files, prefix)\n\tif isinstance(files[0], str):\n\t\tfilesString = files\n\telse:\n\t\tfilesString = [files]\n\tfiles = _concatenate(filesString)\n\n\t# No meio no mesos dos arquivos.\n\t\n\t# Encontradas\n\tfiles_temp = _join(files, \"temp\")\n\n\t# Para cada arquivo que no se encontro nada\n\tif os_path.exists"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\t# se forma do quem remover o nome do arquivo j\u00e1 os arquivos do usu\u00e1rio\n\t\tlist=[]\n\t\tfor file in files:\n\t\t\tfile_split = file.split('\\\\',1)\n\t\t\tif len(file_split) > 1:\n\t\t\t\tlist.append([file,file_split[0]])\n\t\treturn list\n\ndef select_files_by_suffix(suffix, files):\t\"\"\"\n\tGet files which belongs to a document package\n\nRetorna os arquivos da lista `files` cujos"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tjson_doc = xml_doc_from_folder(folder)\n\t\tparsed_data = json_doc['name']\n\t\treturn parsed_data\n\n\n\nif __name__ == '__main__':\n\n\tdata = _get_package_data()\n\n\n\tprint(\"Got %s package data.\" % data['name'])\n\tprint(\"Found %s profiles.\" % len(data['profiles']))\n\n\tprofiles = _explore_profile_folders()\n\tprint(\"Processed %d profiles in %s.\" % ( len(profiles), data))\n\tprint(\"Looking for valid profiles matching"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tpackage_dict = get_files()\n\t\tdatadict_folder = {}\n\t\tfor path, dirs, files in os.walk(folder):\n\t\t\tfor fname in files:\n\t\t\t\tif fname.startswith(package_dict['xml_basename']):\n\t\t\t\t\tfile_path = os.path.join(path, fname)\n\t\t\t\t\tdatafile = open(file_path)\n\t\t\t\t\txml_data = xml.dom.minidom.parseString(file_data.read())\n\t\t\t\t\troot = xml_data.getElementsByTagName('package')[0].childNodes[0].data\n\t\t\t\t\tif root in xml_data."}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\n\t# read in xml\n\txmlfile = get_xml_element(folder + \"package_files/packages.xml\", True)\n\t# read in data dictionaries\n\t# dict format is:\n\t# name\t\tname of the package\n\t# path\t\tpath of folder \"packages\"\n\tdict\t= {}\n\n\t# get each folder entry\n\tfor entry in xmlfile.getiterator(\"entry\"):\n\t\t# skip root node\n\t\tif entry.get(\"id\") == \"\" or not entry.get(\"path\"):\n\t\t\tcontinue\n\n\t\t# get each packages"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\t# read tree by serial number,\n\t\t# extract xml data of files and extract data from folder,\n\t\t# create dictionary for output\n\n\txml_file \t= [a for a in [file for file in os.listdir(folder) if file.startswith(folder + '/')] if\n                 file.endswith(\".xml\")]\n\t\n\t# set basename of.xml if found\n\txml_file \t \t= xml_file[0] if xml_file else \".\" + xml_file[1]\n\n\txml_basename\t= [file for file in os.listdir(folder) if file"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tfile_list = os.listdir(folder)\t\n\t\tfile_list.sort()\n\t\treturn {'folder': '/'.join(file_list)}\n\ndef explore_package(url):\n\t\"\"\"\n\tCheck the package url exists and is valid\n\nParameters\n----------\nurl : str\n    URL to be checked\nReturns\n-------\nbool: True if package is valid\n    \"\"\"\n\t\n\tpackage_file = None\n\ttry:\t\t\t\n\t\tfile_extension = os.path.splitext(url)[1]\n\t\tfile_extension = file_extension.lstrip(os.sep)"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\t# parse folder\n\t\twith open(os.path.join(folder, \"packages.xml\"), 'rt') as f: \n\t\t\t\tpkgs = f.read()\n\n\t\t# parse pkgs\n\t\tpkgs = _parse_xml(pkgs)\n\t\treturn { 'packages':pkgs }, pkgs\n\n\texcept:\n\t\tsys.stderr.write(\"Failed to parse folder {folder} at {root}\\n\".format(**locals()))\n\t\traise\n\n\n\n\ndef _parse_xml(pkgs):\n\t\"\"\"\n\tExtract data from XML packages metadata elements and return a dictionary of the dicts\n\t\"\"\"\n\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\tresult = read_list(folder)\n\tpackages = {}\n\twhile not result == None:\n\t\tresult = read_list(result[\"folder\"])\n\t\tif result!= None:\n\t\t\tpackages[result[\"package\"]] = result[\"dir\"].replace(result[\"package\"], \"\")\n\t\t\tresult = read_list(result[\"folder\"])\n\n\treturn packages\n\n\ndef getPackages(name):\n\t\"\"\"\n\tGet all packages listed under 'name' and returns a dict containing names and files in dicts format.\n\nParameters\n----------\nname : str\n    Package to be returned\nReturns\n"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tfolder_doc = io.open(folder, 'r', encoding='utf-8')\n\tpackage_list = []\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor file in files:\n\t\t\tfile_path = '/'.join([root, file])\n\t\t\tname_split = file_path.split('.')\n\t\t\tif len(name_split) > 2:\n\t\t\t\tfile_path = '/'.join([folder_root, name_split[1]])\n\t\t\t\tfile_list = read_txt_file(file)\n\t\t\t\tfor item in file_list:"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\t# Get filename\n\t\tfilename_base_folder = os.path.basename(folder)\n\t\tfilename_file = filename_base_folder + \".yaml\"\n\t\tif not os.path.exists(filename_file):\n\t\t\treturn {\"Error\": \"The specified file does not exist\", \"Success\": \"Not Found\", \"No files to parse\"}\n\t\n\t\t# Get list of packages (recursive)\n\t\tos.chdir(folder)\n\t\tdict_packages = {}\n\t\tos.system(\"ls -alt %s\" % filename_file)\n\t\ttree = ET.parse(\"%s."}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n        re_regex = re.compile('(?P<pkg>(\\w+\\.\\w+)\\s+)$', re.M)\n        return _regex.findall(open(folder, 'rt').read())\n\ndef get_packages(packages, rootdir):\n    \"\"\" \nFind all packages matching the pattern specified\n\n\tParameters\n\t----------\npackages : dict\n\t\tKey: package name (e.g. 'gentoo-2018-01-01')\n\n\trootdir : string\n\t\tPath to search for package and store paths\n\n\tReturns\n------\ndict_list(dict or list)\n    \"\"\"\n       "}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n        ret = {}\t\t\t\t\t# dict with se o dicion\u00e1rio do arquivo em arquivo no dicion\u00e1rio do pacote\n\twith open(prefix + '_precoq.xml', 'rt') as f:\n\t\tfor i in f:\n\t\t\ti = i.rstrip()\n\t\t\tif i:\n\t\t\t\tret[i] = xmltodict.parse(i)\n\t\t\t\t# ret[i] = str\n\t\t\t\t# ret[i]['archivo'] = str\n\t\t\t\t# ret[i]['elenco'] = str\n\t\t\t\t# ret"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tpass\nfrom django import forms\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom oscar.core.loading import get_classes, get_model\n\nLine = get_model('order', 'Line')\nShippingAddress = get_model('order', 'ShippingAddress')\nPaymentEventType = get_model('order', 'PaymentEventType')\nRefund, DiscountCode = get_model('order', 'Refund')\nProductMode, ProdWithdrawal, Free = get_classes('shipping.methods',\n                                                           ('FixedPayment', 'FreeShipping"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tos.system(f'''\n\tcp --recursive {dir_name}/ \\\n\t{dirname}\n\t\t{prefix+filename}\n\t\t{file_folder}\n\t\t{file_path}\n\t\t')\n\treturn 0.0\n\ndef _eval_folder(prefix, folder_path):\n\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `package` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no ende"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tif not os.path.isdir(file_folder):\n\t\t\tos.makedirs(file_folder)\n\n\tfilename = prefix + filename\n\tfile = open(filename, \"r\")\n\tstr_data = file.read().encode('utf8')\n\tprint \"nome do arquivo e treniendo do arquivo\", str_data\n\tfile.close()\n\treturn str_data\n\ndef do_create_arquivo_file(archivo_pdf):\n\t\"\"\" Fun\u00e7\u00e3o que possui o arquivo e fazer um"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\t# print 'file_path =', file_path\n#\tassert os.path.exists(file_path)\n\tfile = open(file_path, 'r')\n\tall_files = file.readlines()\n\tfile.close()\n\tos.remove(file_path)\n\t# print all_files\n\tprint all_files\n\tfile_list = []\n\tfor file in all_files:\n\t\tfile_path = file.strip()\n\t\twith open(file_path, 'r') as f:\n\t\t\tfor line in f:\n\t\t\t\tline"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tf_xml = XML_Parser(open(prefix + file_path))\n\n\t\tinfo = f_xml.read_node(0,len(f_xml.read_node()))\n\n\t\t\t\n\t\t#print info\n\t\t#print \"file\"\n\t#print \"num\"\n\t#print \"des\"\n\t#print \"an\"\n\t#print \"n\"\n\t#print \"m\"\n\ttry:\n\t    _ = f_xml.read_node(1,len(f_xml.read_node()))\n\t    _ = _.get(\"n\")\n\texcept"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\n    try:\n    \tf = open(os.path.join(file_path, prefix + file_folder, filename), 'r')\n    except Exception as e:\n        print(filepath + \" no existe.\")\n\texit()\n\t\n    # Conviviendo a arquivo para um arquivo do pacote do arquivo\n    f.seek(0)\n    xml_tree = f\n\n    # Genera prendo da arquivo que \u00e9 uma constru\u00e7\u00e3o seja a pesquisa\n    # o xml para verificar que deve um arquivo"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tresult = dict({})\n\t\t\n\tresult['asset'] = prefix+file_path\n\t\n\tjson_file = open(file_path)\t\n\tline_number = 0\n\tfor line in json_file:\n\t\tline_number += 1\n\t\tline = line.replace(\"\\n\", \"\")\t\t\n\n\t\t#print line\n\n\n\t\tresult['package_name'] = line.split(',')[1]\n\t\tresult['package_version'] = line.split(',')[2]\n\t\tresult['package_repo'] = line.split(',')[8]\n\n\t\t#print result[result['package"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tfrom xml_models.models import Xml_File\n\t\tmodel_path = Path('s3xml', str(prefix)))\n\t\tbase, ext = split(file_path)\n\t\tfilename = '.'.join((base + '.csv', ext))\n\t\tbase, ext = split(file_folder)\n\t\tbase, extension = split(base + '.csv')\n\t\tfile = Xml_File(model_path)\n\t\tfile.xml_file = filepath(file_path)\n\t\t\n\t\treturn {\t\t'file': file,\n\t\t\t\t\t'filename': filename,\n\t\t\t\t\t'docpath':"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tpass\n\ndef _add_asset_to_package(file_name, asset, package_name):\n\t\"\"\"\n\tAtiva o dicion\u00e1rio com o novo arquivo do pacote no formato JSON\n\tde arquivos do n\u00f3s c\u00f3digo de dados.\n\n\tArguments\n\t----------\n\tfile_name : str\n\t\tTem pelo `file.json` da pacote no formato JSON\n\n\tstructure (str, str):\n\t\tO contr\u00e1rio iniziada.\n\tasset : str"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tfile_path = self.normalize_path(file_path)\n\n\t\twith codecs.open(file_path, 'rb', 'utf-8') as file:\n\t\t\traw_text=file.read()\n\n\t\tnew_text=new_text.decode(\"utf-8\")\n\t\t# remove lines from front and remove new lines\n\t\traw_text=self.remove_regex(raw_text)\n\t\t# parse the original text and remove any comments that\n\t\t# lie on it.\n\t\twhile len(raw_text)==0 and raw_text[0]==\""}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tlangs = lang.split('-')\n\t\tfile_name = 'original/{0}{1}.pdf'.format(langs[0], langs[1])\n\t\tif not fileExists(file_path):\n\t\t\tnew_file = pdftotext(file_path, lang)\n\t\t\tif file_name not in new_file:\n\t\t\t\treturn self.upload(\n\t\t\t\t\tfile_path,\n\t\t\t\t\tfile_name,\n\t\t\t\t\tself.options['original_width'] and str(self.options['original_width']) or '400'\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tprint('Error:"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\turl = \"https://github.com/artigo02/R/archive/{format}/rendition/\".format(format=lang)\n\t\turllib.request.urlretrieve(url, file_path)\n\t\treturn self.add_file_path(file_path, \".gz\")\nimport unittest\n\nclass Solution:\n    def maximalSum(self, nums: List[int]) -> int:\n        nums.sort()\n        if len(nums) <= 1:\n            return 0\n\n        low, high = 0, len(nums) - 1\n        while low < high:\n            mid"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n\t\t# TODO: implement some format conversions for this language\n\t\t#       (see code in lib/base/data.hpp as reference)\n\n\n\t\t# FIXME: change this - not used yet\n\t\t# use a regular expression since we can write the whole line\n\t\t# with gencode and do encoding detection, we will use tempfs\n\t\t# to open and read from the file and return the result\n\t\t#self.re = re.compile(r'\\t?\"([^\"]+)\",(\".+?\")?|\"([^\"]+?)\"?|\"([^\"]+?)\"?|\\'([^\\\"]+)\",')\n\n\t\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tos.path.exists(file_path)\n\n\tfor line in self.language:\n\t\tif lang not in line.strip():\n\t\t\tcontinue;\n\t\tfor index in line.split(\" \"):\n\t\t\tself.add_line(language=index, file_path=lang, linenos=False, replace=True)\n\n\tself.save_file(file_path, replace=True)\n# coding=utf-8\n# --------------------------------------------------------------------------\n# Code generated by Microsoft (R) AutoRest Code Generator 1.2.2.0\n# Changes may cause incorrect behavior and will be"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\twith open(file_path, 'r') as file:\n\t\t\tfile = file.read()\n\n\t\t\thtml_document = BeautifulSoup(file)\n\n\t\t\t#get rid of the <html> so we can put it in a <html> node\n\t\t\t#so that it stays the same as the original\n\t\t\thtml_text= BeautifulSoup(html_document)\n\t\t\tfor element_string in html_text.findAll(\"body\"):\n\t\t\t\telements = element_string.findAll(\"a\")\n\t\t\t\tname= elements[0].text.encode(\"utf-8\")\n\t\t\t\tname= name[:-1"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\ttry:\n\t\t\t# import ipdb; ipdb.set_trace()\n\t\t\t# title = re.sub(r\"[^\\w\\s\\.\\-\\.,]\",\"\",str(lang))\n\t\t\ttitle = re.sub(r\"^[^\\w\\s\\.\\-]+\",\"_\",lang.upper())\n\t\t\tself.title = title\n\t\t\t# # # # print \"adding\",self.original, lang\n\t\t\t\n\t\t\t# content = file_name.encode(self.original)\n\t\t\t# # content = str(type(content))\n\n\t\t\t# name = self.title.encode(self."}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n\t\n\tif language(lang)!= 'artigo04':\n\t\terror = self.Error(\"Language '{}' not supported\".format(lang))\n\t\t\n\t\treturn\n\t\t\n\tif self.remove_old(file_path):\t\t\n\t\twith codecs.open(file_path, 'rt', encoding='utf-8') as f:\n\t\t\tdata = f.read()\n\t\t\t\n\t\t\tnew_text = data.replace(lang + self.REPLACE_CHAR, self.REPLACEMENT_CHAR)\t\n\t\t\t\n\t\t\tf.seek(0)\n\t\t\tfh = open(file_path, 'wt', encoding='utf"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n#     logging.info(lang)\n#     logging.info('------------------------------')\n#     logging.info(file_path)\n#     f1 = open(file_path, 'r')\n#     f2 = open(file_path, 'a')\n#     f1.close()\n#     f2.close()\n#     # f3 = open(file_path, 'r')\n#     # f4 = open(file_path, 'a')\n#     # f5 = open(file_path, 'a')\n#     #"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tif not os.path.exists(self.data_paths['original_filename']):\n\t\t\tfilepath = os.path.join(os.path.dirname(__file__), self.data_paths['original_filename'])\n\t\t\tif not os.path.exists(filepath):\n\t\t\t\traise Exception(\"Can't find original filename: %s\" % filepath)\n\t\t\t# Get the page\n\t\t\ttext = self.fetch_page(\"http://%s.static.flickr.com/new/%s.png\" % (lang, file_path))\n\t\t\tif not text:\n\t\t\t\traise Exception"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tfilename = filename.replace(\"\\\\\", \"/\")\n\t\tf = open(file_path, \"rb\")\n\t\tif filename[:4]!= \"artigo02\":\n\t\t\tf.seek(0)\n\t\t\timage = BytesIO(f.read())\n\t\tself.add_image(filename, image)\n\n\tdef get_asset_url(self, url):\n\t\tif url.startswith(\"/path/artigo02-gf03.jpg\"):\n\t\t\turl = url.replace(\"\\\\\", \"/\")\n\t\telif url.startswith(\"/artigo02-gf03.png\"):\n\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\t_asset = self.__get_asset(basename, file_path)\n\t\tasset.update({ \n\t\t\t\"width1\": _asset.get(\"width1\"),\n\t\t\t\"height1\": _asset.get(\"height1\"),\n\t\t\t\"title1\": _asset.get(\"title1\"),\n\t\t\t\"width2\": _asset.get(\"width2\"),\n\t\t\t\"height2\": _asset.get(\"height2\"),\n\t\t\t\"cover\": _asset.get(\"cover\"),\n\t\t\t\"aspect\": _asset.get(\"aspect\"),\n\t\t\t\"width3\": _asset.get(\"width"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.files_to_upload.sort()\n\t\tself.files[basename] = file_path\n\t\n\tdef remove_asset(self, basename):\n\t\t\"\"\"\n\t\t\"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\"\n\t\"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t\"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t\"artigo02-gf03.tif\":"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tif not basename in self.assets:\n\t\t\tnew_asset = None\n\t\t\tfor path, _dummy_new_path in self.files_to_add:\n\t\t\t\tnew_path = path\n\t\t\t\tif new_path == basename:\n\t\t\t\t\tbreak\n\t\t\t\tnew_path = re.sub(r'artigo(\\d+).*', os.path.sep, newpath)\n\t\t\t\tself.assets[basename] = new_path\n\t\t\t\tnew_asset = path\n\n\t\t\tself.assets[basename] = self.assets[basename].replace(file_path + '.tiff', '')\n\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tpath, filename = splitpath(basename)\n\t\t\n\t\tbasepath = filepath(file_path)\n\t\n\t\tif not os.path.isfile(path):\n\t\t\tself.add_folder (dirname(filename))\n\t\t\tos.makedirs(dirname(filename))\n\n\t\tif basepath.endswith('/'):\n\t\t\tbasepath = basepath[:-1]\n\t\tself.add_all (filename=filename, root_element=basepath + basepath.replace('\\\\', '/'))\n\n\tdef add_folder (self, dirname):\n\t\t\"\"\"\n\t\tmkdir -p \"artigo02-gf03\"\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\t# check if file exists\n\t\tif not os.path.exists(file_path):\n\t\t\tprint \"The file: \",file_path\n\t\t\treturn False\n\t\ttry:\n\t\t\t# Open the file\n\t\t\twith open(file_path, \"r\") as file:\n\t\t\t\tself.assets[basename]=file.read()\n\t\t\treturn True\n\t\texcept:\n\t\t\treturn False\n\n\t# add a image\n\tdef add(self, file):\n\t\tbasename = os.path.splitext(file)[0]\n\t\tfile_path = os.path.join('/sdcard"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tdir_path = os.path.dirname(file_path)\t\t\t\n        if not dir_path in self.files_to_copy:\n\t\t\tself.files_to_copy[dir_path] = []\n\t\tself.files_to_copy[dir_path].append(basename)\n# -*- coding: utf-8 -*-\n#\n# pysimm documentation build configuration file, created by\n# sphinx-quickstart on Fri Jun 23 21:45:21 2009.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\twith open(file_path, 'rb') as f:\n\t\t\tblob = f.read()\n\t\tself.files['assets'][basename] = base64.b64encode(blob)\n\t\t\n\n\tdef download(self, url='http://www.google.com'):\t\n\t\t#if self.fileExists('assets/img.png') == False:\n\t\t\t#for ext in ['.gif', '.jpg']:\n\t\t\t\t#self.download('img/' + ext)\n\t\t#if self.fileExists('static/images/frame.png') == False:\n\t\t\t#for ext"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tpath, filename = os.path.split(file_path)\n\t\tfile_path_noext = os.path.splitext(filename)\n\t\tfile_path_noext = file_path_noext[0] + '.txt'\n\t\tfile_path_noext = file_path_noext.replace('assets/', '')\n\t\tfile_path_noext = file_path_noext.replace('assets/', '')\n\t\tself.assets_path.append(file_path_noext)\n\t\tself.assets_file_path.append(file_path"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself._add_asset_path(basename, file_path)\n\t\tself._remove_asset_path(basename, self.images_path)\n\t\n\n\tdef del_asset(self, file_path):\n\t\t\"\"\"\n\t\t\"{\n\t  \t\tcall: \"rm -rf {}\"\n\t  \t\tcall: \"rm {}\"\n\t  \t\tcall: \"mv {}\"\n\t\t}\n\t\t\"\"\"\n\t\tself._remove_asset_path(file_path, self.images_path)\n\t\n\n\tdef _remove_asset_path(self, file_path, paths):\n\t"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _make_zipfile(zip_path, file_index):\n\t\"\"\"\n\tTakes zip file path and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nindex : int\n        index of the package where zip file is downloaded\nReturns\n-------\ndict\t\"\"\"\n\t\n\tzip_file = zipfile.ZipFile(zip_path)\n\tzip_file.extractall(index)\n\n\tzip_file.close()\n\t\n\treturn {\n\t\t\"package_name\": os.path.split(zip_path)[1],\n\t\t\"package"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\tzip_path = os.path.expanduser(zip_path)\t\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\t# zip_path should have files: [packageName,packageVersion][file_name].file_name\n\t\tfilename_re = re.compile('([\\w_]*)-([\\w_]*)-([\\w_]*\\.gz)')\n\t\tpackage_list = zf.namelist()\n\t\tif len(package_list) == 1:\n\t\t\tresult = {}\n\t\t\tfor filename in package_list:\n\t\t\t\ttmp = filename.find(\".dat\")\n\t\t\t\tif"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tzip_file = zipfile.ZipFile(zip_path, \"r\")\n\t\t\n\t\t# get all files in root directory (that can be used to find \n\t\t# data)\n\t\tfor root, dirs, files in os.walk(zip_path):\n\t\t\t# skip file names\n\t\t\tif \"_xml\" in dirlist(root):continue\n\t\t\t\n\t\t\tfor file_name in files:\n\t\t\t\t# get specific XML name.\n\t\t\t\txml_path = os.path.join(root, file_name)\n\t\t\t\t# search by XML XML name\n\t\t\t\txml_file = os.path.join("}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n# open archive and read in zip file\n\ndef _explore_csv(csv_path):\t\n\t\"\"\"\n\tGet packages' data from csv_path\n\nGroups files by their MIME type.\n\nParameters\n----------\ncsv_path : str\n    csv file path\nReturns\n-------\ndict\t\"\"\"\n\t\n# open archive and read in csv file\n\ndef _explore_csv_list(csv_path):\t\n\t\"\"\"\n\tGet packages' data from csv_path\n\nGroups files by their MIME type, return as dict.\n\nParameters\n----------\ncsv_"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ndef explore_zipfile(zip_path):\n        zf = zipfile.ZipFile(zip_path)\n        # Get root element from zipfile\n        root = zf.open(\"Contents/Info.plist\")\n        # Remove all packages (main,...)\n        files = [f for f in root.namelist() if f.startswith('main')]\n\t# Reverse sort alphabetically\n        files.sort(reverse=True)\n\t# Reverse sort by extension\n        files.reverse()\n        # Create dictionary data for this package\n        data = {}\n        for f in files:\n\t\tdata[f] = root.read("}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _get_pkg_data(filename) :\n\tdata = None\n\ttry :\n\t\tdata = zipfile.ZipFile(filename, 'r')\t\n\texcept IOError:\n\t\tsys.exit(\"Error while opening.zip file\")\n\treturn data\t\t\t\t#return data of the package\n\ndef is_pkg_pkg(filename, version, release) :\n\tfound = False\n\n\tif release.startswith(\"1.\") and package_info:\n\t\t\tif filename.endswith(\".xml\"):\n\t\t\t\ttry:\n\t\t\t\t\txml_info = ElementTree.ElementTree(ElementTree.XML(zip"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\tzip_obj=zipfile.ZipFile(zip_path)\n\treturn_dict={}\n\tfiles_list=zip_obj.infolist() \n\troot_folder=zip_obj.infolist()[0].filename\n\tfor item in files_list:\n\t\tname=item[0]\n\t\tdata_dict=zip_obj.read(name)\n\t\treturn_dict[name]=data_dict\n\t\t#break\n\treturn return_dict\n\t\n\t\ndef save_zip_to_db(zip_path,package_name,package_type,description,user_"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tpath_str = os.path.join(os.getcwd(), os.path.splitext(zip_path)[0])\n\t\tfilepath = os.path.join(path_str.split(os.sep)[-1])\n\t\tzip_file = zipfile.ZipFile(filepath)\t\n\n\t\tfor dir_path, dir_names, file_names in os.walk(path_str):\n\t\t\tpackage = dir_name.replace(dir_path,'').replace('.zip','.')\n\t\t\tzip_package_filepath = os.path.join(dir_path, os.path.split"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\n\tzipfile = zipfile.ZipFile(zip_path)\n\tcontent_file = zipfile.read(zipfile.namelist()[0])\t\n\tcontent = content_file.decode().replace('\"', '').replace('&', '').replace('<', '').replace('>', '')\n\tyaml = content[content.index('{'):content.index('}')+1]\n\tyaml_data = yaml.rsplit(yaml, 1)\n\tyaml_data = yaml_data[0]\n\n\tyaml_data = yaml_data.lstrip()\n\t\n\treturn yaml_data\n\ndef get_packages(zip_path):\n"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\tzipfile = zipfile.ZipFile(zip_path, 'r')\n\txml_file = zipfile.open('pyml.xml')\n\txml_root = xml_file.read()\n\txml_root = xml_root.replace('_','')\n\txml_name = xml_root.split('/')[-1:][0]\n\tzip_path = os.path.abspath(os.path.join(os.path.dirname(zip_path),os.path.basename(zip_path).split('.')[0] + '.zip'))\n\tnames = os.listdir(zip_"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tdef find_file_no_extension(zip_filepath):\n\t\t\timport zipfile\n\t\t\tfor (key,ext) in self.zip_files_dict[zip_filepath].items():\n\t\t\t\tif ext is not None:\n\t\t\t\t\treturn ext\n\t\t\traise Exception(f'Unrecognized zip filename: {zip_filepath}. '\n\t\t\t\t\t\t\tf'Please add.zip, {zip_filepath} as it is '\n\t\t\t\t\t\t\t'not a valid extension.')\n\t\tfrom zipfile import ZipFile\n\t\tarchive = ZipFile(zip_path, 'r')\n\t\tarchive.extractall(find_file_no_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tzip_list=[]\n\t\tfor filename in file_list_from_zipfile(zip_path):\n\t\t\tzip_list.append(filename)\n\t\treturn zip_list\n\n\t\nfor root, dirnames, filenames in os.walk(path_to_dir_or_files):\n\t\n\tfor filename in fnmatch.filter(files_list_from_zipfile(root), '*.tif'):\n\t\t\n\t\tif filename in filename_list2:\t\n\t\t\tif fnmatch.fnmatch(filename, '*.gff3'): continue\n\t\t\telse: pdb.set_trace()\n\t\t\tfor dirname in dirnames:\n\t\t\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfiles = []\t\t\t\t\t\t# list of file names\n\t\t\n\t\tdirs, _ = os.walk(zip_path)\n\t\tfor _file in dirs:\n\t\t\tif _file.lower().endswith(('.zip', '.tar.gz')):\t# skip folders whose name ends in \".zip\"\n\t\t\t\tcontinue\n\t\t\t\n\t\t# make list of files\n\t\tfiles = []\t\t\t\t\t\t# list of file names\n\t\t\n\t\tfor _file in files: # for each file in directory\n\t\t\tif os.path.splitext(_file)[1] in IMAGE_EXTS: # only files that start with image_ are image"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\nzip_list=files_list_from_zipfile(zip_path)\n\nimport os\nimport sys\nimport re\nimport subprocess\nimport shlex \n\ndef list_files_recursive(dir_path):\n\t\"\"\"\n\tGet files from multiple dirs\n\t\"\"\"\n\tif not os.path.exists(dir_path):\n\t\traise RuntimeError(\"Directory '\" + dir_path + \"' does not exist\")\n\tlist_str=subprocess.check_output([\"ls\", \"-H\", dir_path])\n\tfile_list=[]\n\tfor file_item in list_str."}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\n\tfiles_list = []\n\tfile_paths = find_zip(zippath)\n\t\n\tfor file_path in file_paths:\n\t\t# print(file_path)\n\t\tfile_in = open('file:' + file_path)\n\t\tline = file_in.readline()\n\t\tfile_in.close()\n\t\tfiles_list.append(line)\n\n\treturn files_list\n\n\ndef files_zip_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n  "}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tprint(zip_path)\n\twith zipfile.ZipFile(zip_path) as z:\n\t\tfor file in z.namelist():\n\t\t\tprint(file)\n\n####\n\ndef get_paths():\n    \"\"\"\n\tReturn the folders in `paths`\n\t\"\"\"\n    dirs = []\n    for path in os.listdir(\".\"):\n        full_path = os.path.join(\"..\", path)\n        if os.path.isdir(full_path):\n            dirs.append(path)\n\n    return dirs\n\ndef get_all_files(directory):\n    \"\"\"\n\tYields all"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfiles = {}\n\t\tfor _, v in zipfile.ZipFile(zip_path):\n\t\t\tfiles[v.filename] = v.read('files')\n\n\t\treturn files\n\n\telse:\n\t\tfiles = {}\n\t\tfor f in files_list_from_zipfile(zip_path):\n\t\t\tfiles[f.filename] = f.read('files')\n\n\t\treturn files\n\n\ndef run_all():\n\n\timport os\n\timport glob\n\timport pandas as pd\n\timport scipy.signal as signal\n\tfrom matplotlib import pyplot as plt\n\n\tdef sigmain(signum"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\n\tzip_dir_path = zipfile.ZipFile(zip_path, 'r')\n\n\tzip_dir_path.extractall(os.path.dirname(os.path.abspath(__file__)))\n\n\tfiles = []\n\tfolder_list = []\n\tfile_extension = \"\"\n\n\tfor zipped_path in zip_dir_path.namelist():\n\t\tzip_folder_path = '%s%s/' %(zipped_path, 'zip')\n\t\tfiles.extend(list_file_infos[zipped_path])\n\t\tfolder_list.append(zipped"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\t\n\tif not os.path.isfile(zip_path):\t#check, the path really does exist?\n\t\tprint(zip_path)\n\t\treturn False\n\telse:\t#not a path, we want to process it.\n\t\tzip_path = zip_path.strip('\\\\')\n\t\tprint('path', zip_path)\n\t\ttry:\n\t\t\tf = zipfile.ZipFile(zip_path, 'r')\t#returns a Python's ZipFile object\n\t\texcept zipfile.BadZipfile:\n\t\t\tprint('bad zip', zip_path, 'found!? ')\n\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\t#import re\n\t\t#print(\"re.search(r'tuple (\\d+)\\.zip\" + str(len(paths)) + \")\",end=\"+ \")\n        #for path in paths:\n\t\t#\t# TODO: make sure path is in subdirectory\n\t\t#\tif path!= \"\":\n\t\t#\t\tzip_path=\"/\".join(path.split('/')[1:-1])\n\t\t\n\t\t#\telse:\t\t\t\t\t\t\t\n\t\t#\t\tzip_path=\"/\".join(path.split('/')[-1].split('.')[0:-3] +  \"tar\")\n\t\t\n\t\treturn glob.glob(zip"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif re.search(r'^[a-zA-Z ]*$',content):\n\t\t\tcontent = content.replace(r'[a-zA-Z ]*',b'')\t\t\t\n\t\treturn content\n\telif re.search(r'^\\s*namespace[ \\t]*$', content):\n\t\tcontext_name = re.findall(r'^\\s*namespace\\s*=\\s*[\\\"\\'][a-zA-Z]*\\s*;',content)\n\t\tif context_name:\n\t\t\t# replace in a special namespace (before the opening brace)\n\t\t\t# that isn't needed for anything but CSS properties"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif content[0:2] in namespaces_copy:\n\t\t\tnamespace = namespaces_copy.index(content)\n\t\t\tnew_content = content[2:]\n\t\t\tnamespaces_copy.insert(0, namespace)\n\t\t\tcontent = new_content\n\t\telif content[:2] in namespaces_copy:\n\t\t\tnamespace = namespaces_copy.index(content)\n\t\t\tnew_content = content[2:]\n\t\t\tnamespaces_copy.insert(0, namespace)\n\t\t\tcontent = new_content\n\treturn content, namespaces_ignore\n\n\ndef split_namespaces_w(namespaces):\n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn content.replace('w-st=\"w-st=\"', 'w-st=\"'\t'' )\n\n\tdef fix_doctype(doctype):\n\t\t\"\"\"\n\t\tFormat BIR.\n\t\t\"\"\"\n\n\t\tdoctype = doctype.replace('_','')\n\n\t\tfor line_index, line in enumerate(doctype):\n\n\t\t\tdoctype[line_index] = line.strip()\t\t# \\t and `' signifies a regex like `,`.\n\t\t\tdoctype[line_index] = '\"\"\"{0}'''.format(doctype[line_index]) # replace close tags with single spaces\n\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn \"w\"+content.replace(\" \", \"_\")\n\t\t\n\t\t\n\t@cached\n\tdef get_template(self, options) :\n\t\t\"\"\"\n\t\tThis function will take a template string and attempt to make this\n\t\ta template to reference the template with whatever format it expects.\n\t\t\"\"\"\n\t\traise NotImplemented(\"abstract method\")\n\t\t\n\t\t\n\tdef register_builtin_tags(self):\n\t\t\"\"\"\n\t\tThis function will register the tag for all builtins created\n\t\t\"\"\"\n\t\tself.builtin_tags = []\n\t\tself.builtins = {}\n\t\tself.tag_groups = {}\n\t\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif not content:\tcontent = \"\"\n\t\treturn 'w:%s' % content\t\t\t\n\tdata  = content.split('\\n')\n\tdefs = []\n\tfor line in data:\n\t\tif not line:\n\t\t\tbreak\n\t\tif not '{' in line:\n\t\t\tbreak\n\t\tdefs.append(line.split('{')[1].split('}')[-1][1:-1].split(' '))\n\td = {}\n\tfor d in defs:\td1 = d\n\t\tfor l in d1:\n\t\t\tlines = l.split('=')[:-1]\n\t\t\td1"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tnewcontent=''\n\t\tfor line in [r'\\w:\\s*/:=', r'\\s*\\|']:\n\t\t\tline = line.strip()\n\t\t\tif not line: continue\n\t\t\tif line.find(w_namespace_prefix): newcontent = line\n\t\tnewcontent=newcontent.replace('$(w-namespace-prefix)', w_namespace_prefix)\n\t\tnewcontent=newcontent.replace(\" \", \"\") # otherwise, not used yet\n\t\treturn newcontent\n\n\tcontent = content.replace('\\t','   ')\n\tcontent = content.replace('\\n','   ')\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tcontent = content.replace('w:st=','w:st=\\\"\")')\n\t\tcontent = content.replace('w:st='<w:st=\"' to 'w-st=\"'\t')\n\t\tcontent = content.replace('w:st  =', 'w:st=\"'\t')\n\t\tcontent = content.replace('w:st=\\'\"', 'w:st=\"'\t')\n\t\tcontent = content.replace('w:st=\\'', 'w:st=\"'\t')\n\t\tcontent = content.replace('w:st=/=', 'w:st=\"'\t')\n\t\tcontent = content.replace"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn content.replace(u'\\ue003\\ue002', u'\\ue012')\n\n\ndef read_file(path):\n\t\"\"\"\n\tReturn the full python file, as a single string\n\t\"\"\"\n\tdoc = minidom.parse(path)\n\troot = doc.documentElement\n\tret = \"\"\n\n\tfor attr in root.attributes.keys():\n\t\tif attr.find(\"namespace\")!= -1:\n\t\t\tif attr.find(\"name\")!= -1:\n\t\t\t\t# look for xsi:type\n\t\t\t\tif attr.find(\"name\").text == 'type':"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif content.startswith('$W'):\n\t\t\tvalue='w:'\n\t\telse: \n\t\t\tvalue='w:'+content\n\t\tcontent=content.replace('$W','w:-')\n\t\tcontent=content.replace('$W','w ')\n\t\tcontent=content.replace('$W','w')\n\t\tcontent=content.replace('$W','w-')\n\t\tcontent=content.replace('$W','w_=')\n\t\tcontent=content.replace('$W','w_=')\n\t\treturn value\n\ndef run(confobj):\n\tdebug('confobj: '+str(confobj))\n\t\n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tcontent = content.replace('\"', '\\\"')\n\t\treturn content\n\treturn content\n\n# The 'w:title' format is a synonym that matches many known title parts\n_h_re_title = re.compile(r'<title>([0-9]+)</title>')\ndef fix_title_sub(content):\n    \"\"\"\n    Replace space characters (' ') with a space.\n    \"\"\"\n    content = replace_at_urls(content)\n    content = _h_re_title.sub(r' \\1', content) # replace links and markup,\n    content = content.replace("}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tpubdate_xpaths= pubdate_xpaths.split('\\n')\n\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\tmatch2=re.compile(ur'^pubdate/').match(pubdate_xpath)\n\t\t\tif match2:return 'pubdate/text()'\n\t\treturn node.get('pubdate')\n\tif len(pubdate_xpaths)>0: return ''\n\treturn None\n\ndef convert_node(node, parent_xpath=None, match_parent=True):\n\t\"\"\"\n\tConverts an lxml element into its native type,"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor xpaths in pubdate_xpaths:\n\t\t\tif nodes.get(xpaths):\n\t\t\t\treturn nodes[xpaths]\n\t\treturn None\n\t\n\t\"\"\"\n\tFind the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor apath in pubdate_xpaths:\n\t\tfor node in apath:\n\t\t\tif nodes.get(node):\n\t\t\t\treturn nodes[node]\n\t\n\t\"\"\"\n\tCreate a dummy list that contains only one match\n\t\"\"\"\n\treturn []\n\n\"\"\"\nFind the first match in the pubdate_xpaths list\n\"\"\""}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor i, pubdate in enumerate(pubdate_xpaths):\n\t\t\tif pubdate is None:\n\t\t\t\treturn i\n\treturn -1\n\ndef handle_data(data_node, attr_name):\n\tif data_node.hasAttributeNS(attr_name):\n\t\t# data is of type (node)\n\t\treturn data_node.getAttribute(attr_name)\n\treturn \"\"\n\ndef handle_cdata(data_node, attr_name):\n\tif data_node.hasAttributeNS(attr_name):\n\t\t# data is of type (node"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor pub_xpath in pubdate_xpaths:\n\t\t\treturn node.xpath(pub_xpath[1])\n\n\tdef match_any(node, any_xpaths_xpaths):\t\"\"\"\n\tReturns true if any one of the given xpaths should be matched\n\t\"\"\"\n\tfor any_xpath in any_xpaths_xpaths:\n\t\treturn any_xpath[0] == node.xpath(any_xpath[1])\n\n\nclass Parser():\n\t\"\"\"\n\tThis class is used to parse the content of an article XML file.\n\tAt the end"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor p_xpath in pubdate_xpaths:\n\t\t\tfirst_match = p_xpath.findall('first')\n\t\t\tif len(first_match)>0:\n\t\t\t\treturn first_match[0]\n\n\tdef match_title(node, search_terms):\n        \"\"\"\n        Returns the first match in the titles list\t\n        \"\"\"\n        for x in search_terms:\n            search_str = str(x).strip()\n            if len(search_str)==0 or search_str == \"\":\n                return  \"\"\n            else:\n                if search_str in title_regex:\n                        first_match"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor x in set(pubdate_xpaths):\n\t\t\tif node.get('pubdate', '') == x:\n\t\t\t\treturn x\n\t\treturn None\n\n\tdef match_publisher(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor x in set(pubdate_xpaths):\n\t\t\tif node.get('publisher', '') == publisher:\n\t\t\t\treturn x\n\t\treturn None\n\n\tdef match_series(x):\t\t# <series> tag\n\t\ttry:\n\t\t\tx.series = int(x."}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tc = xmpp.Node('pubdate')\n\t\tif not pubdate_xpaths:\t\t\t\t\n\t\t\tpubdate_xpaths.append(c.addChild(\"pubdate\"))\n\t\tc.setTagData(\"pubdate_from\")\n\t\tfor pubdate_uri in pubdate_xpaths:\t\t\t\t\t\n\t\t\tc.addChild(\"pubtype\", pubdate_uri[1])\n\t\t\tc.addChild(\"node\", pubdate_uri[0])\n\t\treturn c\n\t\n\tdef match_publisher(node, pubdate_xpaths):\n\t\t\"\"\"\n\t\tReturns the first match in the pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tpubdate_ypaths = [y for x in pubdate_xpaths if x == pubdate_ypaths[-1]]\n\t\treturn filter(lambda x: x==pubdate_ypaths[-1], pubdate_xpaths)[0]\n\n\ndef make_dict_from_xml(node, pubdate, dict_path):\n\t\"\"\"\n\tExtracts all the fields from a xml text entry and put them in an dict \n\t\"\"\"\n\txmlstring = \"\"\n\tfor d in node.findall(dict_path):\n\t\txmlstring += d.text\n\treturn xmlstring"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor match, elem in node.items():\n\t\t\tif (match == pubdate_xpaths[-1] and elem.text!= ''):\n\t\t\t\treturn elem.text\n\t\treturn None\n\treturn match_pubdate\n\n\ndef compare_entries(entry_list, expected_entries):\n        \"\"\"\n        Return True if the entries match, else return False\n        \"\"\"\n        return all(1 for item in entry_list if item == expected_entries)\n\n\ndef compare_entries_equal(entry_list1, entry_list2):\n        \"\"\"\n        Return True if the entries do not match, else False."}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\t# find matching tags for the tag, and return the second one\n\t\tif len(pubdate_xpaths):\n\t\t\tfor pubdate_tag_tag, pub date, tags in pubdate_xpaths.values():\n\t\t\t\tif pubdate.find(tag)!= -1: \n\t\t\t\t\treturn pubdate_tag_tag\n\n\t\t# if there are not matches, return None\n\t\treturn None\n\n\t#\n\t# main methods to get the publication dates\n\t#\n\n\tdef getpubdate_for_tag(self,tag):\n\t\t\"\"\"\n\t\tReturns the publication date for the"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber, suppl = issue.group('number','suppl').split(\",\")\t\t\t\t\n\t\treturn number, suppl\n\nclass issue_number_from_sponsor_dict(base.dicttable):\n\t''' \n\tInvestigate the number of the sponsor from the issue.\n\t'''\n\t_table_name = 'issue_number_from_sponsor'\n\n\tdef __init__(self, issue_number = None):\n\t\t'''\n\t\tConstructor.\n\t\t'''\n\t\tself.issue_number = _extract_number_and_supplment_from_issue_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tissue_data = issue.getElementsByTagName('issue')\n\t\t\n\t\tnumber = None\n\t\tfor x in issue_data:\n\t\t\tif x.getAttribute('name') == 'number':\n\t\t\t\tnumber = x.getAttribute('content')\n\t\t\n\t\t# find the number part of the issue description\n\t\tfor x in issue_data:\n\t\t\tif not x.childNodes:\n\t\t\t\treturn {'number':number}, None\n\t\t\t\n\t\t\tfor xc in x.childNodes:\n\t\t\t\ts = \"\"\n\t\t\t\tif xc.nodeName:\n\t\t\t\t\tfor subc in xc.childNodes:\n\t\t\t\t\t\tif subc.nodeName == '"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber, suppl = None, None\n\t\tnumber_re = r'(\\d+)\\s+(?P<number>\\d+)\\s+(?P<suppl>\\d+)'\n\t\tfor match in re.finditer(number_re, issue):\n\t\t\tmatch_number, number = match\n\t\t\t# check for non-zero number elements\n\t\t\tif number is not None:\n\t\t\t\tbreak\n\t\terror_msg = 'Unable to extract issue number to determine the number of a supp-formated issue.'\n\t\tself.logger.error(error_msg)\n\t\traise ParseError(issue.id, issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tdef _convert_value(value_string):\n\t\t\ttry:\t\treturn int(value_string)\n\t\t\texcept Exception: return value_string\n\t\tnumber = []\n\t\tsuppl_index = None\n\t\tfor element in issue.findall('value'):\n\t\t\ttry:\n\t\t\t\tnumber.append(_convert_value(element.text))\n\t\t\texcept Exception:\n\t\t\t\tprint 'Could not parse value: %s' % element.text\n\t\t\t\texception_handler(element, issue)\n\t\tif number: # If there are number and suppl\n\t\t\tsuppl_index = number[0]"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t return [number_index for number_index, string_index in get_number_info(issue).items()]\n\n\ndef _extract_number_and_supplment_from_volume_number_and_volume_index_elements(volume, series_number):\n\t\"\"\"\n\tExtract and return the number and the suppl from the contents of volume number and volume\n\tnumber elements, as well as the volume number from this element.\t\"\"\"\t# noqa\n\treturn [number_and_suppl[index][element] for index, element in enumerate(volume)]\n\n\n# -----------------------------------------------------------------------------\n#"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\n\t\n\tnumber_issue_element = issue.find(\"number\")\n\tsuppl_issue_element = issue.find(\"supplression\")\n\t\n\ttry:\n\t\tissue_number = find_number(number_issue_element, number_issue_element.find(\"numberValue\").text)\n\texcept:\n\t\tissue_number = \"\"\n\n\n\tsuppl_number_details = None\n\n\t# extract the issue number\n\tnumber_issues_info = _extract_issue_number_to_info(number_issue_element.find(\"issueNumber\"))\n\t\n\t# extract the number values of the"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tif \":\" in issue:\n\t\t\tissue_number, suppl_number = issue.split(\":\")\n\t\t\tissue_number = int(issue_number)\n\t\t\tsuppl_number = int(suppl_number)\n\t\t\tsuppl_number += 1\n\t\tissue_number = int(issue_number)\n\n\t\t# Add the issue element's number as an attribute to the issue tag value and use it as a unique value\n\t\tissue_tag[issue] = issue_number\n\n\treturn issue_tag\n\n\ndef _is_number(issue):\n\t\"\"\"\n\tDetermine whether the given element contains"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber = issue[0]\n\t\tsuppl_1_len = len(issue)>1 and 4 or 1\n\t\tsuppl_2_len = len(issue)>1 and 4 or 1\n\t\tprint(\"number: {n} - [3] - {e}\".format(n=number, e=suppl_1_len, e=suppl_2_len))\n\t\tnumber_of_suppl_val = [number_of_suppl_val, suppl_1_len,suppl_2_len]\n\treturn number_of_suppl_val[0:suppl_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tissue_text = get_node_text(issue, 'issue_text', 'text')\n\t\tsuppl = get_node_text(issue, 'number_of_suppl', 'text')\n\t\tif not (suppl or issue_text):\n\t\t\tissue_number = None\n\t\t\treturn issue_number, issue_text\n\t\t\t#raise InvalidIssue('Unable to correctly determine number and supppl.')\n\t\treturn issue_number, issue_text\n\tnode = get_node(hit, 'issue_number', 'text')\n\tissue = [get_node_text(x,"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\n#\tName and Description Element - Part 1\n\tname_elem = issue.find('./Article/Section/Journal/ArticleHeading/ArticleTitle')\n\tname = name_elem.find('./Article/Section/Journal/JournalTitle').text\n\tdesc = name_elem.find('./Article/Section/Journal/ArticleHeading/ArticleTitle')\n\tnumber = int(name_elem.text)\n\tsuppl = name_elem.text if name_elem is not None else None\n\t\n\t\n\t#\tNumber and superseded elements (sub-section, suppl (sub-section 1))"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif \"indent\" in self.__dict__.keys():\n\t\t\tindent = self.indent_str(indent, debug)\n\n\t\t# Add indentation string to output\n\t\t# XXX should this indent be included?\n\t\tif indent:\n\t\t\tself.buf.write(indent)\n\n\t\t# Add indent when in a loop\n\t\tif self.if_else_loop:\n\t\t\tself.buf.write(''* indent)\n\n\t\t# Indent if the body start or end with a newline\n\t\t# or when it is not indented\n\t\tif indent and self.do_indent() and indent >"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tdebug and self.dump_debug_info()\n\t\tindent += 4 *'   '\n\t\tself.__repr(indent, debug)\n\t\treturn\n\n\tdef tostring(self):\n\t\t\"\"\"\n\tReturn a pretty, indented string representation of self.\t\"\"\"\n\t\treturn self.__repr__(indent=0)\n\n\tdef dumpf(self, filename):\n\t\t\"\"\"\n\tTry to dump a Python file.\n\tfile - file to dump\n\t\"\"\"\n\t\tpass\n# -*- coding: utf-8 -*-\n#\n# This file is part of Zenodo.\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tout_str = self.get_output_for(debug)\n\t\tif debug:\n\t\t\tout_str = \"%s\\n\" % out_str\n\t\timport inspect\n\t\treturn \"<%s: %s>%s</%s>\\n\" %\\\n\t\t(inspect.getmodule(inspect.stack()[0][1]), self.__class__.__name__, \\\n\t\tout_str, indent, list(self.child))\n\t\t#return \"<\" + str(self) + \">\"\n\t\n\tdef show_indent(self):\n\t\tif self.child is None:\n\t\t\treturn\n\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug == True:\n\t\t\treturn '\\n'.join(self.pretty_dump(indent+2, \">\")).decode(\"latin1\")\n\t\tfrom pprint import pformat\n\t\tpprint(self.vars())\n\t\treturn pformat(self.vars())\n\t\t\n\tdef update(self, other):\n\t\t\"\"\"\n\t\tUpdate this Node with the values from another Node.\n\t\tFor each variable of node C{self} from this node C{other}, copy and \n\t\tset attributes from C{other} to other.\n\n\t\t@since: 1.1\n\t\t\"\"\"\n\t\tself._variables |="}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\treturn '\\n'.join(self.value._pretty(indent=indent, debug=debug) for x in self.value.pretty)\n\n\n#-----------------------------------------------------------------------------------------------------------------------\n#  RPNType\n#-----------------------------------------------------------------------------------------------------------------------\nclass RPNType:\n\t\"\"\"\n\tRepresents a link type.\n\t\"\"\"\n\t# NOTE: it's not really an easy way to know what to do for this, but for now\n\t# we're not using that in other cases\n\tTYPE_NONE = \"none\"\n\tTYPE_NUMBER = \"number\"\n\tTYPE_BOOLEAN = \"boolean\"\n\tTYPE"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif not self.content:\n\t\t\treturn '%s()' % (',\\n\\t '*indent)\n\t\tif debug:\n\t\t\treturn '%s' % (',\\n\\t '*indent).join([x.__str__(indent + 1) for x in self.content])\n\t\telse:\n\t\t\treturn '%s' % (',\\n\\t '*indent).join([x.__str__(indent + 1) for x in self.content])\n\n\nclass Tag(Leaf):\n\t\"\"\"\n\tAn ordered list of subelements, with attribute values as children.\n\n\t\t>>> root = Tag('ul"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tindent_str = (\" \" * indent)\n\t\tindent_str += \" \" * ((indent+3) * indent)\n\t\tlines = (indent_str + \" \" * indent_str)\n\t\tpadding = \" \" * indent\n\t\tlines += self._prettyFormat(indent + 1, indent_level + 1,\n\t\t\t\t\t\t\t indent + self._indent_level,\n\t\t\t\t\t\t\t indent + 1,\n\t\t\t\t\t\t\t lines[:indent + 1] + [lines[indent]])\n\t\treturn \"\\n\".join(lines)\n\n\tdef _prettyFormat (self, indent, level, indent_level=0, indent_"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\toutput = \"\"\n\t\tif debug:\n\t\t\toutput = \"\\n\" + self.showConsole()\n\t\telse:\n\t\t\toutput = self\n\t\t\n\t\tif self.getInputFilename() == \"\":\n\t\t\treturn output\n\t\t\n\t\tfileStr = StringIO()\n\t\tself.writeToFile(fileStr)\n\t\treturn output\n\t\n\tdef showConsole(self, show_message=None):\n\t\t\"\"\"\n\t\tShows console output.\n\n\t\t@param show_message: optional message to show if None,\n\t\telse if show_message is None, print() as string.\n\n\t\t@return: C{"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\treturn str(self.__dict__)\n\n\tdef __eq__(self, other):\treturn isinstance(other, self.__class__)\n\tdef __ne__(self, other):\treturn not self.__eq__(other)\n\tdef __repr__(self):\treturn self.pretty()\n\ndef prettyrepr(obj, indent=0, debug=False):\t\"\"\"\n\tPrint a pretty representation of a given object. \n\tThis function does a bit of validation to find an appropriate\n\trepresentation for an object.\n\t\"\"\"\n\tpretty_dict = pprint.pformat(obj, 1)\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug: print(indent, \"---\")\n\t\tif indent+1==indent_char:\n\t\t\tsys_stdout.write(indent*indent+'Prefix: '+repr(self.prefix or self.prefix))\n\t\t\tsys_stdout.write(indent*indent+'\\n\")\n\t\telif indent > indent_char:\n\t\t\tsys_stdout.write(indent*indent+'Suffix: '+repr(self.suffix or self.suffix))\n\t\t\tsys_stdout.write(indent*indent+'\\n')\t\t\t\n\t\telif indent == indent_char:\n\t\t\tsys_stdout.write(indent"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        if len(args)==1:\n            return absorb(args[0])\n\t        \n        result = []\n        for e in args[1:]:\n            try :\n                val = int(e)\n                result.append(1+val if val>0 else -1)\n            except Exception:\n                result.append(1+1+val if val<=0 else 1)\n                #raise\n        \n        return absorb(result)\n\n\nclass Fermion_langevin(Fermion_Number_Langevin):\n\n\t\"\"\"\n\tFermion langevin is used by:"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        A, B = args\n        B = B.lower()\n        if A == B or A.endswith(\"b\"):\n            return [a[:-1] + \"b\" for a in args ]\n\n        if A == B or A.startswith(\"b\"):\n            return [a[:-1] + \" \" + b for a in args ]\n\n        if A == B:\n            return [ a[:-1] + \"a\" for a in args ]\n\n        if A == A:\n            return [ a[:-1] + \" \" + b for a in args ]\n        \n        if B.startswith(\"b\"):\n            return [ b[:-"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\treturn [self.abs()] + args\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t    \n        self.args = list(args)\n        for arg in args[1:]:\n            arg = LT(arg).simplify()\n            self.args.append(arg)\n            \n            \n    def __eq__(self, other):\n        if self.__class__ is other.__class__:\n            return self.__class__.__name__ == other.__class__.__name__ and \\\n                reduce(lambda a,b:a&b, other.args) == 1\n        else:\n            return False\n        \n    def __hash__(self):\n        return hash(str(self.args))\n\n    def simplify(self):\n        \"\"\"\n        Simplify the list"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n\n\tnewargs  = list(args)\n\ti = 0\n\twhile i < len(newargs):\n\t\tif newargs[i].is_negative():\n\t\t\tnewargs[i].negate()\n\t\ti += 1\n\treturn newargs\n\n\n##__________________________________________________________________||\nclass Conjunction(object):\n    \"\"\"Helper class to represent an : And expression of conjunctions.\n\n    In this case, the :and operator is assumed to be ANDed together.  A\n    condition and an optional second condition is constructed by the\n    operator, giving the two operators and the resultant expression.\n   "}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n    c = self.clauses\n    absorption = self.cnf\n    a = absorb(a) # eliminate repeated non-negative ones\n    negative = absorb(~c) # eliminate non-positive ones\n    return [i for i in negative if i!= a]\n\n@python_2_unicode_compatible\nclass Alt(Hypothesis):\n    \"\"\"\n    Checks that the condition *a* is true for all `clauses`.\n    \"\"\"\n\n    is_true = True\n    \n\n    def __init__(self, clauses, *args, **kwargs):\n        Hypothesis.__init__(self"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        return ListNode(self.ast, args)\n\n    def bool_conjunction(self):\n        \"\"\"\n\tTrue, if a node is absorption and negate(A) evaluates to True. Otherwise\n\treturn True.\n        \n        When the node is a conjunction, a disjunction is in the expression tree\n        (without any further subexpressions), which means that a disjunction\n        may only be subexpressions that are not negated or added to its children\n        (i.e. A(A)).\n        \"\"\"\n\nclass ExprFunctionNodeImpl(ExprNodeImpl):\n    \"\"\"\n    Implements a function used to transform a"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n \tdef apply(self, tokens, evaluation):\n\t\tif tokens[0] == '|':\n\t\t\treturn tokens[2:]\n\t\telif tokens[0] == 'AND':\n\t\t\treturn reduce(AndOperands, tokens[2:], evaluation)\n\t\telse:\n\t\t\treturn [ tokens ]\n\t\n\tdef tostring(self, tokens):\n\t\tval = ''\n\t\tfor clause in tokens:\n\t\t\tif clause == '&&':\n\t\t\t\tval += '(AND'\n\t\t\telif clause == '|&':\n\t\t\t\tval += '||'\n\t\t\telse:\n\t\t\t\tval += '||(%s"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        if len(args) == 1:\n            args.append(args[0])\n\n\"\"\"\ndef lloyd(self, args):\t\"\"\"\n\tGiven a list of expressions, return the longest common prefix of all\n\texpressions that have the same sum.\n\nSee https://en.wikipedia.org/wiki/Lloyd_law\n\nLloyd::\n\n    A & A & A \u2192 A \u2192 A = A | B, A \u2192 A \u2192 A, A \u2192 A, A \u2192 A, A \u2192 B\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\traise Exception('absorption and negative absorption are reserved for A & B')\n\n\treturn [self.A] + self.B\n\ndef absorb2(self, args):\n\t\"\"\"\n\tGiven a list ``A``, and a list ``B``, return an `Expr` which\n\tis the result of applying the absorption system to the set A and B\n\tthen eliminate any ambiguity conditions.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A <=> (A!= B)"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n    def on_hook(function):\n\t\t\"\"\"\n\t\tDecorator function to add a new function to the registry.\n\t\t\t\t\n\t\t:param function: Hook hook attribute for which to register the callback.\n\t\t\t\t\n\t\t:return function: Modifier function to add an object handler to.\n\t\t\n\t\t\"\"\"\n\t\tdef decorator(function):\n\t\t\t\"\"\"\n\t\t\tDecorator function to add a new function to the registry.\n\t\t\t\t\n\t\t\t:param function: Hook callback attribute for which to register the callback.\n\t\t\t\t\n\t\t:return decorator: Modifier function to add a new callable to the registry.\n\t\t\n\t\t\"\"\"\n\t\tdef new_"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n        # Get plugin handler\n        plugin_handler_inst = HookType.get(hook)    # Get instance of HookType Class\n        plugin_name = hook.__name__   # Get plugin name\n        registered_plugins_names[plugin_name] = plugin_handler_inst\n\t# Register new handler as handler in registry\n\t\n        for plugin_inst in registered_plugins_names.values():\n\t\t\t# Add handler if new plugin name not registered\n\t\t\tif plugin_inst.name not in registered_plugins_names:\n\t\t        \n\t\t    # Add handler with new plugin name to dictionary of available plugins\t\t\n\t\t    dict"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\t@functools.wraps(hook)\n\t\tdef wrapper(listener, handler):\n\t\t\tself._registry[hook.name] = handler\n\t\t\treturn self\n\t\treturn wrapper\n\n\treturn on\n\ndef remove(plugin):\n\t\"\"\"\n\tDecorator function to remove a plugin from the registry.\n\nArgs:\n    plugin (PluginType): Plugin to remove.\n\nReturns:\n    callable: Decorator for removal of an existing plugin.\t\"\"\"\n\t\t@functools.wraps(plugin)\n\t\tdef wrapper(listener):\n\t\t\tif type(plugin) == list:\n\t\t\t\tfor plugin_name in plugin"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\treturn self.onhook(hook)\n\n\tdef __call__(self, f):\n\n\t\t\"\"\"\n\t\tCalls :py:func:`onhook` and returns the decorated function.\n\n\t\t:param f: Hook function. The event to hook.\n\n\t\t:return: The decorated function\n\n\t\t\"\"\"\n\t\tdef _f(*args, **kwargs):\n\t\t\tif len(args) > 0:\n\t\t\t\treturn f(*args, **kwargs)\n\t\t\treturn f(*args)\n\t\treturn _f\n\n\tdef onhook(self, hook):\n\t\tif  hasattr(hook, 'event'):"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n        \treturn self.registry.add_listener(hook)\n\n@on(\"error\")\nclass ErrorHook(HookType):\n\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nThe handler is returned to the registered 'error' event, which\ncontains an exception raised by the specified hook.\n\n\t:param str hook (HookType): Hook attribute for which to register the handler.\n\t:param str func (callable): function to call when called\n\t\"\"\"\n\t\n\tdef __init__(self, f, func):\n\n\t\tself.f = f\n\t\tself."}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tdef decorator(f):\n\t\t\t@wraps(f)\n\t\t\tdef decorator_wrapper(*args, **kwargs):\n\t\t\t\tif self.hooks.has_key(hook):\n\t\t\t\t\tif callable(f):\n\t\t\t\t\t\treturn f(f)\n\t\t\t\t\telse:\n\t\t\t\t\t\treturn f(hook)\n\t\t\t\telse:\n\t\t\t\t\treturn None\n\t\t\treturn decorator_wrapper\n\t\treturn decorator\n\treturn on\n\ndef disable():\n\t\"\"\"\n\tDecorator to remove all listeners on the registry.\n\n\tRemoves all listeners registered handler which no longer exist.\n\t\"\"\"\n\thooks = set(x[0"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n    def _on(self, data):\n        \"\"\"\n\t\tIf the registered key has a value, load it from the storage.\n\t\tIf no value registered for the key and the registered\n\t\tvalue is different from the value given, call this method.\n\t\tElse return None\n\t\t\"\"\"\n        if isinstance(data, tuple): # multiple args -> store as dictionary\n            value = data[0]\n            if value in self.keys(): # try to register the respective handler\n                hv = self.mappings.get(value) or HvGetByKey(value)\n                if hv == None: # register"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tpass\n\n\t## add a new handler to the registry\n\t@on.connect('modifier-bound')\n\tdef on_modifier_bound(hook):\t\t\t\t\n        \"\"\"\n\t\tCallback function to remove a modifier bound function.\n\t\t\"\"\"\n        pass\n\n\t## remove a specific bound function from the registry\n\t@on.connect('modifier-bound', one=True)\n\tdef on_modifier_bound_on(hook):\n\t\t\"\"\"\n\t\tCallback function to remove a specific modifier bound function.\n\t\t\"\"\"\n\t\tpass\n\n\t## add a specific bound function to the registry\n\t@on"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tresult = apply(hook.on, self._addListeners)\n\t\tif result is None:\n\t\t\thook.on = lambda x: True\n\treturn result\n\t\t\n# Copyright (c) 2012 OpenStack Foundation.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tpass\n\n\tdef on_init(self, app):\n\t\t\"\"\"\n\t\tDecorate your handlers with a 'init' method\n\t\t\"\"\"\n\t\tif isinstance(self.init_handler, (type, type)):\n\t\t\tself.init_handler.init_hooks.append(self)\n\t\telse:\n\t\t\tself.init_handler.hooks.append(self)\n\n\tdef on_before_init(self, app):\n\t\t\"\"\"\n\t\tRegister a handler to be run before the app is enabled.\n\t\t\"\"\"\n\t\tif not self.init_handler.hooks:\n\t\t"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\ndef get_default_config(user):\t\t\t\"\"\"\n\tReturs a configuration dictionary for the given user.\n\tExample:\n\tuser = \"default_user\"\n\tfor port in \"2379\":\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\"\"\"\n\ndef get_dbinfo():\n\n\t\"\"\"\n\tReturns an object containing all Datalonian SQL and config information to use.\n\t\"\"\"\n\tpass\n\ndef get_query(es_query, db_name=\""}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    try:\n        db = MySQLdb.connect(host=escd_host, port=int(etcd_port), user=user, passwd=PASSWORD)\n        db.autocommit(True)\n        cursor = db.cursor()\n    except MySQLdb.Error, e:\n        print \"Database connection error:\"\n        print \" {0}\".format(-e),\n        sys.exit(1)\n    \n    cursor.execute(\"\"\"\n        SELECT name FROM user_tables where name regexp \"[0-9]*\";\\\n        \"\"\");\n    table_list = None;\n    for row in cursor:\n        table_list = str(row[1])."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\n\ttry:\n                config_dict = pymysql.connect(host=etcd_host, port=etcd_port, user=user, password=etcd_pass, db=0)\n        except pymysql.err.OperationalError:\n        \tprint(\"Could not connect to etcd!\") \n\t\t#exit(0)\n        except pymysql.err.OperationalError:\n        \tprint(\"Could not connect to etcd! Exit. Exiting...\")\n\t\texit(0)\n\n\n\n\treturn config_dict\n\ndef update_config():\n\t\n \n\t\n\t\n\ttry:\n\t\tcursor = pymysql."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\ndef update_configuration(configuration):\n\t\"\"\"\n\tReturns an updated configuration string.\n\n\tArgs:\n    configuration (dict): The configuration dict from the configuration object.\n\n    Returns:\n    str: The updated configuration.\t\"\"\"\n\n\tupdated_string = \"\"\n\tfor key in configuration:\n\t\t\n\t\tif key == 'hostname':\n\t\t\thostname = configuration[key]\n\n\t\telif key == 'authentication_method':\n\t\t\tauthentication_method = configuration[key]\n\n\t\telif key == 'database_name':\n\t\t\tdatabase_name = configuration[key]\n\n\t\telif key == 'database_user':"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n        # Check to see if the user exists and if not then create it\n        with db.session_scope() as session:\n    \t    query = session.query(DbStaticAuth_ts, DbStaticAuth_s)\n\t\tif not query.count():\n\t\t    n = query.update({'_id': 0, 'created': True, 'config_id': 0})\n\t\t\tif etcd_host == \"localhost\":\n                \t n['host'] = etcd_host \n    \t\telse:\n                \t n['host'] = etcd_host\n    \t    return n\n\n# -------------------------------------------------------------------------------------------------\n\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n        config = {\"user\": user, \"etcd_host\": etcd_host, \"etcd_port\": etcd_port }\n\t\t\n\twith open(\"{}{}.conf\".format(os.getcwd(), \"config.yaml\"), \"w\") as config_file:\t\n\t\tconfig_file.write(json.dumps(config))\t\n\t\t\n\treturn config\n\n\ndef base_test_config():\n\t\"\"\"\n\tCreates a configuration with some simple parameter for testing, which can be\n\tset. This returns the configuration given as a JSON string\n\tand does not save the data to a file which should"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\ndef check_or_create_static_configuration():\n    \"\"\"Creates a configuration file for the default configuration (in the file path) and assigns default values to the user and\n    ports provided by the static authentication.\n\n    Args:\n    None\n\tChecks for a user provided configuration file, if it isn't provided as an argument to this function.\n\n    \"\"\"\n    config_file_path = find_config_file(os.path.join(os.path.expanduser('~'), '.etcd_port'))\n    if not os.path.isfile(config_file_path):\n        create_user_configuration("}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    create_conf = {}\n    create_conf[\"user\"] = user\n    create_conf[\"etcd_host\"] = etcd_host\n    create_conf[\"etcd_port\"] = etcd_port\n    create_conf[\"etcd_password\"] = \"\"\n    create_conf[\"etcd_database\"] = \"{}_1\".format(user)\n    create_conf[\"etcd_key_file\"] = \"\"\n    create_conf[\"etcd_conf_file\"] = \"{}_config.yml\".format(user)\n    create_conf[\"etcd_secret_file\"] = \"\"\n    \n    print(create_conf)"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\n\tusername = get_input(\"Username: \")\n\tpassword = get_input(\"Password: \")\n\tsecret_key = get_input(\"Secret Key: \")\n\treturn {\n\t\t\"username\": username,\n\t\t\"password\": password,\n\t\t\"secretKey\": secret_key\n\t}\n\nimport csv\nimport time\nimport os\nimport numpy as np\nfrom scipy.interpolate import griddata\nfrom. import tools\n\n\nclass HDFFileReader:\n    \"\"\"\n    A class to read the contents of a HDF5 file\n    by creating arrays and then loading a data structure.\n    It"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\ndef get_database_settings(user=None):\n\t\"\"\"\n\tReturns a configuration based on the database parameters.\n\t\n\tArgs:\n     user (str): the username of the DB, if any.\n\tReturns:\n     dict: the configurations.\n\t\n\tSample dictionary.\n\t\"\"\"\n\tconfig = base_config(user, etcd_host, etcd_port)\n\treturn config\n\ndef add_user(user=None, password=\"123456\", username=\"admin\", role=\"admin\", password_hash=None):\n\t\"\"\"\n\tCreate new user by password and username."}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\t\n\t# If we get this far, then this was a single ambiguous\n\t# datetime, take the difference from dt to dt_start, which is\n\t# exactly what we'd like to do before this, and subtract a small\n\t# microsecond difference at midnight.\n\t#\n\t# I'm not sure what time zone I think is, but it seems\n\t# reasonable for this to happen.\n\t\n\treturn dt + dt_start - (dt_start % SECONDS_IN_DAY) + dt.microsecond / SECONDS_IN_DAY\n\n\n_naive_class_"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n \tif dt.utcoffset() is not None:\n\t    return dt.astimezone(self._fold_tz)\n\telse:\n\t    return None\n \n    def has_locale(self, tz):\n\t\"\"\"\n\tCheck whether the given tz is a valid ISO-8601 time zone.\n\n\t:param tz:\n\t    A timezone-aware :class:`~datetime.tzinfo` or\n\t\t    :class:`datetime.tzinfo` to check against.\n\t:return:\n\t    ``True`` if the provided time zone is valid,\n\t\t  ``False`` otherwise.\n\t:rtype:\n\t    Boolean.\n\t\"\"\"\n\tif not"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\ndef _fromtimestamp(ts):\n    \"\"\"\n    Given a timestamp, return the corresponding naive one. (Or\n    returns None if the input is inconsistent with this format).\n\n    :param ts: A timestamp object to lookup. \t\n    \"\"\"\n    # If the input is inconsistent by this format, it has probably\n    # been rejected by timezone-aware timestamps.\n    if ts:\n        if not ts.tzinfo:\n                return ts.replace(tzinfo=utc)\n        return ts\n    else:\n        ts = dateutil.parser.parse(ts)\n        if ts.tzinfo:\n            return ts.replace(tzinfo=utc)\n       "}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    if dt.tzname() == 'UTC':\n        # We have no idea which timezones were \"close enough\" to be\n        # ambiguous, so we just assume that the tz is local.  This is not\n        # necessarily a problem on POSIX - we need it for parsing/converting\n        # to a datetime that is *near* an ambiguous datetime.\n        return dt\n    elif dt.tzinfo is None:\n\tlogger.warning(\"Could not parse timezone {0}\".\n\t\tformat(dt.tzname()))\n\traise ValueError(\"Unable to parse timezone {0}\".\n                     format(dt.tzname()))\n\ttz"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    # Note that if we're an ambiguous datetime, there is nothing that\n    # we can easily calculate from that.\n\tif (dt.tzinfo is tzlocal.None\n        and dt.utcoffset() == datetime.timedelta(0, 1, 0, tzinfo=tzlocal.utc)):\n            return dt\n\n    return dt.replace(tzinfo=tzlocal.utc)\n#encoding:utf-8 \nfrom django.db import models\nfrom django.contrib.auth.models import User, Group\nfrom django.contrib.contenttypes.models import ContentType\n\n'''\n\n\u7528\u6237\u7528\u6237\u76d1\u63a7\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\treturn dt.replace(tzinfo=self)\n\n\tclass _pytz(tzobj):\n\t\t\"\"\"\n\t\tRepresents a particular timezone with timezone-aware data.\n\n\t\tI.e. if ``dt`` does not correspond to a known timezone, ``pytz`` will\n\t\tassume that this is the first time the local machine supports a timezone\n\t\tthat supports at this time.  This will ensure consistent behavior.\n\n\t\tIf the ``dt`` represents a fully known timezone, ``pytz`` will be\n\t\tassumed to be of that time, and the result represented by the\n\t\tprevious time will be midnight"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\tif not isinstance(dt, datetime.datetime):\n# check if dt is a valid date\n\t\tif not dt.utcoffset():\n\t\t\traise ValueError(\"datetime data must be naive\")\n\t\telif dt < datetime.timedelta(0):\n\t\t\traise ValueError(\"datetime data must be naive\")\n\t\t\t\n\tif dt.year < 1970:\n\t\treturn dt\n\telif dt.tz < 1:\n\t\treturn datetime.datetime(dt.year + 1, dt.month - 1, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond, dt.tzinfo"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\tif dt.tzinfo is None:\n\t\treturn dt\n\telse:\n\t\treturn _fromutc(dt).replace(tzinfo=None)\n\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    try:\t# Try to parse the datetime for this one.\n\t\tdt_iso = datetime.datetime.utcfromtimestamp(dt)\n\n\t\t# If this is a fold state, and the datetime contained\n\t\t# an ambiguous datetime object, then use the fold state\n\t\t# in that state.\n\t\tif self.fold == True:\n\t\t\tself.fold = True\n\n\t\t\t# Otherwise, look for a next occurrence of the period start\n\t\t\t# of the ambiguous datetime object as well as any periods past\n\t\t\t# that year.\n\t\t\twhile dt.month == dt_iso.month and dt."}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    return dt - _UTC_ZERO\n\t\ndef utcoffset(dt):\n    \"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates\n\tthe offset of the corresponding day.\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    return _fromoffset(0, -1, 1)\n\ndef tenddelta(days):\n    \"\"\"\n\tUse as \"tdelta\" for one-shots/days.  Default is 7 days.\n\t\"\"\"\n    return days - (days % 7)\n\ndef _fromoffset"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n        return relativedelta(days=self._days,\n\t\t\thours=self._hours,\n\t\t\tminutes=self._minutes)\n\n\t@classmethod\n\tdef from_dtstart(cls, dtstart):\t\n\t\t\"\"\"\n\t\tReturn the corresponding date and time from a :class:`datetime.datetime`\n\t\tand :const:`dateutil.relativedelta.relativedelta` object.\n\t\t\"\"\"\n\t\td = dtstart.replace(tzinfo=start_of_local_timezone(dtstart) if is_aware(dtstart) else None)\n\t\treturn cls(d.year, d.month,"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n      __new__      = DateUtil._tuplize(self, self.__new__)\n      __getnewargs__ = self.__getnewargs__\n      __setstate__ = self.__setstate__\n    \n      return _dateutil_from_tuple(__new__,\n                                     _tuplize(self.hour, self.minute, *self.second, **self.kwds),\n                                     **self.kwds)\n   \n  parse = __parse__\n  \n#----------------------------------------------------------------------------\ndef _dateutil_from_tuple(__tuplize_args, **tupliz_kwds):\n  \"\"\"\n\n  Convert *tuplize_args* and *tupliz_kwds"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn (self._years + self._ months + self._days + self._hours\n\t\t\t + self._minutes + self._seconds) / float(1259.5)\n\ndef _to_int_month(d0, d1):\n    \"\"\"\n    Convert a d0 :class:`datetime.date` object to the number of days since\n    the epoch.  This is needed for round-to-even rounding.  d0 is used only\n    when ``d0 + relativedelta(days=+1)`` is the first value of ``d0``.\n    Examples::\n\n        >>> relativedelta(months=1).to"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn relativedelta(self)\n\t#\n\t#\n#\treturn relativedelta(end_date=relativedelta.relativedelta('year', self))\n#\treturn relativedelta(end_date=dateutil.relativedelta('year', self))\t\n\t#\n\t#\n\t#\n\t#\tif self.end_type.type == 'count':\n\t#\t# we have to treat self.start_date as an end_date\n\t#\t# we have to treat it as an end_date\n\t#\n\t#\telif self.end_type.type == 'duration':"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n\t\n\treturn relativedelta(self.days, self.hours, self.days_plus_one, self.minutes, self.minutes_plus_one)\n\t\t\n\t#def __repr__(self):\n\t#\treturn \"%s(%s, %s)\" % (\n\t#\t\tself.__class__.__name__,\n\t#\t\tself.days,\n\t#\t\tstr(self.hours)\n\t#\t\t\", \".join([repr(self.days), repr(self.hours), repr(self.minutes)])))\n\t\n\t\t\t\ndef default_formatter(years, months,"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\timport pytest\n\nfrom tests.support.asserts import assert_invalidates_len\n\n\ndef test_datetime_from_gregorian():\n    assert to_gregorian(1456061200) == 1456061200\n\ndef test_datetime_before_1980():\n    assert date_before_1980(1456061200) == 1456061200\n\n    assert date_before_1980(1456061256) == 14560641968\n\ndef test_datetime_before_1972():\n    assert date_before_1972(14560612) == 1456"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n    @staticmethod\n    def parse(timestr, default_timezone=None):\n        \"\"\"\n\tParse a time from a string (a human input) and return a :class:`Datedelta` object.\n\tThe timezone either by the strings `'-'``, `-xx`, `+xx:yy`, `z`,\n\t`+xx:zz`, `+xx:zzz` or `+xx:zzzz` are all optional. Returns None if the\n\t`tzinfo` is invalid or none of the elements in `timestr` match the input.\n\n        :param timestr: a string representing a time of"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn relativedelta(years=self.years)\n\nclass Timezone(tzinfo):\n\t\"\"\"\n\tThis class represents the \"local\" time zone (using daylight\n\tas defined in ISO 8601 dates). This is just a wrapper around the\n\t``datetime.tzinfo``.\n\t\"\"\"\n\n\tdef __init__(self, hours, minutes, seconds, timezone, **kwds):\n\t\t\"\"\"Initialize with two dates (hours, minutes, seconds).\n\n\t\tThe keyword arguments are interpreted the same way as\n\t\t:func:`datetime.datetime.strptime`.\n\n\t\t:param hours: Number of hours,"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n\n\treturn date(self._year, self._month, self._day, self._hour, self._minute, self._second, self._microsecond+self._microsecond/1000)\n\n\nclass timedelta(object):\n\tdef __init__(self, date=None, days=None, hours=None, minutes=None, seconds=None, microseconds=None, timezone=None):\n\t\t\"\"\"\n\tA class representing a ISO 8601 duration: days, hours, minutes and seconds.\n\tAdditionally, a date property takes the following keyword arguments:\n\t\n\tyears:   - years of this duration"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn relativedelta(self.days, self.hours)\n\tdef normalized_number(self, precision=None):\n\t\t\"\"\"\n\t\tReturn a version of this object represented entirely using the provided numeric data.\n\t\tA negative precision may be set by specifying the precision parameter in days.\n\t\tThis parameter defaults to 52 digits so that any magnitude positive value in the returned\n\t\tdate will be treated as zero.\n\t\t\"\"\"\n\t\tprecision = precision or self.precision\n\t\tif not precision:\n\t\t\treturn relativedelta(days=-self.days)\n\t\tif precision!= self.precision:\n\t\t\traise ValueError"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc\n\n\nif sys.version_info < (3,):\n\t# Python 3?\n\tdef b64(data): return str(base64.b64encode(data).decode(\"utf8\")).decode(\"utf8\")\nelse:\n\t# Python 2\n\n\tdef b64(data): return data\n\n\ndef bytes2str(data):\n\t\n\treturn data.decode(\"utf8\", \"utf8\")\n\t\t\n\ntry:\n\tunicode\nexcept NameError:\n\tbasestring = str\nelse:\n\tunicode = unicode_str\n\n\ndef str2bytes(data):"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc(*_get_tzname(namefunc).encode('utf-8'))\n\n\n_UTF8 = None\n\ndef _get_encoding():\n\t\"\"\"\n\tRead the charset from the terminal.\n\tE.g. from command line:\n\t\"gbk\\x95\\xa6\\x96\\xa7\\x95\\xa2\\x96\\xa3\\x96\" -> \"gbk\\xc2\\xa6\\x96\\xa7\\x95\\xa2\\x96\"\n\t\"\"\"\n\treturn 'gbk'\n\t\ndef"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tif namefunc is bytes:\n\t\t\t# Python 2\n\t\t\treturn namefunc()\n\t\telif namefunc is unicode:\n\t\t\t# Python 3\n\t\t\treturn namefunc()\n\telse:\n\t\t# Python 3\n\t\treturn namefunc()\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\n    if _PYTHON3: sys.version = [3]\n    try:\n        import locale\n    except:\n        return\n \n    class AsciiStrings:\n        \"\"\"\n        Set attribute names and convert it to unicode if needed\n        \"\"\"\n        encoding = 'ascii'\n\n        _unicode = str\n   \n        # Python 2\n        def __add__(self, other):\n            \"\"\"This method adds unicode strings if possible\"\"\"\n            if isinstance(other, unicode):\n                return unicode.__add__(self, other)\n            elif isinstance(other, _unicode):\n                return other.__class__(self)\n            return unicode.__add__(self, other)\n\n       "}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\timport sys\n\t\treload(sys) #python2 reverts sys so that repr() will return bytes\n\t\treturn unicode(namefunc('utf8'))\n\t# Python 3/3 differences\n\telif sys.version_info[0] < 3:\n\t\tfor k, v in sys.modules.iteritems():\n\t\t\tif v == namefunc:\n\t\t\t\tsys.modules[k] = v\n\t# Python 3 differences\n\telse:\n\t\t# python2 doesn't require Unicode string functions\n\t\tfor k, v in (namefunc(name) for name in dir(namefunc)"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\timport unicodedata\n\n\t\tif namefunc == None or len(namefunc) == 0:\n\t\t\treturn None\n\n\t\tm = namefunc.lower()\n\n\t\tif m == 'u':\n\t\t\treturn unicode(namefunc)\n\t\telif m == 'b':\n\t\t\treturn bytes(u'\\u4e2d')\t\t\t\t\n\t\telif m == 't':\n\t\t\treturn time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(namefunc))\t\n\t\telse:\n\t\t\traise ValueError\n\n\t# end of http://code.activestate.com/recipes/"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn unicode( namefunc.__name__, 'utf-8' )\n\n# ----------------------------------------------------------------------\n# The following functions are for python 2 / 3 compatibility.\n# ----------------------------------------------------------------------\n\n# ----------------------------------------------------------------------\n# Python 2+3 Compatibility shamelessly stolen from Perl version 0.2\n# ----------------------------------------------------------------------\n\ndef _test():\n\t\"\"\"Check this implementation against the official version\"\"\"\n\tteststr = 'Hello, World\\n  Please see\\nyou are using python 2+3.'\n\tassert ascii3.encode_to_raw(teststr) == ['Hello, World', 'Hello,"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tif not namefunc: return\n\tdef encode_unicode(s):\n\t\tif isinstance(s, str): return s\n\t\tif isinstance(s, unicode): return s.encode('ascii', 'backslashreplace')\n\tencode = sys.stdout.encoding or 'ascii'\n\tdecode = sys.stdout.encoding or 'ascii'\n\ts = encode(s)\n\tn_s = len(s)\n\tif n_s < 256: return s\n\tif s.startswith('#'): return s # already unicode\n\tout = []\n\tfor i in range(0"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn codecs.encode(namefunc,'mbcs').decode(\n\t\t\t\tencoding='mbcs').encode('utf-8')\n\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n#coding:utf8\n\"\"\"\nThis module defines the models required for the Drive app. \n\nCopyright (C) 2013  Fr\u00e9d\u00e9ric Massart - FMCorz.net \n\nThis program is free software: you can redistribute it and/or modify \nit under the terms of the GNU General Public License as published by \n"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t return namefunc.encode(\"ASCII\", \"backslashreplace\")\n\ndef main():\n\t\"\"\"\n\tGet the name of default encoding for GNU gettext\n\tNote, the default encoding for the terminal is ENGINE, not\n\tUTF-8\n\t\"\"\"\n\tfrom config import __default_encoding\n\t# get default encoding\n\tencoding = __default_encoding__['encoding']\n\t\n\n\tdef set_charset(encoding):\n\t\t\"\"\"\n\tSet our default encoding, in Unicode string `encoding`\n\t\"\"\"\n\t\tif encoding:\n\t\t\t# windows supports unicode input\n\t\t\ttry:\n\t\t\t\tif"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tversions = {}\n\t\twith open(VERSION_FILE) as f:\n\t\t\tfor line in f:\n\t\t\t\t# Match a line like \"version : 2.0.3\\nVERSION = '2.0.3'\"\n\t\t\t\tmatch = re.match(r\"^(\\s*.*)\\s*$\", line)\n\t\t\t\tif match:\n\t\t\t\t\tversions[match.group(1)] = match.group(2)\n\t\treturn versions\n\n\t@staticmethod\n\tdef get_package_version(package):\n\t\t\"\"\"\n\t\tGet version information for the named package.  \n\t\tShould match"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tglobal VERSION\n\n\t\tif VERSION is None:\n\t\t\tVERSION = _get_version('sphinxcontrib-build')\n\t\treturn VERSION\n\t\n\t@property\n\tdef version_parts():\t\n\t\t\"\"\"\n\t\tReturn the version number as a list of string as a tuple.\n\t\t\"\"\"\n\t\treturn tuple(int(part) for part in WKV[\"version_parts\"].split(\",\"))\n\t\n\t@property\n\tdef version(self):\n\t\t\"\"\"\n\t\tReturn the package version as a string\n\t\t\"\"\"\n\t\ttry:\n\t\t\tret = self.get_version_from_command('"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tvcs_version = 'unknown'\n\t\tout, err = execute(\"git describe --tags --always --match HEAD\".split())\n\t\tif len(out) == 0:\n\t\t\treturn None\n\t\tout = out.replace('-dirty','')\n\t\tout += \"-%s\" % (version,)\n\t\treturn out\n\n\tdef get_version(self, version_suffix = 'current'):\n\t\t\"\"\"\n\t\tRun git describe --always -l for a particular versionsuffix\n\t\treturns a string\n\t\t\"\"\"\n\t\tvcs_version = \"unknown\"\n\t\terr_msg = \"Unable to run"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\treturn VERSION\n\treturn get_version()\n\ndef get_current_branch():\t\n\t\"\"\"\n\tReturn the current branch or a default branch. Also, this can be used, when\n\tusing \"git fetch\" commands; and for commit commands, we use a cache to return\n\tNone.\n\t\"\"\"\n\tenv = os.environ\n\tif not env.get('GIT_BRANCH', None):\n\t\tenv['GIT_BRANCH'] = _get_current_branch()\n\n\treturn env.get('GIT_BRANCH')\n\ndef get_last_commit_message("}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\timport frappe.utils\n\t\timport frappe.model.db_query\n\n\t\tfrom utilities.transaction_base_query import validate_transaction_settings\n\t\tvalidate_transaction_settings()\n\t\tversions = [\"{}.{}\".format(cstr(s[0]), s[1]) for s in frappe.get_all(\"Version\", filters={\"series\": [\"in\", \"v\"]}),\n\t\t       [\"{}.{}\".format(cstr(s[0]), s[1]) for s in frappe.get_all(\"Version\", filters={\"series\": [\"=\", \"v\"]}, as_list=True)[0]]"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\t_Version(False)\n\t\treturn _Version(True)\n\treturn _Version(False)\n\ndef _Version(show_debug=True):\n\t\"\"\"\n\tIf show_debug=False, write the version information to log.\n\t\"\"\"\n\tif show_debug:\n\t\tif ShowDebug is True:\n\t\t\tprint 'Debug: Version information for version %s is:\\n%s\\n' % (__version__, __desc__)\n\t\nif __name__ == '__main__':\n\tif len(sys.argv)==1:\n\t\tversion()\n\telse:\n\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry :\n\t\t\timport gtk, gobject, pango\n\t\texcept ValueError:\n\t\t\treturn None\n\t\tgtkversion = pango.__version__\n\t\n\tif hasattr(gtk, \"require_version\") :\n\t\t_gtk_version = gtk.require_version(v)\n\telse :\n\t\t_gtk_version = None\n\t\n\tif not _gtk_version :\n\t\treturn None\n\t\n\treturn __gtk_version\n\ndef gdk_pixbuf_new_from_file(file, size=512, colormap=None, alpha=False):\n\t\"\"\"\n\tReturn an L"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\ttaggit = tag[0].splitlines('-', 1)[0]\n\t\texcept IndexError:\n\t\t\traise NoVersionError(tag)\n\t\treturn taggit[:taggit.index(':')].strip()\n\n\tdef get_commits(version_tag):\n\t\t\"\"\"\n\t\tGet list of commits made upstream at a given tag.\n\n\t\t:param version_tag: Version tag to find list of commits made upstream.\n\t\t:type version_tag: str\n\t\t\"\"\"\n\t\treturn check_output(['git', 'log', '--format=%B', '--reverse', '--pretty=%s',"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\treturn cversion_to_versions[get_version(exclude_major=True)]\n\ndef get_timestamp(version):\n\ttry:\n\t\treturn cversion_to_timestamp[get_version(exclude_major=True)][version]\n\texcept KeyError:\n\t\treturn None\n\n# -----------\n# Functions\n# -----------\ndef show_versions(quiet=True):\n\t\"\"\"Prints the versions from the database formatted as a list of tuples containing (cversion, timestamp).\"\"\"\n\tdb = get_connection()\n\n\tif quiet >= 0:\n\t\tif db.version"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tversion_str = None\n\t\tif os.path.exists(os.path.join(waf_dir, VERSION_FILE)):\n\t\t\tf = open(os.path.join(waf_dir, VERSION_FILE))\n\t\t\ttry:\n\t\t\t\tversion_str = f.read().strip()\n\t\t\t\tf.close()\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\tif version_str is None:\n\t\t\treturn (0, 0, 0)\n\n\t\t# get main info\n\t\tret = version_str.split('.')\n\t\tmain_version = float(ret[0])\n\t\tmain"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\traise NotImplementedError\n\n# -----------------------------------------------------------------------------\n# Copyright (C) 2013-2014 Bloomberg Finance L.P.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tformatter = style._get_formatter()\n\t\tbuilder = style._get_builder()\n\t\tmetadata = style._get_metadata()\n\t\tbuilder.freeze()\n\t\tif formatter is None:\n\t\t\tif metadata is None:\n\t\t\t\treturn ''\n\t\t\telif builder.format == 1:\n\t\t\t\tmetadata = [formatter.resource_string]\n\t\t\telif builder.format == 3:\n\t\t\t\tmetadata = [formatter.version]\n\t\t\telif builder.format == 4:\n\t\t\t\traise ValueError(\"unsupported or invalid format \" + builder.format)\n\t\t\ttemplate = None\n\t\telse:\n\t\t\tif not"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tparts[\"style\"] = style\n\t\trendered = pieces[\"rendered\"]\n\t\tfrag = rendered[0]\n\t\tif pieces[\"style\"].lower() == \"bg\":\n\t\t\tfrag = highlight(frag, CSS[\"bg\"][\"bgcolor\"])\n\t\tif fragments[\"type\"].lower() not in [\"html\", \"xhtml\"]:\n\t\t\tfrag = frag.replace(\">\", \"\")\n\t\tif \"style\" in rendered[1:]:\n\t\t\tfrag = frag.replace(\">\", \"\")\n\t\treturn (frag, rendered[1:], pieces[\"style\"], pieces)\n\n\tdef __call__(self, text, **kwargs):\n\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\n\t\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"specific\", \"full-revisionid\": pieces.get(\"long\"), \"dirty\": None, \"error\": pieces[\"error\"]}\n\t\n\tstyleguide = style[\"tool\"]\n\tif toolbox == \"maven\":\n\t\treturn maven.render(pieces, styleguide)\n\telif styleguide == \"tango\":\n\t\treturn tango.render(pieces, styleguide)\n\telif styleguide == \"cleancss\":\n\t\treturn cleancss.render(pieces, styleguide)\n\telif styleguide == \"syntax\":\n\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn render_from(pieces, style, {})\n\n\tdef render_any(self, pieces, style):\t\"\"\"\n\tRender any version piece into the requested style.\t\"\"\"\n\t\treturn render(pieces, style, {})\n\ndef render_with(pieces, style):\n\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\treturn rendered_with(pieces, style)\nimport sys, getopt\n\nsys.path.extend(['../pylibs'])\n\nimport numpy\nimport scipy.ndimage as nd\nimport scipy.optimize as spo\nslim ="}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tstyle = style.copy()\n\t\tfor part in pieces:\n\t\t\tif isinstance(part, tuple):\n\t\t\t\tstyle = copy.copy(style)\n\t\t\t\tpart = part[0] + part[1]\n\t\t\telif isinstance(part, int):\n\t\t\t\tpart = str(part) # can raise ValueError\n\t\t\telif not isinstance(part, str):\n\t\t\t\tcontinue\n\t\t\tif style:\n\t\t\t\tpart = render_style(part, style.get('font'), style.get('font_size'))\n\t\t\tpara = render_para(part, style.get('font'), style.get"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tif pieces[\"error\"]:\n\t\t\treturn {\"version\": \"unknown\", \"full-revisionid\": pieces.get(\"long\"), \"dirty\": None, \"error\": pieces[\"error\"]}\n\n\t\tstyle = pieces[\"style\"]\n\t\tif style == \"default\":\n\t\t\tstyle = \"pep440\"\n\t\t\tif style in (\"pep440\", \"pep440-pre\", \"pep440-post\", \"pep440-old\"):\n\t\t\t      style = \"pep440-pre\"\n\t\telif style == \"pep440-post\":\n\t\t        style = \"pep440-post\"\n\t\telif style == \"pep"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tif pieces[\"error\"]:\n\t\t\traise ValueError(\"version must be an error value\")\n\n\t\tmax_columns = None\n\t\tmax_rows = None\n\t\tfor i, item in enumerate(pieces[\"items\"]):\n\t\t\twidth = max_columns or i\n\t\t\tmax_columns = max(max_columns, len(item[1]))\n\t\t\tmax_rows = max(max_rows, i+1)\n\n\t\tstyle[\"max_columns\"] = max_columns + 1\n\t\tstyle[\"max_rows\"] = max_rows + 1\n\n\t\toutput = [\n\t\t\t'<html><body>\\"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn pieces[\"outer\"]\n\nclass PHPVersionSplit:\n\t\"\"\"\n\tA class to split a version string into a list of\n\t\"major.minor\" sub-versions.\n\t\"\"\"\n\tdef __init__(self, parts):\n\t\t\"\"\"\n\t\tEach part in \"parts\" should be a unicode string.  This\n\t\tclass is useful forversion() to portably produce an escaped\n\t\tversion string.\n\t\t\"\"\"\n\t\tif isinstance(parts, str):\n\t\t\tparts = parts.encode(\"utf-8\")\n\n\t\tif _unicode:\n\t\t\tparts = _unicode"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\n\n\n\tstyle['page-count'] = 1\n\n\t# Copy the user data to a temporary file, in case user-supplied data is not available\n\ttemp = tempfile.NamedTemporaryFile(mode='w', suffix='.html')\n\tos.system('xdg-open \"%s\"' % temp.name)\n\n\t# Render the page into the user's temporary file\n\tos.system('xdg-render \"%s\" \"%s\" %s' % (temp.name, pieces[0], pieces[1]))\n\n\t# Render a new page of text instead of normal text\n\tfile = open"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\ndef split_at_column(paragraph, words):\n\t\"\"\"\n\tReturn the column at which we are in the at-most col\n\t(usually the first word).  A \"sentence\" is one sentence,\n\tand a \"word\" is the remaining words.  If two words are not yet\n\tdone, then this is the one where they are.\n\t\"\"\"\n\tdef is_first(x):\n\t\tif len(x) == 0:\n\t\t\treturn False\n\t\telif len(x) == 1:\n\t\t\treturn x[0] == words[0]\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\t# FIXME: doesn't handle leading dots in the middle\n\t\tm = re_colon.match(pieces[0])\n\t\tif m and spaces:\n\t\t\treturn \"\\\\\".join(pieces[1:-1]), m.groups()[0]\n\t\telse:\n\t\t\treturn \".\"\n\n\nclass RezSettings(object):\n\t\"\"\"\n\tRead and write a configuration file with a section\n\t\"\"\"\n\tdef __init__(self,filename):\n\t\tif not os.path.isfile(filename):\n\t\t\traise ValueError(\"Config file '%s' does not exist\" % filename)\n\t\tself"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tresult = ''\n\t\tfor p in pieces:\n\t\t\tif p!= '':\n\t\t\t\tresult = result + p\n\t\treturn result\n\n\t# split string into string and number parts\n\t# e.g. '3,6' becomes '3,6'\n\ts, snum = split(s)\n\tif len(snum) < 3:\n\t\tresult = ''\n\t\tfor p in pieces:\n\t\t\tif p!= '':\n\t\t\t\tresult = result + p\n\t\treturn result\n\telse:\n\t\tresult = parts.join((plus_or_dot, s))\n\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif pieces[0]!= '.':\n\t\t\treturn ''\n\t\tif pieces[0][0]!= '-':\n\t\t\treturn'(%s)' % pieces[0]\n\t\telse:\n\t\t\treturn ''\n\n\n# Parse the options, and match\ndef parse(options):\n\tif len(options)!= 1:\n\t\tprint(\"%s: expecting exactly one argument\" % sys.argv[0])\n\t\treturn 1\n\telif options[0] == '-v':\n\t\tverbose = 1\n\telif options[0] in ('-h', '--help'):\n\t\tprint(\"%s: print help"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tnewpieces = []\n#\t\tfor piece in pieces:\n#\t\t\tif piece.islower() or piece.isupper():\n#\t\t\t\tcontinue\n#\t\t\tnewpieces.append(piece + piece)\n#\t\treturn Path([newpieces])\n\tfirstpiece = pieces[0]\n\tif len(pieces) == 1:\n\t\treturn firstpiece\n\tnewpieces = []\n\tprevious = pieces[0]\n\tfor piece in pieces[1:]:\n\t\tnext = piece.getprev()\n\t\tif piece.name.lower()[-1] in {'"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces) < 1:\n\t\t\treturn None\n\t\ttry:\n\t\t\t# add an extra newline to separate strings -- this will strip everything between <br /> and so on\n\t\t\treturn commonprefix(pieces[0]) + commonsuffix(pieces[1:])\n\t\texcept:\n\t\t\treturn None\n\n\tdef replace_dots(pieces):\t# remove underscores from line ending with a dash\n\t\tl = []\n\t\tfor piece in pieces:\n\t\t\tif \"'\" in piece:\n\t\t\t\tpart = piece.replace(\"'s\", \"\").replace(\"'\", \"\")\n\t\t\telse:\n\t\t\t\tcontinue\n\t\t\tif len"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif pieces.endswith('/'):\n\t\t\treturn pieces[:-1]\t\t# do both\n\t\t# if they have a space we must add a slash\n\t\tif pieces.endswith('/'):\n\t\t\twhile pieces.endswith('/'):\t\n\t\t\t\tpieces.pop()\n\t\treturn '/' + '/'.join(pieces)\n\n\tdef split_paths(self, paths):\n\t\treturn [p for p in paths.split(':') if  p]\n\n\tdef strip_drive(self, path):\n\t\treturn path.strip('/')\n\n\tdef split_drive(self, path):\n\t\tif path[0] == ':':"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tcommon = lambda a, b: a + b\n\t\tres = common(pieces[1:])\n\t\treturn res\n\n\tclass Matcher(object):\n\t\tdef __init__(self):\n\t\t\tself.opcodes = []\n\t\tdef __call__(self, line):\n\t\t\tif line.startswith(\"(op \"):\n\t\t\t\tself.opcodes.append(line[7:-2])\n\t\t\t\treturn []\n\t\t\treturn [\"(\", line]\n\t\tdef __str__(self):\n\t\t\tret = \" \".join(self.opcodes)\n\t\t\treturn ret\n\nclass CompilationError(Exception):\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces) == 1: return pieces[0]\n\t\tif len(pieces) == 2: return pieces[1]\n\t\tplus = self.plus(pieces[0])\n\t\tif not hasattr(plus, \"__next__\"): return None\n\t\tresult = _apply_func(_plus, 1)\n\t\tif result is None: raise AddError(\"unable to compute with \"+repr(long_string))\n\t\treturn result\n\n\tdef count_if_equal(self, pieces1, pieces2):\n\t\tif pieces1 == pieces2: return 1\n\t\tif pieces1 =="}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces) == 0:\n\t\t\treturn ''\n\t\treturn pieces[0] +'' + pieces[1]\n\n\n# ==================================================\n# Argument Parsing\n# ==================================================\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n                description='Give a series of options given as arguments'+\n                    '(as strings).  Specifying --arg=val' +\n                   'prints out a list of options where each option is a' +\n                    \" long, like --arg='--arg2=1,2'\" +\n                    \" prints out a list of options where there are more\" +"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\t# See if anything is using the shell.\n\t\tcommand_name = None\n\t\ttry:\n\t\t\t# We try with user-defined commands first\n\t\t\tp = None\n\t\t\tcommand_name = commands[0]\n\t\t\tif isinstance(command_name, str):\n\t\t\t\t# Handle command command\n\t\t\t\tif command_name.lower() == \"help\":\n\t\t\t\t\tprint Help\n\t\t\t\telif command_name.lower() == \"exit\":\n\t\t\t\t\tp = True\n\t\t\t\telse:\n\t\t\t\t\traise TypeError(\"Unknown command '%s'.\" % command_name)\n\t\t\telse:\n\t\t\t\t# Handle multiple commands"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp = None\n\t\tif args:\n\t\t\tfor c in commands:\n\t\t\t\tif verbose and not hide_stderr:\n\t\t\t\t\tprint(c)\n\t\t\t\tif p is None:\n\t\t\t\t\tp = subprocess.Popen(c, cwd=cwd, env=env)\n\t\t\t\telse:\n\t\t\t\t\tp.wait()\n\t\t\t\t\tif verbose:\n\t\t\t\t\t\tprint(c, end=' ')\n\n\t\telse:\n\t\t\tp = subprocess.Popen(args, cwd=cwd, env=env)\n\n\t\treturn p.wait() == 0\n\n\tdef run_commands(commands, args, cwd=None, verbose"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif verbose:\n\t\t\tprint()\n\t\tif hide_stderr:\n\t\t\tstdout = None\n\t\telse:\n\t\t\tstdout = subprocess.PIPE\n\n\t\tp = subprocess.Popen([c for c in commands], cwd=cwd,\n\t\t                         stdout=stdout, stderr=subprocess.STDOUT)\n\t\toutput = p.communicate()[0] # communicate() returns a tuple with the Popen object\n\t\tif p.returncode not in [0, 1]:\n\t\t\traise RuntimeError(\"Command failed: %s\" % output)\n\n\t\tout = p.stdout.decode(locale.getpreferredencoding())\n\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tlogging.info(' '.join([pipes.quote(c) for c in commands]))\n\t\ttry:\n\t\t\tp = subprocess.Popen(commands, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\texcept Exception:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Call failed to execute the following %s\" %''.join(commands))\n\t\t\traise\n\t\tstdout_data, stderr_data = p.communicate()\n\t\tif p.returncode!= 0 and not hide_stderr:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Call failed to execute the"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\t\n\ttry:\n\t\twith subprocess.Popen(commands, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env) as proc:\t\n\t\t\ttry:\n\t\t\t\tstdout, stderr = proc.communicate()\n\t\t\texcept Exception as e:\n\t\t\t\traise CliError(\"Subprocess.execute() encountered an internal error:\\n%s\" % (e))\n\t\t\tif verbose:\n\t\t\t\tprint stdout\n\t\t\tif hide_stderr:\n\t\t\t\tlines_without_stderr = stdout.expandtabs().splitlines()\n\t\t\t\tfor line in lines_without_stderr:\n\t\t\t\t\tsys.stderr"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp = find_executable(commands, path=cwd)\n\t\tif verbose:\n\t\t\tp =' Running command \"%s\".' %''.join(commands)\n\t\tif hide_stderr:\n\t\t\tp +='(and run this tool with no output)'\n\t\telif debug:\n\t\t\tp +='(' +''.join(commands) + ')'\n\t\treturn (p, True)\n\t\t\n\tp, o, e = run_command(commands, cwd, verbose)\n\tif p == True:\n\t\t# Special case: we have an environment variable, and\n\t\t# we should"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif env:\n\t\t\tenv_list = None\n\t\t\tfor env_value in env:\n\t\t\t\tenv_list = os.environ.get(env_value, None)\n\t\t\t\tif env_list:\n\t\t\t\t\tbreak\n\t\t\tif env_list is None:\n\t\t\t\tprint(\"** the environment variable `%s` is not set and required.\" % env_name, file=sys.stderr)\n\t\t\t\tsys.exit(1)\n\t\t\tcommand = commands[0]\n\t\t\treturn_code = subprocess.call(command, shell=True, stdin=sys.stdin, stdout=sys.stdout"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tlog(' '.join(commands) +'' +''.join([\"[%s]\" % x for x in cmdargs]), 2, 4)\n\n\t\tif dryrun is True:\n\t\t\tstatus = 0\n\t\telif verbose is True:\n\t\t\tstatus = 1\n\t\terrlines = []\n\t\tif cwd is not None:\n\t\t\ttry:\n\t\t\t\tos.chdir(cwd)\n\t\t\t\tf = Popen(*cmdargs, stdout=PIPE, stderr=STDOUT, cwd=cwd_setup, shell=False, universal_newlines=True)\n\t\t\t\terrlines = f.readlines()\n\t\t\t\tf."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif verbose:\n\t\t\ttry:\n\t\t\t\told_stdout, old_stderr = os.popen3(args, cwd, False).communicate()\n\t\t\texcept Exception as e:\n\t\t\t\tos.popen3 = lambda cmd: -1\n\t\t\t(status, output) = call_cmd(commands, args, env=None, verbose=False)\n\t\telse:\n\t\t\tstatus = 0\n\n\t\tif status!= 0 or verbose:\n\t\t\tif show_stdout:\n\t\t\t\tprint(output)\n\n\t\t\tif hide_stderr:\n\t\t\t\t# Only show stderr if a log file was specified.\n\t\t\t\twith"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif type(commands) == str:\n\t\t\tcommands = shlex.split(commands)\n\t\telse:\n\t\t\tcommands = list(commands) # in case it's a string\n\t\tif cwd:\n\t\t\tcommands = [self.encode(cstr,'surrogate_or_strict') for cstr in commands if cstr]\n\t\t\tp = subprocess.Popen(commands, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\t\tstdout, stderr = p.communicate()\n\t\t\tretcode = p.returncode\n\t\t\tif retcode:\n\t\t\t\traise SafeException"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tloc = get_default_loc()\n\t\tconfig.versionfile_source = loc + \"/versioneer.py\"\n\t\tconfig.verbose = False\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\tdef __init__(self, method):\n\t\t Exception.__init__(self)\n\t\tself.method = method\n\n\tdef __str__(self):\n\t\treturn \"{0} is not a valid format for method {1}\\n\"."}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.read_file(\n\t\t\tpath.join(parent.parent.abspath(), 'VERSION'),\n\t\t\tparent.parent.joinpath(parent.pardir,'versioneer.py')\n\t\t)\n\t\t# NOTE: the following line might be unnecessary (and\n        # should never be included anyway):\n        # if config.get('versioneer_build', None) is not None:\n        #     raise ValueError(\"Invalid version string\")\n\n        # It's important to not use objects for setuptools!\n        # But \"standard\" modules installed by distutils should not\n        # provide an __"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\t# unless we update this, the command will keep using the selected version.\n\t\tversionfile_source = \"setup.cfg\"\n\t\tconfig.versionfile_build = \"\"\n\t\tconfig.tag_prefix = \"\"\n\t\tconfig.parentdir_prefix = \"easybuild-\"\n\t\tconfig.versionfile_source_dir = \"easybuild-build\"\n\t\tconfig.tag_prefix_GIT = \"0x2.3\"\n\t\tconfig.tag_prefix_GIT_IMAGE = \"abin\"\n\t\tconfig."}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n\t\t# _version.py\n        cfg = VersioneerConfig()\n        cfg.VCS = \"git\"\n        cfg.style = \"%(STYLE)s\"\n        cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n        cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n        cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n        cfg.verbose = False\n        return cfg\n\n    def get_cmdclass(self, cmd):\n        \"\"\"\n        Return the class to use for all commands.  Typically,"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\t# Only set the output of `versioneer -c` commands.\n\t\tconfig.versionfile_source = \"mistune/_version.py\"\n\t\tconfig.verbose = False\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\tpass\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n    def"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"datrada-\"\n    cfg.versionfile_source = \"datrada/_version.py\"\n    cfg.verbose = False\n    return cfg\n\nclass NotThisMethod(Exception): pass\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\n\t\t# We defer to other modules of _u2w to resolve these\n\t\t# modules\n        config.importer = _importer\n\t\tconfig.cfg = cfg\n\t\tconfig.register_listener(dict(\n\t\t\tpackage_dir = ('lxml', ),\n\t\t\tindex_url = ('http://github.com/pydata/xray/python-xray/tree/v%s/' %\n\t\t\t\t\t\t_imp.python_version()),\n\t\t\tregister_systype_handler ='resolve_symtype',\n\t\t\tunpackree = 'xml', use_2to"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tfrom setuptools import setup\n\t\t# NOTE(dstAnq=0.3) It would require another parent, which uses get_cmdclass()\n\t\t# and parent.__init__(). However, that class defines get_cmdclass() using the\n\t\t# _get_cmdclass() method, and the parent.cmdclass attribute is either None,\n\t\t# which indicates to us no operation.\n\t\tparent = VersioneerConfig()\n\t\tsetup_command = versioneer.get_cmdclass()\n\t\t# This is a bit ugly, but there is no other place that helps\n\t\t# people regress"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tfrom configparser import ConfigParser\n\t\tconfig = ConfigParser()\n\t\tconfig.read(\"setup.cfg\")\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Add a VCS handler to the VCS.\"\"\"\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# these strings will be replaced by git during git-archive.\n\t\t# setup.py/versioneer.py will grep for the variable names, so they must\n\t\t# each be defined on a line of their own. _version.py will just call\n        # get_config() if it's present.\n        cfg = VersioneerConfig()\n        cfg.VCS = \"git\"\n        cfg.style = \"%(STYLE)s\"\n        cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n        cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n        cfg.versionfile_source = \"%("}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tvcs.register_vcs_handler(vcs, method)\n\t\treturn method\n\treturn register_vcs_handler  \n\n# This decorator adds version control to VCS\n# It will be called only if this module is imported, otherwise it does import nueva/ui/gtk*.\n__version__ = {}\n\n# Info panel\n\nclass VersionPanel:\n\t\"\"\"\n\tPanel of version control.\n\t\"\"\"\n\tdef __init__(self, builder, text = None, module_name = None, version = \"HEAD\"):\n\t\t\"\"\"\n\t\tInitialize VersionPanel with name and default"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tif vcs not in VCS_HANDLERS:\n\t\t\t\traise Exception(\"Unknown VCS: %s\" % vcs)\n\t\t\tvcs_handler[vcs] = MethodType(f, vcs, method)\n\t\t\treturn vcs_handler[vcs]\n\n\t\treturn decorate\n\n\treturn register_vcs_handler\n\n\ndef has_vcs_handler(vcs_handler):\n\t\"\"\"Returns true if the VCS has a handler assigned to it.\"\"\"\n\tfor vcs, handlers in iteritems(VCS_HANDLERS):\n\t\tfor vcs_handler in handlers:\n\t\t\tif vcs_handler."}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef handle_vcs_method(self, *args, **kwargs):\n\t\t\tvcs_handler = getattr(vcs, method)\n\t\t\tvcs_handler(self, args, kwargs)\n\t\treturn handle_vcs_method\n\n\treturn register_vcs_handler\n\n\ndef get_outdated():\n\t\"\"\"Get the latest state for the version control system\"\"\"\n\tcmd = \"hg log -1 --pretty=format:'%h %ae\\nHEAD'.\\n$HGLOGBIN\"\n\ttry:\n\t\traw = subprocess.check_output(cmd, shell=True)\n\t\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tfunc = vcs_handler(vcs)\n\t\tfunc.name = method.__name__  # this also sets the function name the VCS expects\n\n\tdef add_vcs_callback(vcs, callback):\n\t\t\"\"\"\n\t\tAdd a new VCS event callback.\n\t\t\"\"\"\n\n\t\t# get the real function pointer.\n\t\tvcsfunc = getattr(vcs, method.__name__)\n\n\t\t# create a new version of the VCS, then set the function name.\n\t\tvcsfunc.func_code = generate_vcs_func(vcs)\n\n\t\t# store the function name.\n\t\tvcsfunc.func"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(vcs.VCS_METHOD_MOVE, f)\n\t\t\tvcs.register_vcs_handler(vcs.VCS_METHOD_COPY_FROM_REVID, f)\n\t\treturn decorate\n\treturn decorate\n\n@register_vcs_handler('git-blob', 'git-blob')\ndef git_blob_handler(view, buf, path):  # pylint: disable=unused-argument\n\tbuf.props.is_blob = True\n\treturn True\n\n@register_vcs_handler('file', 'file"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs_handler_map[vcs] = f\n\t\t\treturn f\n\t\treturn decorate\n\n\treturn register_vcs_handler\n\n\ndef create_command_parser():\n\t\"\"\"Utility function that creates a nice invocation parser for the command.\"\"\"\n\t\n\tusage = \"usage: %prog [options] [args]\"\n\n\tdescription = \"\"\"\\\nThe '{prog}' command is the top level command handler of Mercurial's\nvcs framework. It parses the arguments and creates the configured\nsetup.py in ``conf.py``.\n\"\"\"\n\n\tparser = optparse.OptionParser(usage = usage"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\t\t\t\t\t\t\t\t# pylint: disable=C0111\n\t\tif vcs not in vcs_handlers:\t\t\t\t\t\t\t\t# add_vcs is really only here to avoid an import loop.\n\t\t\tvcs_handlers[vcs] = {}\n\t\tvcs_handlers[vcs][method] = f     # note: f must be callable here\n\n\t\treturn decorate\n\n\tdef create_vcs_handler(vcs, method):\n\t\t# copy from vcs_handlers to add the new one\n\t\tvcs_handlers[vcs][method] = []\n\n\t\tfor vcs_handler in vcs_handlers[vcs"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef wrap(self, do_method):\n\t\t\tsetattr(vcs, self.wrapped_vcs_methods + (method,), do_method)\n\t\t\treturn do_method\n\t\treturn wrap\n\n\ndef create_vcs(config):\n\t\"\"\"\n\tCreates and returns a new VCS object for the given config.\n\t\"\"\"\n\tvcs = config.guess(vcs_mode=True)\n\treturn vcs if vcs is not None else Config.create_from_file(VCS_CONFIG_FILE)\n\n\n# vim: no tabstop=4 shiftwidth=4 softtabstop=4\nfrom collections import"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.vcs_handlers[method.__name__] = f\n\t\t\treturn f\n\t\treturn decorate\n\n\tdef __filter_out_vcs_versions(vcs, target):\n\t\t\"\"\"\n\t\tFilter out VCS-specific VCS versions and return only those that\n\t\tare up-to-date.\n\t\t\"\"\"\n\t\tvcs_path = os.path.join(get_vcs_path(vcs.basedir), vcs.path)\n\n\t\tvcs_paths = []\n\t\tfor vcs_handler in VCS_HANDLERS:\n\t\t\tvcs_paths.extend(vcs_"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tname = method.__name__  # noqa\n\t\tdoc = get_converter().convert(vcs, method, name)\n\t\tdoc['init_version'] = False\n\t\tdoc['name'] = name\n\t\treturn doc\n\n\t@register_vcs_handler('git', 'git')\n\t@register_vcs_handler('hg', 'hg')\n\t@register_vcs_handler('svn','svn')\n\t@register_vcs_handler('cvs', 'cvs')\n\t@register_vcs_handler('cvs-svn', 'cvs-svn')\n\t@register_vcs"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\tcheck_keys = []\n\t\tall_keys = []\n\t\tnum_dirs = 0\n\t\tnum_files = 1\n\t\tnum_bytes = 1\n\t\tnum_objs = 0\n\t\tall_objs = []\n\t\tobj_types = [\"\"]\n\t\tpath = {}\n\t\t\n\t\tfor child in self._get_objects_to_validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\tchild_key = child.get_key()\n\t\t\tif child_key not in path:\n\t\t\t\tpath"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n       ### if validate_objects :\n       ###     good_objects = self.get_valid_objects_in_tree(object_hash, check_digests, show_warnings)\n       ### else :\n       ###     good_objects = []\n\n       ## verify each object in its hierarchy...\n       ## self._process_object_metadata(good_objects)\n\n\n       ##... and finally, validate every object in the storage hierarchy..\n       ## also validate the checksums of the object. (optional)\n       ## TODO : need to check that the checksums of the object are correct!\n       ## for sha1 in good_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t      \n\tif validate_objects is True:\n\t\tif not self.is_valid():\n\t\t\treturn {'data': False,'messages': [] }\n\t\t# Check all keys and types\n\t\tfor key in self._obj_keys:\n\t\t\tif not self.get_value(key) in self.get_key_values(key):\n\t\t\t\treturn {'data': False,'messages': ['Error'] }\n\t\t# Check root data values\n\t\tif self.show_all:\n\t\t\tfor name in self._obj_keys:\n\t\t\t\tif name not in self.get_value"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t# TODO: this does not currently work (maybe it should be so)\n\t\t# TODO: use check_digests/check_objects\n\t\t# TODO: this is really terrible...\n\t\tfor r in self.get_roots(self.get_object_roots(validate_objects,check_digests)):\n\t\t\tobj = r['object']\n\t\t\tobj_guid = r['object_guid']\n\t\t\tobj_data = r['data']\n\n\t\t\t\n\t\tif show_warnings:\n\t\t\tlogger.debug(u\"VALIDATION RELANSLE ERROR: %s\" %"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n        if validate_objects:\n                print(\"VALIDATING A VALID OBJECT, WITH VALID OBJECT\")\n\t\tall_obj = Spartacus.Spartacus.Spartacus()\n\t\tall_obj.register_namespace(\"sthadmin\")\n#\t\tall_obj.check_root_hierarchy()\n\t\t\n\t\tif show_warnings:\n\t\t        \tprint(\"\")\n                        print(\"======================\")\n\t\tnum_objects = len(Spartacus.Spartacus.get_all_sthus())\n\t\tif num_objects!= Spartacus.Spartacus"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\tret = 0\n\t\tfor entnode in self.get_storage_nodes():\n\t\t\tif entnode.get_storage_root()!= []:\n\t\t\t\tif entnode.get_name() == 'nfs_id':\n\t\t\t\t\tret = 1\n\t\t\t\telse:\n\t\t\t\t\tret = 2\n\n\t\tif ret == 0:\n\t\t\tif total is None:\n\t\t\t\ttotal = len(self.get_storage_node_info(node.id))\n\t\t\telse:\n\t\t\t\ttotal = total - 1\n\t\telse:\n\t\t\tif validate_objects:\n\t\t\t\tobj_list = validate_objects\n\t\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\n\n    if not self.readonly and not self.is_readable:\n\t\traise exceptions.ProtectedFolderError(self)\n\n\tif validate_objects:\n\t\tif check_digests:\n\t\t\tdigest_objects = self.get_digest_objects()\n\t\t\tif show_warnings:\n\t\t\t\tprint 'WARNING: ListSha1 objects:', digest_objects\n\t\telse:\n\t\t\tif check_digests:\n\t\t\t\tdigests_read = self.get_digests()\n\t\t\t\tfor digest_object in digests_read:\n\t\t\t\t\tdigest_object['digest']    = digest_object['"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t# TODO: validate objects as well as object ids\n\t\ttry:\n\t\t\tobject_ids = list(self._objects_lookup)\n\t\texcept (TypeError, AttributeError):\n\t\t\tobject_ids = []\n\n\t\tgood_objects = set()\n\n\t\tfor obj_id in object_ids:\n\t\t\t# TODO optimize this: we need to validate objects with same id?\n\t\t\tobject_id = obj_id.rstrip(os.path.sep)\n\t\t\troot_object = self.objects.get(pk=obj_id, content_type__model='')\n\n\t\t\troot_object = self"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\n        if show_warnings:\n            # this is slightly different, but we have no reason to validate\n            # the results (there's no direct way to go about this)\n            self.logger.warning(\"verify_storage:  verifying_hierarchy() -   checking_objects:  {}\".format(validate_objects))\n\n        objects = self._get_objects(root_object_hash=None, check_digests=False)\n\n        num_objects = self.root.countObjects()\n        good_objects = self.root.countEncContents()\n        for i in range(objects):\n            # verify objects\n           "}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\tif check_digests:\n\t\t\tif self.digest.startswith(\"$6$\"):\n\t\t        digest_algorithm=c_alg(self.digest)\n\t\t        if validate_objects == False:\n\t                        digest=binascii.unhexlify(digest_algorithm.decode('utf-8'))\n\t                            return 0\n\t\tif validate_objects == True:\n\t\t      \t\t\t\t\tdigest=hexdigest(digest_algorithm)\n\t                                return 1\n\t\telif show_warnings:\n\t\t        return 1\n\t\telse:\n                        valid_count = count_objects(self.digest, digest_algorithm)\n\t\t\t\t\t\tif valid_count"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\traise NotImplementedError('abstract')\n\n\tdef destroy(self):\t\"\"\"\n\tDelete the OCFL storage root of the current system.\n\t\"\"\"\n\t\tif self._storage_root is not None:\n\t\t\tfor root in self._storage_root.list_storage_entries():\n\t\t\t\t_log.info(\"-- Removing entry: %s (%s)\" % (\n\t\t\t\t\troot[\"dn\"],\n\t\t\t\t\troot[\"path\"]\n\t\t\t\t))\n\t\t\t\tself._storage_root.delete(root)\n\t\troot_dir = self._storage_root.root_dir\n\t\tif self._auto_unlock"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\troot_config_file = os.path.join(self.homedirname, \"odfcfs\")\n\t\ttry:\n\t\t\ttree = xml.etree.ElementTree(file = open(root_config_file,\"w\"))\n\t\t\ttree.write(os.path.join(root_config_file, \"config.xml\"), encoding=\"utf-8\")\n\t\t\ttree.close()\n\t\texcept IOError:\n\t\t\tprint \"ERROR: Could not find config file: %s\" % root_config_file\n\n\tdef shutdown(self):\n\t\tself._close_connection()\n\n\tdef"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tif not ocl.OclRoot.init(self):\n\t\t\traise Exception(\"failed to init ocl storage...\")\n\n\tdef lock(self):\n\t\t'''\n\t\tForce to use a lock.\n\t\t'''\n\t\treturn ocl.OclRoot.lockfile(self)\n\n\tdef unlock(self):\n\t\t'''\n\t\tForce to use a lock.\n\t\t'''\n\t\treturn ocl.OclRoot.lockfile(self)\n\n\tdef open(self, name, flags, mode = \"w\"):\n\t\t'''\n\t\tOpen a storage file\n\n\t\tThe storage file is named"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tif self.file!= '/' and not self.file.endswith('tar.gz'):\t\n\t\t\tself.send_error(\"Error: cannot open ocfs!  File must end in.tar.gz\")\n\t\t\treturn False\n\t\tif OCFSCommand.file_exists(self.file):\t\n\t\t\tself.send_error(\"Error: file %s already exists!\" % self.file)\n\t\t\treturn False\n\t\t# create empty storage root\n\t\tself.send_response(200, 'ok')\n\t\t# generate contents\n\t\tif not self.file.endswith('.tar"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tif not ocfl.storage_root: os.mkdir( ocfl.storage_root, 0755 )\n\t\tif not ocfl.storage_root or not ocfl.storage_root.endswith( \"/\"):\n            ocfl.storage_root = prefix+\"/\"+os.path.basename( ocfl.storage_root )\n            if not ocfl.storage_root.endswith(\"/\"):\n                ocfl.storage_root += \"/\"\n        if not os.path.exists( ocfl.storage_root ):\n            os.mkdir( ocfl."}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tpass\n\t\n\tdef list(self, type = 0, flags = 0, offset = 0, sequence = 0):\n\t\t\"\"\"\n\t\tGet information about the storage.\n\t\t\n\t\ttype\t--\tThe subfolder type from a folder.\n\t\t\n\t\tflags\t--\tSpace separated list of flags for the structure.\n\t\t\n\t\toffset\t\t--\tStarting point of the structure to begin the list (the\n\t\t\tdefault value is 0); a offset to the structure is used.\n\t\t\n\t\tsequence (default is 0); a sequence number used when\n\t\t\tenumerating (CFF-style) the structure by using the\n\t\t\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tcputype = ocflstruct.OcflStruct('OCFL_StorageRoot', ocflstruct.Ocfl_StorageDirectory, self._cdb_version)\n\t\tif self._cdb_options.cdb_options_list is not None:\n\t\t\tcputype.cdb_options_list = self._cdb_options.cdb_options_list\n\t\tcputype.initialize()\n\t\tself._cdb_version = cputype.cdbversion\n\t\tself._is_initialized = True\n\n\tdef __del__(self):\n\t\t\"\"\"\n\t\tClean up O"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tif self.__root:\n\t\t\treturn\n\t\tself.__root = OCFLStorage(self.__root_uri)\n\t\tself.__lock = threading.Lock()\n\t\tself.__path_count = 0\n\t\tself.__id = 1\n\t\tself.__root.init_dir(self.name)\n\t\tself.__root.init_db(self.file)\n\t\tself.read()\n\n\n\tdef read(self):\n\t\t\"\"\"\n\t\tRead an OCR file from the active storage root.\n\t\t@raise OCFLError\n\t\t\"\"\"\n\t\twith self.__lock:\n\t\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.__storage = OCFLStorage(self)\n\t\tself.__root = OCFLStorageRoot(self.__storage)\n\t\t\t\n\tdef getDirList(self):\n\t\t\"\"\"\n\t\tReturn a list containing a list of items in this storage root.\n\t\tThese are not a sub-tree but just a couple of empty entries.\n\t\t\"\"\"\n\t\treturn [x.getDirList() for x in self.__storage.getRootList()]\n\t\t\n\tdef addEntry(self, entry):\n\t\t\"\"\"\n\t\tAdd a single entry to the root.\n\t\tThe given entry will overwrite the"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\treturn 0\n\n\tdef finalize(self):\n\t\t\"\"\"\n\t\tFinalize the OS storage, by removing unused files.\n\t\t\"\"\"\n\t\treturn 0\n\n\tdef add_path_info(self):\n\t\t\"\"\"\n\t\tSet the OS path info, so the client can load the OS device-specific configuration\n\t\t\"\"\"\n\t\t\n\t\tself.path = u\".\"\n\t\n\tdef _get_parentFolder(self, path, pathstr):\n\t\t\"\"\"\n\t\tGet the parent folder of a path if it exists.\t\n\t\tPathstr is a path string. The path is represented by the / (1"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tassert(len(version) > 4)\n\t\tlast, val = version[-1:]\n\t\tif not last.startswith('0') and not val.isdigit():\n\t\t\t# Precondition failed!\n\t\t\treturn None\n\t\telse:\n\t\t\t# Then it has digits.\n\t\t\treturn int(val)\n\treturn next_version(version[4:])from __future__ import division, print_function\n\nimport sys,os\nsys.path.append(\"../../\")\nimport h2o\nfrom tests import pyunit_utils\n\n\n\n\ndef covtype():\n    covtype = h2"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\twhile version.startswith((\"0\", \"1\")):\n\t\t\tversion = version[1:]\n\t\tif version.count(\".\") ^ 2 + \".\" + version.count(\"-\"):\n\t\t\tversion = \".\".join(version.split(\".\")[-2:]) + version[-1]\n\t\tversion = \".\".join(version.split(\".\")[0:2]) + \".\"\n\telse:\n\t\traise ValueError(\"bad version string: %s\" % version)\n\treturn version\n\nclass SimpleDBBase(Database):\n\t\n\t\"\"\"\n\tThis is the abstract base class for a set of common db objects.\n\t\"\"\"\n\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tfor i, j in enumerate(cls.__mro__):\n\t\t\tif j == version:\n\t\t\t\ttry:\n\t\t\t\t\treturn cls.__next_version_number(i)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tcontinue\n\t\t\n\t\tcls.__next_version_number = count\n\n\t\treturn cls\n\ndef get_major_version(filename):\n\t\"\"\"\n\tReturn the major version (if present) of the file given as a string,\n\tNone if not found.\n\t\"\"\"\n\tmatch = re.match(\"[0-9]+.[0-9]+.*\", filename)\n\tif match:\n\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t \n\t# Replace negative integer prefixes with 0\n\tparts = version.split('.')\n\tif len(parts)!= 3:\n            raise ValueError(\"Unsupported version '%s' \" % version)\n\tversion = parts[2].zfill(3)\n    return version\n\ndef parse_pattern(string, regex=None):\n\t\"\"\"A filter to parse a string according to a list of regex patterns.\n\n\t'string' -- The string that has to be filtered.\n\t'regex' -- The regular expression that matches the string.\n\t\"\"\"\n\ttry:\n\t\twords = re.split('\\"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\n        if version.startswith('0'):\n                return 0\t\t# pragma: no cover\n        \n        elif version.isdigit():\n                # XXX: This is a bit nasty, but there's no reason to check this.  Atm\n                # we have an error here.  Should probably be raising something\n                # like:\n                # if version is \"1.2-1-2\", \"1.2dev\", or \"1.2+1\"\n       \n                s = version.replace(\".\",\"\"); # handle leading zeroes, if any\n       \n                # remove any leading digits\n                s = re.sub(r'[^\\d"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\ndef version2_is_one_based(version):\n\t\"\"\"\n\tCompare two version numbers by adding suffixes or suffixes, using the same suffixes.\n\t\n\t>>> version2_is_one_based('2.4')\n\tTrue\n\t>>> version2_is_one_based('2.3')\n\tFalse\n\t>>> version2_is_one_based('2.2')\n\tTrue\n\t>>> version2_is_one_based('2.2-devel')\n\tFalse\n\t>>> version2_is_one_based('2.3+devel"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\ttry:\n\t\t\treturn [int(s) for s in version.split(\".\")]\n\t\texcept ValueError as e:\n\t\t\traise ValueError(\"%s is not a valid version number\" % (e,))\n\ndef _validate_version(version):\n\t\"\"\"\n\tCheck that the version number is valid.\n\t\n\tReturns boolean: True if it is valid, and False otherwise.\n\t\"\"\"\n\tif not version.isdigit():\n\t\traise ValueError(\"%s is not a valid version\" % (version,))\n\tif version < VALID_VERSION[2:]:\t# v1.\n\t\treturn False"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tif len(version) <= 3: return\n\t\tversion += [0,0,0,0,0] + list(version)\n\t\tversion.reverse()\n\telif len(version) <= 6:\n\t\tversion += [0,0,0,0,0] + list(version)\n\t\tversion.reverse()\n\telif version == [0,0,0,0,0,0] or version == [0,0,0,0,0,0]:\n\t\tversion = [0,0,0,0,0] # Remove all consecutive 0 and 0"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tif '-' in version:\n\t\t\tmin_version, max_version = version.split('-')\n\t\t\tif len(min_version) > 1 and min_version.isdigit() and int(min_version) <= max_version:\n\t\t\t\treturn version\n\t\treturn None\n\n\t# Check prefix first and if not split it from version, it's a version prefix.\n\tsuffix_length = len(version)\n\tprefix = ''\n\tif suffix_length == 1: # This is just a version.\n\t\tprefix = version.lstrip('v')\n\telif suffix_length >= 2 and"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tmatch = re.match(r'^[A-Za-z0-9.]+$', version)\n\t\tassert not match, 'Unknown version \"%s\"' % version\n\n\t\tversion_range = match.groupdict()\n\t\tversion_range[6] = int(Version(version_range[6]).from_string()) + 1\n\n\t\treturn next_version(version_range)   \n\ndef prev_version(version):\n\t\"\"\"\n\tPrevious pattern identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tmatch = re.match"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tlogging.debug(\"Validating version inventories\")\n\t\tfile_name=os.path.join(self.root, \"inventory.version\")\n\t\tfiles_in=glob.glob(file_name)\n\t\tif len(files_in)!= 1:\n\t\t\tlogging.error(\"Found %d files in inventory.version\" % len(files_in))\n\t\t\tsys.stderr.write(\"Found %d files in inventory.version\\n\" % len(files_in))\n\t\t\tsys.exit(1)\n\t\tmajor_inventories=0\n\t\tfor f"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\t# Version directories must be provided\n\t\tfor version_dir in version_dirs:\n\t\t\tif version_dir!= self.version_dir:\n\t\t\t\tsys.stderr.write(\"Error: %s is not a valid version folder name \"\n\t\t\t\t\t\t\t\t\t\t % version_dir)\n\t\t\t\tsys.exit(1)\n\t\t# Only a single one should be present\n\t\tversionable_directories = [version_dir]\n\t# TODO: Is there a better way to do this?\n\t# versionable_directories = version_dirs[0]\n        for version_dir in versionable_directories:\n               "}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tif self.version!= 1 and len(version_dirs)!= len(set([x.lower() for x in version_dirs])):\n\t\t\traise ValueError(\"Inventories require versions only for one version: {0}\".format(\", \".join(version_dirs)))\n\t\t\t# NOTE - Version 1 allows that there must be an inventory for each\n\t\t\t# version of the same name in different versions, otherwise this\n\t\t\t# does not result in an error, since this is checked above.\n\t\t\t# This can also be done via the inventory for only one version\n\t\t\t# rather than having to know the version"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tversion_dirs = tuple(version_dirs)\n\t\tall_versions = set(self._get_all_inventory_paths())\n\t\tversions = set()\n\t\tfor dir_name in version_dirs:\n\t\t\tversion_dirs_in_dir = os.listdir(dir_name)\n\t\t\tfor version_dir in version_dirs_in_dir:\n\t\t\t\t# Each file in this directory\n\t\t\t\tversion_dir_name = dir_name[version_dir_name.index('@') + 1:]\n\t\t\t\tif (version_dir not in all_versions and\n\t\t\t\t\tos."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tif len(version_dirs) < 1:\n\t\t\traise ValueError\n\t\tlast_version_dir = 0\n\n\t\tfor version_dir in version_dirs:\n\t\t\tcurrent = os.stat(version_dir)\n\t\t\t# Versions should be ordered longest to shortest.\n\t\t\tif last_version_dir > current[0]:\n\t\t\t\tversion_dirs.insert(0, os.path.join(version_dir,'static/'))\n\t\t\t\tlast_version_dir = current[0]\n\n\ndef get_all_system_files(system_root_dir,version_dirs,"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tif len(version_dirs) > 0: sys.exit(-1)\n\t\tfor version_dir in version_dirs:\n\t\t\tself.validate_file_inventories(os.path.join(version_dir, 'INVENTORY'))\n\n    def validate_file_hashes(self):\n        \"\"\"\n        Verifies all files referenced in the inventory are at least valid.\n\tValidate all references to files, to ensure that the inventory\n\tis still valid.\n\n        This assumes that all files referenced in the inventory are valid, but not for\n\t   the purposes of validating them, since the files that are"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\ttotal_depth = len(path(\"vartree-content-depth\")).rstrip(\"|\")\n\n\t\tfor i in range(0,4):\n\t\t\talldepth = version_dirs[i][1] - version_dirs[i][0]\n\t\t\tif dedupe.inventories:\n\t\t\t\ti_root = version_dirs[i][0]\n\t\t\t\tnode = self.directory.lookup_generic(i_root).get_child(i_root)\n\t\t\t\tif node.content_digest not in  alldepth:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tdel node.content_digests"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\t# First check if the inventory isn't already complete\n\t\ttry:\n\t\t\tversion_dirs.remove('inventory')\n\t\texcept ValueError: pass\n\t\t\n\t# Now go over the inventory tree (assuming there's only one entry).\n\t# Check for all the latest inventory files in the inventory. The 'root group'\n\t# directories are ignored as this just skips over them from the\n\t# list of inventory files.\n\troot_group = \\\n\t\t\tos.path.realpath(os.path.join(os.path.dirname(__file__), '..', '..', 'inventory'))\n\n\tfor entry"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tdef validate_for_validating_sub_directory(directory):\n\t\t\tversion_dir_name = os.path.join(directory, directory.split('/')[-1])\n\t\t\tif not os.path.exists(version_dir_name):\n\t\t\t\tcontinue\n\t\t\tvalid = 0\n\t\t\tfor f in os.listdir(version_dir_name):\n\t\t\t\tversion = f.split('_', 1)\n\t\t\t\tif len(version) > 1 and version[0] == '.' and not os.path.isdir(os.path.join(version_dir_name, f)) and not f."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfiles = []\n\t\tversion_dirs = map(lambda x: x.strip(), version_dirs)\n\n\t\tfor version_dir in version_dirs:\n\t\t\ttry:\n\t\t\t\tversion_file = open(version_dir, \"r\")\n\t\t\texcept IOError:\n\t\t\t\treturn # invalid file, no file\n\t\t\ttry:\n\t\t\t\tversion = file_checksum(version_file)\n\t\t\texcept OSError as oe:\n\t\t\t\tif oe.errno!= errno.ENOENT:\n\t\t\t\t\traise\n\t\t\t\traise InvalidPackage(\n\t\t\t\t\t\"Unable to validate package '%s': %s\"\n\t\t\t\t\t%"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\ndef resolve_path(path):\t#pylint: disable=W0613,W0612\n    \"\"\" Resolve the path \"\"\"\n    return path\n\n\n#The \"parent path\" for an Inventory object will be searched in.\n#  This is the \"parent value\" for an Inventory.\n\nclass Inventory(object):\n    \"\"\" The inventory is an object representing an OCFL Inventory\n    \"\"\"\n    def __init__(self, parent, inventory_file=None):\n        \"\"\" Constructor\n\t\t@param parent the Inventory object\n\t\t@param inventory_file full path to the inventory file\n\t\t\n\t\tNote that the user"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t \n\t# If we have the root directory, we'll assume the object.\n\t# If not, just check it's a file.\n\tif os.path.basename(path)!= '' and os.path.dirname(path)!= '':\n\t\tparent, name = os.path.dirname(path).split(os.sep)\n\t\tif name.startswith('object.'):\n\t\t\treturn 'root'\n\t\telif name.startswith('root.'):\n\t\t\treturn 'object'\n\t\telif parent == 'root':\n\t\t\treturn 'object'\n\telse:\n\t\treturn ''\n\t\ndef"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t#return ['0=file','1=directory']\n\n\t# if path is a directory\n\tif type(path) is str and type(path) is not unicode:\n\t\tif len(path)>0 and path[0]!= '/':\n\t\t\treturn 'directory'\n\t\troot = None\n\t\tobject = None\n\t\tpath = path[1:]\n\telif type(path) not in [str, unicode]:\n\t\treturn path\n\telse:\n\t\troot = path\n\treturn root+os.sep+object\n\ndef load_type_from_file(path):\n   "}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n    path_parts = path.split(\"/\")\n\thead_num=0\n\tfor part in path_parts:\n\t\tif '/root' in path_parts:\n\t\t\thead_num+=1\n\t\t\tif head_num == len(path_parts)-1:\n\t\t\t\treturn (\"root\", 1)\n\t\tif '/object' in path_parts:\n\t\t\tif part == '.hdi':\n\t\t\t\treturn (\"object\", 2)\n\t\t\treturn (\"object\", 0)\n\t\t\t\n\treturn (\"root\", 0)\n\ndef usage():\n\ttry:\n\t\tprint __file__, \":"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t    \n    if path == \"object\":\n\treturn \"root\"\n    if path == \"root\":\n\treturn \"object\"\n    if path == \"object\":\n\tif not file in inventory_types:\n\t\treturn \"file\"\n\telse:\n\t\treturn inventory_types[file]\n    if path == \"file\":\n\tfile_type = \"file\"\n    if path == \"root\":\n\treturn \"root\"\n    elif path == \"object\":\n\tif not object_type in inventory_types:\n\t\treturn \"object\"\n    else:\n\t\treturn inventory_types[object_"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t # TODO - is it really needed anywhere?\n\tfile_type = \"file\"\t\t\t\t\t# assume I am an ICS 5.2 or 2.3?\n\tif path.find(\"data\") <0:\t    # if this could mean the files directory was specified \n\t\ttry:\n\t\t\ttfile_type = file_type.replace(\"data\", \"ochev\").replace(\"ocelot\", \"ochev\")\n\t\texcept AttributeError:\n\t\t\tpass\t\t\t\t\t\t# not a string but could just be one\n\t\n\tif path.find(\"data/\") > -1:\t   "}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# try all files with certain extensions\n\t\textension_files = [ 'root', 'object' ]\n\n\t\t# attempt to open the whole file\n\t\tif os.path.isfile(path) and name == 'file':\n\t\t\tfile_contents = os.open(path).read()\n\t\t\tfile_contents.rstrip()\n\t\t\tfile_contents = file_contents.rstrip('\\n')\n\t\t\tif len(file_contents) > 0:\n\t\t\t\treturn file_contents.strip('\\n')\n\t\t\telse:\n\t\t\t\treturn ''\n\n\n\t\t# only use the first one\n\t\tif path"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tif len(path) > 20:\n\t\t\traise RuntimeError('error: path too long: %d' % len(path))\n\t\treturn [p for p in path[1:4] if (path[0] == '.') and p.count('/')!= 1 ]\n\treturn [p[2:] for p in path if p.startswith('0=') and p.endswith('/')]\n\n\t\n\n#\n#  Find parent inode and parent name which is equal to the path.\n#\ndef find_parent_by_name(inode, name):\t\t\n\t\"\"\"\n\tReturns the parent"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\nclass ocfl_storage_root_t(Structure):\n\t_fields_ = [('file', POINTER(c_char)), ('root', c_int)]\n\t\t\t\n\nclass ocfl_storage_dir_t(Structure):\n\t_fields_ = [('id', c_int), ('name', STRING), ('parent', c_int), ('lastmod', c_int),\n               ('etag', c_char * 128), ('atime', c_int), ('ctime', c_int), ('_db', c_char_p)]\n\n\ndef _is_oss(p):"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t # This may not be ideal...\n\t# TODO: It's hard to read the whole list of files here.\n\t# Check with the root directory to see if it has anything left\n\t# under the path\n\tif not os.path.isdir(path):\n\t\traise IOError(\"Unable to find root path\")\n\n\t# Note: for now, just take the basename part and strip the first part...\n\tbasename = os.path.basename(path)\n\treturn basename[strip_prefix_len(basename['path']):]\n\n\n\n### Other functions ###\n\n\ndef check_path_"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif params['bugids']:\n\t\t\tself.log.debug(\"Bzr parameter(s) modified, committing %s comments\", ', '.join(bug_ids[0][2:]))\n\t\t\tparams['comments'] = \"; \".join(bug_ids[0][2:])\n\t\tself.bz.params.update(params)\n\tdef amend_cc(self, bug_ids):\t\"\"\"\n\tAmend the Bugzilla CC (easier as far as I understand)\n\t\"\"\"\n\t\tif params['bugids']:\n\t\t\tself.log.debug(\"CC parameter("}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tparams.setdefault(\"api_key\", settings.BZAPI_KEY)\n\t\tparams.setdefault(\"bzapi_token\", settings.BZAPI_TOKEN)\n\t\tparams.setdefault(\"api_password\", settings.BZAPI_PASSWORD)\n\n\t\tresult = requests.post(self.url, data=params)\n\t\tif result.status_code!= 200:\n\t\t\tprint result.status_code\n\t\t\traise ValueError(\"This endpoint is not accessible to your \" \\\n\t\t\t\t\t\t\t\t\"bugzilla API.\")\n\n\t\tif \"data\" in result.headers[\"content-type\"].split(';'):\n\t\t\treturn"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tparams = { \"ids\": json.dumps(bug_ids) }\n\t\tdata = requests.put(url=self.__server_url+self.__api_path, params=params)\n\t\treturn data\n\n\tdef upload_bzparam(self, data=None):\t\t\n\t\tparams = { }\n\t\tif data is not None:\n\t\t\tdata = json.dumps(data)\n\t\t\tparams[\"data\"] = data\t\t\n\t\tr = requests.post(self.__server_url+self.__api_path, params=params)\n\t\treturn r\n\t\t\t\t\n\tdef __request"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif params == {}:\n\t\t\treturn self.server.insert_bug_bug_params(bug_ids)\n\n\tdef get_bugzilla_link(self, id):\n\t\t# bugzilla link is a string like this: \"1234.1.5\".\n\t\treturn \"http://bugzilla.example.com/%s\" % id\n\n\tdef append_bugzilla_link(self, id, params, status=\"Active\"):\n\t\tbug_id = self.get_bugzilla_link(id)\n\t\tself.append_params_to_bugzilla_params(params, bug_id)\n\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tbody = bzdecode(params['body'].encode())\n\t\tbody += \"\\nbug_ids =?\"\n\t\tfor ids in bug_ids:\n\t\t\tparam_hash = str(params['ids'][ids])\n\t\t\tidx = len(param_hash) - 3\n\t\t\tbody += \"{bug_ids[%d]} =?, \" % (idx, param_hash, ids)\n\t\treturn body\n\n\tdef create_bzheader(self):\n\t\t\"\"\"\n\t\tCreate the Bugzilla Bugzilla Header\n\t\t\"\"\"\n\t\tbug_ids = self.parse_json(self.request"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif len(params['bug_ids']) == len(bug_ids):\n\t\t\tself.a_params = deepcopy(params)\n\t\t\tself.params = deepcopy(self.a_params)\n\n\tdef clean_bug_ids(self, bug_ids, bug_list, bug_ids_set, options=None):\n\t\t\"\"\"\n\t\tRemove a bug_id from a list of bug ids\n\t\tIf bug_ids is a list of bug ids try to match the given ids\n\t\tOtherwise the bug id is removed from the list\n\t\tIf bug_ids are none, look"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tfor id in bug_ids:\n\t\t\tself.bugzilla_client.edit_update(id, params)\n\n\tdef get_bz_bug(self, id):\t\t\n\t\tbz_params = self.bugzilla_client.get_bz_bug(unicode(id), ['id'])\n\t\treturn bz_params\n\t\n\tdef get_bz_attachments(self, id):\n\t\tbz_params = self.bugzilla_client.get_bz_attachments(unicode(id))\n\t\treturn bz_params\n\t\n\tdef get_bz_labels(self, id):\n\t\tbz_params"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\t# bug ids can contain any number of characters 0x09 and 0x0a\n\t\t# so need to pad values with leading zeroes\n\t\tfor bug_id in bug_ids:\n\t\t\ttry:\n\t\t\t\tparams[bug_id] = {\n\t\t\t\t\t'action': 'amend',\n\t\t\t\t\t'version': '999'\n\t\t\t\t}\n\t\t\texcept KeyError:\n\t\t\t\twritemsg(\n\t\t\t\t\tcolorize('WARN', 'WARN', '(BUGID \"%s\") not found' % bug_id),\n\t\t\t\t\tnoiselevel=-1)\n\n\t\t# update all params with bugzilla as"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif len(bug_ids) < 2:\n\t\t\treturn\n\n\t\tdef do_update_bugzilla_with_params(action, bug_ids):\n\t\t\t# If bug_ids is more, they might be removed.\n\t\t\t# Ambiguous\n\t\t\tif len(bug_ids) >= 3:\n\t\t\t\t# Delete the bugs.\n\t\t\t\taction.bug_unlink(ids=bug_ids)\n\t\t\t# Otherwise, append a new value, otherwise the parameter is already absent.\n\t\t\telse:\n\t\t\t\taction.set_bug_ids(*bug_ids)\n\n\t\tif not bug_"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tself.bzparams = bug_ids\n\t\tlogging.info(\"Creating bug URL: %s, bug_id: %s, params: %s\" % (params, bug_ids, params))\n\t\n\tdef __call__(self):\n\t\tself.start()\n\t\tlogging.debug(\"Bugzilla URL: %s, params: %s\" % (self.bzparams, self.bzparams))\n\t\tself.do_log(\"getting bug info\", level=logging.DEBUG)\n\t\tself.get_bug_info()\n\t\tself.do_log(\"getting bug"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\n\tresult = []\n\tfor d in nodes:\n\t\tfor key, value in d.iteritems():\n\t\t\tif key.startswith('__'):\n\t\t\t\tcontinue\n\t\t\tresult.append(deep_merge_nodes(value))\n\n\treturn result\n\n\n\n\n\n\n\nif __name__ == '__main__':\n\timport unittest\n\tfrom test.integration import run_tests\n\tfrom softwareCenter.paths import get_test_data_file_path\n\n\tdef loader(loader, node=None):\n\t\t\"\"\"\n\t\tA helper function to reduce code size persisted in :py:mod:`YamlLoader"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n        new_config_data = yaml.load(\n            yaml.round_trip_dump(\n                self._config_data, Dumper=yaml.RoundTripDumper),\n            Dumper=yaml.RoundTripDumper)\n \n        self._config_data.clear()\n        self._config_data[new_config_data[0]] = new_config_data\n        self._config_data[new_config_data[0]]['values'][\n            new_config_data[1]]['mapping'] = dict(\n                (x['tag'], new_config_data[1]) for x in new"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n# we should be able to call the recursive merge() method repeatedly\n# so that if the key isn't present in a node's map, then that would conflict with\n# the key in the merged file.\ndef merge_recursive(first, second, merge_func):\n\t'''\n\tGiven two borgmatic configuration files, merge them into a single one, and return the merged result.\n\n\tThis method could be useful so that when there is nested maps\n\tcontaining dictionaries or lists containing strings, the only part of merged\n\tdicts is that dictionaries or lists containing floats, the difference\n\tbetween"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merger(\n\t\t\t[\n\t\t\t\t(scalar.scalar_node, None, lambda t: ruamel.yaml.comments.CommentedMap(comment=[scalar.scalar_node]))\n\t\t\t\tfor t in nodes\n\t\t\t]\n\t\t)\n\n##--end\n\n\nif __name__ == '__main__':\n\tfrom __future__ import print_function\n\n\timport pprint\n\n\t# test argument handling\n\tparser = argparse.ArgumentParser(description='YAML to ease construction order',\n\t\t\t\t\t\t\tepilog='This script is for debugging or debugging a bunch of parameters.',\n\t\t\t\t\t\t\tformatter_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ttmp_node_list = []\n\t\ttmp_node_dict_list = []\n\t\tdeep_search_nodes = []\n\n\t\tmerge_map(deep_merge_nodes, tmp_node_dict_list)\n\t\tmerge_map(deep_merge_nodes, tmp_node_list)\n\n\t\tmerge_node_dict_lists_to_dict_list(tmp_node_dict_list)\n\t\ttmp_node_dict_list = None\n\n\t\tdeep_search_nodes = []\n\t\tmerge_node_dict_lists_to_dict_list(tmp_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(\n\t\t\titertools.chain(\n\t\t\t\tsplit_path_value(node['tag:yaml.org,2002:str'],'retention'),\n\t\t\t\tsplit_path_value(node['tag:yaml.org,2002:map'], 'keep_hourly'),\n\t\t\t\tsplit_path_value(node['tag:yaml.org,2002:int'], 'keep_daily'),\n\t\t\t\tsplit_path_value(node['tag:yaml.org,2002:int'], 'interval'),\n\t\t\t), nodes, new_key\n\t\t)\n\n\ndef list_unique"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ndef deep_merge_config(filename, sections=None):\n\t\"\"\"\n\tWrite the data in the file *filename* to a *sections* list.\n\n\t:param str filename: The configuration filename with *sections* names\n\t             of the sections which have to be written to the file.\n\t:param list sections: The list of sections to merge into the configuration\n\t          for the new configuration file. The sections are in the *sections*\n\t             of the \"new\" configuration file or \"deleted\" ones.\n\t\"\"\"\n\tif filename[-1] == '\\n':\n\t\tsections"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n        if len(nodes):\n                raise self.MergeConflictingConfigError(\n                    \"Deep merges not yet supported in configuration file.\")\n\n        # Load into namespace node\n        node_data = self._load_config(self.namespace, nodes[0])\n\n        # Load into node\n        node = self._make_object(node_data, deep=True)\n\n        return node\n\n    def get_nested_borgmatics(self, namespace=None):\n        \"\"\"\n        Returns a merged dictionary from nested borgmatic configuration data.\n        \"\"\"\n\n        nodes = []\n        # Load into each element in the configuration section\n        for node"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes([\n\t\t\tDeepMergeNode(nodes)\n\t\t])\n\ndef get_file_paths(configspec):\n\tfilepath_list = [];\n\tfor node in configspec:\n\t\tif hasattr(node, 'filename'):\n\t\t\tfilepath = node.filename.split('/')[-1];\n\t\t\tfilepath_list.append(\"./\" + filepath)\n\treturn filepath_list\n\n# -------------------------------\n# Configuration Object for borgmatic configuration file parsing\n# -------------------------------\n\nclass Configuration:\n\t\"\"\"This class is used for managing the information necessary to convert between\ntwo"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn None\n\ndef deep_merge_node(node):\t\treturn deep_merge_nodes([deep_merge_node(subnode) for subnode in node])\n\ndef get_yaml_type(val):\n\t\"\"\"\n\tExtract the yamlType parameter of a value.\n\t\"\"\"\n\tif val is None:\n\t\treturn None\n\telif val.type == 'null':\n\t\treturn None\n\telif val.type == 'int':\n\t\treturn int(val.value)\n\telif val.type == 'float':\n\t\treturn float(val.value"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\tprogram_args = load_program_args(\n\t\targuments,\n\t\t'all',\n\t\t'none',\n\t\t'c',\n\t\t'config',\n\t\t'mod2-1.0.5',\n\t)\n\t\n\t# read in args, and return program-specific arguments\n\treturn tool.parse_args(\n\t\tprogram_args,\n\t\t*arguments\n\t)\n    \ndef build_argparse(arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, build and return\nthem as a argparse instance.\t\"\"\"\n\treturn"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(prog=\"ccs\",\n\t\tdescription=\"Given ccs command-line arguments, parse that file to get arguments.\"\n\t)\n\tparser.add_argument('filename', nargs='?', type=str, help='filename to parse; if omitted, defaults to stdin')\n\tparser.add_argument('-v', '--version', action=\"version\",\n\t\tversion=\"%(prog)s v{version}\"\n\t)\n\tparser.add_argument('--debug', '-g', action=\"store_true\",\n\t\thelp='Activate DEBUG logging' )\n\tparser.add"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n    parser = ArgumentParser(description, formatter_class=RawDescriptionHelpFormatter)\n    parser.add_argument(\"host\",\n                        help = \"The host of OpenWRT\")\n\n    \n    parser.add_argument(\"-i\", \"--interactive\", dest=\"interactive\", action=\"store_true\", help=\"Interactive mode\")\n    parser.add_argument(\"-s\", \"--silent\", dest=\"silent\", action=\"store_true\", help=\"Don't output to STDERR\")\n    parser.add_argument(\"-l\", \"--logfile\", dest=\"logfile\", default=None, help=\"log file\")\n    parser.add_argument(\"-c\", \"--"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tprs = ArgumentParser(USAGE)\n\t\tprs.add_argument(\"-w\", \"--warn\", default=False, action=\"store_true\",\n\t\t\t\t\t\thelp = \"Print information and quit\")\n\t\tprs.add_argument(\"-v\", action=\"version\", version = \"v0.3.0\")\n\t\tprs.add_argument(\"-v\", action=\"get_version\", version=\"v0.3.0\")\n\t\tprs.add_argument(\"-p\", \"--patch\", action=\"store_true\",\n\t\t\t\t\t\thelp=\"Print patch output\")\n\t\tprs.add_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\toptions = parse_arguments_arguments(arguments)\n\t\n\tif options.version:\n\t\tprint((\"Titanium {0} - Version {1} by {2} on {3}\".format(get_version_full(), options.version[0], platform.uname()[1], _platform)))\n\t\tsys.exit(1)\n\t\n\t# If the output format is RGBA, read the output and return it.\n\tif options.output!= None:\n\t\tsys.exit(check_output(get_full_path(options.output)))\n\telse:\n\t\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\t(options, args) = parser.parse_known_args(arguments)\n\treturn Pars(**vars(options))\n\t\n\t\n\t\n\n#!/usr/bin/python\n# Copyright: Ansible Project\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\n\nfrom __future__ import absolute_import, division, print_function\n__metaclass__ = type\n\n\nANSIBLE_METADATA = {'metadata_version': '1.1',\n                   'status': ['preview'],\n                   'supported_by':"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = optparse.OptionParser(description=__doc__.strip()) # This will get the default options...\n\t\targuments, _ = parser.parse_args()\n\t\treturn arguments\n\n\t## Print usage message...\n\tdef usage(self):\n\t\tprint parser.get_usage()\n\n# Set up argument parser\noptions = parse_arguments()\t\n\n# Read argument(s) options\nconfig = MiscOpt(options.config);\n\n\n## Print usage message\ndef usage(*args):\n\tprint sys.argv[0] + \": \" + \" \".join(args)+\"\\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(version=\"0.1\")\n\n\trequired_modules = [\n        'flask',\n        'docopt',\n        'flask_socketio',\n        'flask_wtf',\n        'flask_debugtoolbar',\n        'flask_restful',\n        'flask_limiter',\n        'flask_sqlalchemy',\n        'flask_wtf.csrf',\n        'flask_wtf.csrf.CsrfProtect',\n        'flask_wtf.csrf.CsrfProtectRequestAuthenticate',\n        'flask_wtf.csrf.CsrfProtectResponse',\n        'flask"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"command_line\", nargs=\"?\",\n\t\t\thelp=\"The command-line to parse\")\n\n\t\treturn parser.parse_args()\n\ndef main(directory): # runs this if invoked directly or from the command line\n\tinput_directory = directory\n\tif not os.path.exists(input_directory):\n\t\t\tsys.stderr.write(\"File \\\"{}\\\" not found\\n\".format(input_directory))\n\t\t\tsys.exit(1)\n\t\tprint(\"Reading input file, please wait\")\n\t\tinput_file"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t \n\t# Initialise the argument parser by taking the command name as the first\n\targparser = ArgumentParser()\n\tfor argument in arguments:\n\t\tif argparser.add_argument(argument, help = 'The command-line argument containing the command.')\n\t\telse:\n\t\t\tif argparser.add_argument_group(argument):\n\t\t\t\targparser.add_argument(argument)\n\t\t\t\tif argparser.error:\n\t\t\t\t\tprint 'Error: {}'.format(argument)\n\t\t\t\telse:\n\t\t\t\t\targparser.exit()\t\n\t# Use argument_groups_to_args to make a"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn parser._flags\n\n\tdef arg_string(s):\n\t\t\"\"\"\n\t\tExtracts the argument string from an option string.\tThe option string can\n\t\tbe surrounded by curly braces.\n\t\t\"\"\"\n\t\treturn \" \".join(filter(None, map(lambda p: p.strip(),\n\t\t\tfilter(lambda x: x, s.split(\" \"))))).replace(\" \",\"\")\n\n\tdef arg_metavar(s):\n\t\t\"\"\"\n\t\tReturns the metavar metavar for the argument.\n\t\t\"\"\"\n\t\treturn \":\".join(map(lambda p: \"--%s %"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument('--mode',\n\t\taction='store',\n\t\tdest='mode',\n\t\tdefault='debug',\n\t\tchoices=['debug','release'],\n\t)\n\t\n\tparser.add_argument('--help',\n\t\thelp=argparse.SUPPRESS,\n\t\taction='help',\n\t\tdefault=argparse.SUPPRESS,\n\t)\n\tparser.add_argument('--version',\n\t\thelp=argparse.SUPPRESS,\n\t\taction='version',\n\t\tversion='%(prog)s {0}'.format(__version__)\n\t)\n\tparser.add_argument('--help-subcommand"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tdef add_argument(flags, dest, **defaults):\n\t\t\"\"\"\n\t\tA function to add an argument to a command-line flag.\n\t\tArguments are formatted using the % notation for flags.\n\t\t\"\"\"\n\t\tparser.add_argument(flag=flag, dest=dest, **defaults)\n\n\n\tdef add_argument_group(group, dest, **defaults):\n\n\t\t\"\"\"\n\t\tA function to group an argument.\n\t\t\"\"\"\n\t\t# get the number of arguments in the group, but don't convert to a\n\t\t# string because some of our examples have a different meaning"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tp = parser.add_argument('--foo')\n\t\tp.add_argument('--bar')\n\t\treturn p\n\t\n\tdef __call__(self, args = None):\n\t\tpass\n\n\tdef write(self, string):\n\t\t\"\"\"\n\t\tEmit a log message to stderr, and optionally write to the stdout.\n\t\t\"\"\"\n\t\tm = '[{:%H-%M-%S}]%s' % (datetime.now().year, os.sep, string)\n\t\tself.__stderr_stream.write(m)\n\t\tif self.output:\n\t\t\tprint(string)\n\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\t# A string such as '-x', '-march=native'\n\tflags = []\n\tfor arg in argv:\n\t\tif arg[0] in ['-x', '-march=native']: flags.append(arg[1:])\n\targstring =''.join(flags)\n\targstring = argstring.replace(' ', '')\n\targs = parser.parse_args()\n\t\n\tnewargs={}\n\tfor i,arg in enumerate(args):\n\t\tif arg[0] not in newargs.keys():\n\t\t\tnewargs[args[i]]=arg."}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument(dest=\"flags\",\n\t                    help=\"\"\n\t                    )\n\t\n\treturn parser\n\n##\tEnd: Interface methods \n\ndef run(argv=None):\n\t\"\"\"\n\tParse arguments.\n\t\n\tIf supplied with an argparse option, parse the arguments of the main\n\tprogram.  It *sets* sys.argv, otherwise they are retrieved.  If supplied\n\twith None, a basic argument list will be built from it.\n\t\n\tArgs:\n\t    argv (object): If supplied, interpreted as the command argv list.\n\t\n\tReturns:\n\t    Namespace: The main program"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tdef parse_args(args):\n\t\t# Check for flags that are not flags.\n\t\tfor flag in args.__flag:\n\t\t\tif flag not in (\"--no-header\", \"--no-code-printer\"):\n\t\t\t\traise Exception(\"invalid flags: %s\" % flag)\n\n\t\t# Check for invalid use of flags specified.\n\t\tif args.command not in (\"help\", \"install\", \"install-doc\"):\n\t\t\traise Exception(\"invalid commands: %s\" % args.command)\n\n\t\t# Parse arguments.\n\n\t\treturn args.__dict__\n\n\n\n\n\nif __name__ =="}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tif parser.add_argument_group(\"Flags\")!= \"required\":\n\t\t\tflags = parser.add_argument_group(\"Flags\")\n\t\t\tflags.required = True\n\t\t\tflags.help = \"Whether or not to use this flag as a base flag.\"\n\n\t\treturn flags\n\n\tdef parse(self, args, config):\n\n\t\tflags = self.parser_flags(parser)\n\t\tflags.add_argument('--{0}={1}'.format(flags.required, flags.action), action='store_true')\n\n\t\tif flags.help is not None:\n\t\t\tprint(sys"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tflags = parser.parse_args(sys.argv[1:])\n\t\tflags.append((\"-s\", \"--settings\"))\t# this option is used more than once\n\tflags.append((\"--gui\", \"--gui\"))\t\t# the GUI is optional, but not used\n\treturn flags\n\ndef get_app_settings(file=None):\n\t\"\"\"\n\tGiven the full path to the main settings file, parse and return \n\ta dict containing the app-specific settings to use. This is an updated \n\tversion of the settings.py file by Dr. Moorou\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn sum([flag.get_opt_string('default') for flag in parser._flags]) > 0\n\nclass Command:\n\t\"\"\"\n\tA command is a kind of command that can be added to every node in the tree\n\tusing the argparse parser.\n\t\"\"\"\n\n\tdef __init__(self, name, desc, node=None, parser=None):\n\t\t\"\"\"\n\t\tCreate a ``Cmd`` instance, by reading from a file-like object ``file``,\n\t\tby making sure it is formatted correctly.\n\t\t \n\t\t:param str name: file path, the name of the"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n \tparser = argparse.ArgumentParser(description='Creates an optic/local/cache-path/config-file/configlist file for the project')\n \tparser.add_argument('-c','--config', action='store', help='config-file to read (defaults to \"%s\")' % unparsed_arguments[0])\n \tparser.add_argument('-f','--force', action='store_true', help='ignore existing files/directories (ie, they will fail unless \"force\"=yes)')\n    \n \tparser.add_argument('--force-cache', action='store_true', help='ignore"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targs = unparsed_arguments[0]\n\t\targs = parse_command_line_args(args)\n\t\treturn args[1]\n\ndef parse_command_line_args(args):\n\t\"\"\"\n\tGiven arguments that have a namespace and a command line call the respective function and \n\treturn a tuple: (namespace, args)\n\n\tReturns (namespace, args)\n\t\"\"\"\n\t# FIXME: Currently, this doesn't always return the namespace.\n\tif len(args):\n\t\tnamespace = args[0]\n\telse:\n\t\tnamespace = None\n\n\t#"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n# TODO\n# handle exceptions while parsing\n# pylint: disable=too-many-arguments\n# (see issue #13)\nclass ArgumentParser:\n\tdef __init__(self):\n\t\tself.parser = argparse.ArgumentParser()\n\t\tself.parser.add_argument(\"commands\", metavar=\"COMMAND\", type=str, nargs=\"*\",\n\t\t                         help=\"command-line arguments\")\n\t\tself.parser.add_argument(\"--version\", action=\"version\", version=(lambda x: \".\".join(str(v) for v in x.version) ))\n\t\tself.parser.add_argument(\"--"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\t\n\tdef parse_args(args):\n\t\t\"\"\"\n\t\tParse the given arguments into a Namespace with those arguments.\n\t\t\n\t\tUnparsed arguments are returned as an argparse Namespace with defaults.\n\t\t\"\"\"\n\t\targ_parser = ArgumentParser(description=\"A tool to parse command-line arguments/configuration files. This command \"\n\t\t\t\t\t\t\t\t\t\t\t\"can take many arguments, such as --global or --main with default values. \"\n\t\t\t\t\t\t\t\t\t\t\t\"If an argument is missing, returns the default value given by the \\\n\t\t\t\t\t\t\t\t\t\t\t'--{name:subcommands}=<value>' parameter.\",\n\t\t\t\t\t\t\t\t\t\t\tepilog=\"Specify"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Parser for WXW platform')\n\n\t\tgroup = parser.add_mutually_exclusive_group()\n\n\t\tgroup.add_argument('-v', '--verbose', action='store_true')\n\t\tgroup.add_argument('-U', '--update', type=bool, help='update the wxp package with latest version')\n\t\tgroup.add_argument('--newest', action='store_true', help='create the new version of the package')\n\n\t\tgroup.add_argument('-e', '--export', type=str, help='export the WXW"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\t\n\t# First thing to do is to construct the subparser (i.e., define this with the command-line arguments).\n\t# To do this directly, you'll have to do something like this:\n\timport argparse\n\tparser = argparse.ArgumentParser(description='A program for reading a text file. This is part of Discourse, an Intercourse Data Bank (AC), program for extracting text from the file, and then the number of features in the text.')\n\tparser.add_argument('--version', action='version', version='%(prog)s \"SynthBase\". For more information, check"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targs = dict(arg.split(\"=\") for arg in unparsed_arguments if hasattr(arg, 'rstrip'))\n\t\ttry:\n\t\t\treturn vars(get_parser()).parse_known_args(args)\n\t\texcept SystemExit as inst:\n\t\t\traise SystemExit(u\"%s\" % inst)\n\t\texcept TypeError:\n\t\t\treturn None\n\n\n\n\ndef main():\n\tusage = \"usage: %prog [options] input-file\"\n\tp = argparse.ArgumentParser(usage=usage)\n\tp.add_argument('-v', '--version')\n\tp.add_argument('--"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Tool for processing DOS-based PCR data\")\n\tparser.add_argument(\"-c\", \"--config\", type=str, default=\"./config/pcr\", help=\"Path to configuration file\")\n\tparser.add_argument(\"-d\", \"--dryrun\", help=\"Print the job without executing, don't run any jobs.\", action=\"store_true\")\t\n\tparser.add_argument(\"-i\", \"--host\", type=str, help=\"Host Name\")\n\tparser.add_argument(\"-p\", \"--port\", type=str, help"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n    parser = argparse.ArgumentParser(description=\"Make a Python script that uses the pyscripts.  This is part of the autotest-framework CI server, and has a single-line-commentary \"\n        \"scriptable interface to start and stop automated execution of script modules.\")\n    parser.add_argument('--help', '-h', dest='help', default=argparse.SUPPRESS, action='store_true',\n                        help='show this help message and exit')\n    parser.add_argument(\"--version\", action='version',\n                        version='%(prog)s 0.14')\n\n    args = parser.parse_args"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targuments = parser_arguments()\n\t\targs = args[1:]\n\t\tparser = argparse._SubParsersAction()\n\t\tfor subparser_name, subparser in enumerate(arguments):\n\t\t\t#print \"parsing subparser '%s' with name '%s'\" % (subparsers, subparser_name)\n\t\t\ttry:\n\t\t\t\tnamespace, parser_args = parser.parse_known_args(subparsers, args)\n\t\t\t\targs = args[subparsers:]\n\t\t\t\tif argparser == nsargs:\n\t\t\t\t\tparser.parse_arguments(namespace)\n\t\t\texcept:\n\t\t\t\tparser."}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tsubparser_name = subparsers[0].strip()\n\t\tsubparser = Subcommand(subparser_name, **untagged_arguments)\n\t\tlist_or_parses = [subparser]\n\t\twhile list_or_parses:\n\t\t\tmore_parsed_arguments = list_or_parses.pop()\n\t\t\tfor unparsed_argument in unparsed_arguments:\n\t\t\t\tif unparsed_argument in all_parsers and unparsed_argument not in all_parsers:\n\t\t\t\t\tmore_parsed_arguments.append(None)\n\t\t\t\telse:\n\t\t\t\t\tbreak\n\t\t\tif len(more_parsed"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t \n\tactions = []\n\t\n\t\n\trepo_parser = subparser_for_repository(unparsed_arguments)\n\t\n\t\n\taction_args = actions[0].parse_args(args = repo_parser.parse_args())\n\t\n\tif isinstance(action_args, argparse._SubParsersAction):\n\t\tsubparser_actions.append(action_args)\n\telif isinstance(action_args, dict):\n\t\t# parse actions from subparser named list of arguments\n\t\tsubparser_actions = action_args.keys()\n\t\tfor action_arg in subparser_actions:\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\targs = {}\n\tfor i, arg in enumerate(unparsed_arguments):\n\t\tif arg in arg_parser:\n\t\t\targs[i] = arg_parser[arg]\n\t\t\tif args[i].startswith(\"--\"):\n\t\t\t\targs[i][1] = args[i].lstrip(\"-\")\n\t\n\treturn args\n\t\t\n\n\t\t\ndef execute_subparser(subparsers, namespace):\n\t\"\"\"\n\tRun the registered subcommand.  Re-maps the current subparser to the action's namespace, and\n\tcreates a new subparser with the arguments given as a dictionary to pass along to"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\targs = unparsed_arguments\n\tif not len(args) > 1:\n\t\traise SyntaxError(\"missing one or more argument\")\n\n\tif hasattr(args[0], \"__iter__\"):\n\t\targs = args[0]\n\n\treturn subparsers.add_parser(*args.iteritems())\n\n\nclass FatalError(Exception):\n\tpass\n\n\ndef parse_subparser(subparsers, action):\n\t\"\"\"\n\tGiven a Namespace and a action, return a parser for the corresponding namespace-scoped\n\tsubparser.\n\t\"\"\"\n\n\tdef parser(name, **kwargs):"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(prog=\"subparser\", add_help=False)\n\t#parser.add_argument(\"--repository\", dest=\"repository\", help=\"Repository for the project.\")\n\tparser.add_argument(\"--dest\", \"-d\", nargs=1, metavar=\"destination\",\n\t\t\thelp='Destination directory for the subparser instance (optionals '\n                        'the default location)')\n\tparser.add_argument(\"--all\", \"-a\", dest=\"dryRun\", help=\"Pass the --dry-run argument to s3 commands.\")\n\tparser.add_argument(\"--all-repos\", \"-"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\t# create our parser for subactions\n\tparser = argparse.ArgumentParser(help=\"Manipulates and runs actions. See https://github.com/karzikzak/karzik-cli/wiki/Sub-actions\")\n\tsubparsers = parser.add_subparsers(title=\"Subactions\", dest=\"subparsers\", metavar=\"action\")\n\t\n\t# Parse subparser names/argparser\n\tfor ns in  subparsers:\n\t\tparsed_ns = parse_subparser_name(ns.dest, subparsers)\n\t\t\n\t\tif not parsed_ns:\n\t\t\traise"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tret = ([], False)\n\t\tfor argname, argparser in unparsed_arguments.items():\n\t\t\tif not argparser.choices:\n\t\t\t\traise argparse.ArgumentError(\"The %s argument is missing required argument(s)\" % argname)\n\t\t\tfor choice in argparser.choices:\n\t\t\t\tif choice.dest not in ret:\n\t\t\t\t\tret[choice.dest] = []\n\t\t\t\tret[choice.dest].append(str(argparser))\n\t\treturn ret\n\treturn tuple(parse_subparser_arguments(args, subparsers))\nclass OptParseSubparser(argparse."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\ttry:\n\t\t\timport argparse\n\t\t\tsubparser = subparsers.add_parser(subcommand_name)\n\t\texcept ImportError, ie:\n\t\t\tsubparser = None\n\t\t\n\t\t# get arguments passed in by the user\n\t\targs = subparser.parse_args(unparsed_arguments)\n\t\t\n\t\t# make sure the user knows if an argument is provided\n\t\tif hasattr(args, \"repository\"):\n\t\t\t# get all of the subparser parameters given by the user\n\t\t\tall_options = []\n\t\t\tfor o in subparsers.choices[args.repository]:\n\t\t\t\tparts = o.get_params"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t    \n\tparser = subparsers.add_parser('repository', help=\"repository information and return (with --repository)\")\n\tsubparser_result = parser.add_subparsers(help='action list actions')\n\t\n\t#\n\t# Pull out the options\n\t#\n\t# This is a bit of a hack because argparse's parser doesn't support all the\n\t# option types (which are parsed to separate subparser)\n\t#\n\t# Ideally, the optparse parser was written to be somewhat similar to the optparse\n\t# and should just work for subparser-special options (subparser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\targs = dict()\n\n\tfor arg_name in (i[0] for i in unparsed_arguments if i[1].startswith(\"_\")):\n\t\targ_name = arg_name[2:]\n\t\targs[arg_name[0]] = getattr(argparse, arg_name[0], None)\n\treturn subparsers.parse_args(args)[0]\n\ndef main(arguments):\n\t\"\"\"\n\tParse arguments as defined in parse_subparser_arguments, and run the requested\n\tcommand handler\n\t\"\"\"\n\n\tprint \"Getting started action %s\" % arguments"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t        ##\n\t\targs = parse_args()\n\t\targ0 = args.parser\n\t\targ1 = args.args\n\t\t\t                                                                                                                                \n        ##\n\t\tparser = argparse.ArgumentParser(\"Morph.py - Pruning Morphes\",\n\t                                           description=\"\")\n\t    \t##\n        ## If not supplied, create and check its base options\n\t#        print (\"  Parsing Top Level Parser - \")\n\t#\tif arg0 == 'plot':\n\t#\t\tcreate_argparser()   \n\n    ##\n\t##\n\t##\n\t##\n\t##\n\t##\n\t##\n\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Plays gameplay to the Xbox Gamepad and is a web based command line interface.')\n\t\tparser.add_argument('--name', '-n', type=str, required=True, help='Name for this command. Must match the name used with the \"Gamepad_NAME\" setting')\n\t\tparser.add_argument('--version', help='Set our program version', action='version', version=' '.join(version.VERSION))\n\t\tparser.add_argument('--author', '-a', action='store_true', help='Display the author')\n\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n# Create the top-level parser.\nparser = argparse.ArgumentParser(\n\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\tdescription='A tool that is created to build a top-level parser. Usage: my_parser.py <indir> <outdir>')\n\n# Add optional arguments.\n\n# Create a top-level parser and its subparsers.\n\n# TODO:\n# Get the top level parser\n\ndef my_parse():\n\t\"\"\"\n\tFunction called when the program is launched (the main program)\n\tReturns the new argparse object as a object.\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(prog = 'python.py', description = \"Test Python API\", parents = [make_argparser()])\n\tparser.add_argument('path', metavar = 'path', type = str, help = 'Filesystem path')\n\tparser.add_argument('--version', action = \"version\", version = 'Version', version_action = \"store_true\")\n\treturn parser.parse_args()\n\ndef main():\n\t# Parse arguments\n\targs = get_args()\n\t\n\t# Execute\n\tif args.version and args.version == True"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=__doc__,\n\t\t\t\t\t\t\t\t\tepilog='''\n    To read the docstrings from a.py file, use \"-help\" in the python path.''')\n\n\tparser.add_argument('-t', '--type',\thelp='The parser type',\n\t\t\t\t\t\t\t\t\tchoices=['text', 'int', 'float', 'complex', 'float-float', 'float', 'bytes'])\n\n\tparser.add_argument(\"-r\", \"--raw\", action='store_true',\n\t\t\t\t\t\thelp=\"Print raw input\")\n\n\tparser.add_argument(\"-o\", \"--output\", help=\"Print output\","}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tdef _subparser(name, help):\n\t\tif not help:\n\t\t\treturn None, None\n\t\telse:\n\t\t\thelp_str = \"\\n\".join(help.strip().split(\"\\n\")[0:2])\n\t\t\treturn (name, help_str)\n\n\tdef _name_arg(name, help):\n\t\t\"\"\"Get the string representing a named arg or a help string.\"\"\"\n\t\tif name:\n\t\t\treturn \"--\" + name\n\n\tdef add_args(name, help):\n\t\t\"\"\"Add a name argument to this parser.\"\"\"\n\t\tif help:\n\t\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription='Generate the report as JSON', epilog='Copyright (C) 2015')\n\t\tparser.add_argument('--output_dir', type=str, required=True)\n\t\tparser.add_argument('--output_file', type=str, required=True)\n\t\tparser.add_argument('--dry_run', action='store_true')\n\t\tparser.add_argument('json_file', type=str, nargs='?', default='report.json')\n\t\treturn parser.parse_args()\n\ndef run():\n\t\"\"\"\n\tParse the"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t \n\t# Define the command line parser and parse the arguments\n\tparser = argparse.ArgumentParser(\n\t\tdescription='This script create a command line based on your own command',\n\t\t formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\tadd_help=False)\n\tparser.add_argument('-s', '--source', required=True, help='source directory')\n\tparser.add_argument('-r', '--ref', required=True, help='reference directory')\n\tparser.add_argument('-d', '--dest', required=True, help='destination')\n\t\n\targs = parser.parse"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = ArgumentParser(description=\"{0} v%s\" % (__version__, __url__))\n\t\tparser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')\n\t\tparser.add_argument('-p', '--priority', dest='priority', default=0, type=int, metavar='N', help='Priority level [default = {}]'.format(DEFAULT_PRIORITY))\n\t\tparser.add_argument('--list', action='store_true', help='Print the output as a list')\n\t\tparser.add_argument('--patharg', action='store"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser_help = None\t\t# A reference to the :class:`argparse.ArgumentParser` class from `argparse`.\n\tparser_type = None\n\tparser_usage = 'usage: python %prog'+ os.path.basename(sys.argv[0]) +'\\n\\n'\n\t\n\tparser = argparse.ArgumentParser(\n\t\t\tformatter_class=lambda prog: argparse.HelpFormatter(prog.formatter_class, max_help_position=MAX_HELP_POSITION),\n\t\t\tdescription='A command-line tool that auto-runs arbitrary python program files over the contents of the provided"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(\n\t\t\t[\n\t\t\t\tFlattenMappingNode(\n\t\t\t\t\tkey=ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t\t\t\t\tvalue=ScalarNode(tag='tag:yaml.org,2002:map', value=[\n\t\t\t\t\t\t(\n\t\t\t\t\t\t\tScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t\t\t\t\t\t\tScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t\t\t\t\t\t),\n\t\t\t\t\t\t],\n\t\t\t\t\t),\n\t\t\t\tRecursiveMappingNode({\n\t\t\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\nclass DeepMergeOrder(OrderedDict, collections.Mapping):\n\t\"\"\"\n\tIn a backward dependency ordering order, this class provides an\n\timplementation of dict's items() method, for example:\n\t>>> d = DeepMergeOrder({'k1': 'v1', 'v2': 'v2', 'k3': 'v3'})\n\t>>> sorted(six.iteritems(d))\n\t[('k1', 'v1'), ('v2', 'v2'), ('v3', 'v3')]\n\t\"\"\"\n\t\n\t__slots__ = ()\n\t\n\t__reversed__ ="}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn yaml.representer.SafeRepresenter.represent_mapping(mapping, 'tag:yaml.org,2002:map', 1.0)\n\nclass Node(yaml.nodes._Node):\n\t\"\"\"\n\tA borgmatic configuration node consisting of an ordered, hierarchical, hierarchical\nconfiguration file, consisting of a list of key-value pairs. Two simple keys must\n\tbe defined: a borgmatic configuration file whose \"section:yaml.org;1\" value\n\tcontains a pair of \"tag:yaml.org;2;3;4;5;6;7.9"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tnew_config_file = open(self.config_file,'w')\n\t\tnew_config_file.write('\\n'.join(str(i) for i in nodes))\n\t\tfor (tag,values) in deep_merge_nodes(nodes):\n\t\t\tself.config_file.write('[%s]%s\\n' % (tag,os.linesep))\n\t\t\tself.config_file.write('%s\\n' % ('\\n#'+'' * len(values) +'') +''.join(values))\n\t\tnew_config_file."}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge(nodes, deep)\n\t'''\n\n\ndef get_config_dict_merge(config_dict):\n    \"\"\" Return a new config dictionary with a combined list of merged ConfigDicts.\"\"\"\n\n    merged_config_dict = dict()\n\n    for merge_key, merge_value in config_dict.items():\n        merged_config_dict[merge_key] = merge_value\n\n    return merged_config_dict\n\n\nclass ConfigDict(dict):\n\t\"\"\"\n\tA simple class which helps manage multiple dictionaries. In case you want to\n\tinclude multiple dictionaries, combine them in a"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(deepcopy(nodes))\n\ndef construct_config(configuration):\n\n\t\"\"\"\n\tGiven a configuration dictionary and a configuration key, and a configuration\n\tdefinition, construct a ConfigParser (or subclass) that contains the configuration\n\tas a root of it. The Config class is instantiated by passing in the configuration\n\tdictionary as the only parameter to the __init__() of the ConfigParser constructor.\n\n\tIt returns a ConfigParser instance. The caller can then use the configure()\n\tfunction to instantiate the right class.\n\t\"\"\"\n\n\tconfig = {}\n\tfor (k, v"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(\n\t\t\t[\n\t\t\t\t('tag:yaml.org,2002:str','retention'),   # only preserved tags\n\t\t\t\t('tag:yaml.org,2002:map', 'keep_hourly'),   # only preserved tags\n\t\t\t\t('tag:yaml.org,2002:str', 'keep_daily'),   # only preserved tags\n\t\t\t\t('tag:yaml.org,2002:int', '24'),                 # only preserved tags\n\t\t\t\t('tag:yaml.org,2002:float', '7'),                 # only preserved tags\n\t\t\t\t('tag:yaml."}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t# TODO: We could simplify this algorithm and instead of using it in production:\n\t\t# Perhaps we could optimize this by creating a new tree that looks like a \n\t\t# dictionary which represents a whole configuration tree, and then walking its\n\t\t# contents and merging the whole configuration tree.\n\t\t#\n\t\t# Yes, this means that if two dictionaries have the same key, the nodes\n\t\t# of the last level are the merged, if there are no overlapping nodes\n\t\t# (including no node with key \"mapping_\" in the merged dictionary,\n\t\t# for example: \"mapping6\") then"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(\n\t\t\tnodes[1:]\n\t\t)[1:]\n\n\nclass _DummyTreeWithDummyNode(object):\n    \"\"\"\n\tA very simple \"dummy\" tree that is a dummy node; it is not actually a dummy node\n\t(to be used in a `Node`), and is used when we're \"running\" the tree.\n\tThis class represents a single dummy node and a node with a parent.\tThis class provides\n\tfull control over what nodes to inherit (by inheriting from `_DummyTreeWithDummyNode`.)\n\t\"\"\"\n\tvalue = None\n\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\traise NotImplementedError('Dupe merge not yet implemented in deep_merge_nodes() since merging is not implemented yet.')\n\ndef merge_bundles(a, b):\n\t\"\"\"\n\tGiven a configuration data structure as returned by build_bundles, return a new list of\n\tMergingBundle objects.\n\t\"\"\"\n\tmerged_bundles = []\n\n\tfor key, values in b.iteritems():\n\t\tif len(values) < 2:\n\t\t\traise ConfigException(\"Bundle keys are required for merging.\")\n\n\t\tmerged_values = []\n\t\tfor value in values:\n\t\t\t"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\targparser = ArgumentParser()\n\t#argparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s\" + __version__)\n\t\n\targparser.add_argument(\"--version\", action=\"version\", version=\"%prog \" + __version__)\n\t\n\targparser.add_argument(\"--run\", \"-r\", action=\"store_true\", help=\"Runs a single or pair of commands.\")\n\t\n\targparser.add_argument(\"--run-once\", \"-r\", action=\"store_true\", help=\"Runs a single or pair of commands then exit when successful. No error output will be returned"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tepilog = \"\"\n\t\targuments=arguments\n\t\targparsers = argparse.ArgumentParser(description='A script to convert an input_files from a FITS file into an FITS file.')\n\t\t\n\t\targparsers['input_files'].add_argument(dest='inputFile',\n\t\t\t\t\t\t\t\t\t\t\tmetavar='<input_file>',\n\t\t\t\t\t\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\t\t\t\t\t\thelp='Input file that needs to be converted.')\n\t\t\n\t\targparsers['output_file'].add_argument(dest='outputFilePrefix',\n\t\t\t\t\t\t\t\t\t\t\tmetavar='<prefix>',\n\t\t\t\t\t\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\t\t\t\t\t\thelp='The prefix to create"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n# Parse the command-line arguments.\nparameters = commandParser.parse_args(sys.argv[1:])\n# Create the parser instance and connect it to an existing instance.\nparser = argparse.ArgumentParser(description='A library for performing'+ __program__)\n_handleArguments(parser)\narguments = parameters.parse_args()\n# Process the arguments\narguments.handleArguments(arguments)\n# Run the main program.\nexecute(arguments)\n\"\"\"Support gathering system information of hosts which are running the Apache2 DNS\nserver (ie /etc/apache2/mods-enabled"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\t\tparser = ArgumentParser(prog='python script', description='Scripts to add or remove tags from a document. This is a very simplistic implementation of a Pythonic script.' )\n\n\t\tparser.add_argument('--add', action=\"store_true\",\n\t\t\t\t\t\tdest=\"action\", help=\"Add tag to the document\")\n\t\tparser.add_argument('--rmtag', action='store_true',\n\t\t\t\t\t\tdest=\"rmtag\", help=\"Remove tag from the document in the document.\")\n\t\t#\n\t\t#\n\t\t# These arguments could come in three forms, where the first is the command. Add"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(description=\"Create a parser of options related to the project file.\")\n\tparser.add_argument(\"-c\", \"--config\", metavar = \"config_file\", default = \"config\", type = str,\n\t                    help = \"Path of the configuration file\")\n\n\t# For now, just parse the arguments - these are the defaults\n\tparser.parse_args(*arguments[1:])\n\treturn parser\n\n\ndef main():\n\tparser = parse_arguments()\n\targs = parser.parse_args()\n\tdata = main_project(args.config)\n\thandle_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t # We don't parse the flags yet, but if we do use argparse directly (instead of argparse),\n\t # argparse might be more sophisticated.\n\t parser = argparse.ArgumentParser()\n\n\n\t # Required options\n\t parser.add_argument(\"name\", type=str, help=\"name of the script to execute\")\n\t parser.add_argument(\"file\", type=str, help=\"path to the file containing the script to execute\")\n\t parser.add_argument(\"--args\", type=str, nargs='+', required=True, help=\"additional args to call the script (can be empty)\")\n\t parser.add"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description = \"\"\"\n        A wrapper for ``py_compile_wrapper.py`` that adds this class attribute and\n    its child methods.\n\t\"\"\")\n\n\tparser.add_argument( \"-c\", help = \"command line argument\", type = str, required = True )\n\tparser.add_argument( \"-d\", help = \"command line argument\", type = str, required = True )\n\toptions = parser.parse_args()\n\n\treturn parser, options\n\n\n\ndef main():\n\t\"\"\" \n\tInvoked from ``py_compile_wrapper.py``,"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n    class ArgumentParser(argparse.ArgumentParser):\n        \"\"\"\n        This is the argparse-based interface of argparse that handles the\n        `argparse.ArgumentParser` interface.\n        \"\"\"\n    \n        def __init__(self, prog=None, **kwargs):\n            super(ArgumentParser, self).__init__(**kwargs)\n            \n            # The following line handles some argument processing, i.e., from \n            # argparse where the parameters follow.  \n            #\n            # We want the parser to have the options/arguments from ArgumentParser\n            # to give default values for the arguments.\n            self.prog = prog\n        \n            # Initialize with any required command-line arguments, as standard\n           "}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\targuments = ArgumentParser(conflict_handler=\"resolve\",\n\t                         add_help=False, formatter_class=RawTextHelpFormatter)\n\n\targuments.add_argument(\"infile\", nargs=\"?\", type=FileType('r'),\n\t                      default=sys.stdin,\n\t\t                      help=\"Read the data from this file\", metavar=\"<infile>\")\n\t\n\targuments.add_argument('outfile', nargs=\"?\", type=FileType('w'),\n\t                      default=sys.stdout,\n\t\t                      help=\"Write the data to this file\", metavar=\"<outfile>\")\n\t\n\targuments.add_argument('-e', '--extension',\n\t\t    help='"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\tif len(arguments) > 2:\n\t\traise \"argument format error\"\n\t\t\n\tif os.environ.get(\"DEBUG\", \"0\") == \"1\":\n\t\targuments = arguments[0]\n\t# this only gives false positives\n\targuments = list(arguments)\n\t\n\tparser = argparse.ArgumentParser(\n\t\tdescription='This script will be called to read the specified file(s).')\n\tparser.add_argument('-n','--name',dest=\"filename\",default=[],action=\"append\",help=\"Load a file from the specified name.\")\n\tparser.add_argument('-t','--type"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t # Use a list instead of None as a default.  This allows a better\n\t # error message in case of error reading the command line arguments.\n\t parser.flags = map(ArgumentParser.get_usage, parser.rargs)\n\n\t # If the caller passed no args, re-raise error.\n\t def maybe_make_str(p):\n\t\t\tif p is None:\n\t\t\t\treturn None\n\t\t\ttry:\n\t\t\t\treturn \"--%s %s\" % (get_prog_name(p[0]), p[0])\n\t\t\texcept:\n\t\t\t\treturn \"\"\n\n\t # All the non-opt arguments"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t \n\treturn parser.parse_args(sys.argv[1:len(argv)])\n\ndef main(argv):\n\t\"\"\"\n\tGiven the command line args, parse flags and call the main process, and return\n\tthat.  \n\t\"\"\"\n\n\t# Determine the number of flags by splitting at the first \"--\" and \"--,--\".\n\tnum_opts = len(argv)\n\tnum_args = 0\n\ti   = 0\n\twhile i < num_opts:\n\t\tif argv[i].startswith(\"-\"):\n\t\t\tif argv[i] == \"--\":\n\t\t\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\topts, unknown = parser.parse_known_args()\n\t\n\tif opts.verbose:\n\t\tprint(\"Flags:\\n\\t%s\" % \", \".join(sorted(opts.flags)))\n\t\t\ndef parse_command_line(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument parsed into a\n\tparsonly-built tuple list.\t\"\"\"\n\n\topts, unknown = parser.parse_known_args()\n\t\n\tif opts.verbose:\n\t\tprint(\"\\n\\tOptions:\")\n\t\tprint(\"\\t-h, --help            Print help"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t \n\tparser.add_argument('--help', action='store_true', default=False, \n\t\thelp='Prints detailed help information.')\n\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", default=False,\n\t\thelp=\"Increase output verbosity.\")\n\n\tparser.add_argument(\"-c\", \"--compact\", dest=\"compact\", action=\"store_true\", default=False,\n\t\thelp=\"Generate output compact XML.\")\n\n\tparser.add_argument(\"-t\", \"--text\", dest=\"text\", metavar=\"FILE\", nargs=\"+\", \n\t\thelp"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn [flag for (flag, short_name, flag_string,\n\t\t\tmetavar) in _flags.__dict__.items() if not short_name.startswith('flag_')]\n\ndef _setup_arg_parser():\n\tadd_standard_arguments(parser)\n\tadd_extra_arguments(parser, '-h')\n# vim: set encoding=utf-8\n\n#  Copyright\u00a0(c)\u00a02016 Intel\u00a0Corporation\u00a0\n#\n#  Licensed\u00a0under\u00a0the\u00a0Apache\u00a0License,\u00a0Version\u00a02.0\u00a0(the\u00a0\"License\");\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\t# This needs a better parser type to set, since each option instance can\n\t# have multiple arguments, not just strings.  The easiest way to do this\n\t# is to just use a single argparser object, but this may be useful.\n\tparser.add_argument(\"-c\", \"--config\",\n\t                   nargs=1,\n\t                   metavar=\"PATH\",\n\t                   default=\"\",\n\t                   dest=\"parser_config\",\n\t                   help=\"Use this argparser as a config file instead of stdin.\")\n\t\n\tparser.add_argument(\"opt_args\",\n\t                   metavar=\"ARG\",\n\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\t# Use our parser by default\n\tparser.add_argument(\"-f\", \"--file\", dest=\"outfile\", help=\"Output results to FILE\")\n\t\n\t# Use our parser by default\n\tparser.add_argument(\"-E\", \"--encoding\", dest=\"encoding\", help=\"Output results to ENCODING\")\n\t\n\t# Get a list of all available parsers\n\tavailable_parsers = filter(lambda g: callable(g, parser_type), available_parsers)\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", help=\"Be verbose\")\n\tparser.add_argument(\"-"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tflags = list()\n\tfor cmd in cls.commands:\n\t\tif cmd.option_strings: flags += [cmd]\n\treturn parser.parse_args(flags)\n\n\nclass CommandError(Exception):\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\n\n\n\n\n\n\n\nclass CommandErrorArgumentParser(argparse.ArgumentParser):\n\tdef print_options(self, *args, **kwargs):\n\t\tprint(*args, file=sys.stderr, **kwargs)\n\t\treturn None\n    \n\ndef main():\n\tdef get_all_config_options(argv):\n\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t    \n\tflags = parser.parse_args(sys.argv[1:])\n\treturn flags\n\ndef main():\n\t\n\t\"\"\"\n\t\tReturns the main options, parses the options, and returns the Namespace object containing usage info for the\n\t\tprogram that is running the program.\n\t\t\n\t\tReturns:\n\t\t\tnamespace_options : Namespace options object which contains the flags as expected by the\n\t\t\t\tprogram's parse_args() function.\n \t\t\toptions              - List of Namespace options.\n\t\"\"\"\n\t\n\t# Parse optional arguments\n\tflags = parser_flags(sys.argv[1:])\n\tif len"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\ndef main(inFileNames, options, args):\n\t# Check to ensure we don't have a single \"use all\" and \"--version\", or even\n\t# one \"-version\" argument.\n\tif len(inFileNames) < 2:\n\t\tparser.error(usage_string() + \": too few arguments provided\")\t\t# The usage will be printed out by raising an error.\n\n\trequiredArgsCount = 0\n\trequiredArgsList = []\n\n\tcommandLineFlags = []\t# The arguments we use, as indicated by options.\n\t\n\tfor i in range(len("}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n\n\tbash_completion_args = ['borgmatic', '--help']\n\tbash_completion_arguments = ['--help']\n\n\tbash_completion_arg_parser = argparse.ArgumentParser()\n\tbash_completion_arg_parser.add_argument('--borgmatic-file', '-f',\n\t\t\t\t\t\taction='store', dest='borgmatic_file', type=str,\n\t\t\t\t\t\thelp='a borgmatic command file')\n\n@click.command(argument_default=\"borgmatic.py --help\",\n\t\t\t\t   help=\"Print borgmatic's help\")\n\n@click."}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser.add_argument('-b', '--bash', action='store_true', help='bash completion')\n\t\t\n\t\t# create a list of arguments and options provided, this is a list of arguments\n\t\targs = parser.parse_args()\n\t\t\n\t\tif args.bash:\n\t\t\tcommand = ['bash']\n\t\telse:\n\t\t\tcommand = []\n\t\t\n\t\t# this will be the complete command\n\t\treturn command\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t# -*- coding: utf-8 -*-\n\nfrom __future__ import division, print_function\n\n__all__ = ['fftconvolve', '"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog=BORGMAN_CMD_HELP, description=BORGMAN_CMD_HELP_EPILOG)\n\t\tparser.add_argument('command', nargs='?', type=str)\n\t\treturn parser\n\n\t# ------------------------------\n\t# - borgmatic commands\n\t# ------------------------------\n\t'''Perform an interactive borgmatic command'''\n\tparser = bash_completion()\n\targs = parser.parse_args()\n\t\n\t'''Use the borgmatic command prompt'''\n\targs = argparse.Namespace()\n\targs.command = BORGMAN_CMD"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\timport os\n\t\tshell = os.getenv('SHELL')\n\t\tif not shell:\n\t\t\tshell = '/bin/bash'\n\t\treturn '\"%s\"' % shell\n\telse:\n\t\treturn 'borgmatic bash'\n\n#-----------------------------------------------------------------------------\nclass Borgmatic:\n\t\"\"\"\n\tShell for borgmatic commands to execute in bash terminal mode.\n\tNote: if one side is run_macro() that is a shell argument, that argument will be passed to the shell\n\tto execute the borgmatic command.\n\t\"\"\"\n\tdef __init__(self, shell):\n\t\t\"\"\""}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = ArgumentParser(add_help=False)\n\t\tparser.add_argument(\n\t\t\"command\", nargs=\"?\", \n\t\thelp=\"The command to run\")\n\t\tparser.add_argument(\n\t\t\"argument\", help=\"The argument to be passed to bash completion script\")\n\t\targs = parser.parse_args()\n\n\t\tif args.command == \"run\" or args.command == \"run-to-file\":\n\t\t\tcmd = bash_entry_path\n\t\telse:\n\t\t\t#print \"Error: Only borgmatic, run, and run-to-file commands are"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\treturn '''bash -c \"python /var/log/zookeeper_agent/zk_borgmatic_completion.sh \"$@\"' \t# this is very very important\n\n\tdef bash_exec_on_slave(host_list, *hosts):\n\t\t\"\"\"\n\t\tExecute all of the hosts on this machine (within a lock)\n\t\t\"\"\"\n\t\ttry:\n\t\t\tssh = SSHClient()\n\t\t\tssh.set_missing_host_key_policy(AutoAddPolicy())\n\t\t\tssh.connect(hosts, username='root', password=password)\n\t\t\toutfiles ="}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\treturn _run_borg(commands.get(), context)\n\ndef setup():\t\"\"\"\n\tStart up the user-commands.\n\t\"\"\"\n\timport sys\n\tglobal Borg\n\tcommands = parse_commands()\n\tsys.argv.extend(['init'])\n\tborg = Borg.Borg(commands, sys.argv[1:], debug = '--debug' in sys.argv)\n\nsetup()\n# -*- coding: utf-8 -*-\nfrom.test_base_case import BaseCase\n\n\nclass TestLinterException(BaseCase):\n\n    def test"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\timport readline\n\t\tborglabs_completion_dir = borglabs_completion.get_completion_dir()\n\t\thistory_location = os.path.join(borglabs_completion_dir, \"bashHistory.py\")\n\t\thistory_file = os.path.join(history_location, \".bash-history\")\n\n\t\tif os.path.exists(history_file):\n\t\t\twith open(history_file) as f:\n\t\t\t\tdata = f.read().splitlines()\n\t\t\t\tprint(\"[bash] History taken from Borglabs completed at:\", data["}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Borgmatic Command Line Interface')\n\t\tparser.add_argument('-c', '--command',  help='command to run')\n\t\tparser.add_argument('-r', '--rev-parse',  help='rev-parse command to run')\n\t\tparser.add_argument('-P', '--rev-file', help='file containing revisions for the rev-parse command')\n\t\tparser.add_argument('-d', '--dir',  help='relative path to the project directory to be used for this borgmatic command')\n\t\targs = parser.parse_"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tpass\t\n\n\t# \t#\n\t# \t#\n\t# \t#\n\t# \t#\n\t# \t#\n\n\tdef checkAndGetOptions(self, args, opts):\n\t\t\"\"\"\n\t\tCheck that a user supplied options are sane.\n\t\tIf any of the arguments are invalid, display them and quit.\n\t\t\"\"\"\n\t\tparser = ArgumentParser(prog='bash-completion')\n\t\tparser.add_argument('--help', action=\"store_true\", default=False)\n\t\topts.add_parse_callbacks(sub_parsers_action(parser"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n    # define the parser, and parse the arguments\n    parser = argparse.ArgumentParser(description=\"Parse arguments from a subcommand module.\",\n                                     epilog=\"If no module name is provided, defaults to the 'global' module.\")\n    parser.add_argument(\"-u\", \"--user\", type=str, default=\"\", help=\"Target user; defaults to the \"\n                                                                \"site username if not specified.\")\n    parser.add_argument(\"-p\", \"--password\", type=str, default=\"\", help=\"Target user; defaults to the \"\n                                                                \"site password if not specified.\")\n    parser.add_argument(\"-s\", \"--site\", type"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t \n\targs = argparse.Namespace()\n\targs.update(vars(parser.parse_args()))\n\tif hasattr(args, 'parser_args'):\n\t\targs.parser_args = vars(args)\n\tif hasattr(args, 'argparser'):\n\t\targs.argparser = vars(args)\n\targs.func = parse_functions(unparsed_arguments)\n\tif hasattr(args, 'argparsers'):\n\t\targs.argparsers = vars(args)\n\tif hasattr(args,'subparsers'):\n\t\targs.subparsers = vars(args)\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\n\t\twith open(sys.argv[1]) as f:\n\t\t\targs = parser.parse_args(f)\n\n\t\t\t# Look up all subcommands within the argument list that start with the main script name\n\t\t\tfor command_group, command in parser._subparsers:\n\t\t\t\tfor subcommand in command_group._visible_commands:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfor suboption in command._choices:\n\t\t\t\t\t\t\tif suboption is not None:\n\t\t\t\t\t\t\t\tsubparser = command._subparsers[subcommand]\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif command.dest"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=name(), \n\t\t\t\t\t\t\t\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t\t\t\t\t\t\t\tusage='%(prog)s [options] [<subcommand>]')\n\t\tparser.add_argument('subcommand', nargs='*', help='optional subcommands used by application to run, i.e., \"install\" for \"install\" and \"build\" for \"build\"')\n\t\targuments = vars(parser.parse_args(unparsed_arguments))\n\t\targuments = vars(arguments)\n\t\treturn merge(arguments, unparsed_arguments)\n\n\tdef parse_args(un"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(dest=\"command\", help=\"command to run\")\n\n\t\tparser.add_argument('--run')\n\t\tparser.add_argument('--quiet', dest='quiet', action='store_const', const=True, default=False,\n\t\t                     help='do not prompt for input or log file')\n\t\tparser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,\n\t\t                     help='show verbose output')\n\t\targs = vars(parser.parse_args(*unparsed_arguments))"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\tif len(args) == 0:\n\t\targs = sys.argv[1:]\n\tnamespace = arguments(subparser_names, args)\n\n\t# Create a namespace and all subclasses of the parser being used\n\tnamespaces = {}\n\tfor ns in namespaces:\n\t\tnsname = ns_parser.find_parser(namespaces[ns]).name\n\t\tnsinfo = ns_parser.find_parser(namespaces[ns]).info()\n\n\t\tif len(nsinfo.args) == 0:\n\t\t\tns = nsinfo.namespace(nsname)\n\t\t\tns._parses ="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter, epilog=examples)\n\t\t\n\t\tfor command_name, subparser in sorted(iteritems(all_parsers)):\n\t\t\tns = None\n\t\t\tfor argument in unparsed_arguments:\n\t\t\t\t# if an argument is given for an argument, print help for it as we only want to set that info\n\t\t\t\tif argument.startswith('--' + str(command_name)) or argument==command_name:\n\t\t\t\t\tns = argument # so we can output to that command\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\t# make"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = getParser(unparsed_arguments[0])\n\t\tname = unparsed_arguments[1]\n\n\t\tglobal_args = {}\n\t\ti = 0\n\t\twhile i < len(unparsed_arguments):\n\t\t\tif i == len(args):\n\t\t\t\t# This function is called for every command in order to provide a namespace for all command-line\n\t\t\t\tglobal_args.update(args[i].args)\n\t\t\telse:\n\t\t\t\tassert not isinstance(args[i], namespace)\n\t\t\t\tkey = args[i].args[0].replace('-', '_')\n\t\t\t\tv = args"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\t# Use arg parse_known_args.\n\tparser = get_args_parser(\n\t\t\tprog=\"python %prog\",\n\t\t\tversion=\"%(prog)s Version 0.01\",\n\t\t\tdescription=\"Generate a new version of subcommands\",\n\t\t\tepilog=epilog)\n\tparser.usage.add_argument(\n\t\t\t\"%prog\", action=\"store\", dest=None, help=\"\\nExecute this command in a program.  This is intended to be used either way\\n\"\n\t\t\t\t\"through the development program.  This is usually done by the command-line version\")\n\tparser."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n# define the program's main input parser (not a subclass because to do nothing with it).\narg_parser = argparse.ArgumentParser()\n# add the command line args\narg_parser.add_argument(\"-s\", \"--script\", action=\"store\", type=str, help=\"Script to run, or run_once (if not running a script)\")\narg_parser.add_argument(\"-i\", \"--input\", action=\"store\", type=str, help=\"Filename/path on stdin\")\narg_parser.add_argument(\"-t\", \"--thread-count\", action=\"store\", type=int"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tdef _parse_args_to(args_description):\n\t\t\"\"\"\n\t\tParses an argv-style description of a command with positional actions\n\t\tas a tuple of (a namespace that stores positional arguments, a list of remaining\n\t\targuments not claimed by any subparser).\n\n\t\tNote that an exception is thrown if such args are used without\n\t\tprocesing one of the sub-actions. It is because a\n\t\tparticularly broad usecase for users.\n\t\t\"\"\"\n\n\t\t# Add the command's arguments to our _actions dict\n\t\tactions = {}\n\t\t# And return"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\t# We sort so that the name passed is not unique\n\t\tname_list = [a['metavar'] for a in unparsed_arguments.items()]\n\t\t# Make a dict mapping from dest to parsed names\n\t\tdest_namespace = dict(filter(lambda a: not a['dest_namespace'], name_list))\n\n\t\t# Make a dict mapping from parsed args to namespace of arguments\n\t\targs = dict(filter(lambda a: not a['dest_namespace'], args.items()))\n\n\t\t# Convert the argument names to a Namespace for a function call\n\t\tnamespace = Namespace()\n\t\tfor"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\t\n\tsubparsers_map = {}\n\t\n\tfor subcommand in subcommands.get('subcommands', []):\n\t\tfor args_tuple in unparsed_arguments.get(subcommand):\n\t\t\tfor action, arg_list in args_tuple.items():\n\t\t\t\ttry:\n\t\t\t\t\taction(subcommand, args_tuple)\n\t\t\t\texcept ValueError as e:\n\t\t\t\t\tif e.args[0] == '-' or e.args[0] == '+':\n\t\t\t\t\t\traise\n\t\t\t\telse:\n\t\t\t\t\tsubparsers_map[action] = arg_list\n\t\t\n\treturn subparsers_map\n\ndef _"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tcommand_list = find_commands(unparsed_arguments, subparsers.actions)\n\n\tif len(command_list) == 0:\n\t\t#TODO: Implement error handling\n\t\tsys.exit(command_list[0])\n\n\tresult = []\n\tfor action_name, args in command_list:\n\t\taction = subparsers.actions[action_name]\n\t\targs = args[1:]\n\t\tif args in action.args_registry:\n\t\t\t# TODO: Raise error if action is already handled by argparse.  We must\n\t\t\t# add this as well if it's"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\t\n\t# TODO: maybe a special way to use some more specific parameters from argparse?\n\t# TODO: figure out a \"global\" namespace for this?\n\toptions = {}\n\targs_dict = {}\n\toptions[\"repository_file\"] = args_dict[\"repository\"]\n\toptions[\"repository\"] = unparsed_arguments[0].repository\n\tfor arg in unparsed_arguments[1:]:\n\t\toptions[arg.name] = arg\n\n\t# TODO: this may need to be moved somewhere else (need to remove -C)\n\t# print \"global: \", options\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\targs_dict = {}\n\t\n\tfor subparser in subparsers.choices:\n\t\targs_dict[subparser.dest] = {}\n\t\targs_dict[subparser.dest][\"dest\"] = subparser.metavar\n\n\twhile unparsed_arguments and not args_dict[args_dict[\"subparser\"]] and not \\\n\t\t\t(unparsed_arguments and len(unparsed_arguments) and\n\t\t\tnot args_dict[args_dict[\"subparser\"]] in unparsed_arguments):\n\t\tparsed_arguments.append(args_dict[\"subparser\"](next[1:], next["}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\targs = super(ArgumentParser,\n\t\t\tunparsed_arguments[:-2] + argparse.REMAINDER)\n\t\n\tparsed_arguments = {}\n\tfor k, v in vars(unparsed_arguments).iteritems():\n\t\tif isinstance(v, argparse._SubParsersAction):\n\t\t\tparsed_arguments[k] = v.parse_args()\t\n\tkwargs = vars(unparsed_arguments)\n\t\n\treturn args, kwargs\n\n\nclass ArgumentParserMeta(type):\n\tdef __new__(cls, name, bases, dct):\n\t\treturn super(ArgumentParserMeta, cls).__new"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tfor item in sorted(unparsed_arguments.items()):\n\t\tyield item\n\n# pylint: disable=too-many-lines,too-many-branches\ndef find_args(subparser):\n\t\"\"\"\n\tReturn an argparser containing the argument in a subparser without importing an existing\nsubparser.\n\t\n\tThe argparser can be specified in two ways, either the ArgumentParser or an\nArgumentParser instance, in which case the argument \"subparser\" is ignored. As such,\nthe'subparser' argument is expected to provide the subparser's namespace to search for\nsubparser names"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tdef get_subparsers():\n\t\t\t# We want all subparser names to be the name of the subparser, so let's\n\t\t\t# handle them specially because subparser names can also\n            # be split on \".\"\n\t\t\tsubparser_name = \"repository\"\n\t\t\tfor key in dir(cls):\n\t\t\t\ta = cls.__dict__[key]\n\t\t\t\tif isinstance(a, dict):\n\t\t\t\t\tkey = key.replace('_', '-')\n\t\t\t\t\tif key not in subparser_name and key!= \"subparsers\":\n\t\t\t\t\t\tsubparsers.append(key)\n\t\treturn {\n\t\t\t\"dest"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\t# For convenience, pass options to the subparser's parser with their own\n\t\t# namespace.\n\t\toptions = {}\n\t\tfor subparser in subparsers:\n\t\t\toptions_dict = dict(subparser._parse_args())\n\t\t\toptions.update(options_dict)\n\t\treturn (options, unparsed_arguments)\n\n\treturn map(SubCommandAction, subparsers)\n\n\nclass SubCommandAction(argparse._SubParsersAction, argparse._SubParsersActionDetails):\n\tdef __init__(self, name, options, help, parents, formatter_class, **attrs):\n\t\t"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, description='''\n\tConverts a file in PXPS format into a dictionary.''', epilog='''\n\tThe default values for options can be provided as a string or filename.\n\t''')\n\n\tparser.add_argument('-e','--encoding', help=\"Input file encoding.\",required=True)\n\n\tparser.add_argument('-v','--verbose', help=\"Increase output verbosity.\",action='count')\n\n\treturn parser.parse_args()\n\n\n# main program\n# \nif __name__ == \"__main__\":\n\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\treturn {\n\t\t'make_parser': make_parser,\n\t\t'parser': parser\n\t}\n\ndef run_make_parser(args):\n\t\"\"\"\n\tBuild the argument parser, parse its arguments and return it.\n\t\"\"\"\n\n\tparser2 = make_parser(args.make_parser)\n\n\treturn (parser2.arguments, parser2.kwargs)\n\ndef run_parser_from_args(args):\n\t\"\"\"\n\tBuild a top-level parser from the given args:\n\t- args: [string] The command line arguments.\n\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\ndef parse_args(args):\n    \"\"\"\n    Build commandline and arguments.\n    @param args: a string that equals 'git log -- --date=relative',\n    @type args: str or list\n    @returns argparse.Namespace containing the command's options.\n    \"\"\"\n\n    # Allow this command to be passed in when a -v was given.\n    if len(args) == 0:\t\t# Check for help.\n        cmd_args = parser.parse_args([\"git\", \"--version\"])\n    else:\n        cmd_args = parser.parse_args(args)\n    \n    return cmd_args\n\ndef"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\ndef show_help_message(help_message=None):\n\t\"\"\"\n\tPrint usage information to stderr and exit.\t\n\t\"\"\"\n\tdef show_usage(command=None):\n\t\t\"\"\"\n\t\tPrint help information to stderr and exit.\n\t\t\"\"\"\n\t\tif help_message:\n\t\t\thelp_message()\n\t\tsys.exit(2)\n\tdef show_epilog(command=None):\n\t\t\"\"\"\n\t\tPrint help information to stderr and exit.\n\t\t\"\"\"\n\t\tif help_message:\n\t\t\thelp_message()\n\t\tsys.exit(0)\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = ap.ArgumentParser(\n\t\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t\tdescription='Script used to help building documentation using the Wiktionary API.',\n\t\t\tepilog=\"This is the help for building, writing, and running the API.\",\n\t\t\tformatter_class=argparse.RawTextHelpFormatter)\n\n\t\t# Command line arguments\n\t\tgroup = parser.add_mutually_exclusive_group()\n\t\tgroup.add_argument('-i', '--input', '-i', help=\"The input directory or directory to search in\", dest='searchPaths', action=\"store\", required=False)\n\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description='Create an Ansible inventory file based on Ansible version.')\n\tparser.set_defaults(action=command, help='Generates inventory based on Ansible version.')\n\t# The way nose places a plugin's setup.py module.\n\tmodule = sys.modules[__name__]\n\tfor key, klass in inspect.getmembers(module):  # pylint: disable=deprecated - this part is only required for the controller class\n\t if hasattr(klass, '__setup_hook__'):\n\t\t\tif klass.__setup_hook__.im_func!= module.__setup_hook__:\n#"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\nclass ParseAll(object):\t# TODO: Support parsing a dict, too.\n\tdef __init__(self):\n\t\timport sys\n\t\tfrom os.path import getsize\n\t\tself.parser = argparse.ArgumentParser(\n\t\t\tprog='test',\n\t\t\tdescription=\"Parse command line arguments.\",\n\t\t\tadd_help=True)\n\t\tself.parser.add_argument(\"-t\", \"--type\", metavar='TYPE', default='file',\n\t\t\t\thelp=\"type of file to open.\")\n\t\tself.parser.add_argument('-h', '--help',\n\t\t\t\taction='help',\n\t\t\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = make_parser()\n\t\treturn parser, subparsers\n\n\ndef run_as_shell(parsed_args):\n\t\"\"\"\n\tActually execute this when the program is started, optionally via the run_interactive\n\toption of the user and/or the default arguments.\n\t\"\"\"\n\n\tconfig = Config()\n\n\t# Check for a shell or the user has been given that makes sense\n\t# to run an external command\n\tif parsed_args.run_interactive:\n\t\tif parsed_args.shell =='shell' and not os.access(os.path.join("}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\t#\t\n\t# Create parser and parse the CLI\n\t#\n\targs = my_argparse()\n\targs.update(vars(args))\n\n\targ_name = 'image_type'\n\tparser = argparse.ArgumentParser(description='Create a dataset for OpenCV using the OpenCV library.' \\\n\t\t\t\t\t\t\t  ' It includes image files, labels, sequences and data, timings. The files to read are the dataset in JSON format. '\n\t\t\t\t\t\t\t\t\t\t  ' Each image file is a serialized dataset as an object with attributes, that can be accessed from other tools' )\n\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(usage='usage: %prog add_config')\n\t\tparser.add_argument('command', metavar='command', nargs='?')\n\t\tsub_parsers = parser.add_subparsers(help='sub commands', dest='command')\n\t\tfor name, command in COMMANDS:\n\t\t\tparser_add_config(sub_parsers, name, command)\n\t\tparser.set_defaults(func=main)\n\t\treturn parser\n\ndef add_config(subparsers):\n\t\"\"\"\n\tAdd or modify a parser and then execute it.\n\t\"\"\"\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\n\tgenerator = PageGenerator(response)\t\n\tgenerator.set_page_number(0)\t\t\n\n\tresult_ids = page['result_ids']\n\t\n\tif max_results>0:\n\t\titerator = page.get('page_iterator')\n\t\tif iterator is None:\n\t\t\tpage['result_ids'] = result_ids = []\n\t\telse:\n\t\t\tpage['result_ids'] = result_ids = [p['id'] for p in iterator]\n\n\tfor page_result in response:\n\t\tresult = serialize_wapi(page_result)\n\t\t"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\n\tpage = 1\n\tfor r in response:\n\t\tpage *= r\n\t\tif len(page) >= max_results:\n\t\t\tbreak\n\telse:\n\t\tpage = 0\n\t\t\n\tyield page\n\ndef get_object_link(response, page, max_results):\n\t\"\"\"\n\tReturns WAPI response page by page (first page by page)\n\n\tArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n\t\"\"\"\n\t\n\tpage = page + 1\n\t\n\tlink = '<link rel"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n        for page in response:\n                yield page\n\t\ndef paged(params, max_results):\n    \"\"\"\n    Returns WAPI pagination api query.\n\n    Args:\n        params ([dict]): API parameters.\n        max_results (int): Maximum number of objects to be returned in one page.\n\n    Returns:\n        dict: API results, status code, and response body.\n    \"\"\"\n\n    url_base = 'https://api.wunderground.com/api/'\n    url_parts = {\n        'limit': 'None',\n        'offset': '0',\n        'client_id': 'f7fd84d"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\n\turl = request.url\n\ttry:\n\t\n\t\turl+=\"?page=\"+str(response[0].object_count())\n\n\texcept Exception, e:\n\t\tprint \"ERROR:\"+str(e)\n\t\treturn \"ERROR: \"+url\n\n\n\trequest_url = WAPI_ROOT + url.strip().strip(\"/\")\n\n\t# request_url=response[0].url\n\t# request_url=\"http://127.0.0.1:8888/server?root=\" + str(request.url)\n\n\n\tfor obj in response:\n\n\t\t\n\t\tif \"name\""}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    while True:\n        try:        \n            items = response.findAll(\"div\", attrs={\"class\": \"listing-wrap\"})         \n            for item_wrapper in items:\n\t\turl = {}\n\t\t    \n            name = item_wrapper.h4.text\n            \n            # Extract and add the URL to the object\n            url[\"url\"] = url[\"url\"].replace(\"<div\", \"paging-data-url\")\n            url[\"name\"] = name.replace(\"<div\", \"paging-data-name\").replace(\"<p><b>\", \"\").replace(\"</b><br/>\", \"\")\n            url[\"url\"] = url[\""}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\n\tresults = []\n\n\tfor obj in response:\n\t\tresults.append(wapi(obj))\n\n\t# return results\n\tresultSet = paging(results, max_results)\n\n\t\n\treturn resultSet#! /usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tfor item in response:\n\t\tyield paging(item, max_results)\n\n\ndef get_response_json(response_json: str) -> None:\n\t\"\"\"\n\tReturn JSON data from response_json.\n\t\n\tArgs:\n    response_json (str): Response JSON from WAPI.\n\t\"\"\"\n\treturn convert_object_to_dict(convert_object_to_json(response_json))from __future__ import unicode_literals\n\nimport hashlib\nimport os\nimport shutil\n\nfrom django.core.exceptions import ValidationError\nfrom django.db import transaction"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    while (response is not None and\n           max_results < 0 or\n           len(response['items']) < max_results):\t\t# skip if items exhausted\n            endrange = 0 # start of next loop\n            for each_item in response['items'][endrange:endrange + 500]: # get next page\n                yield each_item\n                response = each_item['next']\n                endrange += 500\n\n\n\ndef get_obj_type(name, parent, parent_id):\n    \"\"\"\n    Method for object getting types, which also retrieves the correct object type by parent:\n\n    :param name: name"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    while True:\n\t\tdata = response.json()['data']\n\t\t\t\n\t\tresults = _parse(data['list']['data'], max_results)\n\n\t\tfor item in results['response']:\n\t\t\tyield item\n\n\t\t# Note, we are returning a list in HTML format, \n\t\t# but a new list is returned\n\t\tfor item in data['list']['data']:\n\t\t\tyield item\n\n\n\n'''\n\tThis helper functions are not useful, they must be created for each\n\tpage so we can do all of them.\n'''\n\ndef _parse(data, max_results=None):"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    paging_response = WapiPagedResult(response)\n    for paging_result in paging_response.result['item_list']:\n\tres = {\n\t    'data': paging_result.get('data', None),\n\t   'max_results': paging_result.get('max_results', 0),\n\t    'next': paging_result.get('next', None)\n\t    }\n    return paging_response\nfrom django.db import models\nfrom django.conf import settings\nfrom django.utils.translation import ugettext_lazy as _\nfrom taggit.managers import"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\ttry:\n\t\t\tresult = int(size)\n\t\t\tsize = int(round(float(size) / 1024.0,1))\n\t\texcept:\n\t\t\tresult = float(size)\n\t\treturn result\n\t\t\n\n\t@staticmethod\n\tdef natural_key(value, level = 1):\t\t\"\"\"\n\tConvert human readable file size to bytes.\n\n\tArgs:\n        value: A string representing a human readable file size (eg: '500K')\n        level: Level to use on the output string. 0: Errors, 1: Warnings, 2: Informational (only in debug mode"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tfrom types import Types\n\t\treturn float(size) / 1000\n\n\tsize = size.strip()\n\tsizes = {\"KB\": 1, \"MB\" : 1048576}\n\tfor size in sizes:\n\t\tif size in sizes:\n\t\t\treturn size\n\n\traise Exception(\"invalid memory size\")\n\n\ndef get_file_size(path: AnyStr, default: int = 1024 * 1024):\n\t\"\"\"\n\tGet file size in bytes (eg: \"100MB\")\n\n\tArgs:\n\t    path(str):\n\t    default: Integer or float, maximum bytes to fetch\n\t\"\"\""}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\ti = 0\n\t\twhile i < len(size)-1:\n\t\t\tif not sys.platform == \"cli\":\n\t\t\t\ti = i*2 # Pad it to next integer to account for different results\n\t\t\tif not re.match(\"[a-z]:[0-9]+\", size[i]):\n\t\t\t\treturn i\n\t\t\tsize = size[i:]\n\t\treturn float(size)\n\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\tMain entry point after the main loop\n\t\"\"\"\n\ttry:\n\t\ta = int(raw_input(\"Enter the size"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t           \n        integer_part, decimal_part = size_part.split(\".\")\n        if integer_part[0] == \"k\": \n\t    return int(decimal_part)\n        else:\n\t    return int(decimal.Decimal(integer_part))\n    \n\ndef size_to_size_tuple(size: str) -> int\n\t\"\"\"\n\tConvert human readable file size to tuple.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    Tuple containing"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\ttry:\n\t\t\tif size.startswith(\"k\"):\t\t\t\t\t\t\t# int\n\t\t\t\tsize = size[2:]\t\t\t\t# [k] long\n\t\t\telif size.endswith(\"b\"):\t\t\t\t\t\t\t# int\n\t\t\t\tsize = size[1:-1]\t\t\t\t\t# [Mb] long\n\t\t\telse:\n\t\t\t\traise RuntimeError(\"invalid file size specification\")\n\t\texcept ValueError:\n\t\t\traise RuntimeError(\"invalid size specification\")\n\n\t\tif len(size)!= 3:\n\t\t\traise RuntimeError(\"invalid file size specification\")\n\n\t\treturn float(size)\n# coding: utf-8\n\"\"\""}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\t# Convert a number to bytes, if possible\n\t\tif \"G\" in size:\n\t\t\tsize = size.split(\"G\")\n\t\tif \"-\" in size:\n\t\t\tsize = \"\".join(size.split(\"-\"))\n\t\tif size[-1].isdigit():\n\t\t\tsize = size[:-1]\n\n\t\ttry:\n\t\t\tsize = float(size)\n\t\texcept ValueError:\n\t\t\traise ValueError(\"File size \\\"%s\\\" can't be converted to human readable\"%size)\t\n\n\t\treturn size\n\n\texcept TypeError:\n\t\traise ValueError(\"File size \\\"%s\\\" must be a non-positive"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n    try:\n\tsize = float(size)\n\tn = size\n\tif not n:\n\t    return 0\n\tfrac_size = 1000000000.0 / size\n\tfor i in range(3, int(math.log10(frac_size) + 1)):\n\t    n = float(n) + n / frac_size * i\n\t    if not size % 1024:\n\t\treturn \"%3g%s\" % (frac_size, size / (1024.0 ** i))\n\treturn int(size)\n\treturn size\n\t## Size is not in a percentage, and"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t    \tsize = size.rstrip(\"K\")\n\treturn int(size[:-2])*1024*1024\n\n\n\n\n\n\ndef size_to_size_in_si(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to size as 'bytes'.\n\n\tArgs:\n        size: An integer representing a human readable file size (eg: '500K')\n\n\tReturns:\n\t\tLength of file in bytes\n\n\tExamples::\n\n\n                \nConventional\n\n    :py:func:`~size_to_bytes` will round to the nearest whole multiple of 1024 characters.\n    \"\"\"\n        size"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n\n\ndef format_size(size: str) -> str:\n\t\"Format a file size into a human readable file size (eg: kilomegabytes)\"\n\ttry:\n\t\tsize = size_to_bytes(size)\n\texcept Exception as e:\n\t\tlogger.error(\n\t\t\t\"Wrong size value: \"\n\t\t\t\"Expected size. It has to be a string \"\n\t\t\t\"representation: %s\" % str(type(size))\n\t\t)\n\t\tlogger.exception(e)\n\t\tlogger.error(\n\t\t\t\"Please check the size before attempting \"\n\t\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn int(size)\n\telif size == \"1\":\n\t\treturn str(size)\n\telif re.match(\"^(pt|us)$\", size, re.IGNORECASE):\n\t\treturn decimal.Decimal(int(float(size)))\n\telif re.match(\"^(pt|us)$', size, re.IGNORECASE):\n\t\tsize = \"%\" + self.fmt[0] + size + \"f\"\n\t\treturn decimal.Decimal(int(float(size))/1000.0)\n\telif re.match(\"^[A-Za-z]$\", size, re.IGNORECASE):"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\n\tif len(dicts) == 1:\t# allow for single-element dicts\n\t\treturn dicts[0]\n\n\telif len(dicts) > 1 and not isinstance(dicts[1], dict):\n\t\traise ValueError(\"only 1 level dicts can be combined\")\n\n\telse:\n\t    # first perform one-element level depth if possible\n\tresult = {}\n\tfor k, v in itertools.groupby(sorted(dicts, key=operator.itemgetter('a')), key=operator.itemgetter('b'), reverse=True):\n\t    for d in v:\n\t\tresult[d[k]] = result"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tn = 0\n\t\tfor d in dicts: n += d.values()\n\t\treturn dict(n)\n\treturn _dictsum(d.values())\n\ndef _dictmax(dicts):\t\"\"\"\n\tMaxiumum values of the dictionaries supplied by iterable dicts.\n\n>>> _dictmax([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tn = 0\n\t\tfor d in dicts: n += d.values()\n\t\treturn dict(max(n))/n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\t\n\t# Make a copy of the combined argument\n\tif len(dicts) > 0:\n\t    return (reduce(operator.add, [dict1, dict2]) for tupleList in dicts)\n\telse:\n\t    result = {}\n\t    for resultKey, resultVal in d.iteritems():\n\t\tresult = merge(result, resultVal)\n\t\n\t    return result\n\t\n\t# Make new argument based on a combination of its arguments\n\treturn _dictsum(dict1.items() + dict2.items())\n\n\ndef average(values, weights=None):\n    \"\"\"\n\tCalculate the arithmetic mean of"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\ndef _dictsort(d):\n\t\"\"\"\n\tSort a given dictionary by value. This is sort dictionary (with equal\n\tnumbers). It is used for sorting lists, so the sorted() method\n\tis still used to sort a list of tuples containing key-value pairs.\n\t\"\"\"\n\t# Use two exceptions to determine the length of the shortest list\n\t# and the desired maximum length.\n\ttry:\n\t\treturn sorted(d.items(), key = lambda elem: elem[1], reverse = True)\n\texcept IndexError:\n\t\treturn list(reversed(d.items()))"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn reduce(add, dicts, {})\n\n\tdef sumTo(self, name):\t# TODO fix all the'set' functions for this function...\n\t\tself.d = name\n\tdef sumToDict(self):\t# this function is mostly copied from the list in which we are writing to the dict for the sake of use\n\t\treturn self.d\n\t#def sumToSet(self, other):\n\t#\tself.d = dict((k, sum(self[k])) for k in other)\n\t#\treturn self\n\ndef add(alist):\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn sum(d.items() for d in dicts)\n\n\tdef _get_first(k, default=None):\n\t\tif k in _defaults:\n\t\t\treturn _defaults[k]\n\t\treturn default\n\tif _defaults is None:\n\t\t_defaults = _get_defaults()\n\tout = {}\n\tfor d in dicts:\n\t\tout.update(_dictsum(d))\n\treturn out\n\ndef _merge(orig, result):\n\tfor k, v in orig.items():\n\t\tif k in result:\n\t\t\tif isinstance(v, dict"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tfor d in dicts:\n\t\t\tyield tuple(sorted(d.items()))\n\treturn dict(itertools.chain(*dicts))\n\ndef _unique(xs):\n\t\"\"\"\n\tBuild the list of unique elements in the iterable xs\n\tin order using a tuple to ensure that duplicates are\n\trepresented as hashable. This is in contrast to\n\titer(x), but for use with a list() called the same number of\n\telements could be used in a pairwise.\n\t\"\"\"\n\tseen = {x: x() for x in xs}\n\tseen_add = seen."}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tresult = 0\n\t\tfor d in dicts:\n\t\t\tresult = result + int(d['c'])\n\t\treturn result\n\n\tdef count(self, n=1):\n\t\t\"\"\"\n\tCalculate a counting of the unique values of n items from an iterator.\n\n>>> _sum([{'a': 1, 'b': 1}, {'a': 2, 'b': 4}, {'b': 7, 'c': 1}], 3)\n{'b': 2, 'c': 4}\n\n\"\"\"\n\t\tif n is None:\n\t\t\titems = map(self)\n\t\t\treturn reduce"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn sum(_dictsum(d)\n\t\t\t\tfor d in dicts.itervalues()\n\t\t\t\tif isinstance(d, _six.string_types))\n\n\tdef add(self, d):\t# pragma: no cover\n\t\tself.update(d)\n\t\treturn self\n\n\tdef update(self, iterable):\t# pragma: no cover\n\t\t\"\"\"\n\tUpdate the specified values in the given iterable.\n\n\t>>> _dictsum({'a': {'b': 2}, 'a': 2}, {'a': 5, 'b': 0})\n\t{'a': 6}\t\"\"\"\n\t\t"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\n\n# In Python 2.6+, the reduce() builtin from functools was\n# slightly different from reduce() in that it was inspired by the\n# Python 2.5 stdlib, which is also available in the Python documentation that\n# explicitly available in that version.  For this reason we need to copy from\n# functools.py because this should be fair:\n# http://mail.python.org/pipermail/python-list/2009-September/405973.html\ndef _reduce(func, iterable, initializer=None):\n\t\"\"\"\n\tUse a closure to combine"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor i, arg in enumerate(url_args):\n\t\t\targ = re.sub(args[i], arg, url_args[i])\n\t\treturn url.replace(str(arg))\n\n\tdef _convert_dict_to_url(self, d):\n\t\td = d.copy()\n\t\td['params'] = dict()\n\t\tparams = d['params'].copy()\n\t\tparams['page'] = self._page\n\t\tparams['count'] = self._count\n\t\tparams['sort'] = self._sort\n\t\tparams['q'] = self._q\n\t\t"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tparts = []\n\t\tfor arg in url_args:\n\t\t\tif isinstance(arg, dict):\n\t\t\t\tparts = replace_url_args(arg, parts)\n\t\t\telif isinstance(arg, list):\n\t\t\t\tparts.extend(arg)\n\t\t\telse: # assume an arg is dict, nothing else\n\t\t\t\tparts.append(arg)\n\t\treturn urlunsplit(parts)\n\n\tdef _parse_response(self, message):\t\n\t\t\"\"\"\n\t\tTries to parse the response text to make sure it is a valid JSON\n\t\tmessage. If not, returns an error message. In"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor k in url_args:\n\t\t\tvalue = url_args[k]\n\t\t\t# ignore None type items\n\t\t\tif value is not None:\n\t\t\t\t# do not replace empty values\n\t\t\t\tif value:\n\t\t\t\t\tcontinue\n\n\t\t# replace values in args\n\t\tfor key, value in url_args.items():\n\t\t\tif value is not None:\n\t\t\t\tvalue = _replace_url_args(value, key)\n\n\t\t# return\n\t\turl_args = url_args[:-1]\t# remove trailing /\n\t\t# it was already escaped with % -\n\t\treturn urlparse."}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif not isinstance(url_args, (list)):\n\t\t\turl_args = url_args.decode('utf-8')\n\n\t\tfor a_arg in url_args:\n\t\t\tif isinstance(a_arg, (unicode, str)):\n\t\t\t\ta_arg = a_arg.encode('utf-8')\n\t\t\t\tif not a_arg:\n\t\t\t\t\t# String should be encoded as utf-8\n\t\t\t\t\tcontinue\n\t\t\tfor p in a_arg.split('&'):\n\t\t\t\tp = p.strip()\n\t\t\t\tif p:\n\t\t\t\t\tp = p.encode('utf-8')"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif isinstance(url, str):\n\t\tif url_args is None or isinstance(url_args, str):\n\t\t\treturn url\n\t\telse:\n\t\t\treturn url.format(*url_args)\n\treturn url\n\ndef _split_url(url):\n\t\"\"\"\n\tGet parts of an URL into tuple\n\t\"\"\"\n\tparts = re.split(\"(?:(?<=[^\\w]|(?<=\\s)))\", url, 1)\n\tif len(parts) > 1:\n\t\treturn parts\n\telse:\n\t\treturn (None, None)\n\ndef _"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor (newkey, newvalues) in url_args.items():\n\t\t\treplacement = url_args[newkey]\n\t\t\treplacement = _replace_urlarg(replacement, newvalues)\n\t\treturn replacement\n\turl_args = url_arg_re.match(url).groupdict()\n\turl_args_expanded = {}\n\tfor item in url_args:\n\t\tnewkey = _replace_url_arg(item, url_args_expanded)\n\t\tnewvalues = _replace_urlarg(item, url_args[newkey])\n\t\tif newvalues!="}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tresult = \"\"\n\t\tfor arg in url_args.items():\n\t\t\tif isinstance(arg, str):\n\t\t\t\tvalue = arg.replace(arg[0], arg[1])\n\t\t\t\turl_args[arg[0]] = value\n\t\t\t\tcontinue\n\t\t\tif isinstance(arg, datetime.datetime):\n\t\t\t\tvalue = arg.strftime(\"%Y-%m-%d %H:%M:%S\")\t\n\t\t\telif isinstance(arg, FileStorage) or\\\n\t\t\t\t\tisinstance(arg, (tuple, list)):\n\t\t\t\tvalue = arg.replace('%S3Object/', S3Uri(url))"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor key, val in url_args.items():\n\t\t\turl = url.replace(key, val)\n\n\treturn url\n\ndef _fix_url_args(url, url_args):\n\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor key, val in url_args.items():\n\t\turl = url.replace(key, val)\n\treturn url\n\n\ndef _fix_url_args2(url_args):\n\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor key, val"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif not isinstance(url_args, (list, tuple)):\n\t\t\t# Encode unicode values in a manner that will be \n\t\t\t# stored in URLArgs and inserted into the query.\n\t\t\tfor idx, item in enumerate(url_args):\n\t\t\t\tnewUrl = url.clone(query_args[idx])\n\t\t\t\tif isinstance(newUrl.params, (list, tuple)):\n\t\t\t\t\tnewUrl.params[0] = self._urlquote_unicode(newUrl.params)\n\t\t\t\tnewUrl.params[1] = self._urlquote_unicode(newUrl.params,\n\t\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\t# Don't do any URL's escaping by stripping out the key value since it's\n\t\t# used for sending the whole message\n\t\tif isinstance(url_args, str):\n\t\t\turl_args = url_args.replace(\"%\", \"%25\")\n\t\telse:\n\t\t\turl_args = url_args.replace(\"%\", \"%22\")\n\t\tif \"%\" in url_args:\n\t\t\turl_args = url_args.replace(\"%\", \"%24\")\n\treturn url % url_args\n\ndef _build_fetch_url(base_url, parameters):\n\t\"\"\""}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn False\n\n\ndef is_bool(val: any, convert: bool) -> int:\t\"\"\"\n\tTry to convert string representation of given boolean value to a bool.\n\tReturn the number such that val == True OR false if val == False and\n\tthe value to convert into 1.\n\tConvert from 0 or 1.\n\t@param val: a string\n\t@param convert: C{True} to convert to 0 or 1\n\t@return: a boolean value\n\t\"\"\"\n\ttry:\n\t\tval = int(val)\n\texcept:\n\t\terr_msg"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str) and val is None or len(val) == 0\n\n\t@staticmethod\n\tdef typeless_string_value(val: Any) -> str:\n\t\t\"\"\"\n\t\tConvert an arbitrary value type to a string value.\n\t\t\"\"\"\n\t\treturn str(val)\n\n\t@staticmethod\n\tdef typeless_string_list(val: list) -> str:\n\t\t\"\"\"\n\t\tConvert a list of string values to a string value.\n\t\t\"\"\"\n\t\treturn [str(val_type) for val_type in val]\n\n\t@staticmethod\n\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str) and VALID_STRING.match(val)\n\n\tdef is_notblank_string(val: any) -> bool:\n\t\t\"\"\"Return whether a string represents a non-None value.\"\"\"\n\t\treturn isinstance(val, str) or not val\n\n\tdef is_empty_string(val: any) -> bool:\n\t\t\"\"\"\n\t\tCheck if a string represents a empty value.\n\t\t\"\"\"\n\t\treturn isinstance(val, str) or val.strip() == \"\" or len(val) == 0 or not val\n\n\tdef is_empty_int("}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\t# a string is non-None if it is True or empty\n        if is_none_list(val) is False:\n            # so the string should be None except as None is just valid.\n\t\t\treturn False\n        return True\n\n\t# Strings cannot be None\n\t# So use empty string or ''\n\n\treturn not (is_none_str(val) or val) or False\n\ndef is_none_list(val: Sequence) -> bool:\n\t\"\"\"\n\tCheck if a value represents a none list; if it is\n\tan iterator an append an element. \"\"\"\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif not isinstance(val, six.string_types):\n\t\t\treturn False\n\n\t\tdef value_to_validate(v):\n\t\t\treturn bool(v) or (len(v) >= 1 and v[0] == '$'\n\t\t\t\t\t\t\tand v[1] in INVALID_CHARS)\n\n\t\treturn value_to_validate\n\n\treturn is_none_string\n\nclass TestConfig(Config):\n\tdef __init__(self):\n\t\ttype: Type['Config']\n\t\tself._settings: SettingsDict[str, str] = {}\n\t\tself._runtime_checks: Dict[str"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn val in {'None', 'not set'}\n\ndef is_str_list(val: str) -> bool:\t\"\"\"\n\tCheck if a string's items are strings, not other types.\t\"\"\"\n\treturn isinstance(val, str)\n\n\ndef safe_get_values_for_key_value(keys: Iterable[Any], values: Iterable[Any]):\n\t\"\"\"\n\tChecks a value for keys in the keys and values, and returns those.\n\n\tArgs:\n\t\tkeys: The list containing the keys and values.\n\t\tvalues: The list containing the values from"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\t# this was made so that bool(val) == False, is_none_string() with a non-empty string literal can have any value.\n\t\t# So, we have to consider the 'true' value here.\n\t\treturn (val is False)\n\telif isinstance(val, (str, int, float, complex, tuple, list, dict)):\n\t# see issue 1094.  I assume it's True at the very least it's not False at the very least.\n\t\treturn bool(val)\n\telif isinstance(val, Decimal):\n\t# see issue 1094"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str)\n\n\ndef is_string_type(val: Any) -> bool:\t\t\t\t\t\t\t\t\"\"\"\n\tCheck if a string is of an acceptable type.\t\"\"\"\n\treturn isinstance(val, type(None))\n\n\ndef is_string_float(val: Any) -> bool:\t\t\t\t\t\t\t\"\"\"\t\t\n\tCheck if a string is a float.\n\t\n\tThe type checking and conversion is done via the check method,\n\tso this checks that the string is a value of type float,\n\tand that it returns None on error.\t\"\"\"\n\tis"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn val is not None\n\n\tdef to_str(self) -> str:\n\t\t\"\"\"\n\t\tConvert the value to a string.\tThe value should be of type ``None``,\n\t\t``int``, ``float``, ``string``, etc.\n\t\t\"\"\"\n\t\treturn str(self.value)\n\n\tdef __bool__(self) -> bool:\n\t\t\"\"\"\n\t\tConsolidate the value to a bool so that if you are adding to\n\t\ta bool type that this method, it should be `True` (or `False`)\n\t\tor `False` (or whatever value is specified"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tpass\n\n\nclass Test:\n\tx: Any\n\n\tdef write(self, x: str, y: float = None) -> None:\n\t\t\"\"\"\n\t\tWrite an example to the console as an Excel spreadsheet.\n\n\t\t@param x: An x coordinate. This will be coerced beforehand, so don't\n\t                  write quotes around it.\n\t\t@type x: float or unicode\n\t\t@param y: An y coordinate. This will be coerced beforehand, so don't\n\t                  write quotes around it.\n\t\t@type y: float or None\n\t\t\"\"\"  # pragma"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n    flags = [ 'quiet','verbose', 'help' ]\n    opt_fmt = optparse.IndentedHelpFormatter(max_help_position=20, width=80, indent_increment=10, min_help_position=1, width_chars=68, width_words=len(flags))\n\n    cfg = optparse.OptionParser(description='Test some bits of the various features of the Cassandra client')\n    opts, args = parser.parse_args()\n    global VERBOSITY_LEVELS\n    VERBOSITY_LEVELS = opts.verbosity\n    VERBOSITY_LE"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\ttry:\n\t\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \"+str(__VERSION__))\n\t\tparser.add_argument(\"--force-generate\", nargs=\"?\", help=\"Force creation of.mdl files\")\n\texcept TypeError:\n\t\tparser.add_argument(\"--force-generate\",\n\t\t\thelp=\"Force creation of.mdl files\")\n\texcept IndexError:\n\t\tparser.add_argument(\"--force-generate\",\n\t\t\thelp=\"Force creation of.mdl files\")\n\t\t\t\t\n\treturn parser\n# END parser_flags\n\nif __name__"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn '|'.join(parser.option_strings)\n\ndef run_tests(arguments):\n\tif arguments.debug_setup:\n\t\tglobal PRINTDIR, RESULT_DIR\n\t\tprint('Running test suite:')\n\t\tfor dirname in sorted(os.listdir(TEST_DIR)):\n\t\t\tif dirname == 'testresult':\n\t\t\t\tcontinue\n\t\t\tif dirname == 'test_perf':\n\t\t\t\tcontinue\n\t\t\tif not os.path.isdir(dirname):\n\t\t\t\t# Not a directory - nothing to do\n\t\t\t\tcontinue\n\t\t\tif not os.path.isfile(dirname):\n\t\t\t\tcontinue\n\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tflags = []\n\t\tfor flag in parser._int_options:\n\t\t\tflags.append(str(flag))\n\t\tfor flag in flags:\n\t\t\tflags.append(flag+\"\\n\")\n\t\tflags.remove('=')\n\t\treturn ''.join(flags)\n\t\t\n\tdef get_arg_parser(self):\n\t\tfor flag in self._flag:\n\t\t\tflag = flag.lower()\n\t\t\tflag = flag[0]\n\t\t\tval = getattr(self, flag)\n\t\t\t\n\t\t\tif val:\n\t\t\t\treturn flag.lower()\n\t\t\t\t\t\n\t\traise ValueError(\"Could not find flag '%s' in"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument('-v','--version',\n\t\t\taction='store_true',\n\t\t\thelp=\"print the program version\")\n\t\tparser.add_argument('-y', '--yes', nargs='*', default=False,\n            help=\"print the program action before exit\")\n\t\ttry:    parser.add_argument('-p', '--password', action='store_true',\n\t\t\thelp=\"print the program's password\")\n\t\texcept: pass\n\treturn parser.parse_args()\n\nif __name__ == '__main__':\n\targs =  parse_args()\n\tif args."}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tif len(parser.prog) == 0 or len(parser.prefix):\n\t\treturn []\n\telse:\n\t\tfor name, _, desc in zip(parser.prefix, parser.desc):\n\t\t\tif len(name) == 0: continue\n\t\t\ttry:\n\t\t\t\t# the arg can be separated by space or a single space\n\t\t\t\tnargs = None\n\t\t\t\tif isinstance(desc, argparse.Action):\n\t\t\t\t\tnargs = 0\n\t\t\t\telif isinstance(desc, argparse._AppendAction):\n\t\t\t\t\tnargs = -1\n\t\t\t\telse:\n\t\t\t\t\tassert hasattr(desc, '__call__')\n\t\t\t\t\tnargs"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t self.set_usage(usage=\"usage: %prog [options] args...\")\n\t parser.add_argument(\"-f\", \"--file\",\n\t\t                  help=\"read input from FILE\", metavar=\"FILE\")\n\t # FIXME: Implement argparser --args?\n\t parser.add_argument(\"-s\", \"--strip\",\n\t\t                  action=\"store_true\", dest=\"strip\",\n\t\t                  help=\"strip trailing whitespace from input\",\n\t\t                  default=False)\n\t parser.add_argument(\"-w\", \"--writeonce\",\n\t\t                  help=\"write output to PIPE\", dest=\"writeonce\",\n\t\t                  action=\"store_true\", dest"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn [flag for flag in parser._actions if flag.dest!= \"--no-opt\"]\n# Copyright (C) 2006 Canonical Ltd\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument(\n\t\t\"-v\", \"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%%(prog)s version \" + __version__\n\t\t)\n\t\n\t# Parse arguments\n\targs = parser.parse_args()\n\t\n\t# Return the parsed options\n\treturn args\n\n\n#===============================================================================\ndef main():\n\t\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Search and match files in a source directory for objects of interest.  The script supports searching and matching on a directory and retrieving and matching on files in the local database.',\n\t\targuments=['src'],\n\t\t\n\t\tepilog"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\t# Parse arguments\n\tparser.add_argument(\n\t\t\"filename\",\n\t\thelp = \"The input file that you are reading.\",\n\t\tdefault = None);\n\n\t# Parse the arguments\n\t(options, left) = parser.parse_args();\n\n\t# Print out the parameters we're about to stdout\n\tprint \"Options: \" + str(options)\n\tprint\n\tprint \"Left: \" + str(left)\n\tprint\n\treturn (left, options);\n\n#\n# Main: parse_args()\n# Parse flags here.\n#\ndef"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # If we start at the root, then we're likely processing all the\n    # subdirectories and do not need to process the same path again again.\n    if path_name and processed.is_dir():\n        return False\n    \n    # Allow any directory names to be excluded\n#    allowed_names = [ os.path.normcase(x) for x in [x.strip() for x in dirlist] if x not in path_name ]\n#    if not allowed_names:\n#        return False\n \n    # The set of valid directories and non-allowed\n#    invalid_chars = [ os."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # Is the requested directory a directory?\n    if not os.path.isdir(path_name):\n        # Check the directory name\n        if verbose:\n            print('Directory \"{}\" is not a directory, skip \"{}\"'.format(os.path.abspath(path_name), path_name))\n        # Do nothing since directory doesn't exist\n\treturn False\t\n\n\t# Need to check the cache of a file on disk\n\ttry:\n\t    cache_name = path_name.split(\"/\")[-2].replace(\".txt\", \"\")\n\texcept IndexError:\n\t    # Directory doesn't exist\n    \treturn False\n\n\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\t\"(Does not look like a file or directory in path_name?)\"\n\t\t# If verbose: print(\"Processing path\", path_name)\n\ti_start = processed\n\ti_end = len(processed)\n\twhile i_start < i_end:\n\t\tif path_name in processed:\n\t\t\tprocess_path(processed, path_name, i_start)\n\t\t\tprocessed.remove(path_name)\n\t\t\ti_start += 1\n\t\ti_start += 1\n\t\tif verbose: print(\".\")\n\treturn True\n\ndef process_path(path_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    new_path = path_name + os.path.sep + processed.GetName()\n    if new_path in processed: return False\n    processed.add(new_path)\n    return True\n    \ndef get_file_from_dir(dir, verbose=False):\n\t\"\"\" Return the file containing the absolute and relative path to the data. \"\"\"\n\tresult = ''\n\tif os.path.isdir(dir): result+='/'\n\ttry:\n\t\tfiles = os.listdir(dir)\n\t\tname = ''\n\t\tentry = ''\n\t\tfor f in files:\n\t\t\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n       \n#TODO: Add recursive test: does not have this function use the recursive flag\n#     @profile\n#     def recursive_check_path(path, recursive, verbose):\n#         entries = os.listdir(path)\n#         if recursive:\n#             entries = [e for e in entries if os.path.isdir(os.path.join(path, e))]\n#             if verbose:\n#                 print \"Found %d files in %s\" % (len(entries), path)\n#             return entries\n#         else:\n#             return False\n\n\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\t#print \"Proceeding: \" + path_name\n\t\treturn is_processed(processed, path_name, verbose)\n\telse:\n\t\t#print \"Proceeding: \" + path_name\n\t\t#print \"(Proceeding: \" + str(path_name) + \")\")\n\t\treturn False\n\ndef walk_dir(root_path=None, file_tree=None):\n\t\"\"\"\n\tWalk a directory of files as well as the directory of subdirectories.\n\tIf path_name is None, return only the files matching path_name\n\t@param"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if not path_name:\n\treturn not os.path.abspath(processed) and verbose # Skip empty entries\n\t\n\tif not processed in processed:\n\tprocessed.add(path_name)\n\n\tif not os.path.isdir(path_name):\n\t\treturn False\n\n\tif not path_name.startswith(os.path.sep):\n\t\treturn False\n\t\t\n\tfor entry in os.listdir(path_name):\n\t\tif entry in processed:\n\t\t\tcontinue\n\t\t\n\t\tif not name_filter(entry):\n\t\t\tcontinue\n\t\telse:\n\t\t\tif verbose:\n\t\t\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # Check the directory exists\n    dir_exists = os.path.isdir(path_name)\n    if dir_exists:\n        # Check the file\n        if verbose:\tprint(dir_path + \" is not empty\")\n        return False\n    else:\t\n        # Generate the new directory name\n        new_path = dir_name + \"/\" \n\t\n        # Check if directory already exists\n        if os.path.isdir(new_path):\n            # Generate the new directory name\n            new_path += \"/\"\n            \n        # Open the file for writing\n        fh = open(path_name, \"w\")\n\n        # Write"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if not processed:\n\t\treturn False\n\n\tparts = path.split(path_name)\n\t#print \"split pathname in %s\"%(parts)\n\t#if parts[0] == '':\n\t#\tprint \"path is empty!\"\n\t#\tprint parts[1]\n\t#else:\n\t#\tif (parts[1] == '') and (parts[3] == ''):\n\t#\t\tprint \"pattern is none!\"\n\t#\t\tprint parts[2]\n\t#\telse:\n\t#\t\tprint \"pattern is none!\"\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if os.path.relpath(path_name, '.'): # a relative path\n        print >>sys.stderr,\"debug: ignoring path\"\n        return False\n    return process_path(path_name)\n\n#############################\n#   Process File Name\n#############################\n\ndef process_file(file_name):\n        \"\"\"\n\tCheck if a file by checking for the full pathname of file_name.\n\tIf it is a file or directory, return ``True`` and report the file status.\n\t\n\tArgs:\n\t\tfile_name: Name of file\n\n\t\tReturn: List of files\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tpoints = []\n#\tfor v in vertices[0]:\n\t\tpts = []\n\t\tfor p in vertices:\n\t\t\tpts.append(v[0][0]*p[0][0]+v[0][1]*p[1][0]+v[0][2]*p[2][0])\n\t\treturn points\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tb = []\n\t\ttry:\n\t\t\tfor v in vertices:\n\t\t\t\ta,b = v[:2]\n\t\t\t\tb.extend([a+i for i in range(3)])\n\t\texcept:\n\t\t\tfor v in vertices:\n\t\t\t\tb.extend([v[:2]+i for i in range(3)])\n\t\treturn b\n\t\t\nclass OddVertex():\n\tvertices = None\n\tdef __init__(self, vertex):\n\t\tself.vertex = vertex\n\t\tself.vertices = []\n\t\tself.polyhedrons = []\n\n\tdef append(self, points"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tpolygon_v = []\n\t\tfor i in range(len(vertices)):\n\t\t\tpoint = vertices[i]\n\t\t\tif point[0] < 3: point = [point[0], point[1], point[2]]\n\t\t\ttriangle = [point[1], point[0]]\n\t\t\tif triangle not in polygon_v:\t\n\t\t\t\tpolygon_v.append(triangle)\n\t\treturn polygon_v\n\n\tdef linesetlist(self):\t\tpass\n\n\tdef vertex_length(self):\t\t\"return a list of vertex length objects\"\n\t\treturn []\n\n\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n##\t\tbend points of the polygon (if they are the same or have a unique point)\n##\t\tif len(vertices) == 2:\n##\t\t\tif (vertices[0], vertices[1]) in dab:\n##\t\t\t\treturn dab[(vertices[0], vertices[1])]\n##\t\t\telse:\n##\t\t\t\treturn None\n##\t\telse:\n##\t\t\tdab.update((vertices[0], vertices[1]))\n##\t\t\treturn dab\n\t\n\tdef vertices2dab(vertices):\t\t\"\"\"\n\treturn a list"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\treturn (list(range(vertices.first()), list(range(vertices.size()))) if (isinstance(vertices, BPolyline) and len(vertices.coeffs) == 1) else (list(range(vertices.last()), list(range(vertices.size()))) if (isinstance(vertices, ABCPolyline) and len(vertices.coeffs) == 1) else (vertices, list(range(vertices.last()), list(range(vertices.size()))) if (isinstance(vertices, ABCPolyline) and len(vertices.coeffs) == 1) else vertices)\n\t\t\n\t#return list(zip("}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tvertex_line = \"\"\n\t\n\t\tfor vertex in vertices:\n\t\t\tvertex_line += \"%d %d\\n\" % (vertex[0],vertex[2])\n\t\treturn vertex_line\t\t\n\n\n\t@staticmethod\n\tdef round4points(points):\n\t\t\"\"\"\n\tround a list of [x,y] coordinates : \n([x1,y1], [x2,y2], [x3,y3]), each point of\n\ta triangle is represented by 8 points. The first point of\n\teach triangle is represented by 13 points. So a polygon with\n\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\treturn (vertices_by_vector(vertices),)\n\ndef polygon3tuple((vertices[0], vertices[2], vertices[3]), (vertices[1], vertices[4], vertices[5])):\t\t#\n\t\"\"\"\n\treturn 3 points for each line of the polygon. This will include the polygon.\n\npolygon with vertices ABCD\nWill return\nDAB, ABCD -> returns ((0, 0), (2, 0)), ((1, 0), (3, 1)), ((2, 3), (4, 3)), ((3, 4), (5, 4))\n#A"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\n\ndef findCDiameters(A,C,D):\n\t\"\"\"\n\treturn 2-tuple of coordinates in the bounding box for vertices of A,C:\n\tA-A      B-A\n\tC-A      C-C     D-C\n\tD-D      D-B    E-D\n\tE-D    F-E     G-H\n\tG-E    G-H    A-H\n\tA-F    F-G     D-H\n\n\tNOTE: points are located at the bottom-right of the polygon, with the"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t return tuple( [ points.vertex_tuple(v) for v in vertices  ] )\n\t\nclass StraightEdge( edge.SmoothEdge ):\n\tdef __init__(self, point1, point2, point3):\n\t\tedge.SmoothEdge.__init__(self, point1, point2, point3)\n\n\tdef is_point_inside(self, point):\n\t\t\"Tells whether the point is inside the polygon.\"\n\t\treturn False\n\n\t@staticmethod\n\tdef _normalize(point):\n\t\tnormalized_point = [0, 0, 0]\n\t\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\ndef triangle3tuples(trianglevertices):\n\t\"\"\"\n\treturn 3 tuples for each triangle vertex\n\tEach tuple must be of the same length\n\tThe tuples are returned in tuple : (A,B,C)\n\npoint(x,y,z) for a point on a given polygon\nReturns:\n\tThe tuple consisting of 3 points on the polygon, the points on the given polygon and a tuple of 2 tuples.\n\t\"\"\"\n\tif (len(trianglevertices)==0):\n\t\treturn (None, None, None)\n\telif (len(trianglevertices)==1):\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tstring = \"\"  # no more significant digit than string -> string\n\t\t# get the character at the first position if any, to use as a separator\n\t\tr = list(str(number))\n\t\twhile len(r) > 0:\n\t\t\tif alphabet[0] == \"L\" or alphabet[0] == \"R\":\n\t\t\t\tr[0] = alphabet[0] + r[0] + alphabet[1]\n\t\t\t\tr[1] = alphabet[1] + r[1] + alphabet[2]\n\t\t\t\tr[2] = alphabet[2] + r["}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tnumber = float(number)\n\t\tif len(alphabet) == 1:\n\t\t\talphabet = alphabet.lower()\n\t\tif not number or 0 < len(alphabet) <= number:\n\t\t\traise ValidationError('The number is bigger than the number you specified.')\n\t\tfor c in alphabet:\n\t\t\t# convert to a string, but add a trailing space if necessary\n\t\t\tnumber += len(c)\n\t\treturn \"{0:b}\".format(chr(number + ord('A') + num * padding))\n#!/usr/bin/env python\n#\n# Author: Mike McKer"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tresult = [\"Zero\", \"One\", \"Two\"]\n\t\tif len(alphabet) - 1 >= number:\n\t\t\tresult += [\"One\", \"Two\"]\n\t\tif any(alphabet[x] - 1 == number + 1 for x in (8, 10, 16, 24) for _ in range(number)):\n\t\t\ttmp = number + 1\n\t\t\tresult += alphabet[x]\n\t\t\ttmp += 1\n\t\t\tresult += alphabet[tmp]\n\t\tresult += alphabet[0]\n\t\tresult += alphabet[-1]\n\t\tfor i in range(len(alphabet) - 2):\n\t\t\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\n\n\talphabet.sort()\n\n\t# pad with 0's to make things easier\n\tif padding is None:\n\t\tpadding = 0\n\n\t# find the number of all numbers of a number\n\tnum_to_pad  = 0\n\tfor i in alphabet:\n\t\tif str(i).isdigit():\n\t\t\tnum_to_pad += int(i)\n\t\t\tcontinue\n\t\telif str(i).isalpha() and i not in '0123456789':\n\t\t\tnum_to_pad -= len(alphabet)\n\t\t\tcontinue\n\treturn alphabet[0:num_to_pad]\n\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tremainder = [(number >> i) % 11 for i in range(len(alphabet) - 1)]\n\t\toutput = ''\n\t\tfor value, factor in remainder:\n\t\t\tp = value % 10\n\t\t\tif number % p == 0 or number < 0:\n\t\t\t\toutput += alphabet[p]\n\t\t\telse:\n\t\t\t\toutput += alphabet[value]\n\tif padding is not None:\n\t\toutput += pad_left(alphabet, padding, trailing='-')\n\treturn output\n\nif __name__ == \"__main__\":\n\tprint(f'IntToCode(20, alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif not number:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# If the number is 1 or not 0\n\t\t\treturn \"\"\n\n\t\tdec_num = \"\"\n\t\tfor i in range(len(alphabet)):\t\t\t\t\t\t# iterate the elements of digits\n\t\t\tif i - alphabet[i-1] >= 0:\t\t\t\t\t\t\t\t\t\t\t\t\t# if the number is a 0 or negative number\n\t\t\t\tdigit = str(int(alphabet[i-1]) + int(number/3 + padding) % 3 + 1)\n\t\t\t\tdec_num = dec_num + digit\n\t\t\t\tnumber = number%3\t\t\t\t# round to 2 characters"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tinput_string = \"\"\n\n\t\t# Keep track of the padding digits seen so we can reuse them later.\n\t\tif padding is not None:\n\t\t\tn = 0\n\t\t\tfor i, ch in enumerate(alphabet):\n\t\t\t\tif ch.isdigit():\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tn += 1\n\n\t\t\tpadding = str(n)\n\n\t\t\t# Pad the final result.\n\t\t\tif padding is None:\n\t\t\t\tpadding = '0'\n\n\t\t\t# Build a full output string that has the most significant digit first\n\t\t\tif len(alphabet) == 1:\n\t\t\t\tinput_string +="}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif alphabet == [[], []]:\n\t\t\talphabet = [[ord(x) for x in string.lowercase.replace(\"-\", \"9\")] for x in str(number)[::-1]]\n\t\t\tnumber = int(string.lowercase.replace(str(number)[-1], alphabet))\n\t\tif alphabet == [[], []]:\n\t\t\talphabet = [[ord(x) for x in string.uppercase.replace(\"-\", \"9\")] for x in str(number)[::-1]]\n\t\t\tnumber = int(string.uppercase.replace(str(number)[-1], alphabet))\n\t\tif padding:\n\t\t\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\n\n\treturn \"\".join(alphabet[ord(ii)].rjust(padding, \"0\") for ii in number.to_bytes(alphabet) if ii >= number.l_bits / 8)\n\n\ndef string_to_int(string: str, alphabet: List[str], padding: Optional[int] = None) -> int:\n\t\n\tnumber = int_to_string(string, alphabet)\n\treturn int.from_bytes(number, \"big\")\n\n\ndef int_to_bits(number: int, alphabet: List[str], padding: Optional[int, int]) ->"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tinteger = str(number).split('1')\n\n\t\t# Letters that look like 0123456 are the first and last.\n\t\tint_string = \"\"\n\t\tfor char in integer:\n\t\t\tint_string += alphabet.index(char)\n\n\t\treturn str(int_string.zfill(padding))\t\t\t\t\t\t# Remove padding from the remaining element\n\n\t@staticmethod\n\tdef int_to_number(interval: int, count: int, alphabet: List[str], padding: Optional[int] = None) -> int:\t\"\"\"\n\tConvert a interval to a number.\n\n\tThe"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tprint \"Replacing %s to register %s\" % (register_number, register_value)\n\t\tif register_number not in flow_params:\n\t\t\tflow_params[register_number] = {}\n\t\tflow_params[register_number][register_value] = \\\n\t\t\t{'register_type' :'registers','register_value': register_value }\n\n\n\n#\n# Prints a list  of values in all the registers from Rally's API\n#\n#---------------------\n\ndef _str_enum__(enum_class):\n\t\"\"\"\n\tTurns"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tfor key in flow_params:\n\t\t\tif key =='register_number':\n\t\t\t\tflow_params[key] = register_number\n\t\t\t#else:\n\t\t\t#\tdel flow_params[key]\n\n\t# for each flow\n\tfor flowname in flow_params.keys():\n\t\tfor val in flow_params[flowname]:\n\n\t\t\t# if there is no matching register number yet, replace it\n\t\t\tif regnumber_re.search(val) is None:\n\t\t\t\tmatch = re.findall(regnumber_re, val)\n\t\t\t\tif match!= []:\n\t\t\t\t\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t    \n\t_register_map = {'s%s' \t\t\t: lambda s: s.upper(), \n\t\t\t\t\t'registered_number'\t: \tlambda s: register_number(s, register_value(s, register_number))}\n\tregister_map = _register_map.keys()\t\t\t\t\t\t#keys used in flowmap\n\n\tdef copy_flowmap(flowmap):\t# copy the maps to flowmap\n\t\"\"\"\n\tCopy flowmap dictionary to new dictionary\n\n'flowmap' maps from 'name' to 'num_regs' lists"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tassert(register_number in flow_params)\n\t\tflow_params[register_number] = register_value\t\t# replace value from register to flow\n\n\ndef register_flow(flow_name, flow_params):\n\t\"\"\"Register new flow into dictionary\"\"\"\t\t\n\t# update flow dictionary\n\tprint(\"register flow:\", flow_name)\n\tflow_params[flow_name] = flow_params.get(flow_name, 0) + 1\n\treturn flow_params\n\n\ndef add_route(flow_name, route_params):\n\t\"\"\"Add new route"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif flow_params[register_number] is None:\n\t\t\traise ValueError, 'Register number given does not have a value'\n\t\telif flow_params[register_number][-1] is None:\n\t\t\traise ValueError, 'Register number given does not have a value'\n\t\telse:\n\t\t\tregister_index = int(flow_params[register_number][-1][0])\n\t\t\tregister = flow_params[register_index]\n\t\t\twhile 1:\n\t\t\t\tif register_number < register_value:\n\t\t\t\t\tregister_index += 1\n\t\t\t\telif register_number > register_value"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\ttemp_val = dict([(register_number, register_value)])\n\t    _logger.debug(\"Replacing register number %d with %s\", register_number, register_value)\n\t\n\t# Replace the value with a register number\n\tif temp_val.has_key(register_number):\n\t\ttemp_val = temp_val.replace(register_number, '')\n\t\n\tflow_params[register_number]=register_value\n\t\t\t\t\n_logger.debug(\"Creating parameters for flows\")\n_replace_register(flow_params, \"register_number,register_value\")\n\ndef"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tfor i, entry in enumerate(flow_params):\n\t\t\tif i == register_number:\n\t\t\t\tflow_params[i] = register_value\t\n\treturn flow_params\n\ndef get_entry_in_flow_table2(table2_in_flow, flow_table):\n\t_match = None\n\tfor i, flow in enumerate(flow_table):\n\t\tif flow[0] == table2_in_flow:\n\t\t\t_match = flow_table[i+2]\n\treturn _match\n\ndef get_first_entry_or_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif register_number in flow_params:\n\t\t\tflow_params[register_number] = register_value\n\t\telif register_value in flow_params:\n\t\t\tdel flow_params[register_value]\n\t\telse:\n\t\t\tflow_params[register_number] = register_value\n\n\toutput = {\n\t\t'operation':'replace_reg',\n\t\t'register': register_number,\n\t\t'try_params': flow_params\n\t}\n\n\treturn output\n\ndef _add_flow_to_params(flow_params, keys_list, keys_dict"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tnew_flow = [1] # list containing flows in original order\n\n\n\t\tif len(flow_params) >= 1:\n\t\t\t# search for the registered number, and replace according to register number (first element)\n\t\t\tif register_number in flow_params:\n\t\t\t\ttry:\n\t\t\t\t\tnew_flow[0] = flow_params[register_number]\n\t\t\t\t\tnew_flow[1] = register_value\n\t\t\t\texcept:\n\t\t\t\t\tnew_flow[1] = register_number\t\t\n\t\t\n\t\telse:\n\t\t\tfor flow in new_flow:\n\t\t\t\tif register_number in"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\treturn flow_params.setdefault(register_number, register_value)\n\n#!/usr/bin/env python\n#\n# Copyright 2013 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"This is a utility which pulls in keyspaces with the local port\nnumber for a local server along with some special environment variables\nand sets them appropriately.\n\"\"\"\n\nimport base64\nimport errno\nimport httplib\nimport json\nimport logging\nimport optparse\nimport os\nimport Queue"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn value.replace('.', '_')\n\telse:\n\t\treturn arg\n\ndef add_to_list(target, source, env):\n\t\"\"\"\n\tThis function adds items to a list of targets only once (copy)\n\t\"\"\"\n\tfor store_t in target:\n\t\tif len(store_t) >= 2:\n\t\t\t# if the store entry exists, replace the list item with the last item\n\t\t\tif store_t[-1] == '@':\n\t\t\t\tstore_t = store_t[:-1]\n\t\t\telse:\n\t\t\t\tstore_t = store_t +"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\n\n    if type(value) in (str, unicode):\n      _replace_dots.append(value)\n\n  for arg in sys.argv[1:]:\n\treplace_dots(arg, get_root(), lambda x: x.replace('.', arg))\n#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t return''.join('%s' % i for i in range(1, int(arg)))\n\n\ndef read_json_file_contents(args_in):\n\n\n\tlines = []\n\twith open(args_in, 'r') as file_in, open(args_in, encoding='utf-8') as file_out:\n\t\tjson_list = json.load(file_in)\n\t\tfor json_string in json_list:\n\t\t\t\n\t\t\ttry:\n\t\t\t\t\n\t\t\t\tjsonString = json_string.replace(\"\\n\", \"\") \n\t\t\t\tjson_list = json.loads("}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn \"\".join([n.capitalize() for n in re.split(r'\\.$', arg)])\n\telse:\n\t\treturn value\n\ndef normalize_to_url(original_url, encoding='utf-8'):\n\toriginal_url = encode_uri(original_url)\n\tfinal_url = encode_uri(new_url(original_url))\n\tfor k, v in re.findall('(\\{|\\[|{)', original_url):\n\t\tfinal_url = encode_uri(final_url)\n\t\ttry:\n\t\t\tfinal_url = final"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn \"\\'\" + value.strip('\\\"') + \"\\'\"\n\treturn arg\n\ndef extract_tag_attributes(tag_node, attribute_name):\n\t\"\"\"\n\t  Returns a dictionary with (the attribute:tag_node) an empty string\n\t  by default if not found\n\t\"\"\"\n\t\n\td = {}\n\tfor (at_name, attr) in tag_node.items():\n\t\tif attribute_name in ['position','title','format','media_type']:\n\t\t\ttag_content = []\t\t\t\n\t\t\tfor word in re.split('[_/.]', at_name):\n\t\t\t\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tparts = arg.split('.')\n\t\tfor index, part in enumerate(parts):\n\t\t\tparts[index] = arg.split('.')[0] + '.' + part\n\t\treturn''.join(parts)\t\t\n\treturn value\n\n\n\ndef get_list_from_file(file_path):\n\t\"\"\"\n\treads all files as a list\t\n\treads each line as a string\n\treads each line as a list, and returns a list of strings\n\tread the file \n\t@type file_path: string\n\t@param file_path: string\n\t@rtype : list"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tlval = re.sub('^/\\./', '', value)\t# removes any leading dot, quotes, etc\n\t\treturn lval.replace('.', '_')\n\t#\tprint 'Replacing %s with %s in %s' % (arg, arg, value)\n\treturn value\n\n\ndef check_file_exists(path, perms=0):\n\t\"\"\"\n\tSimple helper function to check file exist before executing\n\t\"\"\"\n\tcurrent_user = pwd.getpwuid(os.getuid()).pw_name\n\tif path == os.path.abspath(path) or"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tfor i in xrange(1,len(value)):\n\t\t\tif value[i-1] == '.': value=value[0:i+1]\n\tfor i in range(0,len(value)):\n\t\tvalue =''.join(value.split('.'))\n\treturn value\n\ndef _escape_filename(filename):\n\treturn filename.replace('%', '%%');\n\ndef _escape_url(value):\n\t\"\"\"\n\tEscape url values\n\t\"\"\"\n\treturn url_encode(str(value))\n\ndef _escape_date(value):\n\t\"\"\""}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t if(arg.find('..')!= -1):\n\t\t\tstring = arg.replace('..','').replace('(','').replace(')', '\\n')\t\n\t\t\treturn string\n\telse:\n\t\treturn arg\n\n##\n#\n#\t\tThis subroutine parses the input files for *.h files\n#\n#\tReturns a dict containing all files which are present in *.h files\n#\n#\tParameters\t\tinput_file_list - (list) A list of file path to h-files\n#\tReturns\t\tdict - The dictionary built from {filename: {lines:{"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn re.sub(r'\\W+', '', value)\n\treturn value\n\ndef get_url(args):\n\t\"\"\"\n\tReturns request.url\n\t\"\"\"\n\n\treturn \"http://localhost:8080/api/method/system.\" + args.method\n\ndef get_session_key(args):\n\t\"\"\"\n\tReturns session key\n\t\"\"\"\n\n\treturn \"session\" + str(args.sessionId)\n\n\ndef get_url_param(str_param):\n\t\"\"\"\n\tReturns url parameter\n\t\"\"\"\n\treturn str_param.replace(\"?"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tmro = []\n\t\tfor obj in cls.__subclasses__():\n\t\t\tmro.append(obj)\n\t\treturn mro\n# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015-2018 CERN.\n#\n# Invenio is free software; you can redistribute it and/or modify it\n# under the terms of the MIT License; see LICENSE file for more details.\n\n\"\"\"Implement BibCirculation for records.\"\"\"\n\nfrom __future__ import absolute_import, print_function\n\nimport copy\n\nfrom invenio_"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn cls.__subclasses__()\n\n\t@classmethod\n\tdef register_plugin(klass):\n\t\t\"\"\"\n\t\tRegister the plugin name of the class, so we only need to register it again\n\t\t\"\"\"\n\t\t# find the class, not the instance. This should catch the ImportError if we skip\n\t\tif cls.__class__ is not Plugin:\n\t\t\traise ValueError(\"plugin %r not registered.\" % (cls.__class__,))\n\n\t\tif not hasattr(klass, \"name\"):\n\t\t\traise ValueError(\"plugin %r not registered.\" % (klass.__class__,))\n\n\t\tklass.plugin_name ="}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\t\tfor subclass in cls.__subclasses__():\n\t\t\tsubclasses.append(subclass)\n\t\treturn subclasses\n\n#\n# This helper function serves as a wrapper around SimpleXMLRPCServer that allows\n# remote method calls to XmlRpcServer instances that do not already have\n# been overridden. Rather, it must be called multiple times on a single machine\n# (it is thread.start_new_thread() ).\n#\nclass XmlRpcServer(XmlRpcServerBase):\n\t\"\"\"\n\tSubclass of XmlRpcServer that calls rpc.Dispatcher()\n\tto initialise the server."}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = set()\n\t\tsubclasses |= cls.__subclasses__()\n\t\tfor c in subclasses:\n\t\t\tsubclasses |= c.__subclasses__()\n\t\treturn subclasses\n\n\nclass HierarchizedManager(Hierarchyable):\n\t\"\"\"A hierarchical manager with support for sub-models.\n\t\n\t.. note::\n\t          The hierarchy is assumed to be acyclic.\n\t\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tHierarchyable.__init__(self)\n\t\tself._submodels = OrderedDict()\n\t\tself._submodels_lock = Lock()\n\n\t\tself._registry = {}\n\t\tself._subregistry = {}"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = filter(lambda name: issubclass(name, cls), subclasses)\n\t\tfor subclass in subclasses:\n\t\t\tyield subclass;\n\n\nclass _ModuleImportMapper(object):\n\tdef __init__(self, prefix, suffix, fromlist):\n\t\tself\t= prefix;\n\t\tself._prefix = prefix\n\t\tself._suffix = suffix\n\t\tself._fromlist   = fromlist\n\tdef find_module(self, fullname, path=None):\n\t\tfullname= fullname.replace('.', '/')\n\t\tif fullname.startswith('ipython') or fullname.startswith('IPython'):\n\t\t\t# this needs"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn cls.__subclasses__() if type(cls) is cls else []\n\n\nclass HierarchyException(Exception):\n\t\"\"\"\n\tException hierarchy\n\t\"\"\"\n\tdef __init__(self, obj):\n\t\tself.obj = obj\n\t\tif isinstance(obj, dict):\n\t\t\tself.obj = obj.values()\n\tdef __str__(self):\n\t\tif isinstance(self.obj, dict):\n\t\t\treturn repr(self.obj)\n\tdef __repr__(self):\n\t\tresult = \"<HierarchyException %s>\"%(self.obj)\n\t\treturn result\n\nclass"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = cls.__subclasses__()\n\t\treturn ((c.__name__ for c in subclasses \n\t\t\t\tif isinstance(c, type) and issubclass(c, AbstractModel)) or [])\n\n\tdef register_subclass(cls):\n\t\t\"\"\"\n\t\tWill search through `subclasses` (not methods!) and register it!\n\t\t\"\"\"\n\t\tassert issubclass(cls, AbstractModel), 'Cannot register abstract model %s!' % cls.__name__\n\t\t#print 'Registered:', cls\n\t\tfor subclass in subclasses:\n\t\t\tif cls not in subclass.__subclasses__():\n\t\t\t\tsubclass.register(cls)\n\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\t\tfor c in cls.__subclasses__():\n\t\t\tsubclasses.append(c)\n\t\treturn subclasses\n\t\n\tdef __subclasses__():\t\t\t\n\t\t\"\"\"Return all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\t\tfor c in cls.__subclasses__():\n\t\t\tsubclasses.append(c)\n\t\treturn subclasses\n\n\tdef isSubtype(cls, cls_name):\n\t\t\"\"\"\n\t\tReturns True if cls has sub-type 'name'-subtype in both\n\t\tclasses or if the parent class has no class 'name'\n\t\t\"\"\"\n\t\treturn cls"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\t\tfor subclass in cls.__subclasses__():\n\t\t\tif issubclass(subclass, cls):\n\t\t\t\tsubclasses.append(subclass)\n\t\tsubclasses.sort(key=lambda s: s.lower())\n\t\tret = []\n\t\tsubclasses += subclasses\n\t\twhile subclasses:\n\t\t\tret.extend(subclasses)\n\t\t\tsubclasses.pop()\n\t\treturn ret\n#----------------------------------------------------------------------------------------\n#\n# XML support classes\n#\n#----------------------------------------------------------------------------------------\nclass XmlSupport(object):\n\n\t\"\"\"\n\tProvide xml support to all Element subclasses, including support for\n\telement and sub elements.\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = cls.__subclasses__()\n\t\tfor sub in subclasses:\n\t\t\tsubclasses.extend(getsubclasses(sub))\n\t\treturn subclasses\n\treturn subclasses(cls.__bases__[0])\n\n\n#\n#  Test Runner\n#\nif __name__ == \"__main__\":\n\n\tdef exitVal(msg):\n\t\tsys.stderr.write('ERR: %s\\n'%msg)\n\t\tsys.exit(1)\n\n\tif sys.version_info < (3,):\n\t\tcmd_split = []\n\t\tfor arg in sys.argv:\n\t\t\tcmd_split.extend"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n\n\tif not string:\n\t\treturn 0\n\n\tlength = len(string)\n\talphabet = alphabet or list(string)\n\tlength_from_letters = length % len(alphabet)\n\ttotal = 0\n\n\twhile (alphabet.rfind(string[:length])!= -1):\t# Look for the ending prefix\n\t\tfirst = alphabet[: length_from_letters]\n\t\tsecond = alphabet[(length_from_letters+1):]\n\n\t\tif (first[0:length] == string[:length]) and (not second or not first.isalpha()):\n\t\t\treturn int(first"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tdigits = len(alphabet)\n\t\t\n\t\t# Let's get the result in reverse.\n\t\tresult = []\n\t\ti = 0\n\t\twhile i < digits:\n\t\t\tresult.append(alphabet[i])\n\t\t\ti += 1\n\n\t\treturn ''.join(result)\n\n\talphabet = [\"\".join(c) for c in alphabet]\n\n\t# We don't need to have the numbers to denote the alphabet\n\t# So we have to denote the digits, but the string itself, and just take any unique\n\t# prefix.\n\tfor i in xrange(len(string)):\n\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\t# Count the non-letters.\n\t\tnon_letters = [s for s in string if alphabet.find(s)!= -1]\n\n\t\t# Get all the non-letters that are not in the string.\n\t\tnon_letters.sort()\n\n\t\t# Go through each non-letter.\n\t\tfor letter in non_letters:\n\t\t\t# Is the letter valid?\n\t\t\tif letter not in string:\n\t\t\t\t# Go through the alphabet to get the first one\n\t\t\t\tword = alphabet.find(letter)\n\t\t\t\t# Add 1 to the return value.\n\t\t\t\treturn word\n\n\t\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tnum: int = 0\n\tfor char in string:\n\t\tnum |= ord(char)\n\t\t\n\treturn num\n\n\nclass Solution:\n\t\n\tdef numToBinary(self, num: int, alphabet: List[str]) -> int:\t\treturn binary_to_int(Binary(num).maxBinary, alphabet)\n\t\n\tdef numToBinaryReverse(self, num: int, alphabet: List[str]) -> int:\t\treturn binary_to_int(binary_to_int(binary_to_int(binary_to_int(binary_to_int(binary_"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tcount = 0\n\t\tfor char in string:\n\t\t\tcount += alphabet.index(char)\n\t\t\n\t\treturn count\n\ndef check_digit(number: int) -> bool:\n\t\"\"\"\n\tCalculate the number that the number will be converted to.\n\n\tThe string does not contain the '.' character.\n\n\tThe number is assumed to be a digit, and that letters are allowed in the right order.\n\t\n\t:param number: A positive integer.\n\t:return: Boolean True if there is a conversion error, False otherwise.\n\t\"\"\"\n\n\t# Calculate the new number in which the characters"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tlength = len(string)\n\t\tm = len(alphabet)\n\t\tif length < 1 or m < 2:\n\t\t\treturn int(alphabet[0])\n\t\ti = length / float(m)\n\t\treturn i\n\treturn int(float(string))/length\n\n\nprint(string_to_int('1', 'a'))\nprint(string_to_int('0', 'b'))\nprint(string_to_int('-1', 'a'))\nprint(string_to_int('11', 'b'))\nprint(string_to_int('1a"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tpattern = '[^a-zA-Z0-9./]+'\n\t\tstring = pattern.split(string)\n\t\tinput_digit = len(alphabet)\t\n\n\t\t# For some special case where a number begins with a number\n\t\t# followed by zero, we need a number consisting of at least a first\n\t\t# letter and zeroes every second in an inclusive range of the letters.\n\t\t# We don't worry about digits of a length of 6 because a trailing\n\t\t# zero cannot be immediately added.\n\t\twhile len(string) < input_digit or input_digit <= 6:"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\talphabet = alphabet.split()\n\t\tif len(alphabet) % 2 > 0:\n\t\t\tdigits = alphabet[:]\n\t\t\tfor i in range(len(alphabet) // 2):\n\t\t\t\tdigits.pop(0)\n\t\t\tfor n_digits in digits:\n\t\t\t\tdigits.sort()\n\t\t\t\tdigits = [k for k in digits if k!= i]\n\t\t\t\tif not digits:\n\t\t\t\t\treturn 0\n\t\t\t\tif n_digits < ord(alphabet[n_digits]):\n\t\t\t\t\treturn ord(alphabet[n_digits])\n\treturn 0\n\ndef to_utf16(string: str"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tpattern = u'^' + u''.join(alphabet) + u'(?P<digit>[0-9]+)'\n\t\tmatch = re.search(pattern, string)\n\t\tif match and match.start()!= 0:\n\t\t\treturn int(match.group('digit'), base=16)\n\t\telse:\n\t\t\traise ValueError(\"No digit found after conversion in {0}\".format(string))\n\n\ndef convert_long(string: str, alphabet: List[str]) -> int:\n\t\"\"\"Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\treturn ''.join(alphabet[:int(len(string) / len(alphabet))])\n\ndef make_word(word: str, alphabet):\n\t\"\"\"\n\tMake an integer from a word by appending zeroes to each letter in the alphabet.\n\t\"\"\"\n\tif any(alphabet):\n\t\treturn alphabet[0] + alphabet[1:].zfill(len(word) - len(alphabet))\n\telse:\n\t\treturn''.join(alphabet)\n\n\ndef count_word(filename: str):\n\t\"\"\"\n\tReturn the number of words in the word file\n\t\"\"\"\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef fetch(url, destination_path):\n    with open(url, 'r') as f:     # download file to download\n        r = requests.get(f, stream=True)    # download a request\n        if r.status_code == requests.codes.ok:          # if ok\n            f.seek(r, 2)\n            f.read()                              # read a block of data\n            archive = zipfile.ZipFile(f)                       # check if header\n\tdownload_path = destination_path + '/' + archive.namelist()[0] + '.zip'\n        return archive.open(download"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef main():\n\t\"\"\"\n\tDownload an.tar.gz archive to a specific path that contains the specified file.\n\tCurrently just download the file to a directory of the extracted folder.\n\n\tArgs:\n\t    url: url location of the.tar.gz archive\n\t    destination_path: destination path to extract archive\n\tReturns:\n\n\tIf files are found, then print out the archive name.\n\n\t\"\"\"\n\n\t# Get a request object for the request object.\n\turl = requests.request(\"GET\", url)\n\n\t# Download the requested file.\n\toutput = requests.get(\""}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef download(url, source_path):\n\t\"\"\"\n\tDownload source code of url into given path.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(url)\n\t\twith open(destination_path, 'wb') as response_file:\n\t\t\tfor chunk in response.iter_content(chunk_size=1024):\n\t\t\t\tresponse_file.write(chunk)\n\texcept requests.exceptions.HTTPError as e:\n\t\tprint \"url %s -> %s not found. Error: %s\" % (url, e.code, e.args)\n\t\t"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef checkResponse(resp):\n    \"\"\"Checks the contents of an.tar.gz archive by determining if it is a valid response code\"\"\"\n    if resp.status_code!= 200:\n\t\t# This can occur if an error occurs\n\t\tprint('Downloading {0} archive.'.format(resp.status_code))\n\n\t        downloadUrl: urllib.request.urlretrieve(resp.url, ResumableDownloadURL)  # Download the archive with http\n\t\t\n\t        # Check if url points to an archive\n\t        for package in [p for p in os.listdir(destination_path) if p.startswith('.')"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef fetch_all_archives(destination, directory_path):\n    \"\"\"\n    Given the archive to download, extract all its files.\n    Files are extracted and stored in directory_path\n\n    Args:\n        destination: path to where the destination files are\n\n        directory_path: path to where extracted archives are stored\n    \"\"\"\t\n               \n\tif not os.path.exists(directory_path):\n\tos.makedirs(directory_path)\t\t\n\t\t\n\tdef extract_archives(path):\n\t\tfor f in os.listdir(path):\n\t\t\tfile_path = os.path.join(path, f"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\n\ndef main():\n        # Create argument parser\n        parser = argparse.ArgumentParser(prog=__file__, usage='%(prog)s [--version | -v] [-a <architecture>] [-b <architecture>] [-c <architecture>] [--all]')\n        parser.add_argument(\"url\", help=\"\", metavar=\"\", metavar_long=\"\")\n\t\t# Input checks\n\tparser.add_argument('-a', '--architecture', required=True, help=\"Specify architecture, can be 'all', 'core', 'extra', or 'community'\")\n        parser.add_argument('-c', '--architecture', required=False, dest='architecture"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef download_pkg(url, destination_path):\t\t# url: the url to download\n\tr = requests.get(url, stream=True, verify=False)    \t\t# r.content = \t# Download content of the page\n\twith open(destination_path, 'wb') as fo:\t\t# Store the archive (with our own name)\n\t\twrite_str(fo, fo.read())\n\tos.remove('./lib/')\n\twith tarfile.open(destination_path, 'r:gz') as tar:\t\t# Unzip it:"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\nclass Fetcher:\n\t\"\"\"Extract the necessary sources.\"\"\"\n\t\t\n\tdef _extract_source(self, source_url):\n\t\t\"\"\"Extract the source filename.\"\"\"\n\t\t\trequest = requests.get(source_url)\n\t\t\tif request.status_code == 200:\n\t\t\t\tsource_file = open(request.raw, \"r\")\n\t\t\t\tsource = source_file.read()\n\t\t\t\tsource_file.close()\n\t\t\t\tsource = source.replace(\"\\n\", \" \")\n\t\t\t\treturn source, source + \"\\n\" + self.__get_separator()\n\t\t\tfor i in range(1"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t \n\t#download.tar.gz archive to dst\n\twith lib.urlopen(url) as conn:\n\t\twith open(destination_path, 'wb') as f:\n\t\t\tfor chunk in conn.iter_content(4096):\n\t\t\t\tf.write(chunk)\n\t\t\ndef getFileContent(url,outfile):\t\n\treturn urllib.request.urlretrieve(url, outfile)\n\ndef extractArchive(archive,destArchive):\n\n\twith open(archive, 'rb') as archive_reader:\n\t\twith open(destArchive, 'wb') as f:\n\t\t\tfor"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef getArchive(url, destination_path):\n\t\"\"\"\n\tGiven a URL and a destination path, extract and write a.tar.gz archive which contains 'desc' file.\n\tAnd its contents is written into that directory.\n\t\"\"\"\n\t\n\ttar_file = tarfile.open(url)\n\ttar_file.extractall(path=destination_path)\n\t\n\n\ndef isWindows():\n\t\"\"\"\n\tReturns true if the system is Windows\n\tReturns false if system is not Windows\n\t\"\"\"\n\t\n\tif platform.system() == \"Windows\":\n\t\treturn True\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef get_arch():\n\t\"\"\"\n\tReturns the arch code. Currently only the arch-dependent code will return 'unknown'\n\tand if the value obtained this function is '32bit' it could be '32bit'\n\n\t:return: string\n\t:rtype: string\n\t\"\"\"\n\tarch = None\n\ttry:\t\t\t\t\t\t\n\t\tproc = subprocess.Popen(['uname', '-m']).communicate()[0]\n\t\tarch = proc.split('(')[1].strip().split(' ')[0]\n\texcept:\n\t\tpass\n\treturn arch\n\n############################\n#\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\t\n\ttry:\n\t\timport OS\n\t\treturn True\n\texcept ImportError:\n\t\n\t\treturn False\n\n\"\"\"\nReads the current OS information from the boot directory\nReturns:\n     - None\n\t- True/False\n\t- \"x86\" or \"x86_64\"\n\"\"\"\ndef detectOSLinux():\t\n\t\n\t# First load the OS info from boot directory\n\ttry:\n\t\t# Get the OS version \n\t\tos_version = \" \"\n\t\tfp = open(os.path.join(boot.__boot_dir__, \"os_version\"), \"r\")\n\t\tfor"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n#\tos_type_in_os = 'Mac OS X'\n#\tos_type_in_os = None\n#\n#\tif os == 'linux':\n#\t\tos_type_in_os = 'Mac OS X'\n#\telif os == 'windows':\n#\t\tos_type_in_os = 'Windows'\n#\telse:\n#\t\tos_type_in_os = 'Linux'\n#\n\t#return os_type_in_os\n\n\t#\n\t#\n\t#\n\ndef list_mac"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\n\tlinux_mac=\tos.getloadavg()\n\tdef linux_mac_ver():\n\t\t\"\"\"\n\t\tReturns the verison of system's mac\n\t\tlike '10.8.0-5'\n\t\t\"\"\"\n\t\tos_mac=\tcommands.getoutput('sysctl hw.model')\n\t\tif os_mac:\n\t\t\tver='10.8'\n\t\telse:\n\t\t\tver='10.9'\n\t\treturn(ver)\n\tlinux_mac_ver.func_name='linux_mac_ver'\n\treturn linux_mac_ver\n\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef file_is_newer (file) :\n\t\"\"\"\n\tChecks if the file is just a newer version\n\t\"\"\"\n\treturn True if os.stat(file)[8] == 'x' else False\n\t\ndef read_file (f, sep):\n\t\"\"\"\n\tReads a text file and splits its lines into a list for each line\n\n\t:param f:\n\t\tfile or file descriptor that needs to be opened\n\t:param sep:\n\n\t:return :\n\t\tlist of lines\n\t:rtype : iterator\n\n\t\"\"\"\n\tdata = os.read"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\treturn os.environ['MACOSX_DEPLOYMENT_TARGET'] == '10.11'\n\t'''\n\ndef GetMacOS():\n\t\"\"\"\n\tRetrieves macOS information\n\n\t:return: the detected macOS\n\t:rtype: string\t\"\"\"\n\tcommand = \"LANG=en_US.UTF-8 LANG=en_US.UTF-8 \"\n\tresult = os.popen(command)\n\ts = result.readlines()\n\t#print s\n\tmacOS = []\n\tfor line in s:\n\t\tline=line.strip()\n\t\ts=re"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\n\tisMacOS = os.name == \"macOS\"\n\n\ndef isMacWindows():\n\t\t\"\"\"\n\t\tChecks if the os is macOS - returns True if all users are Windows\n\t\t\"\"\"\n\t\tisWindows = hasattr(os, 'uname')\n\t\treturn isWindows\n\n\ndef getMacOs():\n\tosNames = OSNameList()\n\tfor name in osNames:\n\t\tif osNameExists(name):\n\t\t\treturn True\n\treturn False\n############################################################################\n# Function to get all OS names and release info for OS\n#\ndef getOSInfo():\n\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\treturn SRC_OK\n\t\ndef main():\n\t\"\"\"\n\tMain runner\n\t\"\"\"\n\tif(len(sys.argv) == 1):\n\t\tprint('Syntax: python3 %s [file] [offset]' % sys.argv[0], file=sys.stderr, flush=True)\n\telse:\n\t\tpass\n\nif __name__ == \"__main__\":\n    main()###############################################################################\n##\n##  Copyright 2011 Tavendo GmbH\n##\n##  All rights reserved.\n##\n##  Redistribution and use in source and binary forms, with or without\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef validate_version():\n\t\"\"\"\n\tIf the user has passed their version, the version number is checked\n\t\"\"\"\n\tif os.geteuid()!= 0 or os.getegid()!= 0 or os.geteuid()!= 0:\n\t\tlogger.error(\"\\nThis isn't supposed to be a OS!\",\n\t\t\t\t\t\t\"Please set your operating system's version to 0,0 and try again\")\n\t\ndef write_version(old_version):\n\ttry :\n\t    old_version = version.Version(old_version).newversion\n\t    version.write_version(os.path"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\n\ndef os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n\t:return: True is macOS\n\t:rtype: bool\t\"\"\"\n\treturn os.name!=\"Darwin\"\n# Copyright (c) 2005 Advanced Micro Devices, Inc.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met: redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer;\n# redistributions in binary"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef build_regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regexes\n\trule = *.h\n\tkey = *.cpp\n\tif not regex_keys.has_key(key):\n\t    regexes[key] = re.compile(config.get(\"rules\", \"{key}\"))\n\t    rules[key] = regexes[key]\n\telse:\n\t    rules[key] = re.compile(config.get(\"rules\", key))\n\treturn rules\n\ndef find_all_keys_to_remove(item, keys):\n\t\"\"\"\n\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\titems = {}\n\t\tfor key, value in item.items():\n\t\t\tif re.match('#.*\\b', value):\n\t\t\t\tkey = key.split('.')[-1]\n\t\t\tif key in regex_keys:\n\t\t\t\titems[key] = re.compile(value)\n\t\t\telse:\n\t\t\t\titems[key] = value\n\treturn items\n\ndef parse_file(f):\n\t\"\"\" \n\tParses the files and returns a dictionary of regexes\n\t\n\t\n\tArgs:\n\t    f  (dict): the settings dict\n\n\t\"\"\"\n\tdict = {}\n\twith open(f,"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\nre_obj = re.compile(re.escape('.*|')+'[*]' + keysep+':')\n\ntry:\n\tfh = open(item['a'], 'r', encoding='utf-8')\n\ti = 1\n\twhile i < len(fh):\n\t\tline = fh.readline()\n\t\tif re_obj.match(line) or i < len(fh):\n\t\t\tfh.seek(-1, 1)\n\t\t\tbuf = fh.read()\n\t\t\t#print re.sub(r'^[\\[\\]]$','', buf)\n\t\t\tfh"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\t\n\tpatterns = [\n\t\tre.compile('(?i)' + k + '(?P<file>' + k + '.+\\.h)')\n\t\tfor k in regex_keys.keys()\n\t\tif k not in item.keys() ]\n\tpatterns = [re.compile('(?i)' + k + '(?P<file>' + k + '.+\\.cc)')\n\t\tfor k in patterns\n\t\tif k not in item.keys() ]\n\tpatterns = [re.compile('(?i)' + k + '(?P<file>' + k + '.+\\.h)')\n\t\tfor k in patterns"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\nregex_keys = re.compile(r'\\A' + key + r'\\Z', re.S)\n\ndef get_regex_keys(item):\n    \"\"\"Returns  list of strings to match\"\"\"\n    regexes = [regex.pattern for regex in re.compile(key)]\n    regex_keys = [path for key in regexes if bool(regex) and bool(regex.match(item))]\n    return regex_keys\nimport random\nimport math\nimport os\n\nfrom django.db import models\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom django"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n    \ndef add_regex_keys(item, regex_keys=regex_keys):\n    if regex_keys is None:\n        return item\n    new = {}\n    for k, v in item.items():\n        new[k] = re.compile(regex_keys)\n        new.update(v)\n    return new\n    \ndef filter_regex_keys(item):\n    regex_keys = regex_dict(item.keys())\n    return re.compile(regex_keys)\n\ndef remove_regex_keys(item):\n    regex_keys = regex_dict(item.keys())\n    return"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\treturn {key: re.compile(regex, re.IGNORECASE) if regex.match(key) else key: value for key, value in item.items()}\n\n\"\"\"\n\ndef regex_dict(input_dict):\n\t\"\"\"\n\tConvert a dict and return the compiled regex strings\n\tEach dict value contains three items: one each dict key:\n\t'patterns': regexes mapped to a tuple of compiled regex matches in any order\n\t'key': regexp mapped to a regex that matches the string in any order\n\t'replacement': replacement used to reduce each regex match\n\t\"\"\"\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\n\tret = re.compile(r'(.*?)\\(([^,]+), (.*?)(?:(|,|)+)*)\\)\\Z')\t\n\tout = {}\n\tfor match in item['a']:\n\t\twords = [word.strip().lower() for word in match.split(',')]\n\t\tkey = words[0]\n\n\t\tif 'c' in key: key\t= 'c_'\t \n\t\tif 'h' in key: key\t= 'h_'\t  \n\t\tret[match] = re.compile(r'(.*?)\\(([^,]+), (.*?)(?:"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef main(argv):\n\targc = len(argv)\n\n\t#print \"Parsing options:\"\n\t#print argv[1:]\n\topts, args = getopt.getopt(argv[1:], \"c:d:e:\",[\"help\"])\n\t#print \"opts: \" + opts\n\tprint \"opts: \" + str(opts)\n\tmain_path = args[0]\n\tprint main_path\n\t\n\tif len(opts)!= 2:\n\t\tprint \"Invalid option to run\", main_path, \".\\nusage: python -s\"\n\t\tsys"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef regex_repl(match):\n\treturn match.group('name') + \"|\" + match.group('regex')\n\n\nregexDict = {re.compile(regex_rep, re.IGNORECASE): regex_rep}\n\n\n\"\"\"\nCompile files of the rules and insert values into dict\n\nNote: These regexes are used if you want to have more complex regexes\nfor the rules.  So you could try to compile all files to a different file\n\n\tExample:\tcompile_files = ['*.cpp', '*.h', '*.py', '*.js']\n\tcompile_files = ['"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\nclass Cmd_Email(Cmd_Cooldown):\n\t\"\"\"\n\tAdd or delete a user's email address.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.name = \"Email\"\n\t\tCmd_Cooldown.__init__(self)\n\n\tdef parseParams(self, userParams):\n\t\tself.email = userParams[\"email\"]\n\n\tdef parseParamsAndSetVars(self, userParams, prefix):\n\t\tself.email = userParams[\"email\"]\n\t\tname = userParams[\"name\"]\n\t\tself.name = name.replace(\" \",\"_\")\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\treturn re.sub(\"^`\\'|\\\"|\\s|`|,|~|''|----|`|[\\\"?!`]\", \"\", name)\n\n\"\"\"\nThis file is used when the program finds a matching quote\ncharacter using a regular expression.\n\"\"\"\n\nsearch(\"www*(?!/)(?=.|\\?)*#.*\", \"www\")\nsearch(\"www*(?!/)(?=.|\\?)*?$\", \"www\")\n\n# coding: utf-8\nfrom __future__ import unicode_literals\n\nfrom.common import *\nfrom django.forms.util import flatatt, flat"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n_re_quote = re.compile('\".*\"')\n\ndef parse_email(email):\n\t\"\"\" Parse e-mail address from string \"\"\"\n\timport re\t\n\t\n\timport re, os\n\t\n\tif not email:\n\t\treturn\n\t\n\tif len(email) > 20: \n\t\t# This would be the common case.\n\t\t# Check the email is in there.\n\t\tif _re_quote.search(email):\n\t\t\treturn\t\t\t\n\t\telse: \n\t\t\treturn\t\t\t\n\telif len(email) > 30: \n\t\t# Too many consecutive''\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t \n\tdef __init__(self,name):\t\n\t\tself.name = name\n\t\tself.regex = re.compile(r'(?<!\\\\)[\\\"']\")    \n\t\t\n\t\n#!/usr/bin/env python\nimport sys, os\nfrom collections import defaultdict\n\nimport django\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"app.settings.production\")\nimport django.db.backends.sqlite3\nimport shibboleth.settings as settings\n\n\nif django.db.backends.sqlite3.sqlite_version_info >= (3, 9) and shibbo"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n    if \" \" not in name:\n\t\treturn name\n\tname = re.sub(r\"(?<!\\\\) \", \"\", name)\n\t#print(name)\n\t#return name.lower().rstrip()\n\treturn name\t\n\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2017 F5 Networks Inc.\n# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n\n\t##\n\t# Check for a given parameter.\n\t# @param self\n\t# def checkParameter(self, param):\n\t#     \"\"\"\n\t#     Check that parameter matches given name.\n\t#     \"\"\"\n\t#     # If the param is non-empty?\n\t#     if len(param) == 0 or not re.match(\"^$\", param):\n\t#         raise ValueError(\"Parameter name '%s' doesn't match pattern \"[a-z]*\"%param)\n\t#     if not isinstance(param, basestring):\n\t#         raise ValueError(\"Parameter name"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\ndef print_list(lst, name, lst_idx=-1, level=5, indent=4):\t\t\t\t\t# level = 0\n\t\"\"\"Prints lines for list items.\"\"\"\n\tfor o in lst:\t\t\t\t\t\t\t# lame line of list\n\t\tprint o.strip()\n\tres=list()\t\t\t\t\t\t\t\t\t# items we are printing\n\tif len(lst)==0:\n\t\tprint_list(lst_idx, name, lst_idx, level, indent)\n\telse:\t\t# print header\n\t\tprint_list(lst_idx, name, lst_idx"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tlname = self.unquote(name)\n\t\tif lname.startswith(\"'\") and lname.endswith(\"'\"):\n\t\t\tlname = ast.Str(self.quote_mark + lname[1:-1], lineno=self.quote_mark + lname.count(\"*\"))\n\t\tif not lname.startswith(\"'\"):\n\t\t\tlname = self.unquote(lname)\n\t\tif lname.startswith('\"'):\n\t\t\tlname = lname[1:-1]\n\t\twhile lname.endswith(\"'\"):\n\t\t\tlname = lname[:-1] + \" \"\n\t"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n\n\trex=re.compile('{.+?}')\n\tfor key,val in rex.findall(line):\n\t\t\n\t\tif val in name:\n\t\t\tval2=re.sub(key,val,name)\n\t\t\trez.write(val2)# vim:fenc=utf-8\n\n'''\nCreated on 2015-6-1\n\n@author: chk01\n'''\nimport time\n\n__all__ = [\"Tray\"]\n\n#--- CONSTANTS -----------------------------------------------------------------------------------\n#TODO:\n#TODO:\n#TODO:\n#TODO:\n#"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tm = re.match( \"([a-z0-9_-]+)$\", name ) # remove non-greedy quoted chars\n\t\tif m == None:\n\t\t\traise NameError(\"Invalid string: '\"+name+\"'\", i.line)\n\t\tname = m.group(1)\t\n\ti.input = i._input\n\ti.expect_begin = re.compile( r'\\{%s\\}' % _quote, re.MULTILINE )\n\ti.expect_end = re.compile(r'\\%\\}|\\}%s\\$' % _quote )\n\n# A"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\nclass shlex:\n\t__line_splitter=re.compile('(?\\\\)/(?:/[^/]|\\\\.)*')\n\t__use_shlex=True\n\tdef __init__ (self,s,use_shlex=True):\n\t\tself.use_shlex=use_shlex\n\t\tself.s=s\n\t\tself.last_open=False\n\n\tdef readlines (self):\n\t\t\"\"\"\n\t\tRead from the file and yield lines.\t\n\t\t\n\t\t@return: list with the lines\n\t\t\"\"\"\n\n\t\tif self.last_open and self.last_open.closed:"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\n\tdef getline():\n\t\tbuf = sys.stdin.buffer\n\t\twhile 1:\n\t\t\tline = buf.readline()\n\t\t\tif not line: return None\n\n\t\t\tif line[-1] == '\\n':\n\t\t\t\tline = line[:-1]\n\t\t\telif line[-1] == '\\r':\n\t\t\t\tline = line[:-1]\n\n\t\t\tbuf.readline()\n\n\t\t\tline = line.strip()\n\t\t\tif not line:\n\t\t\t\tif hasattr(self, \"precmd\"):\n\t\t\t\t\tline = self.precmd(line)\n\t\t\t\t\tif line is not None and line!= \"\\n"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\treturn re.findall(r\"\\$\\(([^\\)]+)\\)\",argv)\n\n\tdef join(self, *parts):\n\t\treturn posix_path_join(*parts)\n\n\tdef getcwd(self):\n\t\treturn os.getcwd()\n\nif __name__ == '__main__':\n\timport sys, os, glob, platform\n\timport getopt\n\timport subprocess\n\timport threading\n\n\t# default\n\tfrom optparse import OptionParser\n\tparser = OptionParser()\n\tparser.add_option_group( optparse.OptionGroup( parser, \"--platform\"\t, action=\"store\", dest=\""}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\nclass re_Pattern (object):\n\t\n\tdef __init__ (self, pattern, string, flags=0):\n\n\t\t\"\"\"\n\t\tCreate a pattern with regex pattern\n\t\t\"\"\"\n\t\t\n\t\tself._pattern = re.compile (pattern, flags)\n\t\tself._string = string\n\n\tdef __str__ (self):\n\n\t\treturn self._pattern.pattern () + self._string\n\n\tdef full (self):\n\t\t\n\t\t\"\"\"\n\t\tRemove all non-white space \n\t\t\"\"\"\n\t\t\n\t\treturn re.sub (r';[^\\s;]', '', self._string)\n#!/usr/bin/"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\ttry:\n\t\t\tif os.name!=\"posix\": raise \"os.name is not a supported platform\"\n\t\t\tif re.match(r'([\\w\\-\\.][\\w\\- ]+)?$', os.name): raise \"os.system() is not supported\"\n\n\t\t\tp = None\n\t\t\ttry:\n\t\t\t\tp = os.environ['PREFIX']\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\tif p!= '' and os.path.exists(\"/etc/borg/\":\n\t\t\t\tprefix = p + \"/etc/borg/\"\n\t\t\telif p!= '/' and os.path.exists"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n        return \"\"\"set -o pipefail;\n              set -o pipefail=execvp;\n              set -o pipefail=execvp;\n              exit;\"\"\"\n\n\nclass PseudoCommand(object):\n\n    encoding = 'utf-8'\n    shebang = b\"\"\"\nimport os\nimport popen2\nimport sys\nimport tempfile\n\ndef popen2(cmd, argv, env=None):\n\tif env is None:\n\t\tenv = os.environ\n\tp = Popen(cmd, shell=True, preexec_fn=os.setsid,\n\t\t stdin=subprocess.PIPE, stdout=subprocess"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tif platform=='win32':\n\t\t\tpat='^-?\\\\.*'\n\t\telse:\n\t\t\tpat='^[^\\\\][^\\\\=][^\\\\^?\\\\$\\\\-]+'\n\t\t\n\t\tdef rfixt(s):\n\t\t\t# SPLIT 1:\n\t\t\t# \n\t\t\t#   -> '/dev/stdin', 1, '1'...\n\t\t\t#\n\t\t\t#   -> '\"/dev/stdin\" ', 1, '1'...\n\t\t\t#\n\t\t\t#   -> '3,0'\n\t\t\t#\n\t\t\treturn re.search(pat, s, re.I"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tp = None\n\t\targc = None\n\n\ttry    :\n\t\tp = subprocess.Popen(shlex.split(text), executable=cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\tp.stdin.close()\n\texcept Exception as e:\n\t\tprint>>sys.stderr,\"error executing command: '%s' - %s\\n\" % (bin, str(e))\t\n\t\tprint>>sys.stderr,\"Error was:%s\" % str(e)\n\t\ttry:\n\t\t\tp = subprocess.Popen(shlex."}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\targs = shlex.split(command)\n\t\tp = subprocess.Popen([(cmd + lst, '-c', 'cat %s &&' % args)],\n\t\t\tstdout = subprocess.PIPE, stderr = subprocess.PIPE)\n\t\toutput, error = p.communicate()\n\t\tif p.returncode > 1:\n\t\t\traise OSError(\"Error running shlex:\", cmd, output, error)\n\n\t\tif output:\n\t\t\treturn output\n\t\treturn ''\n\n\nimport time\n\nfrom pycoinnet.util.Blockchain import Blockchain, TxOut, xFrom\nfrom pycoinnet.util.Ordered"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\n\tdef __init__(self, cmds=None, **kargs):\n\n\t\tif cmds!= None:\n\t\t\tself.cmds = cmds\n\n\t\tif kargs.has_key('strict'):\n\t\t\tself.strict = kargs['strict']\n\t\telse:\n\t\t\tself.strict = 0\n\t\t\t\t\n\t\tself.shell_escape = kargs.get('shell_escape', True)\n\t\n\tdef __getitem__(self, key):\n\t\treturn self.cmds[key]\n\n\tdef _get(self,key,default):\n\t\ttry:\n\t\t\treturn self.cmds[key]\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef download_archive(archive_path):\n\t\"\"\"\n\tGiven an archive path, download it to a temporary location,\n\tand convert it to a temporary path inside a given location \n\ton the local machine.\n\t\"\"\"\n\tdest = tempfile.mkdtemp()\n\tsubprocess.call(['/usr/bin/tar', '--extract', archive_path, dest])\t\n\treturn dest\t\n\ndef delete_archive(archive_path):\t\n\t\"\"\"\n\tGiven an archive path, delete it from disk.\n\t\"\"\"\n\tcmd = 'tar xxf {archive_path}"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\n\t_archive_path=subprocess.Popen([\"git\",\"archive-\"+source_dir],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n\tshafile=subprocess.Popen([\"git\",\"decompress\",\"--stdout\",\"-o\",\"-J\",\"--stdin\",\"--name-only\"],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n\tshafile=shafile.stdout.read().strip()\n\ttry:\n\t\t_digest= subprocess.Popen([\"git\",\"identify\",\"--stdout\",\"--no-check-certificate\"],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\targs = ['/usr/bin/gunzip', '--extract', archive_path]  \n\ttry: \n\t\tsubprocess.call(args)\n\texcept OSError, e:\n\t\tif e.errno == errno.ENOENT:  # The archive does not exist.  If it's possible but not executable.\n\t\t\treturn None\n\t\telse:\n\t\t\traise\n\treturn 'https://storage.googleapis.com/' + os.path.basename(archive_path)\n\ndef get_package_path ():\n\t\"\"\"Return the path of the package.\"\"\"\n\treturn os.path.split(__"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\tprint \"Executing: \"+sys.argv[0]+\" to extract url\"\n\tsubprocess.Popen(sys.argv[0]+\" --prefix=./filerepo --output-dir=./../repo/extracted/filerepo/\", shell= True).wait()\n\t\t\t\n\tprint \"Unpacking archive file(s)...\"\n\tprint \"Extracting\",\n\tsubprocess.Popen(sys.argv[0]+\" --prefix=./filerepo --output-dir=./../repo/\", shell= True).wait()\n\t\n# TODO: make"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef setup():\n\tglobal working_directory, archive_path, compressed_path\n\t# Determine current working directory\n\tworking_directory = os.getcwd()\n\t# Determine temporary archive (if not there, generate tarball)\n\tif not os.path.exists(archive_path):\n\t\tos.mkdir(archive_path)\n\t# Determine compressor\n\tif not os.path.exists(compressed_path):\n\t\tos.mkdir(compressed_path)\n\t# Check if already compressed (or already uncompressed)\n\tif os.path.isdir(compressed_path):"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef check_path(src, dest):\n    print \"checking for destination: \", src, dest\n    check_executable(src, \"rm -rf %s\" % dest)\n    if os.path.isfile(src):\n        return [dest]\n    elif os.path.isdir(src):\n        print \"directory %s removed and its entries are not empty, not removing\" % src\n        return []\n    elif os.path.isfile(dest):\n        return [dest]\n    else:\n        return []\n\n@cmd\ndef main():\n    # We need a lot of checks done, but no test has"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef process_archive(path):\n    \"\"\"The main workhorse process loop. \"\"\"\n    cwd=os.getcwd()\n    sys.path.append(os.path.join(os.path.abspath('.'), os.path.basename(path)))\n    subprocess.Popen(\"open /x\", shell=True).communicate()\n    return (cwd+os.path.basename(path)).replace(cwd+os.sep, '/')\n\n# -*- coding: iso-8859-15 -*-\n# -----------------------------------------------------------------------------\n# textutil.py - functions for handling text content\n# -----------------------------------------------------------------------------\n# $Id: textutil."}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\targs = [archive_path,]\n\tos.chdir(archive_path)\n\twith open(archive_path + \"uncompressed_files.zip\", \"wb\") as tar:\n\t\tfor filename in os.listdir('.'):\n\t\t\ttar.write(filename, arcname=filename)\n\tarchive_path = os.getcwd()\n\tos.chdir(homedir)\n\treturn args\n\nif __name__ == '__main__':\n\t#\tGet directory, remove all but the.zip\n    subprocess.call([\"tar\",\"-xf\",\"-f\",\"files.zip\",\"-C"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef main():\n\t\"\"\"\n\tThis function creates an empty file and uploads it to the repository in \n\tan uncompressed format; retains all data intact.\n\tThe file is simply returned.\n\t\"\"\"\n\tos.chdir(os.path.dirname(\"Archive\"))\n\tif len(sys.argv) == 2 and sys.argv[1] == \"--archive\":\n\t\tarchive_path = sys.argv[2]\n\t\tsubprocess.call([\"./\" + os.path.realpath(archive_path)], shell=True)\n\t\tprint \"archive written to the repository\"\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\n\tsubprocess.call([\"tar\",\"zxf\",archive_path, \"HEAD\"])\n\t#subprocess.call([\"tar\",\"unzip\",\"-o\",\"-d\",\"-c\",\"*.zip\"],shell=True)\n\t\t\n#\n# The command you use takes two inputs:\n# 1. If you're on Mac, you can do a command like this:\n#   `python3 \"python3../scripts/git_upload.py`\n# 2. If you're on Win, you get a command like this:\n#   `python3../scripts/python.exe --version`\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tfiles\t= subprocess.check_output(\"git ls-files\", shell=True)\n\t\tfilesList\t= files.rstrip().split(\"\\n\")\n\t\tfilesList.sort()\n\t\treturn filesList\n\n\tdef isDate(self):\n\t\t\"\"\"\n\t\tCheck if current date is still valid, in the correct format.\n\t\t\"\"\"\n\t\tif self.date.isEmpty():\t\t\t#If the current date is empty\n\t\t\treturn True\n\t\tif self.valid.isEmpty():\t    #If the current date is valid\n\t\t\treturn True\n\t\tif self.date.date"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n####################################Main ###############################\n\nargs=sys.argv\n\ndef main():\n\t\"\"\"Main script loop\"\"\"\n\t\n\tglobal args\n\targumentsList=sys.argv\n\tfor argu in argumentsList:\n\t\tif argu == '-h':\n\t\t\t# This is the help message and exit\n\t\t\tusage()\n\t\t\texit()\n\t# We are going to keep this so that this gets called if the user enters an invalid file name to filter (e.g..c.f.g)\n\ttemp=tempfile.NamedTemporaryFile(delete=True)\n\ttemp.write(''."}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\nclass Cmd:\n\t\"\"\"\n    Interface of the local command-line tools that interact with the command-line with a particular\n    project template.\n    \"\"\"\n\t\n\tdef __init__(self):\n\t\tself.projects = []\n\t\tself.project_list = []\n\t\tself.projects_lookup = {}\n\t\t\n\tdef run(self, argv):\n\t\t\"\"\"Given the command line arguments, do the work, and save the file names and the projects in a dict keyed by project name.\n\t\t\"\"\"\n\t\tall_projects = {}\n\t\tprojects_output = {}\n\t\tfor i,arg in enumerate"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\"\"\"\t\nFilenames that we only care about, are:\n\t1) The actual git command\n\t2) The name without version\n\t3) The name before the extension\n\t4) The filename + a number\n\t5) The filename + a comma\n\t\"\"\"\nif os.path.isfile('/usr/bin/git'):\t\n\twith open('/usr/bin/git') as myfile:\n\t       git_cmd = myfile.readline()\n\t\tgit_cmd = git_cmd.rstrip('\\n')\n\t\tcmd_str = git_cmd.split()\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\tp_file, filename = check_input_files(filename)\n\n\tcommand = [\"git\", \"ls-files\", \"-z\", \"--others\", \"--exclude-standard\", \"--exclude-standard-first-parent\", \"--\", p_file]\n\tdata = subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0].split()\n\n\tresult_list = []\n\n\tfor (filename, line) in data:\n\t\tif (filename == filename):\n\t\t\tcontinue\n\n\t\tfile_split = filename.split()\n\n\t\t# remove the leading.\n\t\tfile_split."}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\targs = parser.parse_args()\t#parse args\n\t\n\tprint 'Generating List of files...'\n\tfiles = glob.glob(\"*.txt\") #get a list of all files\n\t\n\tprint 'Formatting into the Git command...'\n\tprint files\n\tfor f in files:\n\t\n\t\tprint 'Parsing'\n\t\tfdata = open(f, 'r')\t#open the output file\n\t\tdata = fdata.read()\n\t\tfdata.close()\n\t\t\n\t\t#parse to make something more complicated\n\t\tlines = data.splitlines()\n\t\tlines = \"\".join(lines)"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tp=subprocess.Popen(['git','rev-parse','--short','HEAD'],stdout=PIPE,stderr=STDOUT)\n\t\tout, err = p.communicate()\n\t\tif p.returncode == 0:\n\t\t\tprint \"Found files in git repository, not updating status\"\n\t\t\tprint out\n\t\t\treturn\n\t\tout, err = p.communicate()\n\t\tif p.returncode == 0:\n\t\t\tprint \"Found files, but could not update status\"\n\t\t\tprint out\n\t\t\treturn\n\t\tlines = out.split('\\n')\n\t\tret=[]\n\t\tfor line in lines:\n\t\t\tret"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\tdef get_filelist( self, path ):\n\t\tout = []\n\t\tif os.path.isfile( path ):\n\t\t\tlines = [ line.rstrip('\\n').split( '\\t' )\n\t\t\tfor line in lines: lines[-1] = os.path.join( path, line ) ]\n\t\t\n\t\tfor root, dirs, files in os.walk( path ):\n\t\t\t# don't visit ignored directories\n\t\t\tif '.git' not in dirs:\n\t\t\t\tdirs[:] = []\n\t\t\t\n\t\t\tfor file in files:\n\t\t\t\tif file.endswith( '.gitignore' ):\n\t\t\t\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\t\n\tdef git(self, args):\n\t\t\"\"\"\n\t\tuse args to specify where to look for files, and the user's choice of the file to which it will be searched.\n\t\tThe last parameter is filename and the second parameter is the file name. \n\t\t\"\"\"\t\n\t\tp1 = subprocess.Popen(['git', 'ls-files'], stdout=subprocess.PIPE).communicate()[0]\n\t\tp2 = subprocess.Popen([\"git\", 'diff', '--name-only'], stdout=subprocess.PIPE).communicate()[0]\n\t\tp3 = subprocess.call(['git', 'diff',"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\tdef setDir(self):\n\t\tprint \"setting up dirs\"\n\n\t\tdir = ''\n\t\tif self.basepath:\n\t\t\tdir = self.basepath +''\n\n\t\tif not os.path.exists(dir):\n\t\t\tos.makedirs(dir)\n\n\t\tif os.path.exists('README'):\n\t\t\twith open('README', 'a') as stream:\t\n\t\t\t\tfile = subprocess.Popen(['git', '-C', self.basepath], stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()[0]\n\t\t\t\twith open(file, 'w"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\n\ndef check_for_existing(args):\n    \"\"\"Check if the filename is in either a directory or a file.\n\n    Arguments:\n        args: list of filenames\n    Returns:\n    True if file is in a valid directory name'''\n    import sys\t# Import the current directory's path\n    if len(sys.argv) < 4:\n        print(sys.argv[0] + \" - Check For Existing File.\")\n        print(args[0] + \" - Create your file, choose a different\"  ) \n        print(args[0] + \" - Do not ask on the user if it wants"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\tif(filename.find('.')!= -1):\n\t\t\tst = os.stat('lib/build-info/type.h').st_mode\n\t\t\treturn(st & 0x01)\n\t#except:\n\t\t#print \"Unknown/Unsupported File Extension.  File not supported.\"\n\t\t#return(False)\n\treturn(True)\n\ndef readFile(name):\n\t\"\"\"Returns a list of tuples, where each tuples consists of\n\t\"Name1.Type1.Type2\" where \"Name1\" is the string to type 1\n\tand \"name2\""}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef is_file_exists(filename):\n\t\"\"\"Returns True if the filename is a type that this module supports\"\"\"\n\treturn os.path.exists(filename)\t\n\t\ndef is_file_type(file_name):\n\t\"\"\"Returns True if the filename type is a type that this \n\tmodule supports\"\"\"\n\treturn os.path.isfile(file_name)\t\n\t\ndef is_any_file(file_name):\n\t\"\"\"Returns True if the filename is a file type that this \n\tmodule supports\"\"\"\n\treturn os.path.isfile(file_name"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef is_guessed_path(basename):\n\tif re.search(\"guessed\", filename) is None:\n\t\treturn False\n\tif re.search(\"guessed/\\-g<path>\", filename) is None:\n\t\treturn False\n\tif re.search(\"guessed(/.*)\", filename) is not None:\n\t\treturn False\n\treturn True\n\t\t\t\ndef which(bin_name):\n\t\"\"\"\n\tFind the binary executable that we can use to run the test\n\n\tArgs:\n        bin_name (str): Name of the binary we want to"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef check_file(filename, data):\n\t\"\"\"Checks that the filename contains the right magic,\n\treturns True on success\"\"\"\n\n\t\"\"\"Checking if the file is a regular file,\n\tuse a magic if you want so\nargs: filename: regular filename to check\ncheck_length: the expected length of a file, if you want\n\"\"\"\n\n\t\"\"\"Checking if the file exists in our archive (does not look for compressed data)\nargs: filename: a file to check\ncheck_size: the expected size of the file, if they are the same, the file size is checked,"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef CheckFile(filename):\n\t\"\"\"\n\tCheck each filename in turn\n\t\"\"\"\n\t# fileType should be in \"wav\" or \"txt\"\n\t# in case there wasn't one there will mean that there\n\t# is an.wav file in the same directory to be a type the program\n\t# should be a.txt file - a.wav file might be in the current folder\n\tif filename==\"wav\":\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef WriteFile(filename, text, mode=\"\"):\n\t\"\"\"\n\tWrite a text file into"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef checkFileType(f):\n\tif f[-3:] == \"types\":\n\t\tprint \"This is a file to be read as a list of types\\n\"\n\t\treturn False\n\telif f[-4:] == \"extensions\" :\n\t\tprint \"This is a file to be read as a list of types\\n\"\n\t\treturn False\n\telse:\n\t\tf.filename = f.filename[0:-2] # strip off the leading /\n\treturn True\n\n\ndef getFileExtension(filename):\n\tf = filename[:-3] + \"types\"\n\treturn"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\tpass\n\t\t\t##return os.path.isfile(filename)\n\ndef getFilename(): #return the filename of the current file\n    return os.path.basename(os.path.realpath(sys.argv[0]))\n\ndef getOpenedFileName():\n\treturn fileName\n\n\"\"\"\nCheck whether we have a file for writing\n\nArgs:\n    filename: Path to the file to check\n\nReturns:\n    True if there was a file open and a file exists, False if not\n\"\"\"\ndef checkFileOpen(filename):\n\t\tif os.path.exists(filename):\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\n\ttype1, name1 = os.path.splitext(filename.split('.py')[0])\n\n\tif name1!= type1:\n\treturn False\n\t# Check that it is a Python file\n\tif filename.find('c')!= -1 or filename.find('o')!= -1 or filename.find('cpp')!= -1 or filename.find('txt')!= -1 or filename.find('py')!= -1:\n#\t\tprint \"\\nPython file detected: %s - %s\" % (filename, type1, name1)\n\t\treturn"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef setFileTypes(filename):\n\t\"\"\"Set the file types from the given filename\n\tAssumes that the filename contains only characters allowed\"\"\"\n\t#remove all whitespace before first comma\n\tfor c in filename.strip():\n\t\t\tc = c.strip()\n\t\t\tif not c.isdigit() or ord(c) < 48:\n\t\t\t\tstring = c\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tstring = ''\n\t\t\t\tbreak\n\t#add remaining characters to the filename\n\tpath = os.path.dirname(filename)\n\tif str.find(string,'/')>-1:"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parse_interval(frequency):\n\tif frequency.strip()==\"always\":\n\t\tinterval = datetime.timedelta(days=int(frequency))\n\telif frequency.strip()==\"week\":\n\t\tinterval = datetime.timedelta(weeks=int(frequency))\n\telif frequency.strip()==\"day\":\n\t\tinterval = datetime.timedelta(days=int(frequency))\n\telif frequency.strip()==\"hour\":\n\t\tinterval = datetime.timedelta(hours=int(frequency))\n\telif frequency.strip()==\"minute\":\n\t\tinterval = datetime.timedelta(minutes=int(frequency))"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n        # If the frequency is \"always\" the exception will be raised unless the \n        # frequency could not be parsed:\n        if frequency== \"always\":\n            def parse_timestamp(my_timestamp=None):\n                try:\n                        return datetime.datetime.strptime(my_timestamp)\n                except ValueError:\n                        pass\n\n\t\tif frequency==\"month\":\n\t\t\tdef parse_timestamp(my_timestamp=None):\n                        for fmt in (\"[dD]Y\", \"[dD]\", \"[dD]M\", \"[wW]M\", \"[wW]B\", \"[dD]Y\"):\n                                try:\n                                        return"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef extract(string: str, pattern: str = None):\n\t\"\"\"\n\tExtract a regular expression from a string.\n\tIf a pattern is provided, pattern should be a regular expression,\n\tlike \"3 weeks\".\n\tReturns None if there's no match, or if there are not no matches\n\twith the pattern or with a pattern, where the number if a match\n\tmatches.  Otherwise, an int containing the number of seconds.\n\t\"\"\"\t\n\tmatch = None\n\tregex = None\n\tmatch = pattern\n\tfor match in pattern:\n\t\tmatch = re"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parse_regex(pattern):\n\t\"\"\"\n\t\tGiven a regular expression, return an time.time() instance,\n\t\treturn an instance of datetime.timedelta or None in some cases.\n\n\t If time does not contain a pattern, raise ValueError\n\n\t\tIf the pattern is not in the given duration, raise ValueError\n\t\"\"\"\n\tm = pattern.lower()\n\td = parse_duration(m)\n\tif not d:\n\t\traise ValueError(\"bad time duration pattern: '%s'\" % m)\n\treturn lambda u: datetime.timedelta(weeks = d)\n# Copyright 2015"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\t\n\twith open(frequency, \"rb\") as f:\n\t\tfreq = f.read(1)\n\t\tif len(freq)!= 1:\n\t\t\traise ValueError(\"{0} does not appear to be a valid frequency.\".format(frequency))\n\t\ttry:\n\t\t\thour = int(freq.split(b\"-\")[1])\n\t\t\tif len(hour)!= 1:\n\t\t\t\traise ValueError(\"{0} must be a single number in the range 1-12.\".format(frequency))\n\t\texcept ValueError:\n\t\t\traise ValueError(\"{0} has an invalid frequency.\".format(frequency))\n\t\t\n\t\t"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t \n\ndef parseInterval(interval):\n\t\"\"\"\n\tGiven a string which represents a period of intervals, return a corresponding\nDuration or None if the interval is unknown.\n\nThe parsed duration will be used as a base of the interval and remainder of the\ninterval.\n\nThis function allows both str and Timedelta instances as well as ints.\n\n\"\"\"\n\t# TODO: Check to see if the following has been updated if a problem is\n\t#       found in the module docstring.\n\t# \n\t# TODO: Make sure the string is valid and that it is a valid string\n\t#"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef datetime_or_none(datestr, datetimestr, format=None):\n\t\"\"\"Returns a datetime or None, depending on the date and datetime.\n\nExample usage:\ntimeparser.parse('7 days') --> datetime.datetime(7,7,7)\ntimeparser.parse('1 month') --> datetime.datetime(1916,7,7)\ntimeparser.parse('2 months') --> datetime.datetime(1901,5,1)\ntimeparser.parse('2 weeks') --> datetime.datetime(1917,9,15)\ntimeparser.parse('"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parseDurationString(duration):\n\t\"\"\"\n\tGiven a string duration represented with dots and/or time, return the\n\tproper datetime.timedelta instance, or None if the string is empty.\n\t\"\"\"\n\tif duration == \"\":\n\t\treturn None\n\telif duration == \"\":\n\t\treturn None\n\treturn parseDuration(duration[0:len(duration) - 1])\n\ndef parseDurationUnits(duration):\n\t\"\"\"\n\tGiven a string duration represented using dots and/or time, return the\n\tproper datetime.timedelta instance or None if the duration is None.\n\t"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\t\n\t# Convert to datetime.timedelta if requested\n\tif frequency == \"notspecified\":\n\t\tif \"days\" not in kwargs:\n\t\t\traise ValueError(\"frequency is not specified, but it must be given. \"\n\t\t\t\t\t\"Please provide a frequency.\")\n\t\t\n\t\treturn datetime.timedelta(**kwargs)\n\telif frequency == \"days\":\n\t\tif \"days\" in kwargs:\n\t\t\traise ValueError(\"frequency is not specified, but it must be given. \"\n\t\t\t\t\t\"Please provide a frequency.\")\n\t\t\n\t\tdays = kwargs[\"days\"]\n\t\t\n\t\tif days == \"1\" or days == \"2\":\n\t\t"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef valid_number (frequency) :\n    \"\"\"Function to validate a frequency based on the number in its units.\n\n    Return True if frequency is a supported frequency, else False\n    \"\"\"\n    if frequency.lower() not in supported_frequency_lower :\n\t    logger.error ( \"The given frequency %s needs to be a supported frequency (string)!\"  %(frequency), frequency)\n\t    return False\n\n    for unitmatch in supported_frequency_units :\n      ## check for a known case\n      if frequency.lower()==unitmatch :\n\t    \n\t    ## check for a known suffix\n\t    prefix = unitmatch[0:3"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef getHostIp(host):\n\ttry:\n\t\tsocket.inet_aton(host)\n\t\treturn socket.gethostbyname(host)\n\texcept socket.error:\n\t\treturn False\"\"\"A setuptools based setup module.\n\nSee:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n\"\"\"\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath("}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\t\n\tpattern = re.compile('^\\S+$', re.IGNORECASE)\n\tif pattern.search(host):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef main():\n\tport_mapping = get_port_mapping()\n\t# connect to the socket\n\tserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\tserver_socket.connect((HOST_NAME, PORT_NUMBER))\n\n\twhile True:\n\t\tinput_data = input(\"Enter a port number: \")\n\t\t\t\t\t\t\t\t\t\n\t\thost_string = input_"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ttmp = socket.gethostname()\n\t\treturn (\"localhost\" in host.lower() or \"127.0.0.1\") and temp == tmp\n\ndef get_fqdn():\n\treturn socket.getfqdn()\n\ndef get_ssh_config():\n\t\"\"\"\n\tParses the sshd.conf file\n\t\"\"\"\n\tsshd_ip, sshd_port, sshd_key_file = get_config()\t\n\tif not (sshd_ip and sshd_port and sshd_key_file):\n\t\treturn False\n\telse:\n\t\treturn"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ttry: # checks for the host entry\n\t\t\tif (len(socket.gethostbyname(host)) == 3) & (host[0] == '\"'): # checks for the local host entry\n\t\t\t\treturn True\n\t\t\telse:\t\t# checks for the remote host\n\t\t\t\treturn True\n\t\texcept socket.error: # a socket error occurs\n\t\t\treturn False\n\n\tdef setHostname(self, host):\t# sets the hostname to be used\n\t\t\"\"\"Creates the local host (from the URL as defined): address\"\"\"\n\t\ttry:\n\t\t\tsocket.gethostbyname(str(host))\t#"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n# Functions to access the host socket\ndef connect(host, port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    if port == \"\":\n        return False\n    sock.connect((host, port)) # Connect to the host and port\n    sock.close()\n    return True\n\ndef send_command(command, wait_response=None):\n  \"\"\"Sends a command to the BLE server at host socket\n\n:param command: Command to be sent\n :param wait_response: Command to be executed upon timeout\n\nExample:\nsend_command(\"i"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\treturn (self.hostname == host)\n\n\n\ndef socket_ports():\n\t\"\"\"\n\tDetermines the list of ports used by the remote host\n\tand allows to select the different port for each\n\tconnection\n\t\"\"\"\n\n\tports = []\n\tfor i in range(10000):\n\t\t\tports.append(socket.socket(socket.AF_INET, socket.SOCK_STREAM))\n\n\treturn ports\n\n\n\ndef closeSocket(port):\n\t\"\"\"\n\tCloses the socket connection\n\t\"\"\"\n\t#Closes the socket connection\n\tport.close()\n\n\ndef read"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef check_ipaddress(host, timeout=300):\n\t\"\"\"\n\tChecks if the host is in the target range\n\n:param host: The hostname or ip\n:return: True if the host is in the target\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(host)\n\texcept socket.error, e:\n\t\tprint \"The problem is in the Host Address. \\\n\t\tPlease check the host address and correct the port number  or the port\\\n\t\twith \\\n\t\tcorrect format if you have only 2 octets as the port\"\n\n\n\n#######\n# Main\n"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\treturn 0\n\ndef query_yesno(prompt=''):\n\t\"\"\"\n\tAsk for yes/no for the host\n\n:param yesno: The input yes and no\n:return: True if yes/no, otherwise False\n\t\"\"\"\n\tif prompt:\n\t\tmsg = 'N'\n\t\twhile msg == 'N':\n\t\t\tpass\n\t\tmessage = raw_input(prompt+'\\n'+msg)\n\telse:\n\t\tmessage = ''\n\treturn message\n\t\ndef send_command(command, ip_address = '127.0.0.1', port = 4"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\tif \"::\" in host:\n\t\t\treturn True\n\n\t\ttry:\n\t\t\tsocket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tsocket_file = \"/etc/init.d/init.socket\"\n\t\t\tsocket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tsocket_file_lock = threading.Lock()\n\t\t\tsocket_file_lock.acquire()\n\t\t\t#print \"Socket is created:\"\n\t\t\treturn True\n\t\texcept:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\ndef get_file_path():"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\thost = hostname()\n\t\tif host == \"\":\n\t\t\treturn False\n\t\tif host.find(\":\") == -1 and len(host) > 1:\n\t\t\thost, port = host.split(':')\n\t\telse:\n\t\t\thost, port = host.split(':')\n\t\tif (len(host) == 0 or int(port) == 0) and \":\" in host:\n\t\t\treturn False\n\t\treturn True\n\ndef start_server():\n\t\"\"\"\n\tBegins up the server\n\n:return: True if the server is running and can execute tasks. In order to execute this server as"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tpaths=[]\n\t\tfor p in pattern_list_to_regex:\n\t\t\t# TODO: refactor the whole expression to include more stuff when readily checking syntax.\n\t\t\tp = path_from_pattern(p)\n\t\t\tif os.path.isdir(p):\n\t\t\t\tpaths.extend(glob.glob(os.path.join(p, \"*\")))\t\n\t\t\telse:\n\t\t\t\tpatterns_from_pattern = patterns_from_pattern(p)\n\t\t\t\tif patterns_from_pattern:\n\t\t\t\t\tpaths.extend(glob.glob(patterns_from_pattern))\t\t\t\t\t\n\n\n"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\t\n\treturn re.match(r\"^(.*)(\\.\\.\\.)?$)\", pattern)\n\n#--- LISTENERS ---------------------------------------------------------------------#\n\n#TODO:\n\"\"\"\nReturn a list of all unique paths in a path. These are built, so that\n#each entry in the list is unique in the order in which they occur.\ndef unique_paths(paths):\n\t\"\"\"\n\tReturn a list of the unique paths in paths.\n\t\"\"\"\n\treturn [p for p in paths if unique(glob.iglob(p))]\n\n#--- LISTENER ---------------------------------------------------------------------#\n\ndef path"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\nclass FindFlags(object):\n    \"\"\"\n    Represents the flags used to filter the source paths for include-patterns.\n\n    This class takes a list of path fragments as it is called with\n    the following arguments:\n\n      a. Path fragments to add to the list (this is the same as the one-off\n              list built by find_paths called with the expanded path (a.k.a.\n              glob patterns), as expected).\n\n      b. Path fragments to not add to the list (this is the same as the one-off\n              list built by find_paths called with the expanded path (b.k.a"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tre.compile(r'\\w*::\\w*')\n\t\treturn\n\n\"\"\"\n\tFunction to filter and remove paths in the front of each path to make\n\ta pattern that is to be removed so that we can correctly extract it.\n\t\"\"\"\n\tdef _remove(self, s):\n\t\tr, e = os.path.split(s)\n\t\treturn re.sub(r+'$', '', match.group(0))\n\n\"\"\"\n\tFunction which gets the list of directories, which is a list of paths.\n\t\"\"\"\n\tdef _get_dir_list"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\nclass Matcher:\n\tdef __init__(self, patterns):\n\t\tself.patterns = []\n\t\tassert patterns, \"No patterns to matcher at %d\" % len(patterns)\n\t\tself.include = []\n\t\tself.exclude = []\n\t\tself.order = []\n\t\tself.not_matcher_re = re.compile(r'\\b([a-zA-Z_0-9]+)\\s*(.*?)[\\s,]+.*[\\s:]\\s*(.*)')\n\t\tfor p, p_str in zip(patterns, patterns[1:]):\n\t\t\tself."}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t self.make_paths_absolute(pattern):\n\t\t\tpaths_by_path = path_to_paths_by_pattern(pattern)\n\t\t\toutput = []\t\t\t\n\t\t\tfor path in paths_by_path:\n\t\t\t\tmatches = []\n\t\t\t\tfor glob_pattern in glob.glob(path):\n\t\t\t\t\tmatches.extend(glob.glob(glob_pattern))\n\t\t\t\toutput.extend(matches)\n\t\t\t\t\n\t\t\treturn output\n\n\tdef find_paths_by_pattern(self, pattern):\n\t\tpaths = []\n\t\tfor pat in self.glob_patterns:\n\t\t\tif os."}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n__pat_split_pat = re.compile(r\"(?<!\\\\)//\")\n\ndef _pat_split(pat):\n\t\"\"\"\n\tGiven an iterable which matches a glob patributes pattern, yield each fragment on a new\n\tlist, in order.\n\t@param pat: list of strings\n\t@type pat: iterable\n\t@return:\n\titerator\n\t\"\"\"\n\tin_list = []\n\tbuf = []\n\tfor i, p in enumerate(pat):\t\t\n\t\tif p in ('.', ''):\n\t\t\tin_list.append(i"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n    m = re.match(r'\\.(%s)$' % '|'.join(patterns))\n    pattern = \"%s|%s\" % (m.group(2), _re_escape(patterns.values()))\n    return re.compile(pattern)\n\n\"\"\"\n__all__ = [\"get_paths\", \"match_path_patterns\"]\n\"\"\"\n\ndef get_paths(paths, prefix=\"\", suffix=\"\", expandable=True):\n    \"\"\"\n    Converts sequence of path objects using `glob.glob` for inclusion into paths.\n\n    >>> list(get_paths([\"foo.txt\", \"bar.txt"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\t\n\tdef is_relative(path):\n\t\tif '/' in path:\n\t\t\tpath_parts = path.split('/')\n\t\t\tfor part in path_parts:\n\t\t\t\tif not part: return False\n\t\t\t\telif part[0] == '/':\n\t\t\t\t\tif part[-3:].startswith('*'): return True\n\t\t\t\telif part[-3:].startswith('/'):\n\t\t\t\t\treturn False\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\t\n\tdef process(path):\n\t\tif is_relative(path): prefix = ''\n\t\telse: prefix = os.path.join(get_root_path(), path)"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n            pattern_re = re.compile(r\"\"\"\n                \\*(?<=\\S)\\*\\s+(?P<expr>.*?)(?P=expr)\n                \"\"\", re.VERBOSE)\n \n        match_fragments = []\n        for fragment in pattern_re.finditer(content):\n            match_path = urlparse(fragment.rstrip('/'))\n            match_fragments.append(match_path)\n\n    print(\"Pattern matches: %s\" % match_fragments)\n\n    return matches\n\n\nclass NmapArgs(object):\n    def __init__(self, *args):\n        self.args = args\n\n    def __call__(self,"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n\n\tdef execute(self,command):\n\t\twith cd(\"code/\"):\n\t\t\tself.execute_process(command)\n\t\"\"\"\n\tcreates or updates remote repository\n\n:param branch: the branch name inside the remote repository\n:param remote: the remote hostname/path\n\n:return: True if operation success\n\n:rtype: boolean\n\"\"\"\n\t\n\n\tdef execute_process(self,command):\n\t\tcmd = \"git-core -q -a\"\n\t\tif(command is \"init\"):\n\t\t\tcmd += \" init\"\n\t\telif command is \"fetch\":\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef is_git():\n\treturn _git\t\n\ndef get_git_name():\t\n\t\"\"\"\n\treturns a path to an existing git repository\n\t\t\n\tnote:\n\t\t_get_git_name is slow if we're running on a regular git repo, so try it before returning\"\"\"\n\treturn os.path.join(os.path.dirname(os.path.realpath(__file__)), \"modules\",\"GitScripts\", _get_git_name())\t\n\t\t\ndef get_dir_names():\n\t\"\"\"\n\treturns all the relevant dir names, based on the current working"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef get_repo_path() :\n\t\"\"\"Return path to project dir\n\t\"\"\"\n\treturn os.path.dirname(os.path.dirname(os.path.abspath(__file__)));\n\n\ndef init():\n\t\"\"\"initialize environment and git config file\"\"\"\n\tos.chdir(os.path.dirname(os.path.realpath(__file__)))\n\treturn True\n\ndef update_git_repo():\n\t\"\"\"update git repo\"\"\"\n\tgit = get(get_repo_path(), 'git')\n\n\tif git == '':\n\t\tgit = create_git("}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n# Checks whether the current working directory is a Git\ndef check_git(f):\n\tif(os.path.isfile(f)): \n\t\treturn True\n\telse:\n\t\treturn False\n\n# Returns path and filename of project and binary\ndef extract_project(bin_path, binary):\n\treturn os.path.join(bin_path, '{0}.sh'.format(binary))\n\t\n# Returns a list of available modules\ndef get_available_modules(path):\n    return [os.path.basename(f)[:-3] for f in os.listdir(path"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef is_python():\n\treturn (platform.python_implementation() == 'PyPy')\n\t\t\ndef init():\n\t\"\"\"checks dependencies\n\t@type: bool\"\"\"\n\tif not os.path.exists(VERSION_CHECK_FILE):\n\t\tprint(\"Need the latest version of the project (gittag.py) to continue\")\n\t\texit()\t\n\t\t\t\n\tif not is_python():\n\t\tprint(\"Need the latest version of the project (python-gittag-version.txt), please change versions and try again\")\n\t\texit()\t\n\t\t\t\n\n\texec_path,"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef os_init():\n\t\"\"\"\n\tfunction to set up a os\n\t\"\"\"\n\tif os.name == 'nt':\n\t# Windows\n\t\tpwd = os.path.join('~', '.my_password')\n\t\tos.mkdir(pwd)\n\t\tos.chmod(pwd, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO); \n\t\tos.system('ln -s '+pwd)\n\t\treturn True\n\t\n\t# OS X\n\t\tif os.name == 'darwin':\n\t\t\tos.system('open"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n_homedir = os.path.expanduser('~')\n\ndef home(file_name='None'):\n\t\"\"\"\n\tIf you are on Windows or on Cygwin, then the user is considered owner\n\tand the home directory is returned. Otherwise the script will look for\n\teither \"git\" or \"hg\", and the path to it (which will be set to a\n\tdirectory with the program's configuration, if it exists) to execute\n\tas if it were a git repository.\n\n\t:return: Absolute path of the executable.\n\t:raise FileNotFoundError: If no executable exists"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\treturn os.path.exists(\"/home/eric-bender/git-history\")\n\n\n\n\telif os.name == 'posix':\n\t\t# /usr/bin/git-history <PATH> -- <git-name>\n\t\treturn os.path.exists('/etc/git-history')\n\n\n\nclass GitHistory:\n\t\"\"\"\n\tRepresents a git history file\n\t\"\"\"\n\n\tdef __init__(self, filename):\n\t\t\n\t\tself.filename = filename\n\n\t\tif os.path.exists(self.filename):\n\t\t\ttry:\n\t\t\t\tself.f = open(self."}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\nclass J(object):\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\tself.sha = None\n\t\tself.author = 'Unknown'\n\t\tself.email = ''\n\t\tself.date = ''\n\t\tself.date_time = ''\n\t\tself.commit = ''\n\t\tself.description = ''\n\t\tself.parent = None\n\t\tself.submodules = ''\n\t\tself.submodules_type = ''\n\t\tself.git = ''\n\t\n\tdef set_sha(self, sha):\n\t\tself.sha = sha\n\t\n\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\tcmd_output = shell('git -C %s' % current_directory_setting)\n\t\tcmd_output=cmd_output.rstrip('\\n')\n\t\tcmd_output = cmd_output[:-1]\n\t\tif not cmd_output[-1] == '\\n':\n\t\t\tcmd_output = cmd_output + \" 1>%s\" % cmd_output[-1]\n\t\t\tf = open(\"%s/current/settings.properties.bash\" % current_directory_setting, \"w\")\n\t\t\tf.write(cmd_output.replace('settings._%s_SHELL' %"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n\t# Check if file exists.\n\tif not os.path.isfile(target_config):\n\t\t# Create a temporary directory for overwriting.\n\t\tif os.path.exists(target_config):\n\t\t\traise IOError (\"Existing target config file exists: %s\" % target_config)\n\n\t\t# Try creating target config file.\n\t\tf = open(target_config, 'w')\n\t\tf.write( yaml.dump(data))\n\t\tf.close()\n\n\tif not os.path.isfile(target_config):\n\t\traise IOError (\"Target configuration file not found"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ndef validate(target_filename, src_dir):\n\t\"\"\"Validate src_dir against target_filename, validate it, return whether it's valid.\"\"\"\n\ttry:\n\t\twith open(target_filename, \"rt\" if os.path.islink(target_filename) else \"r\") as f:\n\t\t\ttarget_contents = yaml.load(f.read())\n\t\t\t# Validate all provided files\n\t\t\tfor name, content in target_contents.iteritems():\n\t\t\t\tvalidate(os.path.join(src_dir, name), os.path.join(dst_dir, name"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\t\n\t# create the output file\n\tif not os.path.exists(config_output_dir):\n\t\tos.makedirs(config_output_dir)\n\t\t\n\t# copy out config file\n\twith open((os.path.join(config_output_dir, \"local.yaml\"))):\n\t\t# copy and write file\n\t\tshutil.copyfile(os.path.join(config_output_dir, \"local.yaml\"), \\\n\t\t\tos.path.join(config_template_dir, str(uuid.uuid4())))\n\t\t\n\t# create file if it doesn't exist yet"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ttry:\n\t\t\tfd, target_file_name = tempfile.mkstemp(suffix=\".yml\")\n\t\texcept IOError as e:\n\t\t\terrors.append((e.filename, \"Could not create the file, %s\") % e.strerror)\n\n\t\ttry:\n\t\t\tos.close(fd)\n\t\t\tself.clean(target_file_name)\n\t\t\tself.run_targets.append((target_file_name, target.CONFIG_FILE))\n\t\tfinally:\n\t\t\ttry:\n\t\t\t\tos.remove(target_file_name)\n\t\t\texcept:\n\t\t\t\tpass\n\n\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\nclass CardWriteableFile:\n\n\t\"\"\"CARD class that writes all the content in the specified target.\n\tWill attempt to update the target's version if the version information is available,\n\trebuilding config if necessary. Also, write to config.yml if exists and write to \nconfig.yaml if does not exist and write to config.yml. If canceled within the run, no files\nwill be written.\"\"\"\n\n\tdef __init__(self, targetFile, targetType=None):\n\t\tself.targetType = targetType if targetType else CardFormat.name(targetFile"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n\tprint(\"Config File:\", config_filename + '...')\n\tprint(\"Options: \")\n\tprint('  ')\n\tif options.output_path:\n\t\tprint(\"Output path:\", options.output_path)\n\tif options.templates_path:\n\t\tprint(\"Templates path:\", options.templates_path)\n\n\tif options.config_path:\n\t\tprint(\"Config file:\", options.config_path)\n\tprint(\"Template path:\", options.template_path)\n\n\t# make sure target file exist, if not, create it\n\tif options.target_file"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ndef config_dir_exists(config_dir_name):\n    \"\"\"\n    Check if <config_dir_name> appears in output of \"mkdir *.yml\" in directory <output_file> exists and write if found.\n    \"\"\"\n        \n    if not os.path.isdir(config_dir_name):\n        err_msg = \"\\n***WARNING: No config directory %s found at %s.  Creating it, and creating this directory.\" % (config_dir_name, config_dir_name)\n        print '\\n***ERROR:'+ err_msg\n        create_error_file(config"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t \n\ttry:\n\t\tos.makedirs(dataPath+'/%s/data/' % target.lower(), exist_ok=True)\n\texcept OSError:\n\t\tpass\n\twith open( os.path.join( dataPath, 'config.yaml'), 'w', encoding='utf-8') as cf:\n\t\tcf.write(yaml.safe_dump( target, default_flow_style=False))\n\tif os.path.exists( dataPath):\n\t\traise IOError\n\telse:\n\t\tos.makedirs( dataPath )\n\t  \n\t\n\treturn( target, dataPath"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\tfname = target + '.yaml'\n\t\tif os.path.exists(fname):\n\t\t\tif overwrite:\n\t\t\t\tprint \"ERROR: File already exists\"\n\t\t\treturn False\n\t\tos.makedirs(fname)\n\ndef validate_settings(settings):\n\t\"\"\"\n\tCheck that settings.txt and settings.yaml exist. Raise an Exception at appropriate error level.\n\t\"\"\"\n\t\n\tif not os.path.isfile(settings['settings_yaml']):\n\t\treturn False, \"Settings file doesn't exist: \" + settings['settings_yaml']\n\t\n\tif not os.path.isfile("}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n\t# Load configuration file and create it if it doesn't exist\n\tfd = open(os.path.join(basepath, 'config.yaml'), 'rt', encoding='utf-8')\n\tcontent = fd.read()\n\tif content:\n\t\tfd = open(os.path.join(basepath, 'config.yml'), 'wt', encoding='utf-8')\n\t\tfd.write(content)\n\t\tfd.close()\n\n\t# Render config tree\n\tconfig_tree = []\n\tfor arg_dict in arg_dict_list:\n\t\tconf"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\telse:\n            #print \"Script format: \" + script_cmd\n            return \"%s >> %s\\n\\n%s\\n\\n\" % (script, script_cmd, script_cmd)\n\ndef save_to_script(filename, script_data):\n\twith open(filename, \"w\") as f:\n\t\t#print \"file \" + filename\n\t\tf.write(script_data)\n\tprint \"Saved %s script file to %s\" % (filename, filename)\n\ndef make_cmd_to_list(script_data, shell_"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\treturn script.strip()\n\ndef command_line_script(script):\n\t\"\"\"\n\tthis script is the main line script that\n\tis called by the shell\n\t\"\"\"\n\treturn script\n\n@command_line_script\t\ndef run_command_line_script(script, user_arguments=''):\n\t\"\"\"\n\trun a command line script that passes\n\tinto all the commands specified.\n\tIt takes one or more user_arguments, and returns one\n\tcommand that will be executed as\n\ta shell script in the form\n\t\"RUN_ALL\" where #"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\n\tcmdLines=line_sep.join(line_lst)+line_end.join(line_lst)+line_end.join(\"\\n\")\n\treturn cmdLines\n#end def convert\n\n\ndef build_commands(commands,path):\n\tfinal=\"\"\"commands=[%s \\n\"\"\" % indent(commands)\n\tindent(final)\n\tfor cmd in commands:\n\t\tfinal+=indent(cmd)\n\tfinal+=\"\\n\"\n\treturn final\n#end def build_commands\n\n\n# Main function is called if we run on the main script\n# in the right"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\treturn \"\"\n\n\nclass CmdLineAction(CmdLine):\n\tdef __init__(self, cmd, desc, shortdesc=None, **kwargs):\n\t\t\"\"\"\n\t\tdesc:\n\t\t\tdesc... description to show to user\n\t\tshortdesc:\n\t\t\tshort description of this cmdline\n\t\t\"\"\"\n\t\tCmdLine.__init__(self, *[CmdLineActionBase.__init__, CmdLineAction], **kwargs)\n\t\tself.add_arg(cmd, desc=desc, shortdesc=shortdesc, **kwargs)\n\n\tdef handle_line(self, line):\t\t\t# overridden"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tself.returnVal = script\n\tdef __str__(self):\n\t\tif hasattr(type(self), \"__name__\"):\n\t\t\treturn \" \".join((cmd.strip() for cmd in self))\n\t\telse:\n\t\t\treturn str(self)\n\tdef __add__(self, l):\n\t\tif hasattr(type(l), \"__name__\"):\n\t\t\treturn \" \".join((cmd.strip() for cmd in l))\n\t\telse:\n\t\t\treturn \" \".join((cmd + \" \" + str(self[l])) for cmd in l))\n\tdef __radd__(self"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\ndef main():\n\n  usage = \"usage: %prog <script> <options> [args]\"\n\n  parser = OptionParser(usage=usage)\n\n  parser.add_option(\"-p\", \"--script\", dest=\"path\", help=\"script location\")\n  parser.add_option(\"-r\", \"--rpt\", dest=\"rpt\", default=None, help=\"rpt server\")\n\n  (options, args) = parser.parse_args()\n  if not options.path:\n    options.path = 'none' #use default for script location\n\n  try:\n    server = TelnetServer(int(options."}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\n\treturn script\n\t\t# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom future import standard_library\nstandard_library.install_aliases()\nfrom builtins import *\nfrom builtins import range\nfrom past.utils import old_div\nfrom builtins import object\nstandard_library.install_aliases()\n\nimport cv2\nimport copy\n\n\nif cv2.__version__[0] == '2':\n    import urllib\n\n\nimport glob, threading"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\treturn self.run_with_input(script)\n\tdef run_with_input(self, script):\n\t\t\"\"\"\n\t\texec given script file and return the output\n\t\t\n\t\targuments:\n\t\t\tscript: string\n\t\t\"\"\"\n\t\tp= self.run(script)\n\t\treturn p.splitlines()\n#\tdef run(self,script):\n#\t\tret = call(script,shell=False,stdout=self.log,verbose=True)\n#\t\tif ret!= 0:\n#\t\t\traise ValueError(\"Script is not formatted correctly\")\n#\t\n#"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\treturn self._runCommand(script)[0]\n\tdef _convertCommands(self, commands):\n\t\t# convert commands to lists of lines\n\t\tcommandList = []\n\t\t# iterate over each line, removing whitespace\n\t\tfor command,cmd2,rest in commands:\n\t\t\tif not cmd2==\"\" and cmd2[-1]==\"\\n\":\n\t\t\t\tcommandList.append(cmd2[:-1])\n\t\t\telse:\n\t\t\t\tcommandList.append(cmd2)\n\t\t\n\t\treturn commandList\n\tdef _runCommand(self, cmd):\n\t\t# returns first line of command as a"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tsys.stdout.write(script+\"\\n\")\n\t\treturn\n\tdef write_script(filename, script):\n\t\tcontent = open(filename).read()\n\t\tscript = textwrap.dedent(script)\n\t\tcontent = re.sub('##START_SCRIPT##', content, script)\n\t\topen(filename,'w').write(script)\n\t\tprint(\"script written to %s\" % filename)\n\t\treturn\n\n\n\n\n\nscript_source \t\t=\"\"\t\ncommand, script_args = parse_arguments(sys.argv)\n\ninputfile \t\t=sys.stdin\noutput"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef cleanUp(path, filename, env):\n    \"\"\"Removes the generated file. \"\"\"\n\n    if env[\"return\"] > 0:\n        os.remove(filename)    \n    \n# ----------- main loop --------------------------------\ndef read_template(fname):\n    \"\"\"Reads the template file.\"\"\"\n\n    f = open(fname, \"r\")\n    try:\n        text = f.read().replace(\"$TEMPLATE\", jinja2.Template(filename=fname))\n    finally:\n        f.close()\n    return text\n\ndef add_files(path, files, env):\n    \"\"\"Adds the specified files to the set of.py"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\n\tp = sub.Popen(func, **args)\n\tp.wait()\t# Wait just before making sure the program terminates.\n\tret = p.returncode\n\tif ret!= 0:\n\t\traise ValueError(\"Command failed!\")#!/usr/bin/python\n#\n# Copyright 2012 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache."}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef execute(cmd, **kwargs):\n\t\"\"\"\n\tExecute a command and/or return its stdout.\n\n\tReturn an iterable of (returncode, stdout, stderr) tuples.\n\t\"\"\"\n\tif'stdout' in kws:\n\t\tstdout = kws.pop('stdout')\n\t\tdel kws['stdout']\n\t\n\tp = subprocess.Popen(cmd,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.STDOUT,\n\t\tbufsize=-1,\n\t\tcwd=kwargs.pop('cwd', '.')\n\t)\n\t(out, err) = p.communicate"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run(*args, **kwargs):\n\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run_process(*args, **kwargs):\n    \"\"\"\n    Run a function in a sub-process.\n\nParameters"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\t\n\timport shlex\n\timport os\n\timport subprocess\n\tfrom numpy import load, shape, arange, arange_, log10, isscalar\n\tfrom IPython.core.magic import  BashCompleter\n\tfrom IPython.config.configurable import Configurable\n\tfrom IPython.core.magic_arguments import (argument, argument_group, argument_spec, block_cell_like, \n                                                  parse_argstring)\n\tfrom IPython.core.magic_arguments import (argument, argument_group,argument_spec, block_cell_like, \n                                             parse_argstring)\nexcept"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run_process(func, args, **kwargs):\t\n\tif(func == None):\n\t\tsubprocess.check_call(args, **kwargs)\n\telse:\n\t\tsubprocess.check_call(func, shell=False, **kwargs)\n\n# Main code\nif __name__ == \"__main__\":\n    p = subprocess.Popen([\"/bin/bash\", \"-c\", \"exec mymodule/module/process_run\"],stdout=subprocess.PIPE)\n    while True:\n\t\tline = p.stdout.readline()\n\t\tresult = p.poll()\n\t\tif("}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run_subprocess (func, args, extra_env, logger, popen_kwargs={}):\n\t\"\"\"Run a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n    The function to be run.  It must be in a module that is importable.\n\t*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\n\tif \"env\" in vars(func):\n\t    env = dict"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\t\n\t# Run the function in a sub-process.\n\tout = subprocess.Popen( func, **extra_env )\n\t\n\t# Get the exit code.\n\trc = out.wait()\n\n\treturn rc, out.stdout.read()\n\ndef call(cmd, **kwargs):\n\t\"\"\"Run a command in an unused process.\"\"\"\n\t\n\tout = subprocess.Popen( cmd, **kwargs )\n\t\n\t# get it back.\n\treturn out.stdout.read()\n\n#-------------------------------------------------------------------------------\n\n# The following two functions will be called by the server to send a \n# response"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n# A set of functions that are required by the tests\ntests = [\n\t'c_sum',\n\t'd_sum',\n\t'd_min',\n\t'd_max',\n\t'f_a',\n\t'f_b',\n\t'g_sum',\n\t'g_min',\n\t'g_max',\n\t'h_sum',\n\t'h_min',\n\t'h_max',\n\t'hh_sum',\n\t'hh_min',\n\t'hh_max',\n\t'hh2_sum',\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef check_call(*args, **kwargs):\n\t\"\"\" Run the function with arguments, return the Popen object if\n\tthere are any errors. \"\"\"\n\t# Python <3 uses a buffer to store the command string buffer\n\t# of stdout and stderr\n\tif'stdout' in kwargs and'stderr' in kwargs:\n\t\tstdout = kwargs['stdout']\n\telse:\n\t\tstdout = subprocess.PIPE\n\tif'shell' in kwargs:\n\t\tshell = kwargs['shell']\n\telse:\n\t\tshell = True\n\tif 'universal_newlines' in kwargs:\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tif not re.search('^\\w+$',name):\n\t\t\tif defaultValue is None:\n\t\t\t\traise Error(\"variable name must be specified\")\n\t\t\tif not defaultValue[-1] == \"\\\\\":\n\t\t\t\tdefaultValue = \"\\\\\"+defaultValue\n\t\t\tprint(\"Error : Unrecognized variable name '\"+name+\"'\", file=sys.stderr)\n\t\t\tsys.exit(1)\n\t\treturn os.environ.get(name, defaultValue)\n\t'''\n\tParses name from environment variables and returns the appropriate value for the variable name.\n\t'''\n\tif IsDir(name, dirFlag):\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\n\tenv = \"DISPLAY\"\n\n\treturn env\n\n\ndef get_os(configfilename):\n        \"\"\"\n        Return the os value from environment if not set.\n        If the variable is not defined in environment and no default value is provided, an Error is raised.\n        \"\"\"\n        try:\n                return os.environ[\"DISPLAY\"]\n        except KeyError:\n                logger.info(\"Cannot get the value in the environment\")\n                return \"\"\n\n\nclass System(object):\n        \"\"\"\n        A wrapper system class.\n        \"\"\"\n\n\tdef __init__(self, configfile):\n\n        #read configfile\n        try:\n                with open("}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tret = self.env_config.get(var, None)\n\t\tif ret is not None:\n\t\t\tif var == \"APPNAME\" or ret == \"\":\n\t\t\t\terror(\"Variable %s was not defined in %s env variable %s\" % (var, os.getcwd(), var))\n\t\t\telif var in os.environ:\n\t\t\t\treturn os.environ[var]\n\tdef get_value(self, var, default=None):\n\t\t\"\"\"\n\t\tGet the value of the environment variable VAR\n\t\tIf the variable is not defined in environment and no default value is provided, an"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t return self.__env[env_name] if env_name in self.__env else  default\n\n\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2015, Alcatel-Lucent Inc, 2017 Nokia\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\ttry:\n\t\t\treturn os.environ[os.environ.get(env,var)]\n\t\texcept KeyError:\n\t\t\tif env == None:\n\t\t\t\tenv = os.environ\n\t\t\telif os.path.isfile(env):\n\t\t\t\tenv = env.strip(\"\\\"'\")\n\t\t\treturn os.environ[env]\n\n\tdef get_default(self,env=None,defaultValue=None):\n\t\t\"\"\"Returns a default value when env is not specified in env otherwise returns a default value.\"\"\"\n\t\tenv = env or os.environ\n\t\tdefault = self.get_default_var("}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tval = os.getenv(name.replace(' ', '\\ '))\n\t\tif val == None:\n\t\t\traise Error('Environment variable'+ name +'could not be found in environment')\n\t\treturn val\n\t\ndef getenv_path(env_var_name, default_value):\n\t\"\"\" Get the value from environment variable matching a name and an optional default value.\t\"\"\"\n\tif env_var_name in os.environ:\n\t\tval = os.getenv(env_var_name, default_value)\n\t\tif not os.path.isfile(val):\n\t\t\traise Error"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\ttry:\n\t\t\treturn os.environ[env_match.group(1)]\n\t\texcept KeyError as e:\n\t\t\traise InterpreterException('Undefined variable %s' % e)\n\n\tdef set_command_line(self, command_line):\n\t\t\"\"\"Sets the command_line parameter for ShellCommands instances\"\"\"\n\t\tshell_args = command_line.split()\n\t\tself.shell.push(tuple(shell_args))\n\n\tdef set_output_to_shell(self, output_to_shell):\n\t\t\"\"\"Set the output_to_shell property\"\"\"\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tvar = getVar(match, vars)\n\t\tvalue = getVar(match, default)\n\t\tcheckValue(var, value, env, self.__env)\n#!/usr/bin/env python\n#\n# Copyright 2009 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tvalue = None\n\t\tif matcher is None:\n\t\t\treturn value\n\n\t\tregex = matcher.search(value)\n\t\tif not regex and name is not None:\n\t\t\tname = pattern.sub(r'\\A'+name, value)\n\n\t\tif regex:\n\t\t\tmatcher = None\n\t\t\tvalue = regex.group(1)\n\n\t\tif value is None:\n\t\t\tvalue = pattern.sub(r'\\A'+name, value)\n\n\t\treturn value\n\ndef find_properties(properties, name=None, prefix='!', error_handler=None, error_message=None, **"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tenv_var = self._find_env_variable()\n\t\tif not env_var:\n\t\t\traise Error(\"Environment variable not found.\")\n\n\t\t# Check to see if the variable is in the environment\n\t\tif env_var not in os.environ:\n\t\t\traise Error(\"Environment variable does not exist.\")\n\n\t\t# Search for the specified value in the default (if it exists)\n\t\tdefault_value = os.environ[env_var]\n\n\t\tif default_value is not None:\n\t\t\treturn default_value\n\n\t\t# If the default value does not exist\n\t\traise Error(\""}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\t# urlparse.urlparse() isn't compatible with stdlib.  TODO: find a less hacky way...\n\t\timport urlparse\n\t\tparsed = urlparse.urlparse(image_href)\n\t\tif parsed[0]!= 'http':\n\t\t\traise ValueError(\"Invalid image: %s\" % image_href)\n\t\tscheme = parsed[1]\n\t\tnetloc = parsed[2]\n\t\tuse_ssl = parsed[3]\n\n\t\tif use_ssl == \"1\":\n\t\t\traise ValueError(\"Cannot use a GIF URL with GIF support (experimental).\")\n\n\t\t# We don't have access to"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\timage_url = urlparse(image_href)\n\t\tnetloc = image_url.netloc\n\t\tuse_ssl = True\n\n\t# If we still don't find the image, we are only using parts\n\tif not image_url:\n\t\treturn (\"\", netloc, use_ssl)\n\n\tparts = urlparse(image_url)\n\n\t# We want to replace the scheme and host with a netloc.\n\timage_url = '%s://%s' % (parts[1], parts[2])\n\n\tif netloc == image_url.netloc:\n\t\treturn (\"\", image_url.scheme"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef parse_path(url, options=None):\n\t\"\"\"Parse a URL of an external image/image.\n\n\t:param url: Source image url\n\n\t:param options: A dictionary that can contain any of these keys:\n\t\t\t\t\t'include_in_raw', 'include_in_expanded', 'count', etc.\n\t\t\t\t\t'max_width' -> the integer width of the mature\n\t\t\t\t\t'keep_exif' -> whether to keep exif data or not\n\n\t\t\"\"\"\n\t\n\tif 'include_in_raw' in options:\n\t\treturn parse_bool(options['"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\n\tif not image_href.startswith(\"http\"):\n\t\t# No protocol is specified, so see if there is no url\n\t\tscheme, netloc, path, params, query, fragment = urlparse.urlparse(image_href)\n\t\tscheme = scheme.lower()\n\n\t\tif scheme == 'https':\n\t\t\t# No path?!\n\t\t\treturn image_href, False, False, False\n\n\t\t# We have an image!\n\t\treturn None, True, False, False\n\n\telse:\n\t\t# No relative path --> let's assume it's a relative path\n\t\treturn image_href,"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef extract_images_from_tar(archive, tar, file_prefix, use_ssl):\n\timages = list()\n\n\t# unpack path:../../../..//archiversarya:9\n\tif tarfile.is_tarfile(archive):\n\t\tfor member in tar.getmembers():\n\t\t\tmembers = getattr(member, 'getmembers')\n\t\t\tfor member_path in members:\n\t\t\t\tmemberspath = os.path.join('/', member_path)\n\t\t\t\tmembers_path = os.path.abspath(memberspath)\n\t\t\t\tmember[1] = {}\n\t\t\t\tmembers_path"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef get_full_netloc(image_href, options):\n\t\"\"\"\n\tParse an image href into netloc.\n\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\t\"\"\"\n\tif not options:\n\t\treturn image_href, None\n\n\ttry:\n\t\tnetloc = options[0]\n\t\tuse_ssl = options[1]\n\texcept Exception:\n\t\tuse_ssl = True\n\treturn image_href.split(\"@\", 1)\n"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef get_image_url(image_soup):\n\t\"\"\"Retrieve the url for the given image \"\"\"\n\timage_id = image_soup.find('img')\n\treturn image_url(image_id)\n\ndef get_image_tags(image_soup, tag_name):\n\t\"\"\"Extract and return the image tag and its text content.\n\tKeyword arguments:\n\ttag_name -- Name of the tag to extract,\n\tdefault value: The tag name (using BeautifulSoup default is used)\n\treturns: a tuple: a tag and its text content\n\texample"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\tnetloc = None\n\t\tuse_ssl = True\n\t\t\n\t\tparts = urlparse.urlparse(image_href)\n\t\tif parts.hostname:\n\t\t\tnetloc = parts.hostname\n\t\t\tuse_ssl = False\n\t\tif netloc is not None:\n\t\t\treturn (netloc, parts.path, use_ssl)\n\t\t\n\t\traise ValueError\n\n\timage_id = image_href\n\tnetloc, path, use_ssl = urlparse.urlparse(image_href).netloc, urlparse.urlparse(image_href).path, urlparse.urlparse(image_href).use_ssl\n\tnetloc = netloc.split('@"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\nclass ImageResolver(urlresolver.ReverseResolver):\n\t\"\"\"\n\tA resolver which resolves urls of images into their\n\tcomposite parts.\n\t\"\"\"\n\n\t# Note: The order unifier needs to be stable.\n\t_url_re = re.compile(r'^[0-9A-Za-z@:%_\\-]{4,64}$')\n\t\n\tdef __init__(self, image_href, max_retries=5, retry_wait=1, proxies={}, filename=None):\n\t\t\"\"\"\n\t\t:param image_href: href of an image\n\t\t:param max"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef build_query_from_url(url):\n\t\"\"\"Parse an image url into a dict of query parameters. Only returns\n\turl if the image is found, otherwise returns None.\n\n:param url: (URL) an image url, e.g. \"http://www.example.com/image_1.png\"\n\t:returns: {'image_id': image_id, 'netloc': netloc, 'use_ssl': use_ssl}\n:raises urllib2.URLError:\t\"\"\"\n\turl_parts = urlparse(url)\n\tusername = None\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    if len(input_list) < 1:\n        raise TypeError(\"\"\"input_list_to_str(input_list: list, delimiter='\\t')\"\"\")\n\n    input_list = input_list + ['*']\n    return input_list\n\ndef count_lines(file_list):\n    print 'Count lines:'+ str(file_list)\n    files = []\n    counter = 0\n    for file in file_list:\n        files.append(file)\n        counter += 1\n    return counter\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n#"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n\n\ndef create_dir(input_dir, mode=0o777, verbose=False):\n\t\"\"\"create empty directory or create one.\n\n\tArgs:\n\t\tinput_dir: directory (strings)\t\t\n\t\tmode: 0o777\t\tcreate directories\n\t\tverbose: if True, print output in stdout as well, default is False\n\t\"\"\"\n\t# create directory\n\tif not os.path.exists(input_dir):\n\t\tos.makedirs(input_dir) # create parent\n\tif mode:\n\t\tmode = os.stat(input_dir).st_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef get_filename(input_path):\n\t\"\"\"Takes the string list and removes trailing os seperator characters\"\"\"\n\tfilename = []\n\n\tif len(input_path) > 0:\n\t\tfor i in range(0, len(input_path)):\n\t\t\tfilename.append(input_path[i:i+1] + input_path[i+1].replace(os.path.sep,'.').replace(os.path.sep,'.').replace(os.path.sep,'.'))\n\t\t\n\treturn filename\n\ndef normalize_path(path):\n\t"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef list_dirs(in_list):\n\t\"\"\"\n\tGet the directories where the strings in the list are\n\tlocated.\n\n\tArgs: \n\t\tin_list: list of strings\n\t\n\tReturns:\n\t\tlist of paths of strings that are of the input list\n\t\"\"\"\n\t# The first item is a path of files with one string, which is the pathname\n\tpaths = []\n\t\n\t# If the input list's list elements are different\n\t# return the same set of directories\n\tfirst = in_list[0]\n\tlist_dirs = in_list[1"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n\n\ndef main(input_list):\n\tfor path in input_list:\n\t\tprint \"path = \", path\n\tresult = removeLeadingTrailingTrailingDirectories(path)\n\t# print list for debug\n\t# print result\n\n\nif __name__ == \"__main__\":\n\tmain(sys.argv)\n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    if len(input_list) < 1:\n        raise TypeError(\"Input list is not a non-empty list.\")\n\ndef clean_up(input_list):\n\t\"\"\"\n\tTakes a list of string lists and removes the trailing os seperator characters\n\tfrom the input_list.\n\t\n\tArgs:\n\t\tinput_list: list of strings\n\t\"\"\"\n\tcleaned_list = []\n\tfor input_list in input_list:  \n\t\tos.path.join(*file_paths(input_list))\n\treturn cleaned_list\t\t\n\ndef merge"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n        self.input_list = input_list[:]\n\n\tfor i in range(len(self.input_list)-1):\n\tif self.input_list[i][0:2] == \"///\":\n\t    tmp = self.input_list[i][2:]\n\t    self.input_list[i] = tmp\n\t\n\tfor i in range(self.lenght):\n\tfor j in range(len(self.input_list)-1):\n\t    if self.input_list[i][j] == \"///\":\n\t\tself.input_list[i] = self"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef is_valid_file_name(input_name):\n\t\"\"\"\n\tChecks if a input name is a valid file name\n\t@Input:\n\t@Output: bool\n\t\"\"\"\n\tif os.path.isfile(input_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\t\ndef remove_file_name(input_name):\n\t\"\"\"\n\tRemoves the input string from a given file (in the appropriate format). \n\tThe input string is a path, containing the absolute path to the input file.\n\tThis function is a wrapper so the function"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef remove_trailing_zeros(input_list):\n\t\"\"\"This function will remove trailing zeros in string\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Cleaned list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef is_empty_string(input_list=[]):\n\t\"\"\"Return an empty list, if empty or not in list.\n\tEmpty list if string is empty.\n\tReturns:\n\t\tList of strings\n\nArgs:\n\tinput_list: list of strings\n\nReturns:\n\tList of strings\n\nRaises:\n\tTypeError"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n\t\t\ndef dir_name(directory_path): \t\n\t\"\"\"\n\tFind the best dir_name \n\tIf one path is returned return the name\n\telif two paths are returned return the first file name and its dirname\n\telse return None\n\t\"\"\"\n\t\"\"\"\n\tdir_list=[]\n\tif len(directory_path)==1:\n\t\treturn None\n\telse:\n\t\tdir_list=directory_path\n\t\treturn dir_list[0]\ndef find_dir_first(dir_list):\n\t\"\"\"\n\tfind_dir_first\n\t\tFor the"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef decode(self, char):\n\t\t\"\"\" decode the character and return the replacement \"\"\"\n\t\tmatches = re.findall('[a-z]+',' char)\n\t\tfor item in reversed(matches):\n\t\t\treplTxt = self.re1.sub(item, char)\n\t\t\treplTxt = replTxt.rstrip()\n\t\t\treplTxt = replTxt.replace('\\n','')\n\t\t\treplTxt = re.sub('[.]', '', str(replTxt))\n\t\t\treplTxt = replTxt.strip('.').lower()\n\t\t\treturn replTxt\n\n\tdef genre2(self):\n\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tif not isinstance(pattern, string_types):\t# regex pattern input to be a string \n\t\treturn None\n\telse:\t\t\t\t\t\t# if pattern input is string (e.g. %re, %i,%u)\n\t\t_pattern = re.compile(pattern)\n\t\tif _pattern.search(\"[%i]\", string):  \t# pattern was found\n\t\t\treturn string\n\t\telse:\t\t\t\t\t\t# pattern was not found\n\t\t\treturn None\n\n\nclass StringToRe(object):\n\t\"\"\"\n\tThis class converts given string to regular expression\n\t\"\"\"\n\t\n\tregex"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef __escape_char_class_name(self, char):\n\t\tresult = re.sub(r\"\\w+\\s*\\n[\\s]*$\", \"\", char)\n\t\tresult = re.sub(r\"\\s+\", \"\", result)\n\t\t\n\t\tresult = re.sub(r\"\\{\\{\\s*@\\#[\\}]*+\\n[\\}]*}\", \"\", result)\n\t\tresult = re.sub(r\"\\{[\\s]*\", \"\", result)\n\t\tresult = re.sub(r\"\\}\", \"}\", result)\n\t\t\n\t\tresult = re.sub(r\"^\\s+\","}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef create(self):\n\t\t\"\"\"\n\t\tthis method creates the regex pattern\n\t\t\"\"\"\n\t\tregex_pattern = r\"\\s*\\[(\\d*)\\s*(\\d).*?,\\s*(\\d*.\\d*)\\s*(\\d*)\\s*(\\d)\\s*(\\d*)\\\"\\]\\s*\"\n\t\treturn re.compile(regex_pattern, re.DOTALL)\n\n\tdef addDuplicates(self, a_list): \n\t\t\"\"\"\n\t\tthis method adds each of the list in this list to the list duplicates\n\t\t\"\"\"\n\t\tfor i, j in zip(a_"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\"\"\"\nThe Regex Patterns for the Pattern class \n\"\"\"\nclass Rregex_Pattern():\n\t\"\"\"\n\tThis class parses the Rregex pattern in it \n\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitializes the object\n\t\t\"\"\"\n\t\tself.match_object = {\n\t\t\t\"pattern_id\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t},\t\t\t\t\n\t\t}\n\n\t\"\"\"\n\tThis method parses the given pattern and checks the object format string\n\t\"\"\"\n\tdef parse_pattern(self, pattern_object):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n    def regex_match(regex,string):\n\t\t\"\"\"\n\t\tThis method takes a regex pattern and returns a match result based on it's pattern\n\t\t\"\"\"\n\t\tmatch = re.match(regex,string)\n\t\treturn match\t\n\t     \t\n\n\tdef create_pattern_name(self):\n\t\t\"\"\"\n\t\tThis function creates a regular expression using the specified information\n\t\t\"\"\" \n\t        pattern_name = \"\"\n\t\tif self.regex == None:\n\t\t\tself.regex = re.compile(\"\")\n\t\tpattern_name = self.regex.pattern\t\n\t\tif pattern_name =="}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tresult = re.findall( reString, line, re.DOTALL )       #Split the line into an element list\n\t\tstringList = []\n\t\t\n\t\tfor i in range( 0, len( stringList ) ):\n\t\t\ttemp = []\n\t\t\t\n\t\t\tfor j in range( 0, len( stringList ) ):\n\t\t\t\tif int( stringList[j] ) <= i + 1:\n\t\t\t\t\ttemp.append( stringList[j] )\n\t\t\t\telse:\n\t\t\t\t\ttemp.append(\"\")\n\n\t\t\tstringList[i] = \"|\" + stringList[i] + \"|\" + temp\n\t\t\n\t\tstring"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t self.line = re.sub('(\\w+)\\s+', '',self.line) \n\t\t\t\t\nclass Tweet:\n\t\"\"\"\n\tThis class provides information regarding a tweet according to the following rules:\n\t\n\t - Tweet of a particular user\n\t\t- The following users\n\t\t- Latest tweets (optional): This is most important as the tweet_id or username in the tweet's Tweet feed\n\t\"\"\"\n\t\t\t\n\tdef __init__(self, text, tweet_id = 0, username = None):\n\t\t\"\"\"\n\t\tInitialize the Tweet object\n\t\t\"\"\"\n\t\t\n\t\t# Check if"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n##################################################################\n#\n#\tRevampedia method\n#\nclass Revampedia(CommandDispatcher):\n\tdef at_beginning(self, arg, i):\n\t\t'''Callback function invoked at beginning of a path'''\n\t\tre.arg = arg #this is the line that has been matched\n\t\ttry:\n\t\t\tcompiled_regexp = re.compile(arg);\n\t\texcept Exception as e:\n\t\t\treturn [False, '', '', err_no_match_msg,\n\t\t\t\t   {'line' : int(arg) - len(compiled_regexp.pattern)})]\n\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n#\tThis regex uses a regex to match any character (default: \\s*) \n#\tUse the syntax:   regex_str = [^\\\\s]+     \n\t\n#\t\tThe regex can also match zero or more characters \nregex_str = r'\\1'\n\t\t\t\n#\t\tThe regex can also match zero or more characters\nregex_str2 = r'\\s'\n\t\t\t\n#\t\t\n#\t\t\t\ndef find_index_of_regex_str(regex_str) :\n\tfor i in re.finditer(regex_str, regex_str2) :"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t \n\tout = subprocess.Popen(cmd, stdout = subprocess.PIPE,\n\t\t\t\t\t\t\tshell = True)\n\tout_txt = out.communicate()[0].strip('\\n')\n\treturn out_txt\n\t\ndef make_cmd_c_command(cmd, **input_files):\n    \"\"\" Runs c-function with the given command(s). \n    \n    Args: \n        cmd (str): A string with the user's input.\n    \n    Kwargs:\n        input_files (list): A list of input files that should be checked for\n        special treatment of command argument.\n   \n        kwargs (dict):"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tself.command(cmd)\n\t\treturn\n\n\tdef __str__(self):\n\t\tstr = u\"\"\n\t\tfor f in self.files:\n\t\t\tif len(f) > 2:\n\t\t\t\tstr += u\"\\n\"\n\t\t\telse:\n\t\t\t\tstr += f\n\t\treturn str\n\n\tdef load_from_file(self, path):\t# Load in files from a file\n\t\t\"\"\"\n\t\tLoad a list of XML files into memory and apply the filter.\n\t\t@param path: file to load\n\t\t\"\"\"\n\t\tcmd = u\"awk '{print $1}"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n#---- PROXIED IMPLEMENTATION---------------------------------------\n'''\n\tThis function is a part of the framework that does not attempt\n\tto identify an unknown host system in the host system.\n\t'''\n\ndef checkHosts():\n    \"\"\"\n    Check the list of all host systems (Linux, FreeBSD,...) (defined by\n    constants.local_constants or some system-specific location), as well as the\n    \"virtual\" hosts.\n    \"\"\"\n\n    if os.path.exists('/etc/hosts'):\n        lines = open('/etc/hosts').read()\n\n        for line in lines.split(\"\\n\"):\n           "}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass FileGenerator:\n    \"\"\"Generator that opens files in the current directory.\"\"\"\n    \n    def __init__(self, directory):\n        \"\"\"Construct file generator.\"\"\"\n        self._directory = directory\n        \n    def append(self, filename, line):\n        \"\"\"Append a line to the file.\n\n        Arguments:\n        filename -- the full path of the file.\n        line -- the line to add.\n        \"\"\"\n        with open(filename, \"a\") as file:\n            file.write(line + \"\\n\")\n\n    def run(self, input_file, output_file):\n            \"\"\"Run file generator, creating all files.\"\"\"\n            output_"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\n\n###########################################################################################\n###########################################################################################\n\ndef copyfile(source,dest = None,overwrite = 0):\n\t\"\"\"\n\tCopy a file (source) from one directory to another\n\tfolder.   Can also take a source filename as the second arg\n\twithout any conversion (including directories).\n\t\"\"\"\n\tif source is None:\n\t\tprint \"Usage: %s %s\" % (sys.argv[0],\"copyfile\")\n\ttry:\n\t\tshutil.copy2(source,dest)\n\texcept OSError, (errno, strerror):\n\t\tprint >> sys"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t     \ndef main(argv) :\n\t# get options\n\t# options[0] = the directory containing all of the scripts\n\toptions = [\"run_script.py\", \n\t\t\t\t\t\"input_file\",\n\t\t\t\t\t\"output_file\",\n\t\t\t\t\t]\n\t# get the executable file in the the executable\n\texecutable = os.path.expanduser(\"~/.bash/sh_escape.sh\")\n\t# get the directory that contains the sftp and the path to the.sh programs\n\tdirectory = os.path.dirname(executable)\n\t# get the input files file name\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nfrom subprocess import call\t\"\"\"\n\tCall the given command(s) in a shell, in the child process.\t\"\"\"\n\ndef test():\n    \"\"\"\n    Call the test method in a shell.\t\"\"\"\t\n\t\t\n\t# Set up the shell environment\n\tscript = subprocess.Popen( [ \"env\", \"shell\", SCRIPT]\t)\n\t# The shell command requires a password so we have to do\n\t# it\n\t#subprocess.call( [  \"python\", \"test.py\",\n\t#\t\t\"-r\", SCRIPT\t])\n\tsubprocess.call( [  \""}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass TestCmd(unittest.TestCase):\n\tdef setUp(self):\n\t\tpass;\n\t\t\n\tdef tearDown(self):\n\t\tpass;\n\t\t\n\n\t\n\tdef test_1_cmd(self):\n\t\t# Run a cmd. Return the path to the directory.\n\t\tcmd = 'git log \"'+self.getFileName()+'\"; cd '+self.getDir()+';./cmd.py '+self.getFileName()\n\t\tresult = subprocess.check_output(cmd,shell=True)\n\t\tself.assertEqual(result, cmd[:-1])\n\n\tdef test_2_cmd"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass NagiosProcess(subprocess.Popen):\n\t\n\tdef __str__(self):\n\t\t\n\t\tif len(self) == 1:\n\t\t\tstring = \"Running Nagios (without sudo or with python)\"\n\t\telse:\n\t\t\tstring = \" \".join(self)\n\t\t\t\t\t\n\t\treturn string\n\t\n# -*- coding: utf-8 -*-\n'''\nCreated on Jun 16, 2014\n\n@author: oliwa\n'''\nfrom base.util_network import is_hex\nr=str.maketrans(\" \", \"*\")\ndef _split_words(s):\n\tfor i in"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass Executor:\n\t\"\"\"\n\tA parent class.\n\tThe Executor is used as a parent.\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.cwd = os.getcwd()\n\t\t\n\tdef __enter__(self):\n\t\tself.start_command = True\n\t\treturn self \n\t\n\tdef __exit__(self, *args, **kwargs):\n\t\tself.close_command = True\n\t\t#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\n# Fenrir TTY screen reader:\n# This program is free software"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\ndef main(argv):\n\t#print \"\"\n\timport sys\n\tappversion = \"4.2.15-g10-x86_64-with-gcc\"\t\n\n\t###Parse arguments\n\tparser = argparse.ArgumentParser(description='Prints IP address and MAC address')\n\tparser.add_argument('--ipv4', action='store_true', required=False)\n\tparser.add_argument(\"--reverse\", action=\"store_true\", required=False)\n\targs = parser.parse_args()\n\t\n\t#Set the verbosity level if set\n\tif (args."}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t \n\tif  host == '' or host == 'host.com':\n\t\tprint '\\n[!] Invalid host'\n\t\t\nelif sys.argv[1] =='main' or sys.argv[1] == 'auto':\n\tif len(sys.argv) <= 3:\n\t\tprint '\\n[!] Usage MYSQLIP/HOST OR HOST:PORT'\n\t\tprint '\\n[!] ex) 1) Automatic - Use SSL'\n\t\tprint '\\n[-] ex) 2) Restart and stop DHCP scan'\n\t\tprint '\\n[-] ex) 3"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass NetnsTests(unittest.TestCase):\n    \"\"\"Tests for Nmap tests\n    \"\"\"\n\n    #--------------------------------------------------------------------------\n    def setUp(self):\n        pyroute2_mock.Netns.__init__()\n        self.device = ['192.168.1.1', '192.168.1.2']\n        pyroute2_mock.Netns.device = self.device\n        pyroute2_mock.Netns.route_add_del_route = mock.Mock()\n        pyroute2_mock.Netns.route_add_del_route.return_value = 1\n\n        pyroute2_"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\n\tres=self.api.get(\"/ipaddress/create\")\n\tself.assertEqual(res.status_code,403)\n\n\t\n\t\"\"\"\n\tTests for creation with bad user name \n\t\"\"\"\n\n\twith self.app.test_request_context('/login'):\n\n\t\tauth_id=self.get_auth_id_by_email('valid@email.com', 'password')\n\t\tuser_id= auth_id\t\t\n\n\t\tres=self.api.post('/auth-keys',\n\t\t\tdata=json.dumps({'user_id':user_id}),\n\t\t\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\tif socket_addr is IP or (len(ipaddress.ip_address().split('.')) == 4) or (socket_addr is \"localhost\") :\n\t\t\thost = get_localhost_ip()\n\t\t\tprint host\n\t\telse :\n\t\t\thost = \"http://%s/ip/%s\" % (get_localhost_ip(), int(ipaddress.ip_address().split('.')[0]))\n\n\n# TODO : find other tests\nif __name__ == \"__main__\":\n\tparser = argparse.ArgumentParser(description=\"Test the dns server API\")\n\tparser.add_argument(\"-l\", \"--listen"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\ndef send_msg(ip_address, port, msg):\n\tsock.sendall(bytes(msg, \"UTF-8\"))\n\tip_found = False\n\twhile not ip_found:\n\t\ttry:\n\t\t\t# Sends the packet\n\t\t\tsock.sendall(\"1,0,0,0\\r\\n\".encode('utf8'))\n\t\t\t\n\t\t\tip_address = str(IP(ip_address))\n\t\t\t\n\t\t\tsock.sendall(\"2,0,0\\r\\n\".encode('utf8'))\n\t\t\t\n\t\t\t# Sends the packet\n\t\t\tsock.sendall(\"3,"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t \n\tdef testIPv4(self,ip='',netmask=''):\n\n\t\ttry:\n\t\t\tfrom ipaddress import IPv4Address\n\t\t\tprint(\"test: IPv4\")\n\t\t\tx = IPv4Address('1.2.3.4')\n\t\t\tprint \"x=\" + x\n\t\t\tassert False\n\t\texcept Exception as e:\n\t\t\tprint (\"ERROR TEST\")\n\nif __name__=='__main__':\n\tunittest.main()\n#!/usr/bin/env python2\r\n\r\n'''\r\n@contact: Debian Hughes\r\n@copyright: 2002, 2005  2006  James T"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass ipaddress4Test(unittest.TestCase):\n\tdef setUp(self):\n\t\tpass\n\t\t\n\t\t\n\tdef tearDown(self):\n\t\tpass\n\nclass mainTest(unittest.TestCase):\n\tdef test_ipv4(self):\n\t\tresult = ipaddress4(\"10.0.0.1/24\")\n\t\tip = result.ipv4()\n\t\tself.assertEqual(ip, ('10.0.0.1', 8, 32))\n\t\tself.assertEqual(ip, \"::ffff:10.0.0.1\")\n\t\tself.assertEqual(result.hex(), \""}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\n\ttest_cases = [\n\t\t# IPv6 address\n\t\t\"10::1\",\n\t\t\n\t\t# IPv6 with port\n\t\t\"10E:2::2\",\n\t\t\n\t\t# IPv4 address with trailing spaces\n\t\t\"172::1\",\n\t\t\n\t\t# IPv6 with missing port\n\t\t\".0:0:0::\",\n\t\t\n\t\t# IPv6 with missing trailing spaces\n\t\t\"::1\",\n\t\t\n\t\t# IPv4 address with missing port\n\t\t\"2001::1::2\",\n\t\t\n\t\t# IPv6 with missing trailing spaces\n\t\t\".0\", # 0\n\t\t\".0::\", # 127"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass test_ipv6_addr2(unittest.TestCase):\n\t\n\t\"\"\"\n\tTest case for ipv6 addr2\n\t\"\"\"\n\t\n\tdef setUp(self):\n\t\tpass\n\t\n\tdef tearDown(self):\n\t\tpass\n\t\n\tdef test_ip_range(self):\n\t\tip_range =  \\\n\t\t[\n\t\t\t(0, 0xffffffff), \n\t\t\t(0xffff, 0xffffff)\n\t\t] \n\t\tres = ip_range.check_range()\n\t\t\n\t\tself.failUnless(res)\n\t\t\t\t\n\ttest_ip_range.skip = \"The test suite\""}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_roots(graph, parent):\n\t\"\"\"find_roots(graph, rdflib.RDFS.subClassOf, rdflib.RDFS.subClassOf)\"\"\"\n\tnodes = {}\n\t\n\tdef process_node(parent, current_node):\n\t\tnodes[current_node] = len(graph.successors(parent, current_node))\n\t\n\tdef postprocess_node(parent, current_node, current_edge, parent_pred):\n\t\t#if parent_pred == None or current_pred == None:\n\t\t#\tprint(\"in the following \" +"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_roots(graph, skoscope, root, depth = 0):\n\t\n\t\"\"\"Finds all root from skoscope if depth is specified\n\t\n\tWhen depth=0, find all the immediate (and first) root nodes of a \n\tgiven parent and given property \n\t\"\"\"\n\t\n\tassert isinstance(root, rdflib.URIRef)\n\tcontext = [root]\n\t\n\tlogger = log(context, dom.URIRef(\"urn:biological-generator:Biological:1.0:context\"))\n\t\n\tdef find_ancestor(parent, depth, node):\n\t\tlogger."}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\tsub_prop = (URIRef(\"http://www.w3.org/2002/07/owl#subClassOf\"), URIRef(\"http://www.w3.org/2002/07/owl#property\"),URIRef(\"http://www.w3.org/2002/07/owl#parent\"), URIRef(\"http://www.w3.org/2002/07/owl#child\"), URIRef(\"http://www.w3.org/2002/07/owl#subClassOf\"), URIRef(\"http://www.w3.org/2002"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\n\tdef find_children(graph, child):\n\t\t\"\"\"The algorithm for looking up children.\"\"\"\n\t\t\n\t\tchildren = []\n\t\t\n\t\tfor child_id, child_obj in graph[subClasses[child]]:\n\t\t\tif child_id not in children:\n\t\t\t\tchildren.append(child_obj)\n\t\t\t\n\t\treturn children\n\t\t\n\tdef find_parents(graph, obj):\n\t\t\"\"\"The algorithm for looking up parents.\"\"\"\n\t\t\n\t\tparents = set([ x for x in graph[obj] ])\n\t\t\n\t\tfor parent_id, parent_obj in graph[obj]:\n\t\t\tif parent_id not in"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_roots(graph, rdflib_root):\n\t\"\"\"Search all nodes in the graph that have the same root as the\n\tgraph. If the graph does not have any nodes, return an empty list\n\tso nodes will be filtered\"\"\"\n\tfrom collections import deque\n\n\tif isinstance(rdflib_root, rdflib.Graph):\n\t\troots = [rdfdom_root]\n\telse:\n\t\troots = []\n\t\n\tfor root in roots:\n\t\tif rdfxml_root.identifier == root.identifier:\n\t\t\tcontinue\n\n\t\tfor child in graph.objects(node"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_roots(graph, rdflib.RDFS.subClassOf):\n\t\"\"\"\n\tFind the roots of a particular node for the given subClass of the given\n\tsubClass of the current entity.\n\t\n\treturns a list of \"top level\" classes; each top level is a list of\n\tone-level roots representing the sub-class, which is themselves\n\tderepresented by the current entity.\n\nExample\n\nx   -> {x}\n\t<y>: {f1}\n\t<a> -> {f2}\n\t<b>: {z"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\tdef find_root(graph, subClassOf):\n\t\t\tfor i in graph.all_subclasses(RDFS.subClassOf, subClassOf):\n\t\t\t\treturn i\n\t\t\treturn None\n\t\treturn find_root(graph, SKOS.broader)\n\n\tdef get_properties(graph):\n\t\tproperties = {}\n\t\tfor node, (prop, _, _) in graph.triples((None, SKOS.broader)):\n\t\t\tfor attr, fn in prop._attributes.items():\n\t\t\t\tif isinstance(fn, rdflib.Literal):\n\t\t\t\t\tproperties[attr] ="}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\tchildren = set(graph.subjects(RDFS.subClassOf))\n\tfor p in children:\n\t\tp_prop= p.object\n        prop_prop= 1. / p_prop\n\t\tprint \"<%s>\" % graph.label(p),\n\t\tprint \"(%s %s)\" % (p_prop, prop_prop)\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\ndef print_errors(errors):\n    if errors:\n        # print only the errors\n        for e in errors:\n            # print the message\n            print(\"\\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\tgraph=rdflib.Namespace(graph)\n\t\tprop=rdflib.URIRef(prop)\n\t\tparent=rdflib.URIRef(parent)\n\t\tall=graph.subjects(RDF.SubClassOf)\n\t\tpath=all.next()\n\t\treturn set(path.parts)\n\t\t\t\ndef find_rooted_components(graph, rdflib_node):\n\t\"\"\"For each level of rooted component, find the roots of the level with RDF.subClassOf or SKOS.broader.\n\t\n\tfindRosters(graph)\n\twill return a set"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t       \n\tdef find_children(graph, child, prop):\n\t\tfor child in list(graph.successors_iter(child)):\n\t\t\tfor prop in list(graph.predecessors_iter(child)):\n\t\t\t\tyield from find_children(graph, child, prop)\n\t\t\tyield ()\n\t\n\tparents, sub_prop = find_leaves(graph, child, \"in_subClassOf\")\n\t\n\tif parents:\n\t\tfor parent in parents:\n\t\t\tprint \"  \", parent\n\t\t\n\t\tfor child in [graph.successors_iter(child) for child in graph.success"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\twith codecs.open(self.configfn, 'w', 'utf-8') as stream:\n\t\t\tdump(self._conf, stream)\n\n\tdef _load_environment(self):\n\t\t\"\"\"Loads environment from configuration file\"\"\"\n\t\tself._conf = self._load_file(self.configfn)\n\n\n\tdef _dump_configuration(self):\n\t\t\"\"\"Dump configuration to file\"\"\"\n\t\twith codecs.open(self.configfn, 'w', 'utf-8') as stream:\n\t\t\tdump(self._conf, stream)\n\n\tdef _get_template_path"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\t\n\tdef _set_to_dump(self):\n\t\t\"\"\"\n\t\tRe-encode and write to a json-formatted file\n\t\t\"\"\"\n\t\t\n\t\tfile = open(self.conf_file,'w')\n\t\t\n\t\t# Write a normal python object to disk\n\t\tif isinstance(self.config,list):\n\t\t\tjson_data = self._json_encode(list(self.config))\n\t\t\tfile.write(json_data)\n\t\t\tfile.close()\n\t\t\n\t\t# Add the top level entry to the start of the JSON file\n\t\tself.output_config.append({\"module\": \"__"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t #\n\t self.representationsDict=yaml.load(open('dictionary_dump.yaml').read())\n\t return\n\n\n\n\n# ------------ \n#   test_main\n# ------------ \n\nclass test_main(object):\n\t\"\"\"\n\tA main function\n\t\"\"\"\n\tdef __init__ (self):\n\t\t# \n\t\t# the main\n\t\tself.x = 2\n\t\tself.y = 3\n\t\t\n\t\t# \n\t\t# the end\n\t\tself.end = False\n\t\tself.init = self.init_main()\n\t\tself.run_name ='main"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tself.representations = {}\n\t\tif self.ordered:\n\t\t\tself.representations_ns = {}\n\t\telse:\n\n\t\t\tself.representations_ns = {}\n\n\t\t\tfor repr in self.as_ordered_repr():\n\t\t\t\tself.representations[repr.entity] = repr.value\n\n\tdef is_serializable(self, state):\n\t\tif self.options.pretty_print:\n\t\t\tif state is not None:\n\t\t\t\treturn len(state) == 1 and state[0] in (type(None), unicode)\n\t\t\telse:\n\t\t\t\treturn False\n\t"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tprint(self.tostr(indent=0))\n\n\tdef tostrlist(self, indent=0):\n\t\t\"\"\"\n\t\tPrints the value of self['key']\n\t\t\"\"\"\n\t\tif self['key'].__class__ == collections.OrderedDict:\n\t\t\tkeys = list(self.keys())\n\t\t\tkeys.sort()\n\t\t\tvalues = [self.repr(key) + ':'+ self[key].__repr__(indent+1) for key in keys]\n\t\t\treturn '\\n'.join(values)\n\t\telif self['key'].__class__ == types.ListType and len"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\t\t\n\n# -*- coding: utf-8 -*-\nfrom unittest import TestCase\nfrom montreal_yaml_loader import MontrealYAMLLoader, \\\n    _TestClassThatHasVariables, YAMLLoader\nfrom montreal_yaml_dumper import UITemplateDumper\nfrom montreal_yaml_template_loader import MontrealTemplateLoader, \\\n    _TestClassThatHasOptions, TemplateLoader\nfrom montreal_yaml_template_dumper import MontrealTemplateDumper, \\\n    _TestClassThatHasSettings, TemplateDumper\n\n\nclass TestTempalteLoader(TestCase"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\n\ttry:\n\t\tyaml_txt = self.yamlify()\n\texcept:\n\t\tprint (\"Warning: unable to dump as YAML\")\n\t\treturn \"\"\n\n\toutput = yaml.Dumper().represent_mapping(yaml_txt)\n#\ttry:\n#\t\toutput = yaml.RoundTripDumper.represent_mapping(output)\n\texcept:\n#\t\tpass\n\t\n\treturn output\n\t\t\n\n    def print_pretty(self, obj):\n        \"\"\" print out a python3 pretty format \"\"\"\n\ttry:\n\t\tprint json.dumps(obj,sort_keys=True,indent"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\ttry:\n\t\t\tr = self.serialize(indent=\"    \", sort_keys=True, separators=(\",\", \": \"), default=self.default,\n                                  sort_keys_default=True, indent_num=2)\n\t\texcept Exception as e:\n\t\t\traise e\n\t\telse:\n\t\t\treturn r\n\n\n\tdef dumps_table(self, data, stream, indent=\"  \"):\n\t\t\"\"\" dump the table into a string \"\"\"\n\t\tif type(data) is unicode:\n\t\t\treturn \"\".join(\n\t\t\t\tself.dumps_unicode(value, stream, indent)\n\t\t\t\t\tfor value"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tif isinstance(data, unicode):\n\t\t\ttry:\n\t\t\t\tsdata = data.encode(\"ascii\")\n\t\t\t\tif six.PY2:\n\t\t\t\t\tsdata = sdata.encode(\"utf-8\")\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\telif not isinstance(data, str):\n\t\t\ttry:\n\t\t\t\tsdata = unicode(data).encode(\"ascii\")\n\t\t\texcept UnicodeEncodeError:\n\t\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\tsdata = data.decode(\"ascii\")\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\tout = unicode(data, \"utf-8\")"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\treturn yaml.safe_dump (value, default_flow_style=True,\n\t\t\t\tdefault_style='\"')\n# -*- coding: utf-8 -*-\n\"\"\"QGIS Unit tests for QgsDataProviderRegistry.\n\n.. note:: This program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\"\"\"\n__author__ = 'Nyall Dawson'\n__date__ = '11/07/2016"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tlogging.captureWarnings(True)\n\tlogger = logging.getLogger(\"pym.system.main\")\n\n\tif not os.path.isdir(logs_folder):\n\t\tos.makedirs(logs_folder)\n\n\ttry:\n\t\tlogging.info(\"Starting pym-system\\n...\")\n\t\tlogging.info(\"LOG LEVEL: 0 - INFO - 7 - DEBUG\")\n\t\tlogging.info(\"LOG FILE: %s\" % logs_folder + \"/\" + logs_name + \".txt\")\n\t\tlogging.info(\"Logging level: INFO - 10 - STDOUT - ERROR\")"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef log(self, msg):\n\n\t\tdef __formatter(msg):   \n\t\t\t\n\t\t\tself.emit(\"[%d] %s\"%(int(time.time()*1000), msg), \"INFO\")\n\t\t\t\n\t\t\t\n\t\t\treturn msg\n\n\t\tdef __log__(msg):\t\n\t\t\tself.emit(\"[%d] %s\"%(int(time.time()*1000), msg), \"DEBUG\")\n\t\t\n\t\t\treturn msg\n\t\t\t\n\t\t\n\t\tdef logging(self, levelStr):\n\t\t\tif self.logger is not None:\n\t\t\t\t\n\t\t\t\treturn self.logger.error(levelStr)\t\t\t\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\ndef setup_app(app, debug = True):\n\t\n\t\"\"\"\n\tConfigures Flask Framework\n\t\"\"\"\n\t\n\t# configure log\n\tif debug:\n\t\tlogging.basicConfig(level = logging.DEBUG, filename = 'debug.log', format = '%(asctime)s;%(levelname)s;%(message)s')\n\t\n\t# configure logging for libtorrent (optional)\n\tlogger = logging.getLogger('libtorrent')\n\t\n\tif config.DEBUG==\"0\":\n\t\t\n\t\tapp.logger.setLevel(logging.DEBUG)\n\telse:\n\t\tapp.logger.setLevel(logging.INFO"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreate module instance for logging/debug\n\t\t\"\"\"\n\t\t#logger\t\t\n\t\tself.logtolog=logging.getLogger('LogFile')\n\t\tself.logformat=LoggerFormatter('[%(asctime)s] [%(name)s] - %(levelname)s - %(message)s',\"%Y-%m-%d %H:%M:%S\")\n\t\tself.logtostderr=logging.getLogger('LogFile')\n\t\tself.logtofilehandler=logging.FileHandler(self.filename,'k',False)\t\n\t\tself"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t     \n\t\t__loggerFile = os.path.join(os.path.dirname(__file__),'Logging.log')\n\t\t__loggerFileFullPath =  os.path.join(__loggerFile,'Log',__loggerFileFullPath)\n\t\t## logger\n\t\t_loggerFullPath = os.path.join(__loggerFile,'Log',__loggerFullPath)\n\t\t## config\n\t\t__logger = logging.getLogger(__loggerFullPath)\n\t\t## stdout\n\t\t_stdout = logging.StreamHandler( __loggerFileFullPath)\n\t\t_stdout.setFormatter(logging.Formatter(\"[%(asctime"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.logger = logging.getLogger('zstack')\n\t\t\n\tdef set_log_level(self,log_level = logging.NOTSET):\n\t\tself.logger.setLevel(log_level)\n\t\t\n\tdef set_verbose(self,verbose):\n\t\tself.logger.setLevel(verbose)\n\t\t\n\tdef log(self,message,level=logging.DEBUG):\n\t\tself.logger.log(level,message)\n        \n    def debug(self,message,level=logging.INFO):\n\t\tself.logger.log(level,message)\n\t\t\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\t\n\tdef log(self,message,args):\n\t\tif args:\n\t\t\tmessage = string.join(args,'\\r\\n')\n\t\t\tlogging.debug(message)\n\t\tself.handle_log(message)\n\t\n\tdef addToOutput(self, msg, prefix=''):\n\t\tif getattr(self, 'outBuffer','') is None:\n\t\t\t#self.outBuffer = StringIO()\n\t\t\tself.outBuffer = StringIO()\n\t\t\tself.outBuffer.write(str(msg)+'\\n')\n\t\t\tself.outBuffer.flush()\n\t\telse:\n\t\t\tself."}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef __init__(self,verbose):\n\t\tif not self.verbose:\n            logging.basicConfig(format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',level=logging.INFO)\n        else:\n\t\t logging.basicConfig(format='%(asctime)s %(name)-11s %(levelname)-8s %(message)s',level=logging.INFO)\n   \t\n   \n   \n\n\tdef setLogger(self):\n\n\tif self.logFile==\"\":\n\t    \tlogging.basicConfig()\n \t        if(self.verbose):\n\t        logging.getLogger('').setLevel"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\ndef getLogger():\n\t\"\"\"\n\tGet instance of the application logger. \n\n\t:returns: the logger instance\n\t\"\"\"\n\treturn logging.getLogger(__name__)\n\n# this makes sure that the class does not depend on django.db.models yet\nif not 'django.db.models' in [c.__module__ for c in threading.local.models]: # pragma: no cover\n\tcreate = django.db.models.signals.post_save\n\tpost_save = django.db.models.signals.post_delete\n\n# This class uses a Queue to manage"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\tdef __init__(self):\n\t\t\tlogger = logging.getLogger()\n\t\t\tformatter = logging.Formatter('%(asctime)s %(message)s')\n\t\t\t# create formatter\n\t\t\tformatter.converter = time.gmtime\n\t\t\tself.formatter = formatter\n\t\t\tlogger.setLevel(logging.DEBUG)\n\t\t\t# formatter = logging.Formatter('%(levelname)6s: [%(name)s] %(message)s')\n\t\t\t# set formatter\n\t\t\tlogger.addHandler(self.formatter)\n\t\t\t# logger = logging.getLogger('Test')\n\t\tdef close(self):\n\t\t\tlogger = logging.getLogger()"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n    n_values = shape[1]\n    n_vars = shape[0]\n\n\tshape_str = '(' +str(n_values) + ','  + str(n_vars) + ')'\n\t\n\tif not dtype:\n\t\tdtype = 'complex128'\n\t\n\tdata = np.zeros(shape, dtype=dtype)\n\n\tif(n_values%2 == 1):\n\t\tfor i in range(1,n_values-1):\n\t\t\tdata[i] = int(n_vars * i)\n\telse:\n\t\tfor i in range(1,n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n        #TODO: handle more complex types\n\n\tnumproc = np.size(shape) \n    data_dtype = np.dtype([(numproc, bool),\n        (numproc, dtype),\n        (numproc, bool)]) \n    \n    numproc = (np.max(numproc,axis=0) - np.min(numproc,axis=0))\n    data_array = np.zeros((numproc,int(numproc)), dtype=data_dtype, order='F')\n\n    data_array[...,0] = 1\n\ndef generate_grid_"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\tself.shape = shape, dtype\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tFunction to create the string with a nice overview of it!\n\t\t\"\"\"\n\t\t\t\n\t\tr = \"%s\\n\" % (self.shape)\n\t\tr+= \"Shape: %s\\n\" % (self.shape)\n\t\tr+= \"dtype: %s\\n\" % (self.dtype)\n\t\treturn r\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tFunction to print the array with a nice overview of it!\n\t\t\"\"\"\n\t\treturn \"numpy.array :"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n    return np.zeros(shape=shape, dtype=dtype)\n\ndef rand_vector(size):\n    \"\"\"\n    Generate random integer vector of given size\n    \"\"\"\n    rng = np.random.rand(1)\n    return np.round((rng + 0.5) / 2)\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef create_array(shape, dtype):\n    \"\"\"\n    Create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\t\"\"\"\n\tshape = np.array(shape)\n\tprint shape\n\tprint dtype\n\ndef create_array_with_shape_and_dtype(shape, dtype):\n\tlen = np.prod(shape)\n\tshape = list(shape)\n\tprint len\n\tprint dtype\n\tprint shape[1]\n \ndef delete_array_with_shape(shape, dtype):\n\tlen ="}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\nclass NumpyFormat(object):\n\tdef __ne__(self, other):\n\t\treturn not self == other\n\tdef __add__(self, other):\n\t\tif not self == other:\n\t\t\traise Exception('No copy function')\n\t\traise TypeError('add or inplace not supported')\n\ndef _ensure_bool(var):\n\t\"\"\" Makes sure the type is a boolean or int. \n\tIf the var is boolean, return True\n\tIf the var is int, return int\n\tIf the var is float, return float\n\t\"\"\"\n\tif str(type(var))=='"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef add_diag(shape):\n\t\"\"\"\n\tFunction to create a vector with given shape and values.\n\nParameters\n----------\nshape : tuple\n    shape of the array\nvalues : ndarray\n    values of the vector.\n\"\"\"\n\tl = - np.ones(shape, dtype=np.float32) * 1\n\tfor i in range(size):\n\t\tl[i] = values[i, :]    \n\treturn l\n\n\ndef sub_diag(shape, indices):\n    \"\"\"\n\tFunction to create a vector with given shape and values.\n\nParameters\n----------\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef array_type(x):\n\t\"\"\"\n\tReturn the data type of the array\n\t:Parameters:\n\tx : numpy.dtype\n\t-> `x.dtype`\n\t\"\"\"\n\treturn x.dtype\n\ndef shape(x):\n\t\"\"\"\n\tReturn the shape of a ndarray\n\t:Parameters:\n\tx : numpy.ndarray :class:`numpy.ndarray`\n\t-> `x.shape`\n\t\"\"\"\n\treturn x.shape\n\n# ###################################################################################  \n#\tFunction to create an array of the maximum values for the numpy array.\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t    \n\treturn None\n\ndef copy(value):\n    \"\"\"Copy a function or function-like object from another array\"\"\"\n    out = np.copy(value._numpy_data) if isinstance(value, np.ndarray) else value\n    return out\n\ndef zeros(shape, dtype=float, data=float):\n\t\"\"\"Return a new array of given shape, filled with zeros\n\t\"\"\"\n\tdtype_out = dtype\n\tif isinstance(dtype, np.ndarray) and (dtype!= float or data!= float):\n\t\tdtype_out = np.dtype(dtype)\n\tarray"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef identity(n):\n\tassert len(n.shape) == 2 #should be 2 or equal\n\treturn (n + n.mean()) / n.std()\n\n\ndef create_var(n, shape, dtype=np.float32):\n\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\tParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `dtype`\n    data-type of the array to create\t\"\"\"\n\tfor i in xrange(len(shape)):\n\t\tif (shape[i]!= n) and ("}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tdef init(self):\n                # set the mean position\n                self.mean = np.random.randn(self.N, self.n)\n                # set the variance position\n                self.var = np.random.randn(self.N, self.n)\n               \n   \n        def fun_logprior(self, x):\n                p = np.log(x) - 0.5*np.log(2*np.pi*x)\n                return p\n       \n        def fun_loglikelihood(self, x):\n                p = norm.logpdf(x, self.mean, self.var"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\nclass Gamma(Base):\n\t\"\"\"Probability density function for a Gaussian node.\"\"\"\n\tdef __init__(self, X, Y, theta, mu):\n\t\t\"\"\"Constructor for this Gamma object.\"\"\"\n\t\tself._X = X\n\t\tself._Y = Y\n\t\tself._r = theta\n\t\t\n\t\tself._C = np.diag([np.sum(Y * np.exp(-Y * theta.T)), Y])\n\t\t\n\t\t\n\t\tself._mean = mu\n\t\t\n\t\treturn\n\n\tdef getMean(self):\t\n\t\treturn 1. / self._X.shape[0]"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t \n\tc = np.sqrt( ((x - np.arange(0.1,x) / 2.)**2. )**1. )\n\tomega = c / np.sqrt( x )\n\t\n\tdef _calc_pdf(self, x):\n\t\t\"\"\"\n\t\tCalculates pdf function at x\n\t\t@param x: Parameter\n\t\t@return: pdf for given parameters @return\n\t\t\t(pdf, pdf_chi2_cdf, pdf_pdf).\t\t\n\t\t\"\"\"\n\t\tpdf = []\n\t\tfor k1,k2 in zip(x,x):\n\t\t\t\n\t\t\t#"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n#=====================MAIN=====================================\n\nif __name__ == '__main__':\n\t\"\"\"\n\t\tExample 2:\n\t\t1. Import the libraries.\n\t\t2. In order to do 1-point Gaussian, we add the Gaussian\n\t\tfunctions and have it read the function data for them.\n\n\t\t>>> X\n\t\tarray([[  2.82106761,   0.00000000e+00,   0.00000000e+00],\n\t            [ -0.42793924,   1.58341428,   1.59547687],\n\t            [ -0."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\nclass Gas(object):\n\t\"\"\"\n\tGaussian distributed data model for binary/continuous data.  \n\t\"\"\"\n\tdef mu(self, x, mu=0.1):\n\t\treturn np.exp( -0.5*((x - mu)**2) )\n\t\n\tdef var(self, mu=0.1):\n\t\treturn np.ones(len(mu))*np.exp(-0.5*((x - mu)**2))\n\t\n\tdef k(self, p, q):\n\t\treturn p*q\n\t\n\tdef kmf(self, p, q"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tdef muI(self, mu):\t\t\"\"\"\n\t\tGaussian weight of mu.\n\t\t\"\"\"\n\t\t\n\t\treturn mu + np.random.randn(len(self.x))\t# np.random.randn(len(self.x))\t\n\n\tdef sigmaI(self, sigma):\n\t\t\"\"\"\n\t\tGaussian weight of sigma where a low distribution has mu.\n\t\t\"\"\"\n\t\treturn ((1 - sigma) * np.log(1 - sigma))import sys\nimport os\nimport codecs\nfrom io import StringIO\n\nis_py33 = (sys.version_info"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tdef __init__(self, sigma=1., mean=0., sd=1., \n\t\t\tlength=200.0, time=0.0):\n\t\t\n\n\t\tself.sigma = sigma\n\t\tself.mean = mean\n\t\tself.sd = sd\n\t\tself.length = length\n\t\tself.time = time\n\n\t\tif(self.sd > 1 and self.length > 1.0):\n\t\t\tself.theta_sigma =  np.arctan2(2*np.pi/self.sd*1j, \n\t\t\t\t1-np.arctan2(2*np"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tdef pdf(self, x, mean, sig, tau, alpha, beta):\n\t\t\"\"\"Evaluate the PDF at x for a given set of parameters. \n\t\tNote: alpha (shape parameter of the Gaussian distribution).\n\t\tSee also pdf to compute pdf from these.\n\t\tParameters: mean, sig, tau\n\t\tReturns:\n\t\t\tpdf evaluated at x\n\t\t\"\"\"\n\n\tdef plot_pdf(self):\n\t\t\"\"\"Plots the density at x as a function of lambda, \n\t\tthe pdf, and the derivative of logpdf\"\"\"\n\n\t\talpha_pdf = np.exp(-(self"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\t\"\"\"Initialize.\"\"\"\n\t\tself.ndim = 2\n\t\nclass GaussianLogistic(Kernel):\n\t\"\"\"Gaussian log-linear continuous random variables. \n\t\"\"\"\n\t\t\n\tdef __init__(self, shape, dtype=None, name='GaussianLogistic'):\n\t\t\"\"\"Initialize.\"\"\"\n\t\tself.ndim = 2\n\t\tKernel.__init__(self, dtype, name, ndim=self.ndim)\n\t\n\t\n\tdef expand(self, samples):\n\t\t\"\"\"Expand the data.\"\"\"\n\t\tnew_shape = np.shape(samples)\n\t\tsamples = samples."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\td = 10.\n\tx_0 = np.linspace(-d, d, 50)\n\tx_opt = np.exp(-x_0/5)\n\n\tdef get_param_vec(x):\n\t\treturn np.exp(-x**2)\n\n\tdef get_param_vals(x):\n\t\t#return np.exp(-x**2) * np.cos(3*np.pi*(x - x_0) + 2*x_0*np.cos(2*np.pi*x_0))\n\t\treturn x\n\n\n\tx"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\t\n\tlogging.basicConfig(level=logging.WARNING,\n\t\t\t\t\t\tformat='%(asctime)s %(message)s',\n\t\t\t\t\t\tfile='%(path)s_%(process_name)s.log',\n\t\t\t\t\t\tdatefmt='%Y-%m-%d %H:%M:%S')\n\t\n\tresult = {}\n\t\n\tresult['parsed_configfiles'] = [ ]\n\tresult['parsed_errors'] = {}\n\t\n\tfor configfile in file_paths:\n\t\n\t\tconfname = os.path.splitext(configfile)[0]\n\t\tprocess_name = os.path.splitext(confname)[0]\n\t\t\n\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tlogger = logging.getLogger(\"pseudomix.config\")\n\t\tlogger.info(\"Parsing configuration files: %s\" %''.join(configuration_filename_list))\n\t\t\n\t\t# read configuration with configuration filename\n\t\tf = open(configuration_filename_list[0])\n\t\tlogger.debug(\"Parsing configuration file: %s\" %''.join(configuration_filename_list))\n\t\tconfig = f.read()\n\t\tf.close()\n\n\t\t# initialize logger with the configuration file. \n\t\tlogger = logging.getLogger(\"pseudomix.status\")\n\t\tif"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\ndef parse_configuration_files(configuration_filenames):\n\t\"\"\"\n\tLoad and validate configuration files into a list of dicts of configuration filenames. Return a tuple of\n\tconfiguration dict and a logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tparsed_names = [configuration_filename for configuration_filename in configuration_filenames]\n\t\n\tlogging.debug(\"Parsing configuration files: {0}\".format(\",\".join(list(parsed_names)) ))\n\t\n\treturn parsed_names, map(lambda x: tuple(x[:-1]), parsed_names)\n\n\nimport yaml\n\n\n\ndef validate_"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\tdef __init__(self, configuration_filenames, parse_errors, parse_warnings):\n\t\tsuper(ConfigurationParseError, self).__init__(\n\t\t\t\"Parse error on setting parser. Error: %s\",\n\t\t\tconfiguration_filenames,\n\t\t\tparse_errors, parse_warnings,\n\t\t)\n\t\tif log_record:\n\t\t\tself.log_record = log_record\n\t\telse:\n\t\t\tself.log_record = None\n\n\n\tdef __str__(self):\n\t\treturn \"<ParseError for %s>\" % list(map(lambda x: str(x), self.configuration_"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n##--------------------------------------------------------------------------#\nclass BashLog(BashLogFile):\n\t\"\"\"A class for reading and writing bzip2 archives.\n\n\tIt uses the file \"bzip2\" and a \"zip\" method named \"read\".\n\tWhen instantiated, the following attributes are required:\n\n\t1. bzip2_options  -- The dict from bzip2.options file\n\t2. bzip2_output  -- The location of bzip2 output file\n\t3. bzip_opts  -- Options instance to be used; required if bzip_input is not provided\n\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tlogging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n\t\t\t\t\t\t\t\tformat='%(levelname)s:%(message)s')\n\t\tparser = ConfigParser.ConfigParser()\n\t\tparser.read(configuration)\n\t\tdict_for_parser = {}\n\t\tfor x in range(len(parser.items('configuration'))):\n\t\t\tconfig_file_parser = ConfigParser.ConfigParser()\n\t\t\tconfig_file_parser.read(parser.items('configuration')[x])\n\t\t\tdict_for_parser[x] = dict(config_file_parser.items('configurations')[x].items"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\t_logging = logging.getLogger(name)\n\t_logging.setLevel(level)\n\t# _logging.setLevel(level)\n\t\n\tif not len(configuration):\n\t\tlog_exception(\"No configuration file provided. Creating one.\")\n\t\tconfiguration = load.loadConfiguration(filename=None, create=False)\n\t\n\tlogging_file = os.path.abspath(filename)\n\twith open(configuration, 'r') as f: config_file = json.load(f)\n\t#logging_file_errors = configuration_file_errors['parse errors']\n\t\n\tlogging_"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tlogging.basicConfig()\n\t\t# Parse configuration file, expanding paths, adding environmental vars\n\t\tfilename_vars = self._parse(self.configuration)\t\t\n\n\t\tlogging.debug(\"Filenames passed validation\")\n\t\t\n\t\tlogger=logging.getLogger(self.name)\n\t\thandler=logging.FileHandler(filename_vars['log_conf_file'],mode='w')\n\t\thandler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s: %(message)s ',datefmt=\"%y/%m/%d %H:%M:%S\"))\n\t\tlogger.addHandler(handler)\n\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tconfig = {}\n\t\terror = []\n\t\tfor filename in filenames:\n\t\t\tconf_filename = os.path.join(self.configuration_dir, filename)\n\t\t\ttry:\n\t\t\t\tconfig = open(conf_filename)\n\t\t\t\tparse_errors, log_records = parse_configuration(config.read())\n\t\t\t\tdebug_print(log_records[1])\n\t\t\t\tfor parser_errors, parselog_entries in parse_errors.items():\n\t\t\t\t\traise ConfigLoadFailure(\"Couldn't load the configuration (%s):\\n\" % filename, parselog_entries)\n\t\t\texcept IOError,"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\ndef isIgnoredByDefaultValue(configurationFilename):\n\t\"\"\"\n\tDetermines if a configuration file cannot be parsed by the default value.\n\t\"\"\"\n\t# TODO: Make this the default?\n\tif os.sep in configurationFilename:\n\t\treturn 'ignore'\n\telse:\n\t\treturn 'fail'\n\ndef parseConfigurationFile(configurationFilename):\n\t\"\"\"\n\tParse a configuration file and return the parsed configuration dictionary.\n\t\"\"\"\n\tlogging.debug(\"Parsing configuration file: %s\" % configurationFilename)\n\t\t\n\t# Get the configuration info: configuration\n\tconfig"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t return self._f.read(ord(obj),0)\n\n\tdef tell(self):\n\t\treturn self._f.tell()\n\n\tdef __len__(self):\n\t\treturn self._f.size()\n\n\tdef __getitem__(self,obj):\n\t\tif isinstance(obj,slice):\n\t\t\tl = len(obj)\n\t\t\tif l == 0 :\n\t\t\t\tobj = (0,) * obj.ndim\n\t\t\treturn self._f.__getitem__(obj, 0)\n\t\treturn super(TiffIter,self).__getitem__(obj)\n\n\tdef __iter__(self):\n\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef to_c_array(obj): #Python 3.x, see http://bugs.python.org/issue4194\n\t\"\"\"\n\tConvert `obj` into one comparable type so that it can be shown\n\tin HTML/CSS.\n\n\t:param obj: The type of data to convert.\n\t\"\"\"\n\tif isinstance(obj, array.array):\n\t\treturn obj.tobytes()\n\telif isinstance(obj, unicode):\n\t\treturn obj.encode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn obj\n\telse:\n\t\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef test():\n\t# Check that we can import 'random.py' from this directory.   \n\tprint 'Testing for Python version %s...' % sys.version_info\n\tprint' This will launch the Python test suite (on unix/linux) and check that the generated files are generated.'\n\tprint' Import python modules for testing.'\t\t\t\t\t\n\tpythonpath = os.path.dirname(os.path.realpath(sys.argv[0]))+'/'\n\tpythonversion = sys.version_info[:3]== (2, 6) \t\t# Check Python 3 version"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n###################################################################\ndef \tget_type(data):\n\t\"\"\"\n\tThis is the type declaration of python type.\n\t\"\"\"\n\treturn \t\"c\"\t\ndef \tget_size(self):\n\t\"\"\"\n\tThis function returns the number of bytes contained in\n\ta python type that is defined in Python. \n\t\"\"\"\n\treturn get_type(0)\ndef \tsizeof(obj):\n\t\"\"\"\n\tThis function returns the size of the type declaration of\n\tPython object of known length. \n\t\"\"\"\n\treturn sizeof(obj)\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\timport struct\n\t\tarr = array.array(\"B\")\n\t\tarr.frombytes(obj)\n\t\tarr.tofile(sys.stdout);\n\tdef getvalue(self, obj):\n\t        buf = array.array('B')\n\t        buf.fromstring(obj)\n\t        n = buf.tostring()\n\t   \treturn  n\n\tdef getref(self, f):\n\t\treturn None\n\tdef getdata(self, f):\n\t        # return None if there is no data\n\t\tif self.ref:\n\t\t\treturn self.ref\n\tdef getlength(self"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef ustring_vector(obj, encoding, limit=None):\n\t\"\"\"\n\tConverts a string into a list of a single byte-string.\n\n\t`obj` can be:\n\t- a single character\n\t- a multi-byte string\n\t- a tuple of strings (one string per string)\n\t- a dictionary of strings (one dict per string)\n\n\t`encoding` is the text encoding, or 'utf-8' if none\n\t(default).\n\t\"\"\"\n\tdata = []\n\tnrow = 0\n\tfor c in obj.strip():\n\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef cbyte(obj):\n\t\"\"\"\t\n\tRetrieves a string based on the python 2.x or 3.x\n\tfile descriptor provided as argument.\n\tWill try and decode it afterwards.\n\tIf obj has the data type it will return it as is. \n\t\"\"\"\n\timport sys\n\treturn obj.decode('ascii')\n\t\n\ndef float2hex(f):\n\timport binascii\n\treturn binascii.hexlify(struct.unpack('f', struct.pack('I', f))[0].encode('ASCII'))\n\t\ndef hex2float(fp):\n\tassert fp"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t \n\tif isinstance (obj, basestring):\t# not string, obj is string as it is to be converted to bytes\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn obj\n\t\t\ndef fromStringToBytes(s, returnType='byte'):\n\t'''Converts string *s* to bytes, if type conversion fails, will \nconvert it to *type()*\n\t'''\n\tif isinstance(s, basestring):\t# fails, assume it is a unicode string\n\t\treturn s\n\tif s is None:\n\t\ts = ''\n\ttry:\n\t\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\nclass FancyCheck:\n\t\"\"\"\n\tThis object will check consistency and repairs of a\n\tnumber of data values that are not strictly equivalent to\n\tnumber of fields.  You can specify how the object behaves:\n\n      >>> FancyCheck(123, 0x0) # returns 0x0000\n      0\n      >>> FancyCheck(-123, 0x0) # returns 0x0000\n      1\n      >>> FancyCheck(123.123, 0x0) # returns -123.123\n      -123.123\n    \"\"\"\n\t\n\tdef __init__(self, obj):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\nclass PixelArray:\n\tdef __init__(self, dtype = int):\n\t\t\"\"\"Create a NumPy array with the given size. \n\t\t\"\"\"\n\t\ta = numpy.empty(0, dtype)\n\t\tself.arr = a\n\t\n\tdef __getslice__(self, start, stop):\n\t\t\"\"\"Get the start/stop index of a list slice.\n\t\t\"\"\"\n\t\tr = []\n\t\tif type(start) == str:\n\t\t\tstart = int(start, 10)\n\t\t\tfor i in range(start, stop + 1):\n\t\t\t\tr.append(self.arr"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    try:\n\tif tz is None or tz.utcoffset(ticks)!= 0.0:\n\t    raise ValueError()\n\treturn cls.__from_ticks_utc(ticks)\n#!/usr/bin/python\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    if not isinstance(ticks, (int, long, str, DateTime, date, Time)):\n        raise ValueError(\"ticks must be an integer value\")\n\n    if tz is None:    \n        return Time.from_seconds(ticks)\n    else:\n        return Time.from_fixed(Decimal(ticks))\n\ndef to_decimal_sec(number):\n    \"\"\"\n    Convert number to a decimal sec-since-midnight.\n    e.g. 0x01011011121314151614031416\n\t\"\"\"\n\n    # https://docs.python.org/2/library/decimal.html#basic-"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\t    if len(ticks)>86400:\n\t    raise ValueError(\"ticks must be in range 0 <= ticks < 864000000000\")\n    if ticks>3*60*60:\n        raise ValueError(\"ticks must be in range 0 <= ticks <= 3*60*60\")\n    hour=int(ticks//1)\n    minute=int(ticks//2)\n    second=int(ticks%1)\t\n    return Time(0, 0, minute, hours=hour, minutes=minute, seconds=second)\n\n\"\"\"\n       \nif __name__=='__main__':\n\tfrom random import randrange\n\tfrom"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\t\n\t# First we compute the time.\n\t# The algorithm used is the same as 'diff_with_tz' above.\n\tlast_tick = ticks - 86400 * 1e6\n\t# A time length of 1 hour corresponds to one second,\n\t# and a tick count of 60 that we subtract by\n\t# half an hour, and the total duration of minutes\n\t# in a second.\n\tdelta = (ticks - last_tick) + tz.utcoffset(last_tick)\n\t# Return an offset in microseconds.\n\t# On Windows, floating-point values smaller"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\traise ValueError(\"ticks out of bounds (\" + str(ticks) + \" <= \" + str(6400000000000) + \")\")\n\"\"\"\nThis module manages the registration of the courseware as a HLS consumer.\n\"\"\"\nimport logging\nimport os\nimport sys\n\nfrom django.conf import settings\nfrom django.core.cache import cache\nfrom pytz import UTC\nfrom requests import Timeout\n\nimport oauthlib.common\nfrom oauthlib.oauth1.rfc5849 import signature\nfrom provider.oauth2.models import Client\n\nfrom openedx.core.djangoapps.catalog.cache import ("}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t \n\tticks = float(ticks)\n\tif not (ticks % 1):\n\t\traise ValueError(\"ticks should be an integer value.\")\n\tticks -= 86400000000000\n\tticks += 1\n\treturn datetime.fromtimestamp(ticks)\n\ndef to_ticks(value, tz=None):\n\tvalue = int(value)\n\tdays = (int(ticks) - 1) / 7\n\tvalue += days * (2**32)\n\tvalue -= days * (2**16)\n\ttry:\n\t\treturn datetime.fromtimestamp(value)\n\texcept ValueError:\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\tif not isnanosleep(ticks): return zeros(12, 'f')\n\t\treturn array([float(ticks)])\n\ndef to_decimal_hour_precision(tick_dt):\n\t\"\"\"\n\tConvert ticks (datetime.time) to a number of seconds.\n\n\tThe resulting decimal-hour precision is:\n\n\tn = int(round(tick_dt.total_seconds() * tick_dt.nanosecond()))\n\td = int(round(tick_dt.microseconds * 25.4))\n\th = int(round((tick_dt.total_seconds"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n        if not (0 <= ticks):\traise ValueError('ticks out of bounds')\n\n        o = 0\n        u = 0\n        r = 0\n        jdnup_zero = 0.0\n\n        while o + r < ticks:\n            (d, m) = divmod (ticks - o, ticks)\n            if d == ticks:\n                s = 0\n            else:\n                s = ticks - o\n\n            (d, m) = divmod (ticks, ticks)\t# (ticks - o)\n            if d == ticks:\n                s -= 1\n            else:\n                o += 1\n\tu += s\t# (ticks -"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    if len(ticks)!= 4 or not ticks[0] < 0 or not ticks[1] < 0 or not ticks[2] < 0 or not ticks[3] < 0:\n            raise ValueError(\"ticks is out of bounds\")\n\n    delta = datetime.timedelta(seconds=ticks[2] or 0)\n    seconds = delta.total_seconds() * 1000000000\n\n    if tz:\n        timestamp = datetime.datetime.fromtimestamp(ticks[0], tz)\n    else:\n        timestamp = datetime.datetime.fromtimestamp(ticks[1])\n\n    return cls(seconds=seconds, delta=delta, timestamp="}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n#\tprint \"from_ticks(%s, %s, %s, %s)\" % (str(ticks), str(tz), str(ticks % 1), str(tz % 1))\n\t#\tprint ticks * 1000000.0\n\t#\tprint ticks / 1000000000.0\n\n\tticks -= 61075\n\t#print ticks\n\treturn Time(seconds=ticks / 1000000000.0)\n\n#\tif timezone.utc == None:\n#\t\tprint \"timezone.utc not found.\"\n#\t\traise InvalidDatetime\n\n#\ttimezone.replace(microsecond"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t    \n    if not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple of version numbers\")\n    if protocol_version is None:\n        return {\n            'handler': {\n               'version': {},\n                'handler': {\n                    'dispatch_status': {},\n                    'events': None,\n                    'transport': None,\n                   'send_event': None,\n                   'send_header': None,\n                   'send_body': None,\n                   'send_error': None,\n                   'request_reply': None,\n                   'request_return': None,\n                   'response_reply': None,\n                   "}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n\t\treturn dict()\n\t\n\thandler_map = {}\n\tfor version_number, version in protocol_version:\n\t\thandler = cls.handler_for_version(version_number, version)\n\t\tif handler.version_supported:\n\t\t\thandler_map[version_number] = {}\n\t\t\tfor version_name, version_class in version.items():\n\t\t\t\t# we have to add the default version by default\n\t\t\t\tdefault_handler = version_class(handler)\n\t\t\t\thandler_map[version_number][version_name] = default_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t# TODO this is a copy of dict to not clutter the protocol to allow\n\t\t#       the creation of \"version\" version for some plugins.\n\t\t_protocol_handlers = {}\n\t\tfor version in protocol_versions(protocol_version):\n\t\t\ttry:\n\t\t\t\tversion = int(version)\n\t\t\texcept (AssertionError, TypeError):\n\t\t\t\traise ProtocolError('invalid version argument for protocol version' + str(version))\n\t\t\t_protocol_handlers[protocol_version] = version\n\t\treturn _protocol_handlers\n\t# TODO provide a deprecation warning about an obsolete version which\n\t#       will"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t# convert to tuple if necessary\n\t\tprotocol_version = [ protocol_version ] if protocol_version else []\n\n\t\t# check protocol versions\n\t\tversion_tup_list = [ version_tuple for version_tuple in protocol_version if \\\n\t\t\t   version_tuple[0] == protocol_version[0] and version_tuple[1] >= 0]\n\t\t\n\t\t# list protocols and their respective version\n\thandler_dict = defaultdict(list)\n\ttry:\n\t\tfrom bolt.proto.api_pb2 import Message, Status\n\t\timport json\n\texcept ImportError:\n\t\ttry"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t       \n\tver_items = tuple(map(None, protocol_version))\n\tvers = None\n\tif not isinstance(ver_items, tuple):\n\t        raise TypeError(\"protocol_handlers() argument to protocol_handlers() must be a tuple of\"\n\t\t\t\t\t\t\t\t\t \" length %d\" % len(ver_items))\t\t\t\n\tfor version in ver_items:\n\t\tvers = vers or cls.get_versioned_version()\n\t\tversions[version] = cls._version2handler(vers)\n\treturn versions\n\n##############\n# Service handler classes #\n##############\nclass"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    handlers = {}\n\n    if protocol_version:\t\t# Get the latest compatible protocol version\n        for version in protocol_version:\t\t\t# Get all handlers for the specified version\n            name = version_name(version)\n            handlers[version] = name\n\n    # Try to find one version that satisfies the requested version\n    # TODO this should be extended, in case the other protocol versions \n    #        requested the same name in several places\n    for version in protocol_version:\n        version_handler = handlers.get(version)\n        if version_handler is None or version_handler() is None:\n            raise Bolt"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t# If no protocol version is provided,\n        # a list of all version is not supported and will just return\n        handlers = {}\n\n\t# if we provide a protocol version, use '1'\n\tif protocol_version is None:\n\t\treturn dict()\n\tif not isinstance(protocol_version, int):\n\t\traise TypeError('Protocol version must be an integer')\n\n\t# if we use '2' protocol and the version specified\n\tif protocol_version == (2, 2):\n\t\thandler_class = BoltProtocolHandler\n\telif protocol_version == (2, 4):\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = __all__\n\tm = _handlers.get(proto_version.upper(), None)\n\tif not m:\n\t\traise TypeError(\"Unknown Bolt protocol version '%s'\" % (proto_version,))\n\ttry:\n\t\treturn getattr(m, protocol_version)\n\texcept KeyError:\n\t\tm = _handlers[0]\n\t\traise type(m)(\"%s not available in protocol handlers\" % (protocol_version,))\n\n\ndef __all__ ():\n\treturn __bmap__[(cls,)]\n\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\traise TypeError(\"version is required\")\n\t\tif len(protocol_version) == 4:\n\t\t\traise TypeError(\"no protocol version specified\")\n\t\tall_handlers = {}\n\t\tsupported_transport_version = []\n\t\tfor version in protocol_version:\n\t\t\ttry:\n\t\t\t\tall_handlers[version] = cls.get_version(version)\n\t\t\t\tprotocol_version = version\n\t\t\texcept ValueError:\n\t\t\t\traise TypeError(\"Could not parse version string: %s\" % version)\n\t\tif not (protocol_version in all_handlers):\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\traise NotImplementedError(\"Only subclass ProtocolDescriptor\")\n# -*- coding: utf-8 -*-\n\n\"\"\"\n***************************************************************************\n    Processing.py\n    ---------------------\n    Date                 : May 2010\n    Copyright            : (C) 2010 by Michael Minn\n    Email                : pyqgis at michaelminn dot com\n***************************************************************************\n*                                                                         *\n*   This program is free software; you can redistribute it and/or modify  *\n*   it under the terms of the GNU General Public License as published by  *\n*   the Free Software Foundation; either version 2 of the License, or     *\n*   (at"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tret = cls()\n\t\tfor v in values:\n\t\t\tret.append(Bookmark(string_to_bookmark(v)))\n\t\treturn ret\n\n\nclass Bookmarks(object):\n\t\"\"\"\n\tAn object for the bookmarks on which we have data.\n\n\tBookmarks objects have properties that can have different fields, this\n\tis mainly used for internal bookmarks too. These are simply the text\n\tfor a bookmark you can use to get data for that particular bookmark. This\n\tis not useful however.\n\n\tBookmarks objects have fields that can't be in a bookmark object\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tbookmarks = []\n\t\twhile values:\n\t\t\tvalue_raw = values[:64]\n\t\t\tvalue = values[64:]\n\t\t\ttry:\n\t\t\t\tbookmark, raw_number, short_number, raw_data = \\\n\t\t\t\t\t\tvalue_raw.split(\":\")\n\t\t\t\tbookmarks.append(Bookmark(bookmark, int(raw_number), short_number,\n\t\t\t\t\t\tfloat(raw_data), raw_value_string))\n\t\t\texcept ValueError:\n\t\t\t\traise ValueError(\"Failed to unserialize bookmark %s\" % bookmark)\n\t\t\texcept IndexError:\n\t\t\t\tbreak\n\n\t\tfor bookmark in bookmarks:\n\t\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tfor bookmark in values:\n\t\t\tif not isinstance(bookmark, basestring):\n\t\t\t\traise TypeError('bookmark=%s is not a string' % bookmark)\n\t\t\telif cls.is_type(bookmark):\n\t\t\t\treturn bookmarks.Bookmark(bookmark)\n\t\t\telif cls.is_bookmark(bookmark):\n\t\t\t\treturn bookmark\n\t\t\telse:\n\t\t\t\traise TypeError('bookmark=%s is a non-existent bookmark' % bookmark)\n\n\t@classmethod\n\tdef from_bookmarks(cls, bookmarks):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a single BookmarkSet.\n\n\tReturns: A"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\traw_bookmarks = cls()\n\t\tfor value in values:\n\t\t\tbookmark = raw_bookmarks.add_bookmark(value)\n\t\t\traw_bookmarks.add_bookmark_by_index(bookmark.index)\n\"\"\"Support for Dreambox cameras.\"\"\"\nfrom homeassistant.components.camera import Camera\nfrom homeassistant.components.ffmpeg import DATA_FFMPEG\nfrom homeassistant.components.ffmpeg import DATA_MJPEG_STREAM\nfrom.const import DOMAIN as DREAMBOX_DOMAIN\n\nDEPENDENCIES = ['ds_pudocapture']\n\nDREAMBOX_"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tif len(values) % 2!= 0:\n\t\t\traise ValueError(\"Incorrect bookmark string length; not a multiple of 2! (2 points per line)\")\n\t\tvalues = [int(x, 16) for x in values]\n\t\t# XXX: Do something more useful and write a \"null\" bookmark instead\n\t\t#      of an empty list.\n\t\treturn cls._bookmarks.from_raw_values(values)\n\n\tdef append(self, bookmark):\n\t\t\"\"\"\n\t\tAppend a bookmark to the list.\n\n\t\t:param bookmark: string (raw) A raw bookmark string\n\t\t:"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tr = gospel.raw_values()\n\n\t\tr.sort()\n\t\tr = [x for x in [r.strip()]\n\t\t\t\tif not x.startswith('#')]\n\n\t\ti = 0\n\t\tname, value = values[i]\n\t\ttry:\n\t\t\tname = unquote(name)\n\t\texcept (TypeError, ValueError):\n\t\t\tname = name.encode(gospel.util.sys_encoding, \"replace\")\n\n\t\tbookmarks = cls(name=name, value=value)\n\t\tr.bookmarks.append(bookmarks)\n\n\t\ti += len(values)"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tfor bookmark_tuple in values:\n#\t\timport pdb; pdb.set_trace()\n\t\t\ttry:\n\t\t\t\tbookmark = bookmark_tuple[0]\n\t\t\texcept IndexError:\n\t\t\t\traise AssertionError(\"Malformed bookmark tuple: %r\" % bookmark_tuple)\n\t\t\ttry:\n\t\t\t\tbookmark_ext = bookmark_tuple[1]\n\t\t\texcept IndexError:\n\t\t\t\t# Not found? Try a version that doesn't match.\n\t\t\t\traise AssertionError(\"Malformed bookmark tuple: %r\" % bookmark_tuple)\n\t\t\t# For now, just store it in the bookmark, not to a list of"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\n\n\tdef __contains__(self, key):\n\t\treturn self.contains(key)\n\n\tdef __getitem__(self, key):\n\t\tkey = self.encodeKey(key)\n\t\tif key in self.__values:\n\t\t\treturn [self.__values[key][n] for n in self.getBookmarks()]\n\t\telse:\n\t\t\traise KeyError(\"No such bookmark\")\n\n\tdef __delitem__(self, key):\n\t\tkey = self.encodeKey(key)\n\t\tif key in self.__values:\n\t\t\tdel self.__values[key]\n\t\telse:\n\t\t\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tif not values:\n\t\t\traise EmptyBookmarksError\n\tif not values:\n\t\traise EmptyBookmarksError\n\n\tif len(values) == 0:\n\t\traise EmptyBookmarksError\n\n\tvalue = values[0] \n\titems = []\n\tstart = 0\n\tfor v in values[1:]:\n\t\titems.append(Bookmark(start, value))\n\t\tstart = start + len(v) + 1\n\tmarks = cls(start, *items) \n\tfor c in reversed(items):\n\t\tmarks.append(c)\n\treturn marks\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t return list(Bookmark(v.strip()) for v in values)\n\n\tdef get_raw_bookmarks(self):\n\t\treturn [ self.from_index(0) ]\n\n\tdef serialize_bookmark(self, bookmark):\n\t\tbookmark = {}\n\n\t\tindex = self._index_bytes\n\t\tlast_bookmark_index = index.last_bookmark_index\n\t\tfor i, bookmark in enumerate(self):\n\t\t\tbookmark[i] = bookmark_class(last_bookmark_index=last_bookmark_index())\n\t\t\tlast_bookmark_index = index.last_"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\t# NOTE: the line spacing in the function definition is very\n\t\t# inefficient, but there is no way to compute it from scratch,\n\t\t# so the type of the last element is in general.\n\t\tif bufsize == 1:\n\t\t\tbufsize = 100000\n\t\tif bufsize is None:\n\t\t\ttry:\n\t\t\t\tbuflen = _block_get_bufsize(seq)\n\t\t\texcept (ValueError, OverflowError):\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tbufsize = buflen\n\t\treturn seq[:bufsize], seq[bufsize:]\n\n\ndef _make_fileobj(obj, chunk_size):\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n\n\tif bufsize is None:\n\t\tseqlen = 0\n\telif isinstance(bufsize, int):\n\t\ttry:\n\t\t\tbufsize = int(bufsize)\n\t\texcept ValueError:\n\t\t\traise ValueError(-1, \"invalid bufsize\")\n\n\tif len(seq) < bufsize:\n\t\traise ValueError(-1, \"can't create sequence of up to %d bytes\" % bufsize)\n\n\tseq = iter(seq)\t\t# support iterators <http://docs.python.org/ref/collections.html#collections.Iterator>}\n\n\tif seqlen!= 0:\n\t\tif hasattr(seq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t return _SeqWrapper(seq), hasattr(seq, \"__getitem__\")\n\ndef make_seqwrapper_from_seq(seq):\n\t\"\"\"\n\tMake wrapper around sequence.\n\t\"\"\"\n\tif hasattr(seq, \"__getitem__\"):\n\t\treturn seq\n\treturn _SeqWrapper(seq)\n\nclass Wrapper:\n\t\"\"\"\n\tA wrapper around the sequence (iterable) or iterable\n\t\"\"\"\n\n\tdef __init__(self, seq):\n\t\tself._seq = seq\n\t\tself._offset = 0\n\n\tdef __iter__(self):\n\t\treturn self\n\n\t@property"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq)\n\t\tif len(seq) == 0:\n\t\t\tif type(seq) is not tuple:\n\t\t\t\treturn ()\n\t\t\telse:\n\t\t\t\treturn (seq,)\n\t\telse:\n\t\t\tret = None\n\t\t\tseq_result = []\n\t\t\tfor item in seq:\n\t\t\t\tif hasattr(item, \"find\"):\n\t\t\t\t\tseq_result.extend(item.find(bufsize))\n\t\t\t\telif hasattr(item, \"find_seq\"):\n\t\t\t\t\tseq_result.extend(item.find_seq(bufsize))\n\t\t\t\telif hasattr"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t return sum, map(_get_type, seq), bufsize\n\n\nclass Base(object):\n\tdef __init__(self, parent=None):\n\t\tif parent is None:\n\t\t\tparent = _get_seq()\n\t\t\tif self.context is not None:\n\t\t\t\tself.parent = parent\n\t\t\telse:\n\t\t\t\tself.parent = None\n\t\telse:\n\t\t\tself.parent = parent\n\t\tself._children = []\n\n\tdef _new_child(self, name, _name=None):\n\t\tassert _name is None\n\t\tassert not _name is None\n\t\tassert"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize:\n\t\t\tif type == 'fasta':\n\t\t\t\tfmt = _fasta_arg('U')\n\t\t\telif type == 'qual':\n\t\t\t\tfmt = _qual_arg()\n\n\t\tif types:\n\t\t\tfmt = [_fasta_arg(s) for s in types]\n\t\telse:\n\t\t\tfmt = ''\n\n\t\tdata = _fasta_arg(seq)\n\t\tret = _fasta_header(*data)\n\n\t\tif bufsize: ret += _fasta_bufsize(ret, bufsize)\n\t\treturn (ret, fmt)\n\n# TODO: make _get_seq_with"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\ndef _read_seq_str(file):\n\t\"\"\"\n\tRead a sequence from file. Raise ValueError if not enough data.\n\t\"\"\"\n\ts = ''\n\twith open(file, 'r') if bufsize==None else open(file, 'r', bufsize) as f:\n\t\tfor line in f:\n\t\t\ts += line\n\t\t\tif s[-1]!= '\\\\': s += '\\\\'\n\treturn (s[:-1].encode(), s[:-1])\t\n\t\t\t\t\nif __name__ == '__main__':\n\timport sys\n\t\n\tdata = read_sequence(sys.argv"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif seq is None:\n\t\t\treturn ([], None)\n\n\t\tif not hasattr(seq, 'dtype'):\n\t\t\tseq = tuple(seq)\n\n\t\t\tif maxlen is None:\t# TODO: Make sure this happens\n\t\t\t\tmaxlen = 10\n\t\t\telse:\n\n\t\t\t\tif hasattr(seq, '__len__'):\n\t\t\t\t\t\tmaxlen = len(seq)\n\n\t\t\telif hasattr(seq, 'dtype'):\t# TODO: Make sure this happens\n\t\t\t\traise NotImplementedError # TODO: this block is not present\n\t\t\telse:\n\t\t\t\traise NotImplementedError # TODO\n\t\t\t# end try/except\n\n\t\tstart = max(0,"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n        # This may be extended, so we do it twice:\n        if hasattr(seq, \"next\"):\n            # Check if the sequence type is compatible\n            if seq.type!= TYPE_SEQUENCE:\n                # not the sequence, and we expect the next value\n                seq = seq.next()\n            item = seq.dereference()\n            type = item.type\n            # The sequence type is either DATA or CHAR*\n            return item, type\n        else:\n            # Otherwise, try to make the sequences\n            item = None\n            while True:\n                if bufsize is None or len(seq) == bufsize:\n                    # We haven't hit"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\treturn seq\n\n\t\ttypelens = list(get_typelens(seq, type_conv))\n\t\tif len(typelens)!= len(seq):\n\t\t\traise TypeError(\"sequence %s has incorrect\" % seq)\n\n\t\t# Make sure all the types we got are same length\n\t\tif len(typelens)!= len(set(typelens)):\n\t\t\t# Sequence type mappings can be for example []?\n\t\t\traise ValueError(\"sequence type mappings for %s do not match\" % seq)\n\n\t\t# Add all the lengths of the elements of each type by"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t                                                       \n\tself.normalize(other, recompute=reduc_check())\n\n\tother = self.copy()\n\tif other is not None:\n\t\tother.resize(self.size())\n\t\n\tif scale is None:\n\t\tm = self._mean(other)\n\t\tif mean is not None:\n\t\t\tm /= mean\n\t\tif variance is not None:\n\t\t\tm **= variance\n\t\n\telif isinstance(scale, Numeric.array) and scale.typecode == 'f':\n\t\tif scale > 0.:\n\t\t\traise Exception(('The \\'%s\\' scale is defined in"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\n    if self.bins!= 'uniform' and other is None:\n\t    self._rescale('scale')\n\t    self._rescale('scale')\n\t    self._rescale('scale')\n\t    return\n\tbins = self.bins\n\t\n\thist = np.empty(bins+1)\n\tfor i in range(bins):\n\t    row = self.histogram[i]\n\t    try:\n\t        i0 = i - 1  # 0 index\n\t\trow -= 1\n\t    except IndexError:\n\t        if i < 0:\n\t\t    row = 0\n\t    if i == bins-1:\n\t        continue\n                hist[i]"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\t# compute the scale ourselves...\n\t\tif other is None:\n\t\t\tother = self\n\t\tif not other.is_empty():\n\t\t\tother /= self.scale()\n\n\t\t# compute the new scale:\n\t\tscale_ = float(other.width()) / float(other.height())\n\n\t\t# compute our new shape:\n\t\treturn self.__class__(\n\t\t\tother.meaning(),\n\t\t\tother.interval(),\n\t\t)\n\n\tdef _compute_with_data(self, **kwds):\n\t\t\"\"\"\n\tCompute this histogram with data drawn from data points. We\n\tuse"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other==None:\n\t\t\treturn self\n\t\t\n\t\t# Compute scale as well\n\t\ts = self.scale_hist\n\t\treturn s.rescale(other)\n\n\tdef copy(self):\n\t\t\"\"\"\n\t\tMake a copy of a histogram object.\n\t\t\"\"\"\n\t\treturn self.copy_with(self.hist())\n\n\t@staticmethod\n\tdef unpack(hist):\n\t\t\"\"\"\n\t\tReturn a (scale, n, q) binned histogram object.\n\t\t\"\"\"\n\t\tret = hist.copy()\n\t\ti = histogram._NREFS\n\t\twhile i > 0"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\ttry:\n\t\t\tother = float(other)\n\t\texcept ValueError:\n\t\t\traise ValueError(_exc.InvalidArg('%s scale is not valid' % other))\n\t\tif other == 0.:\n\t\t\treturn # this is a null interval, which means \"never-zero\"\n\n\telif isinstance(other, np.ndarray) and other == 0.:\n\t\ttry:\n\t\t\tother = float(other.item())\n\t\texcept (ValueError, TypeError, AttributeError):\n\t\t\traise ValueError(_exc.InvalidArg('%s scale is not valid' % other))\n\n\telif isinstance(other, (int,"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif isinstance(other, LenaPlot):\n\t\t\tother = other * other\n\t\telse:\n\t\t\traise TypeError(\"cannot rescale %s to %s\" % (other.__class__.__name__, \\\n\t\t\t\ttype(other)))\n\t\treturn self.scale(other, recompute=recompute)\n\n\tdef scale_to(self, other, recompute=False):\n\t\t\"\"\"\n\tCompute or set scale by adding the other histogram.\n\nIf *other* is ``None``, the scaling of the histogram is made\nautomatically using the same scale and the other one in a given way"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t # if is_zero is False, try that now. not too hard to be the case: the\n        # case that will actually try an expensive resize.\n\t\tif not self._initialized:\n\t\t\tself.reset()\n\n\t\tother = _scale_upsample_or_add(other, (self.num_elements(), ), self.num_elements())\n\n\t\tif other is None:\n\t\t\traise TypeError(\n\t\t\t\t'other of histogram with no elements')\n\n\t\tself.num_elements()(other)\n\t\tself(other)\n\tscale.__doc__ = DiscreteScale._scale_docstring"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\traise ValueError(\"other cannot be None\")\n\t\tif other <= 0 or other >= 0:\n\t\t\traise IndexError(\"index out of bounds\")\n\t\tif self.minval is not None and other <= 0:\n\t\t\traise ValueError(\"scale is only partially valid\")\n\t\n\t\tif other is not None:\n\t\t\ttry:\n\t\t\t\tself[-other].scale(*self[:-other])\n\t\t\texcept IndexError:\n\t\t\t\traise ValueError(\"cannot normalize {} with {}\".format(other, self))\n\n\tdef _min_common_keys(self, other):\n\t\t\"\"\"Check if the"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\tif self.range[0] < 0:\n\t\t\t\t# TODO: this is a bit sloppy in that both are zero in\n\t\t\t\t#       both sides. If we just want to support zero-length\n\t\t\t\t#       (e.g. converting a 0 to a 0.0), this will change\n\t\t\t\t#       the length.\n\t\t\t\treturn _scale_(self.range[0])\n\t\t\telse:\n\t\t\t\t# otherwise, just compute based on a vector of length\n\t\t\t\t# equal to that of a vector of points of equal length\n\t\t\t\t# and possibly"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None: return self\n\t\tself_ = _check_scale(self)\n\t\tother_ = _check_scale(other, scale=1.0)\n\t\ttry: result = self_.__class__.scale(self_, other_)\n\t\texcept LascropyError: raise LascropyError('Scaling with {} failed: '.format(\n\t\t\ttype(self).__name__))\n\t\telse: return result\n\n\tdef hist_ratio(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set histogram ratio (ratio for a histogram).\n\nIf *"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif self.dataIn() is None:\n\t\t\tself.dataIn = self.findCoordinate('x')\n\t\t\tself._err = self.getError()\n\t\telse:\n\t\t\tif other is None:\n\t\t\t\t#if no other, make the min and max values zero\n\t\t\t\tother = ()\n\t\t\t\tself._min = (0, 0)\n\t\t\t\tself._max = (1, 1)\n\t\t\t\tself.dataIn(other)\n\t\t\telse:\n\t\t\t\tother = tuple(other)\n\t\t\t\tself._min = tuple(*other)\n\t\t\t\tself._max = tuple(*other)"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\to = self.copy(deep=0)\n\t\telif isinstance(other, LinearSpace):\n\t\t\to = self.__class__(**other.bounds)\n\t\telse:\n\t\t\traise NotImplementedError('Need to implement this!')\n\t\to.set_x(o.x)\n\t\to.set_y(o.y)\n\t\treturn o\n\n\tdef translate(self, xoff=0, yoff=0):\n\t\t\"\"\"\n\t\tTranslate a point in the graph by *xref* (default 0). *xref* \n\t\tin the graph is *"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif type(other) == type(None):\n\t\t\traise AnnAxisException('Can not scale to a nonexistant x or y coordinate')\n\t\tif other == None:\n\t\t\tother = nx.zeros(3)\n\t\tif other.x == 0:\n\t\t\tother.x = 1.\n\t\tif other.y == 0:\n\t\t\tother.y = 0.\n\t\treturn self._scale(other)\n\n\tdef transpose(self, other=None):\t\t\"\"\"\n\tTranspose coordinates, in place, from *other* to self.\n\n\tReturn self."}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tgraph = self._get_graph()\t\t\t\t\t\n\n\t\tif other is None or other == 0 or \\\n\t\t   other is not None:\n\t\t\t# get the new size of the graph\n\t\t\tnew_size = self.size()\n\t\t\tif self.unknown() or new_size == 0:\n\t\t\t\tnew_size = other.size()\n\t\t\tif new_size == self.size():\n\t\t\t\treturn self\n\t\t\tif self.unknown():\n\t\t\t\traise ValueError, \\\n\t\t\t\t   \"cannot rescale other to self.size()\"\n\n\t\telif isinstance(other, Vector2):\n\t\t\t#rescale"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef rotate(self, angle):\n\t\t\"\"\"\n\tRotates the graph clockwise by some degree by *angle*,\n\tin radians counterclockwise by the *rotate* function.\n\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef triangulation(self, x=-1, y=-1, z=-1, a=1.0, b=1.0):\n\t\t\"\"\"\n\tReturns a mesh with *x* & *y* points connected by four quadratic\n\tcoordinates of equal length.\n\n\t:param x: The horizontal coordinates of each point in the"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tassert graphlib is not None,\\\n\t\t   \"graphlib argument must be valid graph lib (e.g. Graph)\"\n\t\tassert isinstance(other, (float,int),\"'other' must be an instance of Graph, not %r\" % other\n\t\tself._graphlib = Graph(other)\n\t\treturn self\n\n\tdef getNpzMap(self):\n\t\t\"\"\"\n\t\tThe current value of the vector NpzMap (or None if nothing\n\t\twas specified). If *None* is provided, no data will be returned.\n\t\t\"\"\"\n\t\ttry: return bool("}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t # XXX: We could use bounds here and do a bit of work here.\n\treturn self._scale(other, False)\n\n\tdef inverse_scale(self, other=None): # XXX: R1705\n\t \"\"\"\n\tGet or set the inverse of a graph.\n\nIf *other* is, return the inverse of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero inverse,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used."}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self\n\t\telse:\n\t\t\tassert isinstance(other, self.__class__)\n\t\t\treturn other.scale(self)\n\t\t\n\tdef __getitem__(self, n=1):\n\t\traise NotImplementedError\n\n\tdef _repr(self, format_spec='plain'):\n\t\t\"\"\" Return a string representation of the graph. For use by the\n\t\tPython graph module \"\"\"\n\t\t# TODO: This would be nice if the graph is the\n\t\t# only graph to be printed? Maybe the graph should\n\t\t# be rendered directly in graph.tex or graph.html?\n\t"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other and other.units == self.units:\n\t\t\tif self.isUnit() and self.hasX():\n\t\t\t\t# In case the value has only 2 dims, the default is 3\n\t\t\t\td = self\n            elif self.isUnit() and isinstance(other, Unit.Type):\n\t\t\t\td = other.convert(self)\n\t\t\telse:\n\t\t\t\t# if the value has 3 dims, there is no conversion needed\n\t\t\t\traise ValueError(\"Not a unit\")\n\t\t\t# make sure its x and y are valid\n\t\t\tif d[0, 1] == 1 or d[2,"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\tother_matrix = self.copy_matrix()\n\t\telse:\n\t\t\tother_matrix = self.copy(other)\n\n\t\tother_matrix *= -1\n\t\tother_matrix = other_matrix.multiply_ones(self.shape)\n\t\treturn other_matrix\n\t\n\t@property\n\tdef data(self): return self._data\n\n\t@data.setter\n\tdef data(self, data_array):\n\t    if data_array.shape == (3,):\n\t            # we convert this to an array of floats\n\t    else:\n\t            # we convert"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t,\n\toptions=HistogramOptions(plot_options),\n\tdoc=_histogramdoc)\n\tif type(make_value) is bool:\n\t\tmake_value = lambda x: make_value and x or 0\n\telif make_value is None and make_value is not None:\n\t\tmake_value = make_value or make_value or 0\n\tmake_value = make_value(make_x)\n\tif type(axes) is int:\n\t\tif make_x:\n\t\t\taxes = [ make_value(y) for y in axes]\n\t\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\n\t\treturn(hist.as_matrix()*make_value(hist.value, make_value, get_coordinate, field_names, scale=scale))\n\n\nclass Hist(object):\n\t\"\"\"\n\tHIST class is an abstraction over histogram of a dataset. It\n\tcontains methods for dealing with histograms.\n\tThe first constructor will be called by the\n\t`HistogramAggregator` which will iterate over hist_bins to make\n\thistograms. If `numbins` and `normalize` are set, bins are reduced\n\tby the value of \\cdots/number of"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t)\n\n\tfig1 = plt.figure()\n\tax1 = fig1.add_subplot(111)\n\n\thist.plot(make_value=make_value, get_value=get_value,\n\t\t\txscale=\"log\",\n\t\t\thist_kws={'range': (0.02,1.0), 'nbins': 4},\n\t\t\tcmap=matplotlib.cm.gist_heat_r)\n\n\tif make_value:\n\t\tax1.scatter(make_value(bin_.mean), make_value(bin_.mean_error),\n\t\t\t\tedgecolor"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n    if make_value is None:\n\t\tmake_value = lambda x: None\n\t\n\tif get_coordinate==\"left\":\n\t\tmake_value=hist_to_graph(hist,make_value=make_value, get_coordinate=\"left\")\n\telif get_coordinate==\"right\":\n\t\tmake_value=hist_to_graph(hist,make_value=make_value, get_coordinate=\"right\")\n\telif get_coordinate==\"middle\":\n\t\tmake_value=hist_to_graph(hist,make_value=make_value, get_coordinate=\"middle"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\treturn HDR_Graph(hist, make_value, get_coordinate, field_names,\n\t\t\t\t\t\t   field_numbers, scale)\n\n\tdef hist2d(hist, bins=None, scale=True):\t#XXX Deprecated. Use histogram.\n\t\t\"\"\"Convert a 1D Histogram into a 2D histogram and optionally\n\t\treturn the values as it would be.\t\"\"\"\n\t\treturn HDR2D(hist, bins=bins, scale=scale)\n\n\"\"\"\ndef hist_to_histogram(hist, field=None,\n                      bins=None, field_names=None,"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t# check length of histogram\n\tif len(hist) < 3:\n\t\traise RuntimeError(\"Must have at least 3 values\")\n\n\tif make_value is None:\n\t\tmake_value = lambda bin: float(bin[1])\n\n\thist = hist[::-1]\n\n\tgraph = make_value(hist, make_value)\n\n\t# convert to graph if asked\n\tif not make_value:\n\t\tmake_value = lambda bin: bin\n\n\t# create node\n\tif get_coordinate == \"left\":\n\t\tgraph.set_style(\"dashed\")\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t.format(field_names=field_names)\n\treturn _histtograph.hist(hist, make_value=make_value, get_coordinate=get_coordinate, **kwds)\n\ndef add_numerical_histogram(hist, **kwds):\n\twarnings.warn(\"`add_numerical_histogram` is deprecated and will be removed in version 0.4.0\", category=FutureWarning)\n\ttry:\n\t\thist = hist.dropna(*kwds)\n\texcept:\n\t\twarnings.warn(\"Could not create histogram when no numerical data was present!\", category="}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tfrom graph import Graph\n\t\tassert isinstance(hist, Graph), \"hist\"\n\t\tassert len(hist) > 0, \"hist missing\"\n\t\treturn Graph(hist)\n\t\t\t\t\n\t# set default width of the bar\n\thist_width = kwargs.get( 'hist_width', 100 )\n\thist_height = kwargs.get( 'hist_height', 200 )\n\t\n\thist_graph = graph_factory( hist_width = hist_width, hist_height = hist_height )\n\thist_hist = make_histogram( hist_width, hist_height )\n\t\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tdefault_range = list(graph.range) + [1,1,1,1,1]\n\t\tminimized_range = ([v for v in default_range if v is not None])\n\t\thist_range = [0.0] * len(minimized_range)\n\n\t\t# this method is used to create the \"new\" hist data\n\t\t# for a histogram bin\n\t\tfor binobj in hist:\n\t\t\t# create the histogram histogram bin\n\t\t\thistogram = (binobj.num_edges, binobj.mean, binobj.max, binobj."}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t        )\n\"\"\"Base class for sparse matrix format handlers.\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\n__all__ = []\n\nfrom functools import partial\nfrom..base import spmatrix, isspmatrix, SparseEfficiencyWarning\nfrom.data import _data_matrix, _minmax_mixin\nfrom.dia import dia_matrix\n\n\nclass SparseFormat(spmatrix.spmatrix):\n    \"\"\"\n    A sparse matrix format handler.\n\n    ``spmatrix`` is the standard superclass to `SparseFormat`.\n\n    ``data.shape = (m,n)``\n      The"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    iface = iface.providedBy(candidate)\n    tentative = tentative or iface.isOrExtends(iface)\n    if not tentative: return False\n    tcid = iface.providedBy(candidate)\n    if tentative:\n        for x in tentative.getInterfaces():\n            for y in x[1]:\n                for z in y[1]:\n                    if not (y[0][0]==z[0] \\\n                            and y [0][1]==z[0]):\n                        raise zope.interface.Invalid(\"%s%s [%d] found here\"\n                                              % ("}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if candidate.providedBy == iface or \\\n\t   candidate.providedBy == iface.original:\n\t    return True\n\t\t\n\telif candidate.providedBy is None:\n\t    return False\n\t\t\n\telif isinstance(candidate, basestring):\n\t    # If the candidate defines an iface that must be implemented by\n\t    # the interface, we check that the candidate's class's\n\t    # interfaces match the interface's bases.\n\t    if (candidate.namespace == iface.namespace and \n        \tcandidate.name == iface.name and \n        \tcandidate.namespace is not None and \n        \tcandidate.name in iface.bases):\n\t\t    return True\n\t\n\t   "}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\t\n\tif __debug__:\n\t\tmethod = candidate.name\n\t\tnamespace = candidate.namespace()\n\telse:\n\t\tmethod = candidate.name + '.' + get_dotted_name(candidate)\n\t\tnamespace = namespace if namespace else candidate.namespace()\n\tns_iface = iface.providedBy(iface)\n\t\n\tif vtype:\n\t\treturn candidate.compareVersions(iface, typ, **kw)\n\telse:\n\t\treturn candidate.compareVersions(iface, **kw)\nimport warnings\nimport numpy as np\nfrom pandas.compat import (range, lrange, lmap"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    is_error = candidate is thing\n    if not is_error: raise TypeError(\"candidate %s must be a %s\" %\n         (candidate.__name__, type(candidate)))\n    if not inspect.isfunction(candidate): \n        msg = \"'%s' can be used as a function. If `providedBy` or \" \\\n            \"the class constructor is given it must define a 'iface' method.\" % candidate\n        raise TypeError(msg)\n    if isinstance(candidate, type):\n        try:\n            is_c = candidate.__class__ is cassandurvey._is_a(\"zope.interface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\t\n\t# The first argument is the error\n\tif candidate is None:\n\t\trval = None\n\t\tif not isinstance(iface, Interface): #pragma NO COVERAGE\n\t\t\ttry:\n\t\t\t\tiface\n\t\t\t\trval = _verify(iface, tentative=tentative, vtype=vtype)\n\t\t\texcept Invalid:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\t# We tried too often -- see if the interface name corresponds\n\t\t\t\t# to an interface known to the code it has.  If it is not,\n\t\t\t# we raise a Validator.\n\t\t\t\tif not ( iface.provided"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # check if we should be providing single interface and the thing\n    # is a known one\n    if tentative and (iface is not None or\n    #   candidate.__providedBy__ is None or\n    #   candidate.__providedBy__ == iface.__name__):\n\t    raise zope.interface.Invalid(\"Only single-interface is \"\n\t\t\t\t\t\t\t   \"currently supported for \"\n\t\t\t\t\t\t\t   \"implementations\")\n\n    # check if we should provide a particular interface type\n    # (which must be None in order to get the correct interface)\n    # and check it for valid specifications\n    if (tentative and not (iface is not None"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # check for illegal inputs\n    forbidden_attrs = zope.interface.Attribute.invalid_attributes\n    for attr in unmet_attrs:\n\t    forbidden_attrs.remove(attr)\n\n    if candidate is None:\n        if iface == iface.object:\n            # special case: we've already tried to add an interface\n            # with an attribute that we weren't allowed to do\n            return True\n    \n        raise zope.interface.Invalid(\n            \"Attempt to add an interface that isn't already known\")\n    iface_method = zope.interface.providedBy(candidate)\n    if iface_method is Interface:\n        log.warning"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if not kw: #pragma: no cover\n        raise Invalid(iface, candidate)\n    elif len(kw) == 1: #single-kwargs\n        def _check_attr(iface, candidate, kw):\n            # The error must be a single element in the extracted attribute\n            # dictionary.\n            for a, v in kw.items():\n                if isinstance(a, dict) and iface is not iface:\n                    break\n                elif isinstance(v, dict) and iface is not iface:\n                    break\n                elif v!= kw[a]:\n                    return False\n            return True\n        candidates = [candidate]\n    else: #pragma"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\t\n\tif not iface.providedBy(candidate):\n\t\traise Invalid('A candidate is required '\n\t\t\t'\\'with an iface that is not: %s.\\'\" % candidate)\n\t\n\tif not candidate.providedBy(iface):\n\t\traise Invalid('%s: %s.' % (iface, candidate))\n\tif not hasattr(candidate,'query'):\n\t\traise Invalid('%s: %s.%s does not provide query interface.' %\n\t\t\t\t\t (iface, candidate.name, candidate.__class__.__name__))\n\t\n\tif candidate not in iface.query:\n\t\traise Invalid('%s: %s.%"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\n\ndef classImplements(iface, specified):\n\t\"\"\"\n\tReturn the class implementing the interface with the specified\n\tinterfaces.\n\t\"\"\"\n\tif not specified:\n\t    return iface\n\telif iface not in specified:\n\t    raise zope.interface.Invalid(\"%s.%s doesn't have '%s'\" % \\\n\t    (iface.__module__, iface.__name__, \\\n\t    specified))\n\telse:\n\t    return iface\n\traise zope.interface.Invalid(\"%s.%s has no '%s'\" % \\\n\t    (iface.__module__, iface.__name__, specified))\t\n\t\ndef providedBy(iface,"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\treturn not iface.providedBy(candidate)\n\t\n\tif not (iface.isInterface() and not iface.ifaces()):\n\t\traise Invalid, \"This interface or interfaces may not be available.\"\n\n\tif not iface.providedBy(candidate):\n\t\traise Invalid, \"Computer is required for objects that aren't provided by %s.\" % candidate.__name__\n\n\tif not iface.contains(candidate):\n\t\traise Invalid, \"Object with ID %s is missing the required '%s' interface.\" % (candidate.id, iface.name)\n\n\t# XXX - what if interfaces are required"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n      \n#!flask/bin/python\n# encoding=utf-8\n\nimport urllib\nimport urllib2\nimport cookielib\nimport re\nimport time\n\ndef main():\n\n   s = cookielib.CookieJar()\n   s.load('cookie.txt')\n   domain =  'http://www.baidu.com'\n   opener = urllib2.build_opener(urllib2.HTTPSHandler())\n   req = urllib2.Request(domain,None,\n              headers = { 'User-Agent' :\n                    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\treturn False\n\n\t# NOTE: we should not return non-None if there's no match, therefor\n\t# checks other exceptions so as not to interfere with obscure\n\t# logic.\n\tif len(obscure) == 0:\n\t\tfor failure in self._failure:\n\t\t\tif failure.trap(DeprecationWarning):\n\t\t\t\tcontinue\n\t\t\tif iface == 'zope.interface.exceptions' and failure.getTraceback() is not None:\n\t\t\t\treturn True\n\n\t# we only use a single place to avoid the distinction between an\n\t# interface as reported"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # There might be multiple methods but no attributes!\n    if oneOf(map(lambda x: x.required_by, iface.names)) > 1:\n        msg = (\"Methods %s are required, but only one of %s was \"\n               \"supplied\" % (iface, iface.names))\n        raise Invalid, msg\n\n    # We check that the candidate is indeed known.\n    if iface.__class__ is not candidate.__class__:\n        raise Invalid, \"%s is not a subclass of %s\" % (iface,\n                                                         candidate.__class__)\n\n    if iface.implementedBy(candidate, iface=iface"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n        for k, v in iface.error.args.iteritems():\n            if not [d for d in v if d[:1] == 'gecko']:\n                raise e\n\tif not iface.providedBy(candidate):\n\t    raise zope.interface.Invalid(\"Incompatible interface: \" + \\\n\t                                       candidate.name())\n\telif not candidate.isMethod():\n\t    raise zope.interface.Invalid(\"Implementation not provided\")\n        cpos = 0\n\t\n        if cpos!= len(candidate.signatures()):\n\t    raise zope.interface.Invalid(\"Constructor requires \" + \\\n\t        \"at least one signature,"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    def present(iface, candidate):\n\t\tif not iface:\n\t\t\treturn True\n\t\tif iface.providedBy(candidate):\n\t\t\treturn True\n\t\n\t\treturn iface.providedBy(candidate)\n\t\n\tdef check(iface, candidate, tentative):\n\t\tif not iface:\n\t\t\tiface = iface.implementedBy(candidate)\n\t\tif not tentative:\n\t\t\treturn iface.providedBy(candidate)\n\t\tif iface.providedBy(candidate): \n\t\t\treturn iface\n\t\n\tif not iface:\n\t\traise Invalid, \"iface not provided by interface\"\n\t\t# assert(0"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    #XXX: Should be merged with some sort of checks to allow for\n    #bug reports and such.\n\n    for candidate, message in candidate.interfaces():\n        if not iface.implements(candidate, *message):\n            continue\n        iface = candidate\n\n    for message in _messageAttributes(message, iface, iface.__name__):\n        iface = getattr(iface, message)\n        if iface.providedBy(candidate, *message):\n            return False\n\n    if config.requirements.required:\n        return _verifyObject(iface, candidate, tentative=True)\n\n    msg = \"Tentative (%s) is not present."}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    interface = iface\n    requiredMethods = iface\n    methodNames = getMethodNames(interface, providedBy, tentative)\n    for name in methodNames:\n\ttry:\n\t    method = availableMethods[name]\n\texcept KeyError:\n\t    raise exc.InterfaceError(iface, name, interface)\n\n\t# The arguments to pass to the exception constructor have changed:\n\t# - accept just the required and optional arguments\n\t# - reject them if present - otherwise return failure\n\targs, varargs, keywords, defaults = inspect.getargspec(method)\n\n\tif defaults:\n\t    # Don't include self and"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t    \n\treturn True\n\n    def validate(iface):\t# is no need to validate anything here, as\n\t#     - just for a sanity check, will already verify that it\n\t#     is a class that implements the 'providedBy' interface.\n\tpass\n# Copyright (c) 2013 OpenStack Foundation.  All rights reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    \ndef implements(*interfaces):\n\t\"\"\"\n\tDeclare and return all the implementations that\n\timplement *interface* provides on a minimum interface.\n\t\n\tThis assumes that the interface itself advertises all of its members\n\t(see the documentation of :func:`zope.interface.InterfaceClass.getDependent`).\n\t\n\tThis function is intended to be used as a decorator for a\n\tClass decorator; it allows you to provide a mechanism for adding and/or\n\tdisabling one or more methods with different signatures.\n\t\n\t>>> from zope.interface import Interface\n\t>>> class I1("}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tcandidate = os.path.realpath(candidate)\n\t\tif not os.access(candidate, os.R_OK):\n\t\t\traise SafeException(\"not readable directory\")\n\t\tifaceFile = \"%s/interface.%s\" % (settings.MEDIA_ROOT, candidate)\n\n\t\treturn ifaceFile in [ \"xul-info\", \"xul-path\", \"xul-url\" ]\n\tif tentative:\n\t\treturn os.access(os.path.dirname(os.path.join(settings.MEDIA_ROOT,'%s' % candidate)), os.R_OK"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tt1 = threading.Thread(target=verifyClassIndication, args=(iface, tentative))\n\t\tt2 = threading.Thread(target=verifyClassIndication, args=(iface, not tentative))\n\t\tif not tentative:\n\t\t\tt1.start()\n\t\t\tt2.start()\n\t\t\tt1.join()\n\t\t\tt2.join()\n\t\twhile t1.isAlive():\n\t\t\tt2.join()\n\t\treturn iface\n\n\tresult = None\n\tfor iface in getConfiguredAdapters():\n\t\tif testclass(iface):\n\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif iface is None and tentative:\n\t\t\treturn 1\n\t\telse:\n\t\t\t# Everything looks ok\n\t\t\treturn (candidate is True)\n\ndef verifyClassAndTypes(iface, candidate, tentative=False):\n\tif iface is None and tentative:\n#\t\tprint \"Verifying class type...\",\n#\t\tdebug(repr(iface))\n\t\treturn 1\n\n\tif tentative:\n\t\t# Everything looks ok\n\t\treturn (candidate is True)\n\telse:\n\t\t# Everything NOT looks good - sometime in a real world"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif iface is None or not iface.isPrimitive():\n\t\t\treturn False\n\t\tif tentative and not iface.ctor():\n\t\t\treturn False\n\t\tfor iface1 in iface.getDescriptors(ipk.IFACE_IFACE_IV):\n\t\t\tif not iface1.getDecryption(ipk.IFACE_IV_ACCEPT):\n\t\t\t\tcontinue\n\t\t\tspec = iface1.getExtendedAttributeString(\"nID\")\n\t\t\tif int(spec) & 2:\n\t\t\t\treturn True\n\t\treturn False\n\n\tdef checkTrustable(self, iface, trusted=False"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif tentative and not iface.isInterface():\n\t\t\treturn True\n\t\tfor iface_candidate in iface.getDescriptors():\n\t\t\tif iface_candidate.matches(candidate):\n\t\t\t\treturn True\n\treturn False\n\ndef verifyClassAdvertise(candidate, tentative=False):\n\t\"\"\"\n\tVerify that it *can* provide *candidate* for both the *iface* and the *candidate*\n\tany candidate with the same **attributes*.\n\t\"\"\"\n\tfor iface_candidate in iface.getDescriptors():\n\t\tif iface_candidate.matches(candidate):"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\ttry:\n\t\t\ta = raw_input(iface)\n\t\t\tif not isNull(string.join(candidate,\"\")): # this should be some sort of check for equality\n\t\t\t\ttry:\n\t\t\t\t\tassert len(a) > 0 # Should have 0 or 1\n\t\t\t\t\tassert len(a) <= 1 # should have 1 or none\n\t\t\t\t\tassert len(string.lstrip(toByteString(a))) >= 1 # should have non-whitespace characters at the end\n\t\t\t\texcept:\n\t\t\t\t\traise InvalidInterface(_(\"Interface has empty string as first argument to verifyClass\"))\n\t\t\t\treturn\n\t\texcept:\n\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif tentative and iface in iface_candidates:\n\t\t\treturn iface_candidates[iface]\n\t\t\t\t\n\t\t# First see if the iface can be used (from the iface list or we will\n\t\t# probably consider the possibility to use the provided package)\n\t\tport = iface.unresolved().port\n\t\tif port is not None and port >= 0:\n\t\t\tfor iface in iface_candidates:\n\t\t\t\tif iface.id == port:\n\t\t\t\t\tpossibleSignatures = iface.signers.keys()\n\t\t\t\t\tif len(possibleSignatures) == 1:\n\t\t\t\t\t\t# Only support testing if they're"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tname = toBindingName(candidate)\n\t\tif iface is None:\n\t\t\t# Completely negative interfaces are not allowed.\n\t\t\treturn\n\t\tif tentative:\n\t\t\tiface = (candidate, iface[0])\n\t\tif iface not in self.__classInfo:\n\t\t\tself.__classInfo[iface] = {}\n\t\tif iface not in self.__classInfo[iface]:\n\t\t\tifaces = self.__classInfo[iface][iface] = []\n\t\t\tif iface == config.usage.extensions.wlan.wlan().lookup():\n\t\t\t\tifaces.append(self.__class"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\t#XXX - this seems wrong.\n\t\ttry:\n\t\t\tifaceObj = candidate.__class__\n\t\texcept :\n\t\t\tifaceObj = candidate\n\t\treturn ifaceObj.__name__ == iface.__name__ if tentative else False\n\treturn 0\t\n\nclass RcInterfaceWrapper(object):\n\t\"\"\"\n\tThis is a wrapper for the RcBridge class.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize this object.\n\t\t\"\"\"\t\t\n\t\tself._iface_cache = {}\n\n\tdef __getitem__(self, ifaceId):\n\t\t\"\"\""}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tfrom zeroinstall.injector import verifyObject\n\t\tif iface not in iface_cache: iface_cache[(iface, candidate)]\t= candidate\n\tif tentative is False: return True\n\telse: return trusted[3] and trusted[2]\n\nclass System(Element):\n\tdef __init__(self, name, filename, installed_by, values,\n\ttype, orig_file = None, remote_repo=None, fetchonly=False):\n\t\t\"\"\"\n\t\tArguments:\n\t\tname\t\t--\tThe basename of the package\n\t\tfilename\t--\tThe"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif bases:\tbases = bases.split(',') # Split into a comma into name, module and file name\n\texplicit_mc = None\n\tfor base in getmro(bases):\n\t\tclazz = find_class_in_module(base, explicit_mc)\n\t\tif clazz: return clazz\n\treturn None\n\ndef find_class_in_module(module, explicit_mc=None):\n\t\"\"\" Look for the class definition within a module, a parent\n\t\tof the module is returned.\"\"\"\n\tmodule = module.__name__\n\tif module == '__init"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tmeta = {}\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = 1\n\t\tfor base in bases:\n\t\t\tif base in explicit_mc.mro():\n\t\t\t\tmeta['metaclass'] = base\n\t\treturn meta\n\nclass _ClassMethodDescriptor:\n\t\"\"\"\n\tPickling support class methods, no python equivalent.\n\t\"\"\"\n\t@staticmethod\n\tdef declare_new_attribute(cls, name, default_value):\n\t\t\"\"\"\n\t\tAdd a name to the class and assigns a docstring to the method.\n\t\t\"\"\"\n\t\tdef mfunc_default("}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif not explicit_mc: explicitly_mc=explicit_mc\n\t\tresult = []\n\t\tif bases:\n\t\t\twhile bases:\n\t\t\t\tbases_name = bases.__class__.__name__\n\t\t\t\tif bases_name not in explicit_mc:\n\t\t\t\t\t\traise MixError(\"The given class %s is already present in the list of bases '%s', which is not present in the database. I cannot be \"\n\t\t\t\t\t\t\t      \"found later.\") % (bases, bases_name)\n\t\t\t\telif hasattr(bases, '_meta'):  # this is added on Django >= 1.8.11 and 2."}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\ttry:\n\t\t\tmro = bases + (explicit_mc,)\n\t\t\tif explicit_mc is not None:\n\t\t\t\tmro = exclude_implicit_metaclass(mro)\n\t\texcept TypeError:\n\t\t\tpass\n\t\treturn mro\n\n\t# override the __new__ for classmethod or some common __new__\n\t__new__ = type.__new__\n\n\t# add type attributes for convenience\n\t__metaclass__ = determineMetaclass\n\n\tdef __init__(self, name, bases, kwds):\n\t\tself.name = name\n\t\tself.bases = bases\n\t\tself.kwds = kwds\n\n\tdef"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = {}\n\t\texplicit_bases = explicit_mc.get(\"bases\",bases)\n\t\tif explicit_bases is not None:\n\t\t\treturn explicit_bases\n\n\t\texplicit_mc = {\n\t\t\t\"dict\": abcs.dict_map.full_mapping,\n\t\t\t\"Enum\": abcs.enum_style_list,\n\t\t\t\"EnumItem\": abcs.dict_map.full_mapping,\n\t\t\t\"IntEnum\": abcs.int_style_list,\n\t\t\t\"IntEnumItem\": abcs.int_"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = bases[0]\n\t\timplicit_mc = explicit_mc.split('.') if explicit_mc else ()\n\t\tif implicit_mc:\n\t\t\timplicit_mc = '__'+implicit_mc[0]+ '__'\n\t\tinherited_mc = explicit_mc[:implicit_mc.index('(')]\n\t\tinherited_mc.reverse() # the first line of the output, ie. 'class'\n\t\textra_kwargs = explicit_mc[implicit_mc.index('('):] # strip off the \"self\"\n\t\tcls_dict"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tis_metaclass = 0\n\t\tfor base in bases:\n\t\t\tif hasattr(base, \"_metaclass_\"):\n\t\t\t\tis_metaclass |= 1\n\t\t\t\tbases.remove(base)\n\t\tif explicit_mc is not None:\n\t\t\tif hasattr(explicit_mc, \"_metaclass_\"):\n\t\t\t\tis_metaclass |= 1\n\t\t\t\timplicit_mc = explicit_mc._metaclass_\n\t\t\telse:\n\t\t\t\timplicit_mc = ()\n\t\t\tfor base in bases:\n\t\t\t\tif hasattr(base, \"_metaclass_\"):\n\t\t\t\t\tis_metaclass |= 1\n\t\t\t\t\tbases.remove(base)\n\t\tif"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\timplicit_mc = []\n\t\telse:\n\t\t\timplicit_mc += explicit_mc\n\n\t\tbases[0][-1] = explicit_mc\n\t\treturn bases, implicit_mc[::-1]\n\telse:\n\t\treturn bases, None\n\nclass MetaBase(type, metaclass(MetaBase, bases, classType)):\n\tmetaType = type\n\tdef __new__(cls, name, bases, attrs):\n\t\tnewclass = super(MetaBase, cls).__new__(cls, name, bases, attrs)\n\t\tif 'Meta' not"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t try:\n\t\t\tfrom bson.builtins import (object, UserDict, UserDictWrapper, UserString)\n\t\texcept ImportError:\n\t\t\tfrom bson.utils.classes import (object,)\n\t\tclass meta(object):\n\t\t\t__metaclass__ = MetaMeta\n\t\treturn meta\n\tif explicit_mc is None: preferred_mc = _preferred_mc\n\tif bases == (): preferred_bases = ()\n\telse: preferred_bases = set(bases); preferred_bases_set = set(preferred_bases)\n\tmc = determineMetaclass(preferred_bases, explicit_mc); mc.__bases"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\n    if bases:\n        raise TypeError(f\"The following 'bases' must specify a class and an explicit __metaclass__; got: {bases}\")\n\n    explicit_mc=explicit_mc or {}\n    explicit_mc.update(explicit_mc)\n\n    cls = None\n    try:\n        for base in bases:\n            if not issubclass(base, base) and issubclass(base[1], (type, base)):\n                raise ValueError(f\"{class_str_from_object} -- The 'bases' attribute on '{base}' needs to be type (type, base[1], and not {base[0]}"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif not self.d.has_key(key):\n\t\t\treturn default\n\t\tself.d[key] = self.d.get(key, default)\n\t\treturn self.d[key]\n\n\ndef getValue(self, key, d=''):\n\tv = self.root_key.get(key, None)\n\tif v is None:\n\t\tv = self.root_value.get(key, '')\n\telif isinstance(v, types.ListType):\n\t\tv = v[0]\n\telif not self.d.has_key(key):"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise\n\t\t\treturn default\n\tpop.__doc__ = pop.__doc__\n\tdel __marker, pop\n\n\ndef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair\n\tof a and b.\t\"\"\"\n\ttry:\n\t\t(k, v) = self.items()[0]\n\texcept IndexError:\n\t\traise KeyError('popitem(): dictionary is empty')\n\n\tremove"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\tif default==self.__marker: raise\n\t\t\telse: return default\n\t\telse: return v\n\tdef popitem(self): # real signature unknown; restored from __doc__\n\t\traise KeyError('dictionary is empty')\n\tdef __reduce__(self): # real signature unknown; restored from __doc__\n\t\t\"\"\" D.__reduce__() -> (k, v), remove entries from D.\"\"\"\n\t\ttry:\n\t\t\treturn self.__class__, (self.get(k, None) for k in self)\n\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t            \n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\t\n\tdef setdefault(self, key, default=__marker):\n\t\tv = self.pop(key)\n\t\treturn v\n\n\tdef update(self, d):\n\t\t\"Pushing a DICT dictionary is equivalent to copying keys from d.\"\n\t\t# Using the standard dictionary.\n\t\tself.update(d)\n\t\t# Removing the entry from the filesystem assumes that if the dictionary is\n\t\t# initialized with a dictionary instance, it needs to be initialized\n\t\t# before reading.\n\t\tif hasattr"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\t\t\n\tdef __repr__(self): return '<pop>'+repr(self.__dict__)\n\t\n\tdef __setitem__(self, key, value):\n\t\tif not is_py3 and isinstance(value, bytes):\n\t\t\tvalue = value.encode(\"utf-8\")\n\t\tself._remove_key(key)\n\t\treturn self.set(key, value)\n\t\n\tdef __getitem__(self, key):\t\n\t\tval = self.get(key)\n\t\tif val is None:\n\t\t\traise KeyError(key"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self.map:\n\t\t\tv, d = self.map[key]\n\t\t\tdel self.map[key]\n\t\t\treturn v, d\n\t\telse:\n\t\t\traise KeyError(key)\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tremove and return an item given as a (item, k) tuple;\n\t\traise KeyError if D is empty.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t(key, val) = self.head\n\t\texcept KeyError:\n\t\t\traise KeyError(\"%r is empty\" % (self.head,))\n\t\telse:\n\t\t\tif"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\ttmp = self[key]\n\t\t\tdel self[key]\n\t\t\treturn tmp\n\t\treturn default\n\n\nclass dqueue (object):\n\tdef __init__(self, maxsize=512):\n\t\tself._queue = []\n\t\tself._maxsize = maxsize\n\n\tdef empty(self):\n\t\treturn len(self._queue) == 0\n\n\tdef put(self, item, priority=0):\n\t\tif len(self._queue) > self.maxsize:\n\t\t\titem = item[:self.maxsize]\n\t\tself._queue.append(item)"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__tags[key]\n\t\texcept KeyError:\n\t\t\traise KeyError(key)\n\tdef __repr__(self):\n\t\treturn \"<D>\"\n\n##////////////////////////////////////////////////////////////\n##  Compatibility Graphs\n##////////////////////////////////////////////////////////////\n\n##////////////////////////////////////////////////////////////\n##  UnImplementedEdgeSubgraph\n##////////////////////////////////////////////////////////////\n\ndef findUnImplementedEdges(G):\n\t\"\"\"\n\tFind (G.subgraph.keys(), G.node[v]) where 'v' is a subset of nodes of\n\tnode 'G'.\n\t\"\"\""}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t        try:\n                        return self.__getitem__(key)\n                except KeyError:\n                        pass\n\n\t\texcept:\n\t\t\traise ConfigError(\"[%s] pop: KeyError: %s\" %(self.__class__.__name__, key))\nfrom __future__ import print_function, division\n\nfrom sympy.core import Eq, S, Symbol, Tuple, sympify\nfrom sympy.core.compatibility import range\nfrom sympy.core.function import Lambda\nfrom sympy.solvers import solve, solve_linear_system, solve_linear_system_LU, Q\nfrom sympy.functions.elementary.piecewise import"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\ts = self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise\n\t\t\treturn default\n\t\treturn s.pop()\n\ndef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair\n\tpair as a 2-tuple; this is handy to use when you want to remove\n\tkeys from the dictionary.\n\t\"\"\"\n\ttry:\n\t\titem = self.items[-1]\n\texcept IndexError:\n\t\traise KeyError('dictionary is"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\treturn (self.key, self.value)\n\n\tdef __contains__(self, item):\n\t\treturn (item == self.key) or (item == self.value)\n\n\nclass LRUCache:\n\t\"\"\"\n\tThis class provides backwards compatibility for the 'live' lru cache. The\n\tresult is cached for up to 3600 seconds and a new LRU (TTL) for the cache.\n\t\"\"\"\n\tdef __init__(self, capacity):\n\t\tself._capacity = capacity\n\t\tself._map = {}\n\t\tself._queue = []\n\n\tdef __getitem__("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tassert not self._index, \"popitem required for this thread\"\n\t\tpk,val = self._index.pop()\n\t\tself._index.remove(pk)\n\t\tif len(self) > self._maxlength:\n\t\t\tself._index.clear()\n\t\t\tself._index.update(self._maxlen-len(self))\n\t# Pops the popularity queue,\n\t#  (and all of the keys,value pairs for each node).\n\t#  Notice that while we use this for optimization,\n\t#  it works this way too often!\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tl = []\n\t\tfor j in reversed(list(self.items())):\n\t\t\ttry:\n\t\t\t\ti = self.items().index(j)\n\t\t\t\tl.append((self.items()[i], j) + (self.items()[i][1],))\n\t\t\texcept ValueError:\t\n\t\t\t\tl.append((-1, j));\n\t\treturn (l, key)\n\n\tdef iteritems(self,):   return self.items().__iter__()\n\tdef keys(self, ): return filter(itemgetter(0), self.iteritems())\n\tdef values(self, ): return self.items"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\twhile True:\n\t\t\tkey = self.first()\n\t\t\tif self.first() == key:\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tyield key, self.first()\n\n\tdef pop(self):\n\t\t\"\"\"\n\tRemove an arbitrary value from the map.\t\n\tIf the map is empty, remove a KeyError.\n\tIf the map is not empty, return ``(key, value)`` pair.\t\n\tIf the map is empty, remove all entries from the map (default).\n\t\t\"\"\"\n\t\tif self.isempty():\n\t\t\traise KeyError(\"the map is empty\")\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\td = dict.pop(self, -1)\n\t\tif d == {}:\n\t\t\treturn (key, value)\n\t\telse:\n\t\t\treturn key, d\n\n\tdef pop(self, key, default = None):\n\t\t\"\"\"\n\t\tReturns the most frequently used key/value pair for *key*. If *default* is\n\t\tspecified, it should be a dictionary that will be returned in the\n\t\tfollowing form:\n\n\t\t \t{'f' : {'value': 0.1, 'key': 'value,..., key: value',...}}\n\n\t\t\tIn this case, the value"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tif not self:\n\t\t\traise ValueError(\"Popitem must be accessed before it can be removed\")\n\t\tif self.key in self:\n\t\t\tself.value = self.pop(self.key)\n\t\t\tself.n -= 1\n\t\t\tself.key = self.key + 1\n\t\telse:\n\t\t\treturn None\n\n\tdef peekitem(self):\t\"\"\"\n\tPeek an item from the heap, and remove it from the heap until it is no longer in the heap.\n\tIf no free or the heap is empty, removes and returns None instead of raising a ValueError.\n\t\"\"\""}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\titem = self.pop()\n\t\tself.save()\n\t\treturn (item[0], item[1])\n\n\tdef _add_items_between(self, start, stop):\n\t\tstart = max(start, 0)\n\t\tstop  = min(stop, len(self) - 1)\n\t\tfor i in range(start, stop):\n\t\t\titem = self[i]\n\t\t\tif i >= start:\n\t\t\t\tfor j in range(start, stop):\n\t\t\t\t\titem = _Tuple(item)\n\t\treturn item\n\n\tdef _remove_empty(self):\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\ttry:return self.cachepop()\n\t\texcept KeyError:pass\t\t# self[key] can be a tuple\n\t\texcept TypeError: pass\n\t\titem = [(k, self[k]) for k in self._keys]\n\t\titem.sort(key=operator.itemgetter(1), reverse=True)\t\t# sort by key, then item[0].count(key)\n\t\tdel self._keys[0]\n\t\tself._save(*item)\n\n\tdef pop(self, key, *default):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value) list"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tv = super(HashlistKeyList, self).popitem()\n\t\tv = tuple(v)\n\t\trv = super(HashlistKeyList, self).pop(key, None)\n\t\treturn rv\n\n\tdef __delitem__(self, key):\t\t\"\"\"\n\tRemove an item from the hashlist.  Key must be a string containing the\n\tname of the current set of items.  The name must be a real\n\tstring name.\n\t\"\"\"\n        self.hgetall(key)\n\t\tindex = self.hget(key)\n\t\tv = tuple(("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\treturn self.pop(key=(lambda _x: not _x[0]))\n\tpopitem = popitem\n\t\n\nif __name__ == \"__main__\":\n\tprint ('*** Example 5 **')\n\tprint ('*** Simple XPSSX2PSSS XPSSX2PSSS')\n\tprint ('0x00000001')\n\tprint ('0x00000052') # (0x00000054 <--)\n\tprint ('0x00000051') # (0x00000055 <--)\n\tprint ('0x00000054 <--')\n\tprint ('0x"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\ttop = self[self.n - 1]\n\t\tbottom = top[self.n]\n\n\t\t# if `top` is always a leaf\n\t\tif top is self:\n\t\t\treturn bottom\r\n\r\n\t\t# if we are in the middle of a merge we need to pop all the nodes that are \n\t\t# not in the middle, otherwise the `k`th leaf should be the value of `key` too, which would be a \n\t\t# mistake for a user's input\n\t\tif top[0]!= None:\n\t\t\tstack.pop(top[0])\n\n\t\t# grab the"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\twhile 1:\n\t\t\tline = self.readline()\n\t\t\tif not line:\n\t\t\t\traise StopIteration\n\t\t\tif line[-1:] == '\\n':\n\t\t\t\tbreak\n\t\treturn line[:-1], line[-1:]\n\t\n\tdef remove_first_line(self):\n\t\t\"\"\"Return the contents of the last unread entry.\n\t\t\"\"\"\n\t\tfor linenum in range(len(self)-1, -1, -1):\n\t\t\tline = self[linenum]\n\t\t\tif len(line) > 1 and line[0] == self[-1]:\n\t\t\t\tline = line[1:]"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self.popitemleft(), self.popitemright()\n\n\tdef back(self, key, value):\n\t\t\"\"\"\n\t\tRetrieve the given `(key, value)` pair with the old value at\n\t\tthe same place.\n\t\t\"\"\"\n\t\tleft = self[key]\n\t\tif left is not None:\n\t\t\treturn left, left.pop(value)\n\t\telse:\n\t\t\treturn [], left\n\n\tdef front(self, key, value):\n\t\t\"\"\"\n\t\tRetrieve the given `(key, value)` pair with the new value at\n\t\tthe same place.\n\t\t\"\"\""}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif not self._cache:\n\t\t\tself._cache = list(self._root.keys())\n\t\tif self._cache in self._cache_map:\n\t\t\tdel self._cache_map[self._cache]\n\t\tif not self._cache_lock:\n\t\t\tself._cache_unlock()\n\t\tif len(self._cache) == 0:\n\t\t\traise KeyError('no answer')\n\t\tk, v = self._cache[-1]\n\t\tresult = self._cache_map.pop(k, (-1, None))\n\t\tif len(self._cache) > 0:"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\titems = [(x[0],x[2]) for x in self.getall() if x[0] == self]\n\t\tif not items:\n\t\t\titems = [(x[0],x[1]) for x in self.getall()]\n\t\tif len(items):\n\t\t\tvalue = max(items, key=operator.itemgetter(1))[1]\n\t\telse:\n\t\t\tvalue = None\n\t\tself.store(values=(value, value))\n\t\tif len(self.cache)>5:\n\t\t\twhile len(self.cache)>0:\n\t\t\t\t(k,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\twhile len(self.stack): \n\t\t\tself.stack.pop()\n\t\ttry:\n\t\t\treturn (self.stack[-1][1], self.stack[-1][0])  # pop from queue, return value\n\t\texcept IndexError:\n\t\t\traise IndexError(\"pop from queue: length is zero, index is %d\"%len(self.stack)-1)\n\n\tdef __init__(self, queue, level):\n\t\t\"\"\"\n\t\tConstructor from an deque, so as to ignore certain methods.\n\t\t\"\"\"\n\t\theapq.heappush(self.stack, (level, queue"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tpopitem = self.peekitem\n\t\tself.push(item=itemitem)\n\t\tif self.move_on:\n\t\t\tself.move()\n\t\treturn itemitem,popitem\n\n\tdef peekitem(self):\treturn self.space.get_item(self.peekitem)\n\n\tdef append(self,itemitem):\t\"Append an item to the list, with a trailing newline.\"\n\t\"\"\"\n\tAppend a new list item.\n\t\"\"\"\n\tself.append_op(\n\t\t\tself.items + [itemitem]+[None]*(len(self)+1"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif (len(self._map) == 0):\n\t\t\traise KeyError(\"'popitem()'\")\n\t\t\n\t\tif self._pos < len(self._map):\n\t\t\tself._remove(self._map.pop(self._pos))\n\t\t\n\t\treturn self._map.popitem(last=False)[1]\nfrom unittest import TestCase\n\nfrom app.user.models import User\nfrom app.user.services import UserService, ServiceType\n\nfrom app.alert.models import Alert\n\nfrom app.alert.models.rules import RulesService\n\n\nclass TestRulesService(TestCase):\n\n    @classmethod"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self.data.popitem()\n\tdef clear(self):\n\t\t\"\"\"\n\t\tRemove all items from the collection.\n\t\t\"\"\"\n\t\tself.data.clear()\n\tdef pop(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tRemove and return any (key, value) pair that has some, else None.\n\t\t\"\"\"\n\t\treturn self.data.popitem(*args, **kwargs)\nimport sys\n\nfrom nose.tools import assert_equal\n\nfrom. import helpers\n\n\ndef test_parse_file(tmpdir, parser, mock_runner, cap"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tvalue, key = _strip(self.pop())\n\t\twhile 1:\n\t\t\ttry:\n\t\t\t\treturn self.pop(key)[0], value\n\t\t\texcept IndexError:\n\t\t\t\tif self._ignore_missing_key(key):\n\t\t\t\t\tpass\n\n\tdef has_key(self, key):\n\t\t\"\"\"Returns *True* if *key* is in the cache, but does not update it.\"\"\"\n\t\treturn _strip(self.get(key))!= None\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"Retrieve the value associated with *key* if applicable,"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tvalue = None\n\t\tself.lpoplpush(self.stack_pop())\n\t\treturn (value, self.stack_pop())\n\n\tdef pop_all(self, key):\n\t\t\"\"\"Remove and return the most recently used `key` items.\n\t\tIf the stack is empty, return ``None``.\n\t\tRaise :exc:`KeyError` if the stack is empty.\n\t\t\"\"\"\n\t\tvalue, popleft = popitem(self, key)\n\t\tvalue = popleft()\n\t\tself.lpushlpush(self.stack_pop())\n\t\treturn (value,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\ttry:\n\t\t\tkey = self._popitemname()\n\t\texcept KeyError:\n\t\t\traise KeyError(key)\n\t\tself._statlog('popitem', key)\n\t\treturn key, self._popitem_val(key)\n\n\tdef clearall(self):\t\"\"\"\n\tRemove all keys from the mailbox.  This function does not\n\tremove all messages from the mailbox itself, but does care about\n\tthemselves.\n\t\"\"\"\t\t\n\t\ttry:\n\t\t\tself._clear_all()\n\t\texcept AttributeError: pass\n\t\t\n\tdef _new_key(self):\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tif self.isdirty:\n\t\t\tself.isdirty = False\n\t\t\traise AttributeError, \"No mutable class was put here\"\n\n\t\t# This code should be changed when this is called:\n\t\t#self.savepath.append(os.path.normpath(os.path.dirname(self.savepath)))\n\t\t#self.load_save()\n\n\t\tif self.cachetime < time.time():\n\t\t\tself.load_save()\n\t\t\treturn self.load(True)\n\n\t\telse:\n\t\t\tself.savepath = []\n\t\t\tself.load_save()\n\t\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\t(key, value) = self.popitem()\n\t\tif key in self.__tokdict:\n\t\t\tdel self.__tokdict[key]\n\t\treturn (key, value)\n\n\tdef pop(self, *args, **kwargs):\n\t\t'''\n\t\tPop an item.  Most often called with an empty key-value pair,\n\t\twhich Guido will usually use a list. Removes all duplicates and returns them.\n\t\tPer default this is called by the default key/value pair.\n\n\t\t'''\n\t\tif len(args) > 1:\n\t\t\traise TypeError, \"pop expected at most"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\ttry: self[key]\n\t\texcept KeyError: pass\n\t\telse: raise KeyError\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tEmpty the cache\n\t\t\"\"\"\n\t\tdel self[:]\n\n\tdef update(self, other):\n\t\t\"\"\"\n\t\tUpdate the cache\n\t\t\"\"\"\n\t\tself.clear()\n\t\tother.clear()\n\t\tself.update(other)\n#!/usr/bin/python\n# coding: utf-8\n# Licensed to the StackStorm, Inc ('StackStorm') under one or more\n# contributor license agreements.  See the NOTICE file distributed"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tassert self.item, \"Item for clear\"\n\t\titem = self.item\n\t\tself.item, self.pos = self.pos, None\n\t\treturn item,\n\n\tdef __getitem__(self, key):\t\"\"\"\n\tUnpickle and return the given `key`.\t\"\"\"\n\tassert self.item, \"Item for clear\"\n\tassert isinstance(key, str) and key == self.item, \"Wrong type of key\"\n\tassert key!= \"value\", \"key does not match value\"\n\titem, self.pos = self.item, None\n\tself"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\ttry:\n\t\t\titem = self.list[self.index(self.current)-1]\n\t\texcept IndexError:\n\t\t\tpass\n\n\t\tself.list.remove(self.current)\n\t\tself.list.insert(0, item)\n\n\t\treturn item\n\n\tdef remove(self, item):\n\t\t\"\"\"\n\t\tRemove the `(key, value)` pair from the list.\n\t\t\"\"\"\n\t\tself.list.remove(self.current)\n\t\tif self.current <= len(self.list) - 1: self.list.append(self.current)\n\n\t# ----------------------------------------------------------------------"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\treturn self.pop()\n\tdef top(self):\t\t\"\"\"\n\tReturn the top element on the list, or raise `StopIteration` if the list is empty.\n\t\"\"\"\n\t\tk = self.head()\n\t\tif k == None:\n\t\t\traise StopIteration()\n\t\telse:\n\t\t\treturn k.value\n\tdef size(self):\treturn len(self)\n\tdef search(self, item, value):\n\t\tfound = False\n\t\tfor (k,v) in self._children.items():\n\t\t\tif v == item:\n\t\t\t\tfound = True\n\t\t\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tif len(self) == 0:\n\t\t\traise KeyError\n\t\telse:\n\t\t\tret = self.pop(random.random())\n\t\t\tself.remove(ret)\n\t\t\treturn ret\n\tdef popitem_and_remove(self, key):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tif len(self) == 0:\n\t\t\traise KeyError\n\t\telse:\n\t\t\tif key in self:\n\t\t\t\twhile True:\n\t\t\t\t\tret = self.pop(random.random())\n\t\t\t\t\tif ret == key: break\n\t\t\t\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\twhile not self._empty():\n\t\t\tself.remove()\n\t\twhile not self._empty():\n\t\t\twith self:\n\t\t\t\tself.popitem()\n\t\t\t\tkey, value = self.popitem()\n\t\t\t\treturn key, value\n\n\tdef remove(self):\n\t\t\"\"\"Remove everything under the topmost `_key` pair.\"\"\"\n\t\tif self._empty():\n\t\t\traise KeyError(\"can't remove '%s' after deleting\" % self.label)\n\t\twhile not self._full():\n\t\t\twith self:\n\t\t\t\tself.popitem()\n\t\t\t\ttry:\n\t\t\t\t\tdel self[self._key]\n\t\t\t\t"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tif self._items:\n\t\t\treturn heapq.heappop(self._items)[1]\n\t\telse:\n\t\t\traise RuntimeError, \"%s not in the heap\" % self.__class__.__name__\n# coding: utf-8\n\nfrom __future__ import unicode_literals\n\nfrom.common import InfoExtractor\nfrom..utils import (\n    int_or_none,\n    orderedSet,\n)\n\n\nclass ZypeIE(InfoExtractor):\n    _VALID_URL = r'https?://(?:www\\.)?zype\\.com/(?:videos|(?:[^/]+/)?(?P<id>[^/]+)?)?"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tvalue = self.values.pop(0)\t# should result in an IndexError\n\t\treturn (value['key'], value)\n\n\tdef random(self, n):\n\t\t\"\"\"\n\t\tReturn a random `n` elements from the set.\n\t\t\"\"\"\n\t\treturn [self.values.pop() for i in xrange(n)]\n\n\tdef copy(self):\n\t\t\"\"\"\n\t\tReturn a shallow copy.\n\t\t\"\"\"\n\t\treturn self.__class__(self.values.copy())\n\n\tdef __cmp__(self, other):\n\t\t\"\"\"\n\t\tSort, sort and compare the list"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tself[self.randomkey()] = self.randomvalue()\n\n\tdef popkey(self):\t\"\"\"\n\tRemove a random `(key, value)` pair, or random its key\n\tas described above.  Remove a randomly chosen key (or an empty\n\tlist raises a ValueError of the same form).\n\t\"\"\"\n\t\tk = random.choice(self.keys)\n\t\tself[k] = self.randomvalue()\n\t\treturn k\n\n\tdef update(self, data):\n\t\t\"\"\"\n\tAdd a new key-value pair, or update an existing key-value pair"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tv = choice(self._gen)\n\t\treturn self._dict.get(v)\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tRemove all items from the pool.\n\t\t\"\"\"\n\t\tself._dict.clear()\n\n\tdef __getitem__(self, key):\t#pylint: disable=W0221\n\t\t\"\"\"\n\t\tGet random item from the pool.\n\t\t@param key: Key to get value from pool.\n\t\t@return: A random `(key, value)` pair.\n\t\t\"\"\"\n\t\tif isinstance(key, slice):\n\t\t\tindex = key.start or"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\ttoprint, pos = self._poptop()\n\t\tself._pos = self.find(toprint, pos)\n\t\treturn (self._keys[0].pop(), self._children[0].pop())\n\n\tdef deselect(self, indices):\n\t\t\"\"\"\n\tRemove and return a subset of items from a list of lists or an integer\n\tset of indices. The parameter `indices` must be a tuple of indices\n\t\"\"\"\n\t\t# TODO: implement\n\t\t# index, minv, maxv, p, n, s, e, f, d, p, c"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\twhile self.tail!= self.sentinel:\n\t\t\trand = random.random()\n\t\t\tif rand <= self.sentinel:\n\t\t\t\tself.tail = rand\n\t\treturn self.sentinel\n\n\tdef __cmp__(self, other):\n\t\treturn self.sentinel == other.sentinel\n\n\tdef __str__(self):\n\t\tresult = []\n\t\tfor i in xrange(self.count):\n\t\t\tresult.append(\"%s = %s\" % (i, str(self.contents[i])))\n\t\treturn \"RandomPartition(%s)\" % (\" \".join(result))\n\n\tdef __lt"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\traise KeyError(\"No available key(s) in dictionary\")\n\n\n\ndef newkeyset():\n\t\"\"\"\n\tConstruct a new `PKeySet` instance.\n\t@return: A new `PKeySet` instance\n\t\"\"\"\n\treturn PKeySet(random.sample(KEYSET_PAIR_LIST_SIZE))\n\n\n\ndef _sortkey(s):\n\t\"\"\"Helper for the sorting of an entry by sorting by value or by key.\"\"\"\n\tkey_len = len(s)\n\tif key"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\twhile not self.head:\n\t\t\tself.head = choice(self.heads)\n\t\tidx = choice(self.idx_map.choices)\n\t\tself.idx_map.remove(idx)\n\t\tkey, val = self.keygen.getrandbits(numbits, self.getn_bits)\n\t\twhile val in self.heads:\n\t\t\tself.idx_map.remove(val)\n\t\t\tkey += key[:self.getn_bits]\n\t\t\tval = self.valgen.getrandbits(numbits, self.getn_bits"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\treturn randint(0, self._random_count()-1)\n\n\tdef popdict(self):\t\"\"\"Return a random dictionary from every key/value pair.\n\n\tUsage::\n\n\t\t>>> popdict = DictType()\n\t\t>>> popdict['foo'] = 1\n\t\t>>> popdict['foo']\n\t\t1\n\t\t>>> popdict.pop('bar')\n\t\t2\n\t\"\"\"\n\n\tdef random(self, key=None, value=None):\t\"\"\"\n\tReturn a random items from the dictionary.\n\n\tUsage::\n\n\t\t>>> pop = randomedict('foo', 1"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tpopitem = self.heap[self._index]\n\t\theap[self._index] = popitem\n\t\treturn popitem\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tReturn a new `key = random`.\n\t\t\"\"\"\n\t\treturn random.choice(self.elements)\n\n\tdef elements(self):\n\t\t\"\"\"\n\t\tReturn a list of all currently defined elements.  \n\t\t\"\"\"\n\t\treturn filter(lambda item : item[2] == self, self)\n\n\tdef _max_index(self):\n\t\t\"\"\"\n\t\tReturn an index into the max heap dictionary."}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tdef _build_regex(regexes: List[Tuple[Regexp,...]]) -> Optional[Pattern]:\n\t\t\tregex_list    = []\n\t\t\tregex_list += []\n\t\t\tfor regex in regexes:\n\t\t\t\tregex_list.extend(pattern_to_regex_list(regex))\n\t\t\treturn Regex(_regex_list, regexes, self)\n\n\t\treturn Pattern(_create_regex_from_list(self.get_args(), lambda arg: True, None))\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn the string form of this template syntax.\n\t\t\"\"\""}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tout = re.escape(self.escape)\n\t\tif len(out) > 0 and out[0] == '[' and out[-1] == ']':\t# string is already delimited\n\t\t\tout = '[^\\w\\s]\\s*' + out[1:-1]\n\t\treturn out\n\nclass InTypeMatcher:\n\t\"\"\"\n\tUse this to match a string to an extension type.\n\n\t>>> from portage import _create_in_type_matcher\n\t>>> _create_in_type_matcher.check('str')\n\t>>> _create_in_type_"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(six.u(\"\"))\n\n\tdef _create_prefix_to_regex(self, regex_prefix: Pattern, is_prefix: bool) -> Pattern:\t\"\"\"\n\tCreate the prefix-to-regex parameter regular expression.\n\t\n\tCreates a prefix-to-regex prefix regex expression (:class:`re.compile`).\t\"\"\"\n\t\tregex = six.u(\"^.+$\")\n\t\tregex_prefix = re.compile(regex_prefix)\n\t\treturn regex(six.u(regex_prefix))\n\n\tdef _create_suffix_to_regex(self, regex_"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self.make_pattern() + \"/\" + super(InStyleName, self)._create_in_regex()\n\n\t@staticmethod\n\tdef create_regex(pattern: str) -> Callable[[re.Pattern], str]:\n\t\t\"\"\"\n\t\tCreate a regular expression without changing the regex.\n\t\t\tParameters\n\n\t\t:param pattern: The regular expression to be used for setting the regex.\n\t\t:type pattern: str\n\n\t\tReturns: A :class:`re.Pattern` with a pattern object.\n\t\t\"\"\"\n\t\treturn RePattern(pattern)\n\n\t@staticmethod\n\tdef from_function"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn Pattern(PatternType.IN)\n\n\t\"\"\"\n\t:param pattern: an instance of :class:`re.Pattern` for this parameter\n\t:return: the regular expressions used\n\t:rtype: :class:`re.PatternTuple` or :class:`re.Pattern`\n\t\"\"\"\n\tpattern = None\n\tif pattern_text:\t\tpattern = self.re_sub(pattern_text)\n\treturn pattern\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\traise NotImplementedError()\n\ndef _create_out_regex(self) -> Pattern:\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n\tReturns the out-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\traise NotImplementedError()\n\ndef _create_match_regex(self) -> Pattern:\t\"\"\"\n\tCreate the pattern matching regular expression.\n\n\tReturns the pattern matching regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\traise NotImplementedError()\n\ndef _create_search_regex(self, search: search_factory) -> Pattern:\t\"\"\"\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tregex = Pattern()\n\t\tregex.pattern = self._pattern\n\t\treturn regex\n\n\tdef _create_out_regex(self) -> Pattern:\t\"\"\"\n\tCreate the out-style parameter regular expression (:class:`re.Pattern`):\n\n.. code-block:: python\n\n\t\tdef _delete_regex(self) -> Pattern:\n\t\t\treturn _create_in_regex()\n\n\t\"\"\"\n\treturn Pattern(r'(?<!\\\\)\\\\[A-Za-z]+')\n\n\t@classmethod\n\tdef parse(cls, pattern: Union[str, Pattern]) -> Pattern:\n\t\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn Pattern(self.pattern, self.pattern, self)\n\n\nclass Pattern(re.MatchObject, PatternMixin):\n\t\"\"\"\n\tBase class for :class:`re.Pattern` and :class:`re.MatchObject`.\n\t\"\"\"\n\n\tdef __init__(self, pattern: str, default: Pattern = None) -> None:\n\t\t\"\"\"\n\t\tInitialize this object.\n\n\t\t.. note::\n\t\t\tFor a regular expression pattern, one must not contain any\n\t\tcharacter except underscores.\n\t\t\"\"\"\n\t\tif not pattern:\n\t\t\traise ValueError(\"Empty expression\")\n\n\t\tself"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tfrom re import compile\n\t\treturn compile(self._regex_text, re.U)\n\n\t@staticmethod\n\tdef _regex_text(text: str) -> str:\n\t\t\"\"\"\n\t\tReturns a regular expression used by a text command, with the compiled\n\t\tpattern as arguments.\n\n\t\t:param str text: A string of `text` to use\n\n\t\t:returns :class:`re.Pattern`\n\t\t\"\"\"\n\t\treturn \"^(?P<m>[a-zA-Z0-9_ ]+)(-(?P<s>[+-.]?\\d+)(\\-\\d\\d?"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tin_style = self._regex[-1]\n\t\tif in_style.regex == '':\n\t\t\traise IkaUtilsError(u'Invalid regular expression: {0}'.format(in_style.regex))\n\t\treturn re.compile(in_style.regex)\n\n\tdef _search_in_string(self) -> Pattern:\t\"\"\"\n\tSearch every in-style parameter regular expression in a string.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tin_style = self._regex[-1]\n\t\tfor s in self._"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\n    \tconverter = _converting._Converter(self)\n    \treturn converter\n    \t\n    \n\n# -*- coding: utf-8 -*-\n# Copyright (c) 2015, Frappe Technologies and contributors\n# For license information, please see license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nimport unittest, erpnext\n\nclass TestContact(unittest.TestCase):\n\tdef test_get_contact_list_for_contact_group(self):\n\t\t# when contact group is created, get contact list\n\t\tso = self.create_contact_and_user()\n\n\t\t#"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._Converter\n\n\t@staticmethod\n\tdef map(parameter: _Converter) -> _Converting:\n\t\t\"\"\"\n\t\tMap a parameter style converter.\n\t\tUse this function to map the parameter style to the\n\t\tstyle `parameter_style`.\n\t\t@param parameter: the parameter that you want to map\n\t\tto pythographs.parameter_style.\n\t\t\"\"\"\n\t\tconverter = _convert_parameter_style(parameter)\n\t\treturn _trimming._Converting.map(converter)\n\n\t@staticmethod\n\tdef combine(parameter: _Converter, transformation"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\t# Note: we use converters for all parameters because the\n\t\t# parameter is a single, contiguous, variable-length vector and\n\t\t# requires a different number of components. Note that we keep track of\n\t\t# the number of components here, because the computation of type\n\t\t# parameters directly depends on the number of components, thus\n\t\t# the type parameter is simply the value.\n\n\t\t# A Parameter-style converter is:\n\t\t# int:\t\t\tThe number of values that this parameter must have.\n\t\t# float:\t\t\tThe number of values that this parameter can have.\n\t\t#"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting.converter\n\tdef _create_type(self) -> _type.Type:\n\t\t\"\"\"\n\t\tCreate the parameter type conversion.\n\t\t\n\t\t:returns: :class:`Type`: the parameter type conversion.\n\t\t\"\"\"\n\t\treturn _type.Type\n\t@classmethod\n\tdef _create_parameter(cls, child_module, node):\n\t\tfor child in node.children:\n\t\t\tif not isinstance(child, _parameter.Parameter):\n\t\t\t\traise ValueError('Invalid parameter node.')\n\t\tif len(node.children) > 0:\n\t\t\tif isinstance(node"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tresult = Converting._Converter()\n\t\tfor param_name, param_value in self.parameters.items():\n\t\t\tif result._param_type == param_type or result._param_type == string_type:\n\t\t\t\tresult._convert_type = param_value\n\t\t\t\tresult.add_param(param_name, param_value)\n\t\tlogger.debug(\"Parameter style converter returning: %s\"% (result._converter,))\n\t\treturn result\n\n\tdef __repr__(self):\n\t\treturn \"%s(%s)\"%(self.__class__.__name__,repr(self._converter))\n\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._Converter(self)\n\n\tdef get_parameter_style_converter(self, parameter_name:str) -> _parameterstyle._ParameterStyleConverter:\n\tr\"\"\"\n\tGiven a parameter style name, see :func:`._converter`.\n\t\"\"\"\n\treturn _parameterstyle._ParameterStyleConverter(self, parameter_name)\n\ndef find_converter(path_to_converter:PathConverter) -> PathConverter:\n\t\"\"\"\n\tFind a converter object that can be used for converting a path to a parameter style converter.\n\n\tArguments:\n\t    path_to"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t return self._conversion  # type: ignore\n\n\tdef _create_pyfunc(self) -> _converting._FunctionType:\t\"\"\"\n\tCreate the parameter-related python function.\n\nReturns the parameter related python function (:class:`._converting._FunctionType`).\t\"\"\"\n\t return self._pyfunc       # type: ignore\n\n\tdef create_pyfunc(self) -> _converting._FunctionWrapper:\t\"\"\"\n\tCreate the parameter-related python function (no decorators).\n\nReturns the parameter related python function (:class:`._converting._FunctionWrapper`).\t\"\"\"\n\t return self._pyfunc"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn self._converting.converter\n\n\t@property\n\tdef conversion_options(self) -> str:\n\t\t\"\"\"\n\t\tParameters for the parameter style conversion.\n\n\t\t\"\"\"\n\t\treturn self._converting.conversion_options\n\n\t@property\n\tdef conversion_options_options(self) -> str:\n\t\t\"\"\"\n\t\tParameters for additional parameters the parameters used in the conversion options.\n\n\t\t\"\"\"\n\t\treturn {\n\t\t\t'output-delimiter':'',\n\t\t\t'remoother-delimiter': ',',\n\t\t\t'stoffset-in-first-column': '',\n\t\t\t'no-"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\n\t\treturn _converting._Converter(\n\t\t\tself, None,\n\t\t\t(\n\t\t\t\tParameterItem.UNDEFINED,\n\t\t\t\tParameterItem.INDEX,\n\t\t\t\tParameterItem.NAME,\n\t\t\t\tParameterItem.DESCRIPTION,\n\t\t\t\tParameterItem.DEFAULT,\n\t\t\t\tParameterItem.TYPE,\n\t\t\t\tParameterItem.VALUE,\n\t\t\t\tParameterItem.NO_DEFAULT,\n\t\t\t\tParameterItem.RECURSIVE_LINE,\n\t\t\t\tParameterItem.VALUE_MODE,\n\t\t\t\tParameterItem.VALUE_TEMPLATE,\n\t\t\t)\n\t\t)from datetime import date\nimport logging\nimport pickle\nimport requests\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\t# TODO: consider adding the converter options for this.\n\t\treturn _Converting(self)\n\n\t@property\n\tdef type(self) -> str:\n\t\t\"\"\"The parameter type.\"\"\"\n\t\treturn self._type\n\n\t@type.setter\n\tdef type(self, value: str) -> None:\n\t\tself._type = value\n\n\t@property\n\tdef number(self) -> str:\n\t\t\"\"\"The parameter number.\"\"\"\n\t\treturn self._number\n\n\t@number.setter\n\tdef number(self, value: str) -> None:\n\t\tself._number ="}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 22.16 GiB total capacity; 20.36 GiB already allocated; 11.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\t# TODO: We could use something like datetime.strptime() here, but\n        # right now the classes for these cannot use this interface.\n        raise TypeError\n\n    def _parse(self, timestr, default=None):\n        \"\"\"\n        Given a string, parse it and return a :class:`datetime.datetime`\n        object.\n\n        :param timestr:\n        Any date/time string using the supported formats.\n\n        :param default:\n        The default datetime object, if this is a datetime object and not\n        ``None``, elements specified in ``timestr`` replace elements in the\n        default object.\n\n        :return:\n       "}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n        # This implementation is a bit hackish and uses the current\n        # time plus the number of significant digits.\n        calendar.timegm((2000, 1, 1, 0, 0, 0, 0, -1, -1))\n        fmt = \"%Y-%m-%d %H:%M:%S,%f\"\n        format = fmt % (calendar.timegm(tuple(imap(int, fmt[:-1])),\n                                    None, -1, -1, 0, -1))\n        try:\n            result = parse(timestr, default, ignoretz=ignoretz,\n                             tzinfos=tzinfos,"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\t# See #158 for the error codes that can be raised when converting\n\t\t# a datetime object to a datetime object.\n\t\t# The max integer size used is 2048.\n\t# I tested on that.\n\n\ndef parsedate(timestr, default=None):\n\t\"\"\"\n\tParse a date/time string into a :class:`datetime.datetime` object.\n\n\tIf ``fuzzy_with_tokens=True`` is passed, then :func:`parse`\n\tis invoked to transform ``timestr`` into a :class:`datetime.datetime`\n\tobject, using the parsed results"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t \n    # Make sure we don't have bad tzinfo support.\n    if isinstance(tzinfos, _tzinfo_class):\n        raise TypeError(\"tzinfo already knows how to parse non-datetime \"\n                        \"objects without a tzinfo.\")\n\n    res = self._resultf('year', re.compile(r'^([-+]?[0-9]*)[dD]$',\n                                  re.IGNORECASE), re.IGNORECASE)\n    m = res.match(timestr)\n    if m is None:\n        raise ValueError(\"invalid date (pattern '%s', timez sign,..., \"\n                         \"only 'January'), or a future date without time"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\t# TODO - Warning\n\t# >>> parse(\"1999-12-31T23:01:23\", default=None, ignoretz=True)\n\t# datetime.datetime(1999, 12, 31, 23, 0, tzinfo=tzutc())\n\n\t# we won't miss a strict datetime.datetime() parameter so we just\n\t# raise a ValueError since datetime.datetime(1999, 12, 31, 23, 0, tzinfo=tzstr())\n\t# will succeed\n\traise ValueError(\"Date given was not a valid datetime.datetime()\")\n\n# parse_time_str(timestr"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    if fuzzy_with_tokens:\n        fuzzy_pattern = fuzzy_token_format + r\"\\d+-\\d+ \\d+:\\d+:\\d+%\"\n    else:\n        fuzzy_pattern = r\"^\\d+(\\.\\d+)?\\s?(?:%s|%s|%s|$)\" % (\n            ISO8601_PATTERN,\n            FuzzyVersion.ISO8601_MIN,\n            FuzzyVersion.ISO8601_MAX,\n            DateTimePattern\n        )\n    time_components = ((\"SECOND\", timeoffset(None, timestr, -1)),\n                        (\"MINUTE\", timedelta(seconds=1)),"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n        # Note: we coerce this behavior to be a regular regular\n        # expression if the parsed object contains a match for the pattern\n        # parse/normalize operations; this means that if the provided\n        # string contains numeric digits, the parsing result will be wrong.\n        # A future revision of the library will change this behavior to\n        # be resolved by :func:`parse()`.\n        if isinstance(timestr, compat.string_types):\n            timestr = re.compile(timestr)\n        m = _BYTESPAT.match(timestr)\n        if m is None:\n            raise ValueError(\"incorrect date/time %r"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\t# Try parsing \"today\", \"today\" and timezones against the\n\t\t# IANA timezone. In this case, it is necessary to ensure that\n\t\t# \"today\" has a sensible offset from UTC. If that's not the\n\t\t# case, UTC is assumed.\n\t\tif \"today\" not in kwargs:\n\t\t\ttry:\n\t\t\t\tself._parsed_string = self._parse_date(timestr)\n\t\t\texcept ValueError:\n\t\t\t\t# Failing that, if parsing fails normalize the tuple.\n\t\t\t\tself._parsed_string = self._normalize(timestr)\n\t\telif"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    if fuzzy_with_tokens:\n        raise NotImplementedError(\"The fuzzy parsers don't support tokens longer than %s characters\" %(max_chars,))\n\n    if len(timestr)!= 4:\n        raise ParserError(\"invalid date format '%s'\" % timestr)\n    value = time.strptime(timestr, input_format)\n\n    try:\n        # If input was a string, it can be a datetime.datetime object.\n        assert parser is None\n        value = _dt_from_rfc3339(value)\n    except:\n        # If an invalid date is passed, try parsing with the default\n        # parser."}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\tnow = datetime.now(tz=tzstr(next(self._parsed).tzname()))\n\t\tif now > self.MAX_AGE:\n\t\t\traise OverflowError(\"parsed date too large (%d)\" % now)\n\n\tif default is None:\n\t\tdefault = datetime.now()\n\n\ttry:\n\t\tif hasattr(default, 'tzinfo') and default.tzinfo is not None:\n\t\t\tdefault = default.replace(tzinfo = tzinfos.pop(0))\n\t\treturn parse(timestr, default, **kwargs)\n\texcept (TypeError, ValueError):\n\t\tpass\n\texcept OverflowError"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n        return datetime.datetime(dt.year, dt.month, dt.day, dt.hour,\n                                 dt.minute, dt.second, dt.microsecond)\n\n    def toutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime, calculates a datetime\n\tin a new timezone.\n\t\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\t# naive datetime.datetime.  The reason for this is we have one:\n\t\t# - It will compare the \"naive\" datetime.datetime (so its naive),\n\t\t# - It will add a \"fold\" datetime.datetime into the tuple\n\t\t#   (iso-style datetimes and dates converted to a datetime.date)\n\t\t# - It will then return the normal datetime.datetime (or, if this\n\t\t#   is \"fold\", this function will return the tuple):\n\t\t# - If this result is the exact result (i.e., its naive), it'll\n\t\t#"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\t# we don't have a meaningful time zone, but the\n\t\t# algorithm here is pretty unlikely so let's be explicit that\n\t\treturn self.duplicate(dt)\n\tassert(isinstance(dt, datetime.datetime))\n\n\tif isinstance(dt, _timezone.tzinfo):\n\t\tassert dt.dst() in self.known_timezones\n\t\tassert dt.tz_offset == self.known_timezones[dt.dst()]\n\t\treturn dt\n\telif isinstance(dt, basestring):\n\t\tm = _timezone.match(dt)\n\t\tassert(m is not None)"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t         \n\t# TODO: test!\n\t# if dt.tzinfo.dst(dt) == dt:\n\t#\treturn dt\n\t# return dt.replace(tzinfo=utc)\n\n\tif dt.tzinfo is None:\n\t\t# We can't do it for ISO compilations--ISO has no DST rules, and so the\n\t\t# following line doesn't work for any other timezone, yet.\n\t\t# So, let's only work in iso 8601-compliant time zones.\n\t\tself.check_date_in_dst_zone(\"EST5EDT\")\n\t"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\t# What we'll use for this computation?\n\tif dt.tzinfo is None:\n\t\treturn dt\n\t\n\t# We have an ambiguous datetime. Since, e.g., in Vespero where\n\t# we're trying to convert an ambiguous datetime to a time,\n\t# we treat a \"fold\", or an \"interim\" datetime if we're only looking\n\t# at the beginning of the ambiguous datetime.  In either case,\n\t# then the equivalent is simply \"fold\", but for now, we consider the\n\t# same interval we have for this comparison anyway.\n"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    if dt.tzinfo:\n        return dt\n    elif datetime.timezone.utc:\n        return datetime.timezone.utc\n    else:\n        raise ValueError(\"Unrecognized timezone.\")\n\n\ndef _now_timestamp():\n    \"\"\"Internal-ish, but not a proper timestamp.  This could be handy\n    if we want to use a datetime in the future but the next call\n    to ``fromutc(datetime.datetime.utcnow())``.\n    \"\"\"\n    return datetime.datetime.now()\n\n\ndef is_holiday(year, month, day):\n    \"\"\"Determines if the date is in ``weekday`` days of the given"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\treturn dt.replace(tzinfo=None, microsecond=0).astimezone(self)\n\ufefffrom django.contrib.gis import admin\nfrom django.db import models\nfrom django.db.models import Lookups\nfrom django.db import transaction\nfrom django.conf import settings\nfrom django.core.exceptions import ValidationError, QNameError\nfrom django.core.urlresolvers import reverse\nfrom django.utils.translation import ugettext_lazy as _, ugettext\nfrom django.utils.translation import pgettext_lazy\n\n\nclass Location(models.Model):\n    name = models.CharField(_('name'),"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    if self.tzinfo and dt.tzinfo:\n\t\treturn dt\n\t\n\tfor offset, delta, _ in self._delta_to_seconds(dt):\n\t\t\n\t\tif offset < 0:\n\t\t\treturn datetime.strptime(delta, self.DATETIME_FSL)\n\t\telse:\n\t\t\treturn datetime.fromutc(delta)\n\n\treturn None\n\n\ndef _tz_info_to_string(tz, default = _tz.tzname()):\n\t\"\"\"\n\tConvert one tz object to a standard time zone specification\n\t\"\"\"\n\tif tz is None:\n\t\treturn default\n\t\n\t# TODO"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\treturn dt\n\n\ndef fromtimestamp(self, t):\n\t\"\"\"\n\tConverts a :class:`datetime.datetime` object to a :class:`int`\n\ttime interval.\n\t\"\"\"\n\t\n\n\tif t:\n\t    self.normalize()\n\t    return t\n\t    #raise NotImplementedError\n\n\telse:\n\t    return dt\n\n\ndef fromordinal(self, id):\n\t\"\"\"\n\tReturns a :class:`datetime.datetime` instance in a\n\tordinal date representation. :param id: the ID of the occurrence.\n\t\"\"\"\n\n\treturn datetime.date( (int(id),6"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\tif (dt.tzinfo.zone is None or\n\t\t\tself._default_timezone!= dt.tzinfo.zone):\n\t    raise pytz.AmbiguousTimeError(\n\t\t\t\"Timezone {} does not match timezone {}\".format(\n\t\t\t\tself._default_timezone, dt.tzinfo.zone.tzname(dt)\n\t\t\t)\n\t\t)\n\n\treturn dt.astimezone(self._default_timezone)\n#!/usr/bin/env python\n'''Test for the pfilter module.\n\nTests the module for the pfilter module based on the python-pfilter\nmodule.\n'''\n\n# Copyright"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    assert isinstance(tzinfo, tzinfo), \"'tzinfo' must be a pytz\"\n    dt = dt.astimezone(tzinfo)\n    assert isinstance(dt, datetime.datetime)\n    return dt\n\ndef parse_tzstring(tzstr):\n    \"\"\"\n    Parse a string passed to ``parse`` in a timezone-aware format.\n\n    :param tzstr:\n    :return:\n    \"\"\"\n\n    if tzstr.count(\"\\\\n\")!= 2:\n        return None\n\n    tz_parsed = parse(tzstr)\n    if tz_parsed is None:\n        return tzstr\n    return tz_parsed\n\ndef parse_date("}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    tz = default_tzinfo(dt, tzinfo)\n    result = dt.astimezone(tz)\n    if hasattr(tz, 'normalize'):\n        result = tz.normalize(result)\n    return result\n\ndef make_aware(dt, tzinfo):\n    \"\"\"\n    Returns a ``datetime.datetime`` instance with a *tzinfo* component\n    appropriate for use with it.  If tzinfo is ``None``, it defaults to\n    ``pytz.UTC``.\n\n    For ``tzinfo`` instances in the above supported formats (either by\n    ``pytz.timezone.LOCAL``, or ``pytz.FixedOffset`` instances), localize the\n    result"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n_parser_cache = {}\n\ndef get_localzone():\t# TODO: Use the global tzutils.zone module here instead of using pytz.\n\t# The default is a naive :py:class:`datetime.tzinfo` instance.\n\n\t# TODO:  This assumes that a specific time zone is specified\n\t# in the configuration and localzone is a common one.\n\n\t# TODO:  Can this be made simpler?\n\t# TODO:  Or should we be using pytz. \n\t# TODO:  We should return the timezone name?\n\n\tglobal _parser_cache\n\t"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\nclass tzinfo(tzoffset):\n\t\"\"\"\n\tThis is just a :py:class:`tzinfo` with additional methods for\n\tarithmetic calculations on the offset\n\n    The datetime class is **strongly** fast and widely only.\n\tIt simply subclasses :py:class:`tzinfo` of :py:class:`SimpleDate`.\n\t\"\"\"\n\tdef __init__(self, dt, **kwargs):\n\t\tsuper().__init__(**kwargs)\n\t\tself.dt = dt\n\ndef datetime(*params, **kwargs):\n\t\"\"\"\n\tA shortcut for ``datetime.datetime.now`` that"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\treturn dt.replace(tzinfo)\n\t\nclass Timezone(tzinfo):\n\tdef __init__(self, *args, **kwargs):\n\t\t# pytz.UTC: offset -> object\n\t\tsuper(TimeZone, self).__init__(*args, **kwargs.pop('tzinfo', None))\n\t\t\n\t\t# If timezone 'utc' keyword argument is used for 'aware' datetime\n\t\t# instances, we need to handle these instances ourselves:\n\t\tif hasattr(self, 'tzinfo'):\n\t\t\t# pytz.timezone has no __getattr__ or hasattr (at this point)\n\t\t\tself.__getattr__ = self."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\nimport datetime\n\nclass parsedatetime(object):\n\t\"\"\"\n\tParses the `parsedatetime` function which returns a :py:class:`datetime.datetime`\n\tobject, given a string of the form ``yyyymmdd`` with the string of the form,\n\tas returned by :py:func:`parsedatetime`.\n\t\"\"\"\n\tdef __init__(self, raw):\n\t\ts = raw.strip()\n\t\tif s[-1] == 'Z':\n\t\t\ts = s[:-1]\n\t\tm = _nondecimal_regex.match(s)\n\t\tif"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    def __new__(self, dt=None, **kwargs):\n        kwargs.pop('tzinfo', None) # force is to reset to 'utc' to avoid confusion\n        kwargs.setdefault('fold', False)\n        self.tzinfo = tzoffset(dt or default_tzinfo(), **kwargs)\n        self.fold = kwargs.pop('fold', True)\n        return self\n        \n    def utcoffset(self, dt):\n        return self.tzinfo.utcoffset(dt or default_tzinfo())\n    \n    def dst(self, dt):\n        return self.tzinfo.dst(dt or default_tzinfo())\n    \n    def tzname(self,"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\nno_tzinfo = timezone.get_default_timezone()\n\ndef new_tzname(dt, name, offset=0):\n    \"\"\"\n    Changes the time zone of the specified datetime, timezone by changing the\n    time zone offset\n\n    :param dt: The `datetime.datetime` instance to assign a local time. See\n    :py:func:`new_datetime` for details.\n\n    :param str name: The name of the new timezone.\n\n    Example:\n\n    >>> new_tzname(dj.localize(datetime(2013, 1, 1), 1))\n    'DATE-TIME'\n    >>> new_tzname("}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n\n\ndef validate_tzinfo(tzinfo_arg, tzinfo=None):\n    \"\"\"\n\tValidates a tzinfo in a parse datetime string, and that it does \n\tnot require a global timezone (not mandatory).\n\t\"\"\"\n    if tzinfo:\n        if isinstance(tzinfo, datetime.tzinfo):\n            tzinfo = tzinfo.zone\n        else:\n            tzinfo = default_tzinfo(tzinfo)\n    if tzinfo is None:\n        return None\n\n    parsed = parse(dt)\n    if parsed.tzinfo is None:\n        raise Exception((u'Invalid tzinfo: %s'\n                       % parsed.tzinfo))\n    return parsed\n#!/usr"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n        if isinstance(tzinfo, tzoffset):\n            tz = tzinfo.utcoffset(dt)\n        else:\n            tz = dt.replace(tzinfo=tzinfo)\n            if tz.tzinfo and hasattr(tz.tzinfo, 'normalize'):\n                tz = tz.normalize(tz.tzinfo.utcoffset(dt).strftime('%Z'))\n        return tz\n        \n    if isinstance(dt, timedelta):\n        return dt + default_tzinfo(dt)\n    elif isinstance(dt, datetime):\n        return dt + default_tzinfo(dt.date())\n    elif isinstance(dt, date):\n        return dt\n    elif isinstance(dt, time):\n        return"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tself.filehandle.seek(self.bsize())\n\t\tself.filehandle.truncate()\n\t\tself.bsize = 0\n\n\t\tif before:\n\t\t\tself.filehandle.write(before)\n\t\tif after:\n\t\t\tself.filehandle.write(after)\n\n\tdef __repr__(self) -> str:\n\t\treturn f\"<FileOpener:{self.filehandle}|{self.size}>\"\n\ndef read_header(filehandle: filelike) -> bytes:\n\t\"\"\"\n\tRead and return the header (i.e. the beginning of the file"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    def load_text(self) -> str:\n        import unicodedata\n        \n        if self.is_ascii:\n            return unicodedata.lookup('ASCII', self.data, self.data[:self.data.find(b'.')])\n        else:\n            return unicodedata.lookup(self.data.decode('ascii'), self.data)\n    \n    def load_text_string(self) -> str:\n        if self.is_ascii:\n            return unicodedata.lookup(self.data, self.data * self.data.count(' '))\n        else:\n            return unicodedata.lookup(self.data.encode('ascii'))\n    \n   "}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\t# If previous chunk was empty, skip this chunk.\n\t\tif after:\n\t\t\tif not self.has_key('delimiter'):\n\t\t\t\traise ValueError(\"delimiter must be set first.\")\n\t\t\tself['delimiter'] = before[:self['delimiter']]\n\t\telse:\n\t\t\tself['delimiter'] = after[:self['delimiter']]\n\t\tend = 0\n\t\t# We may have a final newline of the next chunk\n\t\twhile (end < len(self['delimiter'] + self['chunk_separator'])):\n\t\t\tself['chunk_separator'] += self['delimiter']\n\t\t\tend += 1\n\t\t#"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tself._lines.add(before)\n\t\tself._lines.add(after)\n\ndef set_cut_characters_from_base(self: bytes, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slices from the original base\n\tof each block defined in #{after:0x20}. \t\n\t\"\"\"\n\tself._lines.add(b\"\\x20\\x20\\x20\")\n\tself._lines.add(before)\n\tself._lines.add(after)\n\ndef set_cut_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\traise NotImplementedError()\t\nfrom __future__ import print_function\nimport argparse\nimport json\nimport multiprocessing as mp\nimport threading\nimport re\nimport queue as mpr\nimport sys\nimport urllib.parse\nfrom collections import namedtuple\nfrom functools import wraps as _wraps\n\n#from config import CONFIG as cfg\n\ndef _get_data():\n    try:\n        data_file = open(\"data.json\", \"r\")\n        data = json.loads(data_file.read())\n        data_file.close()\n    except IOError:\n        data_file = open(\"data.json\","}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\t# set split_max and split_start at max index of delimiters\n\t\tmax_index = self.splitfile_index(before)[1]\n\t\tcut_len = after.index('cut')\n\n\t\tif split_len <= max_index:\n\t\t\tbreak\n\n\tself.f.seek(2, 1)\n\tpos_lines = [before[0:cut_len], after[cut_len:]\n\n\tif split_len > 0:\n\t\tlen_before = before[cut_len:]\n\t\tpos_lines = [after[cut_len:], cut_len"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\t# TODO: this is still an API that can be much faster.\n\t\t_io.set_fill_char(self.fd, 0)\n\t\tself.clear()\n\t\tfor s in split_lines(before):\n\t\t\tself.fd.seek(s)\n\t\tself.fd.write(bytes(before))\n\t\tself.fd.seek(0)\n\n\tdef add_cut_chars(self, new_text: bytes, before: bytes) -> None:\t\"\"\"\n\tStart a new cut at |new_text| in chunks.\n\n\tArgs:\n    new"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tself._end_of_file = 0\n\tself._clear()\n\tself._clear_buffer()\n\tself._file = []\n\tself._data = []\n\tself._index = -1\n\tself._cut_off = 0\n\n\tself._cut = len(before)\n\n\twhile not self._should_append(self._cut_off, len(after)):\n\t\tself._file.append(before[self._cut_off:len(before)])\n\t\tself._data.append(after[self._cut_off:len(after)])\n\t\tself"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\t# Set the file pointer before any padding (in bytes).\n\t\tb_prev_end = self.file_pointer\n\t\tb_next = self.prev_file_pointer + before\n\n\t\twhile b_prev_end!= before:\n\t\t\tself.file_pointer += self.read_next_byte()\n\t\t\tb_prev_end = self.prev_file_pointer + before\n\t\tself.file_pointer = b_prev_end\n\t\tself.prev_file_pointer = b_next\n\n\tdef read_next_byte(self) -> int:\n\t\t#"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    try:\n        if sys.version_info < (3,):\n            bytes = bytes.decode(sys.getfilesystemencoding())\n        else:\n            bytes = str(bytes)\n\n    except AttributeError:\n        bytes = str(bytes)\n        \n    newfile = open(file, \"w\")\n    newfile.writelines(before)\n\n    if sys.version_info < (3,):\n        bytes = bytes.decode(sys.getfilesystemencoding())\n    else:\n        bytes = str(bytes)\n\n    newfile = open(file, \"w\")\n    newfile.writelines(after)\n# -*- coding: utf-8 -*-"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif len(request.data)!= 20:\n\t\t\tlogger.debug(\"request data ({})'.format(len(request.data)))\n\t\t\treturn False\n\n\t\tif request.body.startswith(\"/msg\"):\n\t\t\tlogger.debug(\"message ({}): {:,.2f}\".format(request.body, request.body.split(\":\", 1)[1]))\n\t\t\treturn True\n\t\telse:\n\t\t\tlogger.debug(\"could not identify a request {}\".format(request.body))\n\t\t\treturn False\n\treturn False\n\ndef handle_ping(server):\n\t\"\"\"\n\tPing a remote"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\t# TODO: Reconsider creating a function to test the API. Can there be a decent format?\n\n\t\t# Is there a request for this.\n\t\ttry:\n\t\t\t# Can try to detect a public reply.\n\t\t\tif type(request) is Message:\n\n\t\t\t\tif request.req_id in [1,1,1,1,1,1,1,1,1,0,1,1]:\n\t\t\t\t\t# It is a pub or spam.\n\t\t\t\t\treturn\n\t\t\t\tif request.req_id is 11 and request.has_key(\"to\"):\n\t\t\t\t\t# It"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tglobal DID\n\t\tkey = str(request.user.id)\n\t\td = Diaspora.objects.filter(key=key).first()\n\t\tif d is None:\n\t\t\treturn None\n\t\temail = d.message.data\n\t\tpmsg = Message.objects.filter(email=email).first()\n\t\tpublic = True\n\t\tmsgcount = pmsg.message_count\n\t\tif msgcount == 0:\n\t\t\tif DID!= \"000\":\n\t\t\t\tpublic = False\n\t\t\t\treturn DID\n\t\t\treturn request\n\t#if there can be no sender"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tfor payload in request.body:\n\t\t\tif not isinstance(payload, CardRequest): continue # ignore boards\n\t\t\tif payload.id == request.id: return payload\n\t\treturn None # ignore other types\n\n\t## Check to see if this is a Diaspora request. If so, return the original request.\n\tfor payload in request.body:\n\t\tif (payload.id in request.tags) or (not isinstance(payload, CardRequest)):\n\t\t\treturn payload\n\n\t## Check by matching message. If they are the same, it is ignored.\n\tfor payload"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\ttry:\n\t\t\tself.signature(request.headers[\"X-Auth-SigAlg\"])\n\t\t\treturn True\n\t\texcept (KeyError, ValueError):\n\t\t\treturn False\n\n\tdef identify_response(self, request: ResponseType):\t\"\"\"\n\tGiven a Request and the ResponseType, identify and return the response.\n\n\tIf this request is not a legacy or has been signed by a GCS device yet, we are now trying to find a response in that case.\n\tIf a signature was available, we will now find a response in the signature database\n\tand return the response.\n\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\treturn request.headers.get(BULP_ALIVEPION_HEADER)!= BULP_ALIVEPION_PAYLOAD\n\n\nclass Error(Exception):\t\n  \"\"\"Base class for all exceptions\"\"\"\n  status = 500\n\n  def __init__(self):\n    super(Error, self).__init__('Error %s', 'http_error')\n\n  def __str__(self):\n    return 'Error on %s %s: %s' % (self.status, self.message, self.details)\n\n  def __repr__(self):\n    return str(self)\n\n\nclass HTTPError(Error"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tmsg_type = request.message_type()\n\n\t\tif msg_type == MessageType.Command:\n\t\t\treturn True\n\t\telif msg_type == MessageType.Result:\n\t\t\treturn False\n\t\telif msg_type == MessageType.CommandWithResult:\n\t\t\treturn True\n\t\telif msg_type == MessageType.ResultWithResultWithDetails:\n\t\t\treturn True\n\t\telif msg_type == MessageType.ResultWithDetailsWithParameters:\n\t\t\treturn True\n\t\telif msg_type == MessageType.ReportMessage:\n\t\t\treturn True\n\t\telif msg_type == MessageType."}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\ttry:\n\t\t\tif request.get_header(settings.DIASP_SIGNATURE)[0_SUBJECT].signature == settings.DIASP_SIGNATURE or \\\n\t\t\t\trequest.get_header(settings.DIASP_SIGNATURE)[0_MESSAGE].signature == settings.DIASP_SIGNATURE:\n\t\t\t\tself.signature = settings.DIASP_SIGNATURE_REVERSE\n\t\t\telse:\n\t\t\t\tself.signature = request.get_header(settings.DIASP_SIGNATURE)[0_SHORT].signature\n\t\t\treturn True\n\t\texcept DiasporaException as e:\n\t\t\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tfor msg in request.get_messages():\n\t\t\tif msg.type == \"PUBLIC\":\n\t\t\t\treturn'message'\n\t\tmsg = { \"action\": \"Identify/request\" } # This is a message for the user who is the owner of the message\n\ndef get_user_name(user: str) -> str:\n\treturn user.first_name\n\ndef get_message_data(request: RequestType, message_id: int) -> str:\n\treturn json.dumps({\n\t\t'id' : message_id,\n\t\t'source' : request."}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif (message.type == MessageRequestACK or\n\t\t\tmessage.type == MessageRequestNAK):\n\t\t\treturn is_legacy_payload(request)\n\t\telse:\n\t\t\treturn False\n\treturn False\n\nclass RequestType:\n\t\"\"\"\n\tMessage types of data.\n\t\"\"\"\n\n\tdef __init__(self, id: int, type: int, payload: str) -> None:\n\t\t\"\"\"\n\t\tMessage type to be passed in (for processing)\n\n\t\tArguments:\n\t\t\tid:\t\t# Identifier of the message\n\t\t\ttype:\t\t# Type of the message"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request.form.get(constants.IDENTIFY_REQUEST)\n\traise ImproperlyConfigured(\n\t\t'\"Identify_Request\" view does not implement the '\n\t\t'Identify_Request view at this time')\nimport pygame\n\nclass State:\n    def __init__(self):\n        self.lastState = GameState.INIT\n        self.gameState = GameState.INITIAL_STATE\n        self.player = GameState.PLAYER\n        self.nextPlayer = GameState.PLAYER\n        self.gameType = GameState.WAIT\n        self.exitFlag = False\n        self.screenState ="}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn True\n\n\t# endregion\nimport json\nfrom collections import OrderedDict\n\nfrom django.http import HttpResponse\nfrom django.http import JsonResponse\nfrom django.views.generic import View, TemplateView\nfrom django.conf import settings\nfrom datetime import datetime, date\nfrom django.core.urlresolvers import reverse, reverse_lazy\nfrom django.utils import timezone\nfrom django.utils.translation import ugettext as _\n\nfrom braces.views import LoginRequiredMixin, StaffuserRequiredMixin\nfrom django_filters import rest_framework as filters\nfrom rest_framework.decorators import api_view, authentication"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif \"type\" in request.keys():\n\t\t\tresponse: ResponseType = request[\"type\"]\n\t\t\tif not request.get(\"name\", \"\") == u\"matrix\":\n\t\t\t\traise RuntimeError(\"\")\n\n\tdef parse_query(request: RequestType) -> Optional[SearchTerm]:\n\t\t\"\"\"\n\t\tTries to parse a query using the provided SearchTerm. Returns None if the query fails,\n\t\totherwise tries each individual entity in the model.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tquery: str = request[\"query\"]\n\n\t\t\tif not query.endswith(\" \"):\n\t\n\t\t\t\ttry:\n\t\t\t\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request.is_fully_identified() and request.get_type_names() == (\n\t\tShape.TYPE_COMMAND_NAME,\n\t)\n\n\nclass MatrixController(AbstractGameController):\n\t\"\"\"This controller is used to set things up that is a MatrixController for GameServer.\"\"\"\n\n\tdef __init__(self) -> None:\n\t\tself._mat_controller: MatrixController = request.get_mat_controller()\n\t\tself._handle_command: Optional[Handle] = request.handle_command\n\t\tself._is_running: bool = request.is_running"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn (request.is_request_send and request.get.is_matrix)\n\n\t@classmethod\n\tasync def check_exists_matrix_request(cls, request: RequestType) -> bool:\n\t\t\"\"\"\n\t\tCheck if this request can be cast to the correct matrix request (only for testing purposes)\n\t\t:param request: the request you are trying to identify a Matrix request\n\t\t\t@return:\n\t\t\t`True` if the request accepts a matrix\n\t\t\t`False` if it throws an exception which will handle this message\n\t\t\"\"\"\n\t\tif isinstance(request, MatrixRequest):"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif request is None:\n\t\t\treturn False\n\t\tif isinstance(request, MatrixRequest):\n\t\t\treturn not request.is_operator()\n\t\tif request.is_matrix:\n\t\t\treturn True\n\t\treturn False\n\n\tdef get_user_profile(self, request: RequestType, *args: Any) -> UserProfile:\n\t\tpass\n\n\tdef get_server_settings(self, request: RequestType, *args: Any) -> dict[str, str]:\n\t\tpass\n\n\nclass DefaultStream:\n\tdef __init__(\n\t\trealm,\n\t\tresponse: StreamingHttpResponse,"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treq_type = type(request)\n\t\tfor s in RequestType:\n\t\t\tif s in request.__slots__:\n\t\t\t\treq_type = req_type.slots[s]\n\t\t\telse:\n\t\t\t\traise AssertionError(f\"Unknown req-type: {request.type}\")\n\t\t\t\tbreak\n\t\treturn req_type in {'MatrixRequest', 'VectorRequest', 'ImageRequest'}\n\n\tdef get_vector(self) -> Vector:\n\t\t\"\"\"\n\t\tRetrieves the vector from the request's source, as well as a (1,0) entry\n\t\t\"\"\"\n\t\tfor s in Request"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request.is_matserver_request()\n\n\nclass MatrixServerResponse(object):\n\t\"\"\"\n\tRepresents a response to the Matrix server\n\t\"\"\"\n\n    def __init__(self, msg: MatrixMessage) -> None:\n        self.status = 0\n        self.matrix_server = MatrixServer.instance_from_request(msg)\n\n    def get_status(self) -> Tuple[int, str]:\n        \"\"\"\n        Get the status from the server\n        \"\"\"\n        server_status = self.matrix_server.is_up_to_date_status()\n        server_status_str ="}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tis_test_request = True\n\t\tfor test_req in request.tests:\n\t\t\tmatches = [m.matches() for m in test_req.matches if not is_test_request]\n\t\t\tif len(matches) == 0:\t\t\t\t\t# if test request is empty\n\t\t\t\tis_test_request = False\n\t\treturn is_test_request\n\n\tdef _is_match_in_request_test(self, request: RequestType, test_request: Request.TestCase) -> bool:\n\t\tfor test_req in request.tests:\n\t\t\tmatches = [m"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\ttry :\n\t\t\trow_index = request.index('*')\n\t\t\tcol_index = request.index(')')\n\n\t\t\treturn row_index is int(row_index) and col_index is int(col_index)\n\t\texcept ValueError:\n\t\t\tif not hasattr(request, 'is_matrix') or not hasattr(request, 'is_text'):\n\t\t\t\treturn False\n\n\t\t\treturn 'Error: Matrix requests must be made of text, ascii and/or binary. Matrix must be of binary type.'\n\n\t@staticmethod\n\tdef get_response_type() -> ResponseType:\n\t\t"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn str(dt.strftime('%Y-%m-%dT%H:%M'))\n\n\tt = str(dt)\t\n\tutc_datetime = dt.astimezone(local)\n\tret = utc_datetime.strftime(\"%s\")\n\treturn ret \n\n\ndef format_dt_tz(dt):\n\t\"\"\"\n\tFormat an ISO8601 time string from the way that D* nodes expect.\t\"\"\"\n\tt = ''\n\tutc_datetime = dt.astimezone(local)\n\tret = utc_datetime.strftime(\"%a %b %d %H:%M\")\n\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt.resolution not in set([None, '0', '','','']):\n\t\t\traise Exception(\"Invalid resolution\")\n\t\treturn dt.strftime(\"%d %b %Y %H:%M:%S %Z\")\n\n\tdef format_time(timeobj):\n\t\t\"\"\"\n\t\tFormat time objects to be displayed in a date\n\t\t\"\"\"\n\t\tif isinstance(timeobj, datetime.time):\n\t\t\treturn timeobj.isoformat()\n\t\treturn str(timeobj)\n\n\tclass InvalidTime(Exception):\n\t\t\"\"\"\n\t\tError raised when the time is before 'now', now"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt.strftime(\"%d/%b/%Y %H:%M:%S\")\n\nclass HalfTime:\n\tdef __init__(self, time):\n\t\t\"\"\"\n\t\tConverts a datetime argument as in hhmmss.\n\n\t\ttime = HHMMSS or 24hr or 1h3m5s.\n\t\t\"\"\"\n\t\tself.time = int(time)\n\n\tdef adjust(self, seconds):\n\t\t\"\"\"\n\t\tTakes a duration in minutes and returns the new time\n\n\t\tseconds: float\n\t\t\"\"\"\n\t\ttry:\n\t\t\tnew_seconds = int(seconds)\n\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt:\n\t\t\treturn dt.strftime(\"%x %H:%M:%S\")\n\t\telse:\n\t\t\treturn \"\"\n\n\tdef write(self, fn):\n\t\t\"\"\"\n\t\tWrite the generated file by the writer. This is mainly for the\n\t\tdata-validation in other files. \n\t\t\"\"\"\n\t\tf = open(fn, 'w')\n\n\t\tf.write('/* Generated by %s%s */' % (self.versionString, self.datestring))\n\t\tf.write(' */\\n')\n\t\tf.write('#include <sos_build/sos"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tn = dt.strftime(\"%H:%M\")\n\t\tfor k in (\"y\", \"m\", \"d\"):\n\t\t\tif dt.find(k)>=0: n = n[:(len(n)-1)] + str(n[k])\n\t\treturn n\n\tdef format_dt_with_d(dt):\tn = dt.strftime(\"%H:%M:%S\")\n\t\treturn n.replace(\":d\",\"+\")\n\tfmt_to_d = {\n\t\t\t\"y\" : FORMAT_y_with_d,\n\t\t\t\"m\" : FORMAT"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt+relativedelta(days=+1, hour=0, minute=0, second=0, microsecond=0, tzinfo=pytz.UTC)\n\n# -----------------------------------------------------------------------------------\n# Node class\n# -----------------------------------------------------------------------------------\nclass Node():\n\tdef __init__(self):\n\t\t'''\n\t\tConstructor\n\t\t\n\t\t@param node_id: Node id is ID\n\t\t'''\n\t\t'Initializes Node with id '+str(node_id)+' and all required attributes'\n\t\tNode._checkNodeType(node_id)\n\t\tself.node_type = node_id\n\t\n\t\t'''\n\t\tSets"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt.tzinfo is None:\t\n\t\t\t# python datetime.tzinfo is not consistent with aware datetime/time objects\n\t\t\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\t\telse:\t\t\t\t\t\n\t\t\t# python datetime.tzinfo.dst():\n\t\t\treturn dt.replace(tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/London'))\t\n\t\t\n\t\treturn dt\n\t\n\tfrom_string = format_dt\n\ndef timestamp_str(time_stamp):\n\t\"\"\"\n\tConvert the time_stamp to unix epoch time string.  The string"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt.strftime('%Y-%m-%dT%H:%M:%S')\n\treturn \"UTC\"\n\ndef get_tz():\n\t\"\"\"\n\tGet the timezone of the local database to assume it's UTC.\n\n\t:returns: the timezone\n\t\"\"\"\n\treturn timezoneManager.getInternalTimezone().getText()\n\ndef get_local_datetime(obj):\n\tif obj is None:\n\t\treturn None\n\tif \".\" in obj:\n\t\tdt = obj.split(\".\")[0]\n\t\tdt = get_local_datetime(dt)\n\telse:\n\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt.microsecond <= 0:\n\t\t\treturn format_dt_decimal(dt)\n\t\t# Hour: d must be < 0\n\t\t# Use the number of digits of precision for the number of fractions\n\tfmt_decimal = \"%%.%df\" % (dt.microsecond // 10**6)\n\treturn fmt_decimal\n\ndef format_dt_decimal(dt):\n\t\"\"\"\n\tFormat a dt. This returns a string with the format dd. HH:mm:ss.mmm -\n\tH:mm:ss.mmm.ss.ss.ss."}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\t# TODO: use a list comprehension?\n\t\tif isinstance(dt, datetime):\n\t\t\treturn dt.strftime(\"%d-%m-%Y %H:%M:%S\")\n\t\telif isinstance(dt, datetime):\n\t\t\treturn dt.strftime(\"%d/%m/%Y, %H:%M:%S\")\n\t\telse:\n\t\t\traise AssertionError(f\"Unknown type {type(dt)}\")\n\n@contextmanager\ndef log_call(message=None):\n\t\"\"\"\n\tRun a python function.\n\tIf the given message was not None, it will be printed to the console\n"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttags = set()\n\t\tfor m in re.finditer(tagRE, text, re.IGNORECASE):\n\t\t\ttags.add(m.group())\n\n\t\treturn tags, replacer(tags, text)\n\telse:\n\t\treturn re.split(tagRE, text)\n\n\ndef find_references(text: str) -> Set[Tuple[str, Any]]:\n\t\"\"\"\n\tFind references in text\n\nTries to ignore references inside code blocks.\n\nOptionally, if passed a \"repl\", will also replace the tag word with the result\nof the repl function called"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttags = []\n\t\tm = re.search(RE_TAG_PATTERN, text)\n\t\tif m: tags.append(m.group(1))\n\t\tif m: tags.append(m.group(2))\n\t\tif not tags: return text, replacer()\n\t\telse: return tags, text\n\n\ndef replace_all(text: str, replacer: callable = None) -> Set:\t\t\"\"\"\n\tReplace all text in RE_TAG_PATTERN with regex group for the replce, which will be replaced each time we\n\tcheck whether regex group"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tstripped_text = self.strip_tags(text)\n\t\t# if the text contains no tags, use the text\n\t\tres = replacer(stripped_text)\n\t\tstripped_text = re.sub(TAG_PATTERN, \" \", res)\n\t\treplaced_text = re.sub(TAG_PATTERN, \" \", stripped_text)\n\t\treturn replaced_text, stripped_text\n\n\tdef strip_tags(self, text: str) -> List[str]:\t# TODO: implement strip_tags using regex.\n\t\tstripped_text = self.strip_all(text"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tre_tagged = lambda m: searchRE(m, replacer)\n\n\t\ttext = _tagRE.sub(\"\\g<1>\\g<2>\", text)\n\n\t\t# Replace tags\n\t\ttags = re_tagged(text)\n\n\t\treturn set(), tags\n\t#end: find_tags\n\n#------------------------------------------------------------------------------\n#\n#------------------------------------------------------------------------------\n\n#-------------------\ndef main(argv) -> int:\n\t\"\"\"Parse arguments and run command.\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"gendiff\",\n\t\tdescription=\"\"\"A tool for testing/removing code blocks.\"\"\",\n\t\tepilog=\""}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tif text in re.findall(\"\\\\b\\\\w+{4}\", text, re.S|re.I):\n\t\t\ttags: Set[str] = Set()\n\t\t\treplacement: Optional[str] = ReplacementRule.NO_TAG\n\t\t\treplacer: Optional[Rewriter] = ReplacementRule.SOSHES\n\t\t\tfor match in re.finditer(Replacer, text, flags=re.MULTILINE):\n\t\t\t\ttag: str = match.group(tag_lower_first=True)\n\t\t\t\treplacement: Optional[str] = ReplacementRule.NO_TAG\n\t\t\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tif not text:\n\t\t\treturn\n\n\t\ttext = unicode_func(text)\n\n\t\tfor match in regex_tags:\n\t\t\tmatch = match.match(text)\n\t\t\tif match:\n\t\t\t\tyield (match.group(1) or \"\")\n\t\t# else match is None\n\n\t# Find tags on <a> tags.\n\tfor match in regex_tags:\n\t\tmatch = match.match(text)\n\t\tif match:\n\t\t\ttags = [tag for tag in _tag_re.findall(match.group(1)) if tag]\n\t\t\tif len(tags)"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttags = set(re.findall(r'<[\\w\\s]+>', text, flags=re.DOTALL))\n\t\tif tags:\n\t\t\ttag = list(tags)[0]\n\t\telse:\n\t\t\ttag = None\n\t\treturn set(replacer(tag))\n\treturn (tags, text)\n\t\ndef search(text: str, replacer: Callable = None) -> Set[str]:\n\t\"\"\"\n\tCheck for tags in text.\n\nTries to match all tags inside the text with the regular expression\nspecified for the regex.\n\nTries to replace"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tresult = set()\n\t\tfor m in RE_TAG.finditer(text):\n\t\t\tname = m.group(1)\n\t\t\tif name in result:\n\t\t\t\t# Replaced tag found.\n\t\t\t\tm2 = RE_TAG.sub(\"\", text[name]).split()\n\t\t\t\tif replacer is None:\n\t\t\t\t\tif len(m2) > 1:\n\t\t\t\t\t\traise NameError(f\"Can not find replacement tag {name!r} at line {line}\")\n\t\t\t\t\tresult.add(\".\".join(m2[0]))\n\t\t\t\telse:\n\t\t\t\t\tresult.add(name)"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tresult = set()\n\t\ttext_lines = text.split(\"\\n\") if text else []\n\t\tfor line_text in text_lines:\n\t\t\t# Try to find the tags in the line using regex.\n\t\t\t# The line of the match is from the last line with a '\\n' and a '\\r'.\n\t\t\t# The regex will be compiled with '\\d+\\.\\d+\\.\\d+.\\r+\\n' (single whitespace is removed, if it exists).\n\t\t\tmatched_tag = regex.search(line_text)\n\t\t\tif not matched_tag:\n\t\t\t\t# Then we search"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\treturn tags(replacer(text))\n\n# For debugging: print a \"replacer\".\ntag_regex = re.compile(\"^[!#$%&'*+/=?^_`{|}~-]+$\")\ndef tags(replacer: Callable) -> Set[str]:\n\treturn tag_regex.findall(str(replacer))\n\n# TODO: Make this be a mixin\ndef replace_with_tags(text: str, replacer: Callable = None) -> ReplaceWith:\n\treturn replace_tag(text, replacer, False)\n"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t if not text:\t# ignore empty text nodes\n\t\t\treturn text\n\t\tnew_text = ''\n\t\tfor c in text:\n\t\t\tif c == \"\\n\":\t# skip empty lines\n\t\t\t\tif new_text:\t# empty (new text node)\n\t\t\t\t\treturn False   # break text; nothing to do, already complete\n\t\t\t\tcontinue\n\n\t\t\tif c in (\"-\", \"_\"):\t# add attribute as new link\n\t\t\t\tattr_name = c[:-1]\n\t\t\t\tnew_text += c.lower()\n\t\t\t\tcontinue\n\n\t\t\t# handle URLs\n\t\t\tif c == '.':\t# ignore relative"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\t# The text links may contain newlines (including newlines inside\n\t\t# them), preserve linebreaks, and align consecutive whitespaces with a\n\t\t# single space. To achieve this we save and process it.\n\n\t\tdef handle_text(text):\n\t\t\tnew_text, new_link_text = process_text(text)\n\n\t\t\t# Check if new_text includes linkify and new_link_text for an HTML\n\t\t\t# tag or otherwise it is a plain Text node.\n\t\t\tif not new_text.strip():\n\t\t\t\t# We have a text node without text.\n\t\t\t\treturn\n\t\t"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tif text is None:\n\t\t\treturn text\n\t\tif not isinstance(text, basestring):\n\t\t\ttext = unicode(text, 'ISO-8859-1')\n\t\t_text = text.lower()\n\t\tif _text in no_link_tags:\n\t\t\treturn None\n\t\tlinks = []\n\n\t\t# text is either an email address or a link.\n\t\tif '@' in _text:\n\t\t\t_text = text.split('@')[0].strip()\n\t\telse:\n\t\t\t_text = _text.upper().strip()\n\t\tfor link in re.findall(r\"(\\"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttext = re_link.sub(lambda m: process_text(m.group(1), m.group(2)), text)\n\t\ttext = text.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\")\n\n\t\ttry:\n\t\t\treturn text + \"\\n\"\n\t\texcept TypeError:\n\t\t\tif isinstance(text, re_link): text = process_text(text)\n\t\t\tif sys.version==\"3\": text = regex_code.sub(lambda m: process_code(m.group(1), m.group(2)), text)\n\t\t\telse: text ="}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ndef main():\n\tfrom config import *\n\tparser = argparse.ArgumentParser(description='Generate and output wiki links from source code.')\n\tparser.add_argument('sourceCode')\n\tparser.add_argument('outputFile')\n\targs = parser.parse_args()\n\n\tif args.sourceCode is None:\n\t\tconfigException(\"Please provide a source code.\")\t\t\t\n\t\n\t# The first two lines replace tags to prevent duplicates.\n\treplace_line_replaces = []\n\t\t\n\twikiParser = wiki_parser.WikiParser()\t\n\twikiParser.parse(args."}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\n\t\n\ttext_url=text\n\n\t#getting all text links\n\ttext_links=[]\n\tfor link_text in text.find_all('a'):\n\t\n\t\tif 'href' in link_text.attrs:\n\t\t\t\n\t\t\t#check to see how many links are already added (i.e. 0 in total links)\t\n\t\t\t#if so, save to lists in text and add in list\n\t\t\tcount=0\n\t\t\tfor l in link_text.parent.parent.find_all('a', {'href':True}):\n\n\t\t\t\t#check if title is \"a\" and links"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\t# Extract URLs within text\n\t\turls = []\n\t\tfor match in TAG_RE.finditer(text):\n\t\t\ttag, url, pos = match.groups()\n\t\t\tif not url:\n\t\t\t\t# No link exists\n\t\t\t\tcontinue\n\t\t\turl = url.strip()\n\t\t\tif tag == 'a':\n\t\t\t\t# Anchor text\n\t\t\t\tmatch = TAG_RE.match(text, pos)\n\t\t\telif tag == 'img':\n\t\t\t\turl = url\n\t\t\telif tag == 'a' and RE_URL_ADJAB in hrefs_adja:\n\t\t\t\turl = href"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tdef parse_url(url):\n\t\t\t\"\"\"\n\t\t\turl can be a link or a path, like unix:////foo/bar\n\t\t\t(which will be checked using urlsplit) or just plain. Quoting\n\t\t\tthe URL will double quote it if needed.\n\n\t\t\tOnly basic URI input is parsed on urlparse.urlsplit without\n\t\t\tquoting first because it wants to be quoted by urllib.parse.unquote\n\t\t\t\"\"\"\n\t\t\tif isinstance(url,tuple):\t# url is already a link\n\t\t\t\treturn url.scheme\t\t\t# scheme is http, ftp, or file://, but needs no URL args"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tif \"<a\" in text:\t\t\t\t\t\t\t\t\t# remove @link: from text:\n\t\t\ttry:\n\t\t\t\tlink = text.split(\"</a>\")\n\t\t\t\ttext = text.replace(link[0], link[1]) # remove leading whitespace:\n\t\t\texcept:\n\t\t\t\tpass\t\t\t\t\t# already escaped\n\n\tfor link in text.split():\n\t\tif \"link:\" in link:\n\t\t\tif link[1]!= '#':\t\t# remove leading whitespace:\n\t\t\t\tlink = link.split()[0]\n\t\t\t\ttext = text.replace(link, \"link:\")\n\n\t\t\tm = re_"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tadd_to_index = text\n\tdef process_link_tag(text):\n\t\t\"\"\"\n\t\tProcess text inside an anchor element. The attributes and linkifying\n\t\tHTML inline elements are found and added to a list.\n\t\t\"\"\"\n\t\tfor attr,val in link_attrs:\n\t\t\tif val:\n\t\t\t\ttag = 'a_'+attr+'_'+val\n\t\t\t\tfor (a_attr_key,a_attr_val) in attrs:\n\t\t\t\t\tif a_attr_key == 'href': # anchor with href\n\t\t\t\t\t\thref = parse_escaped_html(a_attr"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tresult = requests.head(url)\n\t\tresult.raise_for_status()\n\t\treturn result.headers.get(\n\t\t\t\"Content-Type\", \"\").split(\";\")[0]\n\telse:\n\t\treturn None\n\ndef import_doctype_from_file(path: str) -> Optional[str]:\n\twith open(path, 'r') as infile:\n\t\tdoctype_data = json.load(infile)\n\n\tfor d in doctype_data:\n\t\tfor key in d.keys():\n\t\t\tif key.endswith('doctype') and \\\n\t\t\t\tnot '"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\ttry:\n\t\t\tif not url.startswith('http'):\n\t\t\t\traise ValueError\n\t\texcept ValueError:\n\t\t\t# bad URL - not a git repo\n\t\t\treturn None\n\t\texcept AttributeError:\n\t\t\tpass\n\n\t\ttry:\n\t\t\treturn requests.head(url)\n\t\texcept requests.exceptions.RequestException:\n\t\t\tpass\n\n\t\t# assume it's a file or directory\n\t\tif os.path.isdir(url):\n\t\t\treturn url\n\t\telse:\n\t\t\treturn None\n\n\t# make sure we understand the url\n\turl = url.strip(' +',')\n\tif url[0"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tresponse = requests.head(url)\n\t\tresponse.raise_for_status()\n\t\treturn response.headers.get('content-type', None)\n\treturn None\nfrom flask import jsonify, request\n\nfrom tests.base import TestCase\n\nfrom app.auth.models import User\n\n\nclass TestUserEndpoint(TestCase):\n    def test_user_endpoint(self):\n        \"\"\"Test user endpoint.\"\"\"\n        user = User(User(email='john@example.com', password='cat'))\n        db.session.add(user)\n        db.session.commit()\n\n        with self."}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\turl = url.lstrip()\n\n\t\tpattern_type = re.compile(\n\t\t\tr\"^(?:application/|image/application|text/plain)|application/json\"\n\t\t\tr\"|\"application/json|text/x-json|application/x-json-stream|application/xml\"\n\t\t\tr\"|\"image/png|application/x-png|image/png;q=0.9,image/jpeg|image/jpeg;q=0.9,application/xml\"\n\t\t\tr\",\"application/x-vnd.microsoft.icon\"\n\t\t\tr\"|\"image/"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\twith open(url) as url_file:\n\t\t\treturn url_file.readline().strip()\n\n\tdef check_content_type_and_size(self, content_type: str, size: int) -> None:\n\t\tmatch = CONTENT_TYPE_RE.match(content_type)\n\t\tif not match:\n\t\t\treturn\n\t\tctype = match.group(\"content-type\")\n\t\tself.size = size\n\t\tself.type = match.group(\"content-type-parameter\")\n\t\tif not ctype:\n\t\t\tself.handle_404()\n\t\telse:"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\t# First trying to get the remote url from the headers.\n\t\t# If that fails, try the 'normal' server URL first.\n\t\ttry:\n\t\t\tresponse = requests.get(url[5:])\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\treturn 'could not fetch remote content type: {}'.format(e)\n\t\tresponse.raise_for_status()\n\n\t\ttry:\n\t\t\t# Then try the 'gzip' server URL, if available. \n\t\t\t# If it does not work, attempt to use the 'gzip' compression method\n\t\t\t# on windows\n\t\t\tif response"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\twith open(url, \"rb\") as page_file:\n\t\t\tcontent_type = page_file.read().decode(\"utf-8\")\n\treturn content_type\n\ndef remove_file_and_folder(path: str) -> None:\n\t\"\"\"\n\tRecursively remove a specific path below the local storage.\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\tshutil.rmtree(path)\n\telif os.path.isfile(path):\n\t\tos.remove(path)\n\telse:\n\t\tlogging.warning(\"Could not find specified path"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tm3u8_response = requests.get(url, auth=HTTPBasicAuth(USERNAME, PASSWORD))\n\t\tif m3u8_response.status_code == 200:\n\t\t\treturn m3u8_response.text.split('charset=')[-1].strip()\n\treturn None\n\ndef set_avatar(user: User) -> None:\n\tpath: str = '%s/%s.jpg' % (settings.LOCAL_UPLOADS_DIR, user.username)\n\timage_path: Path\n\timage_data: bytes\n\timage_type:"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tparsed = urllib.parse.urlparse(url)\n\t\ttry:\n\t\t\tscheme = str(parsed.scheme)\n\t\texcept Exception:\n\t\t\treturn None\n\t\tif scheme not in ALREADY_SCHEMES:\n\t\t\treturn None\n\t\treturn ALREADY_SCHEMES[scheme]\n\treturn fetch_content_type(url)\n\ndef is_archive_file(url: str) -> bool:\n\t\"\"\"Returns True if the file at the given URL looks like an archive in the last\n\ttwo directories on the local machine.\"\"\"\n\turl_parts = urllib.parse.urlparse("}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tcontent_type = 'application/x-www-form-urlencoded'\n\t\ttry:\n\t\t\treq = Request(url)\n\t\t\treq.get_method = lambda: 'HEAD'\n\t\t\tresp = urlopen(req, content_type)\n\t\t\ttry:\n\t\t\t\tcontent_type = resp.info()['content-type']\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\texcept:\n\t\t\t\tpass\n\t\texcept Exception:\n\t\t\tself.logger.exception(\"Failed to fetch remote content-type.\")\n\t\tif content_type in ['application/zip', 'application/x-tar']:\t#"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tfor word in word_list:\n\t\t\tif word.lower() == tag.lower():\n\t\t\t\treturn word\n\t\n\t@staticmethod\n\tdef __load_config(text: str) -> Dict[str, Any]:\n\t\t\"\"\"\n\t\tLoad the WordInfo config file and return its content as a dictionary.\n\t\t\"\"\"\t\n\t\tword_list = []\n\t\twith open(settings.WORDINFO_FILE, 'r') as f:\n\t\t\tfor line in f:\t\t\t\t\t# skip blank lines\n\t\t\t\tword_list.append(line.strip())\n\t\t\t\tbreak\n\t\treturn word_"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn (re.search(tag + '', text)!= None) and len(tag) > 0\ndef build_url(words, word_count, tags_count):\n\ttag_count = 0\n\ttag_word_count = 0\n\tfor word in words:\n\t\tif word_count < len(word):\n\t\t\ttag = word[tag_word_count]\n\t\telif tag_count < len(word):\n\t\t\ttag = word[tag_count]\n\t\telse:\n\t\t\ttag = word[:-1]\n\t\tif tag!= word_count and tag!="}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tinput = test_utils.get_test_input(\"Enter 1 to tag: \")\n\t\toutput = test_utils.get_test_output(\"Should tag: {0}\".format(input))\n\t\tassert output == \"true\"\n\n\ndef test_random_positive():\n\t\"\"\"\n\tTest random positive numbers.\t\n\t\"\"\"\n\tinput = test_utils.get_test_input(\"enter a random: \")\n\toutput = test_utils.get_test_output(\"{0} is randomly positive\".format(input))\n\n\t# Verify the output is correct\n\tassert"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\ttag = tag.lower()\n\t\treturn tag in tag_to_tags\n\n# =============================================================================\n#\n#\t\t\t\t\t\tMain code.\n#\n# =============================================================================\n#\n\nif __name__ == \"__main__\":\n        \"\"\" Main code. \"\"\"\n\n        version_info = 0.1\n        \n        # Check if called from test code\n        if len(sys.argv) < 2 and sys.argv[1] == '-r':\n                print_usage(sys.stdout)\n                exit()\n\n        # Open and read the parameter file.\n        if len(sys.argv) >= 2:\n                try:\n                        with"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag == \"PR\" or tag == \"PR_TAG\"\n\n\nclass SentimentData:\n\t\"\"\"\n\tManage the vocabulary and lemmatized information for a word.\n\t\"\"\"\n\t# TODO\n\t# test for tag consistency with the Lemmatizer (as opposed to tag consistency).\n\t# make sure it works.\n\n\tdef __init__(self, max_word_size: int) -> None:\n\t\tself.max_word_size = max_word_size\n\t\tself.tag_table = [tuple(random.choice(string"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag in \"aeiou\" or \"-\" in tag or \"iudan\" in tag\n# Copyright 2011-2017, Damian Johnson and The Tor Project\n# See LICENSE for licensing information\n\n\"\"\"\nParsing for version numbers from terms.txt.\n\"\"\"\n\nimport datetime\nimport re\nimport sys\nimport operator\n\nfrom tor import strutils\n\ntry:\n    version_info = strutils.parse_version_info\nexcept ValueError:\n    version_info = 'Unknown'\n\ntry:\n    parse_version_info = strutils.parse_version_"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif not tag.strip():\n\t\t\traise ParserException('Empty tag')\n\treturn True\n\ndef tag(string: str) -> bool:\t\"\"\"\n\tTest to see if the string contains a valid (untag) tag \n\t(the same as a previous test by the parser function as for\n\ta \"test_tag\".)\n\t\"\"\"\n\ttag = tag_re.search(string)\n\treturn True if tag else False\n\n\nclass TestTag(unittest.TestCase):\t# pylint: disable=R0903\n\tdef get_test_tag(self, string"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif not tag.isupper() or tag.isupper() in ('AA', 'AB'):\n\t\t\treturn False\n\t\telif not tag.isalpha():\n\t\t\treturn False # not an initial punctuation is a tag\n\n\treturn True\n\ndef generateRandom(n, k):\n    m = [None] * n\n    while len(m) < k:\n        m.append(random.random())\n    return m\n\ndef getRandomChar():\n    char = []\n    for i in range(n):\n        char.append(\"x\")\n    return ''.join(char)\n    \ndef createTestArray"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn not is_word(tag)\n\n\ndef is_word(tag: str) -> bool:\n\t\"\"\"\n\tCheck to see whether the given tag is a word or punctuation.\n\n\t:param tag: The tag to check.\n\t\"\"\"\n\treturn not any(tag.isalpha() or tag.isdigit()  for tag in re.findall(r'[\\w.-]+', tag))\n\n\ndef clean(text: str) -> str:\n\t\"\"\"\n\tClean a given text.\n\n\t:param text: The text to clean.\n\t\"\"\"\n\ttext_cleaned ="}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag.lower() in ['a', 'an', 'and', 'the', 'that']\n\treturn False\n\n\ndef read(content: list) -> str:\n\t\"\"\"Read the entire content.\n\t\n\t:param content: The text to parse\n\t:return: The parsed content\n\t\"\"\"\n\traw_content = content[len(CONTENT_TEXT):]\n\tparsed_content = raw_content.split('\\n')\n\tparsed_content = [x for x in parsed_content if (len(x) > 0 and x[0]!= '#')]\n\tparsed"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tchild_dict = {}\t\n\tfor child_child in node:\n\t\tchild_dict[child_child[\"tag\"]] = child_child\n\t\n\treturn child_dict\t\n\t\ndef xml_xml_to_dict(xml_string):\t\n\t\"\"\"\n\tTurn xml string into python dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tto_dict = {}\t\n\tfor xml_child in xml_string:\t\n\t\t\n\t\txml_dict = dict ()\n\t\tif xml_child.tag in"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tif isinstance(node, minidom.Element): return node.childNodes.__dict__ # avoid circular import \n\t\treturn dict((n.nodeName, n) for n in node.childNodes.childNodes if n.nodeType!= n.TEXT_NODE)\n\ndef xml_getNodeValue(node, tag):\n\treturn node.getElementsByTagName(tag)[0].nodeValue\n    \ndef xml_getAttribute(node, tag):\n\t\"\"\"\n\tLook for a child node with tag == tag. Contents, attributes and\n\tcontent in the <attr> tag will be substituted.\n\nReturns a tuple containing the"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tdef lookup_child(child, tag_name, parent=None):\n\t\t\t\"\"\"Check if child is in the dictionary.\"\"\"\n\t\t\tif tag_name.lower() == tag_name:\n\t\t\t\treturn child\n\t\t\telse:\n\t\t\t\tif parent is None:\n\t\t\t\t\tparent = child\n\t\t\t\tfor child2 in xml_children_as_dict(child):\n\t\t\t\t\tif tag_name.lower() in child2.keys():#or tag_name.lower() == child2.keys()[0]:\n\t\t\t\t\t\treturn child2[tag_name.lower()]\n#\t\t\t\telse:\n#"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\treturn dict([(name, node.get(name)) for name in node.keys()])\n\n\ndef xml_children_as_list(node):\t\"\"\"\n\tTurn the children of node <xml> into a list of child nodes, for nodes representing top-level elements.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\treturn [child.tag for child in node.getchildren() if not child.is_root()]\n\n\ndef json_to_xml(d):\n\tif d is None:\n\t\treturn ''\n\telse:\n\t"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n#\tprint \"Start :\\n\\n\", xmlsplit\n\n\tglobal last_id, next_node\n\tlast_id = node['id'] \n\tnext_node = xml.Element(\"end\", attributes={\"xmlns\": xsd_ns})\n\n\treturn_node_ids = []\n\tglobal last_id, next_node\n\tlast_id = last_id + 1\n\tnext_node = xml.Element(\"n\", attributes={\"xmlns\": xsd_ns})\n\n\n\treturn_dict(next_node, return_node_ids)\n\ndef xml_"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tndict = xml_elem_as_dict(node)\n\t\tfor k in ndict.keys():\n\t\t\tif ndict[k].__class__!= xml_elem_as_dict:\n\t\t\t\t#raise Exception(\"child %s has child nodes but not children\" % k)\n\t\t\t\tsys.stderr.write( \"xml_children_as_dict :\\n%s\\n\\t\\t\\n\" % str(ndict[k].__class__))\n\t\t\t\tcontinue\n\t\t\tfor c in ndict[k]:\n\t\t\t\tif c in exclude_classes:\n\t\t\t\t\tcontinue\n\t\t\t"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\ttag_name = node.tag\n\t\tif tag_name in _children:\n\t\t\tchild_dict = _children[tag_name]\n\t\t\tif isinstance(child_dict, list):\n\t\t\t\tchild_dict = [_xml_children_as_dict(child_elem) for child_elem in child_dict]\n\t\t\t\treturn_dict = _xml_children_as_dict(child_dict)\n\t\tif tag_name in _xml_children_as_dict(node):\n\t\t\tchild_dict = _xml_children_as_dict(node)\n\t\t\tif isinstance"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t \n\treturn defaultdict(dict)\n\n\ndef children_as_dict(node):\t\n\t\"\"\"\n\tTurn the children of node <xml> into a dictionary, keyed by tag name.\n\nThis is only a shallow conversation - children are not recursively processed.\t\"\"\"\n\t \n\treturn defaultdict(dict)\n\n\ndef add_child_node(key, node):\n\tnode[key] = node.get(key, {})\n\tnode[\"children\"] = []\n\n\tfor c in node.getchildren():\n\t\tadd_node(c, node.get(c))\n\n\t"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\ttry:\n\t\t\ttag_details = node.xpath('./tag:{}'.format(xml_tag_name))\n\t\t\tfor tag_detail in tag_details:\n\t\t\t\ttag_name = tag_detail.tag[3:].strip()\n\t\t\t\ttag = node.getchildren()\n\t\t\t\tif len(child_nodes) == 0 or tag[1].text.strip() == '':\n\t\t\t\t\tfor c_child_node in node:\n\t\t\t\t\t\t# Add the child node to the dict\n\t\t\t\t\t\tc_child_node.append(c_child_node.text.strip())\n\t\t\t\t\tnode.append"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n\t\n\tfor child in node.children:\n\t\tif child.tag == \"children\":\n\t\t\tchild_children = xml_children_as_dict(child)\n\t\t\tif child_children:\n\t\t\t\t# If this is a top level list of children as leaf with id > 1, then add child as parent\n\t\t\t\tfor i in range(len(child_children)):\n\t\t\t\t\t# child nodes have id > 1 as parent\n\t\t\t\t\tif len(child_children) > 1 and child_children[i].tag == \"children\" and (parent_id == i + 1 or parent_name in child"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender = CourserManager.objects.filter(sender_ref=sender_handle).first()\n\t\trecipient = CourserManager.objects.filter(recipient_ref=recipient_handle).first()\n\t\tif sender!= recipient:\n\t\t\traise ValueError(\"The sender sent by {} is {}, but was not {}\".format(str(sender), str(recipient), sender_handle))\n\tprint(\"Checking the sender of {} is {}\".format(str(sender), str(recipient)))\n\t# Get object\n\tsender = CourserManager.objects.get(object_ref=sender_handle"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\ndef check_recipients(sender_handle1, recipient_handle1):\n\t\"\"\"\n\tEnsure that sending a list of recipients and one is a valid recipient. The sender should already have the right\n\trecipient id.\n\tOtherwise, if a recipient with that ID exists, the error message is sent to each recipient.\n\tThe sender must be a recipient with at least one recipient and not a different sender.\n\tThe check is performed for \"recipient_id\" and \"sender_id\" properties.\n\tReturns a list of errors found in the payload.\n\t\"\"\"\n\t# Check validity"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\trecipient = message_content.get_owner(entity_handle)\n\t\tsender_handle_recipient = get_proper_handle_recipient(sender_handle)\n\t\tassert(sender_handle_recipient == recipient)\n\n\t\trecipient.recipient_type = message_content.get_owner(sender_handle_recipient)\n\t\trecipient.recipient_id = message_content.get_owner(sender_handle)\n\t\tsender.add_recipient(recipient)\n\t\tsender.recipient_full_name = message_content.get_owner(sender_handle)\n\n\t\tif message_content."}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tassert sender_handle.get_email_subject() == entity_handle.get_email_subject()\n\n\t\t# This is not guaranteed if the sender is banned. So don\u2019t check any of them. Don\u2019t check the\n\t\t# 'is_banned' attribute and rely on the check_read_permission() method.\n\n\t\t# All users in the payload belong to the sender who are also admins.\n\t\tif sender_handle.get_email_subject() == entity_handle.get_email_subject() and \\\n           sender_handle.get_email_recipient()"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n\t\n\tif sender_handle in sender.get_author_list():\n\t\tsender_handle.send(sender.get_author_list())\n\t\treturn True\n\t\t\n\tif entity_handle not in sender.get_entity_handles():\n\t\tif(check_if_sender_is_entity_already_in_list(sender, entity_handle)):\n\t\t\tsender.add_entity_handle_list(entity_handle)\n\t\t\treturn True\n\t\t\n\treturn False\n\t\n\"\"\"\nFunction that will be called by our module when a mod has been loaded.\nHere is an example of what the"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender_handle_id = get_sender_id(sender_handle)\n\t\tif sender_handle_id!= entity_handle.handle_id:\t\t\t\t\t\t# This check is not needed, it should be true, just to be sure\n\t\t\tlogging.error(\"Didn't find sender_handle_id in DB for this match\")\n\t\t\traise CheckRecipientError(\"Couldn't find sender_handle_id in DB for this match\")\n\n\t\tif not is_same_sha256(sender_handle.sha256, entity_handle.sha256):    # check the sender is not the same"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender_token = yield client.verify_token(sender_handle, access_token)\n\t\tentity_token = yield client.verify_token(entity_handle, access_token)\n\t\tassert sender_token == entity_token\n\n\ndef _send_payload(client, sender_handle, obj_handle, origin_handle, payload, origin_type):\n\t\"\"\" A helper function for _send_payload and _send_payload_on_entity().\"\"\"\n\n\t# Find the origin of the message -- that can be useful when the origin is a\n\t# command, and if so,"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n\n\t# Now lets see if the recipient and the object matches. If they don't match we'll have to\n\t# add the check to be done out of the loop.\n\tif not check_recipient(sender_handle, entity_handle):\n\t    raise ValidationError, (\"sender\", \"receiver\")\n\n\t# Now we need to check whether the person was already registered in the session.\n\tnick = entity_handle.get_nick()\n\tperson = entity_handle.get_person()\n\n\t# We have to do the following:\n\t# Check if the recipient has a custom nick (ie"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsubject = \"\"\n\t\tif \"sender\" in entity_handle.attributes:\n\t\t\tsubject = entity_handle.attributes[\"sender\"]\n\t\telif \"sender_name\" in entity_handle.attributes:\n\t\t\tsubject = entity_handle.attributes[\"sender_name\"]\n\t\telif \"target_handle\" in entity_handle.attributes:\n\t\t\tsubject = entity_handle.attributes[\"target_handle\"]\n\t\tif subject == sender_handle or subject == sender:\n#\t\t\tprint(\"sender_handle\")\n\t\t\tif (subject == receiver_handle or subject == sender):\n\t\t\t\tif ("}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\t# The message we need to generate is not the same one.\n\n\t\t# Note that this could be done during delivery, if the previous version of the framework is set to the latest,\n\t\t# we need to change it to 'in process'.\n\t\tif ((sender_handle.payload.find(old_name)!= -1) or\n\t\t\t'sender' not in old_name):\n\t\t\treturn True\n\t\t# The sender is the same as the current one. Let's try to send out the appropriate\n\t\t# sender.\n\t\tsender_name = sender_handle.name.encode('"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tfrom nodeinfo import NodeInfo\n\t\ts = NodeInfo()\n\t\ts['id'] = DocumentPath(url)\n\t\tif document_path:\n\t\t\ts['localname'] = document_path\n\t\tnodes = s.nodeinfo()\n\t\tfor node in nodes:\n\t\t\tif (node.has_key('id') and node['id'] is not None):\n\t\t\t\tcontinue\n\t\t\tpath = node['localname'].replace(document_path, '')\n\t\t\tif (path.startswith('/') and path[1:] in ['.well-known', '.wld']): # ignore localname."}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\turl = self._full_url(url, self.WS_ROOT)\n\t\treturn self._load_remote_node_info(url, document_path)\n\t\t\n\tdef _load_remote_node_info(self, url, document_path):\n\t\treturn fetch_url(url, document_path, self.WS_ROOT)\n\t\t\n\t\n\tdef insert(self, path, item):\n\t\t\"\"\"inserts something into the database.\"\"\"\n\t\treturn self._op_insert(path, item)\n\t\n\tdef _op_insert(self, nodeinfo, document_path):\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\turl_parts = urllib.parse.urlparse(url)\n\t\turi = url_parts[1]\n\t\tif document_path:\n\t\t\tpath = document_path\n\t\telse:\n\t\t\tpath = os.path.join(os.path.dirname(__file__), 'nodeinfo', url)\n\t\t\t\n\t\tif os.path.exists(path):\n\t\t\tif not os.path.isdir(os.path.split(path)[0]):\n\t\t\t\t\traise IOError(\"%s is not a directory, abort.\" % path)\n\t\t\tfh = File(path, 'r')\n\t\t\troot_"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None: return None\n\n\t\t# First, try the cache first\n\t\tif use_cache: return None\n\t\t_filename = os.path.join(CFG_CACHE_LAST, \"%s.xml\" % url)\n\t\tif not os.path.exists(_filename):\n\t\t\t# Fetch the document from the cache\n\t\t\treturn load_embedded_document(url)\n\t\telse:\n\t\t\t# Load and cache the document from the file locally and parse it\n\t\t\tlocal = urllib2.urlopen(_filename).read()\n\t\t\t# First, parse the local document to build"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n\n\t# default document path\n\t# TODO: Make this more robust... \n\tdoc_path = utils.get_path('nodeinfo', document_path)\n\t\n\t# generate Document using the basics given\n\tparams = {'no-cache':1} \n\n\t# generate HTTP request headers\n\treqheaders = {\"Authorization\":\"userpass\"}\n\t# generate HTTP request body\n\tbody = '<?xml version=' + __version_number__ + '; encoding=utf-8'\n\n\tresponse = http_request('GET', page_address + url, data=body, reqheaders="}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n\t\n\t# If the path is not specified, then use the document path to\n\t# this script.  This is useful when the developer only\n\t# wants to use a path that has spaces in it.  If there is\n\t# other path, then try to find self.document_path.\n\tif document_path is None:\n\t\ttry:\n\t\t\timport urlparse\n\t\t\tdocument_path = os.path.abspath( os.path.join( os.path.dirname(__file__), \"node-info.harness-path\" ) )\n\t\texcept ImportError:\n\t\t\tpass\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n\n\tif document_path:\n\t\tdoc = Document(document_path)\n\telse:\n\t\tdoc = Doc()\n\n\tdef _clean_doc(doc):\n\t\tdoc.documentElement.replaceChild(0, DOM)\n\t\tdoc.documentElement = None\n\t\tdoc.doctype = None\n\t\treturn doc\n\n\tdef build_doc(nodeinfo):\n\t\tdoc = Document()\n\t\tif nodeinfo.docinfo.doctype: doc.doctype = nodeinfo.docinfo.doctype\n\t\tdoc.appendChild(nodeinfo.docinfo)\n\n\t\tdef add_header(name, content):\n\t\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tdoc = parse_nodeinfo_document(url)\n\t\tif doc is None:\n\t\t\treturn None\n\n\t\twel = nodes.wave._NodeinfoWire(doc.document())\n\t\tif not wel:\n\t\t\traise IOError('%s is not a valid NodeInfo document: file is not a well-known document' % url)\n\n\t\twel['type'] = nodes.File.DIR if get_os()!= \"windows\" else nodes.Directory\n\n\t\twell_known = nodes.NodeInfo(nodes.worldspace())\n\t\twell_known.title = \"/Worl"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif not DocumentManager:\n\t\t\traise RuntimeError(\"Please document this script via PHUA.\")\n\t\treturn DocumentManager.load(document_path).get_nodeinfo()\n\telse:\n\t\treturn DocumentManager.load(url)['well-known']\n\ndef update_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tUpdate a NodeInfo.well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = url.split('.well-known')[1]\n\t\tif document_path[-1] == '-':\n\t\t\tdocument_path = document_path[:-1]\n\t\treturn _make_protocol_nodes(document_path)\n\ndef _nodeinfo_create_document(nodeinfo_elt, document_path=None):\n\t\"\"\"\n\tGenerate a document for a single nodeinfo elt, which also contains\n\tinformation about nodetypes.\n\t\n\t:type nodeinfo_elt: domutils.NodeInfo\n\t:arg nodeinfo"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tkeys = _verify_keys(doc, signature)\n\t\tfor key in keys:\n\t\t\tassert key in public_key.public_elements(), key\n\t\n\tfor key in keys:\n\t\tassert key in public_key.public_elements(), key\n\ndef _get_key_from_doc(doc):\n\tkeys = doc.getElementsByTagName(\"key\")\n\tassert keys.length == 1, 'Only one key found in doc'\n\treturn keys[0]\n\ndef _verify_keys(doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\n#===============================================================================\n#                    MAIN\n#===============================================================================\n\nclass MAIN (BaseReceiptSender):\n\t\"\"\"\n\tConstructs a MAIN request and associated documentation.\n\t\n\t- 'api' (optional) If no 'api' prefix is explicitly set, API\n\t\tprefix will be the version of the API. By default this\n\t\tis 'v2'.\n\t\n\t- 'data' (optional) The raw data associated with the\n\t\tgenerated SOAP message (if this is not provided, the message\n\t\twill be derived from the class attributes named\n\t\t'msg')\n\t\n\t- 'auth' ("}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tsigned_doc = ensure_etree(doc,'signed-doc')\n\t\tsigned_doc.assertValid(signers_sign(SIGNED_NAMESPACE, signed_doc)).assertHasFeatures((NS_XSI, NS_PUBKEY, NS_SIGNED_SUBJECT))\n\n###################\n###  LOGGING/MONITOR UTILITIES      ###\n###################\n\nDEFAULT_NAMESPACE = Namespace.fallback (Namespace.log, '/logs', 'http://logstash.net/logstash/')\nDEFAULT_TAGGER = Namespace (Namespace.log, '/logs', 'http://logstash"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\n#\tprint 'Verifying message signing...'\n\t#assert len(signature) > 1024\n\tassert verify_signature(public_key, doc, signature) is True, \\\n\t\t'Invalid signature received, verified: signature=%r, issuer=%r' % (signature, verify_issuer(public_key, doc))\n\ndef verify_serial(key, doc):\n\t\"\"\"\n\tVerify the signature issued by the identity by verifying the\n\tsignature that the message signed by the proof-of-work.\t\n\t\"\"\"\n\tassert len(key) >= 2048\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t#Verify signature: let's extract the nonce from it\n\t\t#print \"verify_signature\", signature\n\t\t#signature = signature.replace(\":\",\"\")\n\t\tsig = base64.decodestring(signature)\n\t\t#Check our pubkey\n\t\tverify_pk = public_key.verify(sig, public_key.publickey())\n\t\t\t\t\n\t\t#Check the signature to see if the signature matches our public key\n\t\tassert verify_pk == signature\n\n\t\treturn verify_pk\n\t#Verify the signature is sane\n\treturn pubkey\n\n#-------------------------------------------------------------------------------\n\n# Main part of the main routine"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t# Check that the signature is valid.\n\t\t# It should have at least \"public_key\", \"xml_signature\" or \"msg1\" in\n\t\t# the tree!\n\t\tsigned_xml = None\n\t\tsigned_msg = None\n\n\t\ttry:\n\t\t\tsigned_xml_as_bytes = signed_xml.SerializeToString()\n\t\t\tassert signature[0:len(signature) - 3] == \\\n\t\t\t\t  signed_xml_as_bytes\n\t\texcept AssertionError: # signature is invalid\n\t\t\tlog.debug(\"Expected signature: {0}\".format(signature))\n\n\t\tassert not doc"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t# Let's look at the signature element in the document\n\t\tsigned_doc = etree.fromstring(signature)\n\t\tsigned_xml = ET.Element(\"signed\")\n\t\tsigned_xml.set(\"version\", \"1.0\")\n\t\tsigned_root = signed_doc.find(signed_xml)\n\t\tprint(\"Found document with signature\", signed_root.tag, \"signed.\")\n\t\tsigned_signature = etree.tostring(signed_root, pretty_print=True)\t\n\t\tprint(\"Signature is:\", signed_signature)\n\t\tif public_key is None or public_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tif not public_key.verify_key(signature, doc):\n\t\t\traise ValueError(\"{} signature is incorrect\".format(signature))\n\n\tdef create_message_id(msg, id_number):\n\t\t\"\"\"\n\t\tGenerate a message_id number that can be used to make it meaningful.\n\t\t\"\"\"\n\t\tn = 2\n\t\tif id_number == 1:\n\t\t\tmsg_id = '{}-{}-{}'.format(\"0\"*n, n, 1)\n\t\telif id_number == 2:\n\t\t\tmsg_id = '{}-{}-{}'.format(\"0\"*n, n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t# Verify the root element doesn't contain the <public> element\n\t\t# that is in document. Raise exception if not.\n\t\tif public_key.find(\"public\") <= 0:\n\t\t\treturn True\n\t\t\n\t\t# Verify the signature doesn't contain the <name> element\n\t\t# that is in document. Raise exception if not.\n\t\t#if signature.find(\"name\") <= 0:\n\t\t#    return True\n\t\t\n\t\t# Verify the signed message is well-formed\n\t\t# Verify that the signature does contain the intended message\n\t\t#\n\t\t# For example, the following message are not"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tfrom pytango.srv import Client, BaseClientNoTty, ServerContext\n\n\n\t# verify signature\n\tsignature = xml_tools.verify_signature(signature, public_key)\n\tif not signature:\n\t\tlogger.debug(\"Signature did not give us this message...\")\n\t\treturn False\n\n\t# verify signature\n\tif SignatureVerifyType.get(signature) == None:\n\t\tlogger.debug(\"No Signature in '%s!'\"%(signature))\n\t\treturn False\n\n\t# verify signatures on user keys\n\tsig_public_key = None\n\tsig_private_key"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdoc = etree.fromstring(document, parser=etree.XMLParser(remove_blank_text=True))\n\t\t# Get doc title and parse it, if it fails, take from document\n\t\ttitle = doc.xpath('//h3[@class=\"pagetitle\"]')\t\n\t\tif len(title) == 0:\n\t\t\ttitle = doc.xpath('//h3[last()]/text()')[0].strip()\n\t\t\ttitle = title[:title.find('<title>')]\n\t\t\ttitle = title.replace('\\n', '') # Truncate title\n\t\t\ttitle = self.clean_title("}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdoc = document.replace('Diaspora webfinger.', 'Xrd WebFinger').replace('Diaspora fare.', 'Dominant').replace('Diaspora fioering.', 'Doradores')\n\t\tif document.startswith('Diaspora Fioering'): # Diaspora Fioering\n\t\t\tdoc = 'Dominant Fioering'\n\t\telif document.startswith('Dominant'): # Diaspora\n\t\t\tdoc = 'Dominant'\n\t\tif '/docbook' in document:\n\t\t\tdoc = '/doc"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdoc = json.loads(document)\n\n\t\tif doc[\"type\"] == \"webpage\":\n\t\t\tid = doc[\"id\"]\n\t\t\ttitle = doc[\"title\"]\n\n\t\t\treturn {\n\t\t\t\t\"type\": \"webpage\",\n\t\t\t\t\"id\": id,\n\t\t\t\t\"url\": doc[\"owner\"],\n\t\t\t\t\"created_at\": doc[\"firstcreated\"],\n\t\t\t\t\"permalink\": [{\"url\": url, \"width\": \"100%\"}],\n\t\t\t\t\"title\": [quote(title.replace('\"', \"'\"), \":\").replace(\"title=\", \"\"), quote(title), quote(title) if re."}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdiaspora_data = dict()\n\t\twith io.open(document, mode='r', encoding=\"utf-8\") as f:\n\t\t\tfor line in f:\n\t\t\t\telement, title, url, daspora = get_element(line)\n\t\t\t\tdiaspora = {\n\t\t\t\t\t\"title\": element,\n\t\t\t\t\t\"url\": url,\n\t\t\t\t\t\"daspora\": daspora\n\t\t\t\t}\n\t\t\t\tdiaspora_data[element] = diaspora_data.get(element, {})\n\t\t\t\tdel diaspora_data["}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tresponse = {}\n\t\ttry:\n\t\t\tresponse = parse_diaspora_webfinger_json(document)\n\t\texcept ValueError:\n\t\t\tpass\n\t\tfinally:\n\t\t\tif not isinstance(response, dict):\n\t\t\t\traise ValueError(\"Not a valid json document: json object expected!\")\n\t\treturn response\n\n\tdef get_document(self, url: str, query: dict, limit: int = 100, offset: Optional[int]=0) -> tuple:\n\t\t\"\"\"\n\t\tReturn the content of a Diaspora JSON response document.\n\t\t\n\t\thttp://diaspora"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\n\n\ttry:\n\t\ttree = ElementTree.fromstring(document, etree.XMLParser(encoding='utf-8'))\n\t\tnodes = tree.findall(ns('rdf:Description', ns('dcterms:subject', ns('dcterms:subjectRef', 'rdf:type'))) )\n\t\tdoc = []\n\t\tfor node in nodes:\n\t\t\tif node.get(ns('dcterms:predicate'))!= None:\n\t\t\t\tdoc.append(node.get('rdf:resourceIdentifier'))\n\t\n\t\t\tif node.get(ns('dcterms:property'))!= None:\n\t\t\t\tdoc.append("}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t # Diaspora Faculty (in JSON format)\n\t soup = BeautifulSoup(document, 'html.parser')\n\t tables = soup.find_all('tbody') # Diaspora Table\n\t # Add the first table in only one row\n\t tables[0].insert(0, tables[0].get_text().strip()) # Add table header\n\t tables[1].insert(0, tables[1].get_text().strip()) # Add first td\n\t tables[2].insert(0, tables[2].get_text().strip()) # Add second td\n\t tables[3].insert("}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdoc = json.loads(document)\n\t\tif not doc:\n\t\t\treturn None\n\t\treturn webfinger(doc)\n\n\tdef validate_and_raise(document: str) -> Tuple[int, str, int, str, str]:\n\t\treturn validation(document)\n\n\tdef document_to_message(content: str) -> str:\n\t\t\"\"\"Convert message to JSON document\"\"\"\n\t\ttry:\n\t\t\tdoc = json.loads(str(content), object_pairs_hook=OrderedDict)\n\t\t\tif \"_id\" in doc:\n\t\t\t\tassert False, f\"Document message"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tf = open(document, \"r\")\n\t\td = _parse_webfinger_xml_old(f.read())\n\n\t\td[\"name\"] = _parse_dusket_name(d[\"name\"])\n\t\td[\"name_long\"] = _parse_dusket_name(d[\"name\"])\n\t\td[\"phone\"] = _parse_phone_name(d[\"phone\"])\n\n\t\t# \"address\": d[\"address\"]\n\t\t# \"postcode\" : d[\"postcode\"]\n\t\t# \"city\": d[\"city\"]\n\t\t# \""}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\treturn document.replace('\\n', '').replace('\\r', '').replace('\\\\n', '')\n\n\tdef _parse_discovered(document: str) -> Dict:\n\t\t\"\"\"\n\t\tParse Discovered Statements text.\n\t\t\"\"\"\n\t\tdocuments = [s.strip().lower() for s in document.split('\\n')]\n\t\tdocuments.extend(['N', 'P', 'U', 'P S', 'W', 'UH'] * 7)\n\t\tdocuments.extend(['L', 'M', 'N', 'L S', 'W', 'MH'] * 11)\n\t\t"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tif handle in webfinger_document_cache:\n\t\t\treturn webfinger_document_cache[handle]\n\n\t\twebfinger_document_cache[handle] = try_get_webfinger_document(handle)\n\t\treturn webfinger_document_cache[handle]\n\n\t@staticmethod\n\tdef try_check_webfinger_document_for_webfinger_type(handle: str, webfinger_type: str) -> bool:\n\t\t\"\"\"\n\t\tTry to fetch a WebFingerDocument for an `WebFingerType`.\t\n\n\t\t:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t result = weechat.config_get_plugin('webfinger_document')\n\t result.get_string(handle, \"url\", weechat.WEECHAT_RC_OK)\n\t return result\n\n\t@weechat.command('get_rfc7033_document', aliases=('sr',), pass_user=True)\n\t@ensure_session_id\n\t@checks.admin_or_permissions(ban_members=True)\n\t@commands.guild_only()\n\tasync def get_rfc7033_document(self, ctx: commands.Context, url: str"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\t_logger.info(\"Try to retrieve an RFC7033 webfinger document (try #{})\".format(handle))\n\t\ttry:\n\t\t\tself.current_document = next(self.handle[1].search(handle).groups())\n\t\t\tself.open_document()\n\t\t\treturn self.current_document.text\n\t\texcept StopIteration:\n\t\t\traise RuntimeError(\"Could not retrieve an RFC7033 document\")\n\n\tdef close_document(self) -> Optional[str]:\n\t\tif self.current_document is not None:\n\t\t\tself.handle.close()\n\t\t\tself.current"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\twith urllib.request.urlopen(URL + '/?request=getwebfinger_document') as f:\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tbuffer_ = f.read()\n\t\t\t\t\t\t\tif '</html>' in buffer_:\n\t\t\t\t\t\t\t\treturn buffer_\n\t\t\t# This has to be a URL or a file (path), not just text.\n\t\t\twith urllib.request.urlopen(URL + '/?file=' + urllib.request.url2pathname(handle)) as f:\n\t\t\t\tbuffer_ = f.read()\n\t\t\t\tif os.name == 'nt':\n\t\t\t\t\tbuffer_ ="}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\tif handle in ('eioe.pdf', 'eioe.txt'):\n\t\t\t\treturn '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n' \\\n\t\t\t\t\t'<!DOCTYPE html>\\n' \\\n\t\t\t\t\t'<html lang=\"en\">\\n' \\\n\t\t\t\t\t'<head>\\n' \\\n\t\t\t\t\t'<meta charset=\"utf8\">\\n' \\\n\t\t\t\t\t'mimeType=\"application/octet-stream\"\\n' \\\n\t\t\t\t\t'mimeSize=\"104857600\">\\n' \\\n\t\t\t\t\t'<title>WebFinger for'+ \\\n\t\t\t\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tresult = get_rfc7033_data(handle)\n\t\treturn result\n\n\ndef get_rfc7033_data(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to parse an RFC7033 HTTPFinger XML document into a Python dictionary;\n\ttry_retrieve_rfc7033_data() returns the data as a parsed dictionary, else None.\n\t\"\"\"\n\n\t# First see if it is a supported PubNub Real Document or a PubNub\n\txml_info = parse_httpfpnub_xml(handler.content, handle)"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\treturn urllib2.urlopen(handle).read().replace('<!DOCTYPE HTML PUBLIC', '').replace('<!doctype HTML PUBLIC', '').replace('</!DOCTYPE HTML PUBLIC ', '')\n\texcept Exception: pass\n\ndef try_get_wrote_docs(word: str) -> Any:\n\ttry:\n\t\turl = 'https://www.rfc-softwebfinger.org/v8/' + word.strip().replace('/', '_').split()[0] + '/index.html'\n\t\treturn urllib2.urlopen(url).read().replace('\\n', '')\n\texcept Exception: pass\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tresult = []\n\t\tdoc_path = handle\n\t\ttry:\n\t\t\twith open(doc_path, \"r\") as webfinger_doc:\n\t\t\t\tdoc_text = webfinger_doc.read()\n\t\t\t\tfor doc in DocBuilder().parse(doc_text):\n\t\t\t\t\tid = doc.id\n\t\t\t\t\tif not id or not id.startswith(\"h1\"):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\ttitle = doc.title_description.text\n\t\t\t\t\ttype = doc.mime_type.text\n\t\t\t\t\tpath = _make_webfinger_document_path(id, title, type"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tif not await session.is_connected():\n\t\t\treturn None\n\n\t\tif session.url is None:\n\t\t\treturn \"Could not retrieve webfinger\"\n\t\tif not await extract_webfinger(handle):\n\t\t\treturn \"Could not retrieve webfinger: \" + handle\n\t\treturn None\n\n\t# Use this to keep going apart more stable\n\tdef get_page_count(self, url: str, session: aiohttp.ClientSession) -> t.Optional[t.Optional[str]]:\n\t\t\"\"\"\n\t\t@param url: The url to retrieve the count for.\n\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\t# Try to parse. If success, return the file's 'content' and let the library handle parsing.\n\t\t\tfile_path, content = _parse_rfc7033(handle, raise_errors=False)\n\t\t\treturn file_path\n\t\texcept LookupError:\n\t\t\t# If for whatever reason we can't find the file, but let the library handle parsing.\n\t\t\treturn None\n\n\treturn None\n\"\"\"\nThis module does basic sanity and introspection for FlickrAPI.py.\n\nCreated on Sept 8, 2014\n\n@author: Yifan Y"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\traise NotImplementedError,'retrieve_and_parse_diaspora_webfinger'\n\n\tdoc = parse(handle).next()\n\n\tif 'DiasporaWebFingerList' in doc:\n\t\t# The DiasporaWebFingerList document isn't valid XML\n\t\treturn retrieve_and_parse_diaspora_webfinger_list(handle)\n\n\treturn None\n\t# end handle\n\ndef retrieve_and_parse_diaspora_webfinger_list(handle):\t\"\"\"\n\tRetrieve a and parse a remote Dias"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tdoc = handle.retrieve_doc()\n\t\tif doc.has_remote(\"http://www.diasporawebfinger.com\"):\n\t\t\tdiasporawebfinger_document = {\n\t\t\t\t\t\t\t\t\n\t\t\t\t\"name\": doc.get_remote(\n\t\t\t\t\t\"authoritativeWebfactionWebfactionId\")[0].text\n\t\t\t\t}\n\t\t\treturn diasporawebfinger_document\n\t\t\n\telse:\n\t\traise Exception(\"Server probably doesn't support Diaspora Webfinger.\")\n\t\t\ndef retrieve_and_parse_diaspora_coupling(handle"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\thandle = \"1\" if handle is None else handle\n\t\tres = dict()\n\n\t\tif not webfinger:\n\t\t\t# Use the default Webfinger document if we could not find a remote Diaspora feed.\n\t\t\t# (e.g. for the \"no such doc\" error)\n\t\t\treturn res\n\n\t\t# Remove trailing quotes so that all words that contain dashes will be\n\t\t# removed.\n\t\tws_chars = re.compile(r'-\\s|\\n')\n\t\tws_chars_to_process = ws_chars.findall(handle)\n\n\t\tfor"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\ttry:\n\t\t\thandle.seek(-1024, 2)\n\t\texcept IOError:\n\t\t\treturn {}\t\t# TODO: Check if we can detect more than 64K?\n\t\tlength = handle.tell()\n\t\tlengthL = struct.unpack('>L', handle.read(16))\n\t\tlength = util.normalize_packet_length(lengthL)\n\t\tlength = int(length * 4 * 2 + 4) + 4 * (2 + length)\n\t\tdata = handle.read(length)\n\n\t\tdata = unmarshal(int, data)\n\t\tremote = data[:length *"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tdict = None\n\t\twhile True:\n\t\t\tchunk = handle.read(CHUNK_SIZE)\n\t\t\tif not chunk:\n\t\t\t\tbreak\n\t\t\tif len(chunk) == 0: # EOF\n\t\t\t\tcontinue\n\t\t\tif len(chunk) < REF_LENGTH:\n\t\t\t\tbreak\n\t\t\tid, length = chunk.split()\n\t\t\t#if id!= '1':\n\t\t\t#\tcontinue\n\n\t\t\t# FIXME: Use ID3 tags from the first page\n\t\t\t# FIXME: Use ID3 v-comment for these values\n\t\t\ttag_code = id\n\t\t\tdata = {'tag_code':"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tfile_name = handle.get_local_path()\n\t\tif filename.is_uri_remote() or file_name.split(os.sep)[-1] == \"robofab.web.harvesters.file.download\":\n\t\t\treturn_value = None\n\t\t\t# TODO: this handles the case where the object is already fetched...?\n\t\t\t#       Or the object is a single directory (i.e. a directory)\n\t\t\ttry:\n\t\t\t\tfilename = urllib2.unquote(str(handle.get_uri()))\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\t\timport"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\treturn diaspora.client.session.do_request(\"https://api.dvdn.com/webfinger/v4.0/execute?action=get&id=%s\" %(handle.get('id')), data={'handle': handle.get('id')})\n\n\"\"\"SCons.Options.DictOption\n\nThis file defines the option type for SCons implementing'set' options.\n\nIn a 'user''section', one can look like:\n  --long\n  --keyword=Eliminate keywords that match <word>\n  --keyword=Use keyword in the input file\n\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\n\t\t\n\t\t\n\ttry: # We need to check for Diaspora webfinger documents\n\t\tdocument = w3c.retrieve_w3c_document(handle)\t\n\texcept:\t\t\n\t\traise ValueError(\"Unable to retrieve %s\" % handle)\n\t\n\t\"\"\"\n\tReturn the data in the Diaspora webfinger document of the given document, represented as a tuple of two strings::\n\n\t\t\"document type\": doc type found\n\t\t\"document id\": document id found, or None\n\t\"\"\"\n\t\n\ttry:\n\t\tdocument_type = document['document type']\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\treturn_code = {}\n\t\treturn_code['success'] = False\n\t\treturn_code['message'] = \"OK\"\n\t\treturn_code['dia2d_webfinger_document'] = handle.response.read()\n\t\treturn_code['dia2d_webfinger_version'] = handle.response.getheader('version')\n\t\treturn_code['dia2d_webfinger_data'] = handle.response.read()\n\t\treturn_code['success'] = True\t\n\t\treturn return_code\n\treturn json.loads(retrieve_and_parse"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\treturn _deserialize(\"/Diaspora/Webfinger/{}?document={}\".format(_extract_name(handle.document), handle.document), handle.document)\n\"\"\"Support for Blink air systems.\"\"\"\nimport logging\n\nimport voluptuous as vol\nfrom blinkpy.exceptions import (\n    BlinkException,\n    InvalidCredentialsException,\n    SmileRemoteException,\n)\n\nfrom homeassistant.components.air_quality import PLATFORM_SCHEMA, AirQualityEntity\nfrom homeassistant.const import CONF_ACCESS_TOKEN, CONF_HOST, CONF_NAME, CONF_TOKEN\nimport homeassistant.helpers"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tlogger.info(\"Downloading Diaspora host-meta document {0}\".format(host))\n\t\treq = urllib2.Request(\"https://{0}\".format(host), None, {'User-Agent': \"Diaspora\"})\n\t\tresponse = urllib2.urlopen(req)\t\n\t\tdoc = parse_xml_(response)\n\t\treturn doc\n\n\tdef retrieve_hosts(username):\n\t\t\"\"\"\n\tGiven the username for a Diaspora user, retrieve the host-names for the current user.\n\n\t:arg username: User's username.\n\t:returns: List"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn retrieve_diaspora(host, \"hosts\")\n\n\"\"\"\nThe Host meta document element\n\"\"\"\nget_host_meta = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<host-meta xmlns=\"http://xml.vidispine.com/schema/vidispine\">\n  <meta itemprop=\"dc\">\n    <metasource>\n\"\"\"\n\n\"\"\"\nThe <content>text</content> element.\n\n:arg content: Text content\n:return: ``XRD`` instance\t\"\"\"\n\n\"\"\"\nThis will create a host"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\ttry:\n\t\t\treturn XRD(host, 'Diaspora Host Meta')\n\t\texcept NotFound:\n\t\t\traise UnknownHostError(host)\n\texcept UnknownHostError, exc:\n\t\traise UnknownHostError(host, exc)\n\t\t\t\n\t@property\n\tdef host_name(self):\n\t\t\"\"\"Get remote Diaspora host name.\"\"\"\n\t\treturn self._host\n\n\t@host_name.setter\n\tdef host_name(self, host_name):\n\t\t\"\"\"Set new remote Diaspora host name.\"\"\"\n\t\tself._host = host_name"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn connect_nexenta_with_retry(host, \"doc/DiasporaHostMeta.php/DiasporaHostMeta.php\")\n\n\tdef retrieve_diaspora_host_sys(host):\n\t\t\"\"\"\n\tRetrieve a remote Diaspora host-system document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn connect_nexenta_with_retry(host, \"doc/DiasporaHostSys.php/DiasporaHostSys.php\")\n\n\tdef retrieve_"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\n\n\tfor host_node in host.find('./{http://www.w3.org/2005/Atom}host'):\n\t\tif host_node.text and 'http://www.w3.org/2005/Atom' in host_node.text.lower():\n\t\t\treturn host_node.text.split('http://www.w3.org/2005/Atom')[1].split('}')[0].strip(' \\\" ')\n\t\n\treturn None\n\ndef retrieve_opentree_host_meta(host):\t\"\"\"\n\tRetrieve a document's opentree host-"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn _retrieve_rd()\n\ndef add_diaspora_host_meta(host):\n\t\"\"\"\n\tThis function adds the Diaspora host-meta document.\n\n:arg host: Host to add to the `diaspora-hosts`-meta document.\n:type host: :py:class:`host:`\n\t:returns: ``None``\n\t\"\"\"\n\treturn _add_diaspora_host_meta(host, diaspora_hosts_conf)\n\ndef get_host(host_name):\n\t\"\"\""}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tresponse = self._request('GET', host)\n\t\tparsed = response.splitlines()[-1]\n\t\treturn self._parse_record(parsed, host)\n\n\tdef retrieve_disambiguation_by_title(self, title):\n\t\t\"\"\"\n\tRetrieve the title of the disambiguation by performing a lookup on the database.\n\n\t:arg title: Title of the disambiguation (including the title section)\n\t:returns: Instance of ``XRDS`` or ``None`` when the title cannot be decoded or if the lookup\n\t       failed.\n\t:raises: :py:exc"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn _retrieve_service_url(host, \"service/datacenter/\" + domain_root)\n\ndef retrieve_diaspora_service_metadata(host):\t\"\"\"\n\tRetrieve a remote Diaspora service-metadata document, i.e.\n\tavailable via XDR service.\n\n\tReturns None if host does not support Diaspora.\n\t\"\"\"\n\ttry:\n\t\tservice_url = retrieve_diaspora_service_url(host, \"service/xml/config/services/service.xsd\")\n\t\tif os.path.exists(service"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn client.retrieve_meta(host)\n\ndef retrieve_diaspora_host_list(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-list document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` list\t\"\"\"\n\t\treturn client.retrieve_list(host)\n\ndef list_diaspora_host_name_servers(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-name-servers document.\n\n:arg host: Host to list from\n:returns: ``XRD``"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\n\n\n\t\t# Get host details\n\t\turl = HOST_DETAIL % host\n\t\tdata = urllib.urlopen(url).read()\n\n\t\ttry:\n\t\t\tdoc = xmltodict.parse(data)\n\t\t\thosts_info = doc.findall('host-info')\n\n\t\t\t# Get Diaspora host-id (0-indexed) information\n\t\t\tfor host_info in hosts_info:\n\n\t\t\t\tlink_xml = host_info.find('link[@href]')\n\n\t\t\t\tif link_xml is not None:\n\n\t\t\t\t\t# Fetch host info\n\t\t\t\t\thost_id = host_info."}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\twith requests.session() as s:\n\t\t\tif method == \"post\":\n\t\t\t\tr = s.post(url, data=data, timeout=timeout, **kwargs)\n\t\t\telif method == \"get\":\n\t\t\t\tr = s.get(url, timeout=timeout, **kwargs)\n\t\t\telse:\n\t\t\t\traise Exception(\"Invalid HTTP method for : {}\".format(method))\n\n\t\t\ttry:\n\t\t\t\t# Success\n\t\t\t\treturn r.status_code, r.json()\n\t\t\texcept Exception, e:\n\t\t\t\tlogger.error(e)\n\t\t\t\traise Exception(\"send_document(): An"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif not kwargs.has_key('timeout'): kwargs['timeout'] = 10\n\t\tresponse = None\n\t\ttry:\n\t\t\tresponse = urllib2.urlopen(url, data, timeout=kwargs['timeout']).read()\n\t\texcept urllib2.HTTPError, e:\n\t\t\t# HTTP error\n\t\t\tif e.code == 417:\n\t\t\t\t# Server not reachable\n\t\t\t\tif e.msg == \"502\": # Gateway timeout\n\t\t\t\t\traise GatewayTimeout(\"Gateway timeout\", e)\n\t\t\t\telse:\n\t\t\t\t\traise ServerError(*((e.msg + \" \" + str(e.code)), e."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tlogger.info(\"%s %s%s\" % (method.upper(), \"=\"*50, url))\n\n\tresult = \"No \" + method.upper() + \" found for POST\"\n\tif timeout is not None:\n\t\tresult += \" with timeout of \" + str(timeout)\n\n\tdata = data.copy() if data is not None else {}\n\n\tlogger.debug(\"%s %s\" % (method.upper(), \"=\"*50))\n\tlogger.info(method.upper())\n\n\ttry:\n\t\tresp = requests.post(url, data=data, timeout="}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif not kwargs.get('retries'):\n\t\t\tkwargs['retries'] = 10\n\t\t\n\t\tres = None\n\t\ttry:\n\t\t\tres = requests.post(url, data=data, timeout=timeout, **kwargs)\n\t\t\tres.raise_for_status()\n\t\texcept requests.exceptions.HTTPError, e:\n\t\t\tres = e\n\t\t\t# some error\n\t\t\tif res.status_code!= 404:\n\t\t\t\traise e\n\t\t\n\t\treturn res\n\texcept requests.exceptions.ConnectionError, e:    \n\t\tprint \"Connection error; giving up\"\n\t\tprint url\n\t\tprint \"Response"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\t#build POST data\n\n\t\tpost_data = urllib.urlencode(data)\n\n\t\tif isinstance(timeout, int):\n\t\t\tpost_data += \"&timeout=%d\" % (timeout,)\n\n\t\tlogging.debug(\"Request to send: %s with body %s\", post_data, data)\n\n\t\t#make request\n\t\tlogging.debug(\"Sending request...\")\n\t\tlogging.debug(\"URL: %s\", url)\n\t\tlogging.debug(\"data: %s\", data)\n\n\t\tif method == \"post\":\n\t\t\trequest = urllib2.Request(url, post_data) #"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\n\n\tconn = _create_connection(url=url, **kwargs)\n\n\tresp = None\n\tstatus = None\n\t\n\ttry:\n\t\tresp = conn.request(method, url, data, timeout, **kwargs)\n\t\tstatus = resp.status_code\n\texcept:\n\t\tresponse, status = conn.request(method, url, data, timeout, **kwargs)\n\t\tif resp is None:\n\t\t\tresp = None\n\t\telse:\n\t\t\tstatus = resp.status_code\n\t\t\tif not 200 <= status <= 299:\n\t\t\t\traise HTTPException(status, url, resp"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\n\t\n\tresponse = None\n\n\ttry:\n\t\tif method in [\"post\", \"put\", \"delete\", \"head\", \"options\", \"put\"]:\n\t\t\tdata = json.dumps(data)\n\t\t\turl = '/'.join([url, \"?signature=\" + hmac(self.api_secret, hash_method, data)])\n\texcept:\n\t\tpass\n\n\tresponse = requests.post(url, data=data, timeout=timeout, **kwargs)  #, **kwargs)  #, **kwargs)  #, **kwargs)  #, **kwargs)\n\n\treturn response.status_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\ttry:\n\t\t\tresponse = requests.post(url, data=data, timeout=timeout, **kwargs)\n\t\texcept requests.exceptions.Timeout as exc:\n\t\t\tlogging.error(exc)\n\t\t\treturn None, exc\n\t\tif response.status_code == 500: #Request has reached timeout.\n\t\t\tlogging.exception(\"Request timed out. Retrying in 5 seconds.\")\n\t\t\treturn None, None\n\t\ttry:\n\t\t\tassert response.headers['X-Sendfile'] == url\n\t\t\tlogging.debug(\"Data: %s\", data)\n\t\t\tlogging.debug(\"Response: %"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\turl = encode_multipart_formdata(url, skip_headers=True, data=urlencode(data or {}), timeout=timeout)\n\t\ttry:\n\t\t\tres = urllib2.urlopen(url, data or None)\n\t\texcept urllib2.HTTPError, e:\n\t\t\tresult = e\n\t\texcept urllib2.URLError, e:\n\t\t\tprint e\n\t\t\treturn None, str(e)\n\n\t\treturn res, result\n\n\ndef encode_multipart_formdata(fields, skip_headers=False, data=None, timeout=10, method=\"POST\", stream=False):\n\t\"\"\"\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif not isinstance(data, (bytes, bytearray)):\n\t\t\tdata = urlencode(data).encode(\"ascii\")\n\n\t\turl = urljoin_rfc(BASE_URL.rstrip(\"/\"), url)\n\t\tif not data or method in (\"GET\", \"POST\"):\n\t\t\traise HTTPException(\"Invalid method to POST to '{0}'.\".format(url))\n\n\t\tresponse = None\n\t\ttry:\n\t\t\tif method in (\"GET\", \"OPTIONS\", \"CONNECT\", \"PATCH\",):\n\t\t\t\tresponse = requests.post(url, data=data, timeout=timeout)\n\t\t\telse:\n\t\t\t\trequest_tries ="}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tfor kv in keys:\n\t\t\tdic.__setitem__(kv, val)\n\nclass dict_delete(object):\n\t\"\"\"\n\thelper class to delete dictionary\n\t\"\"\"\n\tdef __delitem__(self, key):\n\t\t\"\"\"delete a nested key\"\"\"\n\t\tdel self[key]\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"set a nested key value with a reference\"\"\"\n\t\tself[key] = dict_setitem(self, key, key, value)\n\tdef __iter__(self):\n\t\t\"\"\"iterate over children of the list\"\"\"\n\t\tfor elem"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tsub_keys = (keys,)\n\t\tkeys = _recursive_update_recursive(sub_keys, dic)\n\t\tfor t, v in zip(keys, val):\n\t\t\tdict_insert(v, t, key)\n\n\telif isinstance(val, dict) and not isinstance(val, list):\n\t\tkeys = _update_recursive(val, dic)\n\t\tfor t, d in zip(keys, val):\n\t\t\tdict_insert(d, t, key)\n\telse:\n\t\tdict_insert(val, key, val)\n##############################################################################\n# Copyright ("}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tfor key in keys:\n\t\t\tdict_insert({'dict':val, 'key':key}, str(key))\n\ndef dstack(dic):\treturn list(dic.keys()) if 'dict' in dic.keys() else [dic]\ndef dstackv(*keys):\n\t\treturn list(dic.values()) if 'dict' in dic.values() else [dic]\n\ns ='string(str(x)) for x in'str2(x, str(x)) for x in 1, 2..x, 3......x'\nprint s, dstack("}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tfor key in keys:\n\t\t\tif dic.get(key, None) is None:\n\t\t\t\tdic[key] = {}\n\t\t\tdic = dic[key]\n\n\t# check if the key is a list or tuple of keys\n\tif type(keys) is list and type(val) is tuple:\n\t\t# check for a key in val\n\t\tfor i in range(**keys) if val == '*':\n\t\t\t# recursively add the root level keys if required\n\t\t\tif len(keys)>i:\n\t\t\t\tdic[keys[i]] = {'val': 1, 'key1': keys"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif type(val) == dict:\n\t\t\treturn dict_insert(dic, val, key, *keys)\n\tif not keys:\n\t\treturn val\n\tif len(keys) == 1:\n\t\treturn dic.setdefault(keys[0], {})\n\td = dic\n\tfor t in keys:\n\t\tif key in t:\n\t\t\tif type(val) == dict:\n\t\t\t\td = dict_insert(d, val, key, *t[key])\n\t\t\telse:\n\t\t\t\td = dic.setdefault(t, {})\n\t\t\td = d[t]"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif not isinstance(key, basestring):\n\t\t\traise TypeError(\"First arg must be string\")\n\t\td = dic\n\t\tfor key in keys[:-1]:\n\t\t\td = d[key]\n\t\tif len(d):\n\t\t\td[keys[-1]] = val\n\t\telse:\n\t\t\traise ValueError(\"Inconsistent key passed: %s\" % val)\n\n\n'''\n\ndef dict_remove(dic, x):\n\treturn [x.pop(x) for x in dic.items()]\n\n\ndef check_type_of_values(dic, *keys):\n\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n        seq = [key] + list(keys)\n\n        dict_insert(seq, key, val, [])\n\n# Example:\n# >>> t.insert({'a':0, 'b':0, 'c':100})\n# >>> t['a']=\"z\"\n\n# >>> t.insert({'a':0, 'b':0, 'c':100, 'd':2})\n# >>> t['a']\n# 'z'\n\n# --------\n# list of dict methods\n# --------\n\ndef dict_keys(d):\n\t\"\"\"\n\tbuild a list"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\td=dic\n\t\tfor key in keys:\n\t\t\td=d[key]\n\t\tdict_insert(d, val, key, *keys)\n\ndef main():\n\tif len(sys.argv) > 1:\n\t\tsys.stderr.write(\"Usage Error. One or more config files must be given\\n\")\n\t\tusage()\n\t\t\n\tconfig = open(sys.argv[1], 'r')\n\t\n\tconfigs= yaml.load(config)\n\t\n\tfor config in configs.keys():\n\t\tprint (config.tag)\n\t\tif config == 'args':\n\t\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tcurr = dic\n\t\tfor k, v in keys.iteritems():\n\t\t\tif k in curr:\n\t\t\t\tcurr = curr[k]\n\t\t\telse:\n\t\t\t\tcurr[k] = {}\n\t\t\t\tcurr = curr[k]\n\t\t\tcurr = add_key_val(v, val, k)\n\t\t\tdic = curr\n\telse:\n\t\tdic[key] = val\n\t\tcurr = dic\n\treturn curr\n\n\n# return a dictionary containing all the values for a given key.  If\n# key already exists, the dictionary returns a new dict instead\ndef dict_get("}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif type(val) is dict:\n\t\t\tdic[key] = dict_insert(val, val, key, *keys)\n\t\telse:\n\t\t\tdic[key] = val\n\ndef dict_append(dic, key, val, *keys):\n\t\"\"\"\n\tappend a value of a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_append({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to append the nested key value into\n:param val: a value to append to"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tinischema = load_module(\"inischema\", settings_dirs)\n\n\t\t# initialize spec\n\t\tspec = Spec(spec_option)\n\n\t\t# get file names from inischema\n\t\toutischema_path = os.path.join(settings_dirs[\"inischema\"][\"path\"], inischema)\n\n\t\tif spec.spec_file_name:\n\t\t\toutischema_path += \"/\" + os.path.basename(spec.spec_file_name) + \"_\" + inischema\n\n\t\t# create instance and store it\n\t\tinstance = IniType(spec"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn IniType(settings_dirs, spec_option)\n\n\tdef list_of_directories(self):\n\t\t\"\"\"\n\t\tlist the directories in the format:\n\t\t[[prefix], [filename]]\n\t\t\"\"\"\n\t\treturn [prefix+self.root_fname for prefix in self.settings_dirs]\n\n\tdef create_workspace_and_build_settings(self):\n\t\t\"\"\"\n\t\tCreate the workspace and the build settings and set the settings in it.\n\n\t\tReturns a list of settings (and settings loaded)\n\t\t\"\"\"\n\t\tworkspace_type = self.settings.workspace"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\t# Create the settings directory\n\t\tself._check_set_directory(settings_dirs)\n\t\t# Generate the informative syntax tree (with all of the options set to false)\t\n\t\tinformed_syntax_tree = []\n\t\tfor settings_dir in settings_dirs:\n\t\t\tfor spec_option in spec_option:\n\t\t\t\tfor i in file_names_in_spec(settings_dir):\n\t\t\t\t\tinformed_syntax_tree.append(str(random.randint(1,len(file_names_in_spec(settings_dir))))) \n\t\t\t\tif syntax_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\n\tintype_dict = {}\n\tintype_dict['type_list'] = []\n\n\n\n\n\n\n\n\n\n##\n## Generate file_names by taking in the settings and a setting\n##\n\ndef generate_file_names(settings_dirs, settings_spec, field):\n\n\tfile_names = []\n\n\tfile_name = settings_spec['filename']\n\tfile_name_name = file_name.split('.')[0] \n\n\tif not os.path.exists(file_name_name):\n\t\n\t\t###\n\t\tprint \"ERROR: I can not find the requested file"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tif spec_option == 1:\n\t\t\t# simple list\n\t\t\tfile_list = get_inis_types(settings_dirs[0])\n\t\t#elif spec_option == 2:\n\t\telif spec_option == 3:\n\t\t\t# simple list (simple)\n\t\t\tfile_list = get_inis_types(settings_dirs[1])\n\t\t#elif spec_option == 4:\n\t\telif spec_option == 5:\n\t\t\t# simple list (complex)\n\t\t\tfile_list = get_inis_types(settings_dirs[2])\n\t\telse:"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tfile_list = []\n\t\t# first create a unique filename from every file in settings_dirs\n\t\tfor filename in os.listdir(settings_dirs):\n\t\t\tif filename!='settings.xml':\n\t\t\t\tfile_list.append(filename)\n\t\t# second create a file with the name of the input file\n\t\tfor filename in file_list:\n\t\t\tinfile=os.path.join(settings_dirs, filename)\n\t\t\tinfile_name=os.path.basename(infile)\n\t\t\tfile_path=os.path.join(settings_dirs,infile_name)\n\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn '\\n\\n'.join([\"\\t%s\" % (v[0], )  for v in settings_dirs])\n\n\tif spec_option in ('pmag_files', 'pmag_plots'):\t\treturn list_of_file_names_pmag\n\tif spec_option == 'pmag_files':\t\treturn list_of_file_names_pmag\n\tif spec_option == 'pmag_plots':\t\treturn list_of_file_names_pmag\n\t\t\n\tif spec_option =='mag_files':\n\t\tif"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n       if settings_dirs is None:\n              settings_dirs = []\n\n\n       for settings_dir in settings_dirs: # For every set of settings_dirs\n            if type(settings_dir) is str:\n\t       settings_dirs.append(settings_dir)\n       if not os.path.isdir(settings_dirs[0]) and spec_option == \"simple\":\n       #\n          # Look up the file path corresponding to a given settings_name\n            # in the input file directory that needs to be created\n            #\n          # Note that we create the file, load it\n            # and open it.\n            return"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\n\tnew_out = []\n \ttemp = settings_dirs\n \n\tlist_of_file_names = []\n \n\t#Loop through all possible combinations of input file combinations, find all combinations\n\t#Each combinations have been found, so we generate the combinations with combinations of input one at a time\n\tfor combination in itertools.combinations(list_of_file_names, spec_option):\n\t\tnew_out = []\n\n\t\n\t\tfor x in combination:\n\t\t\t#print x\n\t\t\ttemp = file_names(x, spec_option)\n\t\t\t#print temp\n\t\t\tif"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tinfo = {}\n\t\tinfo['config_path'] = settings_dirs['analysis_dir_rel_path']\n\t\tinfo['code_path'] = settings_dirs['code_dir']\n\t\tinfo['db_root'] = settings_dirs['db_root']\n\t\tinfo['input_conf_path'] = settings_dirs['input_conf_path']\n\t\tinfo['input_path'] = settings_dirs['input_path']\n\t\tinfo['output_conf_path'] = settings_dirs['output_conf_path']\n\t\tinfo['output_path'] = settings"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn CLIENT.get(\"ansible_config_manager\", {})\n\n\nclass _CliNamespace(LazyName):\n    def __init__(self, name, *args, **kwargs):\n        super(_CliNamespace, self).__init__(name)\n        self._name = name\n\n    def __repr__(self):\n        cname = self._name.lower()\n        return \"<cli.{cls_name}(version={version}, {cls_version})>\".format(\n            cls_name=cname, version=type(self).__name__,\n            version=self.version)\n\n    @classmethod\n    def _get_item_names("}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn CLI_CONFIG_MANAGERS[0] if cls.has_broker_config_manager() == True else CLI_CONFIG_MANAGERS[1]\n\t#end __init__\n\n\n#end class AnsibleConfigManager\"\"\"A module to provide help on testing pytest itself.\r\n\r\nTo use this module, put it at the top of the source tree and\r\nuse the class name of your test in a module inheriting from\r\n`pytest.main`, or from the main module.  You can also use the\r\nfunction `pytest.mark.xfail` and `pytest.mark.skipif"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._config_manager\n\n\t@staticmethod\n\tdef _load_module_file(module_name):\n\t\t\"\"\"\n\t\tLoads an ansible module from either the home or the conf dir. This method is used as part of the\n\t\tfollowing method, to load a module from within the conf dir. The method\n\t\tmay raise an exception if the module couldn't be loaded due to a missing\n\t\tmodule path, or due to a missing file path.\n\t\t:type module_name: module\n\t\t:rtype: class object\n\t\t\"\"\"\n\t\tmodule_file_path = os.path"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._config_manager\n\n\tdef __init__(self, host):\n\t\t\"\"\"\n\t\tHosts can be passed as arguments to a docker containers via the CLI or API.\n\t\tDocker is used for access to the host.\n\t\tThe return value of an instance of DockerContainers or the host_module object.\n\t\t\"\"\"\n\t\tsuper(GCPDockerManager, self).__init__(host)\n\t\tself.docker_util = GcpUtil(self, host)\n\n\t\t# TODO: This is hacky because the default implementation of the host_module object\n\t\t#      could be over"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._get_runtime_config_manager()\n\nconfiguration_manager = ConfigurationManager()\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Feb 23 10:27:06 2016\n\n@author: Jaime\n\"\"\"\nimport numpy as np\n\n\"\"\"\nGiven a list of points (and associated cellular locations), find the maximum of each\npoints in the list\n\"\"\"\n\ndef max_points(points):\n    \"\"\"Compute the max point of a list of points, i.e., their area\n    \n    Args:\n        points(list(Point)): The list"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn __salt__['config.get']('ansible_config_manager', 'get')\n\n\tdef config_file(cls):\n\t\t\"\"\"\n\t\tGets the file for the state.\n\n\t\t:return: config file\n\t\t\"\"\"\n\t\treturn __salt__['config.get']('ansible_config_manager', 'config_file')\n\n\t_instance = None\n\n\tdef __new__(cls):\n\t\t\"\"\"\n\t\tInitializes a new state object\n\n\t\t:return: a new state object\n\t\t\"\"\"\n\t\t# pylint: disable=global-statement\n\t\tif not cls._instance:\n\t\t\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._ansible_config_manager\n\n\t@classmethod\n\t@contextmanager\n\tdef _config_manager(cls):\n\t\t\"\"\"\n\t\tUsed for testing.\n\t\tShould be used by tests.\n\t\t\"\"\"\n\t\tglobal _configured_paths\n\n\t\t_configured_paths = []\n\t\tfor path, name in cls._configured_paths:\n\t\t\tpath_parts = path.split('ansible-config-manager')\n\t\t\tfull_name = path_parts[0]\n\t\t\tif full_name[0]!= '$':\n\t\t\t\tfull_name = os.path.join(sys"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._ansible_manager\n\"\"\"\r\nPURPOSE: purposes(data_path, file_type):\r\n    This file contains all functions related to functions in PURPOSE.\r\n===============================================================================\r\n\"\"\"\r\nimport json\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom math import floor\r\n\r\n# PHOTOS:\r\n\r\n#####################################################################################\r\ndef get_path_for_csv(location):\r\n\r\n    location_id_list = ['NA_L1']  # NA\r\n    index_location = [location[0], ['NA_L2"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\t# This isn't really a general-purpose function, but it's here\n\t\t# because, because we're caching the result.\n\t\tif cls._instance is None and cls._config is not None:\n\t\t\treturn cls._config_manager\n\t\ttry:\n\t\t\treturn cls._instance._config_manager\n\t\texcept AttributeError:\n\t\t\tdef _fn():\n\t\t\t\tcls._instance = ConfigManager()\n\t\t\t\treturn cls._instance\n\t\treturn _fn()\n\t@staticmethod\n\tdef _instance():\n\t\tif not hasattr(cls, \"_instance\"):\n\t\t\tcls._instance = ConfigManager()"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tif cls._conf_mgr is None:\n\t\t\tcls._conf_mgr = ConfigObj.ConfigObj(cls._config_file_path, cls._path_mgr.ext, version='')\n\t\treturn cls._conf_mgr\n\treturn None\n\n################################################################################\n# Utils\n################################################################################\n\ndef mergelists(a, b):\n\t\"\"\"Merges list a and returns those that are not in b\"\"\"\n\tres = []\n\ttry:\n\t\tfor x in a:\n\t\t\tif x not in b:\n\t\t\t\tres.append(x)\n\texcept TypeError:\n\t\tres"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tfrom.workspacereader import WorkspaceReader\n\t\tmanager = cls\n\t\tname = \"workspace_manager\"\n\t\tif manager.has_workspace(name):\n\t\t\treturn manager\n\n\t\traise RuntimeError(\"Workspace is not present.\")\n\n\t@classmethod\n\tdef workspace_state(cls):\treturn WorkspaceReader.workspace_state\n\n\t@classmethod\n\tdef unsaved_workspace_manager(cls):\treturn workspace_manager(\"unsaved_workspace_manager\")\n\n\t@classmethod\n\tdef workspace_manager_unsaved_state(cls, workspace_manager_class):\treturn workspace_manager"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls.get('workspace_manager')\n\n\n_workspace_manager = None\t# cache access\n\n\ndef init_workspace_manager(app, workspace_manager):\n\t\"\"\"\n\tInitializer\n\t:param app: The :class:`~octoprint.settings.Application` instance.\n\t:param workspace_manager:\n\t:return: The :class:`~octoprint.settings.WorkspaceManager` instance.\n\t\"\"\"\n\tglobal _workspace_manager\n\tif not workspace_manager:\n\t\tinit_workspace_manager(app, workspace_manager.__class__())\n\treturn"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn workspace_manager\n\n## Manager\n\nclass manager(object):\n\t\"\"\"\n\tA Manager that runs user commands such as /bin/ls etc.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.init_commands()\t\t\t\t\t\t\t\t# Set up commands to be executed. \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# This will be used for all commands.\n\n\t\tself._load_commands()\t\t\t\t\t\t\t\t\t# Load commands. \n\n\tdef _load_commands(self):\n\t\tif self._settings.get('user.shell', False): # Not on startup\n\t\t\tshell_"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tif not cls.WorkspaceManager:\n\t\t\traise ValueError(\"No WorkspaceManager instance!\")\n\n\t\treturn cls.WorkspaceManager\n\n\t@classmethod\n\tdef open(cls, fileName=''):\n\t\t\"\"\"\n\t\tOpens the current workspace.\n\n\t\t:param string fileName: the name of the file to open\n\t\t\"\"\"\n\t\tif fileName == '':\n\t\t\tfileName = os.path.join(\n\t\t\t\tos.path.expandvars('$HOME'),\n\t\t\t\t'WORKSPACE')\n\t\t\tws = _workspaceManager()\n\t\telse:\n\t\t\tws = cls.WorkspaceManager.get(fileName)\n\n\t\tif"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tcls.clear()\n\t\tfor workspace in cls.workspace_manager_items:\n\t\t\twidget_name = cls.workspace_manager_items_widget_dict[workspace.name]\n\t\t\tworkspace_widget = workspace.display_widget\n\t\t\tif workspace_widget is not None:\n\t\t\t\tif isinstance(workspace_widget, QtGui.QMainWindow):\n\t\t\t\t\tif widget_name not in workspace.widget(widget_name):\n\t\t\t\t\t\tworkspace.widget(widget_name).setParent(None)\n\t\t\t\t\t\tworkspace.widget(widget_name).show()\n\t\treturn cls.workspace_manager_items"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._spaces.get_item_handle(cls._space_group, cls._space_name)\n\n\t@classmethod\n\tdef create(cls):\n\t\t\"\"\"\n\t\tReturns a workspace manager.\t\n\n\t\tThis method instantiates a workspace manager. \n\t\t\"\"\"\n\t\t# Get the current class instance\n\t\tcls = workspace_manager.get(cls._space_group, cls._space_name)\t\n\n\t\t# Set the workspace\n\t\tcls._spaces[cls._space_group] = cls\n\n\t\treturn cls\n\n\tdef close(self):\n\t\t\"\"\"\n\t\tCloses"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\traise NotImplementedError( \"workspace manager: method not available to manager workspace manager\" )\n\n\nclass WorkspaceFactory(object):\n\t\"\"\"\n\tCreates a workspace class object instance from a dictionary in python that is able to load projects and\n\tprojects their contents to the project files.\t\"\"\"\n\n\tdef __init__(self, workspace_class):\n\t\tsuper(WorkspaceFactory, self).__init__()\n\t\tif 'project_files' in workspace_class:\n\t\t\tworkspace_class.update( workspace_class['project_files'] )\n\t\telse:\n\t\t\traise ConfigurationError( \"No project files"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn wstools.WorkspaceManager.Instance()\n\n\tdef is_workspace_manager_registered(cls):\t#For running wstools on Windows\n\t\treturn wstools.WorkspaceManager.Registered()\n\n\tdef is_workspace_manager_defined(cls, name):\t#To be used for wstools only\n\t\treturn wstools.WorkspaceManager.ManagerDefined(name)\n\t# TODO: add exception for missing workspace name and wrong name\n\t# TODO: add exception for duplicate workspace names (workspace names with same module name)\n\t# TODO: refactor - remove any code"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn WorkspaceManager(cls)\n\n\tdef workspace_manager_of(cls, obj):\n\t\t\"\"\"\n\t\tChecks whether the workspace manager instance is the given object.\n\t\t\"\"\"\n\t\tif hasattr(obj,'workspace_manager',None):\n\t\t\t# get from other workspace managers.\n\t\t\tworkspace_manager = obj.workspace_manager\n\t\t\tif isinstance(workspace_manager,cls):\n\t\t\t\tworkspace_manager = workspace_manager.workspace_manager\n\t\t\treturn workspaces_manager.workspace_manager_of(workspace_manager)\n\t\treturn False\n\n\n\t@classmethod\n\tdef is_"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._workspace_manager\n\n\tdef load_library(cls, name):\n\t\t\"\"\"\n\t\tLoads a specific library by name.\n\t\t`name` is the full module path including file extension.\n\t\t\"\"\"\n\t\tpackage = import_module(name)\n\t\tname_to_loader = dict_to_instance(PackageFileLoader)\n\t\tmodule_name = package.__name__\n\n\t\t# Try to get the library module class. If that doesn't work, we try defaulting to Python name.\n\t\tmodule = import_module(module_name)\n\n\t\tif not inspect."}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tif cls._plugin_manager is None:\n\t\t\tcls._plugin_manager = PluginManager()\n\t\t\n\t\treturn cls._plugin_manager\n\n\t@staticmethod\n\tdef plugin(plugin):\n\t\t\"\"\"\n\t\tReturns a Plugin instance corresponding to a CDN, or None.\n\t\t\"\"\"\n\t\tif plugin.startswith('ldap_mdns'):\n\t\t\tplugin_manager = PluginManager(plugin[4:])\n\t\telse:\n\t\t\tmodule = __import__(plugin, globals(), locals(), [], -1)\n\t\t\tfor attr_name in dir(module):\n\t\t\t\tattr_value = getattr(module,"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.manager\n\n\tdef register(self, plugin, implementation):\n\t\t\"\"\"\n\t\tRegisters a plugin with the implementation plugin using the passed in plugin.\n\t\tWe'll do all the stuff needed to communicate with the plugin.\n\t\t\"\"\"\n\n\t\tself.manager.register_plugin(plugin, implementation)\n\n\nclass PluginManager(object):\n\t\"\"\"\n\tThe plugin manager\n\tA PluginManager should inherit from this, which is responsible for reading\n\tthe plugins. It is meant to be subclassed by each PluginManager subclass.\n\t\"\"\"\n\tpass\n\n#############################################################################\n##\n##"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\timport warnings\n\t\twarnings.warn(\n\t\t\tDeprecationWarning(\"Plugins may not be loaded. Ignoring the plugin manager is strongly discouraged.\"),\n\t\t\tDeprecationWarning, stacklevel=2)\n\t\treturn cls.plugins_manager\n\n\t@property\n\tdef default_settings(self):\n\t\treturn self._settings\n\n\t@property\n\tdef settings_path(self):\n\t\t\"\"\"\n\t\tThe :term:`dotted path` to the settings folder for the plugin.\n\n\t\t.. warning::\n\t\t ~~ Please note that this method **ADVANCED** this function\n\t\t ties ``plugin_manager`` and"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\n\t\n\treturn plugin_manager.get(cls.name)\t\n\t\n\nclass Plugin(object):\n\n\t\"\"\"\n\tBase for all plugins.\t\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, id=None, info=None, session=None):\n\t\t\"\"\"\n\t\tInitializes a new plugin.\n\t\t\n\t\t``id``\n\t\t\tPlugin ID\n\t\t\n\t\t``info``\n\t\t\tName and url for the plugin, if applicable.\n\t\t\"\"\"\n\t\tself.id = id\n\t\tself.info = info\n\t\tself.session = session\n\n\t\tself._cache = {}\n\t\tself._cache"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tif not cls._plugins_manager:\n\t\t\tPluginsManager.instance = cls()\n\t\treturn cls._plugins_manager\n\nclass PluginsManagerFactory(object):\n\t\"\"\"\n\tA factory for the PluginsManager class.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._plugins_manager = PluginsManager()\n\n\tdef get_plugin_by_name(self, plugin_name, *args, **kwargs):\n\t\t\"\"\"\n\t\tRetrieves a plugin by its canonical string name.  \n\t\tIf the plugin is called with no arguments returns None.\n\t\tIf the name lookup fails"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.plugins_manager_cls(cls)\n\n\t@classmethod\n\tdef plugins_manager_cls(cls, plugin_type):\n\t\t\"\"\"\n\t\tReturns the plugin manager.\n\t\t\"\"\"\n\t\tplugin_manager = plugins[plugin_type]\n\t\tif plugin_manager == 'None':\n\t\t\treturn None\n\t\treturn cls_for_plugin_type(plugin_manager, cls)\n\n\t@classmethod\n\tdef get_plugin_manager(cls, plugin_type):\n\t\t\"\"\"\n\t\tReturns the plugin manager.\n\t\t\"\"\"\n\t\tplugin_manager = plugins_manager"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls._plugins_manager \n\n\ndef init_plugins_manager():\n    \"\"\"\n\tInitializes all registered plugins.\n\t\"\"\"\n    global plugins_manager\n    plugins_manager = PluginManager(plugins_manager)\n# encoding=utf-8\n#\n# Project:   Wiresharker Poseidon\n# Authors:    Wiresharker.ai  <idenkaton@gmail.com>\n#\n# This module is the actual language for the Lino Engineering project.\n#\n# We try to implement new, better default behaviour here and see:"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn PluginManager(plugins)\n\n\tdef run(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tRuns the plugins in the current process.\n\t\t\"\"\"\n\t\tplugins = self.plugins_manager()\n\t\tinstalled_plugins = self.installed_plugins()\n\t\tplugins.extend(installed_plugins)\n\n\t\ttry:\n\t\t\tplugins.run(*args, **kwargs)\n\t\texcept Exception, e:\n\t\t\tplugins.last_error = e\n\n\t\treturn False\n\ndef parse_args(argument_list, plugins, version_number):\n\t\"\"\"\n\tParses the arguments"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn _plugin_manager\n\n\t@classmethod\n\tdef plugin_manager(cls):\n\t\t\"\"\"\n\t\tGets the plugin manager.\t\"\"\"\n\t\treturn _plugin_manager\n\n\tdef plugin_file_data_for_display(self, display):\n\t\t\"\"\"\n\t\tReturns the plugin file (e.g. template, file name,...) for display.\n\t\t\"\"\"\n\t\treturn []\n\n\tdef plugins(self, order_by=None):\n\t\t\"\"\"\n\t\tRetrieves the enabled plugins - this will be shown all if plugins are\n\t\tinstalled or on the same page.\n\n\t\t:"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.registry.plugins\n\n\nclass Plugin(PluginBase):\n\n\t# plugin configuration\n\tPLUGIN_CONF_NAME = plugin_conf_name\n\tDEFAULT_CONFIG = None\n\n\n\tdef __init__(self):\n\t\tself._config\t= PluginConfig(self.PLUGIN_CONF_NAME)\n\n\tdef get_info(self):\n\t\treturn None\n\n\tdef get_type(self):\n\t\treturn 'plugin'\n\n\n\t@decorators.lazy()\n\tdef plugin_add_file_hook(self, file_path, callback):\n\t\tdef callback_add_file"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif not spec_content or not spec:\n\t\t\traise IRValidatorException(\"Missing content or no spec content\")\n\n\t\trequired_fields = set(spec.keys())\n\t\trequired_fields.intersection(spec_content.keys())\n\t\tspec.setdefault(\"required_fields\", [])\n\n\t\tall_fields = set()\n\t\tfor field in spec_content:\n\t\t\tvalue = spec[field]\n\t\t\tif value: all_fields.add(field)\n\n\t\trequired_fields.difference_update(all_fields)\n\n\t\t_return = {}\n\t\tfor field in required_fields"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content is None: # create empty spec and return empty\n\t\t\treturn cls()\n\t\tspec_parser = SpecfileParser()\n\t\tspec = spec_parser.parse(spec_content)\n\n\t\tkeys = spec_parser.keys()\n\t\tvals = []\n\t\tfor each_key_data in keys:\n\t\t\tvals.append(spec[each_key_data])\n\n\t\tvalidate_required_fields(spec)\n\t\tvalidate_required_fields(vals)\n\t\treturn vals\n\n\n\tdef validate_all_data(self, dct, spec_content):\n\t\t\"\"\""}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content is None or spec_content.strip() == \"\":\n\t\t\traise IRValidatorException(\"%s does not contain a valid spec!\" % spec_path)\n\t\t\n\t\t# check content type is defined (it might already be wrongly handled in __init__)\n\t\tvalidate_type(spec_content)\n\t\t\n\t\t# check spec structure definition (for more specific info about spec files)\n\t\tif SpecParser.validate_from_dict(spec_content) == None:\n\t\t\traise IRValidatorException(\"%s is missing section: %s\" % (spec_path, spec_content))"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\n\tspec = OrderedDict()\n\tif not spec_content:\n\t\tspec_content = open(spec_file, 'r').readlines()\n\n\t# parse all spec data into the content and return as a dict\n\tfor line in spec_content:\n\t\tline = line.strip()\n\t\tif len(line) == 0 or line[0] == '#':\n\t\t\tcontinue\n\t\tkey, value = line.split('=', 1)\n\t\tif key == 'data':\n\t\t\t_type, _content = value.split()\n\t\t\ttype_value = _type.strip()\n\t\t\t_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t  # content of spec file\n\tspec_content = spec_content or {}\n\tif not spec_content or not isinstance(spec_content, dict):\n\t\traise IRValidatorException('spec_content dict is missing in spec file')\n\twith open(spec_content['spec_file'], 'r') as f:\n\t\tspec_content = yaml.load(f)\n\t# load spec data\n\traw_definition = json.loads(spec_content['data'])\n\tspec = _clean_spec_structure(spec_content)\n\tspec['metadata'] = _"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\traise IRValidatorException(\"file not found!\")\n\n\n\tif spec_content:\n\t\tspec_path = spec_content.split('.')\n\t\tif not len(spec_path)==2:\n\t\t\traise IRException(\"incorrect number of fields?\" + str(len(spec_path)) + \" spec\", spec_path, len(spec_content))\n\n\t\tif not spec_path[len(spec_path)-1] == \"#\":\n\t\t\traise IRException(\"unknown or unsupported content tag '%s'\" % spec_path[len(spec_path)-1], spec_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tvalidate_yaml(spec_content)\n\n\tdef validate_spec_file_content(self, spec_file):\n\t\t# open spec file content to read content\n\t\tfile_content = load_specs_file(self, spec_file, validate=True)\n\n\t\t# get file content for validating\n\t\tvalidated_content = self.validate_content(file_content)\n\n\t\t# if the spec is correct it's valid\n\t\tif not validated_content:\n\t\t\traise IRValidatorException('spec content is empty')\n\n\t# returns: Dictionary with data loaded from an yaml file"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tlogger.info(\"running main load of %s\", spec_content)\n\t\tif not spec_content: return {}\n\t\terrors = {}\n\t\tlogger.debug(\"using spec file content %s for validation\", spec_content)\n\n\t\tdata, _ = yaml.load(spec_content, Loader=yaml.FullLoader)\n\n\telse:\n\t\terrors['spec'] = {'metadata': {'required': False, 'validator': itrvalidationvalidator.ItrValidator}},'spec_content must be present in spec file'\n\t\traise IRValidatorException(errors)\n\n\tif error_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tspecification_content = pkg_resources.resource_string(__name__,'spec.yml')\n\t\tdata = dict()\n\t\tfor spec_dict in yaml.load_all(spec_content).values():\n\t\t\tdata[spec_dict['name']] = cls.load(spec_dict)\n\t\tfor name, data_dict in data.items():\n\t\t\tif name in ['id', 'name', 'title','version']:\n\t\t\t\tcontinue\n\t\t\tif name not in cls.required_keys:\n\t\t\t\traise IRValidatorException(\"required field '%s' missing\" % name)\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\traise NotImplementedError(\"Validate from content missing in spec file.\")\n\n\n##########################################\n##    IR\n##########################################\n\n\nclass IR(object):\n\t\"\"\"\n\tiR - object for the iR specification\n\t\"\"\"\n\tdef __init__(self, spec_content):\n\t\tself.spec_content = spec_content\n\n\t@staticmethod\n\tdef loads(spec_contents):\n\t\t\"\"\"\n\t\tloads from spec file content\n\t\t\"\"\"\n\t\treturn IR(spec_contents)\n\n\t@property\n\tdef yaml_content(self):\n\t\t\"\"\"\n\t\tread yaml"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\timport yaml\n\t\timport os\n\t\timport yaml.reader\n\t\tyaml_files = [yaml_file]\n\n\t\tif cls.yaml_files!= yaml_files:\t#if we don't have the same list of given files...\n\t\t\traise IRValidatorException(\"One or more YAML files for the given version is missing!\")\n\n\n\n\t\tfor yaml_file in yaml_files:\n\t\t\tif not os.path.exists(yaml_file):\n\t\t\t\traise IRValidatorException(\n\t\t\t\t\t\"YAML file does not exist: {}\".format(yaml_file))\n\n\t\tfor yaml_file in"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\n\t\n\tif yaml_file is None:\n    \t\traise IRValidatorException(\"Empty YAML file\")\n\n    try:\n\t\twith open(yaml_file, 'r') as file_stream:\n\t\t\tyaml_data = yaml.load(file_stream)\n\texcept Exception, e:\n\t\traise IRValidatorException(\"Error while loading YAML file: %s\" % e)\n\n\tif not isinstance(yaml_data, dict):\n\t\traise IRValidatorException(\"YAML file doesn't contain required data\")\n\n\tfor key in list(yaml_data.keys()):\n\t\tif key not"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tglobal _validators\n\t\tif \"key_filename\" not in _parsers:\n\t\t\traise IRValidatorException(\"YAML file must contain key_filename entry\")\n\t\tif \"data\" not in _parsers[\"key_filename\"]:\n\t\t\traise IRValidatorException(\"YAML file must contain either key_filename or data\")\n\t\t_parsers[\"key_filename\"][\"data\"] = yaml_file\n\t\t_parsers[\"key_filename\"][\"data_order\"] = _parsers[\"key_filename\"][\"order\"]\n\t\t_parsers[\"key_filename\"][\"data_types\"] = len(\n\t\t\tlist(set("}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\ttry:\n\t\t\tyaml_dict = yaml.safe_load(open(yaml_file).read())\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException(\"Missing YAML file in file: {0}\".format(yaml_file))\n\t\t\t\n\t\t#check parameters that are present\n\t\trequired_parameters = cls._validate_required_parameters(yaml_dict)\n\t\tif required_parameters:\n\t\t\tif not cls.__check_required_parameters(required_parameters):\n\t\t\t\traise IRValidatorException(\n                    \"The following parameters are missing in file. \"\n                    \"This file is optional"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif not yaml_file:\n\t\t\traise IRValidatorException(\"Must specify input file (yaml-file)!\")\n\n\t\ttry:\n\t\t\tdata = yaml.safe_load(yaml_file, Loader=yaml.FullLoader)\n\t\texcept:\n\t\t\traise IRValidatorException(\"Invalid YAML file!\")\n\n\t\treturn data\n\n\t@classmethod\n\tdef read_config_file(cls, file_name=None):\n\t\t\"\"\"\n\t\tLoads YAML config file, checks that all fields are defined in file_name\n\tReturns a dictionary with data loaded from a config file\t\"\"\"\n\t\tdata"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\ttry:\n\t\t\tyaml_data = yaml.safe_load(yaml_file)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(\"YAML in file doesn't match the required field: {{!s}}\".format(e))\n\t\treturn validate(yaml_data, cls.required_data_keys)\n\n\t@classmethod\n\tdef _add_yaml_required_data_keys(cls):\n\t\tfor key in cls.required_data_keys_map.keys():\n\t\t\tif key not in cls.data_types_map:\n\t\t\t\traise I"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\t# check if file exists, if not raise an exception\n\t\tlogger.debug(\"validate_from_file file: {0}\".format(yaml_file))\n\t\tif not os.path.isfile(yaml_file):\n\t\t\traise IRValidatorException('YAML file {0} doesn\\'t exist'.format(yaml_file), 'YAML file not found', 'Validate data.')\n\t\t# try to validate YAML file\n\t\tparsed = yaml.safe_load(open(yaml_file, 'r'))\n\t\t# if check, the data itself\n\t\tif 'run' not in parsed:"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\n\n\tobj = yaml_file\n\tvalid = {}\n\tif yaml_file is None: raise IRValidatorException(\"YAML file must be specified to use this feature\")\n\ttry:\n\t\tobj_yaml = YAML.load(yaml_file, Loader=Loader)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"YAML file is invalid YAML: %s (%s)\" % (yaml_file, exc))\n\tif not object_is_valid(obj_yaml):\n\t\traise IRValidatorException(\"YAML file is invalid YAML: %s (%s"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\timport yaml\n\t\twith open(yaml_file, 'r', encoding=\"utf-8\") as fd:\n\t\t\tyaml_data = fd.read()\n\t\t\tyaml_data = yaml.parse(yaml_data)\n\t\t\treturn yaml_data\n\n\tdef save(self, _path, _data = None):\n\t\tpass\n#!/usr/bin/env python2.7\n# Copyright 2015, Google Inc.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n#"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\n\n\treturn get_required_fields(yaml_file)\n\t\t\t\n\t\"\"\" validate_from_file\n\t\"\"\"\n\t# check for mandatory fields\n\tmandatory_fields = []\n\ttry:\n\t\tmandatory_fields = cls.__mandatory_fields__\n\texcept AttributeError:\n\t\tmandatory_fields = []\n\tfor field in mandatory_fields:\n\t\tif not hasattr(cls, field) or \\ \n\t\t\thasattr(cls, '_required_fields__')\\\n\t\t\tand not field in cls._required_fields__:\n\t\t\traise IRValidatorException('mandatory field %s is missing' %"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tif (not parser_dict):\n\t\tparser_dict = {}\n\n\n\t\t# search for a list of files, and if a template file is specified (i.e., by one file)\n\t\tif (hasattr(self, 'rule_files')):\n\t\t\tpaths = []\n\t\t\text_files = []\n\t\t\tfor file in self.rule_files:\n\t\t\t\tif os.path.exists(file):\n\t\t\t\t\tpaths.extend(self._get_files(file, parser_dict, ext_files))\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t# search for a list of directories (no recursive rules)\n\t\t"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tself.all_groups = self.parse_group(self.spec_file)\n\t\tself.includes.extend(self._include_groups_aux(parser_dict))\n\n\tdef parse_group(self, group_spec):\n\t\t\"\"\"\n\t\tParses group spec and returns a tuple with 2 things,\n\t\t:\n\t\t(1) group_name, group_desc\n\n\tParameters\n\t----------\n\tgroup_spec: dict\n\t\tThe parsed group spec\n\tReturns\n\t-------\n\tall_groups: dict\n\t\tThe found group_name's contents and a dict containing"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\t# Parse the include file\n\t\tlines = _include_lines(parser_dict)\n\n\t\tdef _include_group(line):\n\t\t\t\"\"\"Add the include group to the lines read.\"\"\"\n\t\t\treturn len(line.include_path) + len(line.line_no) + 1\n\n\t\tlines = [_include_group(line) for line in lines]\n\n\t\tcurrent_block = None\n\t\tcurrent_include = None\n\t\tcurrent_exclude = None\n\t\tcurrent_filename = None\n\t\tcurrent_lineno = 0\n\n\t\t# Parse the current include file as a list of include lines"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\t(section, match_section) = self._create_section_definitions(parser_dict)\n\t\tsections = parser_dict.sections()\n\t\tsection.update(self._get_section_defaults(match_section, sections))\n\t\treturn _include_items(parser_dict.items(), parser_dict)\n\n\t\"\"\"\n\tParse the [d]ecfile option.\n\t\"\"\"\n\tdef _parse_d(self, parser_dict):\n\t\tif parser_dict.has_key(\"d\"):\n\t\t\tret = {}\n\t\t\tfor opt, val in parser_dict"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tinclude_dict_directive = False\n\t\tfor key in parser_dict.keys():\n\t\t\tparser_dict[key] = self._include_vars(parser_dict[key])\n\t\t\tif include_dict_directive:\n\t\t\t\tinclude_dict_directive = False\n\n\tdef _get_include_vars(self, include_dict_directive):\n\t\t\"\"\"\n\t\tProvides the include dict directive to get the includes in the spec file.\n\t\t\"\"\"\n\t\tmydefs = self.get(self.section, 'include_dict')\n\t\tmydefs_copy = deepcopy(mydefs)"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tfor include in parser_dict.get(\"include\", []):\n\t\t\tif not include in self.spec_files:\n\t\t\t\traise ValueError(\"Cannot include not in specified specification file {!r}\".format(\n\t\t\t\t                      include))\n\n\t# ----------------------------------------------------------------------------\n\tdef build(self, spec):\n\t\t\"\"\"\n\t\tBuild the spec from the provided spec.\n\n\t\tBelieve mechanisms are provided by parsing the given specification\n\t\tfile and applying them to the extension module.\n\n\t\t:param spec: The specification\n\t\t:type spec: dict\n\t\t\"\"\"\n\t\tif not (isinstance(spec, (list,"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tparser_dict['include_groups'] = self.includes\n\n\tdef _parse_file(self, filepath):\t\t# this function is not intended to be used in the\n\t\t# spec files as of now.  We only support the directives\n\t\t# section, section_name and... and a section + option section name.\n\t\tsection_name = 'UNKNOWN'\t\t\t\t# The parser is not able to parse a section.\n\t\tdirective_name = 'UNKNOWN'\n\t\toption_section_name = 'UNKNOWN'\n\t\tif ':' in filepath:\n\t\t\tfilepath_tokens = filepath."}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tinclude_list_of_patterns = self._include_groups_regex.finditer(parser_dict[\"include_list\"])\n\t\t\t\t\n\t\tfor line in include_list_of_patterns:\n\t\t\tinfo = parser_dict['include_list'][line.rstrip(os.linesep)+1:-1].split(\";\")\n\t\t\tinclude_group_name = None\n\n\t\t\ttry:\n\t\t\t\tinclude_group_name = info[1].strip(\"\\\"\")\n\t\t\texcept IndexError:\n\t\t\t\tlogger.error(\"Could not parse include line: \\\"{0}\\\"\".format(line))\n\t\t\t\n\t\t\tif include_group"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\t# find included files, from path of include file\n\t\t# must come before any include/file to be processed including also\n\t\t# including file not supplied\n\t\tdef include_recurse(node):\n\t\t\tresult = []\n\t\t\tif node.name in groupcache[self._filename]:\n\t\t\t\tresult.append(node)\n\t\t\tfor c in node:\n\t\t\t\tif c in self._incl_groups: continue\n\t\t\t\tincl_nodes = self._incl_groups[c]\n\t\t\t\tincl_filename = inc_node = nodes.section_of(c)\n\t\t\t\tfilename = self._filename\n\t\t\t"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tif parser_dict.has_key(self.include_opt):\n\t\t\thandler = self.includes_dict.get(self.include_opt, None)\n\t\t\tparser_dict[self.include_opt] = handler(parser_dict, template=self.template_data)\n\n\tdef _include_dicts(self, parser_dict):\n\t\thandler = self.includes_dict.get(self.include_opt, None)\n\t\tif handler: return handler(parser_dict, template=self.template_data)\n\t\tif not self.includes_list:\n\t\t"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tspec_defaults = {}\n\t\tfor spec in self._get_specs():\n\t\t\tspec_defaults.update(source_defaults[spec.get_name()])\n\t\treturn spec_defaults\n\nclass _SpecDict(UserDict.UserDict):\n\t\"\"\"Sub-class of UserDict that represents immutable ordered dictionaries.\"\"\"\n\n\t_allowed_keys = get_inherited_reserved_keys()\n\n\tdef __init__(self, dict={}):\n\t\tUserDict.UserDict.__init__(self)\n\t\tself._keys = {}\n\t\tfor key, value in dict.items():\n\t\t\tvalue.add(_SpecDictEntry"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\t# NOTE: This list is not currently updated (see below) as there will make a new\n\t\t#       reference cycle.\n\t\tfor k in self.params.keys:\n\t\t\tif k in self.defaults:\n\t\t\t\tcontinue\n\t\t\tif k in self.sources:\n\t\t\t\tcontinue\n\t\t\tobj = self.params[k]\n\t\t\tloc = self.sources[k]\n\t\t\tparam = self.params[k]['default']\n\t\t\tif param == None:\n\t\t\t\tcontinue\n\t\t\tdefault = param.default\n\t\t\tif param.type == int:\n\t\t\t\tif loc > len(loc"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tpass\n\n\tdef get_dep(self):\t\t\"\"\"\n\tResolve dependencies of various tasks\n\t\"\"\"\n\tfor node in self.to_nodes():\n\t\tfor task in self.deps_nodes:\n\t\t\tif len(task.inputs)!= 1 or not task.hasrun:\n\t\t\t\tcontinue\n\t\t\tif (task.inputs[0].id == node.id):\n\t\t\t\tdep_value = task.generator.bld.deps[task.inputs[0].id].variant_of\n\t\t\telse:\n\t\t\t\tdep_value = task.generator.bld.deps[node."}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\treturn {\n\t\t\t'require_package': True,\n\t\t}\n\n\tdef resolve(self, spec, kwds):\n\t\t\"\"\"\n\t\tResolves a given package.\n\t\tIn particular, this calls the 'complete_command_line_args' and ``register``\n\t\tfunction of this package's 'parse' method. Override this method if your\n\t\tpackage returns something else (i.e., there is no special\n\t\t``complete_command_line_args``).\n\t\t\"\"\"\n\n\t\tif self.spec_only_imports_are_satisfied:\n\t\t\tif not kwds:\n\t\t\t\t"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tself.compiler = self.compiler.lower()\n\t\tif self.compiler == 'c':\tself.archiver = \"g++\"\n\t\tself.is_found = False\n\t\tspecs = self.get_src_args()\t\n\t\tself.include_dirs.append(specs['include_dir'])\n\t\tself.libraries = [os.path.normpath(x) for x in specs['libraries']]\n\t\tself.library_dirs\t= [x.strip('\\\\').strip('/') for x in specs['library_dirs']]\n\t\tenv_vars=['CC','CXX','F"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tif self._spec_defaults is None:\n\t\t\tself._spec_defaults = self._default_spec_defaults.copy()\n\t\treturn self._spec_defaults\n\tdef get_options(self):\n\t\treturn self._options\n\tdef get_flags(self):\n\t\treturn self._flags\n\tdef get_deps(self):\n\t\treturn self._deps\n\tdef get_src(self):\n\t\treturn self._src\n\tdef get_bld(self):\n\t\treturn self._bld\n\tdef get_name(self):\n\t\treturn self._name\n\t"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\t# default values - not used to be in the configuration\n\t\tdefaults = {\n\t\t\t# command line arguments\n\t\t\t'loglevels': ('WARNING', 'INFO', 'ERROR', 'CRITICAL'),\n\t\t\t# log levels\n\t\t\t'llpisodes_from_defaults()\n\t\t}\n\t\tfor key in self._keys:\n\t\t\tif key not in defaults:\n\t\t\t\tdefaults[key] = self._config._get_command_line_defaults().get(key)\n\t\t# save the updated defaults\n\t\tself._config._set_command_line_defaults(defaults=defaults, command_line"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tspec_defaults = OrderedDict()\n\n\t\t# use user specified defaults from configuration file. defaults\n\t\t# may be overwritten later by the user.\n\t\tcommand = self.get_command_of_type('command for getting default values from command line')\n\t\tfor line, item in self.get_specs():\n\t\t\tif'' not in line: continue\n\t\t\tspec_defaults[line] = item[0]\n\n\t\t# combine set of default parameters and configfiles, if defined\n\t\tdefault_options = OrderedDict()\n\t\tdefaultdict = OrderedDict()\n\t\toptiondict = OrderedDict()\n\n\t\tfor"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tself.required_fields = self.resolve_fields(self.field_names)\n\t\tself.field_names = \", \".join(self.field_names) if self.field_names is not None else '\"\"'\n\t\tif self.source_type == \"file\":\n\t\t\tsource_type = self.source_type\n\t\t\tself.source_type = source_type\n\t\t\tself.source_file = self.source_obj.replace('.py', '.pyw') # no file based files\n\t\telif self.source_type == \"folder\":\n\t\t\tself.source"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tif len(self.get_parameters_stack()) > 1:\t# no point in merging - nothing to do\n\t\t\treturn\n\t\tif len(self.get_return_parameters()) > 1:\t# no point in merging - nothing to do\n\t\t\tself.warnings.append(\"Only one return parameter found\")\n\t\t\treturn\n\t\tfor i, p in enumerate(self.get_parameters_stack()):\t# find and validate arguments\n\t\t\ttry:\n\t\t\t\tfor j in self.get_return_parameters():\n\t\t\t\t\tif j not in p.get_parameters():\n\t\t\t\t\t\treturn p\n\t\t"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = {'oldarg': 'oldvalue'}\n\t\treturn deprecated_args\n\n\t@staticmethod\n\tdef from_legacy(legacy_format):\n\t\treturn LegacyOptions(legacy_format)\n\n\t@staticmethod\n\tdef from_legacy_dict(legacy_dict):\n\t\treturn LegacyOptions.process_deprecated_options(legacy_dict)\n#!/usr/bin/env python\n\n# Copyright 2003 Dave Abrahams\n# Copyright 2002, 2003, 2004 Vladimir Prus\n# Distributed under the Boost Software License, Version 1.0.\n# ("}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn dict(\n\t\t\treformat=True,\n\t\t\tversion='%(prog)s {}'.format(__version__),\n\t\t\tdeprecations=self.deprecations,\n\t\t\tdefault_section=self,\n\t\t\tsection_by_section='deprecated'\n\t\t)\n\n\t@inject\n\tdef add_options(self, parser):\n\t\tOption.registry.register(self, parser)\n\t\tOptions.registry.register(self)\n\n\tdef validate_options(self, validator):\n\t\t\"\"\"\n\t\tCheck all options are actually used before doing anything else.\n\t\tThis is done by checking the type and"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn {\"deprecated\": self.deprecated, \"removed\": self.removed,\n\t\t\t\t\"deprecated_by\": frappe.session.user}\n\n\tdef on_trash(self):\n\t\tfrappe.delete_doc(\"File\", scrub(self.name))\n\t\tfrappe.delete_doc(\"File Type\", scrub(self.name), force = self.purge_older)\n\ndef delete_file_version(path):\n\t\"\"\"\n\tDelete file version using File API.\n\t\"\"\"\n\tfrom frappe.utils.file_manager import delete_file\n\tfile_name = os"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tret = {}\n\n\t\tdef add_option(*args_with_deprecation):\n\t\t\t\tret = ret.setdefault('deprecated', [])\n\t\t\t\tret['deprecated'].extend(args_with_deprecation)\n\n\t\tfor deprecated in self.deprecated:\n\t\t\tif callable(deprecated):\n\t\t\t\tret[deprecated] = add_option(arg='run_from', action='store_true')\n\t\t\t\tret[deprecated].append(self.check_cmd('-r', deprecated, optional=True, removed=False))\n\t\t\telse:\n\t\t\t\tret['deprecated'].append(deprecated)\n\n\t\tret.update(add"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn dict(opt_test = self.opt_test, opt_foo = self.opt_foo)\n\n\t# -----------------------------------------------------------------------------\n\t# MAIN EXECUTION\n\t#------------------------------------------------------------\n\t# this function is given to the command line interface. For\n\t# documentation, check the test and see if the command-line\n\t# interface has been given a proper command. If errors are\n\t# raised, exit the test.\"\"\"\n\n\tdef check_command_line_interface(self):\n\t\t\"\"\"\n\t\tCheck whether the current test is a valid command line interface.\n\t    If errors are raised,"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = {}\n\n\tdef new_method(self):\n\n\t\tif webnotes.session.user == 'Administrator':\n\t\t\treturn None\n\t\telse:\n\t\t\traise NotImplementedError\n\t\n\tdef add_system_manager_role():\n\n\t\tfrom webnotes.utils import add_user\n\t\tadd_user(webnotes.session.user, 'Administrator')\n\n\tdef get_roles_for_system_manager():\n \t\twebnotes.permissions.add_user_role(\"System Manager\")\n\tdef add_system_manager():\n\t\tsystem_manager_role = webnotes.bean(\"System Manager\","}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn dict([\n\t\t\tdict(\n\t\t\t\taction='store_true',\n\t\t\t\thelp=\"Activate, but do not actually change options\"),\n\t\t\tdict(\n\t\t\t\taction='store_false',\n\t\t\t\tdefault=True,\n\t\t\t\thelp=\"No, don't automatically activate this command\"),\n\t\t])\n\n\tdef run_on_install(self, pkg, **kwargs):\n\t\ttry:\n\t\t\trun_setup(self.install_cmd, args=['--root=/'])\n\t\texcept InstallError:\n\t\t\treturn self.install_cmd\n\n\tdef install_cmd(self, basecmd, conf"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated = {}\n\t\tif self.deprecated is None:\n\t\t\tdeprecated = self.app._existing_app_data.default_app_data\n\t\tif isinstance(self.deprecated, dict):\n\t\t\tdeprecated.update(self.deprecated)\n\t\t\tdeleted = [d for d in self.deprecated if not d.isdigit()]\n\t\t\tfor key in deleted:\n\t\t\t\tdeprecated[key] = None\n\t\treturn deprecated\n\n\tdef get_app_path(self, app_name):\n\t\t\"\"\"\n\t\tReturns the root path of the app without the app name.\n\t\t\"\"\"\n\t\twith"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = {}\n\t\ttmp = {}\t# dict to avoid duplicate calls for self argument\n\t\tfor deprecated_opt in DEPRECATED_OPTIONS:\n\t\t\tif not deprecated_opt in tmp:\n\t\t\t\ttmp[deprecated_opt] = getattr(self, deprecated_opt)\n\n\t\treturn deprecated_args\n\n\tdef to_list(self):\n\t\treturn [self.__class__.__name__, \"option::\", \"options::\"] + self.get_deprecated_args()\n\n\tdef get_option_list(self):\n\t\treturn sorted(self.__class__.__dict__)\n\ndef deprecated(*"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tdeprecated_args['removed_from'] = self.removed.split()\n\t\tdeprecated_args['msg'] = \"Issuing removed messages below:\"\n\t\tdeprecated_args['removed_msg'] = \"\"\n\t\tdeprecated_args['alternative'] = \"\"\n\n\t\t# make global instances of deprecated classes\n\t\tfor parent in self.from_defaults or ():\n\t\t\tif isinstance(parent, basestring):\n\t\t\t\tparent = self.get(parent, {})\n\t\t\tfor deprecated_arg in parent.keys():\n\t\t\t\t# find the module path with the deprecated (for backwards compatibility"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor (key, arg_type, default) in answer_file_args.iteritems():\n\t\t\tif not key in list(cli_args):\n\t\t\t\tcli_args[key] = default\n\t\tif cli_args['deprecated'] is not None:\n\t\t\tself.warn('The following command arguments are deprecated: %s\\n' %\n\t\t\t\t(' '.join(cli_args['deprecated']),))\n\n\tdef get_default_config(self, config, section, option):\n\t\t\"\"\"\n\t\tReturns the default value for a given config section with the\n\t\tgiven name (from defaults in"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\terrors = []\n\t\tfor argument in required_to_be_provided_by_cli_arg:\n\t\t\tanswer_file_args[command_name] = argument\n\t\t\terrors += validate_arg_deprecation(argument, cli_args.get(argument))\n\t\tif errors:\n\t\t\tprint \"\\n\".join(errors)\n\t\t\tsys.exit(\"Exiting due to missing arguments\\n\")\n\n\tdef write_answer_file(command_name, answers, output_file):\n\n\t\t\"\"\"\n\t\tWrites the given answer file to the provided output file and opens the output of said answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tself.validate(cli_args)\n\t\tif not answer_file_args.get('action') == 'append' \\\n\t\t\t\tor answer_file_args.get('action') =='store' \\\n\t\t\t\tor answer_file_args.get('type') == 'import':\n\t\t\treturn\n\n\t\targs = {\n\t\t\t'dest1': answer_file_args['name'],\n\t\t\t'dest2': answer_file_args['name'],\n\t\t}\n\t\tself.parser(args)\n\n\tdef validate(self, cli_args):\n\t\t\"\"\"\n\t\tValidates"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tdeprecations = []\n\n\t\tfor key in cli_args:\n\t\t\twarning_for_flag = False\n\n\t\t\tif (key in answer_file_args):\n\t\t\t\tfile_path_absolute = os.path.abspath(\n\t\t\t\t\tos.path.join(answer_file_args[key], os.pardir))\n\t\t\t\tfile_path_relative = os.path.relpath(\n\t\t\t\t\tfile_path_absolute, os.path.normpath('/'))\n\n\t\t\t\t# ensure the script is located in the same directory as the test\n\t\t\t\t# directory\n\t\t\t\tif not os.path.isdir(file_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tmissing_args = [key for key in cli_args if key in answer_file_args]\n\t\tif missing_args:\n\t\t\tprint(\"Missing arguments on command line: \" + str(missing_args))\n\t\tif cli_args:\n\t\t\tfor argument in cli_args:\n\t\t\t\tif argument in answer_file_args and argument in answer_file_args[argument]:\n\t\t\t\t\tfile_path = answer_file_args[argument][argument]\n\t\t\t\t\tfile_extension = os.path.splitext(file_path)[1].lower()\n\t\t\t\t\tif file_extension in answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif len(answer_file_args) < 1:\n\t\t\traise CLIException(\"Missing arg: use --%s option\" % cli_args[0].args[0])\n\t\tfor arg in answer_file_args:\n\t\t\tif not isinstance(arg, dict):\n\t\t\t\traise CLIException(\"Invalid args: use --%s option for value %s in file\" % (cli_args[0].args[0], arg))\n\n\tdef parse(self, args):\n\t\t\"\"\"\n\t\tPerforms the command, parses the input to the action, and then checks if\n\t\tit"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor deprecated_arg, argument_name in cli_args.items():\n\t\t\tif arg_name in self.__deprecated_args:\n\t\t\t\tmsg = \"--\" + self.__deprecated_args[argument_name] + \" is deprecated\"\n\t\t\t\t# TODO: replace with an actual command?\n\t                msg += \" (see below)\"\n\t\t\tprint(msg)\nfrom __future__ import division\r\n\r\nimport numpy as np\r\nimport pytest\r\n\r\nfrom pandas.core.sparse.sparsetools import csr_diff\r\nfrom pandas.sparse.sputils import is_extension_array_dtype"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\t# Add to new arguments or remove from args\n\t\treplaced = vars(cli_args)\n\t\tremoved = set(get_list_diff(set(answer_file_args.keys()), set(preserved)))\n\t\tfor argfile_name in answer_file_args:\n\t\t\targ_file = answer_file_args[argfile_name]\n\t\t\t# If this is a non-file and the file does not have the requested name, use it\n\t\t\tif arg_file.isfile:\n\t\t\t\tif argfile.name in replaced:\n\t\t\t\t\tcontinue\n\t\t\t\t# If this is a"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tout = cli_args.get('deprecated', None)\n\t\tout = output_messages(out, print_type=None, encoding='utf-8')\n\t\twith open(answer_file_args['path']) as answers,\\\n\t\t\topen(answer_file_args['path'], 'w') as out_answers:\n\t\t\tout_answers.write(json.dumps(answers))\n\t\t\tif out_answers.isatty():\n\t\t\t\tprint_error(('Input answer file {0} exists and was '\n\t\t\t\t\t'{1} and was not opened properly; you can read it in text mode"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif cli_args[\"--help\"]:\n\t\t\tself._usage()\n\t\t\tsys.exit(self._help())\n\t\telif cli_args[\"--verbose\"]:\n\t\t\tself._usage()\n\t\t\tsys.exit(self._help())\n\t\telse:\n\t\t\traise IllegalInputError(\"Invalid CLI Arguments\")\n\n\tdef _usage(self):\n\t\t\"\"\"\n\t\tPrints the program usage.\n\t\t\n\t\tPrint the program usage.\n\t\t\n\t\t@return: the usage of the program\n\t\t@rtype: str\n\t\t\"\"\"\n\t\tsys.stderr.write(\"Usage: <script> <options>\\n"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tret = [i.name + ':' + f for i in self.commands]\n\t\tret.extend(command_options_dict[command_name].keys())\n\t\tret.extend(command_options_dict[command_name].values())\n\t\treturn ret\n\t\t\n\tdef get_command_dict(self):\n\t\t\"\"\"\n\t\tReturns all the command options as a dictionnary: {option: value} for command named `cli.run.example.1`,\n\t\tthis is the same as calling: {option: value}\n\t\t`command.run(123)` means that"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn command_name+\" \"\n\tdef has_subparser(self):                         return 1\n\tdef has_option(self, option_name_parts): return 1\n\tdef get_sub_options(self, options):\n        \"\"\"\n        Returns options from a given OptionParser\n        \"\"\"\n        index = 0\n        if self.has_option(options):\n            try:\n                sub_options = self.sub_options[options]\n            except KeyError:\n                sub_options = []\n                self.missing_sub_options()\n\n                self.sub_parser = None\n                return []\n\n            while index < len(sub"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\targs = self.options(command_name)\n\t\toptions=[]\n\t\tfor param in (args):\n\t\t\tparams,help = get_param_value(param)\n\t\t\toptions.append(help)\n\t\treturn options\n\n\n#!/usr/bin/env python\n# Copyright 2013 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"Generates an.isolate file that can be used as the cache for test failures.\n\nThis script is invoked from the command line by the parent"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tcmdList = []\n\t\tfor cmd in self.commands:\n\t\t\tif cmd.name == command_name:\n\t\t\t\tcmdList.append(cmd)\n\t\treturn cmdList\n\t\n\tdef set_opt_from_cli(self, option_to_set):\n\t\t\"\"\"\n\tSets the given option from a specific command\n\n:param option_to_set: the name of the option to be set to\n\"\"\"\n\t\tfor cmd in self.commands:\n\t\t\tif cmd.opt_name == option_to_set:\n\t\t\t\tcmd.opt_cmd = None\n\t\t\t\tbreak"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self.cparser.get_all(command_name)\n\n\tdef get_parser_values(self, command_name, **kwargs):\n\t\t\"\"\"\n\tThis method returns a list with all the defined option_string:val commands\n\n\tFor example, get_parser_values(self, command_name, search_key, default_opts='o', verbose=1)\n\n\tThis method will return a list with the option names, case sensitive, lower-cased, or space-separated.\n\n\tfor example:\n\n\t>>> my_parser.set_default_prompt(\"foo"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tif self.global_config is None:\n\t\t\tself.global_config = {}\t\n\t\treturn self.global_config.get(command_name, [])\n\n\tdef print_options(self, options, output=sys.stdout):\n\t\t\"\"\"\n\t\tPrints in the standard format\n\t\t\"\"\"\n\t\tif options.quiet:\n\t\t\toutput.write(self.help_message())\n\t\t\treturn\n\t\tfor o in options.opt_list:\n\t\t\toutput.write(\"  %s \" % o.strip())\n\t\t\tif not self.__is_in_global_section("}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tcmds = self._commands[command_name]\n\t\tret = []\n\t\tfor cmd_object in cmds:\n\t\t\tcommand = cmd_object[1].strip()\n\t\t\tcmd_opt = []\n\n\t\t\tif command == \"#all\" or command == \"#all-help\" and not cmd_object[2].is_help:\n\t\t\t\tcmd_opt.append(cmd_object[0])\n\t\t\t\tcontinue\n\n\t\t\tif len(cmd_opt) > 0 and cmd_opt[0]!= '':\n\t\t\t\tcmd_opt_obj = CmdOption.from_string(cmd_opt)"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tout, _ = self._run_command_with_exceptions(command_name).exit_status\n\t\tout = out[out.find('ERROR : ') + 4 :]\n\t\treturn out\n\n\tdef get_help(self, command_name):\t\"\"\"\n\tPrint help on a command name\n\n\tCommand help is an optional command to show the help on a specific\n\tcommand\n\n\tUse :py:func:`do_help` to obtain the name of the sub-command.\n\n\tThis function returns the help string\n\t\"\"\"\n\t\treturn self.do_help(command_name"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self.conn.get_commands_dict()[command_name]\n\n\tdef update(self, options=None):\n\t\t\"\"\"Update the libvirt hypervisor\"\"\"\n\t\t# Don't let it raise an exception if the name of the hypervisor\n\t\t# does not match any of the defined options\n\t\tif self.name is None:\n\t\t\traise NameError(\"Name not specified is not valid\")\n\t\tflags = []\n\t\tif options:\n\t\t\tflags = set(options)\n\t\tflags.sort()\n\t\tfor flag in flags:\n\t\t\toptions += [flag]\n\n\t\t# Update only the configuration"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tresult = command_util.run_command(command_name, ignore_status=True)\n\t\tresult_list = result.output.split('\\n') \\\n\t\t\t+ '\\n\\n'\n\t\tresult_dict = {}\n\t\tfor item in result_list:\n\t\t\tlist_item = re.split(r'\\s{2,}', result_dict[item])\n\t\t\tresult_dict[item] = list_item\n\t\treturn result_dict\n\n\t\t\n\t#\n\t# The actual command line argument\n\t#\n\t# @param argv:  The list of all"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tresult = None\n\t\tspec_name, specification_args = self._get_spec(command_name, argument_name)\n\n\t\tif not argument_name:\n\t\t\traise ValueError(spec_name + \" argument missing.\")\n\n\t\tif len(specification_args) > 0:\n\t\t\traise ValueError(spec_name + \" argument argument ambiguous.\")\n\n\t\treturn result, specification_args\n\n\tdef get_command_spec(self, command_name):\n\t\t\"\"\"\n\t\tGets the spec for a specified command name.\t`command_name` is the name of the command.\t\"\"\"\n\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self.parser.get_config().get(section=DEFAULTSECT, name=argument_name, fallback=\"Unknown option %s\" % command_name)\n\n\tdef get_command(self, command_name, argument_name):\n\t\t\"\"\"get_command will return the command line argument given a command name that was given, e.g. 'add', 'delete'.\"\"\"\n\t\tif command_name in self.commands:\n\t\t\tcmd = self.commands[command_name]\n\t\telif command_name in ('help', 'args'):\n\t\t\tcmd = 'help'\n\t\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tif self._command_has_option(command_name, argument_name):\n\t\t\tself._raise_exception(\"No such '{}'\".format(argument_name))\n\t\telse:\t\n\t\t\traise InstrumentParameterException(\"No such '{}'\".format(argument_name))\n\t\n\tdef get_active_instrument_configuration(self):\t\t\"\"\"\n\tReturns the active configuration for this instruments platform.\n\t\"\"\"\t\n\t\tret_val = self._do_cmd_resp(Command.GET_ACTIVE_CONFIGURATION_RESPONSE)\n\t\treturn {'name': ret_val['name']}\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self.config_parser.options[self.commands[command_name]].get_option_spec(\n\t\t\tself.config_parser, argument_name\n\t\t)\n\n\nclass Listener(BaseListener):\n\t\"\"\"\n\tListeners provide a way of reading from the user's configuration file.\n\t\n\tEvery listener should have a listener implementation which\n\tprovides an instance of Listener.\n\t\"\"\"\n\tdef init_plugins(self):\t\t\n\t\t\"\"\"\n\t\tIf the listener implementation has not been used by the plugin,\n\t\traise a NoDataFound.\n\t\t\"\"\""}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tfor spec in self.option_specs:\n\t\t\tif spec.name == option_name and str(parameter_name)[:-len(spec.name)] == spec.name:\n\t\t\t\treturn spec\n\t\traise KeyError(\"No such option: \" + str(option_name))\n\t\n\tdef get_command_options(self):\n\t\n\t\treturn []\t\t\t\t\t\t\t# commands that require help\n\t\n\tdef get_usage_data_description(self):\n\t\tdesc = \"\"\n\t\tfor param_name, param_spec in self.options:\n\t\t\tparam_type = getattr(self, param_name"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn get_conf().get('%s.%s'%(command_name, argument_name),None)\n\t\n\tdef set_option_specs(self, command_name, argument_list):\n\t\t\"\"\"\n\tSets the specification for the specified option list.\n\t\"\"\"\n\t\tget_conf().setdefault('%s.%s'%(command_name, argument_list[0]),'')\n\t\tget_conf().setdefault('%s.%s'%(command_name, argument_list[1]),'')\n\n\nclass SettingsWrapper(object):\n\t\n\tdef __init__(self, config):\n\t\tself.__config="}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tif command_name in self._command_names:\n\t\t\tspec = self._command_names[command_name]\n\t\telse:\n\t\t\tspec = list()\n\n\t\tfor m in handler_methods[command_name]:\t\t\t\t# Find the Command\n\n\t\t\tif m[2]!=argument_name:\n\t\t\t\tif m[1]!=argument_name:\n\t\t\t\t\tspec.append(m)\n\t\t\telse:\n\t\t\t\tspec.append(m)\n\t\t\t\n\t\t\treturn spec\n\t\n\t\n\tdef get_command(self, command_name):\n\t\tif command_name in self._command_"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self.data[command_name].get_option(argument_name)\n\n\tdef set_command_def_opts(self, command_def_opts):\n\t\t\"\"\"\n\t\tSets the argument command def options, in the order: --name @ --options @\n\t\t\"\"\"\n\t\treturn self.data[command_def_opts[0]][1].set_options(list_filter=self.filter_options)\n\n\tdef format_command_def(self, command):\n\t\t\"\"\"\n\t\tFormats command definition.\n\t\t\"\"\"\n\t\treturn self.data[command_def"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self.get_options(command_name)[argument_name]\n\n\ndef get_options(command_name):\t\n\t\"\"\"\n\tRetrieves the set of command options for the specified name.\n\t\"\"\"\n\toptions = {}\n\n\tfor name in dir(get_command_module(command_name)):\n\t\tif name.startswith(command_name + '_'):\n\t\t\toptions[name[len(command_name) + 1:]] = getattr(get_command_module(command_name), command_name)\n\n\treturn options\nimport unittest, sys\nsys.path"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tassert command_name is not None\n\t\tassert argument_name is not None\n\t\tassert isinstance(command_name, str)\n\n\t\tif command_name in Command.COMMAND_NAME_SETS:\n\t\t\treturn Command.COMMAND_NAME_SETS[command_name]\n\n\t\treturn None\n\n\tdef get_command_class(self):\n\t\tif self._command_class is None:\n\t\t\tself._command_class = Command\n\t\treturn self._command_class\n\n\tdef get_usage(self):\n\t\t\"\"\"\n\t\tReturns a usage string for the command.\n\t\t\"\"\""}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn []\n\n\tdef get_arg_names(self):\n\t\treturn self.args\n\t\n\tdef get_nargs(self):\n\t\treturn len(self.args), len(self.params)\n\t\n\tdef get_vararg(self, args):\n\t\t\"\"\"\n\tList of argument names\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg.name == arg.ident:\n\t\t\t\treturn arg\n\t\n\tdef get_index(self):\n\t\t\"\"\"\n\tReturn the index of the current block\n\t\t\"\"\"\n\t\tret = 0\n\t\ti = self.block_list"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tif len(args) == 0:\n\t\t\treturn ''\n\t\treturn args[0]\n\n\tdef show_parameters(self):\n\t\t\"\"\"\n\tprint to stdout the parameters of the command\n\t\"\"\"\n\t\tfor a_param in self.parameters:\n\t\t\tprint a_param\n\t\t\n\tdef get_name(self):\n\t\t\"\"\"\n\tprint the name of the command\n\t\t\"\"\"\n\t\treturn self.name\n\t\t\n\tdef get_helptext(self):\n\t\t\"\"\"\n\thelp text for the command\n\t\t\"\"\"\n\t\treturn self.helptext\n\t\t\n\tdef get_output_"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tself._silent_arg_list.append(args)\n\t\tif len(self._silent_arg_list) > 1:\n\t\t\treturn \"Silent Argument\"\n\t\telse:\n\t\t\treturn \"Silent Argument\"\n\n\tdef get_args_with_default(self):\n\t\t\"\"\"\n\tlist of argument names including defaults\n\n:return: list, slienced argument names\t\"\"\"\n\t\treturn list(self._args_with_default)\n\n\tdef add_argument(self, arg_name, default_arg = None):\n\t\t\"\"\"\n\tAdd argument"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent_args = []\n\t\tfor arg in args:\n\t\t\tif hasattr(arg,'silent'):\n\t\t\t\tsilent_args.append(arg)\n\t\t\telse:\n\t\t\t\tif args.find(arg) >= 0:\n\t\t\t\t\treturn args.replace(arg, '')\n\t\treturn silent_args\n\nif __name__ == '__main__':\n    pass\nfrom __future__ import unicode_literals, division, absolute_import\nfrom builtins import *  # noqa pylint: disable=unused-import, redefined-builtin\n\nimport logging\nimport sys\nfrom datetime import time\n\nfrom"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tprint(\"Warning: %d arguments were ignored...\" % len(args))\n\t\tsilent = [arg['arg'] for arg in args]\n\t\tfor arg in silent:\n\t\t\tprint(\"   -\" + arg)\n\t\treturn silent\n\n\tdef print_error(self):\n\t\t\"\"\"\n\tprint error information\n\n:return: a string (including exception message) and exit status\n\n:throws: SystemExit, Exception\n:throws: SystemExit(SystemExit(...), Error)\n:throws: SystemExit, (...), Error or Exception\n:throws: SystemExit(sys.exc_info()[1])"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent_pargs = ['idl_name', 'data_type', 'idl_version']\n\t\tsilent_pargs = self.__filter_boolean_arguments(silent_pargs)\n\t\tlist = []\n\t\tslienced_args = []\n\t\tfor arg in silent_pargs:\n\t\t\tif arg in args:\n\t\t\t\tslienced_arg = args[arg].replace('_', '-')\n\t\t\telif arg in args:\n\t\t\t\tlist += [arg]\n\t\t\telif arg in silent_pargs:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tlist.append(arg)"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn [ arg for arg in args if arg not in (\"--quiet\",\"--silent\") ]\n\tdef get_silent_args_with_forcedir(self, args, forcedir=None):\n\t\tsilent_args = self.get_silent_args(args)\n\t\tif args not in silent_args:\n\t\t\treturn args\n\t\tif os.path.isdir(silent_args[0]):\n\t\t\tsubdir = \"\"\n\t\t\tpath = os.listdir(silent_args[0])\n\t\t\tfor i in path:\n\t\t\t\tsubdir = os.path.join(silent_args["}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn map(lambda i: self.__arguments[i], [self.__arguments[\"target\"], self.__arguments[\"value\"]])\n\t\t\ndef create_parser(definition):\n\tret = CliParser()\n\n\treturn ret\n\ndef parse_args():\n\tparser = argparse.ArgumentParser(description=\"Morpher\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"verbose mode\")\n\n\targuments, args = parser.parse_known_args()\n\n\treturn list(map(lambda (key, value): arguments[key], args))\n\n\t\n"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn self.sprintf(\"%s\\n\" % (\" \".join(args)))\n\n\tdef set_loglevel(self, val):\n\t\tself.loglevel = val.upper()\n\n\tdef getLogger(self):\n\t\treturn self.logger.level\n\n\tdef get_name(self):\n\t\treturn self.name or \"\"\n\n\t@staticmethod\n\tdef log(msg, *args, **kwargs):\n\t\t\"\"\"\n\tLogger decorator\n\t\"\"\"\n\t\tdef _log(msg, *args_, **kwargs):\n\t\t\t\"\"\"\n\t# log(*args,**kwargs)\n\t\tfor"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tif self.parent.config[\"silent_args\"]:\n\t\t\treturn list(args)\n\t\telif self.parent.currentRecord:\n\t\t\treturn list(self.parent.currentRecord.getOutputData().keys())\n\t\telif self.parent.currentFunc:\n\t\t\tmsg = \"The function '%s' has been specified both of:\\n'%s' and '%s'\"\\\n\t\t\t\t% (self.parent.currentFunc.name,\n\t\t\t\t\tstr(self.parent.currentFunc),\n\t\t\t\t\tstr(self.parent.previousFunc.name))\n\t\t\tmsg += \"\\nAre you sure"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\trequired = 1\n\t\tfor arg in args:\n\t\t\tif arg not in self.opts.__dict__:\n\t\t\t\treqd = mandatory\n\t\t\t\tfor opt in self.opts:\n\t\t\t\t\treqd = reqd or getattr(opt,'required', 0)\n\t\t\t\t\tislist = getattr(self.opts, opt.name, None) or 0\n\t\t\t\t\tif required and islist: reqd = None\n\t\t\t\tif required and not reqd:\n\t\t\t\t\tmissing = [opt.name for opt in self.opts if opt.required and not reqd]\n\t\t\t\t\tif missing:\n\t\t\t\t\t\tmsg = \""}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\targs_missing = []\n\t\tfor arg in args:\n\t\t\tif not arg.required:\n\t\t\t\tmissing = False\n\t\t\telse:\n\t\t\t\tif not args_missing:\n\t\t\t\t\targs_missing = args_missing + get_missing(arg.name)\n\t\t\t\telse:\n\t\t\t\t\tmsg = \"missing required argument: %s\" % arg.name\n\t\t\t\t\tmissing = True\n\t\t\tif missing:\n\t\t\t\targs_missing.append(arg.name)\n\t\treturn args_missing\n\n\tdef validate_args_with_usage(self, req, args):\t\t\t# pylint: disable="}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor req in args:\n\t\t\tif not \"required\" in req.keys():\n\t\t\t\tprint \"Error: %s requires an argument.\" % req[\"name\"]\n\t\t\t\traise InvalidRequestError\n\n\tdef validate_and_validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if all the args are valid.\tIf invalid, log a warning and terminate.\n\t\tIf invalid, log a warning that the args are invalid.\n\t\tIf not, report a warning that the invalid argument names are invalid.\"\"\"\n\n\t\t# Check if all the arguments are valid for some argument values.\n\t\tfor req in"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif len(args) < 2:\n\t\t\traise ValueError('This utility requires at least one name.')\n\t\tfor a in args[1:]:\n\t\t\tif not is_keyword(a):\n\t\t\t\traise ValueError('Each keyword must be given once.')\n\nregister_checker(FunctionChecker.is_valid_name,\n\t\t\t CheckKeyword)\nregister_checker(FunctionChecker.is_valid_name,\n\t CheckKeyword)\nregister_checker(FunctionChecker.is_valid_name_or_keyword, \n\t OptionalReference)\n\"\"\"\nCopyright (c) 2016 Keith Sterling\n\n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif len(args)!= 2:\n\t\t\tthrow(_(\"Not given arguments.\"))\n\n\t\tfor arg in args:\n\t\t\tif arg in self.required_for:\n\t\t\t\tcontinue\n\n\t\t\tif \"required\" in self.flags.strip().lower():\n\t\t\t\tif self.required_for not in self.flags.strip().lower():\n\t\t\t\t\tthrow(_(\"Required argument not present.\"))\n\t\t\tif \"default\" not in self.flags.strip().lower():\n\t\t\t\tthrow(_(\"Required argument not present.\"))\n\n\tdef execute(self):\n\t\t\"\"\"\n\t\tExecute the command.\n\n\t\tReturns\n\t\t-------\n\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not argument_validator.validate_argument(arg):\t\t\n\t\t\t\tself.add_error(arg)\n\t\t\t\n\tdef validate_file_file(self, filename):\t\t\n\t\t\"\"\"\n\t\tValidate file with syntax highlighting settings and file type.\n\t\t\"\"\"\n\t\tself.validate_syntax_highlight(filename)\t\n\t\tfile_type = filename.split('.')\n\t\ttry:\t\t\t\t\n\t\t\twith open(filename, 'r') as f:\t\t\t\t\n\t\t\t\tcontent = f.read()\n\t\t\tself.validate_file(content)\n\t\texcept IOError"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\thas_length = []\n\t\tfor x in args:\n\t\t\tif len(x)==0: \n\t\t\t\thas_length.append(False)\n\t\t\telse:\n\t\t\t\tfor y in x:\n\t\t\t\t\tif not y in has_length:\n\t\t\t\t\t\thas_length[0] = True\n\t\treturn has_length\n\n\t@property\n\tdef name(self):\n\t\t\"\"\"\n\t\tReturn the name of the command.\n\t\t\"\"\"\n\t\treturn self.__class__.__name__# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor param in args:\n\t\t\tif not param in self.requires:\n\t\t\t\traise UsageError(\"Missing required argument: %s\" % param)\n\n\t####################\n\t# Helper methods\n\t####################\n\tdef set_vars (self, args):\n\t\t\"\"\"\n\t\tUse standard set variables for the current call - set the parameters and return\n\t\ta dict of the values keyed on one of the variables names defined in the arguments.\n\t\t\"\"\"\n\t\tfor var in vars(args).keys():\n\t\t\tself.set(var, args[var])\n\n\tdef set (self, var"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor param in self.required_parameters:\n\t\t\tif not getattr(args, param, None):\n\t\t\t\traise ImproperlyConfigured(\"Missing parameter: '%s'\" % param)\n\t\t\t\nclass BaseTestApp(App):\n\tdef __init__(self, *args, **kwargs):\n\t\tself.initialize_custom_logging()\n\t\tsuper(BaseTestApp, self).__init__(*args, **kwargs)\n\t\t\n\tdef initialize_custom_logging(self):\n\t\t# configure logging\n\t\tlogger = logging.getLogger('TestApp')\n\t\tlogger.setLevel(logging.INFO)\n\t\t_handler = logging"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tpassed = True\t\t\n\t\tfor arg in args:\n\t\t\tif self.is_required_arg(arg):\n\t\t\t\tpassed, reason = self.validator.validate_arg(self.required_args[arg])\n\t\t\t\tif not passed:\n\t\t\t\t\tprint reason\n\t\treturn passed\n\n\tdef validate_arg(self, args, arg, expected_nargs):\n\t\tpos = 0\n\t\tfor arg2, arg1 in args:\n\t\t\tif not self.is_required_arg(arg2) or self.is_required_arg(arg2):\n\t\t\t\tmissing_arg = arg"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tneeded_args = set()\n\t\tfor argument in options_spec:\n\t\t\tif argument_is_set(command_name, argument):\n\t\t\t\tneeded_args.add(argument)\n\t\treturn required_args\n\n\tdef _get_arguments(self):\n\t\tif 'optional_when' in self.options_spec:\n\t\t\tif'request' not in self.options_spec:\n\t\t\t\traise errors.UsageError(\"no \\\"required_when\\\" option specified and no'request\\'\"\n                                           \"argument given\") \n\t\telif'request' in self.options_spec:\n\t\t\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\toptions_spec = { \"required_when\" : True }\n\t\treturn options_spec\n\n\tdef validate_args(self):\n\t\tfor argument in self.args:\n\t\t\t\tif argument not in self._get_possible_arguments(\"required_when\", argument):\n\t\t\t\t\tlog.error(\"%s argument is not valid for given command\", argument)\n\t\t\t\t\traise exception.ValidationException(message=\"Argument is not valid\")\n\n\tdef execute(self):\n\t\toptions_spec, args = self._get_args(True)\n\t\tfor flag in options_spec:\n\t\t\tif not flag in"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\t_config.logger.debug(\"_get_conditionally_required_args: %s\" % (command_name))\n\t\tif not command_name in command_spec['required_when']:\n\t\t\treturn\n\t\targs = command_spec['required_when'][command_name].split()\n\t\tif'required_when' in options_spec and options_spec['required_when']:\n\t\t\trequired_when = options_spec['required_when'].split()\n\t\t\tfor name, value in required_when:\n\t\t\t\targs.append(name + '=' + value)\n\t\telse:\n\t\t\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tif command_name!= 'help':\n\t\t\tif'required_when' not in options_spec:\n\t\t\t\toptions_spec['required_when'] = 'false'\n\t\t\telse:\n\t\t\t\toptions_spec['required_when'] = 'true'\n\n\t\tif'required_when' not in options_spec:\n\t\t\treturn [], []\n\n\t\trequired_when = options_spec['required_when']\n\t\targuments = self._process_options(required_when)\n\t\tif arguments:\n\t\t\treturn arguments, []\n\n\t\treturn [], []\n\n\n\t@staticmethod\n\tdef _process_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\treturn self._get_commands_from_options(command_name, options_spec,\n\t\t\t\tTrue, False, True)\n\nclass command_get_arguments_list(Command_get_arguments):\n\t\"\"\"This class generates a list of command arguments. It is a list of tuples of (command name, value)\n\tthat will serve as input to the command. The user can use this list and the command can also be\n\tcreated as the command's name or as input from the input files. \n\t\"\"\"\n\tdef __init__(self, *args):\n\t\t\"\"\"Initializes this"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\toutput = [] # list of strings\n\t\tfor option_name in options_spec[command_name]:\n\t\t\tif self._required_when(option_name):\n\t\t\t\toutput.append(option_name + \" (required_when)\")\n\t\treturn output\n\t# end _get_conditionally_required_args\n\n\tdef _get_args(self, command_name, args_name):\n\t\targs = []\n\t\tfor option_name in options_spec[command_name]:\n\t\t\tif (self._required_when(option_name) and\n\t\t\t\t(len(args_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\targs_regex = '\\s*(--{0})'.format(' '.join(options_spec))\n\t\targs_compiled = re.compile(' +(?:'+args_regex+r')')\n\t\tmatch = args_compiled.match(command_name)\n\t\tif not match:\n\t\t\treturn None, None\n\t\treturn args_regex.groupdict()\n\t\n\tdef _format_arg(self, command, value):\n\t\t\"\"\"Formats value.\"\"\"\n\t\tcommand_re = re.compile(r'(--{0})?$'.format(command))\n\t\t\n\t\tregex_str = u'{"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\treturn ['--'+command_name] + options_spec[command_name] + \\\n\t\t\t  options_spec.get(required_when[0], [])\n\t\n\tdef required_args(self, command_name) -> _SatisfiableRequirements :\n\t\t\"\"\"\n\t\tList any arguments with ``required_when`` condition matched\n\t\tany arguments not present in ``args`` to the ``execute()`` method.\n\t\t\n\t\t:return: list, list of argument names with matched ``required_when``\n        condition\t\"\"\"\n\t\traise NotImplementedError(\"Must implement required_args() method\")\n\n\tdef execute("}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\t_args = []\n\t\t\n\t\tfor option in config['command_line_args']:\n\t\t\tif option['required_when'] == command_name:\n\t\t\t\tcontinue\n\t\t\t\n\t\t\trequired_arg_index = len(option['argument_name']) - 1\n\t\t\tif required_arg_index == 0:\n\t\t\t\tneeded_arg = ''\n\t\t\t\t# we can only decide the command\n\t\t\t\tif command_name[-1] in '-+':\n\t\t\t\t\tneeded_arg = command_name[:-1] + '*'\n\t\t\t\telse:\n\t\t\t\t\tneeded_arg = command_name + '*'\n\t\t\t\t# if the command"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\trequired_opts = []\n\t\twhile len(required_opts) < len(getattr(self,'required_when_options', [])):\n\t\t\trequired_opts.append(getattr(self,'required_when_options', []))\n\t\t\trequired_opts.append(getattr(self, 'defaults', []))\n\t\toption_names = [s for s in getattr(self,'required_when_options', []) \\\n\t\t\t\t\t if getattr(s, (option_name,)) not in required_opts]\n\t\treturn option_names\n\n\tdef match(self, command_name, arguments):\n\t"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self._max_length:\n\t\t\traise Exception(\n\t\t\t\t\"Length of %s must be <= %s\" %\n\t\t\t\t(self._description, self._max_length))\n\n# -*- coding: utf-8 -*-\n\"\"\"\nThis file is part of DXJANGO Messaging System.\n\nCopyright (C) 2013 METAJ Ness Sj\u00f8b\u00e6k\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn len([x for x in args if len(x) > 1])\n\nclass Argument:\n\t\"\"\" A argument that might need to be stored in the config.\n\t\n\tAttributes:\n\t\tvalue \t    An object holding parsed arguments.\n\t\tname\t\t\tThe short, human-readable name of the argument.\n\t\toptional\t \tWhether the argument is optional.\n\t\tdefault \tA default value if arg_name is undefined.\n\t\tminval \t    The value of the minimum value in allowed values.\n\t\tmaxval \t    The value of the maximum value in allowed values.\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > len(self._command_line_arguments):\n\t\t\tself._logger.error(\"Incorrect number of arguments. \" \\\n\t\t\t\t+ str(len(args)))\n\t\t\tself._send_and_read(\n\t\t\t\tu'{0}Arguments must be less than {1} characters.'.format(\n\t\t\t\t\tlen(self._command_line_arguments),\n\t\t\t\t\tself._command_line_arguments))\n\n\tdef _send_and_read(self, command):\n\t\tif len(command) > len(self._command_line_arguments):\n\t\t\tself._send"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor i in args:\n\t\t\tif i >= len(self.params):\n\t\t\t\tif self.default is None:\n\t\t\t\t\traise DataError(\"required\")\n\t\t\t\telse:\n\t\t\t\t\tyield len(self.params)\n\t\t\t\treturn\n\t\t\tself.params[i] = len(self.params) - 1\n\t\t\tdata['length'] = len(self.params)\n\n\tdef build_call(self, return_type, params, args, required = 0):\n\t\t\"\"\"\n\t\tBuild a call against the function with given parameters.\n\n\t\tUse the specified function to build a call, then"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tself.is_too_long = True\n\n\t\tif args is None:\n\t\t\traise TypeError(\"argument to check_length_args should be a sequence of :py:obj:`int`.\")\n\n\t\tif len(args) > 3 or args[3] is None:\n\t\t\traise ValueError(\"length must be greater than 3, but is {!r}\".format(args))\n\n\t\tfor arg in args:\n\t\t\tif not self.is_too_long:\n\t\t\t\traise ValueError(\"arguments passed to check_length_args should all be longer than {}\".format(\n\t\t\t\t\tself.length - 3))"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn len(args) < len(self._field_names) + 1\n\n\t@property\n\tdef field_names(self):\n\t\t\"\"\"\n\t\tValid column names from a query parameter for a SELECT query.\n\t\"\"\"\n\t\treturn self._field_names\n\n\t@property\n\tdef unique_fields(self):\n\t\t\"\"\"\n\t\tFieldnames that do not appear in a query, and do not fit into a UniqueConstraint.\n\t\t\"\"\"\n\t\treturn set([field for field in self._unique_fields if not isinstance(field, fields.IntegerField)])\n\n\tdef validate(self"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) >= length_min_length:\n\t\t\treturn False\n\t\tfor x in args:\n\t\t\tif len(x) > length_min_length:\n\t\t\t\treturn False\n\t\treturn True\n\n\ndef parse_string_ranges(string, length_min_length, length_max_length, length_pattern, pattern):\n\t\"\"\"\n\tParse a string with a number of range delimiters separated by a delimiter.\n\tA range delimiter is defined by the string delimiter [;].\n\tAny string beginning with *, is considered a string of one of the delimiters\n\tand all"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args)!= len(set(args)):\n\t\t\treturn True\n\n\t\tfor word in args:\n\t\t\tif len(word)!= 1 and word[0] not in self.allowed_word_chars:\n\t\t\t\treturn True\n\n\t\treturn False\n\n\tdef __validate_args_length(self, args, length=2):\n\t\t\"\"\"\n\tCheck if length of the received arguments must fit into provided length.\n\n:param args: The received arguments.\t\"\"\"\n\t\tlength = len(args)\n\t\tif length > length:\n\t\t\treturn False\n\n\t\tif length %"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not args or len(args) < 2:\n\t\t\traise ValueError(\"Invalid length value\")\n\n\tdef validate_value(self, value, length = \"1\"):\n\t\t\"\"\"\n\t\tCheck if given value is a valid value.\n\t\tif length < 0, use default length:\n\t\tIf length is less than zero, use long length: 0; otherwise, use +ve value: x.\n\t\t\"\"\"\n\n\t\tif length < 0:\n\t\t\tlength = len(str(value))\n\n\t\tlchars = [\"-\", \"x\"]\t# Long ones\n\t\tlong_value = 0\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 0 or len(args) > 32:\n\t\t\tmsg = \"%s requires at least length and can not be longer than 32\"\\\n\t\telse:\n\t\t\tmsg = \"%s requires at least length %d\"\n\t\tself.__logger.error(msg % (self.__class__.service_name, len(args)))\n\t\t\n\n\tdef validate_arg_length(self, arg):\n\t\t\"\"\"\n\t\tCheck if valid length is specified.\n\t\t@param arg: The received arguments.\n\t\t@rtype: Integer\n\t\t@return: True if valid length is specified,"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not isinstance(args, tuple):\n\t\t\tself._logger.error('validate_choices_args arguments is not a tuple')\n\n\tdef validate_choices(self, values, args):\n\t\t\"\"\"\n\t\tValidate values against specified choices. This involves\n\t\tparsing only the first non-keyword argument.\n\n\t\tThis method performs several validations against the given\n\t\tvalues against multiple given choices.\n\t\t\"\"\"\n\n\t\tif isinstance(values, basestring):\n\t\t\tvalues = [values]\n\t\tif not values:\n\t\t\treturn\n\n\t\ttry:\n\t\t\tfor value in values:\n\t\t\t\tchoice ="}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not isinstance(args, (list, tuple)):\n\t\t\targs = [args]\n\n\t\tfor arg in args:\n\t\t\tif frappe.get_meta(arg[0]).__class__.__name__ == \"ChoiceField\":\n\t\t\t\targs.append(frappe.get_meta(arg[0]))\n\t\t\telse:\n\t\t\t\tmsgprint(frappe._(\"Field This argument is not valid: {0}\").format(arg[0]), raise_exception=1)\n\n\tdef validate_required_if_duplicate(self):\n\t\tif self.fieldtype in ['Check', 'Int', 'Currency"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not args:\t# None is selected\n\t\t\treturn None\n\t\ttry:\n\t\t\tfor key in args:\n\t\t\t\tif key in self._choices:\n\t\t\t\t\tchoices = self._choices[key]\n\t\t\t\t\tif callable(choices):\n\t\t\t\t\t\treturn choices()\n\t\t\t\t\telse:\n\t\t\t\t\t\treturn choices\n\t\texcept TypeError:  # when args is not a dict\n\t\t\treturn None\n# -*- coding: utf-8 -*-\n\n#    Copyright 2017 SMACIC-FAS-HEP\n#\n#    This file is part of Wirecloud.\n#\n#    Wirecloud"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tchoices = get_valid_choices()\n\t\tif len(args) > 0:\n\t\t\tfor key, value in args:\n\t\t\t\tif key in choices:\n\t\t\t\t\treturn value\n\n\t@staticmethod\n\tdef validate_from_string(arg):\n\t\t\"\"\"Check if an arg is a valid option in string format.\n\n\t\t:param arg:\tThe option or option string.\n\t\t:returns:\tTrue if valid, False otherwise.\n\t\t\"\"\"\n\t\tif isinstance(arg, string_types):\n\t\t\tif isinstance(arg, (tuple, list)):\n\t\t\t\treturn arg in VALID_"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not isinstance(args, tuple):\n\t\t\traise InvalidData(\"Expected atleast 2 items for option '{}': '{}'\".format(args, type(args)))\n\n\tdef validate_required_args(self, args):\n\t\t\"\"\"\n\tCheck if value of required arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not isinstance(args, tuple):\n\t\t\traise InvalidData(\"Expected atleast 2 items for option '{}': '{}'\".format(args, type(args)))\n\n\tdef validate_invalid_choices(self, args):\n\t\t\"\"\""}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\t# To set default/initial value of choices,\n\t\t# convert string arguments to boolean\n\t\tif not args:\n\t\t\targs = self.valid_choices_args\n\t\tif len(args) > len(self.valid_choices_args):\n\t\t\tself.throw(\"Not enough options supplied.\")\n\t\tif args[len(args)-1] in self.valid_choices_args:\n\t\t\targs = args[1:len(args)]\n\t\treturn args\n\nclass OptionsSet(Set):\n\tdef add_to(self, *args, **kwargs): # pragma: no"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\targuments = args[1:]\n\t\t\n\t\tif strValidator.is_string(arguments[0]):\n\t\t\t# It is a string\n\t\t\ttry:\n\t\t\t\targuments[0] = str(arguments[0])\n\t\t\texcept:\n\t\t\t\traise ValueError, 'The arguments cannot be converted into a list'\n\t\telse:\n\t\t\t# It is a list\n\t\t\targuments = [arguments]\n\n\t\t# If they're all valid args, then it is a list.\n\t\tif arguments == []:\n\t\t\tpass\n\t\telse:\n\t\t\tif len(arguments) > 1:\n\t\t\t\traise ValueError, 'The"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor r in self.options:\n\t\t\tif paladin is not None and r[0] == paladin \\\n\t\t\t\tand not has_exact_or_comparable_elements(r[1]):\n\t\t\t\tr[1] = paladin\n\t\t\telif paladin == r[0]:\n\t\t\t\tr[1] = None\n\n\tdef validate_empty_choice_message(self, message, code=None):\n\t\t\"\"\"\n\t\tIf a message has a blank choice, raise a ValidationError with :class:`.ValidationError`.\n\t\t\"\"\"\n\t\tif code is"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif self.has_chosen_result() and (not args or args[0] not in self.chosen_args):\n\t\t\tself.set_chosen_result(args)\n\n\tdef validate_valid_choices(self):\n\t\t\"\"\"\n\t\tValidates that the provided arguments are within the valid\n\t\tchoices.\n\t\t\"\"\"\n\n\t\tfor i, arg in enumerate(self.chosen_args):\n\t\t\tif self.is_choice_valid(arg):\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tmsg = \"{0}: not a valid choice for {1}.\".format(i + 1"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args, (list, tuple)):\n\t\t\tpass\n\t\telif isinstance(args, str):\n\t\t\tpass\n\t\telse:\t\t\traise AssertionError(\"value argument must be list or string.\")\n\n\tdef validate_valid_values(self, data):\n\t\t\"\"\"\n\t\tCheck if the supplied value is valid for this parameter.\n\n\t\t:param data: The user data.\n\t\t\"\"\"\n\t\tif not data:\n\t\t\treturn\n\n\t\tfor id in data:\n\t\t\tif not isinstance(id, (int, long, float)):\n\t\t\t\tself.raise_error(\"Invalid value for choice"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor sgn in args:\n\t\t\tv = self.get_value(sgn, 1)\n\t\t\ttry:\n\t\t\t\tx = int(v or 1)\n\t\t\t\tif x < v:\n\t\t\t\t\traise ValueError(\"%s has invalid value: %s\" % (sgn, str(v)))\n\t\t\texcept ValueError:\n\t\t\t\traise ValueError(\"Invalid value %s for %s: %s\" % \\\n\t\t\t\t\t(v, sgn, str(v)))\n\t\treturn True\n\n\tdef validate_length(self, args, options=None):\n\t\t\"\"\"\n\t\tCheck if value of"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\targs_min = list(map(lambda x: min(int(x), self.min_value_number), args))\n\t\targs_max = list(map(lambda x: max(int(x), self.max_value_number), args))\n\t\tfor i in xrange(min(self.min_value_number, args_min[0])):\n\t\t\tif self.min_value > args_min[1]:\n\t\t\t\traise ArgumentRangeError(\"Value of %s must be greater than %s.\" % (args_min[0], args_min[1]))\n\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) == 4:\n\t\t\tmin_value = float(args[0])\n\t\t\tmax_value = float(args[1])\n\t\t\tif min_value is not None and max_value is not None:\n\t\t\t\tif min_value < 0 or min_value > max_value:\n\t\t\t\t\traise ValueError(\"Invalid value for Min/max_value: %s\" %\n\t\t\t\t\t\t self.format_value(min_value, max_value))\n\t\telse:\n\t\t\traise ValueError(\"Invalid number given.\")\n\n\tdef parse_args(self, args):\n\t\t\"\"\"\n\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args)!= len(self.min_value) or len(args)!= len(self.max_value):\n\t\t\tself._error.error_invalid('Min and Max value arguments are not the same length, please retry')\n\t\tfor i in range(len(args)):\n\t\t\tif args[i] < self.min_value[i]:\n\t\t\t\tself._error.error_invalid('Min value is less than minimum value, please retry')\n\t\t\tif args[i] > self.max_value[i]:\n\t\t\t\tself._error.error_invalid('Max"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args)!= len(self.param_args) + len(self.optional_params):\n\t\tif len(args) > len(self.param_args):\n\t\t\tlogging.error('Incorrect number of arguments in the call')\n\t\t\treturn False\n\t\tfor i in range(len(args)):\n\t\t\targ = args[i]\n\t\t\tif self.param_args[i] < arg:\n\t\t\t\tlogging.error('Invalid value of optional argument %d' % arg)\n\t\t\t\treturn False\n\t\treturn True\n\n\n\n################################################################################\n################################################################################\n################################################################################\n\nclass AbstractValue"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg.isdigit():\n\t\t\t\traise Exception(\"Invalid option '{arg0}'.\".format(arg0=arg));\n\n\t\tif len(args) < 2:\n\t\t\traise Exception(\"Invalid option '{arg0}'.\".format(arg0=args[0]))\n\ndef validate_optional_arg(self, args):\n\t\"\"\"\n\tParse a comma-separated list of values using the optional arguments of\n\tconfig.py, and extract valid values.\n\n:param args: The list of values to parse.\t\"\"\"\n\targ_str = args"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not self.parser.get(arg):\n\t\t\t\tself.set_error(arg, _(\"The argument \\\"{0}\\\" needs to be greater than the minimum value of {1}!\").format(arg, args))\n\n\tdef validate_name_length(self):\n\t\tif not self.validate_max_length(self.name):\n\t\t\tself.set_error(_(\"The name length of the name is invalid.\"))\n\n\tdef validate_email_address(self):\n\t\tif not self.validate_email_address_exists():\n\t\t\tself"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tmin_value = c_int()\n\t\tmax_value = c_int()\n\t\tfor r in args:\n\t\t\tmin_value = convert_to_float_or_int(r)\n\t\t\tmax_value = convert_to_float_or_int(r)\n\n\t\tif min_value >= max_value:\n\t\t\tself.report_error('Integer value is out of range')\n\n\tdef validate_min_value(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tminv, maxv = None, None\n\t\tfor arg in args:\n\t\t\targ = self._trim(arg)\n\t\t\tif arg[1] > arg[0]:\n\t\t\t\tminv = arg[1]\n\t\t\tif arg[1] < arg[0]:\n\t\t\t\tmaxv = arg[1]\n\t\tmin_val, max_val = 0.0, None\n\t\tfor (key, value) in self._options.items():\n\t\t\tif value > maxv or value < minv:\n\t\t\t\tmax_val = value\n\t\tif maxv is not None and"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 1:\n\t\t\treturn False\n\t\tif self.input_format.lower() == 'integer' and \\\n\t\t\t  (not self.input_format == 'decimal' or\n\t\t\t\tself.input_format == 'time'):\n\t\t\tif not self.input_format == 'decimal':\n\t\t\t\treturn False\n\t\t\tif not self.input_format == 'time':\n\t\t\t\treturn False\n\t\telif 'now' in args and args['now'] < time.time():\n\t\t\tif not self.input_format == 'time':\n\t\t\t\treturn False\n\t\telif '"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tfrom bamboo.opt.argumetrics import \\\n\t\t\tArgFlag\t\t# @UnusedImport\n\targ_type = ArgFlag(type_name, ArgFlag.BOOLEAN, 1, 0,\n\t\t\t\t\t\"Argument type of argument\")\n\targ_spec = ArgFlag(type_name, ArgFlag.COLLECTION | ArgFlag.BOOLEAN,\n\t\t\t\t\t   2, 0,\n\t\t\t\"Argument specification of argument\")\n\tself.argument_type = ArgType(self.descriptor.get_argument_syntax())\n\n\tfor i in range(len(self.argument_type.argument"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\traise NotImplementedError(\"Derived complex types have not been implemented to inherit.\")\n\n\n\tdef build_arguments_type(self, parser):\n\t\tsuper().build_arguments_type(parser)\n\t\tparser.add_argument(\"--type-name\", action='store', dest=\"type_name\", type=str, default=None, nargs=\"+\", help=\"<name>\\nDefines the complex argument type\\n \\n\\\n\t  This is a command line option that can take subcommands (e.g. --subcmd my_parser.MySubcmdArgumentType)\".split())\n\n\tdef parser_add_complex"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\ts=\"\"\"\\\n\tauto f(x) = x%(type_name)s.get%(option_name)s()\n\t\"\"\"%{'type_name':type_name, 'option_name':option_name}\n\t\treturn s\n\n\t# the command\n\telif subcommand == \"c\" or subcommand == \"f\":\n\t\treturn \\\n\tcreate_complex_argumet_type(self, subcommand, 'c', command_name, 'x', argument_type_name = \"parameter\",\targument = \"parameter\",\targument_default = argument_type_spec"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treturn self.arg_object(type_name, option_name, subcommand)\n\n\tdef generate_argparser(self, command_list, args, argparser):\n\t\t\"\"\"Build a new command line argument parser\n\t:param command_list: the command list\t(positional arguments)\n\t:param list args: list of command args to store\n\t:param argparser: the argument parser\t(positional arguments)\n\t\"\"\"\n\t\tfor command in command_list:\n\t\t\tif command[0]!= \"-\":\n\t\t\t\targparser.add_argument(command[0], nargs=\"+\","}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tif not subcommand:\n\t\t\traise RuntimeError('subcommand required, add more')\n\t\targuments = ''\n\t\tparameters = ''\n\t\tfor arg in subcommand.split(','):\n\t\t\tif re.compile(arg):\n\t\t\t\targuments += arg.strip()[1:]\n\t\t\t\tcontinue\n\t\t\tif re.compile(arg[:5]):\n\t\t\t\tparts = arg[5:].lstrip().split(' ')\n\t\t\t\tif len(parts) == len(arguments):\n\t\t\t\t\tparameters += parts[2:]\n\t\t\t\telse:\n\t\t\t\t\traise RuntimeError('invalid complex argument specified')\n\t\t\telse:\n\t\t\t\targuments +='\"command \""}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treturn create_complex_argumet(self, subcommand, type_name, option_name, spec_option)\n\n\tdef build_argumet(self, opt_level, option_name, spec_option):\n\t\t\"\"\"\n\t\tBuild an argument option.\n\t\n\t:param opt_level: integer representing how many argument to add\n\t:param option_name: name of the option created\n\t:param spec_option: option's specifications\n\t:return: the argument option instance\t\"\"\"\n\t\t\n\t\targ_t = self.build_option(opt_level, option_name"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tself._spec_option_dict = {\n\t\t\t\t\t'test_type_name':'__special__complex__{0}'.format(option_name),\n\t\t\t\t\t'test_type_name_option':'__special__{0}__'.format(option_name),\n\t\t\t\t\t'__module__':self.__module__+'.'+type_name+'.'+ type_name,\n\t\t\t\t\t'__doc__':\"\"\"this method defines the following options/attributes in module_name()\n\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: tuple of arguments\n:raises"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treturn create_command_base_class.create_complex_argumet_type(self, subcommand, type_name, option_name)\n\n\t#def create_argument(self, subcommand, identifier, argument_type_name, argument_name,\n\t#\targument_specifier_list = []):\n\t#\t\"\"\"\n\t#\tCreates an argument to be used during the build process.\n\t#\tArgument specifier format for argument type is 'optional' if unspecified.\n\t#\t<argument argument specifier list>\n\t#\t<argument specifier spec_option>\n\t#"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\t# TODO check the spec/option types\n\t\t# TODO look at the documentation for the docs\n\t\targtype_option = 'options.' + option_name\n\t\targtype = type(argtype_option)\n\t\tself.add_section_header('ARGUMENT-TYPE', argtype_option, argtype.name)\n\t\tcomplex_type = argtype()\n\n\t\targ = self.add_text(\"Complex Argument \" + type_name)\n\t\targ.add_color('PINKAGE_ARGUMENT_NAME', argtype_option, True)\n\t\targ.add_color('"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treturn create_complex_type(self._client, subcommand, option_name, spec_option)\n\n\t#######################################\n\t### \tMethod to create complex argumet classes\n\t#######################################\n\n\tdef create_complex_method(self, subcommand, option_name):\n\t\t\"\"\"\n\tBuild the complex method class\n\t\n\t:param subcommand: the command name\n\t:param option_name: the option name\n\t:return: the complex method instance\t\"\"\"\n\t\treturn create_complex_type(self._client, subcommand, option_name)\n\n\tdef create_complex"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targs = args.get(\"args\")\n\n\t\tif isinstance(args, dict):\n\t\t\tcontrol_args = args.get(\"control_args\")\n\t\t\tnested_args = args.get(\"nested_args\")\n\t\telif isinstance(args, list):\n\t\t\tcontrol_args = args[:]  # type: ignore\n\n\t\treturn {\"advanced\": (\n\t\t\t\t\"options\" in args or self._meta_options,\n\t\t\t\t\"action\": (\n\t\t\t\t\t\"set\", {\"multiple_instances\": True})\n\t\t\t\tif control_args else None,\n\t\t\t\t\"options_select\": (\n\t\t\t\t\t\"options"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tnested_args = {}\n\t\tfor t in self.nested_hooks:\n\t\t\tnested_args.setdefault(t, []).append(args[t])\n\t\tnested_args = list(nested_args.items())\n\t\tfor t, args in nested_args:\n\t\t\tif isinstance(args, dict):\n\t\t\t\targs.update(get_nested_custom_and_control_args(args[t], args[t]))\n\t\t\telif t in self.command_hooks:\n\t\t\t\targs.update(get_nested_custom_and_control_args(args[t], t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tfor keyname in args.split():\n\t\t\tkey = keyname.replace('_', '-')\n\t\t\tif key in self.custom_args and self.custom_args[keyname][0] == key:\n\t\t\t\targvalues = self.custom_args[keyname][1]\n\t\t\t\tfor argvalue in list(args):\n\t\t\t\t\tif argname in argvalues:\n\t\t\t\t\t\targvalues[argname] = str(argvalue[1])\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\targvalue = args.index(key)\n\t\t\t\texcept ValueError:\n\t\t\t\t\traise TypeError('argname', key)\n\t\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targs = args.split(';')\n\n\t\t# Set CLI and parameters\n\t\tcli = args.pop(0) if args else {}\n\t\tctrl_args = dict({})\n\t\tctrl_args[cli.upper()] = args\n\n        # Call get_command() and get_arguments()\n\t\treturn dict(\n\t\t\tcontrol_args   = ctrl_args,\n\t\t\tnested_args   = args,\n\t\t\t# Nested command\n\t\t\tcommand = cli,\n\t\t)\n\n\tdef get_arguments(self, args):\n            \"\"\"\n\t\t\tParse argument list from args, and add them to the argument list."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\t# TODO: fix this function\n\t\treturn args\n\n\tdef get_args(self, args, args_id):\n\t\targs_id = int(args_id)\n\t\tif (not args_id in self.args.keys()):\n\t\t\tlogging.error(\"Invalid id/value provided for arg {}\".format(args_id))\n\n\t\t# Make sure we don't mutate any options whose name is\n\t\t# equal to the default value.\n\t\tif ('options' not in args):\n\t\t\targs['options'] = self.options\n\t\t# TODO: better error checking - we should re"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tret = {}\n\t\tfor k, v in args[0].items():\n\t\t\t# remove empty items\n\t\t\tif v == '':\n\t\t\t\tcontinue\n\t\t\tret[k] = v\n\n\t\tret.update(self.nested_custom_and_control_args(ret))\n\n\t\treturn ret\n# -*- coding: utf-8 -*-\n\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tparams = self.get_basic_args()\n\t\tif params is not None:\n\t\t\tparams = params.copy()\n\t\treturn validate_params(params, argtype='dict')\n\telif args.startswith('$'):\n\t\tsimple_action_parser.parse_args(args, namespace=self.namespace)\n\t# if no args, try to parse all the custom options\n\telse:\n\t\t# if one of the custom options is just a tuple,\n\t\t# make it a tuple for the new options:\n\t\toptions = []\n\t\tfor i in range(len(args"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tfrom copy import deepcopy\n\tfor i, item in args.iteritems():\t\t\n\t\tkey, value = str(i + 1), str(i + 1)\n\t\t\n\t\tfor d in get_nested_custom_and_control_args(item) :\n\t\t\tif key == \"args\" : key = \"args(d=dict(str(item[0])), d.update(dict(str(item[1])),))\"\n\t\t\tif key == \"control_args\" : \n\t\t\t\tkey = \"control_args(d=dict(str(item[0])), d.update"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\t# get all nested arguments into separate dict that is passed to the\n\t\t# main Ansible playbook\n\t\ttry:\n\t\t\tif len(args) == 0:\n\t\t\t\tdict = self._parse_args_args(args)\n\t\t\t\t# parse the args list out of args_list\n\t\t\t\tcontrol_args = self._get_control_args(\n\t\t\t\t\targs,\n\t\t\t\t\tkwargs={'argument_spec': args},\n\t\t\t\t\treturn_fields=('name', 'namespace', 'fields', 'description', 'no_log', 'default_value'),\n\t\t\t\t\tsyntax=\"kwargs\",\n\t\t\t\t\taction='merge',\n\t\t\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\treturn self._nested_custom_condition2_args(args)\n\n\t@staticmethod\n\tdef _nested_custom_condition2_args(args):\n\t\tnested_args = []\n\t\tif args['action'] =='store_true': nested_args.append('--bool')\n\t\telif args['action'] == 'append': nested_args.append('--append')\n\t\telif args['action'] == 'overwrite': nested_args.append('--overwrite')\n\t\telif args['action'] == 'extend': nested_args.append('--extend')\n\t\telif args['action'] == '"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars:\n\t\t\ttry:\n\t\t\t\tif not isinstance(extra_vars, list):\n\t\t\t\t\textra_vars = [extra_vars]\n\t\t\t\tenv = self.env\n\t\t\t\tenv.update(zip(extra_vars, [vars_dict]*len(extra_vars)))\n\t\t\t\tfor x in range(len(extra_vars)):\n\t\t\t\t\tenv[extra_vars[x]] = extra_vars[x]\n\t\t\texcept Exception:\n\t\t\t\tif env.env:\n\t\t\t\t\t# only raise if any of the variables has changed\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\tfor ("}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tresult = {}\n\t\tfor k, v in vars_dict.items():\n\t\t\tresult[k] = v\n\t\tif extra_vars:\n\t\t\tfor k, v in extra_vars:\n\t\t\t\tresult[k] = v\n\t\treturn result\n\telse:\n\t\treturn vars_dict\n\ndef read_options():\n\t\"\"\"\n\tRead all options from the cmd line\n\t@rtype: dict\n\t@return: Dictionary of variables\n\t\"\"\"\n\topts = {}\n\topts['command_line_options'] = {}\n\topts['help_option_names']"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars is not None:\n\t\t\tassert isinstance(extra_vars, types.ListType)\n\t\t\tnew_extra_vars = []\n\t\t\tfor extra_var in extra_vars:\n\t\t\t\tnew_value = env.get(extra_var)\n\t\t\t\tif new_value is None:\n\t\t\tnew_extra_vars.append(extra_var)\n\t\t\tenv[name] = new_extra_vars\n\t\n\tenv.update(vars_dict)\n\n@conf\ndef CHECK_LEVELS(conf, msg):\n\tcsum = getattr(Options.options,"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor key,val in vars_dict:\n\t\t\tif isinstance(val,str):\n\t\t\t\tval = val.rstrip()\n\t\t\t\ttry:\n\t\t\t\t\tprev_index = vars_dict[key].index(val)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tprev_index = vars_dict[key].index(prev_index + 1)\n\t\t\t\tvars_dict[key] = val[prev_index:] + val[:prev_index]\n\t\t\telse:\n\t\t\t\tvars_dict[key] += val\n\ndef task_vars_from_preprocessors(task_generator):\n\t\"\"\"\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif not extra_vars: extra_vars = vars_dict.keys()\n\n\t# premerge the extra_vars dict\n\tif vars_dict:\n\t\tfor k in extra_vars:\n\t\t\tif k in vars_dict:\n\t\t\t\tenv.extra_vars.append(k)\n\t\t\t else:\n\t\t\t\tenv[k] = extra_vars[k]\n\n\n\tfor i in range(len(vars_dict)):\n\n\t\tif not isinstance(vars_dict[i], dict):\n\n\t\t\t# merge meta-vars\n\t\t\t# first merge items\n\n\t\t\tif vars_dict["}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\textra_vars = map(lambda x: x.strip(), extra_vars)\n\t\treturn merge_dicts(vars_dict, extra_vars)\n\telse:\n\t\treturn vars_dict\n\ndef load_file_strings(base_paths):\n\t\"\"\"\n\tRead in a file listing, pulling data sections, overriding data from a\n\tfile list\n\n\t:param base_path: string - list of strings\n\t:return: data structure - { \"key\": \"filename\", \"value\": [list of lines] }\n\t\"\"\"\n\tfile_lines = read_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tmerged_extra_vars = {}\n\t\tfor k in extra_vars:\n\t\t\tmerged_extra_vars[k] = vars_dict.get(k)\n\n\t\treturn merged_extra_vars\n\n\t@staticmethod\n\tdef _split_path(s):\n\t\treturn re.split(r'[/*]', s.strip())\n\n\t@staticmethod\n\tdef _get_ansible_vars_from_path(path):\n\t\twith open(path) as fh:\n\t\t\tdata = fh.read()\n\t\t\treturn data.split(\"\\n\")\n\n\t@staticmethod\n\tdef _"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tresult_dict = {}\n\t\tfor key in extra_vars or list(vars_dict.keys()):\n\t\t\tif extra_vars and key not in extra_vars:\n\t\t\t\tresult_dict[key] = vars_dict[key]\n\t\t\t\tdel extra_vars[key]\n\n\t\treturn result_dict\n\n\tdef _make_dir(self, d1, d2):\n\t\t\"\"\"Create a directory: 'd2' will be created if it doesn't exist already\n\t\tand 'd1' will be renamed\"\"\"\n\t\tif not os.path.isdir(d2):\n\t\t"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars is None or extra_vars is \"None\":\n\t\t\t# no extra_vars is also valid, simply return\n\t\t\treturn \"\"\n\n\t\tresult = ''\n\t\tfor extra_var in extra_vars:\n\t\t\tresult += extra_var +''\n\t\tresult = result.strip(\" \")\n\t\treturn result\n\n\nclass ConfigManagerBase(object):\n\t__metaclass__ = ABCMeta\n\n\tdef __init__(self, logger, settings):\n\t\tself._logger = logger\n\t\tself._settings = settings\n\n\t@abstractmethod\n\tdef create_config(self, base_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor var in chain(*extra_vars):\n\t\t\tif var in vars_dict:\n\t\t\t\tvars_dict[var] += extra_vars[var]\n\t\t\t\tdel env[var]\n\ndef get_extra_vars(env):\n\t\"\"\"\n\tGet a clean (flat) copy of ``env`` without modifying it\n\t\"\"\"\n\tenv = env[:]\n\tenv.update(EXTRA_ENV)\t# make sure to copy for non-standard env vars\n\n\tdef add_to_vars(k):\n\t\ttry:\n\t\t\tret = env[k]\n\t\texcept"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\ndef check_cli_arguments_and_config(arguments=None, config=None):\n\t'''\n\tTakes arguments or configuration files, and looks for those which\n\tcan be called by ansible_playbook() on it.\n\tThis is not implemented, but can be used by any plugins, as there are\n\tother limitations.  The idea is to be able to pass in \"extra_vars\"\n\targument, which indicates the config file(s) to check.\n\n\tAn argument may be specified, which should be:\n\t- variable substitution like 'path': { [ 'ansible-"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n    command = None\n    \n    if verbose:\n        \tlogging.basicConfig(level=logging.DEBUG)\n    elif extra_vars:\t\tlogging.getLogger(__name__).debug(\"Applying with extra vars: %s\" % extra_vars)\n        \n    # NOTE: argparse only processes --help in the context of the argument parser\n    # and doesn't require --shell\n    if '--help' in ir_plugin.cmd_args and ir_plugin.cmd_args.get('--help', False) is True or '--shell' in ir_plugin.cmd_args:\n        command = ir_plugin.cmd_args['"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\tif verbose:\n\t\tlogger = logging.getLogger('pulsar.plugins.%s' % ir_plugin)\n\telse:\n\t\tlogger = None\n\n\tlogger.debug(\"'ansible-playbook' helper script called {0} \"\n\t\t\t\t  \"with directory {1}\".format(ansible_path, ir_workspace))\n\n\tif verbose:\n\t\tstdout.write(utils.read('ansible_playbook.cli') + \"\\n\")\n\tstdout.flush()\n\n\tif verbose:\n\t\t# Print the extra vars if requested.\n\t\tlogger.debug(\"<<\")\n\t\tfor n in"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t# Ensure that the playbook is parsed:\n\t\targs = {'oar_name': playbook_path}\n\t\tif verbose is not None:\n\t\t\targs['verbosity'] = verbose\n\n\t\t# We have to do the import here:\n\t\t#\n\t\t# The plugin must be a reference to the actual Plugin class:\n\t\t#\n\t\t# A plugin is a tuple (cls, method)\n\t\t# where:\n\t\t#\t1) the class of the plugin and method is a textual string\n\t\t#\t2) the method of the plugin, the argument list\n\t\t#\t3) the"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\nclass AnsibleRunner:\n\t\"\"\"\n\tThe main ansible runner.\n\t\"\"\"\n\t\n\tdef __init__(self, name):\n\t\tself.__name = name\t\n\t\tself.__ir_workspace = ir_workspace\n\t\tself.__ir_plugin = ir_plugin\n\t\n\tdef run(self):\n\t\t\"\"\"\n\t\tRun the ansible executable.\n\t\t\n\t\tIf no args are passed in, the command-line will be used instead.\n\t\t\"\"\"\n\t\t\n\t\t# If we are running in an existing folder, use the default,\n\t\t# and the current workspace to which we should run out, and the"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\n\timport os \n\tfrom ansible.runner.action import PlaybookCLI\n\tfrom ansible.runner.connection_info import ConnectionInformation\n\tfrom ansible.executor.playbook_executor import PlaybookExecutor\n\tfrom ansible.playbook import Playbook\n\tfrom ansible.plugins.loader import add_all\n\t\n\tparser \t= ir_plugin.options_dict\n\tversion = ansible_version\n\n\tplugin_version = '.'.join(map(str, ir_plugin.version.split('.')))\n\tplugin_name \t= ir_plugin.name\n\n\t# Initialize the connection with the parameters"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\treturn None\n\nclass AnsibleRunner(Runner):\n\n\tdef __init__(self, name, module_path, test_target, args={}, verbose=False):\n\t\tself.verbose = verbose\n\t\tself._init = True\n\t\tRunner.__init__(self, name, module_path, test_target, args, verbose)\n\nclass AnsibleExecutor(Executor):\n\n\tdef __init__(self, name, module_path, test_target, args={}, verbose=False):\n\t\tself.verbose = verbose # not using verbose here. it's only good for debugging\n\t\tself._init"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\t\t\n\t\timport tempfile\n\t\timport re\n\t\timport shutil\n\n\t__salt__ = {}\n\n\tansible_args = {'ansible_connection': {'connection_user':'ansible_user',\n\t\t\t\t\t\t\t'connection_pass': 'ansible_password','port':'22'}\n\t\t\t, 'ansible_connection_options': {\n\t\t\t\t\t\t\t\t\t'connection_string' : connection,\n\t\t\t\t\t\t\t'connection_extra_options' : '', 'port' : 22,\n\t\t\t\t\t\t\t\t\t'subprocess_arguments' : '', \"playbook_name\": 'ansible_commandlines.yml'}\t\n\t\t\t\t }\n\n\tdef __getattr__(self, k"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t# Get absolute path to playbook\n\t\tAnsiblePlaybook().run(ansible_playbook, verbose, extra_vars, extra_cli_args=[playbook_path])\n\n\texcept Exception as e:\n\t\t# This is only true in order to avoid logging an error\n\t\tlogging.getLogger(\"ansible\").exception(\"Error while executing ansible playbook\")\n\t\tlogging.getLogger(\"cloudhands.task\").error(\"Exception when running ansible playbook\")\n\"\"\"This module contains helper methods for managing and modifying ``django-treebeard``\nresources, such as templates and views.\n\"\"\"\nfrom django.core.exceptions"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\n\treturn shell.pcall(\n\t\tshell.get_command('ansible-playbook', {'playbook_path': playbook_path,'verbose': verbose} if extra_vars is None else extra_vars),\n\t\tshell.get_command('ansible-playbook-runner', {'ansible_cmd': ir_plugin.config['ansible-playbook_runner']}),\n\t\tshell.get_command('ansible-playbook-callbacks', {'ansible_playbook_callbacks': ir_plugin.plugin_callbacks}),\n\t\tshell.get_command('ansible-playbook-handlers', {'ansible_yaml_parser': ir"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tprint(\"RUN playbook\")\n\t\tplaybook = AnsibleRunner(parent_build_path, ir_workspace, ir_plugin)\n\t\tplaybooks_to_update = playbook.get_playbook_changes()\n\t\tprint_playbook_diff(playbooks_to_update)\n\t\t# TODO: Check if ansible is doing a remote run\n\t\tansible_output = runner.run_playbook(vars_dict)\n\t\tplaybook.clean_up(self._get_worktree_name())\n\ndef print_playbook_diff(playbook_diff):\n\tdef get_prefix(prefix):\n\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n\n\tdef _write_plugin_info():\n\t\t\"\"\"\n\t\tWrites info on plugin installation\n\t\t\"\"\"\n\n\t\tplugin_name = vars_dict.get('name', None)\n\t\tplugin_version = vars_dict.get('version', None)\n\t\tplugin_repository = vars_dict.get('repository', None)\n\n\n\t\t# Build the name for the plugin and version\n\t\tplugin_name_parts = plugin_version.split('.')\n\t\tplugin_name_parts[1] = '.'.join(plugin_name_parts[:2])\n\n\t\t# Set the plugin name to the"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tresults = {}\n\t\tresults[\"state\"] = \"unknown\"\n\t\tresults[\"ansible_version\"] = cli_args.ansible\n\t\tresults[\"ansible_playbook\"] = ansible_version\n\t\treturn results\n\ndef run_playbook(cli_args, vars_dict, ir_workspace):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:return: ansible results\t\"\"\"\n\tresults = {}\n\tresults[\"state\"] = \"unknown\"\n\tresults[\"ansible_version\"] = cli_args."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tif os.path.isfile(ir_workspace.filename):\n\t\t\tos.remove(ir_workspace.filename)\n\t\telse:\n\t\t\t# Make a temp directory\n\t\t\tos.makedirs(ir_workspace.filename)\n\n\t\t# Create a temporary file name, and write out the params\n\t\ttemp_filename = \"{0}.new\".format(ir_plugin.type)\n\t\twith open(temp_filename, 'w+b') as outf:\n\t\t\t# Get the list of vars from the configuration for this execution\n\t\t\tvariables = vars_dict.keys()\n\t\t\tfor variable in"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# get script_data.yaml from the user's file, and return a dict of data\n\t\t# used for cli run\n\t\tscript_data = commonops.ScriptData()\n\t\tscript_data.update(vars_dict)\n\n\t\tif cli_args.vars_file_vars:\n\t\t\tscript_data.update(vars_dict, overwrite=True)\n\t\t\tfilename = os.path.dirname(cli_args.vars_file_vars).__str__()\n\t\t\tscript_data.update({'filename': filename})\n\n\t\t\tscript_data.update(vars_dict, overwrite="}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tfor key in vars_dict.keys():\n\t\t\tansible_cmd = \"ansible {} {}\".format(key, vars_dict[key])\n\t\t\tlog.debug(\"Executing command with ansible {}\".format(ansible_cmd))\n\t\t\tret = subprocess.check_call(ansible_cmd.split())\n\t\t\tif ret!= 0:\n\t\t\t\tlog.critical(\"Could not execute ansible {}.{} command '{}'\".format(\" \".join(vars_dict.keys()), key, ansible_cmd))\n\t\t\t\tlog.debug(\"Output for: \" + ansible_cmd)\n\t\t\t\tfor line in ret.output.splitlines"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t_setup_common_vars_dict(vars_dict, cli_args, ir_workspace, ir_plugin)\n\t\t\n\t\t# Setup cli client\n\t\tcli = ansible_cli.AnsibleCli(\n\t\t\thost=vars_dict[\"host\"],\n\t\t\tport=vars_dict[\"port\"],\n\t\t\tbase_api_token=vars_dict[\"base_api_token\"],\n\t\t\ttls=vars_dict[\"tls\"],\n\t\t\tendpoint=vars_dict[\"endpoint\"],\n\t\t\tapi_version=vars_dict[\"api_version\"]\n\t\t)\n\t\t\n\t\t# Validate variables passed via cli\n\t\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tret = ansible.runner.RunnerResult()\n\t\tret.step = cli_args[0]\n\t\tret.output = cli_args[1]\n\t\tret.exit_code = cli_args[7]\n\t\tret.error = cli_args[8]\n\t\tret.success = cli_args[9]\n\t\treturn ret\n\n\ttry:\n\t\tvariables = list_variables(ir_workspace)\n\t\tvariables.append('INTRASE_GIT_USER=%s' % getpass.getuser())\n\t\tvars_dict[vars_dict['_SUB_CMD']]"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# Create ansible results\n\t\tresults = execute(cli_args)\n\t\t# Get the ansible variables\n\t\tvars_results = vars_dict[\"vars_results\"]\n\t\t# Get the ansible result\n\t\tresult = manage(scope, dict(zip_file=ir_workspace.out, vars_results=vars_results, ir_plugin=ir_plugin), cwd=ir_workspace.workspace, log_file=ir_workspace.out)\n\n\t\t# Get the ansible playbook results\n\t\tif output_type == \"csv\":\n\t\t\tplaybook_results = output_format.playbook_results"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tif cli_args[1].get(\"vars_dict\"):\n\t\t\tvars_dict = _prepare_vars(cli_args[1][\"vars_dict\"])\n\n\t\tfrom ansible.runner.return_data import ReturnData\n\n\t\treturn ReturnData(vars_dict)\n\n\tfrom ansible.runner.return_data import ReturnData\n\n\treturn cli_args, ir_workspace\n\n\ndef _create_plugin_output_folder(args, ir_plugin):\n\t# Create directory for analysis\n\tp = create_directory(args, ir_plugin[\"out_dir\"])\n\n\t# Return"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor key, value in values_dict.items():\n\t\t\tself.env.set(key, value)\n\t\tself.args = values_dict\n\t#\tself.env.set(['PROMPT']) if self.prompt else None\n\n\tdef _get(self, opt, value):\n\t\tself._check_cli_args(opt)\n\t\tif opt == '--help':\n\t\t\tself.parser.prog=sys.argv[0]+' --help'\n\t\t\tprint \"Usage: virsh %s <options>\" % self.args\n\t\telif opt == '--xml':\n\t\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = dict(values_dict)\n        for key, value in six.iteritems(values_dict):\n            if value!= None and value!= '':\n                self._convert_value(parser_name, key, value)\n\n    def _convert_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying args listparam.\n\nParameters passed to a virsh commands can be retrieved by parsing values_dict param.\nIf no argument is provided or the value_dict is not a dict or a list, the\nargs list will be set"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\ttry:\n\t\t\tvalues = values_dict.values()\n\t\texcept AttributeError:\n\t\t\tvalues = values_dict.values()\n\t\tfor key, value in values:\n\t\t\tself._args_dict[key] = value or \"\"\n\n\tdef parse_args(self):\n\t\t\"\"\"\n\t\tParse the command line arguments using OptionParser.\n\n\t\tOverride to do something useful.\n\n\t\tReturns: optparse.Values instance\n\t\t\"\"\"\n\t\treturn self._parse_common_options(sys.argv)\n\n\tdef _parse_common_options(self, option_argv):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\targs_dict = values_dict.copy()\n\t\tdel args_dict[\"virsh_exec\"]\n\t\tdel args_dict[\"virsh_opt\"]\n\t\tret = []\n\t\tvirsh_options = [[], [\"--connect=/path\"]]\n\t\tvirsh_cmd = [\"virsh\", \"-v\", \"--connect=/path\"]\n\t\tfor arg in self.__cli_args:\n\t\t\tvirsh_key, _arg = arg.split(\"=\", 1)\n\t\t\tvirsh_opts = [s.split(\"=\") for s in virsh_opts]\n\t\t\tdel args[:]\n\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = self._set_config_values(values_dict)\n\t\tself.log.debug(\"value_dict: \" + str(values_dict))\n\t\treturn values_dict\n\n\tdef _set_cli_options(self, opt_list, values_dict):\n\t\tself.log.cli_opt(opt_list)\n\n\t\tfor opt in opt_list:\n\t\t\tvalues_list = values_dict.get_list(opt)\n\t\t\tself._remove_config_data(values_list, op = None)\n\n\tdef parse_cli_args"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = {}\n\t\tfor arg in values_dict.keys():\n\t\t\tif hasattr(values_dict[arg], '__call__'):\n\t\t\t\targs, kargs = values_dict[arg]\n\t\t\t\t\n\t\t\t\tkwargs = {}\n\t\t\t\tfor k in self.keys():\t# Loop over all arg attributes in self\n\t\t\t\t\tkwargs[k] = values_dict[arg]\t\t\n\n\t\t\t\targs, kargs = self.get_parser_kw_args(parser_name, args, kargs)\n\t\t\t\tif args or kwargs:\n\t\t\t\t\tif len(args) == 1 and type(args"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tif values_dict is None:\n\t\t\tpass\n\t\telif isinstance(values_dict, types.StringType):\n\t\t\tself.arguments.append(\n\t\t\t\tself._split_cli_args(*values_dict[1:]))\n\t\telse:\n\t\t\traise ValueError(\"The value %s has incorrect type\" % values_dict)\n\n\tdef _get_command_args(self, description=None, skip_name=None, **options):\n\t\tret = Command.get_command_args(self, description=description,\n\t\t\t\t\t\t\t options=options)\n\t\tif skip_name is not None:"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\targ_parser = argparse.ArgumentParser()\n\t\targ_parser.add_argument(\n\t\t\t'-o', '--output-params',\n\t\t\ttype=utils.parse_name_list(values_dict),\n\t\t\tnargs='+',\n\t\t\tdefault=[],\n\t\t\thelp=\"additional output params\",\n\t\t)\n\t\targs = arg_parser.parse_args()\n\t\tif args.output_params:\n\t\t\tif len(args.output_params)!= len(values_dict):\n\t\t\t\traise errors.Error(\"output parameters count mismatch with values\")\n\t\t\tfor o_param, o_val"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor arg in values_dict.keys():\n\t\t\tconvert_arg(parser_name, arg, values_dict[arg])\n\t\t\t\t\n\tdef _convert_args1(self, opts, values_dict):\n\t\tfor key, opt in opts:\n\t\t\tif not opt.name:\n\t\t\t\topts[key] = self._long_opt.get(opt, self._long_opt['--%s' % opt])\n\t\tvalues_dict.pop('name', None)\n\t\treturn optparse.Values(opts)\n\n\tdef _convert_args2(self, opts, values"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = convert_args_to_dict(values_dict)\n\t\tfor arg in values_dict:\n\t\t\tif arg in self._boolean_states:\n\t\t\t\tself.params[arg] = 'on'\n\t\t\t\tself.passthrough.append(arg)\n\t\t\telif arg in self._int_states:\n\t\t\t\tself.params[arg] = 'on'\n\t\t\t\tself.passthrough.append(arg)\n\t\t\telif arg in self._float_states:\n\t\t\t\tself.params[arg] = 'on'\n\t\t\t\tself.passthrough.append(arg)\n\t\t\telif"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\t# TODO: Maybe better if no metadata in the same directory as plugin.py.\n\t\t# Currently this method will never be used unless some of the metadata in the same dir have been manually registered.\n\t\t# FIXME: This is probably safe if we could find a better way instead of hard coding all the metadata.\n\t\t# If we want to fix this we could move to a new flat structure for our own plugin.\n\t\t# I think I'll keep this separate code below as it is.\n\t\t# Some of the plugin types may define additional metadata as well, like\n\t\t# the plugin class, or"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tfrom octoprint.util import dict_merge\n\tfrom collections import defaultdict\n\tdef flatten_dict(d, parent_key=''):\n\t\t\"\"\"\n\t\tRecursively flatten dict and list (recursive)\n\n\t\tThis is a recursive function, so we ignore the root for invalid json\n\n\t\t>>> class MyPlugin(OctoPrintPlugin):\n\t\t...\tdef get_entry_map(self, group, name):\n\t\t...\t\tg.get_group_dict('entry1', group is None)\n\t\t...\t\tg.get_group_dict('entry2', group is '"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tflat_dict = dict()\n\t\tfor plugin_module in glob.glob('%s/*.py' % plugin_dir):\n\t\t\tplugin_module = plugin_module.replace('.py', '')\n\t\t\tif os.path.isfile(plugin_module) and '__.' not in plugin_module:\n\t\t\t\tcontinue\n\t\t\tif os.path.isfile(plugin_module[:-3] + '.zip') and os.stat(plugin_module[:-3] + '.zip').st_size == 0:\n\t\t\t\tcontinue\n\t\t\tif os.path.isfile(plugin_module[:-3] + '.py"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tspec = plugin_dir.split('/')\n\t\ttry:\n\t\t\treturn dict((key, val.split('=')) for key, val in spec)\n\t\texcept Exception:\n\t\t\treturn dict()\n\telse:\n\t\tlogger.error(\"Error while parsing %s\" % (\n\t\t\tplugin), exc_info=True)\n\t\treturn {}\n\t\ndef flatten_dict(d):\n\t\"\"\"\n\tCreates a flat dict from the dictionary d. this takes the values\n\tspecified by _config\n\n\t:param d: The dictionary from which flat dicts will be created.\n\t\tTo change a"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\t# create the metadata dict\n\t\tmeta_path = os.path.join(plugin_dir,'metadata')\n\n\t\tplugin_meta = utils.load_yaml_file(meta_path)\t# pylint: disable=W1401\n\t\treturn normalize_metadata(plugin_meta)\n\n\tdef _plugin_plugins_conf_keys(self):\n\t\tplugin_names = self._plugin_mapping_to_list('plugins')\n\t\treturn dict((p, getpwuid(pw)) for p, pw in plugin_names.items())\n\n\tdef _get_plugin_map"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tspec_path   = os.path.join(plugin_dir, \"plugin.spec\")\n\t\tif not os.path.exists(spec_path):\n\t\t\treturn None\n\t\tspec      = unyaml.load(open(spec_path))\n\t\tif \"name\" in spec: spec[\"name\"] = spec[\"name\"].lower()\n\t\tif \"author\" in spec: spec[\"author\"] = \" \".join(spec[\"author\"])\n\t\treturn spec\n\treturn None\n\nclass IdleTaskContext(TaskContext):\n\t\"\"\"\n\tA more abstract task context, intended"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tspec_properties = None\n\n\t\tdef validate_path(_name):\n\t\t\tif not _name:\n\t\t\t\traise ValueError(\"_name not set\")\n\t\t\tif not _name.startswith(\"_\"):\n\t\t\t\traise ValueError(\"_name must be of the form plugin_name:plugin_path_to_target_with_plugins\")\n\n\t\tpath, target_name = os.path.split(_name)\n\t\ttarget_name_components = target_name.split('.')\n\t\tif len(target_name_components) < 2:\n\t\t\traise ValueError(\"_name does not have a name component: '%s'\""}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tplugin_dir = os.path.expanduser(plugin_dir)\n\t\tplugin_names = os.listdir(plugin_dir)\n\t\tplugin_spec = []\n\t\tfor plugin_name in plugin_names:\n\t\t\tplugin_spec.append({'name': plugin_name, 'description': ''})\n\t\treturn plugin_spec\n\treturn {}\n\ndef load_from_config_paths(plugin_dir, config_paths):\n\t\"\"\"Loads plugins and applies some defaults\n\n\t:param plugin_dir: A path to the plugin's dir\n\t:param config_paths: A"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tplugin_dict = {}\n\t\tplugin_files = glob(os.path.join(plugin_dir, \"*.py\"))\n\t\tplugin_files.sort()\n\t\tfor name in plugin_files:\n\t\t\tfilename = os.path.splitext(name)[0]\n\t\t\tif not '__init__' in name:\n\t\t\t\tcontinue\n\t\t\tf = open(os.path.join(plugin_dir, name))\n\t\t\tplugin_dict[name] = f.read()\n\t\treturn plugin_dict\n\n\t@staticmethod\n\tdef get_plugin_dir_with_files(directory,"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\treturn dict([(\"%s/%s\" % (plugin_dir, sub_dir), getattr(plugin_dir, sub_dir)) for sub_dir in get_plugin_sub_dirs(plugin_dir)])\n\ndef get_plugin_sub_dirs(path):\n        \"\"\"\n\tGet a list of subdirectories, from the plugins directory path\n\n:param path: a directory path\n\t:return:\n\tA list of directories (in path) in the plugins directory\n\t\"\"\"\n\treturn [d for d in os.listdir(path) if is_plugin_dir(os."}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself._config_path = None\n\n\ndef get_configfile():\n\t\"\"\"\n\tReturn a copy of the configuration file paths in ``config/`\n\t\n\t:return: the ``config/`` (read by the caller).\n\t:rtype: ``string``\n\t\"\"\"\n\tglobal _config_path\n\tif _config_path is None:\n\t\t_config_path = os.path.join(get_config_root(), \"config\")\n\treturn _config_path\n\n\ndef write_configuration(path, config):\n\t\"\"\"\n\tGet or create a configuration file"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself._config = ConfigReader.load_settings_from_config_file(self._config_path)\n\n\tdef get_db_prefix(self, use_db_name=True):\n\t\t\"\"\"\n\t\tGet the prefix\n\t\t\"\"\"\n\t\treturn self._config['db_prefix'] if use_db_name else 'db.'\n\n\tdef get_output_path(self):\n\t\t\"\"\"\n\t\tGet the path for the output directory\n\t\t\"\"\"\n\n\t\treturn self._config['output_path']\n\n\tdef get_db_type(self):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not self.environ:\n\t\t\tself.environ = os.environ\n\t\tself.environ.clear_environ()\t\n\n\tdef load(self):\n\t\t\"\"\"\n\t\tLoad configuration.\n\t\t\"\"\"\n\t\ttry:\n\t\t\twith open(self.config_path, \"r\") as file:\n\t\t\t\tself.config = json.load(file)\n\t\texcept IOError:\n\t\t\tself.config = {}\n\n\t\tprint(str(self.config))\t\t\n\t\tif self.show_config:\n\t\t\tself.show_config.show_config()\n\t\t\tprint(\"Config:\")"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tassert not 'APPNAME' in os.environ\n\t\tassert not 'DISPLAY' in os.environ\n\t\tenv = os.environ\n\t\tfor name in 'DISPLAY', 'PATH':\n\t\t\tif env[name]:\n\t\t\t\tenv[name] = os.environ[name]\n\t\t\n\t\t\n\t\t\n\nfrom django.contrib import auth\nfrom django.conf.urls import patterns, url\nfrom django.contrib.auth.views import password_change, password_change_done\nfrom django.views.generic.base import RedirectView\n\nfrom password_reset import urls\n\nurlpatterns = patterns('',\n   "}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself.data[\"configpath\"] = None\n\tdef load_config(self):\t\"\"\"\n\tLoad the configuration from the file system.\t\"\"\"\n\tdef get_all_config(self):\t\"\"\"\n\tRetrieve all config from all config files as a dict.\t\"\"\"\n\tif not path.exists(self.configpath):\n\t\tself.load_config()\n\t\tself.data[\"configpath\"] = self.configpath\n\t\treturn self.data[\"config\"]\n\telse:\t\n\t\tf = open(self.configpath)\n\t\ttry:\n\t\t\tdata"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif self.debug:\n\t\t\t# print('inject_config:' + self.name +'>>'+ self.envvar)\n\t\t\tself.envvar_values += (None, None)\n\t\telse:\n\t\t\tif os.getenv('PYWIKIBOT_CONFIG') is None:\n\t\t\t\tprint(self.envvar)\n\t\t\t\traise FWConst.RuntimeException(self.envvar)\n\t\t\tself.envvar_values += (self.envvar, None)\n\n\tdef set_config(self, value):\n\t\t\"\"\"\n\t\tSet the environment variable to value"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not hasattr(self,'path'):\n\t\t\traise Errors.WafError('config.py:inject_config() argument is not set',self)\n\t\tfor v in(self.environ_prefix,self.bin_path):\n\t\t\ttry:\n\t\t\t\ti = self.path.index(v)\n\t\t\texcept ValueError:\n\t\t\t\tLogs.debug('config.py: inject_config() option %r: invalid index %r' % (\n\t\t\t\t\tv,i))\n\t\t\t\ti=1\n\t\t\tif i<0:\n\t\t\t\tLogs.warn('config.py: inject_config"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not environ.get(\"PATH\"):\n\t\t\traise ConfigurationError, \\\n\t\t\t\t\"You must specify a DMI path via the --path option.\"\n\n\t\tif self.env:\n\t\t\tenviron[\"PATH\"] = self.env\n\t\telse:\n\t\t\tself.env = environ.get(\"PATH\", \"\").split(os.pathsep)\n\t\t\tself.env += self.additional_env_path\n\n\tdef load(self):\n\t\t\"\"\"\n\t\tLoad the configuration module (if present).\n\t\t\"\"\"\n\t\tif not os.path.exists(self.file_path):\n\t\t\traise Config"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not self.environ:\n\t\t\treturn\n\n\t\tv = None\n\n\t\t# ensure the key does exist:\n\t\ttry:\n\t\t\tos.stat(self.abspath)\n\t\texcept OSError:\n\t\t\tpass\n\n\t\ttry:\n\t\t\tos.stat(self.abspath + '.config').st_mtime\n\t\texcept OSError:\n\t\t\tpass\n\n\t\tif not v:\n\t\t\tself.fatal(\"couldn't find environment variable '%s'; aborting!\" % self.abspath)\n\n\t\tself.environ['LIBDIR'] = self.get_lib()\n\t\tself.environ['LIBPATH'] ="}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tfor path in (\"/etc/frappe/app/frappe\"):\n\t\t\tself.install_app_repo(path)\n\n\tdef link_config(self, destination, source):\n\t\t\"\"\"\n\t\tCreate a symlink in destination with a link that a the current app needs to execute for the destination.\n\n\t\t:param destination: path of the the target app. Don't copy this unless you want to specify something else\n\t\t:type destination: str\n\n\t\t:param source: source for the symlink\n\t\t:type source: str\n\t\t\"\"\"\n\t\tif not os.path.exists(source):\n\t\t"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tif (self._main_parser and self._main_parser.prog is None\n\t\t\tand self._subparsers.get(\"--version\") is not None\n\t\t\tand self._subparsers.get(\"--debug\") is not None\n\t\t\tand self._subparsers.get(\"--silent\") is not None\n\t\t\tand self._subparsers.get(\"--version\") is not None\n\t\t\tand self._subparsers.get(\"--quiet\") is not None):\n\t\t\tself._main_parser.prog = self._subparsers.get(\n\t\t\t\t\"--version\")\n\t\t\tif self._main_parser.prog is"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\targ_parser = argparse.ArgumentParser(add_help=False)\n\t\targ_parser.add_argument(\"subcmd\", help=\"the subcommand\")\n\t\targ_parser.add_argument(\"--path\", nargs=1, help=\"the root entry point\")\n\t\targ_parser.add_argument(\"--type\", nargs=1, help=\"the package \"\n\t\t\t\t\t\t\t\t\t   \"package type\")\n\t\targ_parser.add_argument(\"--config\", nargs=1, help=\"the configuration options\")\n\t\targ_parser.add_argument(\"--target\", nargs=1, help=\"the target \"\n\t\t\t\t\t\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself._args.add_argument(\"--{0}\".format(ROOT_PROGRAM_NAME),\n\t\t\t\t\t\t\t\taction=ExtendCmdAction, nargs=0)\n\n\ndef extend_parser_create(parent_parser, root_subparsers):\n\t\"\"\"\n\tAdds the spec command to the main entry point for constructing\n\tthe argparse subparser and its subparser subparsers.\n\n:param parser: the subparser object to extend.\n\t\"\"\"\n\n\targs = parser.parse_args()\n\n\tif not isinstance(args, argparse.Namespace):\n\t\traise ValueError(\"invalid arg: {0}. Expected"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tfor (option, action, group_title) in self.subparser_actions:\n\t\t\tparent_parser = self.parents[None]\n\t\t\tparser = subparsers.add_parser(parent_parser.get_name(),\n*self.parents[None].get_option_strings())\n\t\t\tparser.set_defaults(action=action)\n\t\t\tparser.set_defaults(dest=self, parser_name=group_title, help=\"set as global option\")\n\t\t\tparser.set_defaults(help=\"set as global option\")\n\t\t\tself.parser = parser\n\n\t\tparser_"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tbase_subparser = super(ExtendCLI, self).add_parser(root_subparsers)\n\t\t\n\n\t\t\"\"\"\n\t\t\tAdd the command and description arguments as options to the main entry point.\n\n\t\t\t\tThis method also adds the base commandline options that are needed to\n\t\t\t\tcomplete command line parsing (ie the -h and help options) to\n\t\t\t\tbe useful for parsing command line arguments.  \n\n\t\t\tFor example, --help will produce a 'usage' subcommand to list all available commands and usage info.\n\t\t\"\"\"\n\t\tif \"--help\" in self._options:\n\t\t\treturn base_subparser\n\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tfor ns in self.namespace_subparsers:\n\t\t\tif isinstance(ns, SubParser):\n\t\t\t\tns.parser = self\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tns.add_argument(\n\t\t\t\t\tns.dest, type=root_argparse_type(),\n\t\t\t\t\tdefault=self)\n\t\t\t\tcontinue\n\n\t# --------------------------\n\t# PRIVATE METHODS:\n\t# --------------------------\n\tdef run(self):\n\t\tpass\n\n\tdef run2(self):\n\t\tns = None\n\n\t\t# parse the input arguments\n\t\targs = self.args\n\t\targs.dest = self"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tsubparser.add_argument_group(\"Required\")\n\n\t\t# Add each subparser to its own config object.\n\t\tfor subcommand in self.subcommands:\n\t\t\tsubparser.add_parser(subcommand.NAME, formatter_class=self.formatter_class, help=subcommand.HELP, description=subcommand.DESCRIPTION, epilog=subcommand.EPILOG)\n\t\n\tdef print_cli(self, epilog, level):\n\t\tself.subcommands.print_cli(level, epilog, False)\n\n\tdef print_message(self, message):\n\t\tself.sub"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tif self._parser is None or self._parser.prog in root_subparsers:\n\t\t\treturn\n\t\ttop_subparsers = self._parser._subparsers.items()\n\t\tfor (command, parser) in top_subparsers:\n\t\t\tparser_map = getattr(parser, \"maps\", None)\n\n\t\t\tkwargs = self._ensure_kwargs(\n\t\t\t\tparser=parser,\n\t\t\t\tparent_parser=parser_map\n\t\t\t)\n\n\t\t\tparser.add_argument(*kwargs)\n\n\tdef run_command(self, command, argv=None, **kwargs):\n\t\tif argv is"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself._parser = None\n\t\tif root_subparsers is not None:\n\t\t\tself._parser = common.add_net_help_command(root_subparsers, 'cli')\n\n\tdef run(self, parsed_args):\n\t\tself._ns = parser.parse_args(parsed_args)\n\n\nclass ConfigCommandsCli(object):\n\tdef __init__(self, args, namespace):\n\t\t# Get a special arguments to manage our particular parser.\n\t\tself._args = args\n\t\tself._ns = parser.parse_args(args)\n\n\t\tif self._"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\n\n\n################################################################################\n# Main entry point, executed when run directly:\nif __name__ == \"__main__\":\n\tpass\n\n\nfrom __future__ import print_function, division\n\nfrom sympy.core import S, symbols\nfrom sympy.integrals.risch import drewrite\nfrom sympy.integrals.transforms import (LaplaceTransform,\n        NonseparableDifferential, NonElementaryIntegralTransform,\n        NonElementarySumTransform, ParametrizedDifferential)\nfrom sympy.integrals.transforms import rsolve\nfrom sympy.utilities.pytest import XFAIL, raises, slow\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t \n\tret = []\t# root should not be deleted, so this should be the last element\n\tfor sub_path, child in path.items():\n\t\tif not is_child(sub_path, root):\n\t\t\tret.append(sub_path)\n\n\tfor sub_path in ret:\n\t\treplace_paths(sub_path, root, ret)\n\n\treturn ret\n\ndef remove_all(path, pattern):\n\t\"\"\"\n\tRemove `pattern` from every directory in path.\n\t\"\"\"\n\tret = []\n\tfor (root_path, dir_"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tpath = path + \"/\" + root\n\t\ttry:\n\t\t\trmtree(path)\n\t\texcept OSError:\n\t\t\tlogger.exception(\"Error while remove root \\\"%s\\\".\", path)\n\ndef replace_file_ext(file, ext):\n\tnew_path = os.path.splitext(file)[0] + ext\n\treturn new_path\n\ndef run_command(command, cwd=None, quiet=False):\n\tdef _prepend_env(dict_env, name, values):\n\t\tappend_if_none_in_dict_env(values, name,"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tfor name in os.listdir(path):\n\t\t\tpath_path = os.path.join(path, name)\n\t\t\tif os.path.isdir(path_path):\n\t\t\t\trmtree(path_path)\n\t\t\telse:\n\t\t\t\tos.unlink(path_path)\n\telse:\n\t\tos.rmdir(path)\n\t\t\ndef createdir(path, mode=0o777, sudo=False, groups=None):\n\ttry:\n\t\tos.makedirs(path, mode, sudo, groups)\n\texcept OSError:\n\t\traise\n\treturn path\ndef path"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\timport errno\n\t\timport os.path\n\t\timport fnmatch\n\t\treturn ''.join(os.listdir(root)) + '\\033[0m' + ''.join(\n\t\t\tfnmatch.fnmatch(name, pattern)\n\t\t\tfor name in fnmatch.filter(os.listdir(root), pattern)\n\t\t)\n\nclass PackageInfo:\n\tdef __init__(self, pkgname):\n\t\t\"\"\" Initialize with the package name given, where pkgname is the package name \"\"\"\n\t\tself.fullname = \"%s/%s\" % (PKGDB, pkgname)\n\t\tself.built = False\n\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\timport os, re, sys\n\t\tnewPath = root + path\n\t\tif path == root: return\n\t\telif os.path.isdir( root + path ): shutil.rmtree( root + path )\n\t\telse: logging.error(\"Not a dir : \" + root)\n\n\tdef copy_root(path, root, prefix=\"\", extension=\"\"):\n\t\t\"\"\"\n\t\tCopy root from path, throw exception on failure.\n\t\tNOTE: if path!= root raise exception, unless specified\n\t\tas \"root\", then it will be raised.\"\"\"\t\n\t\tif path == root: return\n\t\tpath ="}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\trmdir(root)\n\t\texcept:\n\t\t\tpass\n\ndef move_path(from_path, path, dest, overwrite=True):\n\t\"\"\"\n\tMoves source path from_path to dest.\n\tNOTE If dest already exists, it is ignored.\n\tNOTE If overwrite is true, it must be a directory.\n\t\"\"\"\n\tpath = os.path.realpath(os.path.expanduser(path))\n\tif not overwrite and os.path.isdir(path):\n\t\ttry:\n\t\t\tmkdir(path, 0o700)\n\t\texcept"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\tos.remove(path)\n\t\texcept OSError:\n\t\t\tpass\n\n\ndef removePath(path):\n\t\"\"\"\n\tRemove dir/file, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\tos.rmdir(path)\n\texcept OSError:\n\t\tpass\n\n\nclass TestSyncClass(unittest.TestCase):\n\t\"\"\"\n\tUse cases will pass the tests and execute some sync code to generate xml \n\tfiles using the sync module.\n\t\"\"\"\n\tdef setUp(self):\n\t\tself._moduleDir = os.path.realpath(os."}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif not path or path[0]!= \"/\":\n\t\t\traise IOError(\"Invalid root element\")\n\t\tp = path.split(\"/\")[1:]\t\t\t# trim everything after empty element\n\t\twhile p!= \"\" and p!= \"..\":\t# remove everything after the p\n\t\t\tif p.startswith(\"..\"):\n\t\t\t\tp = p[1:]\n\t\t\telif p!= \"\":\n\t\t\t\traise IOError(\"Invalid root element %s\" % p)\n\t\twhile p[0] == \"=\":\t\t\t\t# remove leading space and comma\n\t\t\tp = p[1:]\n\t\treturn root.rstrip(\"/\")"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif not root.endswith(os.sep):\n\t\t\tpath += os.sep\n\t\tif not root.startswith(os.sep):\n\t\t\troot = root[len(os.sep):]\n\t\tpath = os.path.join(path, ignore_dotdot)\n\t\tif os.path.islink(path):\n\t\t\ttry:\n\t\t\t\tremove(path)\n\t\t\texcept OSError:\n\t\t\t\treturn False\n\t\telif os.path.isdir(path):\n\t\t\ttry:\n\t\t\t\tremovedirs(path)\n\t\t\texcept OSError:\n\t\t\t\treturn False\n\t\telse:\n\t\t\traise\n\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tresult=root\n\t\twhile not result or result.lower().startswith(\".git\"):\n\t\t\tresult=os.path.dirname (path)\n\t\twhile not result or result.lower().startswith(\"/\"):\n\t\t\tresult=os.path.basename (path)\n\t\treturn result\n\treturn strip_root\n\ndef fetch_tarball(tarball_path, target_dir):\n\t\"\"\" fetch the given tarball, recursively\"\"\"\n\n\ttarget_dir = strip_root(target_dir, \"\")\n\n\tprint \"fetching \" + tarball_path \n\t\n\t# find all the matching tarballs and extract"}]}
