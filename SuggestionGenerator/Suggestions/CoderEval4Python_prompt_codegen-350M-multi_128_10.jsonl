{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn Time(nanoseconds)\n\ndef _unpack_array(a):\n\t\"\"\"\n\tUnpacks a `bytearray` based on an index. This is done by `unpacker` (http://unpacker.readthedocs.org/en/stable/).\n\t\"\"\"\n\treturn bytearray(a)\n\ndef _unpacker(a, offset):\n\t\"\"\"\n\tUnpacks `a` and returns its individual components. `a` should be a `bytearray` that\n\tcontains a `by"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn _time.time(nanoseconds)\t\t\n\ndef get_tz():\n\t\"\"\"get a value from tz or local timezone\"\"\"\n\treturn _time.tzname(timezone())\n\ndef get_system_time():\n\t\"\"\"get current system time in timezone\"\"\"\n\treturn _time.time()\n\ndef get_local_time():\n\t\"\"\"get current time in this environment, as `Time`\"\"\"\n\treturn _time.localtime()\n\n# END\n\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tt = tz == None? 'T' : tz\n\treturn LocalTime(\n\t\tnanoseconds / 1000000000, t\n\t)\n\ndef hydrate_timedelta(\n\tduration, tz=None, *,\n\tformat='raw',\n\tprecision='raw',\n\tas_numpy_array=False\n):\n\n\t\"\"\"\n\tHydrator for timedelta values.\n\n\t:param duration:\n\t:param tz:\n\t:return:\n\t\"\"\"\n\n\ttry:\n\t\treturn LocalTimeDelta(\n\t\t\tduration / 100"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn Time(*time((1<<64), tz)) - tz.nanoseconds\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tts = np.fromiter((nsec for nsec in range(2)), np.int64, 1)\n\tif tz is not None:\n\t\tts[ts.i >= 23] += tz\n\treturn ts > 1000000000\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t# this is the main purpose of this function.\n\t# this function calls the method passed \n\t# by the user to its `dt`_ constructor.\n\n\treturn tz.tzset()\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tpass\n\n\ndef _get_time_from_time_zone(time):\t\n\t\"\"\"Returns a timezone as an `Interval` object\"\"\"\n\tif time.tzinfo:\t\t\t\t\n\t\treturn time.replace(tzinfo=time.tzinfo)\n\treturn time\n\ndef _to_time(time, tz=None):\t\n\treturn time\n\t\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn tz.localtime(nanoseconds)\n\ndef local_time(seconds, tz=None):\t\"\"\"\n\tCreates a local time object from a time in seconds.\n\n:param seconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn Time.from_local_time(seconds, tz=tz)\n\ndef get_from_local_time(date):\n\treturn Time.get(date) * 1000.\n\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif nanoseconds is None: return nanoseconds\n\tif tz is None: tz = time.tzname\n\tutc = tz if not utc else tz.replace(minute=0, hour=0, dayofweek=0, dayofmonth=0)\n\treturn Time(nanoseconds//100000.0, utc=True, zone='UTC')\t\n\n#===================================================================\n# Standard definitions\n#\n\ntime = LocalTime()\ntime = hydrate_time(0, tz=utc)    \ntime ="}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\ttime = Time(nanoseconds)\n\tlocaltime = LocalTime(nanoseconds)\n\n\t# Create a datetime from the input time\n\tif tz is not None:\n\t\ttime.tz_convert(tz)\n\n\treturn time, localtime\n\ndef make_time_zone(iso_tz):\n\t\"\"\"\n\tCreates a TimeZone object containing a timezone string\n\t:param iso_tz:\n\t:return: timezone\n\t\"\"\"\n\n\ttz = TimeZone()\n\n\tif tz.tzname:\n\t\t# Add a"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn value"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn Timedelta(value.days, value.seconds, value.microseconds)\n\n\ndef string_to_timedelta(raw_value):\n\t\"\"\"\n\tThis converts a raw raw `string` to `timedelta`.\n\tFrom ``string_to_timedelta`` from the Python Cookbook\n\t\"\"\"\n\tif not isinstance(raw_value, datetime.timedelta):\n\t\traise ValueError(\"Provided raw datetime is not a timedelta\")\n\treturn raw_value\n\n\ndef timedelta_to_string(dt_str):\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif value.days < 0:\n\t\treturn (timedelta(days=-(value.days+1))).total_seconds()\n\telse:\n\t\treturn (value).total_seconds()\n\ndef make_default_args(request, request_type):\n\t\"\"\"\n\tRetrieve all the data necessary for `request_type` from the request. This\n\tfunction assumes the request is a PAGINEL request and does some type\n\tconversions.\n\n:param request:\n:type request: :class:`django.http.HttpRequest`\n:param"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif isinstance(value, dict):\n\t\tif \"from\" in value:\n\t\t\treturn utils.utils.timedelta_to_string(value[\"from\"]), utils.utils.timedelta_to_string(value[\"to\"])\n\t\telse:\n\t\t\treturn utils.utils.timedelta_to_string(value), utils.utils.timedelta_to_string(value)\n\telif isinstance(value, timedelta):\n\t\treturn utils.utils.timedelta_to_string(value)\n\telse:\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\ttim_nano_sec_str = value.microseconds * 1000000 + value.seconds\n\ttim_nano_sec = int(float(tim_nano_sec_str) / 1000)\n\t\n\treturn (tim_nano_sec * 1000 + microseconds_to_units(tim_nano_sec_str) * 100 + minutes_to_units(value.seconds) * 100 + days_to_units(value.days))\n\ndef microseconds_to_units(value):\n\treturn (value / 1000)\n\ndef minutes_to"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif isinstance (value, datetime):\n\t\treturn value.isoformat()\n\tif isinstance (value, timedelta):\n\t\treturn value.seconds\n\tif isinstance (value, float):\n\t\treturn value\n\n\n\treturn value\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif isinstance(value, timedelta):\n\t\treturn value\n\tif not value:\n\t\treturn value\n\treturn value.total_seconds()\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif value:\n\t\treturn timedelta(seconds=value.seconds)\t\n\telse:\n\t\treturn None\t\n\t"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn value\n\ndef to_timedelta(value):\t\"\"\"\n\tConverts a `timedelta`\n\n:param value:\n:type value: `string` or `date`\n:return:\t\"\"\"\n\ttry:\n\t\tif isinstance(value, basestring) and value in [timedelta.DAY, timedelta.MONTH]:\n\t\t\treturn timedelta(days=1)\n\t\telif isinstance(value, timedelta) and value.days > 0:\n\t\t\treturn value\n\t\telif isinstance(value, timedelta):\t#"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif isinstance(value, timedelta):\n\t\treturn value\n\telif isinstance(value, long):\n\t\treturn value\n\telif isinstance(value, bool):\n\t\treturn value\n\telse:\n\t\treturn timedelta(0)\n\n\t\nDEFAULT_DATE_FORMAT = \"%m-%d-%Y\"\n\n\nDEFAULT_DATE_TIME_FORMAT = \"%Y-%M-%dT%H:%M:%S.%f\"\n\n\ndef get_current_user_id(request):\n\ttry:\n"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tvalue = time.strftime(\"%b %d %H:%M:%S %Z\", value)\n\treturn value\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\n\tif value is not None:\n\t\treturn value.time\n\t\t\n\n\n\t\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\ttime = None\n\tif type(value) in [int, long]:\ttime = time.localtime(value)\n\t#if isinstance(value, basestring):\tvalue = value.split(' ')\n\tif time:\n\t\treturn time.tm_year + year, time.tm_mon + month, time.tm_mday\n\telse:\n\t\treturn None\n\n\t#default: get time in local timezone as a tuple or string\n\n@perf.guardedmethod\ndef parse_time(d):\t\"\"\"\n\tReturn string representation of time"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif isinstance(value, basestring):\n\t\t\"\"\"Dehydrate from a string representation\"\"\"\n\t\ttry:\n\t\t\tvalue = time.ctime(string_to_utc_time(value))\n\t\texcept ValueError:\n\t\t\tpass\n\telif isinstance(value, dict):\n\t\t\"\"\"Dehydrate from a dictionary representation\"\"\"\n\t\tfor k, v in value.items():\n\t\t\tdehydrate_time(k)\n\n\treturn value\n\n\ndef parse_date(string):\n\t\"\"\"\n\tParses a date string to a UTC timestamp.\n\t"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value.minute == 1:\n\t\treturn _(\"%d: %02d\") % (value.hour, value.minute)\n\treturn '%02d:%02d' % (value.hour, value.minute)\n\ndef get_time_object(value):\n\tif value is None:\n\t\treturn None\n\n\ttry:\n\t\treturn datetime.time(value.hour, value.minute)\n\texcept Exception, e:\n\t\treturn value\n\n\ndef get_date_object(value):\n\tif value is None:\n\t\treturn None\n\n\t"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\ttry:\n\t\treturn value.isoformat()\n\texcept AttributeError:\n\t\treturn value # pragma: no cover\n\ndef dehydrate_recreation(value):\n\t\"\"\"\tDehydrates the recreation value.\n\n:param value:\n:type value: int\n:return:\t\"\"\"\n\ttry:\n\t\treturn int(value)\n\texcept ValueError:\n\t\treturn value # pragma: no cover\n\ndef is_date(str):\n\t\"\"\"\tChecks for dates and times.\n\n:param str:\n:type str:\n:"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn \"\"\"[\n\t\t\"$date\",\n\t\t\"$time\",\n\t\t\"$milliseconds\",\n\t\t]\"\"\".format(\n\t\t$date=value.strftime(\"%Y-%m-%d %H:%M:%S\"),\n\t\t$milliseconds=d().value,\n\t\t)\n\ndef build_data(item):\n\treturn {\"$datetime\": date_format(item.get(\"$datetime\"), \"mmm_dd_yyyy\"),\n\t\t\"$message\": item.get(\"$message\"),\n\t\t\"$type\": \""}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn pd.to_timestamp(value)\n\ndef process_time(obj, metric=\"processing\"):\n\ttry:\n\t\tobj = pd.to_datetime(obj)\n\t\tif isinstance(obj, datetime.time):\n\t\t\tobj = obj.time()\n\t\telif isinstance(obj, datetime.datetime):\n\t\t\tobj = obj.time()\n\t\telif isinstance(obj, str):\n\t\t\tobj = pd.to_datetime(obj)\n\texcept TypeError:\n\t\tpass\n\treturn pd.to"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif isinstance(value, datetime):\n\t\treturn str(value.strftime(\"%m/%d/%y.%H:%M:%S\"))\n\n\nclass User(object):\n    \"\"\" A user \"\"\"\n\n    def __init__(self):\n        \"\"\" Initializer User class \"\"\"\n        self.name = \"\"\n        self.date_created = None\n        self.update_time = None\n\n        \n    def __unicode__(self):\n        return \"%s - %s\" %(self.name, self.date_created)"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn value.isoformat()[0:8]\n\t\n\ndef dehydrate_time_difference(value):\t\"\"\"\n\tDehydrator for `time_difference` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn value.isoformat()[0:12]\n\t\t\n\ndef flatten(l):\n\t\"\"\"Flatten a list.\"\"\"\n\tif isinstance(l, list):\n\t\tfor i in l:\n\t\t\tl.append(i)\t\n\treturn l\n\t\t\n\t\t\ndef"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn value.m\n\ndef hydrate_point(point, n):\t\"\"\"hydrate_point returns a hydrated point given a\n\tset of arguments\"\"\"\n\treturn Point(n,point.m)\n\ndef parse_point(point):\t\"\"\"Parser for points\"\"\"\n\treturn Point(point)\n\n\ndef get_parsedate(point):\n\t\"\"\"\n\tDehydrates and recovers a time from a points\n\t:param point: points\n\t:type point: List[Point]\n\t:return: time_dictionary in string format\n\t:r"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\ttry:\n\t\tif self.model[6] == 'R':\n\t\t\treturn str(value.mul(0.5))\n\t\telse:\n\t\t\treturn str(value.mul(0.5).add(0.5))\n\texcept AttributeError:\n\t\treturn value.toString(np.dtype(self.model[6]))\n\n\n\t\nclass Dataset(object):\n\t\"\"\"\n\tDataset from the network (and optionally other info) used as\n\tone point for classification for the model.\n\t\n\t\n\t'''"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn (value.x, value.y)\n\n"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tvalue = value.geojson # Convert Point to Point.geojson\n\treturn value.attributes\n\n#=======================================================================\n#=======================================================================\n\n\n#======================================\n#======================================\n\ndef parse_polygon_json(json_string):\n\tpolygon = []\n\tif json_string is not None:\n\t\ttry:\n\t\t\tjson_string = json_string.decode(\"utf-8\")\n\t\texcept UnicodeDecodeError:\n\t\t\treturn json_string\n\n\t\tfor line in json_string.split(\"\\n\"):\n\t\t\tline = line"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn value.__dehydrate__()\n#\n"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\n\treturn value"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif isinstance(value, list):\n\t\tres = []\n\t\tfor v in value:\t\t\n\t\t\tres.append(hue.hue_to_int(dehydrate_point(v)))\n\t\treturn res\t\n\n\tif isinstance(value, tuple):\n\t\tres = ()\t\t\n\t\tres = tuple(res)\n\t\tres = [hue.hue_to_int(dehydrate_point(v)) for v in value]\n\t\treturn res\n\ndef _dehydrate_hue_sensor(sensor_id, field, args"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tpoint = value | 0\n\tpoint_class = type(point)\n\ttry:\n\t\tif point_class == int:\n\t\t\t# if it is an integer we can do a quick look-up\n\t\t\tif value.p >= 0:\n\t\t\t\tvalue = float(point) # the value is signed\n\t\t\t\tpoint = point - 0.5 # the sign is inverted\n\n\t\t\telse:\n\t\t\t\tpoint = (-1) * point # the signal is inverted\n\t\t\t\tvalue = value + 0.5 # the sign is kept\n\t\telif point_class == float:\n\t\t\tif value >= 0"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tfrom gdeo import geom\n\treturn geom.Point(value).as_dict()['geometry']\n\n@mongoengine.connect('database')\ndef connect(**kwargs):\n\t\"\"\"Connect to MongoDB using a given URL\"\"\"\n\t# Load the model so that user cannot override connect\n\tmodel = kwargs.get('model')\n\tsession = ConnectionAdapter.get_session()\n\tsession.connect(model, **kwargs)\n\n@mongoengine.connect('database')\ndef connect_as_admin(**kwargs):\n\t"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\ttry:\n\t\tdvalue = float(value.x.replace('*', ''))\n\texcept ValueError:\n\t\tdvalue = float(value.y.replace('*', ''))\n\treturn dvalue\t\n\n\ndef get_time(x, y=None, *args, **kwargs):\n\t\"\"\"\n\tExtract time for a feature.  It only works for features from\n\tthe image, like text, line, or anything that can be used as\n\ta feature.\n\n:param x:\n:type x: string\n:return:\n:r"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn  self.obj.get(\"key\",()).keys()\t\t\t\t\n\t\t\t\t\t\n\tdef hasKey(self, key):\t\"\"\"\n\tReturn True if the key is set\n\n:param key: the key to check\n:type key: string\n:rtype: bool\n\t\"\"\"\n\tkey_name = self.keyToName(key)\t\t\t\t\n\t\t\n\treturn self.obj.has_key(key_name)\n\n\t\t\n\tdef get(self, key, default_val=None):\t\"\"\"\n\tReturn the field at the key\n\n:param"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self._key.keys()\n\n\t# the value of each key, as key name\n\t# and the value\n\t# of that key.\n\t\n\t# in the case of a nonempty hash key\n\t# the value is a dictionary, where\n\t# the key for each value in the dictionary\n\t# is the key in the hash, or the same\n\t# name or key if there appears to be more\n\t# than one, in which case the key is\n\t# the first in the list\n        \n    def values(self):\t\"\"\"\n\tReturn the list"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.d_record.keys()\n\t\nclass dRecord(object):\n\t\n\tdef __init__(self):\n\t\tself.d_record = {}\n\t\t\n\tdef set_record(self, name, v, default):\n\t\tself.d_record[name] = (v, default)\t\n\t\t\n\tdef get_record(self, name, default):\n\t\treturn self._record.get(name)\t\n\t\t\n\tdef add_record(self, name, v, default):\n\t\tself.set_record(name, v,"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.reclist.keys()\n\n\n\tdef __getitem__(self, index):\n\t\t\"\"\"\n\t\t\tGet record by index.\n\t\t\n\t\t:param index: index\n\t\t:type index: int\n\t\t:return: record\t\n\t\t:rtype: dict\n\t\t\"\"\"\t\n\t\treturn self.reclist.__getitem__(index) \n\n\n\tdef _getattr(self, attr):\n\t\t\"\"\"\t\t\n\t\t\tGet attribute by key.\n\t\t\n\t\t:param key: key (name if attribute is of that type)\n\t\t:type key"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.keyList\n\n# key and value are of type python data structure,\n# for example, a dict or a str.\n# Key/values for keys (not keys in the same database) shall be specified\n# only once, but keys may be repeated.\n\ndef getListOfKeyNames(rec):\n\tkeyList = list(rec.keyList)\n\n\n\t\n\n\t\n\treturn keyList\n\ndef getListOfKeyValues(rec):\n\tkeyList = list(rec.keyList)\n\n\n\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.__keys\n\n\t\ndef keys2String(self):\n\t\"\"\"\n\tReturn a string representation of an array of key names\n\t\"\"\"\n\ts = ''\n\tfor key in self.__keys:\n\t\ts += key + ':' + self.__record[key]['value']\n\treturn s\n\t\n\ndef setValues(self, record_in, rec_out, key_in = None):\t\n\t\"\"\"\n\tSet record value from record_dict for key_in to the record_in\n\t\n\t\"\"\"\n\tif key_in"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self._keys.keys()\n\ndef lookup_keys (self, k):\n\t\"\"\"\n\tLookup a key from the record.\n\n\t:param k: key\t\"\"\"\n\treturn self._keys.get(k)\n\n\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.key\n\tif self.count!= 0:\n\t\treturn self.keys()\n\telse:\n\t\treturn []\n\t#returns a dict of keys \t\n\"\"\"\n\"\"\"\n\t\n\tdef keysWithVal(self, value):\n\t\t\"\"\"Returns a list of keys for which the value is matched to an entry in key map\n\n:param value: the value\n\t\"\"\"\n\t\treturn self.key.lower().find(value)\n\n\tdef _keyValExist(self, value):\n\t\t\"\"\"Return True if the value exists in the record, False otherwise"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\n\tkeys = []\n\tfor key in self.keys_map.keys():\n\t\tkeys.append(key)\n\n\treturn keys\n\ndef _compare_data_set(data_set, record_record):\n\t\"\"\"\n\tCompare two data_sets.\n\n\t:return: True if they are the same\n\t:rtype: bool\n\n\t\"\"\"\n\n\tif isinstance(data_set, (list, tuple)):\n\t\treturn False\n\telif not hasattr(data_set, 'next') or not hasattr(record_record, 'next'"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.keys\n"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tversion_map = []\n\tif protocol_version is not None:\n\t\tfor version in protocol_version:\n\t\t\th = None\n\t\t\ttry:\n\t\t\t\th = cls._protocol_handlers[version].instance(version)\n\t\t\texcept NotImplementedError:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tversion_map.append((version, h))\n\telse:\n\t\t# default protocol version\n\t\tfor ver_str, cls in cls._protocol_handlers.items():\n\t\t\t# TODO: check for unsupportable version (e.g."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\treturn {version: cls for version in protocol_versions if version >= 0}\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tproto_versions = [protocol_version]\n\tif protocol_version:\n\t\tproto_versions.append(protocol_version)\n\treturn {key: cls(*proto_version) for key, proto_version in proto_versions}\n\n\ndef _get_server(address, protocol_version):\n\tproto_versions = [protocol_version]\n\tif protocol_version:\n\t\tproto_versions.append(protocol_version)\n\treturn tornado.ioloop.IOLoop.current()[0].server_address_from_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\thandlers = {}\n\t\tfor protocol in get_protocols(protocol_ver=protocol_ver):\n\t\t\thandlers[protocol_ver[0]] = protocol.__class__\n\t\treturn handlers\n\tlogger.log(PROGRAM_LOG_LEVEL, 'The desired protocol version for the following protocols have been passed: {}'.format(protocol_version))\n\treturn {}\n\n#############################\n#  BOLT_PROTO_HANDLERS  #\n#############################\n\nPR"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tglobal _protocol_version_map\n\tif len(protocol_version) < 1:\n\t\traise TypeError('Expected a protocol_version tuple', protocol_version)\n\t_protocol_version_map = {}\n\tfor version in protocol_version:\n\t\tif version is None:\n\t\t\t_protocol_version_map.update({None: Object})\n\t\telse:\n\t\t\t_protocol_version_map \\\n\t\t\t\t.update({tuple(version): metaclass.Object})\n\treturn _protocol_version_map\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = [v for v in range(3, 4)]\n\tif max(protocol_version) > 3:\n\t\traise TypeError(\"Bolt3 does not support protocol versions >=3\")\n\treturn {1: Bolt3Handler, 2: Brocer3Handler, 3: Bolt4Handler}\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocol_version = protocol_version or ()\n\tif not protocol_version:\n\t\tprotocol_version = (min(cls.protocols), max(cls.protocols))\n\treturn {}.fromkeys(protocol_version, cls)\n\t\nb = BoltProtocolHandler(\"bolts\", \"Bolt-protocol handler\", version=0)\n\ndef version_to_protocol(version):\n\tif version == 0: return b\n\treturn [protocol_name() for protocol_name in b.protocols]\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version:\n\t\tassert(len(protocol_version) == 2)\n\t\treturn {\"v0.%s\" % cls.version_str(version): cls for version in protocol_version}\n\ndef get_available_protocol(version=None):\n\t\"\"\"\n\tReturn a tuple of the protocol classes that are available,\neither through the class attributes (v0.3.0+, v0.4.0+) or\nthe protocol handlers (v0.3.0, v0.4.0).\n\nSee `get_protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (1,3,0)\n\tfor prot_ver in protocol_version:\n\t\tif prot_ver == protocol_version:  # Only return the same version tuple\n\t\t\tif len(protocol_handlers) > prot_ver:\n\t\t\t\tprotocol_handlers[prot_ver] = cls.protocol_handler_map.get(prot_ver, lambda x:None)\n\t\t\treturn protocol_handlers\n\telse:\n\t\tif not len(protocol_handlers):\n\t\t\traise Type"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\treturn {\n\t\t\t2: ProtocolVersion(5, 0),    # Version.py\n\t\t\t6: ProtocolVersion(5, 1),\n\t\t\t1: ProtocolVersion(4, 0),   # JSON, see docs\n\t\t\t2: ProtocolVersion(3, 0),\t # JSON\n\t\t\t6: ProtocolVersion(4, 1),\n\t\t\t1: ProtocolVersion(3, 1),\n\t\t\t2: ProtocolVersion(3, 2),\t # JSON\n\t\t\t6: ProtocolVersion(4, 2),\n\t\t\t1: ProtocolVersion(3, 2),\n\t\t\t2: ProtocolVersion(3, 3"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\treturn property(func_wrappers._tx_fun(timeout=timeout))\n\t\ndef query_tx(query_string):\n\t\"\"\"\n    A function wrapping the :func:`dbms.query.query.Query.query` method into the internal ``query.run`` method.\n    The ``Transaction.query_tx`` function can be used as decorator. \n    Only the :class:`Transaction` instance can be passed. If ``Transaction.query_tx`` is not called, a\n    :class:`ValueError` will be excuted.\n\n\t:param"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\tdef decorator(f):\n\t\tf.func_instrumentation = 'unit_of_work'\n\t\tf.unit_of_work = unit_of_work\n\t\tf.metadata = metadata\n\t\tif timeout is not None:\n\t\t\tf.timeout = timeout\n\t\treturn f\n\n\treturn decorator\n\n@decorator\ndef unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThe transaction/transaction timeout in seconds as a percentage.\n\t\n\tThe default timeout is 100%.\n\t\n\t:param metadata:\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\t# Call the super-class method\n\t\tsuper(TimeoutDecorator, self).__init__(**kwargs)\n\t\tif'metadata' in kwargs:\n\t\t\tself._settxMetaData(kwargs.pop('metadata'))\n\n\tdef _settxMetaData(self, metadata):\n\t\tif not isinstance(metadata, dict): \t# Convert to dict first, otherwise metadata.execute will throw an error\n\t\t\ttry: \n\t\t\t\tmetadata = dict((k, metadata) for k, v in metadata"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, timeout=None):\n\t\tself.timeout = unit_of_work.transaction_timeout(timeout)\n\t\n\tdef __call__(self, *args, **kwargs):\n\t\tif len(args) < 1:\n\t\t\traise RuntimeError(\"missing transaction arguments\")\n\t\t\t\n\t\tobj = args[0]\n\t\t\n\t\tself.timeout += 1\n\t\tif min(args) == self.timeout:\n\t\t\treturn obj(self.timeout)\n\t\t\n\t\tkwargs['timeout'] = self.timeout\n\t\treturn super(unit_"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tif timeout is not None:\n\t\treturn lambda tx, ctx=ctx: tx.run(\"MATCH (u)<-[:WORK_ON]-(v) WHERE ctx(u, 'u'<>u, 'v'<>v) RETURN u\") \n\telse:\n\t\tdef timeout_decorator(function):\n\t\t\treturn lambda tx, (tx, ctx): \n\t\t\t\treturn function\n\t\t\t\n\t\tdef timeout_decorator_factory() :\n\t\t\tdef timeout_decorator_function(*function_args, **function_kwargs"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, metadata=None, unit_of_work=None, timeout=None):\n\t\tsuper(unit_of_work, self).__init__(metadata=metadata, unit_of_work=unit_of_work, timeout=timeout)\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, timeout):\n\t\ttry:\n\t\t\tself.timeout = float(timeout)\n\t\texcept:\n\t\t\tself.timeout = 0\n\tdef __call__(self, tx):\n\t\tq = tx._query\n\n\t\tif isinstance(q, neo4j._repo_factory.Transaction):\n\t\t\treturn tx.retain(self.timeout)\n\t\telse:\n\t\t\ttx.close()\n\t\t\tdb = tx.__db__\n\t\t\treturn tx.run(\"CREATE TRANSACTION {timeout}; COMMIT TRANSACTION; END;\".format(timeout"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tfrom neo4j import dbms\n\n\t@unit_of_work(timeout)\n\tdef transaction_wrapped(*args, **kwargs):\n\t\twith dbms.transaction():\n\t\t\treturn tx_wrapped(*args, **kwargs)\n\n\t@transaction_wrapped.decorator\n\tdef transaction_decorator_wrapped(*args, **kwargs):\n\t\twith dbms.transaction():\n\t\t\treturn decorated_tx_wrapped(*args, **kwargs)\n\n\t@transaction_decorator_wrapped.decorator\n\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t@wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tif not timeout:\n\t\t\treturn func(*args, **kwargs)\n\t\telse:\n\t\t\ttry:\n\t\t\t\ttimeout = timedelta(microseconds=timeout)\n\t\t\texcept ValueError:\n\t\t\t\traise TypeError(\"timeout should be of type timedelta or float\")\n\t\t\ttimeout_end = datetime.datetime.now() - timeout\n\t\t\t# prevent timeout, the timeout value of zero will be ignored by the database and the current transaction will not be triggered\n\t\t\t# so a timeout value of other than zero"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, timeout=None):\n\t\tif timeout is None and not metadata is None:\n\t\t\tself.timeout = timeout\n\n\t\tif isinstance(timeout, timedelta):\n\t\t\tself.timeout = timeout.normalize()\n\n\tdef __call__(self, func):\n\t\t\"\"\"\n\t\tThis decorator sets a transaction timeout using ``timeout`` argument.\n\t\t\"\"\"\n\t\tfunc.transactionTimeout = type(self.timeout)(timeout=self.timeout)\n\t\treturn func\n"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tif isinstance(key, tuple):\n\t\tkey = key[0]\n\tif self.index.has_key(key):\n\t\treturn self.index[key]\n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tif type(key) is not tuple:\n\t\treturn super(IntIndexer,self).index(key)\n\telse: return index(key[0])\n\t\ndef _(self, key):\t\"\"\"\n\treturn self[key]\n\t\"\"\"\n\n\tif type(key) is str: \n\t\treturn self[_(key)]\n\treturn _(self[key])\n\nIntIndexer.indexer = _\n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.__index__(key)\n\n###\tKey Methods\n\ndef __delitem__(self, key):\n\t\"\"\"\n\tDelete item at the index of the given key unless the key belongs to the item\n\n:param key: item key\n:type key:\tstr\n:return: None\n:rtype: None\n\t\"\"\"\n\tif __debug__:\n\t\tassert __debug__ and key in self.__index__\n\n\tself.__index__.pop(key, None)\n\n##\tKey Method Definitions\n\ndef __getitem__(self"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.dictf.find(key)\n\n\n\t\ndef get_data(items):\n\t\"\"\"\n\t\tReturns full list of data.\n\n\t\t:param items: list of keys\n\t\t:type items: int,list\n\t\t:rtype: list\n\t\t\"\"\"\n\treturn items[:]"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\n\treturn key in self\n\ndef find_last(self, key):\t\"\"\"\n\tSearch the list for the given item, and return the index of the item before\n\tit (if found).\n\n:param key: a key\n:return: list index\n:rtype: int\n\n\"\"\"\n\t\n\treturn key[1] if key[0] is None else key[1]\n\t\t\n\t\n\ndef main(args_list, argv):\n\t\"\"\"\n\tParse and run cmdline list of command line arguments.\n\n:param args_list"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\ttry:\n\t\treturn self.cache.index(key)\n\texcept KeyError:\n\t\treturn None\n\t\n\t\n\t\n#==============================================================================\nclass Cursor(object):\n\t\"\"\"\n\tA cursor that iterates over items\n\t\"\"\"\n\tdef __init__(self, parent, cache):\n\t\t\"\"\"\n\t\tCreate a cursor for a given :class:`List`\n\t\t\n\t\t:param parent: the list's listitem\n\t\t:type parent: :class:`List`\n\t\t:param cache: A :class:`Cache` instance\n\t\t:"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.__index(key)\n\ndef __index(key):\n\t\"\"\"Return the index of the given item.\n\nUsage: ::\n\t\n\t\tfrom itertools import islice\n\n\t\tdef __index(key):\n\t\t\tprint(\"%s %d\" % (key, islice(*[index(i) for i in iter(key)])))\n\t\"\"\"\n\t# return the key: [index[key], key]\n\treturn ([index(key), key], key)\n\t\ndef _make_list(index, index_map):\n\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.data.getIndex(key)\n\n############################################################################\n#\n# The rest is from bttv.\n#\n# @note: this function is just an adapter that converts from\n#       a Python dict to python's OrderedDict.\n#\n############################################################################\ndef dict2dict(d):\n    newdict = OrderedDict()\n    for k, v in d.items():\n        newdict[k] = v\n    return newdict\n\ndef dict2tuple(d):\n\tnewtuple = tuple()\n\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self._get(key).__index__()\treturn\t\n\n\n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tn = 0\n\tfor n, x in enumerate(self.indexes):\n\t\tif key == x:\n\t\t\treturn n\n\treturn -1\n\n#------------------------------------------------------------------------------\n#\n#\tCreate a list of Index items from a dictionary\n#\n#------------------------------------------------------------------------------\ndef dict_items(d):\n\t\"\"\"\n\tReturn a list of all the items in the dict\n\twhere keys are strings and values are lists of integers.\n\t\"\"\"\n\tL = []\n\tfor (k, v) in iteritems(d):\n\t\tvL = map(int, v)"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tkey_values = [ value for key,value in self.records.items() if \\\n                          (key in keys) ]\n\treturn key_values\n"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn RecordToList(self.object, self.include, keys)\n\n\n\t## Returns the record or the object matching the record indexed by\n\t## the keys provided\n\t##\n\t## Note: no objects will be returned if a record has\n\t##   multiple objects corresponding to the keys provided.\n\t\n\tdef by_index(self, *keys):\t\"\"\"\tReturn a dictionary indexing the keys into values\n\n:param keys: indexes of the item objects to include; if they all\n            have to be indexes\n:return: dictionary of the record or object containing keys\n\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\n\treturn self.get_records(\"_self\", \"Value\", \"\", keys)\n\ndef valuesByProperty(self, p, key):\n\t\"\"\"\n\tReturn the values of the records, optionally filtering to\ninclude only certain values by index or key.\n\n:param p: name of the property to look for; if omitted, all values\n         will be included\n:param key: key of record \n:return: list of values, optionally filtered for a key\n:rtype: list\n\"\"\"\n\treturn self.get_records(\"_self\", \"_all\""}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn self.__values(filter=dict(key=keys or tuple(x) for x in keys))\n\t\n\t\ndef values(self, *values):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by value.\n\n:param values: values to include; if none are provided,\n               values will be included the\n               same length as ``list(data)`` or `list(dict_field)`\n               if ``force_all`` is desired\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif not keys:\n\t\treturn self.values()\n\telse:\n\t\tself.cursor.execute('select value from %s where id=%s and value = %s and key like \"%s\" limit 1' % \n\t\t\t\t(self._table, tuple(keys)+('\"'+self._delimiter+'\"'), self._delimiter, self._index))\n\t\treturn list(self.cursor)"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tindexes = self._get_indexes(keys)\n\trecord = self._get_records(keys,indexes)\n\tvalues = []\n\tfor value in record:\n\t\tif value['index'] in indexes:\n\t\t\tvalues.append(value['data'])\n\treturn values\n\ndef get_records_list(query,indexes=None):\n\t\"\"\" Return the list of records within the list.\n\n\t:param query: a base query object for this listing\n\t:type query: :class:`pyotl.query.Query`\n\t:"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif len(self.keys) == 0:\n\t\treturn self.items\n\telif len(keys) == 0:\n\t\treturn self.keys\n\telif len(keys) > 1:\n\t\treturn list(i for k in keys for i in self.items if k[0] in keys)\n\n\treturn [self.keys[0]] + (self.items[0][1] for i in self.items if i[1])\n\ndef contains(rec, list):\t\"\"\"\n\tReturn true if rec contains a given list\n\n:param rec: list of"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn self.items(**{'include':keys})\n\t\n"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn self.__dict__.get('values', [])\n\n\t\n\t\n"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif self.index:\n\t\treturn self.db.values({\n\t\t\tself.key: self.values(self.key, None)\n\t\t})\n\telse:\n\t\treturn self.db.values(*keys)\n\n\ndef all(self, f, key=None, skip_remaining=False):\n\t\"\"\"\n\tLike ``f`` but always returns ``True`` if ``f(key)`` is ``True``.\n\n\t:param key: a search key or a list of keys; if len(key) is 0 or None, all keys\n\t\t\tare included.\n"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn dict((k,(v,t)) for k,v,t in self.keys())\n\n\t\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif not isinstance(keys, tuple):\n\t\traise TypeError, 'data() should be called with tuple'\n\tdata = collections.OrderedDict()\n\tfor key in keys:\n\t\tif isinstance(key, type(None)):\n\t\t\tbreak\n\t\t# check if it's an index\n\t\ttry:\n\t\t\td = {k:v}\n\t\t\tindex = int(key) if isinstance(key, slice) else int(key) * -1\n\t\t\tindex = int(key) if index <= 0 else 0\n\t\t\tfor i in range(index, len"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tkeys = self._as_key(*keys)\t### indexing the list instead of the dict if needed.\n\treturn dict(items(zip(self.items(), keys )))\t### keys passed were already defined above\n\n\t\ndef data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally include only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tflt = lambda x: self.format(x).match('<{}>'.format(x).encode('UTF-8'))\n\tout = {\"{}\".format(x) for x in keys}\n\tfor k in keys:\n\t\ttry:\n\t\t\tval = flt(self.get(k))\n\t\texcept IndexError:\n\t\t\tval = None\n\t\tout.setdefault(k,val)\n\treturn unicode(out.items()).encode('UTF-8')\n\n#------------------------------------------------------------------------------#\ndef update(db,records):\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif isinstance(keys, integer_types) or not isinstance(keys, sequence):\n\t\tkeys = [(key, None) for key in keys]\n\tif isinstance(keys, string_types):\n\t\tkeys = [int(key) for key in keys.split(',')]\n\tkeys = dict([(self.keys[int(index)], key) for index, key in \n\t\t\t\t\t enumerate(keys)])\n\treturn dict([(key, value) for key, value in self.items(keys)])\n\t\ndef values(self, *keys"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif len(keys) and len(keys) == len(self.d):\n\t\treturn self.data(keys)\n\telse:\n\t\traise IndexError(keys)\n\t\n\t\n\t\nclass Subset(object):\n\t\"\"\"\n\tThis class implements a list that only returns some subset of\n\tdictionaries from a set, so that you can iterate over that set\n\twithout specifying a value or index.  This is useful to make the\n\tset available to clients.\n\t\n\t:param values: the set of records to include\n\t:type values:"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn self[keys]\n\nclass RecOrder(object):\n\t\"\"\"\n\tAn order for a Record instance.\n\tNote that only Records that have\n\t:attr:`~DataContainer.__dataslice__` with \n\t:attr:`~DataContainer.__dtypeslice__`, :attr:`~DataContainer.__structslice__`\n\tand :attr:`~DataContainer.__structslice_indices__` will be in the result.\n\n\t>>> order = None\n\t>>> for r in df.sort(\"x"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn dict((self.field(key).value(), value) for key, value in self._raw)\n\ndef to_csv(filename='-', header=0, delimiter='', quote=None, skiprows=0, header_dst=None, index_dst=None, index_cols=None, sort=False, escape=True):\n\t\"\"\"\n\tWrite out records in CSV format to a file. The output is stored in the\n\tfile-like object ``sys.stdout`.\n\n\t:param filename: file name to save the file\n\t:param"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn _Data(self.record().data(*keys))\n\nclass _RecordBuilder(object):\n\t# The :mod:`pymcda.record` class has builtins; don't go into the build_record.py\n\t# module without them!\n\tpass\n\n\tdef __init__(self):\n\t\tself.formatter = ObjectSerializer()\n\n\tdef build_record(self):\n\t\treturn _RecordBuilder()\n\n\tdef _make_record(self):\n\t\t\"\"\"\n\t\tMake a :py:class:`PyMCDA.record.Abstract record"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tval = {}\n\tfor item in self.__dict__.items():\n\t\tif isinstance(item[0], int):\n\t\t\tif item[0] not in keys:\n\t\t\t\traise IndexError(\"Index must be in the list of keys used in the record\")\n\t\t\tval[item[0]] = str(item[1][keys.index(item[0])])\n\t\telse:\n\t\t\tval[item[0]] = item[1].get(keys)\n\tfor key in keys:\n\t\ttry:\n\t\t\tval[key] = self[key]\n\t\texcept"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\txlen = self[0]\n\txval = struct.unpack(\"!H\", self[1:3])[0]\n\ts = struct.unpack(\"!H\", self[3:10])\n\treturn s[1].value << 16 | s[0].value\n\ndef pop_u32(self):\t\"\"\"\n\tRemove the last 4 bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\txlen = self[0]\n\txval = struct.unpack(\"!L\", self[1:"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tvalue = struct.pack(\"<H\", self.u16())\n\ti = len(value) - 2\n\tself.u16(value[:i])\n\treturn value[i:].rstrip('\\x00')\n\ndef pop_i16(self):\t\"\"\"\tRemove the first two bytes of data, returning it as a big-endian signed integer.\n\"\"\"\n\tvalue = struct.pack('<i', self.i16())\n\ti = len(value) - 2\n\tself.i16(value[:i])\n\treturn value["}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t# Make sure the last two bytes we care about stay two bytes,\n\t# then cut off (1 to 0) the two-byte data, then do a little\n\t# arithmetic to get the value.\n\t#\n\t#\tThis makes the code slightly cleaner.  The two bytes we\n\t#\t\tasked us for are 2 bytes, and the result is an int,\n\t#\t\tand no overflow occurs.\n\tbytes = self.raw.remove(2)\n\treturn struct.unpack(\"<%u\" % len(bytes), bytes)[0]\n\n\t# End"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack_from(\"!h\", self.get_data(10, 'utf-16le'), 2)[0]\n\n\tdef pop_u8(self):\n\t\treturn struct.unpack_from(\"!b\", self.get_data(5, 'utf-16le'), 2)[0]\n\n\tdef peek_u32(self, offset):\n\t\treturn struct.unpack('!L', self.get_data(offset, 'utf-16le'))[0]\n\n\tdef pop_u64(self):\n\t\treturn struct.unpack"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tdata = self.raw_data.pop()[0 : 2]\n\treturn struct.unpack(\"<H\", data)[0]\n\n\t# Add size of padding (2 bytes) and actual data\n\tself.raw_data += b\"\\x00\\x00\"\n\treturn b\"\".join(self.pad_data) + b\"\"\n\n\t\t\n\n\n# ---------------------------------------------------------------------\n# Helper functions\n\ndef pop_u32le(self):\n\t\"\"\"\n\tRemove the last 3 bytes of data, returning them as a little-endian\n32-bit little-"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn pack_int(self.pop_byte()+self.pop_byte()<<8, self.endian)\n\n\n\tdef pop_uchar(self):\t\"\"\"\n\tRemove the last byte of data, returning it as a signed-byte.\n\t\t\"\"\"\n\t \tif self.pop_byte()!= 0:\n\t\t\traise ValueError\n\t\treturn pack_int(self.pop_byte(), self.endian)\n\t \t\n\t\t\n\tdef pop_u16(self):\t\"\"\"\n\tRemove the last 2 bytes of data, returning them as a"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn ''.join(chr(self.buf[n-1])*32 for n in range(2, 16))\n\n\ndef pop_u32(self):\n\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\treturn self.buf[10:].pop(0)\n\t\t\n\n\ndef pop_float(self):\n\t\"\"\"\n\tRemove the last byte of data, returning it as a float.\tIt\n\treturns a tuple of two floats, first is the unsigned int of"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack('>H', self.read_bytes(1,2))[0]\n\n\n# ===========================================================================\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn self.decode_le16(0)\t\t\t\t\t\t# remove last two bytes ---------------\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack('!B', self._in.read(1))[0]\t\n\n\t\ndef pop_string(self):\treturn self.pop_uint16()\ndef pop_string_short(self):\n\treturn self.pop_string(2).decode('UTF-8')\n\t\n\t\ndef push_u16(self, x):\tself.push_uint16(x)\ndef push_string(self, p):\tself.push_string(p)\ndef push_string_short(self, p):\n\tself."}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t):\n\tif dehydration_hooks is None:\n\t\tself.dehydrate_hooks = {}\n\t\tself.heade = None\n\t\treturn self\n\thandler_funcs = self._deserialize(dehydration_hooks)\n\tassert len(handler_funcs) > 0\n\thashes = [None]*2 \n\tfor handler_func in handler_funcs:\n\t\tif callable(handler_func):\n\t\t\thashes[0] = list(hash(obj))\n\t\t\t#TODO: use a random number\n\t\t\th"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t#\tThe default will always yield the latest response, or be returned immediately\n\t#\twhen a call returns.  A list of handlers is the default handler_fns.\n\t#\tThis may be used in future to replace the default handler_fns with an arbitrary list of handlers\n\t#\n\n\t# if given a qid is an integer, that indicates that\n\t# this message should be discarded at a later time\n\n\tif qid < 0:\n\t\tqid = len(self.responses)\n\t\tif qid == 0:\n\t\t\tself.ack(0)\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t),\n\t\n\tqueue=1,\n\ttimeout=1,\n\ttimeout_class=Timeout,\n]:\n\t\n\t\n\tdef _build_message(self, data, hdr):\n\t\tmsg = pack('d', self.type.pack(data)) + \\\n\t\t\t('\\n'.join([\"%s=%s\"%(h(hdr),h) for h in hdr]) + '\\n')\n\t\treturn msg\n\n\tdef register_callbacks(self, queue):\n\t\ttids = queue.get()\n\t\tif (tids"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t);\n\t\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t) : return None\t\n\ntry :\n\timport weakref as wr\n\timport Queue\n\timport threading\n\t\n\timport packets\n\t# import mqtt_msgstream\n\t# import mqtt_msgstream.pack_qos\n\n\n\t## FIXME: remove this once packstream has enough functions\n\tclass DisconnectQueue:\n\t\tdef __len__(self):\n\t\t\treturn 1\n\t\tdef __getitem__(self, s):\n\t\t\tl = str(item)\t# string(item) *<*>*2\n\t\t\treturn l"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t) -> packstream.Response\ndef response(headers=None, body=None, **kwparams):\n\t'''\n\tReceive a message as a text/binary stream.\n\t\t\t* Optional argument 'headers' maps header keys to values\n\t\t\t* Optional argument 'body' is a bytes-like object containing raw data\n\n\tHandling of the 'OK' value is defined in the following (but probably\n\texpected to be the same for all of these responses):\n\t\t\t* PackStream response - when received, object is of\n\t\t\ttype packstream.Result, see next line\n\t\t\t* pack"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\timport types\n\th = types.DictType(_hydrate.discard,\n\t\t _hydrate.decode,\n\t\t _hydrate.encode,\n\t\t _hydrate.hydrate,\n\t\t _hydrate.register,\n\t\t _hydrate.unregister,\n\t\t _hydrate.load,\n\t\t _hydrate.compile,\n\t\t _hydrate.get_dehydrate,\n\t\t _hydrate.get_hydrate,\n\t\t _hydrate.get_load,\n\t\t _hydrate.add_add,\n\t\t _hydrate."}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t),\n\n\t'''dispense(self, record, n=None, qid = None, hydration_hooks = None,\n\t\tdehydration_hooks = None, handlers = None):\n\t'''\n\tAppends a DISCPROG message to the output queue.\n\n:param n: number of records to discard, default = 0 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param qid = id of the qid requesting the dispense\n:param hydration_hooks:\n    "}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t)     \t\n\t@asynbot\n\tdef main(self):        \n\t    while 1 == 1:        \n\t\ttry:            \n\t\t    time.sleep(1)             \n\t\t    response = self.pack_response(self.response)         \n\t\t    self.response = self.pack_response(self.response)     \n\t\texcept TypeError:       \n\t\t    if self.message_filter(self.message, self.response):   \n\t\t        return          \n\t\t\n\t\telif self.message_filter(self.message, self.response):   #"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(OutputType, self).__init__(*args, **kwargs)\n\t\t\t\t\n\t\t# Default QID to use if none is specified\n\t\tif args[0]:\n\t\t\tself.qid = args[0]\n\t\telse:\n\t\t\tself.qid = -1\n\t\t\t\n\t\t# Default dehydration hooks to use if none is specified\n\t\tif args[1]:\n\t\t\tself.dehydration_hooks = args[1]\n\t\telse:\n\t\t\tself.dehydration_"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\ttry:\n\t\tif mode == \"READ\":\n\t\t\tbegin = self._makeCall( \"begin\" )\n\t\telif mode == \"WRITE\" or mode == \"NONE\":\n\t\t\treturn self._makeCall(\"begin\", (timeout, bookmarks, metadata, hydration_hooks, dehydration_hooks, handlers))\n\texcept (ConnectionError, ConnectionClosed) as connex:\n\t\tpass\n\telse:\n\t\treturn Response(\"Error: %s\" % connex)\n\ndef _makeCall( method, data = None, callback  = None, timeout = 0"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t)\n\nclass ResponseCursor:\n\t\n\tdef __init__(self, db):\n\t\tself.db = db\n\t\n\tdef __enter__(self):\n\t\tself._curstate = 0\n\t\tself._nextstate = 0\n\t\n\tdef __exit__(self, exc, value, tb): pass\n\t\n\tdef _init(self, first, last):\n\t\tif first == last and first == 0:\n\t\t\traise AssertionError('begin expects first to > 0, last to <= nbr of records.')\n\t\t\t\n\t\t#"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tResponse = Message(\"BEGIN\", \"BEGIN\", \"\")\n\tdb = \"default\"\n\timp_user = None\n\n\tif \"BIN\" not in cmd or not cmd.BIN() or cmd.BIN().startswith(\"BEGIN READ\"):\n\t\tResponse.set_error(\"Command not compatible with running transactions. Please use the BEGIN transaction instead (as it is a no-op).\", None)\n\t\treturn\n\n\tif mode == None:\n\t\tmode = \"READ\"\n\tif timeout == None:\n\t\ttimeout = DEFAULT_DEHYDRATE"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn self.start(mode, bookmarks, metadata, timeout, db, imp_user,\n\tdehydration_funcs=dehydration_hooks, hydration_funcs=hydration_hooks,\n\tbookmark_func=bookmarks)\t\n\n\t\n\t\ndef end_transaction(self, tx):\n\t\"\"\"Terminates any ongoing transaction and destroys it.\"\"\"\n\tlogger = logging.getLogger(__name__)\n\tif logger.getEffectiveLevel() < logging.WARN or logger.getEffectiveLevel() < logging.INFO:\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tResponse('begin')\n\tif mode is not None:\n\t\tResponse('begin', 'BEGIN', {'mode':mode,'bookmarks':bookmarks,'metadata':metadata, 'timeout': timeout}, 'begin', bookmarks, metadata, timeout)\n\tif mode is 'READ' or 'WRITE':\n\t\treturn Response(None,'BEGIN', None, None, timeout)\n\n\treturn None\n\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tpass\n\n\treturn Response(None,\n\t\t\"\",\n\t\t\"\",\n\t\t\"\")\n\tpass\n\n\t# end\n# end\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tresponse = Response()\n\tresponse['BEGIN'] = _buildResponse(mode=mode, bookmarks=bookmarks, metadata=metadata)\n\tresponse['STOP']  = _buildResponse(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout)\n\tresponse['COMMIT']  = _buildResponse(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout)\n\n\treturn response\n\ndef _buildResponse(mode=None, bookmarks=None, metadata=None, timeout=None):\n\t\"\"\"\n\tBuilds an"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tResponse(\"begin\", None)\n\ndef end(self, mode=None):\n\t\"\"\"\n\tCommits transaction.\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t\"\"\"\n\tResponse(\"end\")\n\ndef reset(self, mode=None):\n\t\"\"\"\n\tClears transaction.\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t\"\"\"\n\tResponse(\"reset\")\n\ndef cancel(self, timeout=None):\n\t\"\"\"\n\tCancels a"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t_ = self.__module__,'response'\n\t\n\t# If the operation being sent fails, return a 503 in-band response\n\tif mode not in (\"READ\", \"WRITE\", \"READ_WRITE\"):\n\t\treturn Response(status_code=503, raw=True)\n\n\t# If the function parameters are not iterable, return a 400 Bad Request error\n\tif not isinstance(bookmarks, container_type):\n\t\treturn Response(status_code=400, body=\"%s not iterable\" % bookmarks, raw=True)\n\t\t\n\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn Response(self._stream_send(messagenode, mode, bookmarks, metadata, timeout))\n\nclass Response:\n\n\tdef apply_headers(self, request):\n\t\theaders = request.headers\n\t\tif headers is not None:\n\n\t\t\tfor header in headers:\n\t\t\t\tname = header[\"header\"]\n\t\t\t\tval = headers[header[\"header\"]]\n\t\t\t\tif header[\"parameter\"] in name.split('_')[-1]=='true':\n\n\t\t\t\t\tif header[\"type\"] == 'bool' or header[\"type\"] == 'bool/short' or header[\"type"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n <= 1:\treturn n\n\telif n >= 3 and n.is_even :\treturn 2*n\n\telif n >= 3 and n.is_odd:\treturn 2*(n-1)\n\t\n\treturn n\n\t\n\t\n    \nif __name__ == \"__main__\":\n    #print round_half_to_even(3.2)\n    print round_half_to_even(3)"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n<0:\n\t\traise ValueError\n\tif n>1:\n\t\tif n==2:\n\t\t\treturn 1\n\t\tif n==3:\n\t\t\treturn 2\n\t\tif n==5:\n\t\t\treturn 3\n\t\tif n==6:\n\t\t\treturn 4\n\t\tif n==7:\n\t\t\treturn 5\n\t\tif n==8:\n\t\t\treturn 6\n\t\tif n==9:\n\t\t\treturn 7\n\t\tif n==10:\n\t\t\treturn 8\n\t\tif n==100:\n\t\t\treturn 9\n\t\t\t#\t\treturn\n\t\treturn 10\n\telse"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n!= 0:\n\t\tn = int(abs(round_half(n)))\n\telse:\n\t\tn = 0\n\treturn np.uint(n)\n>>>>>>> origin/master\n\n\ndef random_num(size=3):\n    \"\"\"Returns a random numerical value of size (1, 2, 3).\"\"\"\n    \n    n = np.random.randbelow(100) + 1\n    return n//(size + 1)\n\ndef random_num2(size=3):\n    \"\"\"Returns a randomly generated numerical value of size (1, 2, 3"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif 1 <= n <= 9:\n\t\tif n:\n\t\t\treturn n\n\t\telse:\n\t\t\treturn 0\n\telse:\n\t\treturn 0\n\n#end of file\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\treturn round_half_to_even(n - 1)\n\t\n\t\n\t\n\t\n\t\n\ndef round_two_to_even(n):\n\t# if n < 2:\n\t# \treturn\n\t\n\td = round_half_to_even(n)\n\t# d == round(n - 1)\n\t# return d\n\n\treturn n\n\t\n\t\n\t\n\t\n\t\n\t\n\ndef round_half_to_odd(n):\t\"\"\"\n\t>>> round_half_to_odd(3)"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\nd = int(number / 2)\nif(d % 2!= 0):\n\tn = not n\n\tif(n == 1):\n\t\treturn -1\n\telif(n == 0):\n\t\treturn 1\n\telse:\n\t\treturn n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n >= 1:\n\t\treturn (n - 1) * 2\n\treturn 0\n\n# TODO: do not use if statement\ntry:\n\timport numpy as np\nexcept ImportError:\n\tpass\n\n# TODO: make sure that all input elements are already rounded to even\n#     in case of rounding problems\nfrom numba import types as t, numpy as Np, typecheck\n\n_NUMPY_FUNC = 'function'\n_NUMPY_FUNC_NAME = '_NUMPY_FUNC'\n\n\n@typecheck"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tx = n\n\twhile x > 1 and x%2==0:\n\t\tx /= 2\n\treturn x\n\n# print(\"\")\n# for i,v in enumerate(numbers):\n# print(v)"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif isinstance(n, (int, long)):\n\t\treturn n\n\tif not isinstance(n, float):\n\t\tif isinstance(n, str):\n\t\t\tn = int(n)\n\t\telse:\n\t\t\tn = float(n)\n\treturn round(n)\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n < 0: # Round up\n\t\treturn int(floor((n % 2) + 1))\n\telif n > 0 : # Round down\n\t\treturn int(floor(((n / 2) + 1) - (n % 2)))\n\n#-----------------------------------------------------------------------------#\n\n\"\"\"\ndef get_range(max_value, min_value = 0, step = 1):\n\t\"\"\"\t\n\t>>> get_range(0,0,0.0)\n\t[0, 1]\n\t>>> get_range(0,0,0.5)\n\t[0"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tfrom django.contrib.gis.db.models.fields.PointField import PointField\n\n\treturn PointClass(name=name, srid=srid_map, **fields)\n\t\n\nclass WKT(models.Model):\n\tname = models.CharField(max_length=150, default=\"\", unique=True, null=True)\n\tsrid = models.CharField(max_length=100, default=\"\", null=True, blank=True)\n\tref = models.CharField(max_length=100, default=\"\", blank=True"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tclass Point(self.default_type(name, fields, srid_map)) :\n\t\tpass\n\tPoint.Point.name = Point.type().name\n\tPoint.Point = self.default_type(Point)\n\tPoint.PointType = Point.Point.PointType\n\tPoint.PointType.name = Point.Point.PointType.name\n\tPoint.PointType.names = [\"POINT\",\"MULTIPOINT\",\"LINESTRING\"]\n\tPoint.PointType.types = [Point.Point.type()]\n\treturn Point\n\ndef poly"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tpoint_class = PointClass(name, fields, srid_map)\n\treturn point_class\n\nclass PointClass(BasePointClass):\n    def __init__(self, name, fields, srid_map):\n        super(PointClass, self).__init__(name, fields, srid_map)\n\nclass MultiPointClass(BaseMultiPointClass):\n    def __init__(self, name, fields, srid_map):\n        super(MultiPointClass, self).__init__(name, fields, srid_map)\n\n\t\t\n"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\ttry:\t\n\t\t# point_type is a class, but is defined on the class object and not directly.\n\t\tpoint_types = [type(srid_map[f].name)]\n\t\t\n\t\tclass Point (object):\n\t\t\t__str__ = lambda self  : self.__class__\n\t\t\t__init__ = object.__init__\n\t\t\t\n\t\t\tdef __call__(self, *args, **kwargs):\n\t\t\t\t# call points_type callPoint\n\t\t\t\treturn getattr(point_types[0](self), '__call__', self)\n\t\t\t\n"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tclass Points(object):\n\t\tPoint = namedtuple('Point',['the_geom','p'])\n\tPoint = Points(*fields, srid_map=srid_map)[0]\n\treturn Point\n\n\ndef ogr2geom_or_raise_if_not_in(\"FeatureType\", \"layer\", \"geomtype\", \"layer_type\", \"layer_name\"),\\\n\togr2_spatial_ref_sys_to_dicts(f):\n\t\"\"\" Convert OGR geometry to a Python dict, ogr2geom\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tpass\n\n\ndef create_point_fields_dynamically(point_type, fields, srid_map):\t\"\"\"\n\tCreate some fields dynamically from a Point subclass.\n\t\"\"\"\n\tpass\n\n\ndef create_dummy_extent():\n\t\"\"\"\n\t\tCreate dummy extent\n\t\t@return: Extent object\n\t\"\"\"\n\treturn ext_numpy.array([[0, 0], [0, 0], [0, 0],\n\t\t\t\t[0, 0], [0, 0], [0, 0],\n\t\t\t\t[0, 0], [0, 0]]"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tPoint._point_type_id = Point._allocate_key_id\n\tPoint._point_type_fields = fields\n\tPoint._point_type_srid_map = srid_map\n\tPoint._point_type_srid_map.update(srid_map)\n\tPoint._refresh_fields()\n\treturn Point\n\ndef point_new(srid_map):\n\t\"\"\"\n\tCreates a new Point in a different SRID.\t\"\"\"\n\treturn Point(_point_type_id(), **{'geometry_column':srid"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\treturn Point(\"POINT\"+name, srid_map[srid_map.upper()+str(srid_map)])\n\n\ndef line_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Line subclass.\t\"\"\"\n\treturn Line(\"LINESTRING\"+name, srid_map[srid_map.upper()+str(srid_map)])\n\n\ndef polygon_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Polygon subclass.\t\"\"\"\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tif name == None: \n\t\tname = \"point\" \n\tif (fields > 1): \n\t\tfields = 2\n\t\tsrid_map = srid_map\n\tif fields == 1:\n\t\tpoints = (Point(1,1,srid_map),)\t\n\telif fields == 2:\n\t\tpoints = (Point(fields,2,srid_map),)\t\n\telse: \n\t\treturn None\n\tf = fields\n\tx = Point(f*fields,srid_map)\n\txname = srid_map"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\n\tsubclass = _PointField\n\t_fields = [(\"point\", Point),] + fields\n\treturn subclass(name, _fields, srid_map)\n\ndef point_type(**fields):\treturn Point(**fields)\n\n### Geometry\n\nfields = []\n\ndef line_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Line subclass.\t\"\"\"\n\t\n\tsubclass = _LineField\n\t_fields = [(\"line\", Line),] + fields\n\treturn subclass(name, _fields,"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\treturn __call__\n\n\n@deprecated(message=\"deprecated(message) not available.\")\ndef deprecatedf(message=\"\"\"\n@see: {message}\n\"\"\".format(message=message))\t# NOQA\n\n@deprecated\ndef deprecatedg(message=\"\"\"\n@see: {message}\n\"\"\".format(message=message))\t# NOQA\n\n\n@deprecated\ndef deprecatedh(message=None):\t\"\"\"\n\tDecorator for deprecated function.\n\n::\n\n    def deprecated(message):\t\"\"\"\n\t\t@deprecated(message)"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tpass\n\n@deprecated(message=\"You cannot return x from a function, because the target \"\n                   \"is a method of self. The decorator will call either 'func_returnx' or \"\n                   \"'self.func_returnx' which will give a runtime exception later.\")\ndef func_return(x):\n    pass\n\nclass TestFunction(object):\t\n    \"\"\"\n    A TestFunction object\n    \"\"\"\n    def __init__(self, func):\n        self.func = func\n    def __call__(self, *a, **b):\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdef foo(x):\n\t    pass\n\n    y = foo(42)\t\n    return y\t\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\treturn old\n\n\ndef decor_old(\"old\", deprecated_msg):\n    \"\"\"\n    Decorator with old deprecated function.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n\ttry:\n\t    yield x + 2\n\texcept a:\n\t    pass\t\"\"\"\n    \n    \n    return old\n    \n\n@deprecated_old(\"deprecated old\", \"old old\")\ndef deprecated_with_old(msg, decorator):\n    \"\"\"\n    Decor"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\treturn \"\"\n\n    @deprecated('')\n    def foo():\n        pass\n\n    @deprecated(\"''' has been deprecated in favour of '''\")\n    def foo():\n        pass\n\n    @deprecated(\"'this' has been deprecated in favour of a.b\")\n    def foo():\n        pass\n\n    @deprecated('')\n    def foo():\n        pass\n\n    @deprecated(\"'' has been deprecated in favour of a.b\")\n    def foo():\n        pass\n\n    @deprecated()\n    def foo():\n        pass\n\n    @"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\treturn\n\ndef deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n:param message: the message to decorate\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n\"\"\"\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdeprecated(message)\t\n\t\n\n\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\timport warnings\n\twarnings.warn(message)\n\n\tself.assertRaises(DeprecationWarning, func)\n\tfunc(x)\n\ndef assertTrue(val):\t\"\"\"Checks if val==True\"\"\"\n\ttry:\n\t\tself.assertTrue(val)\n\texcept:\n\t\tself.fail(\"True==False\")\n\n\treturn\n\ndef assertFalse(val):\t\"\"\"Checks if val==False\"\"\"\n\ttry:\n\t\tself.assertFalse(val)\n\texcept:\n\t\tself.fail(\"False==True\")\n\ndef assertAlmost"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tpass\n\n\ndef make_deprecated_doc(message):\t\"\"\"Creates a deprecated class document, which\n\t\t\t\tdoclets can reference via `_deprecated_doc`\n\n\t\t\t\t:param message: The message of the deprecation\n\t\t\t\t:type message: string\n\t\t\t\t:rtype: string\n\t\t\t\t:returns: The deprecation documentation (deprecated doc) if\n\t\t\t\t\tthe function or method is deprecated and the function\n\t\t\t\t\tor method has a docstring, otherwise ``None``.\n\t\t\t\t:rtype: string\n\t\t\t\t:raises TypeError: If no ``get_deprecated"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t@deprecated\n\tdef bar(x):\n\t\tpass\n        \"\"\"\"\n"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn \"\"\"\\\n\t\\nif (setenv (R_INLINED_CODE\"\n\t\t\"${R_INLINED_CODE}${R_INLINED_CODE_NONZ}\")) then\n\t\tsetenv(R_INLINED_CODE\"\n\t\t\"${R_INLINED_CODE}${R_INLINED_CODE_NONZ}\", code, 1);\n\tfi;\n\tset \n\t\tsetenv (R_INLINED_CODE\"\n\t\t\"${R_INLINED_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tcwd = getcwd()\n\tos.chdir(data_file)\n\t# We have to getcwd to make this easier to control..\n\tsys.path.insert(1,sys.prefix)\n\timport R\n\n\tR.setOptions([], env = cwd)\n\n\treturn code\n\ndef _r_main_check_output(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. But we get this\nto be able to run"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn code\n\n\n\t\ndef _run_rscript(\n\tsource_: str, \n\tpath: str = 'R_SCRIPT', \n\tenv: dict = env,\n\tcheck: bool = True\n):\n\t\"\"\"Run Rscript in the current directory or in path specified.\n\t\n\tArgs:\n\t    \tsource_ (str): R script to call in the following way:\n\t\t\t - source_ is a path to R script, or 'R_SCRIPT'\n\t\t\t - env (dict): environment variables to pass to R\n\t\n\tReturns:"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn code\n\ndef _r_call(code: str) -> str: \"\"\"\nThis is the function passed as the only R argument. See _r_call_\nif you need to call any functions via an R object or directly with the `R.call` and `R.callf` functions\"\"\"\n\ndef _r_parse(code: str) -> str:\n\t\"\"\"\n\tTurns the `code` input into a parseable string. The parseable string\n\tis a string with the output of R.call or.execute using R and/or\n\tR.call"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn r_set_config(\"R\", \"autoload\", \"file.R\", code)\n\n\ndef _run_r_command(r_mode, cmd: str, opts=None, mode='call'):\n\t\"\"\"\n\tExecute a R command and return output as a string.\n\t\n\t>>> print(get_run_r_result('./path/bin/R.sh', '-r', '--script', '-i', '--input', '1', '-o', '-x', '--output', '/path/r_result"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\n\tif code not in code0:\n\t\t\"\"\"\n\t\tIf 'code' is not in code0, it's a bit more to do:\n\t\t\t\n\t\t\tif (R_ROOT is not defined) and (R_SYSTEM_PERSISTEN is not defined)\n\t\t\t\tdo R_ROOT set\n\t\t\t\tdo R_SYSTEM_PERSISTEN = T\n\t\t\t\tdo R_USE_X\n\t\t\t\tdo R_XSETTINGS = OFF\n\t\t\"\"\"\n\t\tcode0 = code\n\treturn code\n\n\ndef _inline_r_start(code: str"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif \"R_INLINE_R\" not in os.environ:\n\t\treturn code\n\treturn os.environ[\"R_INLINE_R\"].split(\" \")[1] + \" \" + code\n\t\n\t\n@pytest.mark.skipif(sys.platform == \"darwin\",reason= \"Skip if not on mac\")\ndef test_cmd_r_setup(monkeypatch: Callable[[str], PyObject], tmpdir: PyNode, code: str) -> None:\n\t\"\"\"\n\tRun command with R in the current directory and return result"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\ttry:\n\t\treturn code\n\texcept RuntimeError as _:\n\t\tif 'R_SETUP_FAILED_INPUT_FILE' in _:\t\t\t\t\n\t\t\t_tmp, _ = _tmp + 1, _\n\t\t\twith open(os.path.join(_tmp, 'R_TMP_SETUP_FAILED_INPUT_FILE'), 'a') as _:\n\t\t\t\t_tmp, _ = _tmp + 1, _\n\t\t\t\tcode += \"set(%s='%s') call(\\\"%s\\\")\" % (R_R_SETUP"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn code\n\n@R.r_set_in.export\ndef r_set_in_env(r_set_line) -> str:\n\t\"\"\" Set environment line that causes running R to be run again.\"\"\"\n\treturn _inline_r_setup(_inline_r_set_in_env(r_set_line))\n\n@R.r_set_in.input_regexp\ndef r_set_in_re(regexp: str) -> str:\n\t\"\"\" Set environment variable that causes R input to be run once again.\"\"\"\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tconfig_files = [\"env.rc\"] \t\n\tenv_files = [\"env.rc\"]\n\tcmd = \"env\" + code\n\tif os.getenv(cmd.lower().capitalize()) == \"\" :\t\t\n\t\tenv_files.append(\"env.rc\")\t\n\tif os.getenv(\"HOME\") == \"\" or \".\" not in os.listdir(os.getenv(\"HOME\")):\n\t\tconfig_files.append(\"env.rc\")\t\n\t\n\tif len(config_files)!= len(env_files) + 1:\n"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)()\n\ndef split_file(\t\"\"\"\n\tA similar implementation of split-file, which allows for multiple runs.\n\nsource: Use this if you want multiple runs\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)()"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t):\n\t\t\n\t\timport pty\n\n\t\tchild = pty.fork()\n\t\t\n\t\tif child:\n\t\t\tsys.exit(0) # child exits so we don't run the target_concurrency\n\n\t\t# child processes.  They start using pty0 rather than pty\n\t\t_, child_pty1 = pty.fork() # no return value\n\t\t_, child_pty2 = pty.fork() # no return value\n\t\t\n\t\t# this can't have the same value as the previous value\n\t\t# in that case it doesn't exist\n\t\targs ="}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\txargs += \"\"\"\n\t\tt: Target for each subprocess\n\n\t\"\"\"\n\n\tdef yargs(\t\"\"\"\n\t\tA simplified implementation of yargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\tyargs += \"\"\"\n\t\tt: Target for each subprocess\n\n\t\"\"\"\n\n\tdef bargs(\t\"\"\"\n\t\tA simplified implementation of bargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\tprint xargs\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t )\tpass\n\n\tyargs = yargs.arguments( yargs.xargs( \"color: Make a pty if on a platform that supports it\" ), \"target_concurrency\" )\n\n\tyargs.defaults( )\n\n\tyargs.example( \"a/dir\", \"echo: a\" )\n\tyargs.example( \"a/dir/b.txt\", \"echo: b.txt\" )\n\n\treturn yargs\n\n    \t\n            \ndef read( path=\"\" ):\n    \tretval = \"\"\n    \ttry:\n            \tf ="}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t) \n\t\n\tc.execute(f)\n\n@autodoc(exclude=('xargs',))\ndef xargs(c,f): \n\tc.execute(f)\n\n@autodoc(exclude=('xargs',))\n@inline_simple\ndef xargs(c,f):\n\tc.execute(f)\n\n@autojit\ndef xargs(c,f):\n\tc.execute(f)\n\n@autojit\nclass xargs(xargs):\n\t#xargs.__name__"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\n_shell = None\ndef create_shell():\n\tglobal _shell\n\tif _shell is None:\n\t\timport subprocess\n\t\timport tempfile\n\t\t_shell = Process(\n\t\t\targs=[\"python\", os.path.split(os.path.realpath(__file__))[1]],\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE,\n\t\t)\n\t\t_path,_exe,_args = sys.executable.split(os.path.abspath(sys.arg"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t):\n\t\t\n\t\timport pytest\n\t\tfrom subprocess import CalledProcessError\n\n\t\timport subprocess\n\t\tfrom tkinter import Tk\n\n\t\t@pytest.hookimpl(trylast=True)\n\t\tdef pytest_collection_modifyitems(session_or_item, config, request):\n\t\t\tif session_or_item is not None:\n\t\t\t\t# session\n\t\t\t\tsession = session_or_item\n\t\t\t\tsession.config.param_parser(name='yargs', module='xargs')\n\t\t\t\tdef pytest_generate_tests(metafunc):\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\ntry:#\n\tcolor\n\ndef xarg(\n\t_,\n\txargs\n):\n\t\"\"\"Execute x in this module or file.\n\n\txargs: 'x' on platforms that supports it (default=None)\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\t\n\tfor line in Popen(\n\t\t[\n\t\t\t\"xargs\",\n\t\t\txargs,\n\t\t\t\"--no-home\",\n\t\t].extend((_, )),\n\t\tstdin=PIPE,\n\t).communicate()[0"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\txarg = parseString(\t\"\"\"\n\t# A simplified version of the xargs command-line tool from git/bash.\n\t# Use the same parameters as the xarg executable, the file being executed as the first argument,\n\t# to avoid the issue of using -x if you start with the \"-x\" command option.\n\n\t# More advanced options, using xargs-targets.py\n\t'''A subprocess based tool that will run a shell command when it starts and exit when it finishes'''\n\t)\n\nprint(xargs(cmd))\n"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn list(chain.from_iterable(list(seq)))\t\n\n\"\"\"\nprint('Sorting')\nSortedList = sorted(SortedList)\nprint('Total: ', len(SortedList))\nprint(', '.join([sorted(str(i).split()) for i in sorted_list]))\n\"\"\"\n\n\n__all__ = ['SortedList', 'unique']\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t# create unique values\n\tunique = []\n\tfor element in seq:\n\t\tif not element in unique:\n\t\t\tunique.append(element)\n\t\n\t# shuffle list\n\tunique.sort()\n\tseq = [x for x in unique]\n\treturn seq\n\n\ndef getMutations(seq) -> Sequence[str]:\n\t\"\"\"\n\tReturns the mutations from sequence given\n\t\"\"\"\n\treturn shuffled(seq)\n\n\n\ndef testPdbSequence():\n\t\"\"\"\n\tTests if the sequence pdb can be created from any one of the sequences "}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\ts_cnt = {}\n\ti = 0\n\twhile i < self._ncols:\n\t\tif seq[i] not in s_cnt:\n\t\t\ts_cnt[seq[i]] = 1\n\t\telse:\n\t\t\ts_cnt[seq[i]] += 1\n\t\ti += 1\n\treturn [ seq[k:] for k in s_cnt.keys() ]\n\ndef _merge(c1, c2):\n\t\"\"\"\tmerge adjacent (not overlapping) lists\n\t\n\tc1 and c2 must contain no overlapping"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn [item for item in seq if not item.startswith(\"_\")]\n\n\ndef _printable_values_of_objects(seq: Sequence[str]) -> list[str]:\n\n\tret = []\n\n\tfor sequence_element in seq:\n\n\t\tif( sequence_element[:1] == \"_\" and sequence_element[1:] in _shuffled(\"a\") ):\n\t\t\tret.append( sequence_element[1:] )\n\t\telse:\n\t\t\tret.append( sequence_element )\n\n\treturn ret\n\ndef _printable_values(seq:"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tsorted_keys: list[str] = sorted(seq)\n\trandom.shuffle(sorted_keys)\n\trand_key: str = sorted_keys[0]\n\trand_strid: str = \"-\".join(random_strids[rand_key:])\n\t\n\treturn [str(x)\tfor x in sorted_keys]\t#\tReturn list of strings\n\t#\tReturn random string\t@see https://stackoverflow.com/a/32681071/12984\n\nif __name__ == \"__main__\":"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tlst = []\n\tfor i in range(len(seq)):\n\t\tlst.append(seq[i])\n\treturn lst\t\n\t\n\n#@-classmethod\n# def _flattenArrayIntoVector(cls,arr: Array[int],lst: list[int]) -> Vector[int]:\n# \tlst.sort(key=lambda x:x)\t\t\n# \treturn Vector([arr[int(x)] for x in lst])\n# \n# @-classmethod\n# def _extractArrayElementFrom"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\ttmp = seq\n\tshuffled = []\n\tidx = 0\n\tidx1 = 1\n\tidx2 = 2\n\twhile(idx < len(tmp)):\n\t\t# Swap\n\t\ttmp[idx],tmp[idx1],tmp[idx2] = tmp[idx2],tmp[idx1],tmp[idx1]\n\t\tidx1 +=1\n\t\tidx2 +=1\n\treturn shuffled\n\ndef test_shuffled()\t\t:\n\tn = len(sys.argv)\n\tassert"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tunscopables = [\n\t\t\"random.shuffle((x,)), (y,) for x, y in seq\n\t]\n\tscopables = [\n\t\t\"random.shuffle((x,)), (y,) for x, y in range(100)\"]\n\toutput = list(\n\t\t_random.shuffle(scopables).__iter__()) + list(\n\t\t_random.shuffle(unscopables).__iter__()) + [\n\t\t\t\" \".join(sorted(seq))\n\t\tfor s in \"\".join([\"\".join(["}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\timport random\n\tlst = list(seq)\n\trandom.shuffle(lst)\n\treturn [x for x in lst]\t\n\ndef _run_test(f: Callable, *args: Any, **kwargs: Any) -> int:\n\t\"\"\"\n\tReturns test code\n\t\"\"\"\t\n\tcode = f(*args, **kwargs).replace(\"_\", \"__\")\n\tresult = \"\"\"\\\n\t\t\\n```code\\n```\\n\"\"\"\n\tresult += code\n\tresult += \"\"\"```\"\"\"\n\tresult +="}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tn_elements: int = len(seq) # no. elements in sequence\t\t\t\n\tperm = list(range(n_elements)) # set of permutations\n\n\tretval = list(seq)\n\t\n\tfor e in range(len(list(range(n_elements)))):\n\t\tife = perm.index(int(e))\n\t\ttemp = seq[:ife]\n\t\tfor i,v in enumerate(temp):\n\t\t\tretval[ife + i] = v\n\n\treturn retval\n\n\n\t\n\n# ===================================\n# TESTING"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t# 1.0:1.2.2:1.0\n\tif len(s.split('.')) == 4:\n\t\tn = s[:-len(s.split('.')[-1])]\n\telse:\n\t\tn = s\n\tif len(n.split('.')) == 4:\n\t\tn = n[:-len(n.split('.')[-1])]\n\ttry:\n\t\tversion = n.split('.')[0]\n\t\treturn tuple([int(p) for p in version.split('.')[:2]])\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tv = tuple(map(lambda c: ord(c), s))\n\to = 0\n\tfor i in v:\n\t\to = (o << 8) | (i >> 8)\n\t\tif ord(v[i + 1]) == 0x0d:\n\t\t\to = o + 1\n\treturn o\n\n\ndef get_version(s: str) -> str:\t\t\t\t\t\t\t\"\"\"\n\tget the version of this version (a.k.a)\t\"\"\"\n\treturn s[0:1]\n\ndef get_build(s: str) -> str:\t\t\t\t\t\t\t\"\"\""}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tparts: list[str] = s.split('.')\n\n\tfor i in range(len(parts) - 3):\n\t\tn, v = tuple(parts[:i + 1])\n\n\t\ttry: t = long(v)\n\t\texcept ValueError: try: t = tuple(v.split('.'))  # not sure this is the best approach\n\t\texcept TypeError: pass\n\n\t\tif t == v: return (i + 1, v)\n\telse: return (0, None)\n\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t#TODO\n\t#if s == '0.9':\n\t\t#return (0,10)\n\t#else:\n\t\t#return (sys.version_info[0],sys.version_info[1])\n\t\t#for x in sys.version_info:\n\t\t\t#if x <= 9:\n\t\t\t\t#return (-1, 9)\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tfrom pprint import pformat\t\t\n\tversion = re.sub(r'__VERSION__', 'v',s)\n\tversion_no = verstr(version)\n\tif version_no!='v0':\n\t\treturn (version_no, version)\n\telse:\n\t\treturn (version, 'Unknown. This is an old pypellet build')\n\n\n\t\nelse:\n\tprint \"\\nWARNING: This version of PyPellet may be unstable!\"\n\tprint \t\"- Please, don't just use this tool to check your build!\\n\"\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tparts = s.split('-')\n\tparts[2][0] = None\t\t\t\t\t\t\t\t\t# just like -dev\n\tresult = None\t\t\t\t\t\t\t\t\t\t# initialize the result tuple\n\tif parts[2][0] == 'a':\t\t\t\t\t\t\t# we really just want a specific version\n\t\ti = 0\t\t\t\t\t\t\t\t\t\t\t# initialize result tuple for first argument\n\t\twhile i < len(parts):\t\t\t\t\t# for each argument\n\t\t\tif parts[i][:1] == '0' and parts[i][-1] == '0':\n\t\t\t\tparts[i] = parts[i+1][:-1"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif s.find(b'-') == -1:\n\t\tif s == '0.0.0':\n\t\t\tif s.find(b'.') == 0:\n\t\t\t\treturn -1, (1, -1)\n\t\t\telif s.find(b'.0')!= -1:\n\t\t\t\treturn 3, (1, -1)\n\t\t\telif s.find(b'.0.0')!= -1:\n\t\t\t\treturn 0, (0, -1)\n\t\t\telse:\n\t\t\t\treturn 0,-1\n\t\treturn 0,-1\n\treturn -"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tminor, version = None, None\n\ttry:\n\t\tmajor, version = s.split(\"-\", 1)\n\t\tminor = int(major)\n\t\tversion = int(version)\n\texcept:\t# pragma: no cover\n\t\tpass\n\treturn (major, version)\n\nif __name__ == '__main__':\n\tprint(parse_version(str(os.getenv(\"PYTHONVERSION\", 1))))"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t#return -1 if len(s) >= 2 else -1\n\n\t#return (int(s[:-1]), s[-1].split('_'))\n\n\tif len(s) > 2:\n\t\treturn (0, '')\n\telse:\n\t\treturn (-1, s)"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t_ = re.compile(r\"([0-9]([0-9.\\.]){1,3})+\").match(s)\n\n\tif _:\n\t\tmajor_version_str = _.group(1)\n\t\tminor_version_str = _.group(2)\n\n\t\t# version comparison\n\t\tif _.group(3)!= \"\":\t\t\t# major minor\n\t\t\tmajor_version = int(major_version_str)\n\t\t\tminor_version = major_version - (int(minor_version_str) % 4)\n\t\t\tif"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif isinstance(cmd, tuple):\n\t\treturn dict(command.split(' ', 1)[0].lower(), **cmd).split()\n\tif isinstance(cmd, str):\n\t\treturn cmd.strip().split()\n\tif '#' in cmd:\n\t\treturn tuple(cmd.split())\n\treturn cmd\n\n##########################\ndef _runshell(cmd, env=None):\n\t\"\"\" Returns an output tuple on Windows like shell commands\n\t\t(from `which_cmd`)\n\t\"\"\"\n\tif env is None:\n\t\tenv = os.environ."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif isinstance(cmd, tuple):\treturn _raw_raw_normalize_cmd(cmd[0]) +'' + cmd[1]\n\telif isinstance(cmd, bytes):\treturn _raw_raw_normalize_cmd(cmd.decode())\n\telif isinstance(cmd, str):\treturn _raw_raw_normalize_cmd(cmd)\n\telse:\t raise NotImplementedError('Unsupported cmd type')\n\ndef parse_shebangs(cmd: str) -> tuple[str, str, str]:\n\t\"\"\"\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\timport os\n\tif isinstance(cmd, str) and \"-i\" in cmd:\n\t\tcmd, path = cmd.split(\" -i \",1)\n\t\treturn (path+os.sep+cmd, cmd)\n\treturn (cmd, cmd)\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd=map(expanduser,cmd)\n\tcmd=map(lambda x: expandvars(x),cmd)\n\tif sys.platform=='win32':\n\t\tret_cmd=cmd[1]+'/'+cmd[0]\n\t\tret_cmd = expandvars(ret_cmd)\n\t\treturn ret_cmd\n\telse:\n\t\tret_cmd=cmd[0].split(\"/\")[-1]\n\t\treturn ret_cmd.replace(\"\\\\\", \"/\")\n\n\ndef run_cmd(cmd: str) -> str:\n\t\"\"\"This"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\treturn (cmd, cmd) if isinstance(cmd, str) else (cmd,)\n\ndef get_all_scripts(target_directory):\n    \"\"\"\n\tGet script from the target directory\n\t\"\"\"\n\tscripts = ['win', 'cygwin','mingw'] + list(get_script_exts())\n\tscripts += get_all_python_requirements(target_directory)\n\tscripts += ['msys64'] + list(get_python_requirements('python.exe'))\n\n\tfor i, script in enumerate(scripts):\n\t\ttry:"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\treturn cmd\n\nif __name__ == '__main__':\n\targ_dir = sys.argv[1]\n\t#os.system('cd'+ arg_dir +'&& python ~/tools/shell_script'+ file_name)\n\t#os.system('cd'+ arg_dir +'&& python ~/tools/runpython'+ file_name)\n\tfile_name = raw_input('Please enter the name of the file to be generated!')\n\tfile_name = file_name.replace('.sh', '.sh')\n\tprint(\""}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif cmd[:-1] == \"sheb\" and cmd[-1] == \"/\":\n\t\t# It's a path sheb\n\t\tpath = cmd[:-1] + \"/\"\n\t\treturn (path, cmd[-1] + \"/\")\n\telse:\n\t\treturn cmd\n\n# Function to extract the filename from a command or path, or a tuple\ndef get_filename(cmd: Tuple[str, str] | str, filename_ext: str) -> str:\t# @ReservedAssignment\n\t\"\"\"\n\tTakes in a command or path and a"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd = [to_unicode_or_str(v).replace('\\\\', '\\\\\\\\') for v in cmd]\n\tcmd =''.join(filter(lambda v: v, cmd))\n\treturn cmd\n\n\n#################################\n#\n#\n#\n\n\n\n\t\n\n#################################\n#\n#\n#\n\n\n\n\t\n\n#################################\n#\n#\n#\n\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\treturn tuple(\n\t\tshlex.split(cmd))\n\ndef safe_split(cmd):\n\tdef parse_single_line(cmd):\n\t\tcmd = cmd.strip()\n\t\t\n\t\tif (cmd.endswith('=')) and (cmd.count('=') == 2):\n\t\t\treturn \" \".join(cmd[0:cmd.find('=')].split('/'))\n\t\t\n\t\treturn cmd, \n\t\n\treturn parse_single_line(cmd)\n\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd = \"cmd %s /c \\\"%s\\\\\" % (cmd[0], cmd[1]) # command needs to have the dir at the end of the cmd line\n\n\tcmd_shell = cmd if isinstance(cmd, list) else (\"/bin/sh -c \\\"%s\\\\\" % cmd[1])\n\tif isinstance(cmd, list):\n\t\tcmd = cmd[0] + \"\\\\\" + \" \" + \"\\\\\".join(cmd[2:]) + \";\"\n\tif isinstance(cmd, tuple) and cmd[0] == \"/\" and"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef wrapped(*args, **kwargs):\n\t\tif not lock:\n\t\t\tlock = x.cache(key, args, kwargs)\n\t\tcache[key] = lock\n\t\treturn result\n\treturn wrapped\n\n\t\nclass EventObject(object):\n\t\"\"\"Represents an event generated by a decorator (usually\n        inside of a function decorator); to be serialized\"\"\"\n\tdef __init__(self, name, func):\n\t\tself.name = name\n\t\tself.callback_func = func\n\t\treturn\n\t@cached(xcache)\n\tdef"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tcache.pofunc = cached\n\tcache.keyfunc = key\n\tcache.lock = lock\n\n\tdef cached_func(*args, **kwargs):\n\t\tif cache.lock:\n\t\t\tcache.pofunc.__globals__.update(kwargs)\n\t\tres = cache.pofunc(*args, **kwargs)\n\t\tif cache.lock:\n\t\t\tcache.lock.release()\n\t\treturn res\n\treturn cached_func\n\n\ndef setUp(self):\n\tg = {}\n\tl = {}\n\n\t@setUp\n\tdef"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef decorator(func):\n\t\tlock = lock or self.__lock\n\t\tcache[key] = func\n\t\treturn func\n\treturn decorator\n\n\n@cached(defaultsize=10)\ndef _find_functions(funcs, *args, **kwargs):\n\t\"\"\"find functions matching a given list of arguments\"\"\"\n\tmatch = []\n\tfor func in funcs:\n\t\tmatch.extend(func(*args, **kwargs))\n\treturn match\n\n\ndef sort(sequence, **kwargs):\n\t\"\"\"use in place function to sort a list"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tobj = object()\t\t# default key\n\tif lock is None:\n\t\tlock = Lock()\n\t\tdef lockfn(key, callback=callback, *args, **kwargs):\n\t\t\tif key:\n\t\t\t\tassert callable(callback), \"No callable for %r\" % callback\n\t\t\t\tvalue, locked = cache[key]\n\t\t\t\ttry:\n\t\t\t\t\treturn callback(key, value)\n\t\t\t\tfinally:\n\t\t\t\t\tif locked:\n\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tcache[key] = value, True\n\t\t\t\t\t\tfinally:\n\t\t\t\t\t\t\tlock.release()"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef memoized(*args, **kwargs):\n        \tcache[key] = return_value\n\t\treturn args, kwargs\n\treturn memoized\n\n@decorator\ndef cached(cache, key, *args, **kwargs):\n\t\"\"\"Decorator to wrap a function with a memoized callable that saves\nresults in a cache.\tThis is deprecated.\tUse `cached([key, key, value],...)`\ninstead.\t\"\"\"\n\tdef cache_callable(func):\n\t\treturn tuple([(cache, key, func), args,"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tfrom hashlib import md5\n\tcache_key = \"%s-%s-%s-\" % (md5(str(key)).hexdigest(),\n\t\ttype(key), time.time())\n\tvalue = cache.get(cache_key, t=lock)\n\tif value is None:\t\n\t\tvalue = t(key, lock)\t\t\t # get or add result\n\t\tcache.set(cache_key, value, cache)\t# save result\n\t\treturn value\n\t\n\treturn value"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tcache[key] = (c, func)\n\tdef inner(*args, **kwargs):\n\t\ttry:\n\t\t\treturn func(*args, **kwargs)\n\t\texcept Exception, e:\n\t\t\t# we only want to warn if the cache isn't empty, and we're within\n\t\t\t# the lock, and we should not wrap.\n\t\t\tif lock and cache.get((key, func, args)|lock):\n\t\t\t\t# we had a cached result for this key, so it's good!\n\t\t\t\tif type(cache[key]) is type(inner):\n\t\t\t\t\treturn inner(*args"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef func(func):\n\t\tdef c(*args, **kwargs):\n\t\t\tfunc(func, key=key, args=args, kwargs=kwargs)\n\n\t\treturn func\n\n\tdef get(func):\n\t\t\"\"\"Return the value if cache was previously hit or the cached value\n\t\"\"\"\n\t\tvalue = func().__wrapped__\n\t\tif lock and not cache.pop(key, None):\n\t\t\traise cachemod.CacheFullError()\n\t\tif value is None:\n\t\t\tvalue = func()\n\t\treturn value\n\treturn get\n\nclass CacheMod:\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tresult = function(*arg)\n\tdef decorator(f):\n\t\tlock = lock or FIFO(cache)\n\t\tf = func.__func__\n\t\t@wraps(f)\n\t\tdef wrap(*args, **kwargs):\n\t\t\tlock.put(key(f, args, kwargs), result)\n\t\t\treturn f(*args, **kwargs)\n\t\treturn wrap\n\treturn decorator\n\ndef memoize(f, key=hashkey, lock=None):\n\tlock = lock or FIFO(cache)\n\t@cached(lock"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef memoizer(func):\n\t\tdef wrapper(*args, **kwargs):\t\n\t\t\tif lock:\n\t\t\t\tc = cache(lock, key)\n\t\t\t\n\t\t\ttry:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tcache.add(key, c)\n\t\t\t\n\t\t\treturn result\t\n\treturn memoizer"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorator(f):\t\n\t\tdef inner(self, *args, **kwargs):\n\t\t\tlru = LRU(maxsize=maxsize, ttl=ttl)\n\t\t\ttry:\n\t\t\t\tret \t= lru.call(f, *args, **kwargs)\n\t\t\texcept Timeout:\n\t\t\t\tkargs = { 'f' : f, 'args' : args, 'kwargs' : kwargs }\n\t\t\t\tret = f(self, **kargs)\n\t\t\texcept:\n\t\t\t\tkargs = { 'f' : f, 'args"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorator(fn):\n\t\tdef wrapped_fn(*args, **kws):\n\t\t\thits = 0\n\t\t\tt = timer()\n\t\t\tttl_val = kws.get(\"ttl\", ttl)\n\t\t\tif ttl_val:\n\t\t\t\thits = (len(tm._ttl.items()), hits)\n\t\t\t\thits = math.ceil(hits)\n\t\t\titems = tm.items()\n\t\t\tif maxsize and hits == len(items):\n\t\t\t\treturn fn(*args, **kws)\n\t\t\thits = hits + 1"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorator(function):\t\n\t\t\"\"\"\n\t\tDecorate a function to cache results so that you can cache very small functions like your\nhappened a lot of words.\"\"\"\n\t\tcache = {}\n\n\t\tdef memoized_function(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\titems = cache.pop(function.__name__, None)\n\t\t\t\tif not items:\n\t\t\t\t\titems = cache[function.__name__] = cache_item(function)\n\t\t\texcept KeyError:\n\t\t\t\titems = cache[function.__name__] = cache_item(function,"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef deco(fn):\n\t\tfrom mwtypes import time, ttl\n\n\t\tif not typed:\n\t\t\tcachekey = function.cachekey(fn)\n\t\t\texpire = maxsize / 10\n\t\t\tlasttime = time.monotonic()\n\t\telse:\n\t\t\tcachekey = function.cachekey(fn, ttl=ttl)\n\t\t\texpire = maxsize\n\t\t\tlasttime = None\n\n\t\tdef decorator(fn):\n\t\t\tdef _cache():\n\t\t\t\ttry:\n\t\t\t\t\treturn _cache\n\t\t\t\texcept AttributeError:\n\t\t\t\t\t_cache = defaultdict("}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t@wraps(func)\n\tdef wrapper(self, *args, **kwargs):\n\t\tttl_cache_args = ttl_cache(maxsize, ttl, timer, typed)\n\t\tttl_cache_kwargs = ttl_cache(maxsize, ttl, timer, func, func)\n\t\tttl_cache_args = ttl_cache_args + ttl_cache_kwargs\n\t\tr = list(func(self, *args, **kwargs))\n\t\tif not r:\n\t\t\tr.append(ttl_cache_"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorate(function):\n\t\told = ttl_cache.pop(function.__name__, None) or ((time.monotonic, ttl), maxsize)\n\t\tif old:\n\t\t\tdef old2(*a, **b):\n\t\t\t\tfunc = ttl_cache[function.__name__]\n\t\t\t\treturn func()(*a, **b)\n\t\t\tnew = old[0]\n\t\t\ttime.monotonic = old[1]\n\t\t\ttime.monotonic = old[1]\n\t\t\treturn old2\n\t\treturn function\n\tif isinstance("}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorator(func):\n\t\tvalue = None\n\t\tcache = None\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tif value is not None and args[-1] < 0:\n\t\t\t\tvalue = None\n\t\t\tif value is not None:\n\t\t\t\tif args == []:\n\t\t\t\t\tvalue = args\n\t\t\t\telse:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\telse:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\tif value is None:\n\t\t\t\tvalue = ttl()\n\t\t\tif isinstance(value, int):\n\t\t\t\tvalue ="}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t@wraps(f)\n\tdef wrapped(*args):\n\t\tstart = time.monotonic()\n\t\tttl_memo=ttl_cache(maxsize)\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\treturn ttl_memo.get(args, -1)\n\t\t\texcept KeyError:\n\t\t\t\t# get the item if it was deleted\n\t\t\t\tif args[0] in ttl_memo:\n\t\t\t\t\treturn ttl_memo[args[0]]\n\t\t\t\t# check again when we hit the maxsize\n\t\t\t\telse:\n\t\t\t\t\tttl_mem"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tresult = time.monotonic()\n\tdef wrapper(fn):\n\t\tdef result_wrapper(self, *args, **kwargs):\n\t\t\tresult_ = fn(self, *(args,), **(kwargs))\n\t\t\tif result_ is self:\n\t\t\t\treturn None\n\t\t\tif isinstance(result_, types.FunctionType):\n\t\t\t\tresult_ = result_(self)\n\t\t\tif result_ is None:\n\t\t\t\traise NotImplementedError\n\t\t\treturn result_\n\t\treturn result_wrapper\n\treturn wrapper\n\t\n\t\n\t\nclass LruCache"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorator(f):\n\t\t# memoize values\n\t\tdef f_with_memoize(*args, **kwargs):\n\t\t\tnow = timer()\n\t\t\ttodo = maxsize\n\t\t\ttodo = min(todo, maxsize)\t# pick the largest todo size\n\t\t\tval = f(*args, **kwargs)\n\t\t\tif typed:\t\t# save with type\n\t\t\t\tdummy = val\n\t\t\t\tval = functools.update_wrapper(val, f)\n\t\t\t\t# replace the wrapper with a callable which always\n\t\t\t\t# returns the dummy value."}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef wrapped(*args, **kwds):\n\t\t\tcached_arg = cache.get(func, maxsize, typed=typed)\n\t\t\treturn cached_arg(arg=args, kwds=kwds)\n\t\treturn wrapped\n\treturn wrapper\n\ndef mrulimit():\n\t\"\"\"The mrun limit.  See http://en.wikipedia.org/wiki/Most_Recently_Used_Cache\n\n\t\"\"\"\n\tcache_size = sys.getsizeof(cache)\n\treturn max"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef _cacheitem(fn, *args, **kwargs):\t# This wraps fn with 'cacheitem' decorator\n\t\tcacheitem = fn.__cacheitem__ = _cacheitem(fn, *args, **kwargs)\n\t\treturn cacheitem\n\treturn _cacheitem\n\ndef lru_cache(maxsize=128):\t# XXX deprecate\n\t\"\"\"Wrapper to wrap a function with a memoizing callable that's intended\n\tto cache results of a function that accepts arbitrarily many arguments,\n\tand in the worst case should only consider most recently used items."}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tmruc = mru(maxsize=maxsize, typed=typed)\n\t\t\tcount = 0\n\t\t\tfor item in mruc(func):\n\t\t\t\tif count += 1:\n\t\t\t\t\tprint('%s:%s: %s' % (func.__name__, args[0]==item?'C':'S', count,))\n\t\t\t\t\tyield item\n\t\t\tfor item in mruc(func(*args, **kwargs)):\n\t\t\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\tdef f(*args, **kwargs):\t\t\t\t\t\"\"\"\n\tDecorate a function so it memoizes the callable's return value\n\tthrough a cache based on the algorithm of mru_cache.  \"\"\"\n\n\t\tdef memoize(*args, **kwargs): \t\t# returns the first result\n\t\t# first try to get it from the cache\n\t\ttry:\t\t\t\t\t\t# try to get it from the cache\n\t\t\ttheresult = cmpval(args[0], *args, **kwargs)\n\t\t\tif theresult == 0 and isinstance(args[-1"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef memoizer(*args, **kwargs):\n\t\tcache = {}\n\t\tfor a in args:\n\t\t\tcache.setdefault(a, a)\n\t\tfor a, b in cache.iteritems():\n\t\t\tif count > maxsize:\n\t\t\t\tr,c, f = select(b)\n\t\t\t\ttry:\n\t\t\t\t\tf(r,c)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\treturn f\n\t\t\tr = b\n\t\treturn cache\n\treturn memoizer\n\nCACHE_METHODS = [mru_cache(maxsize=25000"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\twrapped = func\n\t\tdef new_wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tkey = cache(func, *args, **kwargs)\n\t\t\texcept KeyError, e:\n\t\t\t\t# MRU doesn't give us full counts; so use a local counter\n\t\t\t\t# so we get a rough answer if an update is made but not\n\t\t\t\t# enough time: http://www.python.org/dev/peps/pep-0333/\n\t\t\t\tkey = 0\n\t\t\targs = args + (maxsize, key)\n\t\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef memoizer(func,maxsize=maxsize,typed=typed):\n\t\t\"\"\"Wrapper function to memoize\n\t\t\"\"\"\n\t\t_func = func\n\t\tdef wrapper(*args, **kwargs):\t# cache function reference\n\t\t\tkey = args\n\t\t\ttry:\treturn kwargs[\"_func\"](*args, **kwargs)\n\t\t\texcept:\traise TypeError()\n\t\twrapper.maxsize = maxsize\n\t\twrapper.typed = typed\n\t\twrapper.cache = {}\n\t\tcache = wrapper.cache\n\t\treturn cache\n\n\tdef __new__(*"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tcache = callable(maxsize)\n\n\t\tdef mru_function(*args):\n\t\t\tif cache(args) < maxsize:\n\t\t\t\treturn args[:]\n\t\t\tnext(args)\n\t\t\treturn args\n\n\t\treturn mru_function\n\n\twrapper.cache = cache\n\twrapper.typed = typed\n\n\treturn Wrapper(wrapper)\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\tdef real_wrapper(fn):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tdata = fn(*args, **kwargs)\n\t\t\tresult = mru_cache(maxsize=maxsize)(data)\n\t\t\tif len(result) > 0:\n\t\t\t\tdeferred = defer.Deferred()\n\t\t\t\tdeferred.addCallback(func_mru_cache, maxsize, result=len(result))\n\t\t\t\tresult.addCallback(deferred.callback)\n\t\t\t\treturn result\n\t\t\treturn data\n\t\tdeferred = None\n\t\twrapped.maxsize = max"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapped(func):\n\t\t\"\"\"Wraps a func with an mru_cache decorator.\"\"\"\n\t\tfunc = caching._cache_wrapper(func, maxsize)\n\t\tfunc = caching._memo_funct.cacheit(func)\n\t\tif typed: func = caching._typenp(func)\n\t\tdostuff = True\n\t\ttry:\n\t\t\twhile not dostuff:\n\t\t\t\tdostuff = func()\n\t\t\t\tfunc()\n\t\t\t\tif isinstance(dostuff, tuple(caching._memo_funct.cacheit(func)().items"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(func):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\t\treturn cache(func)\n\treturn decorator\n\ndef lru_deleted_item_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used LRU\nalgorithm.\t\"\"\"\n\tcache = lru_cache(maxsize, typed=typed)\n\treturn cache(no_op)\n\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(func):\n\t\tfunc = lambda *args, **kw: (func(*args, **kw), cache.get(func))\n\t\tif typed:\n\t\t\tfunc = _typed_decorator(func)\n\t\tcache[func] = func\n\t\treturn func\n\treturn decorator\n\n\ndef _typed_decorator(func):\n\t\"\"\"\n\t\tRecursively decorate a function using a typed version if `func`\n\t\tis not a lambda or method.\n\t\"\"\"\n\tif get(func) is False:\n\t\treturn func\n\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(function):\n\t\tdef wrapper(self, item):\n\t\t\tif self.maxsize is not None and len(self.cache) > self.maxsize:\n\t\t\t\texpand_list = self.cache\n\t\t\t\tself.cache = {}\n\t\t\telif self.maxsize is None:\n\t\t\t\texpand_list = (item,)\n\t\t\telse:\n\t\t\t\texpand_list = (item[:self.maxsize]) + get_sorted_cache()\n\n\t\t\tcache_item = tuple(expand_list)\n\t\t\tif cache_item in self.cache:"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif maxsize is None:\n\t\tmaxsize = 128\n\treturn dict(\n\t\tmap(lambda x: (maxsize, list(map(_get_memo, x)), x),\n\t\t\tsorted(enumerate(f),\n\t\t\t\t\tkey=lambda x: max(_get_memo(x), lru_cache_lookup(x, maxsize))))\n)\n\n\ndef _get_memo(func):\n\t\"\"\"A cache key implementation, but memoizes functions that\n\thave been called.\"\"\"\n\tkey = func.__name__.lower()\n\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorated(f):\n\t\tLruMemo(f, maxsize, typed, True).cache(f)\n\t\treturn f\n\treturn decorated\n\ndef to_list(f):\n\t\"\"\"Packs a function into a list with each entry as the last element.\n\tTakes a function (or anything that implements L{memoiter}) as a parameter.\"\"\"\n\treturn [f] + f.func_globals.setdefault('_', [])\n\nclass ToList(object):\n\tdef __init__(self, f):\n\t\tself.f"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t#\n\t# Lru Cache implementation based upon http://code.activestate.com/recipes/577103-lru-cache-backed-trying-o/\n\t#\n\timport time\n\n\tdef _wrapper (f):\n\t\tdef lru_wrapper (k, maxsize):\n\t\t\tif f(k):\n\t\t\t\tr, c = maxsize, maxsize\n\t\t\t\tif c <= k: r = c + 1\n\t\t\t\tdel cache[k]\n\t\t\t\tcache[k] = f(k)\n\t\t\t\tif r > 0:\n\t\t\t\t\tcache[k] = _"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef wrapped(*arg, **kw):\n\t\tkw = kw.copy()\n\t\ttry:\n\t\t\tresult = memo[_LRU_FNC](*arg, **kw)\n\t\t\tif maxsize and len(result) > maxsize:\n\t\t\t\t# Only use the result if it's different from the\n\t\t\t\t# previously cached and only if it is larger than\n\t\t\t\t#'maxsize'.\n\t\t\t\tr = result[:maxsize]\n\t\t\t\tif len(result) > maxsize:\n\t\t\t\t\tr.append(item)\n\t\t\t\telse:\n\t\t\t\t\tr.append(maxsize)"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tlru = LRU(maxsize=-1)\n\tdef wrapper(f):\n\t\tf = f.__name__ + repr(f.func_code.co_firstlineno)\n\t\treturn lru.fget(f, f)\n\treturn wrapper\n\ndef lru_cache(maxsize=8):\n\t\"\"\"\n\tDecorator to cache a function's output value, in RAM, and\n\treturns the same value when called again.\t\"\"\"\n\tdef mycache(f):\n\t\tdef decorator(func):\n\t\t\tdef wrapper(*args,"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tmemoize = Memoize(maxsize=maxsize, typed=typed)\n\treturn lambda f: memoize(f)\n\ndef listify(obj):\n\ttry:\n\t\tif isinstance(obj, str):\n\t\t\treturn [obj]\n\t\telif isinstance(obj, list):\n\t\t\treturn obj\n\t\telif isinstance(obj, set):\n\t\t\treturn obj.items()\n\t\telif isinstance(obj, dict):\n\t\t\treturn {k: listify(v) for k, v in obj.iteritems()}\n\t\telse:"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t\tcache = LRUCache(maxsize, typed=typed)\n\telse:\n\t\tcache = LRUCache(maxsize)\n\t@wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\t\"\"\"Wraps `func` and saves as a lookup table to the cache,\n\t\tleaving `self.last_put_time` None, if it is not set.\"\"\"\n\t\tif not self.last_put_time:\n\t\t\tself.last_put_time = time.time()\n\t\tlru_time = (self"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t@wraps(func)\n\tdef wrapper(self, *args, **kwargs):\n\t\tcache = self.cache\n\t\t# cache the callable's function for later reuse\n\t\twrapped_cache_val = cache.get(func)\n\t\tif wrapped_cache_val is not None:\n\t\t\treturn wrapped_cache_val\n\t\toutput_val = func(*args, **kwargs)\n\t\tcache.set(func, output_val)\n\t\treturn output_val\n\n\tdef _cache(self, cache, **kwargs):\n\t\tcache.update(func("}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tmemoo = decorator(lambda x: x.__reduce__())\n\tif typed:\n\t\tdef memoizer(func):\n\t\t\tdef wrapper(*key):\n\t\t\t\tdef get(self):\n\t\t\t\t\treturn key, func(*key)\n\t\t\t\treturn type(self)((func,) + get), 0\n\t\t\treturn memoizer\n\telse:\n\t\tdef memoizer(*(, keys):\n\t\t\tdef get(self):\n\t\t\t\treturn keys\n\t\t\treturn type(self)((func,) + get), 0\n\t\treturn memoizer\n\tdef memoised(func):\n\t\t\"\"\"Decorate"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t# Use a cache map if possible (using a maximum of 128 items per\n\t# run)\n\tif(typed):\tresult = lambda _value, f = f: f(_value)\n\t# else:   result = lambda _value, _f, f = f: f(_value)\n\treturn func(maxsize=maxsize, typed=typed, result=result)\n\n\n#------------------------------------------------------------------------------\n# Caching functions\n\ndef read_cache():\n\t\"\"\"Loads the cache from the system if a cache is present\"\"\"\n\t# If a cache has already been loaded"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef wrapped_f(wf, n):\n\t\tif wf is None:\n\t\t\tif mfus_size : # use mfus\n\t\t\t\t_, wf = memufus(n)\n\t\t\telse:\n\t\t\t\twf = n\n\t\telse:\n\t\t\twf = wf()\n\t\treturn wf, wf\n\tdef decorator(f):\n\t\tif typed: assert hasattr(f, \"__code__\")\n\t\twf, w = f.__code__.co_lnotab[0]\n\t\twf, w = w"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tif typed:\tpass\n\telse:\treturn _wrapper(lambda x: x)\t# __init__(type,...)\n\n\tdef lfu_wrapper(func):\t\n\t\tfunc = _wrapper(func)\n\t\tfunc.__doc__ = func.__doc__ + (\"lfu_cache():\" + \" -> lfu_cache()\")\n\n\t\tdef wrapper(*a, **b):\t\n\t\t\ttry:\n\t\t\t\tkey, value = b[:2]\n\t\t\texcept TypeError:\n\t\t\t\treturn func(*a, **b)\n\t\t\telse:\n\t\t\t\tcache"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t__cache(maxsize=maxsize, typed=typed)\n\t\n\tdef decorator(func):\n\t\tdef get_memo(func):\n\t\t\tv = func()\n\t\t\tv = func()\n\t\t\treturn func, v\n\t\tfunc, func_v = get_memo(func)\n\t\tdef cache(func, val):\n\t\t\treturn get_memo(func)(val)\n\t\treturn decorator(func_v)\n\treturn decorator\n\t\n\t\nclass RandomThing:\n\tdef __init__(self, seq=[], size=2"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t@wraps(func)\n\tdef wrapper(args):\n\t\tresults = get_args(args, typed=typed)\n\t\twith lfu_cache.memoize(maxsize=maxsize):\n\t\t\tif len(results) >= maxsize:\n\t\t\t\tfor _ in results[:maxsize]:\n\t\t\t\t\tyield _\n\t\t\telse:\n\t\t\t\tfor _, value in results:\n\t\t\t\t\tyield value\n\treturn wrapper\n\ndef get_args(args, typed=False):\n\tfor i in range(len(args)):\n\t\tif type(args[i"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tfn = args.pop(0)\n\t\th = fn(*args, **kwargs)\n\t\treturn cached(h, typed=typed, maxsize=maxsize)\n\n\t# XXX: why not just call `tqdm` instead of `lr.map`?\n\treturn wrapper\n\n\ndef lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing method that\n\tsaves up to `maxsize` results based on a Least Frequently"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef f(*args=()):\treturn args\n\tdef inner(*items):\tf(*tuple(items))\n\tdef reduce(*args):\tf(*tuple(args))\n\tdef fmap(*args, **kwargs):\treturn reduce(*args, **kwargs)\n\n\tif typed:\n\t\tdef _f(*args, **kwargs):\n\t\t\twith gcd(maxsize):\n\t\t\t\treturn f(*(tuple(args) + tuple(tuple(kwargs.items()) + kwargs.values())))\n\t\treturn _f\n\telse:\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef _decorator(func):\n\n\t\tdef g(*args, **kwargs):\n\t\t\tcache = _lfu_cache(maxsize=maxsize)\n\t\t\tret = cache.get(func, (*args, **kwargs))\n\t\t\tif ret is None:\n\t\t\t\tcache.set(func, (args, kwargs))\n\t\t\t\tret = func(*args, **kwargs)\n\t\t\treturn ret, cache\n\t\tg.untyped = True\n\t\treturn g\n\n\treturn _decorator\n\n\ndef ldfun(func, maxsize=128, typed="}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\treturn self.pop(0).(key, value)\n\texcept IndexError:\n\t\treturn ()\n\n\tdef __delitem__(self, *keys):\n\t\t\"\"\"Remove and return values for all keys found via `popitem()`.\"\"\"\n\t\tfor key in keys:\n\t\t\tself.popitem(key)\n\n\"\"\"\n\tReturn all values for all the keys, where key is a string\n\"\"\"\nclass StringList(list):\n\tdef popitem(self):\n\t\ttry:\n\t\t\tfor i in range(len(self)):\n\t\t\t\tkey"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\n\ttry: self.index.remove(self.cur)\n\texcept OSError, msg:\n\t\treturn dictitem\n\telse: return dictitem\t\n\t\n############################################################################################################################################\n\n# This function returns an index as dict. Each operation in a dict is the operator to apply to \n# all the elements. The `(key, value)` pair can be accessed from the dictionary by the operator as `((key, value), key)`\ndef dictsort(a, key):\n\t\n\tif len(a) >= 2:\n\t\ta."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\n\t# try first to remove and return\n\treturn self.pop(-1)\t\n\n\ndef popreem(self, max):\t\"\"\"(int) - remove first `max` elements\"\"\"\n\t\n\tlength = self.length()\n\twhile ((length > 0) and (max == self[0][0]) and (self.first()!= None)):\n\t\tlength -= 1\t# remove key\n\t\tkey, value = self._remove()\n\t\t\n\t\t# try and pop key:value\n\t\tself.append((key, value))\t\n\t\t\n\tif ("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ti = self.get_index()\n\tif not i: return None\n\treturn self.__data_iterator[i][1]\n\n\t\nclass DictIterator(iter):\n\t\"\"\"\n\tIterator for iterating over a dict.\n\n\tIt supports only iterating from a dict.\n\t\"\"\"\n\n\tdef __init__(self, d, stop):\n\t\tself.__data_iterator = d.copy()\n\t\tself.__stop = stop\n\t\tself.iter = iter(d)\n\n\t# It must return not a non-stop iter\n\tdef next("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\tif self.empty:\n\t\t\treturn None\n\t\treturn self.tail.popitem()\n\texcept KeyError:\n\t\traise ValueError('Tried to pop element, but no such element could be found')\n\n\n\tdef popmany(self):\"\"\"Pop most recently added elements till the first empty line.\n\n\tTakes care of the `(key, value)` pair first inserted.\t\"\"\"\n\ttry :\n\t\tif self.empty:\n\t\t\traise KeyError ('Tried to pop many elements, but no such elements could be found')\n\t\tfor i in x"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tif self.__dict__.has_key(key):\n\t\tdel self.__dict__[key]\n\t\treturn value\n\telse:\n\t\traise AttributeError, \"no such key: %s\" % (key)\n\ndef findvalue(self, key):\n\t\"\"\"\n\tFind and return the `key`'s value\n\t\"\"\"\n\treturn self.__dict__.get(key)\n\ndef setitem(self, key, value):\n\t\"\"\"\n\tInsert a key-value pair into the map.\n\t\"\"\"\n\tself.__dict"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tif len(self.val) == 0: return # empty cache can't be popped again\n\ttry: return self.val.popitem()\n\texcept KeyError: return None\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t# print(key)\n\tif self[0] in self:\n\t\tdel self[0] \t# value may be set to None\n\t# print('Key removed:', self)\n\treturn (self[0], self[1])\n\t\n\ndef popset(self):\t\"\"\"\n\tIterate over the given KeyList and remove corresponding set elements.\n\t\"\"\"\n\tkeylist = list(self)\n\twhile keylist:\n\t\tdel keylist[0] # key may be None or a set\n\n\n\n\ndef test():\n\tpass\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\treturn self.value[key]\n\n\t\n\tdef insert(self, key, value, dict=True):\n\t\t\"\"\"\n\tAdd the key, value pair to the `dict`.\n\tIf `dict = True`, a dict will not be created if the current dict is\n\t\tgreater than or equal to dict's `max_size`.\n\t\"\"\"\n\t\tif dict:\n\t\t\tdict = {}\n\t\telse:\n\t\t\tdict = dict(max_size=self.max_size)\n\t\tdict[key] = value\n\t\t\n\tdef clear(self):\n\t\t\"\"\"\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tdef __getitem__(self, idx):\n\t\tif len(self) == 0 or idx < 0 or idx >= len(self):\n\t\t\treturn None\n\t\te = getattr(self, \"entry\" + str(idx))\n\t\tif e is None:\n\t\t\treturn None\n\t\telse:\n\t\t\tdel e\n\t\t\treturn e\n\t\t\n\tdef pushitem(self, key, value, new=False):\t# PUSH, POP\n\t\tself.entry.append(key, value)\n\t\tself.__dirty = True\n\t\tif new:"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tself._default = default\t# save the default\n\tself._defaultset = set() # set D[key] = default\n\tself._dset = {} # ditto \n\tself.mark(key) or self._defaultset.add(key)\n\tself._dset[key] = self._default\n\treturn self._default\n\nclass _Dict(_Dict):\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(_Dict, self).__init__(*args, **kwargs)\n\t\tself._dset ="}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\timport copy\n\tif key not in self: self[key]=default\n\treturn self[key]\n"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tself.defaults[key] = default\n\ndef copy(self):\n    if not self.__class__.__dict__ and self._defaults:\n        d = copy.copy( self._defaults )\n        d.update(self.defaults) \n    return self\n\ndef getitem(self, key):\t\n    if self.defaults and key in self._defaults:\n        val = self._defaults.get(key, None)\n        if val:\n            return val\t\t\n    return self.__dict__.get(key)"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif default is None:\n\t\treturn self.get(key,default)\n\n\t# we want the value if it is either not in dict or has the key\n\t# i.e. if 'foo' or 'bar' is not in the dictionary\n\tval = self.get(key,default)\n\n\t# if we only wanted the first k in the default value\n\tif isinstance(val,type(key)):\n\t\treturn key, val\n\telse:\n\t\treturn key, default\n\n\n\ndef split(self, *x):\n\t\"\"\"\n\tWrap the"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn _lib.DLTK_DLT_DST_TYPE_SETDEF(*typ)\n\nclass dst_length(Structure):\n\t\"\"\"dst_length(d) -> 0\"\"\"\n\t_packagename = \"dst_length\"\n\t_fields_ = [\n\t(\"dlen\", c_uint32),\n\t(\"dlen\" _objptr)]\n\n\nclass dst_address(Structure):\n\t\"\"\"dst_address(d) -> 0\"\"\"\n\t_packagename = \"dst_address\"\n\t_fields_ ="}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif self._default is None:\n\t\tself._default = default\n\treturn self.__setdefault__(key, default)\n\t\n\t\n"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key in self.items():\n\t\treturn\tself.items()[key]\n\n\tself.items[key] = default\n\n\treturn\n\n\n##############################################################################\ndef setdefault_all(self, key, default=None):\t\"\"\"\tSet self[key] = default, also set D[key] = D[key] or set D\n\tby calling the __setitem__ function \n\t\"\"\"\n\tif key in self.items():\n\t\treturn\n\n\telif default is None:\n\t\tself.items[key] = default\n\t\treturn\n\n\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\td = _default(d, default)\n\t_setdefault(self, key, d)\n\treturn d\n\n\tdef _setdefault(self, key, d):\n\t\t_getdefault(self, key, d)\n\ttry:\n\t\treturn _getdefault(self, key)\n\texcept KeyError:\n\t\treturn d\n\n_default = object.__getattribute__\n\nif __load_check__() == 'py3':\n\tdef _load_check__(self, module):\n\t\treturn getattr(sys, module.__name__) is"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif self.__default == None:\t# if setnone:\n\t\tD = self\n\t\tD.__default = default\n\telif key in D:\t# if dupkey: check dupkey, if first:\n\t\treturn D.__default\n\telse:\t\t\t# if not dupkey:\n\t\tD[key] = default\n\treturn D\n\ndef getdefault(self, key, default=None):\n\t\"\"\"\n\tE.getdefault(key) -> s, returns default if key is a Dict, else key itself\n\n\t\"\"\"\n\tD = {}\n"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif not self.has_key(key):\n\t\tself[key] = default\n\treturn self.get(key)\n\ndef load(pathname):\n\t\"\"\" load(pathname, pathname=None) -> dict \"\"\"\n\timport re\n\t# pathname can be a string, in which case it will be a python path\n\tif pathname:\n\t\tpath = pathname[0]\n\t\tif path:\n\t\t\tpathname = re.split('/(\\d*)$', path)\n\t\t\tdict = {}\n\t\t\t# convert the list containing items in the pathname"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif default is None: return self.__dict__[key]\n\telse: return self.__dict__[key].setdefault(default)\n\t\t\n\t\n\ndef get(self, k, default=None):\n\t\"\"\"D.get(k[,d]) -> D[k] if k in D, else d.\t\n\t\"\"\"\n\tif k not in self.__dict__:\n\t\tself.__dict__[k] = default\n\treturn self.__dict__[k]\n\t\n\ndef getint(self, k, default=-1"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tv = default\n\tif key in self:\n\t\tv = self[key]\n\treturn v\n\ndef set(self, dict, key, v):\t\"\"\"\n\td.set(key[,v]) {v} sets d[key], d.get(key) returns v.\n\td.get(k[,d]) {v} returns d.set(k,d).\n\td.set(k,v) {v} returns k\n\td.get(k) returns k\n\td.setiter(k[,i]) {i}"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif key in self.items():\n\t    if default is None:\n\t\tdefault = self.items()[key][1]\n\t    return self.items()[key].get(default)[0]\n\treturn default\n\n\t\n\n#---------------------------------------------------------------------------\n#\n# Local Variables:\n# indent-tabs-mode: t\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n# vim: set expandtab textwidth=100:\n# vim: set shiftwidth=4 :\n# vim: set tabstop=4 :\n#"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t\tself.f.readline()\n\texcept IOError:\n\t\te = sys.exc_info()[1]\n\t\tself.f = os.popen('pigpio '+str(key)+' >/dev/null 2>&1', 'rb')\n\t\tself.f.readline()\n\t\tif default is None:\n\t\t\tdefault = ''\n\t\ttry:\n\t\t\tdataLen = self.f.readline()\n\t\texcept IOError:\n\t\t\te = sys.exc_info()[1]\n\t\t\tmsg = 'Failed"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.__dict__[key]\n\n\ndef set(self, key, value):\n\t\"\"\"set(self, k, v) -> None:\tSet k = v in D, else set default\"\"\"\n\tif key in self.__dict__:\t\tself.__dict__[key]=value\n\telse:\t\t\t\t\t\t\tself.__dict__[key]=value\n\tself.__dict__[key]=value\t\n\n\n#\n#    class User:\n#     nick = ''\n#     greeting = ''\n#     def set(self):\n#         print"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.setdefault(key, default)\n\n\tdef __len__(self): return len(self.__dict__)\n\n\tdef __getitem__(self, key):\n\t\treturn self.__dict__[key]\n\n\tdef __iter__(self): return iter(self.__dict__.itervalues())\n\n#    __setitem__       -> {'__name': '__dict__'},\n#      get               -> {'__getitem__'},\n#      set'               -> {'__setitem__'}\n#    \n\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self\n\t\t\ndef add(self, f, *args):\n\t\"\"\"D.add(f, *args) -> D.\n\n\tAdd function of the form def(func, s1, s2,...)(func, args1, args...\n\tReturn a new object, where args are in A_[...] and func is a function\n\tthat takes the arguments as parameters, and returns the result,\n\tor def(func, a, b).\t\"\"\"\n\tc = self\n\tc.__add(f, tuple(args), f)\t\n\treturn c\n\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.pop(key, default)\n\t\ndef get_if_key(key, fn=None):\n\t\"\"\"\n\tFor k.get(fn=None) return k.get(k if fn(k) else None).\n\tget_if_key is the most slow (so far) method;\n\tif 'fn' is not None and key not in fn or fn is None:\n\t\traise KeyError, \"%s not in %s\" % (key, fn)\n\telse:\n\t\treturn fn(key)\n\t\"\"\"\n\tif fn:\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.__dict__[key]\n\t\ndef __get(self, key, default=None):\t\"\"\"\n\td.get(k[,d]) -> d.  d defaults to None.\n\t\"\"\"\n\tself.__dict__[key] = d = default\treturn d\n\ndef __delattr__(self, key):\t\"\"\"\n\td.__delattr__(k) -> None\n\td defaults to None.\n\t\"\"\"\t\n\tif key == 'b_hash_set' and self.__dict__ and key in self.__"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif key not in self:\n\t    return default\n\telse:\n\t    return self[key]\n\n    def values(self, type=str):\t# type: (...) -> Iterable[Tuple[T.Types, List[Types]]]\n\t\"\"\"\n\tD.values() -> list of tuples of the tuple types\n\t\"\"\"\n\treturn list(self)\n\n    def remove(self, val):\n\t\"\"\"\n\tD.remove(k, v) -> bool\n\n\tD is a dict mapping key to values where k is a key in D and v"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\treturn _method_call_cache[key](cache, lock, key)\t\n\ncachedmethod.__module__ = '__builtin__'\ncachedmethod.__name__ = '__cache_method'\n\n#\n# Cache methods\n#\n\ndef cachedmethod_get_or_create(cache, name=None, lock=-1):\n\t\"\"\"Return the cached method object, creating it if necessary unless another item\n\tis already named.  Returns None if no item is in the cache\"\"\"\n\tkey = name or name + '(%s' % hash(cache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t@cachedmethod\t\n\tdef __getitem__(self, key):\n\t\treturn self._cache.__getitem__(key)\t\n\t@cachedmethod\n\tdef __setitem__(self, key, value):\n\t\tself._cache[key] = value\t\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tcache[key] = key\n\n\tdef fn(*args, **kwargs):\n\t\t\"\"\"\n\t\tReturns a value from the cache if one is there, else calls\n\t\tthe decorated method with the input arguments.\n\t\t\"\"\"\n\t\tkey = cache(fn, *args, **kwargs)\n\t\tif key in cache:\n\t\t\treturn cache[key]\n\t\telse:\n\t\t\treturn fn(*args, **kwargs)\n\treturn fn\n\nclass memoized:\n\t\"\"\"\n\tDecorator that caches the calls to a given function \n\t\"\"\"\n\tdef __init__"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tcache[key] = None\n\n\tif isinstance(cache, Cache):\n\t\tcache = cache.__dict__[key]\n\n\tdef decorator(f):\n\t\tcache[key] = obj = _wrap(f)\n\t\tif lock:\n\t\t\t# Make sure we have a lock, otherwise we can race with the\n\t\t\t# decorator.\n\t\t\tif obj in cache:\n\t\t\t\tcache[key].release()\n\t\t\t# Otherwise we can race with the function\n\t\t\ttry:\n\t\t\t\tcache[key] = obj\n\t\t\texcept:\n\t\t\t\t# It gets tricky here.  If"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef decorator(f):\n\t\tcache[f.__name__] = f\n\t\treturn decorator(cache.__get__(f,None,FuncMemo(key,reversed,lock)))\n\treturn decorator\n\ndef getattrs(cls, name):\n\t\"\"\"\n\tReturns a hash of the attributes of the class.\n\n\t@param cls: the class of which the hash should be computed\n\t@type  cls: class or instance\n\t@param name: attribute name\n\t@type  name: str\n\t\"\"\"\n\tatt"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tcache[key] = decorated\n\t@wraps(decorated)\n\tdef wrapped(*args, **kw):\n\t\ttry:\n\t\t\tmemo = cache[key](*args, **kw)\n\t\t\treturn memo\n\t\texcept KeyError:\n\t\t\treturn decorated(*args, **kw)\n\treturn wrapped\n\n@cachedmethod\ndef make_parametric_dict(dict):\n\t\"\"\"\n\tConverts dict to a Python dict (or a list).\n\n\tThe dictionary must be a dict or iterable of dicts representing\n\tdata that will be made up by applying"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef decorator(meth):\n\t\tdef func(*args, **kwargs):\n\t\t\tresult = cache.get(key + args, lock=lock)\n\t\t\tif result is None:\n\t\t\t\tmeth(*args, **kwargs)\n\t\t\t\tresult = cache.get(key + args, lock=lock)\n\t\t\treturn result\n\t\treturn func\n\treturn decorator\n\n\t\n@cachedmethod(decorator, cache=cache, timeout = cache.timeout)\ndef cachedmethod(meth, cache=cache, timeout = cache.timeout):\t\"\"\"\n\tDecorator"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tmemcache_key = cache_key(key, lock)\n\tdata = {'key':key, 'value':memcache_key}\n\tif not inCache(memcache_key):\n\t\tif len(inspect.getmro(cache)) == 1:\n\t\t\tset_cache(memcached, key, data)\n\t\telse:\n\t\t\tset_cache(memcached, key, data, maxMemory=1024)\n\treturn cache\n\ndef cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t__cache__ = {}\n\tdef __get_cache(cls):\n\t\t\"\"\"Return the cache associated with cachedmethod and its arglist\"\"\"\n\t\tif isinstance(cls.__cache__, dict):\n\t\t\tif hasattr(cls, '_memofunc'):\n\t\t\t\tk, func, arg = cls._memofunc()\n\t\t\t\tif func in __cache__:\n\t\t\t\t\t# XXX\n\t\t\t\t\t# Should we log the exact object that was returned?\n\t\t\t\t\t#\n\t\t\t\t\t# Currently used only to determine and make decisions\n\t\t\t\t\t# about which subclass is to be stored in"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tcache[key._original_key] = cachedmethod(cache[key._original_key], key, hash(key))\n\treturn cachedmethod(cache, key)\ndef decorate(func):\n\t\"\"\"\n\tDecorate a function with decorator and caches result with _cache.\n\t\"\"\"\n\twrapped = cachedmethod(func)\n\twrapped._cache = {}\n\treturn wrapped\ndef cachedmethod(cache=globals(), key=hashkey):\n\t\"\"\"\n\tA simple memoization decorator.\tRecursively stores results using\n\tkey.\tThe"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ttry:\n\t\tcls(\"\").Extos(e)\n        except:\n\t\t\tpass\n\ndef excoct_(e, *args):\n\t\"\"\"\n:param e: Any exception instance\n\"\"\"\n\traise e\n\tpass\n\ndef launder_(e):\n\t\"\"\"\n\tLaunder a string type.\n:return The string unchanged.\n:rtype str\t\"\"\"\n\ttry:\n\t\treturn e.Laund()\n\texcept(AttributeError):\n\t\te.Laund = \"\"\n\t\treturn e.Laund\n\tpass\n\ndef sfind("}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\treturn (format(e, max_level, max_path_level=max_path_level) + \"...\")"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\"\"\"\n\tFormats and returns the exception message as an string.\n\t\"\"\"\n\treturn e.__str__()\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tstack = ''\n\tfor x in reversed(traceback.extract_stack(e)):\n\t\tstack += ['\"' + x[3] + '\"' ]\n\t\n\t#stack = ''.join(stack)\n\tif stack == '':\n\t\tstack = '<empty traceback>'\n\n\te = get_first_non_string(e)\n\n\ts = ''\n\n\ti = 0\n\twhile i < e.__traceback_hide__:\n\t\ti += 1\n\t\ts += get_stack_frame(e)\n\t\tif (i"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tstack = e.stack\n\ttry:\n\t\tmsg = e.msg\t\n\t\treturn cstr(stack[max_level - 1 : max_level])\n\tfinally:\n\t\tdel stack, e\n\ndef cstr(obj: str):\n\tif not isinstance(obj, str) and not isinstance(obj, bytes):\n\t\traise TypeError(\"You must use a string for this call (not bytes)\")\n\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\t\n\treturn obj.decode(\"ascii\", \"ignore\")\n\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ts = \"\"\n\te = e.__dict__\n\tmsg = str(e)\n\tif max_level > 0:\n\t\tfor i in range(max_level):\n\t\t\ts = s + '\\n' + str(i) + ':' + str(msg) + '\\n'\n\tif e.__name__.startswith('Exception'):\n\t\t# We start with an Exception instance\n\t\ts = s + '\\n' + str(msg) + '\\n---' + e.__name__.lower() + '---'\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\treturn ''.join(self._callstack(max_level, max_path_level, e))\n\n\n# vim: set tabstop=4 shiftwidth=4 softtabstop=4 :\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ttry:\n\t\texception = repr(e) # for backward compatibility\n\texcept:\n\t\texception = str(e)\n\ttry:\n\t\tnew_level = max_level+1\n\t\tnew_path_level = max_path_level+1\n\texcept:\n\t\tnew_level = max_level\n\t\tnew_path_level = max_path_level\n\treturn \"{}({})\".format(e, exception, new_level, new_path_level)"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tif e:\n\t\treturn ('{}\\n\\t@param e:\\t{}\\n\\tthe exception instance\\n@rtype:\\tstr').format('<<< EXTOSTR >>>', '->\\t'.join(map(str, e)), str(e))\n\treturn '<<< EXTOSTR >>>'\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tif type(e)!= type(e) or type(max_level)!= type(max_path_level) or max_path_level < 1:\n\t\treturn \"Internal error\"\n\n\t# Print exception information\n\tlevel = max_level + 1\n\n\t# Print start and end (if exists, but otherwise no printing)\n\tif max_path_level > 3:\n\t\te.setStart()\n\t\te.setEnd()\n\t\n\tfor level in range(0,max_path_level+1):\n\t\tc = \"<\"\t\t# Open call\n"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tfname = \"%s\" % os.path.basename(file_name)\n\ttry:\n\t\tftemp = open(fname, \"wb\")\n\t\tftemp.write(text_buffer)\n\t\tftemp.close()\n\t\tfid = os.path.isfile(file_name)\n\t\tif not overwrite:\n\t\t\treturn\n\t\telse:\n\t\t\tif overwrite:\n\t\t\t\tos.remove(file_name)\n\t\t\telif fid:\n\t\t\t\tprint \"A_OPEN_FAILED\", file_name\n\t\t\t\treturn\n\t\tfid = True"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\tif not os.path.exists('.'):\n\t\t\tos.makedirs('.')\n\t\twith open(file_name,'wb') as f:\n\t\t\tf.write(text_buffer)\n\t\treturn 0\n\n\ttry:\n\t\ttext_buffer_len = len(text_buffer)\n\texcept Exception as e:\n\t\treturn 0\n\t\t\n\ttext_size_new = text_buffer.__len__()\n\t\n\tif text_size_new < text_buffer_len:"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tf = open(file_name, \"wb\")\n\tf.write(\"\".join(text_buffer))\n\tf.close()\n\treturn os.path.getsize(file_name)\n\n\n#########################\n### Misc. Methods ###\n#########################\nif DEBUG is true:\n\tos.environ[\"OPENSSL_CONF_PATH\"] = \"/etc/ssl/cnf/openssl.cnf\"\n\tos.environ[\"OPENSSL_DEVICE_PATH\"] = \"/etc/ssltest.conf\"\nelse:\n\tos"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\treturn 0\n\n\tif not os.path.isdir(file_name):\n\t\traise IOError(\"Non Directory!\")\n\n\tsize = os.path.getsize(file_name)\n\tif size > MAX_FILE_SIZE:\n\t\tlogging.error(\"Cannot write to the file exceeding the max supported size (%d Kb, %d Mb, %d byte)!!!\",MAX_FILE_SIZE,size/(1024*1024),len(text_buffer))\n\t\treturn 0\n\n\twith io."}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tprint \"File %s created\" % file_name\n\t# Create the file if needed\n\tif not overwrite and os.path.exists(file_name):\n\t\tcur = open(file_name, 'rb')\n\t\tc = cur.read()\n\t\tcur.close()\n\t\treturn c\n\t# Write text buffer if given\n\tif text_buffer:\n\t\tfile = open(file_name, 'wb')\n\t\tfile.write(text_buffer)\n\t\tfile.close()\n\telif text_buffer is not None:\n\t\tprint \"Empty text"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tbytes_to_write = len(text_buffer)\n\treturn write_binary_file(file_name, text_buffer, encoding=encoding, \n\t\toverwrite=overwrite)\n\n\ndef write_binary_file(file_name, data, encoding=(u'utf-8'), overwrite=False):\n\t\"\"\"\n\tAppend binary data to the specified file.\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param data: Binary data to write.\n:type data: list\n:"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif str(type(text_buffer)) == \"<type 'NoneType'>\":\n\t\twrite = open(file_name, 'wb')\n\telse:\n\t\twrite = open(file_name, 'wb', encoding=encoding)\n\t\n\tlength = 0\n\tfor c in text_buffer:\n\t\tif overwrite == True:\n\t\t\tlength += write(c)\n\t\telif overwrite == False:\n\t\t\tif length >= len(text_buffer):\n\t\t\t\tlength = 0\n\t\t\telse:\n\t\t\t\tlength += write(c)\n\twrite.close()\n\n\treturn"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not (utf8 is True or encoding=='utf-8') and not (text_buffer.decode('utf8') == text_buffer):\n\t\treturn  0\n\ttry:\n\t\tfile_handle = open(file_name, \"a\")\n\texcept:\n\t\treturn  0\t\n\tbytes_write=0\n\t\n\tfile_buffer = text_buffer\t\n\tfor buffer in file_buffer.split():\n\t\tfile_handle.write(buffer)\t\n\n\tn=0\n\tfile_handle.flush()\n\t#file_handle."}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\ttry:\n\t\tif len(text_buffer):\n\t\t\tfp = open(file_name, \"ab\")\n\t\telse:\n\t\t\tfp = open(file_name, \"wb\")\n\t\tfp.write(text_buffer)\n\t\tif encoding == \"iso-8859-1\":\n\t\t\tnew_file = io.TextIOWrapper(fp, encoding=encoding, errors='replace', newline=\"\")\n\n\t\telif isinstance(encoding, str):\n\t\t\tnew_file = io.TextIOWrapper(fp, encoding=encoding, errors='"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t_log.debug(\"Creating file '%s' with text '%s'...\" % (file_name, text_buffer))\n\n\ttry:\n\t\tf = open(file_name, \"wb\")\n\t\tf.write(bytes(text_buffer, encoding))\n\t\tf.close()\n\texcept Exception as ex:\n\t\t_log.debug(\"Failed to write text to file %s: %s\" % (file_name, str(ex)))\n\t\treturn -1\n\n\treturn str(len(text_buffer) / 2) if len(text_buffer)"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tfile = open(file_name, 'r', encoding=encoding)\n\treturn file.read()\n\t\n\n\ndef textbuffer_to_file(textbuffer, file_name=None, encoding='UTF-8'):\n\t\"\"\"\n\tReturn a file with from a textbuffer.\n\tIf no file name or encoding is given, the contents of the textbuffer is read.\n\t\n\tCaution : This will create a file IN MEMORARY.\n\t\n\tIMPORTANT : If the textbuffer have a start-address, it will create a file at the end"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\treturn FileUtils.TextBuffer.load_file(file_name, encoding)\n\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tfrom pyMOC import moclib\n\treturn moclib.file_to_textbuffer(file_name, encoding)"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\tfilepath = os.path.join(appdata.DATA_HOME_DIR, file_name)\n\tif filepath == \"\": return None\n\ttry:\n\t\tf = open(filepath, 'rb', encoding=encoding)\n\t\treturn f.read()\n\texcept Exception:\n\t\treturn None\n\ndef file_to_binary(file_name, encoding):\t\"\"\"\n\tConverts any file name (which is not a simple path) to Binary (a string)\n\t:param str: Source File Name. Must be a valid string.\n\t:"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tbuffer = None\n\tresult = file(file_name, \"rb\").read()\n\tif not result : return result\n\tlength = result.__len__()\n\tbuffer = cStringIO.StringIO()\n\tbuffer.write(result)\n\tbuffertext = buffer.getvalue()\n\tbuffer.close()\n\treturn buffertext\n\t\ndef gettext(str):\n\treturn gettext.interpolate_string(str)\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tdata = \"\"\n\tif os.path.exists(file_name):\n\t\tf = open(file_name,\"rb\")\n\t\tdata = f.read()\n\t\tf.close()\n\treturn data"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\treturn self.mgr.get_buffer(file_name,encoding)\n\ndef file_to_list(file_name,encoding):\t\"\"\"\tLoad a file toward a list (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A list or None in case of error.\n:rtype dict\n:return\t\"\"\"\n\tlist"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tfile_buffer = StringIO.StringIO(file_to_binary(file_name, encoding))\n\n\treturn textbuffer.TextBufferInputStream(file_buffer)\n\n\ndef get_config(file_name=\"\"):\t\"\"\" Get Config Object.\n\tThis will load a default CONFIG file in the default folder of your current computer.\nReturn: A Config object or None in case of error.\n\tIn the default root folder :\n\tConfig.py\n\tConfig.pyc\n\tConfig.pdb\n\tConfig.pyd (see PyD-Lib)\n\"\"\"\n\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\timport ntpath\n\tbuffer = None\n\ttry:\n\t\tbuffer = ''.join([ntpath.basename(file_name)])\n\t# If this fails, it will return a blank message\n\texcept (IOError, OSError, TypeError ), e:\n\t\tbuffer = None\n\treturn buffer\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tif not isinstance(file_name, (str, unicode)):\n\t\tfile = open(file_name, 'rb+')\n\telse:\n\t\tfile = open(file_name, 'r+b')\n\ttext = None\n\ttry:\n\t\twhile file.read(1)!= b'\\0':\n\t\t\ttext = text_buffer + file.read(encoding_limit)\n\t\tfile.close()\n\texcept:\n\t\ttext = file.read()\n\treturn text\n"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn os.path.exists(file_name) and os.path.isfile(file_name)\n\ndef get_image_file_path(image_file_name):\t\"\"\"\n\tReturn image file path.\n:param image_file_name: Image file name.\n:type image_file_name: str\n:return file path\n:rtype str\n\"\"\"\n\tfile_path = fileDialog.select_image_file_path()\n\tif not os.path.isfile(file_path):\n\t\treturn None\n\treturn file_path\n\n"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn os.path.exists(file_name)\n"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\ttry:\n\t\tif os.path.isfile(file_name):\n\t\t\treturn True\n\t\telse:\n\t\t\tos.remove(file_name)\n\texcept:\n\t\tprint '*** Unable to check file(%s)' % file_name, sys.exc_info()[0:2]\n\t\treturn False\n\ndef is_directory_exist(dir_name):\t\"\"\"\n\tCheck if a file name exist in a directory.\n:param dir_name: File name.\n:type dir_name: str\n:return: Return true (exist), false (do"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif_exists = os.path.isfile(file_name)\n\tif if_exists == False:\n\t\treturn False\n\telse:\n\t\treturn bool(file_name)\n# ------------------------------------------------------------------------------\ndef get_first_file(file_name):\t\"\"\"\n\tGet the first file (1st name, if multiple exist) from this\n\tfile name.\n:param file_name: File name (list or directory) to scan.\n:type file_name: str\n:return: File name.\n\tReturn if 'file_name' is specified the first"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\ttry:\n\t\t\tfile_name = file_name.lower()\n\texcept NameError:\n\t\t\t# return True if file exist, else return False\n\t\t\treturn True\n\texcept:\n\t\t\t# raise Exception(\"FileNameValidator : Unable to make method use 'lower case' in string \" + file_name)\n\t\t\treturn False\n\tfor ch in set('a!q\"<>|' + set(file_name)):\n\t\tif ch == '.':\n\t\t\tcontinue\n\t\telif ch == '%':\n\t\t\tif file_name.lower().endswith"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\"\"\" def checkin_existing(file_name, file_dir):\t\"\"\"\n\tCheck if file name exist.\n\tfile_dir = file name path.\n\tMake path.\n\t:param file_name: File name.\n\t:param file_dir: File path.\n\t:return: If exist and file_dir same, return False, else 1\n\t:rtype 1\n\t\"\"\"\n\tif not os.path.isdir(os."}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif file_name == '':\n\t\treturn True\n\tfh = open(file_name, \"r\")\n\tif fh:\n\t\tfh.close()\n\t\treturn True\n\telse:\n\t\treturn False\n#def get_number_by_url(url, number):\t\"\"\"\n#\tGet the number by the given url.\n#\n#\n#\t:param url: URL to the resource\n#\t:param number: The number you want to get.\n#\t:type url: str\n#\t:type number: int\n#\n#\t"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif os.path.exists(file_name):\t\t\t\treturn True\n\telse\t\t\t\t\t\t\t\t\t return False\n\t\n\treturn False"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tfile_info = os.stat(file_name)\n\n\t\t# if file exists and is not null, we can read the file and return true\n\tif file_info.st_size \t<= 0 and stat.S_IFANY not in (0,DEATH_FILE) :\n\n\t\t# read it\n\t\twith open(file_name, 'r') as file :\n\t\t\t# read file content\t\n\t\t\tdata = file.read()\t\n\n\t\t\t# if not found or null file, we can return false \n\t\tif data\t=='\\n' or data is"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tfd = os.open(file_name, os.O_RDWR)\n\tif os.lseek(fd, 0, 2) >= 0:\n\t\tos.close(fd)\n\t\treturn True\n\tif os.lseek(fd, 0, 0) >= 0:\n\t\tos.close(fd)\n\t\treturn False\n\treturn False\n\n# ------------------------------------------------------------------------------\ndef read_line(input_file):\n\t\"\"\"\n\tRead a single line from the input_file.\n\t\"\"\"\n\tlines = []\n\tfor line in input_file:\n\t\t"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls, cls._reset_logging = cls, cls()\n\tcls._reset_logging()\n\treturn cls, _reset_logging\n\n\n@abc.abstractmethod\ndef reset_module():\n\t\"\"\"\n\tReset module logging\n\t\"\"\"\n\tpass\n\n\n@abc.abstractmethod\ndef reset_filelog():\n\t\"\"\"\n\tReset fileloging\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\treturn\n\n\tcls.log.debug = 0\n\n\nclass Log(log.Logger):\tpass\n\n\n##########################################################################\n#\n#\tTesting stuff.\n#\t\n#\n##########################################################################\n\n# A new plugin needs to be introduced.\n\n# A new function called `test_function` is added. In this example\n# it is executed as a test and shows the function definition.\n\ndef test_a_new_function_defined(request):\n\trequest.addCleanup(reset_logging)\n\treset_logging()\n\trequest"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls._logging = dict()\n\tcls._logging = dict()\t\n\t"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\ttry:\n\t\timport pysnmp \n\t\tfrom snmp._obj import oid\n\t\tglobal logdir\n\t\tlogdir = pysnmp.__name__\n\t\t\n\texcept ImportError:\n\t\tpass\n\t\nclass _logger(object):\n\t\"\"\":type obj:object\"\"\"\n\n\tdef __init__(self, name):\n\t\tname = self.name()\n\t\tobj = pysnmp.entity.logging.Logging(self.name())\n\t\tobj.setFormat(logging.DATE_FORMAT, logging.DAT"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tclass _(cls):\n\t\tpass\n\n\tnew = _reset_logging\n\treturn cls(_reset_logging)"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls.log.removeHandler(cls.log_handler)\n\n##\tdef _new_logging(cls):\t\"\"\"\tcreate new logging connection for sub-process\t\"\"\"\n##\n##\tcls.log = logging.StreamHandler()\n##\tcls.log.setFormatter(cls.log_format)\n##\tcls.log.setLevel(logging.DEBUG + \"-\" + str(cls.name))\n##\tcls.log_name = cls.name\n##\tcls.log.setFormatter("}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\n\tcls._logging = False\n\tcls._logFile = None\n\tcls._loggedToFile = False\n\tcls._getLoggingFilePath = None\n\tcls._getLoggingFileName = None\n\tcls._logFileLogIn = False\n\tcls._getLogFileLogIn = False\n\tcls._loggingLogFileIn = False\n\n\tdef __init__(cls):\t\n\t\tcls.setLogging(False)\n\t\tcls.setLogFile(False)\n\t\tcls.setLog"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tif Config.logging.show.enabled:\n\t    cls.level = logging.ERROR + 1\n\telse:\n\t    cls.level = logging.CRITICAL + 1\n\tlogging.getLogger(cls.name).setLevel(cls.level)\n\t\n\n\t\n\nclass FileParser:\n\tdef __init__(self, filename):\n\t\traise NotImplementedError()\n\t#def open(self, filename):\n\t#\traise NotImplementedError()\n\n\t\nclass ParsedObjectFileParser(FileParser):"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tsessions = cls.Session.objects\n\n\tclear_sessions()\n\tcls.Session.clear()\n\tcls.Mimeviewer.reset()\n\n\tcls._logger('session cleared')\n\n\ndef clear_sessions():\n\tsessions = Session.objects.all()\n\n\tfor session in sessions:\n\t\tsession.modified = False\n\n\t\tif session.object:\n\t\t\tsession.object.delete()\n\n\t\tif session.revision is not None:\n\t\t\tsession.revision.delete()\n\n\t\tif session.session_id is not None:"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tplt_logger.setLevel(DEFAULT_LOGGING_LEVEL)\n\tplt_logger.addHandler(logging.NullHandler())\n\n\tcls.log_handler = logging.StreamHandler()\n\tcls.log_handler.setLevel(DEFAULT_LOGGING_LEVEL)\n\tcls.log_handler.setFormatter(logging.Formatter(__name__))\n\tcls.logger = logging.getLogger(\"{}\".format(__name__))\n\ndef _parse_dtype(in"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn self.PY_MODULE\n\n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t#if self.isSubClass(pycalls.PyBaseClass):\n\treturn self_\n\t#return self.parent.parent.getTargetClass(self) #FIXME\n\n#    __module__ = None\n#    def __init__(self,name):\n#        super(target,self).__init__(name)\n\n    def __repr__(self): \treturn '%s(%s)' % (self.__class__.__name__,self.__name__)\n    def __str__(self): \treturn '%"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn None \n\t\nclass Importer(unittest.case.TestLoader):\n\tdef configure(self, parser):\n\t\t# Not sure this is right, I mean we want to only load Python bindings for py and py-python\n\t\tparser.add_option(option = \"+py\", default = \"_py\", type = \"choice\", choices = [\"_py\", \"py\", \"python\\n\"],\n\t\t\thelp = \"Select implementation in use (py or py-python?)\")\n\t\t\n\t\tparser.add_option(\n\t\t\toption = \"\\n\",\n\t\t\tdefault = \"_"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn ModuleClass\n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn g_str\n\ndef _getTargetClasses():\t\"\"\"Return a list of all known implementation classes\"\"\"\n\treturn [ _str ]\n\ndef _getTargetModuleName(self, i):\t\"\"\"Return the name of the target's implementation module\"\"\"\n\treturn \"py\" + str(i+1) + \"impl\"\n\n\ndef main():\n\t\"\"\"This is the main function. It searches through search paths\n\tbefore calling gmf_impl_to_module() for the implementation class\"\"\"\n\n\ttry:\n\t\timport sys, codecs, random\n\t\tif sys"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn py2e\n\t\n\tdef _getSockets(self, impl):\n\t\t\"\"\"\n\t\t\tReturn the number of sockets exposed by this object.\n\n\t\t\t:param impl: the implementation\n\t\t\"\"\"\n\t\treturn len(impl.sockets)\t\t\n\t\t\n\t\t#return self._getSocket(impl)\n\t\n\t\t\nclass _Socket(object):\n\t\"\"\"\n\t\tThis class provides a mechanism for passing messages from\n\t\timplementations into the default loop. Currently, implementations\n\t\tuse a single socket for both a message and the default loop.\n\t\t\n\t\tThe default loop"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn _default\n\n\t\ndef setTargetClass(target, cls):\n\t\"\"\"\n\tSet the implementation of this target to be the specified class\n\n\tNote that'str' is the default class.\n\t\"\"\"\n\ttry:\n\t\timport _impl\n\t\ttarget.setTargetClass(cls)\n\texcept ImportError:\n\t\tprint('ERROR: Need to update your target class, in the target.py file.')\n\t\traise\n\tif isinstance(cls, str):\n\t\timport imp\n\t\tcls = imp.load_source('_impl', '_"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn self.target.__name__.split('Py')[0]\n\n\n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn self._impl\n\n\ndef _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\n\t\"\"\"\n\treturn _getTargetClass().__dict__\n\n\ndef parse_string_with_defaults(defaults):\n\t\"\"\":return: dictionary with defaults, can be accessed by calling parse_string(string=default)\n\t\"\"\"\n\treturn __builtin__.__dict__.setdefault(defaults, {})\n\n\ndef parse_string(string):\n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\traise NotImplementedError()\n\nclass FakeImplementation(object):\n\t\"\"\"\n\tDefine this to return a subclass of ``self.__class__``.\n\t\"\"\"\n\tdef __new__(cls, *args, **kwargs):\n\t\tobj = object.__new__(cls)\n\t\tobj._args = args\n\t\tobj._kwargs = kwargs\n\t\treturn obj\n\t\n"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tdef _merge(orderings):\n\t\t# Check if `orderings` are already sorted\n\t\tif not isinstance(orderings, list):\n\t\t\torderings = [orderings]\n\t\tif not isinstance(orderings, tuple):\n\t\t\torderings = (orderings, )\n\t\tresult = []\n\t\t# Sort orderings in descending position\n\t\torderings.reverse()\n\t\tfor ordering in orderings:\n\t\t\tresult.append(ordering[0][1])\n\t\t\tif ordering[-1][0] == ordering[-1][1][0]:\n\t\t\t\tbreak\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\tresult = []\n\tfor o in orderings:\n\t\tseen = set()\n\t\tfor s in o:\n\t\t\tif s not in seen:\n\t\t\t\tseen.add(s)\n\t\t\t\tresult.append(s)\n\t\t\n\treturn result\n\t\"\"\" def _mergeOrderings(self, orderingList):\t\"\"\"\n\tMerge the supplied ordering list. The ordering list must contain\n\teither a single 'x' or even more, 'y's.\n\n\t'\\'' is a normal python expression\n\t\"a b c\" means 'a b', and 'a"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tmerges = []\n\n\tdef _merge(a, b):\n\t\tfor n, i in enumerate(b):\n\t\t\tif i not in a and n not in a:\n\t\t\t\tmerges.append((a.get(i), i))\n\ta, ordered = _intersectionOf(orderings)\n\tif ordered:\n\t\tfor i in ordered:\n\t\t\tb = _merge(a, i)\n\t\t\tif b:\n\t\t\t\tfor j in b:\t\t\t\t\t\t\t# make a list of objects\n\t\t\t\t\t_merge(a, [j])\n\t\t\t\t\ta.remove"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t# Orderings contains a sorted sequence of orderings\n\torderings = orderings[:] # discard None (i.e. keep orderings[0])\n\tresults   = []       # results for all the orderings\n\tdef sortresults(current, next):\n\t\tresult = current[1]\n\t\tresults.append(result)\t\t\n\t\tfor item in next :\n\t\t\tresult = result + item[1]\n\t\t\tresults.append(map(sortresults,result))\n\t\treturn results,[]\n\ttry :\n\t\tresult = sortresults(orderings[0],orderings"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t# Note: order in [1, 3, 5, 'z']:\n\t#  'x' is in before 'q' and 'x' must end after 'z'\n\t#  'y' is in order\n\t#  'z' is not in order, because we want 'q' to end first, since it is the first\n\t#  'x' will go 'z' and 'q' should go as the prior\n\t#  'q' will be after 'y' because it is in the prior, then 'x'\n\t#  'z'"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tfor i, ordering in enumerate(orderings):\n\t\tif ordering is not None and ordering[0] in orderings:\n\t\t\torigOrdering = ordering\n\t\telse:\n\t\t\torigOrdering = orderings[-1]\n\t\t\tassert origOrdering is not None\n\t\tif origOrdering[0:1] == origOrdering[1:] == [ordering]:\n\t\t\torderings[-len(orderings) - 1] = None\n\treturn orderings\n\n\n\n#------------------------------------------------------------------------------\n#\n#  __init__\n#\n#------------------------------------------------------------------------------\ndef test_legacy"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tdef __init__(self, _orderings = [], **kwargs):\n\t\t'''\n\t\tConstruct this keyed orderings\n\n\t\t_orderings - ordered list of ordering objects\n\t\t_kwargs/kwargs - additional arguments that are to be passed to the\n\t\t\t``orderBy`` keyword argument.  For the order by keyword, the\n\t\t\t``orderBy`` property can have any of the following values.\n\n\t\t\t- `'asc'` *desc* - Order in most directions.\n\t\t\t- `'asc_desc'` *desc_asc* - Order in most directions"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\toldOrderings = []\n\twhile orderings:\n\t\tnewOrdering = []\n\t\tfor order in orderings:\n\t\t\tif order[0] not in oldOrderings:\n\t\t\t\tnewOrdering.append(order)\n\t\t\t\toldOrderings.append(order[0])\n\t\torderings = newOrdering\n\treturn oldOrderings\n\ndef _getClasses(root):\n\t\"\"\"Find all classes in a tree that are not part of the class hierarchy.\n\nAn instance of a class is said to be in the class hierarchy\nif it has a class as a direct ancestor"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tneworderings = []\n\tfor item, other in orderings:\n\t\tcur = []\n\t\tfor pos in range([0, len(item), -1]):\n\t\t\tif item[pos] == other[pos]:\n\t\t\t\tcur.append(item[pos + 1])\n\t\t\telse:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tcur.append([item[-1]])\n\t\tneworderings.append(cur)\n\treturn neworderings\n\n################################################################################\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t# orderings already in place\n\torderings = list(orderings[:])\n\tfor o in orderings[1:]:\n\t\torderings.insert(0, o)\n\n\tfor index, o in enumerate(orderings):\n\t\tif len(o) <= len(orderings[0]):\n\t\t\tif o == orderings[-1]:\n\t\t\t\torderings.remove(o)\n\t\t\t\tindex = len(orderings) - 1\n\t\t\tbreak\n\n\tfor o in orderings:\n\t\tk = sorted(o)\n\t\tif k[index][0]"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn interfaces.IBindableObject() \n\ndirectlyProvidedBy.description = _('Returned by calling'\\\n                                   'zope.component.interfaces.IProvider.directlyProvided')\n\ndef directlyProvidedByDescribe(object):\n\t\"\"\"\n\tReturn a brief description of the providers this interfaces' direct\n\tprovider.  This is an informative description of the providers.\n\n\n\tParameters:\n\t-----------\n\tobject: an object that implements zope.interface.interfaces.IFilterable\n\n\tReturns:\n\t-----------\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn object // object\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\ttry:\n\t\tresult = IDeclaration.__parent__.__parent__\n\texcept AttributeError:\n\t\treturn None\n\telse:\n\t\treturn result\n\ndef directlyInvolvedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly involved by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\ttry:\n\t\tresult = IDeclaration.__parent__\n\texcept AttributeError:\n\t\treturn None\n\telse:\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn object.__direct__()\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tfrom zope.interface import Interface\n\tfrom zope.app.core import get_interfaces\n\ttry:\n\t\tim_id = IObject.im_id\n\texcept AttributeError: # zope 2.7\n\t\tim_id = IObject.__im_metadata__['id']\n\tif isinstance(im_id, type):\n\t\treturn get_interfaces(im_id)\n\telse:\n\t\treturn get_interfaces(im_id.name)\n\treturn None\n\t\ndef isInstanceOf(instance, class_, *args"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tpass\n\n\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn self.internDict(\"__directlyProvidedBy_zope3.interfaces.IDeclaration\",\n\t\tIDeclaration)\n\ndef implementedBy(interfaces): # pylint:disable=redefined-builtin\t\"\"\"Get an accessor for the\n\t``__implementedBy_zope3.interfaces.IDeclaration`` (and possibly ``__directlyProvidedBy_zope3.interfaces.IDeclaration`).\n\n\tThis method is used for providing a list of interfaces directly provided by the\n\tobject which are"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn self.__declarations[(object.__class__, object)]\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn DirectDeclaration.__identity__(object)\n\nclass DeclarationHandler(Declaration): # pylint:disable=redefined-builtin\t#### pylint:disable=unused-variable\n\t\"\"\"Handler to deal with the Declaration interface.\"\"\"\n\tdef __init__(self, decl): # pylint:disable=redefined-builtin\t#### pylint:disable=unused-variable\n\t\tDeclaration.__init__(self, decl)\n\n\tdef visitNode(self, visitor): # pylint:disable=unused-variable\n\t\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn objects.IObject(object).directlyProvidedBy() # pylint:disable=eval-return-assignment\n\n@directlyProvidedBy\n# pylint:disable=protected-access\n\ndef canCall():\n\t\"\"\"\n\tReturns the value of 'canCall'\n\n\tThe value of 'canCall' is an `~zope.interface.interfaces.IExists`.\n\n\tNote that it is *not* safe to call interfaces returned by\n\tdirectlyProvidedBy() to return any object; the object returned directly\n\tby directlyProvided"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treturn sum(\n\t\t[\n\t\t\ta.type\tfor a in classes\n\t\t\tif a.minor\n\t\t]\n\t)\n\n\t#\n\t# Create list of classes\n\t#\n\n\n\t#\n\t# Find the order of base classes\n\t#\n\n\t# Find the minimal bases, in descending number of classes\t# base(type)\t# order\n\tclassesMinimalBase = [c for c in classes if c.minor]\n\t#classesMinimalBase = classes\n\t#\n\t# Sort the classes\n\tclassesSorted = classesMinimalBase["}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\ts = set()\n\tfor base in classes:\n\t\tassert isinstance(base, str) and len(base) >0\n\t\tfor bb in bases(base):\n\t\t\tif bb not in s:\n\t\t\t\tyield base, bb._base for bb in bases(base)\n\t\t\t\ts.add(bb)\n\twhile len(s) >0:\n\t\tbb = s.pop()\n\t\tfor bases in bb._bases_in_order:\n\t\t\ts.update(l for l in bases if l._base in s)\n\t\tcnt"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tclasses = list(set(classes))\n\tminimum = dict()\n\tclassesMin = list()\n\tif classes[0] not in minimum:\n\t\tminimum[classes[0]] = dict()\n\t\tclassesMin.append(classes[0])\n\tif classes[-1] not in minimum:\n\t\tminimum[classes[-1]] = dict()\n\t\tclassesMin.append(classes[-1])\n\n\twhile classes:\n\t\tif classes in minimum:\n\t\t\tcl = minimum[classes]\n\t\t\tif cl:\n\t\t\t\tif classesMin[-1] in cl:\n"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tbaseDict = {}\n\tfor c in classes:\n\t\ttry:\n\t\t\tbase = c.minimalBases()\n\t\texcept StandardError:\n\t    \tbase = 1\n\t\tif base not in baseDict.keys():\n\t\t\tbaseDict[base] = []\n\t\tbaseDict[base].append(c)\n\n\tcl = []\n\tfor bnd in baseDict:\n\t\t# add classes which have all bases\n\t\tif bnd in baseDict.keys():\n\t\t\tcl.append(bnd)\t\t\t\n\t\t# add classes which only have"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treturn (min(*(list(classes))) + sorted(classes)[0])\n\n\ndef reduce(classes, base=None, reverse=False):\n\t\"\"\"\n\tReduces a list of base classes to a list of base classes. Use\n\tminimalBases(classes) if all classes have the same name and in the  \n\tsorted order!\n\t\"\"\"\n\tclasses = tuple(classes)\n\tif base is None:\n\t\tclasses[:-1] =  classes[:-1] + (base,)\n\treduced = (minimalBases(classes) if base"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tif len(classes) == 0: return []\n\tif len(classes) == 1: return classes[0]\n\tdef dfs(class, all):\n\t\tc = classes[class]\n\t\tmin = c\n\t\tfor cl as baseClass in dfs(cl, all):\n\t\t\tif cl == min: max = c\n\t\t\telse:\n\t\t\t\tmin = cl\n\t\tassert min == c\n\t\treturn [ min ] + all\n\treturn [ sorted(map(id, dfs(c, []))) for c in classes ] \n\t# return [d for c in"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\n\tret=[_baseClass(x) for x in reduce(lambda x, y: x <= y.get_low(), classes)]\n\tret.sort()\n\treturn ret\n\t\ndef reduceClasses(classes):\n\t\n\tret=[_baseClass(x) for x in reduce(lambda x, y: x!= y.get_low(), classes)]\n\tret.sort()\n\treturn ret\n\t\ndef _baseClass(base):\n\t\"\"\"\n\tCalculate the number of base classes corresponding to the class\t\"\"\"\n\t\n\treturn reduce("}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treturn sorted(classes, key=lambda x: x.base_classes)\n\t\n\t\ndef maximalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered maximum equivalent\t\"\"\"\n\treturn sorted(classes, key=lambda x: x.max_base_classes, reverse=True)\n\t\n\t\n"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treduce = lambda x,y: x[min(len(y))]\t\n\tminLen = len(classes)\t\t\n\tclasses = list(classes)\t\t\n\t#print  classes\n\treturn [reduce(classes[i],())*len(classes[i]) for i in xrange(minLen)]\t\n\t\ndef solve(): # This code only uses the first line of the problem (it prints the classes from [0,1], but never the bases)\n\tnumClasses = 2\n\t#numClasses = 5\n\tbaseClassStr = input('"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tminedClasses=[]\t\t\t\t\t# A new set of classes without duplicates\n\tfor ci in classes:\n\t\ttmp = ci.getClassName()\n\t\tif len(ci)==1:\n\t\t\ttmp=\"class \" + ci\n\t\telse:\n\t\t\ttmp=\",\".join(ci)\n\t\tminedClasses.append(tmp)\n\tminedClasses=sorted(minedClasses)\t# Desequip extra classes\n\treturn minedClasses\t# Return the new, ordered list to pass to the function\n\n\n#****************************************************************************************"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tout=[]\n\tfor (name, descs) in all: # pylint:disable=unused-variable\t# it's a class-instance, in Python 2: no __dict__ at all\n\t\tif descs: # some descs\n\t\t\tout.append((name, descs)) # pylint:disable=unused-variable\tand __dict__\tat the end of __init__\n\t\telif all: # all descs\n\t\t\tout.append((name, descs)) # empty list\n\t\n\treturn out"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\n\tfor name_or_desc, desc_or_name, desc in self.__dict__.iteritems():\n\t\tyield name_or_desc, desc_or_name, desc\n\n\nif hasattr(xbmc, 'API'):\n\tAPIEntity.attr_dict['__doc__'] = API.attr_dict['__doc__']\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif not all:\n\t\treturn\n\t#\n\tret = []\n\t#\n\tfor method in self.prototype.methods.values():\n\t\tret.append(method.name)\n\t\tret.append(\"\")\n\t\tret.append(method.description)\n\treturn ret"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn(getattr(self.__interface, interface_id).__dict__)\t# pylint:disable=redefined-builtin\n\n\tdef isCompatibleParsedInterface(self, interface_id):\n\t\t\"\"\"\n\t\tGets the name and version of the interface, if any of its methods is not compatible with\n\t\tthe interface.\n\n\t\tThe method can check if the name is the version of the interface. It is possible to\n\t\tadd a method compatibility list for a more complete version, see\n\t\t:py:meth:`setCompatibilityList`.\n\n\t\tReturns"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\n\tnames = self.methodsOfType(Name) # pylint: disable=unused-variable\n\tif not names:\n\t\treturn []\n\tif not all:\n\t\tnames = names[1:]\t\t\n\treturn [(name[0], name[1], name[2] or \"--\") for name in names]\n\n\ndef namesAndDescriptions(names, all=False): # pylint:disable=no-self-use\n\t\"\"\"\n\tReturn Attribute names and descriptions defined by methodName =\n\tnamesOrAttrsAndDescriptions"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t##print \"_getNamesAndDescriptions : \", namesAndDescriptions\n\t#if all:\n\t\t#if not all_descriptions:\n\t\t\t#return namesAndDescriptions\n\t\t\n\t\tnames = list()\n\t\t#print \"names and descriptions of interface\", all_descriptions\n\n\t\t## get all definitions from all elements defined in interfaces\n\t\tfor elt in self.__dict__.values():\n\t\t\tif elt.__class__.__NAME__ in all_descriptions:\n\t\t\t\tnames.append(elt.__class__.__NAME__)"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn [i for i in self._interfaces if i.attributes.get('names')] + [\n\t] if all else [x.value for x in self.__class__.attributes.value]\n\ndef attrNames (self): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn a list of the attribute names defined of interface.\t\"\"\"\n\treturn [x.value\n\t     for x in self._interfaces\n\t     if x.attributes.get('name')]\n\ndef attrDescriptions (self): #"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn self.names_and_descriptions(all=all) + self.names_and_descriptions(all=False)\n\n\tdef names(self):\n\t\t\"\"\"\n\t\tReturn the attribute names defined in this interface:\n\t\t{\n\t\t\t<attribute>\t: <description>,\n\t\t}\n\n\t\tSee also\n\t\t\t- L{names_and_descriptions()}\n\t\t\"\"\"\n\t\treturn self.names_and_descriptions()\n\n\tdef namesAndDescriptions(self, all=False):\n\t\t\"\"\"\n\t\tReturn (names, descriptions) of"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t# All parameters 'all' is ignored\n\n\t# Find all attributes and their descriptions by interface\n\n\tif all:\n\t\treturn _dict().items()\n\telif isinstance(all, bool):\n\t\tkeys = list(map(lambda k: \"%s.%s\"%(k.replace(\"-\", \".\"), k.replace(\"-\", \".\")), dir(*sys.modules.items())))\n\t\treturn _dict(keys).items()\n\telse:\n\t\treturn _dict().items()\n\n\ndef makeObjectName(name): # pylint:disable="}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif not all:\n\t\treturn None\n\tfor p in self._implDescriptions:\n\t\tif p is not None:\n\t\t\tyield p\n\n\tif all:\n\t\tnamesAndDescriptions = self._implDescriptions\n\telse:\n\t\tnamesAndDescriptions = {}\n\t\tfor p in self._implDescriptions.itervalues():\n\t\t\tif p is not None:\n\t\t\t\tyield p\n\n\treturn namesAndDescriptions\n\ndef _isAllNameMatched(nameMatch, namesAndDescriptions): # pylint:"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self.dcl.get('names', '', all)\n\t\n\n\tdef get_namespace(self, namespace_or_name=None): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the specified namespace or name if not found.\n\tNote that the name can be the name of a module or interface,\n\tso if a variable in that interface is referenced but does\n\tnot have the'module' attribute, the result is undefined.\n\t\"\"\"\n\treturn self.dcl.get('namespace', namespace_or_name,\n\t\t\t\t\t\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tnames, names_len = self.interface([\"NAMES\"])\n\tif all:\n\t\tif len(names) > 1:\n\t\t\treturn names, names_len\n\t\telse:\n\t\t\treturn names[0], names_len\n\n\treturn names[0], names_len\n\t\n\t\n\t\n\t# Functions to return/set the default value, from a definition.\t\n\t\n\tdef default_value(self):\n\t\treturn self.interface([\"DEFVAL\"])[\"DEFVAL\"].split()[-1]\n\t\n\t\t\n##\n#\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn super(Interface, self)._names(all=all)\n\n\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t    return self._map.get(type(self), [])\n\n\t@names.setter\n\tdef names(self, names):\n\t\tnames = [(int(x),x) for x in names.split(\",\")]\n\t\tnames = [x for x in reversed(sorted(names))]\n\t\tself._map = dict(zip(names, names))\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self._get(AttributeInterfaceNames, all=all)\n\n# --- interfaces ---------------------------------------------------------------------------------------------\n\n\nclass AttributesInterface(Interface):\n\t\"\"\"\n\tReturns a list of all attributes.\n\n\tThe attributes names by default are prefixed by ``attribute``.\n\n\tAlternatively the `name` argument can be passed, which is the\n\tname of the attribute (if it is a string) or the list of its attributes (if\n\tit is a list or if it is a dictionary).\n\t\"\"\"\n\tname = Attribute\n\t__doc__ = Interface.__doc__\n\n\tdef"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif not self._attributes:\n\t\traise AttributeError, ('No attribute was defined')\n\tresult=list(set(all or [a.name for a in self._attributes]))\n\treturn result\n\ndef get_attributes(self, all=False):\n\t\"\"\"\n\tGet the attribute names defined by the interface.\t\"\"\"\n\tif not self._attributes:\n\t\traise AttributeError, ('No attribute was defined')\n\tresult=list(set(all or [a.name for a in self._attributes]))\n\treturn result\n\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\n\treturn self.__call__()\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif all:\n\t\tif self is Attrib_Interface1: #pylint:disable=unsubscriptable-return\n\t\t\treturn getattr(__builtin__, names())\n\t\telse:\n\t\t\t# Get an attribute list of the class.\t\n\t\t\treturn Class_Interface.names(self, True)\n\t\t\n\tdef __getattr__(self,name):\n\t\t\"\"\" If a name is not in the attribute list, \n\t\t\t\traise AttributeError. \"\"\"\n\t\tif name in self.__dict__:\n\t\t\treturn self.__dict__[name]\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tfor o in self._attribs.keys():\n\t\te = self.to_dsk(o)\n\t\tif e == None or self.names(e) == []:\n\t\t\treturn None\n\t\tif all:\n\t\t\tf = set(self.names(e))\n\t\telse:\n\t\t\tf = f.union(set(self.names(e)))\n\t\treturn list(f)\n\n# vim: set sw=2 et tw=0 fenc=utf-8 ft=python:\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn sorted(map(getattr, self.iface, all_ifaces))\n\n\n"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif sequence == '*' or sequence == '':\n\t\treturn output\n\telif isinstance(sequence, tuple) or isinstance(sequence, list):\n\t\tif output is not None:\n\t\t\tsequence = ', '\n\t\telse:\n\t\t\toutput = [ '=' * 4, sequence ]\n\t\treturn ''.join(output)\n\telif _isinterface(sequence.type):\n\t\treturn sequence\n\telse:\n\t\treturn sequence\n\n\tdef _check_spec(spec):\n\t\tif isinstance(spec, str):\n\t\t\t# It's a specification list\n\t\t\tspec"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif isinstance(sequence, types.InterfaceType):\tseq =  (sequence,)\n\telif isinstance(sequence, (types.ClassType, types.DictType)):\tseq = ()\n\telse:                                                      \treturn sequence\n\n\tif output is None: return seq\n\telse:                                                     return zip(seq, output)\n\n\ndef _normalizelhs(lhs, body):\n\tlhs = _normalizeargs(lhs)\n\tif isinstance(lhs, Exception):\n\t\tlhs ="}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\tdef recurse_interfaces(self, iface, other_interfaces=None):\n\t\ttry:\n\t\t\treturn self._normalizeargs(other_interfaces, iface)\n\t\texcept KeyError, exc:\n\t\t\tassert isinstance(exc, InterfaceDef), f\"Exception %r: {exc}\"\n\t\t\treturn other_interfaces\n\n\tout = ()\n\tseqs = [i for i in sequence.split(\"<\")]\n\tfor seq_indx, interface in enumerate(seqs):\n\t\tiface, other_iface = _exp"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tnormal = {}\n\n\t# expand all single interfaces, tuples and declarations\n\tfor arg in sequence:\n\t\tif isinstance(arg, Interface):\n\t\t\tnormal[arg]=_normalizarr2(arg)\n\t\tif isinstance(arg, tuple) or isinstance(arg, list):\n\t\t\tfor decl in arg: _normalizarr2(decl)\n\t\tif arg in normal:\n\t\t\tif not _isargname(arg, output):\n\t\t\t\tnormal[arg]=_normalizarr2(arg, output)\n\n\tseq = [normal.get(arg, arg)"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif not output is None:\n\t\toutput = [x.strip() for x in output.split(';')]\t# remove surrounding ()\n\t\targs = []\n\t\tpreargs = []\n\t\targs = [x.strip() for x in sequence]\n\t\tif args:\n\t\t\tfor i in range(0, len(args)):\n\t\t\t\tpreargs.append(args[i])\n\t\treturn (preargs, args)\n\treturn sequence\t\n\ndef setattr(target, name, value):\t\"\"\"\n\tTakes a Declaition argument name.  Takes a"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif not 'in' in sequence:\n\t\traise ValueError, \"No 'in' declaration present\"\n\tif isinstance(sequence['in'], tuple):\n\t\tinlist = sequence['in']\n\telse:\n\t\tinlist = sequence['in']\n\n\tassert type(inlist) is dict, \\\n\t\t\"Type of 'in' attribute must be dictionary\"\n\tfor key in inlist.keys():\n\t\tassert isinstance(key, basestring), \\\n\t\t\t\"The key must be a string\"\n\tif len(inlist.keys())!= 1 or"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif isinstance(sequence, list) or isinstance(sequence, tuple):\n\t\targv = sequence\n\telif isinstance(sequence, abc.IProtocol):\n\t\tassert sequence.isprotocol(), \"Protocol is expected\"\n\t\targv = sequence.attr\n\telse:\n\t\tassert hasattr(sequence, \"defdecl\"), \"Cannot normalize %s\"\n\t\targv = ()\n\t\t\t\n\tdef normalizeit(d):\n\t\tif isinstance(d, types.Interface):\n\t\t\tassert d.isprotocol(), \"Interface is expected\""}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output is None:\n\t\toutput = []\n\tif isinstance(sequence, (basestring, int, float, long)):\n\t\treturn output + [sequence]\n\tif any([ isinstance(arg, InterfaceRef) and not str(arg.argspec.type) == 'InterfaceRef' for arg in sequence ]):\n\t\treturn output\n\n\tif isinstance(sequence, list):\n\t\targs = tuple( _normalizeargs(x) for x in sequence )\n\t\tfor i, arg in enumerate( args ):\n\t\t\tif isinstance(arg, InterfaceRef"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tnormalization = dict()\n\tif output is None:\n\t\toutput=\"stdout\"\n\ttry:\n\t\tdef normalizeinterface(x, output=None):\n\t\t\t#if x and x in normalization:\n\t\t\treturn normalization[str(x)]\n\t\tdef normalizetype(x, output=None):\n\t\t\t#if x.__name__ == \"type\":\n\t\t\tif x in normalization:\n\t\t\t\treturn normalization[str(x)]\n\t\t\telse:\n\t\t\t\treturn x\n\t\t#x = sequence\n\t\t#normalization[\"(%s)\" % x]"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output == None:\n\t\toutput = \"def\" ## no output, e.g. def A = ( ( B ( C ) : F ): ) -> ( D : N )\"\"\"\t# no output, just output\n\t\n\t#print \"Args:\", sequence\n\t\n\tif \"}\" in sequence:\n\t\traise ValueError(\"No output in %s\" % sequence)\n\t\t#print \"We cannot normalize return arguments\"\n\t\t\n\tif type(sequence)is dict:\n\t\t#print \"Normalize\", sequence\n\t\treturn dict(((k,v)+(output"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\timport c_opt\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n__c_optimizations_available = _c_optimizations_available\n\ndef load_c_optimizations():\t\"\"\" Load the C optimizations (currently C_OPT) \"\"\"\n\treturn _build_opt_module(c_opt.load_c_optimizations())\n__load_c_optimizations = load_c_optimizations\n\ndef reset_c_optimizations():\t\"\"\" Reset the C optimizations. \"\"\"\n\tmodule = _build_opt_module(c"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\timport imp\n\treturn imp.find_module('_c_optimization') is [(2, 0)] or False\n\n\nif not _c_optimizations_available():\n\traise ImportError, \"No C optimizer available.\"\n\n#----------------------------------------------------------------------------\n# Copyright Andrew Robinson 2008-2012\n#=============================================================================\n"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tfrom csharp import optimization\n\treturn not set(['cpoptimization', 'coptimization']) & set(optimization.available())\n\ndef _get_modules():\n\tcompile = {}\n\tif _c_optimizations_available(): compile.update(_c_optimizations)\n\tif _shared_optimizations_available(): compile.update(_shared_optimizations)\n\tcompile.update(_global_optimizations)\n\tif _system_optimizations_available():\n\t\tcompile.update(_system_optimizations)\n\t\tif compile['debug']:\n"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tif 'GLR.C' in _c_modules:\n\t\treturn 1\n\telse:\n\t\treturn 0\n\ndef add_c_optimizations(d, path=None, path_args=None,\n                      debug=1,\n                      quiet=0,\n                      **kwargs):\n\t\"\"\"\n\tAdd C optimizations to the Python module. See\n`this link for details:\n<http://docs.python.org/lib/module-compat.html#module-compat.add-code-optimizations>`__\n\nC code is not included. If"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn hasattr(sys, \"getopt\") and hasattr(sys, \"getopt_basic\")\n\n\n\n\ndef get_compilers():\n\t\"\"\"Return the compiler modules required by this system.\"\"\"\n\timport distutils.sysconfig\n\tcompilers = [c for c in distutils.sysconfig.get_config_vars([\"CC\", \"CFLAGS\"]).split()\n\t\t\t\tif get_c_compiler() in c.split()]\n\tcompilers = [compiler(compiler.__name__) for compiler in compilers]\n\treturn comp"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn os.popen(\"which cppoptimization\").read().strip()\n\ndef _cpp_optimizations_available():\t\"\"\"This gives up if the compiler isn't built\n\twith cpp optimization or if that does not exist.\n\t\"\"\"\n\tif not _c_optimizations_available():\n\t\traise ImportError\n\treturn True\n\ndef optimize(*args, **kw):\n\t\"\"\"If it is possible to optimize or link a given set of modules,\n\treturn an equivalent set of c_compilation parameters as a string\nor None if the modules are of c_comp"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn __import__(\"c\")\n\ndef _h_optimization(opt):\t\"\"\"\n\tReturn the header module, if available, otherwise\na false value.\n\t\"\"\"\n\treturn opt.check_import(opt.h)\n\n\n# This is called from the import hook\nif _h_optimization(opt):\t\t\topt.header.cxxopts('-I\"%s\"' % _h_path(_h_optimization(opt)))\nelif _c_optimizations_available():\t\topt.header.cxxopts('-DUSE_%s"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn hasattr(_c_optimizations, '__c_optimisation_available')"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tdef _load_c_optimization_module():\n\t\ttry:\n\t\t\tfrom ctypes import _library\n\t\t\treturn True\n\t\texcept ImportError:\n\t\t\treturn False\n\treturn _load_c_optimization_module()\n\n__all__ = ['get_c_optimization_module', 'C_optimize']"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n\treturn lib.__c_optimizations_available() == 0\n\n\n \n####################################################################\n\ndef _c_print_options( options ):\n\t\"\"\"_c_print_options converts a list of options to a single string. \n\t\n\tIt then calls tostring(), fromstring() etc. on each option in\n\tthe list of options. It returns the string that would be returned\n\tby s.tostring().\t\n\n\tIf the function _needs_ more details see the documentation in\n\t_c_print_options_function_details.\n\t\"\"\""}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn _use_c_impl() == 0 and _PURE_PYTHON\n\n\ndef _do_c_optimizations():\t\"\"\"Return a true value if we should not make C\n\toptimizations\"\"\"\n\n\treturn (_do_python() and _use_c_impl())!= 0\n\n\ndef _check_c_optimizations():\t\"\"\"Return a true value if we should not make C\n\toptimizations\"\"\"\n\n\treturn _must_cpp() or _do_c_optimizations()\t\t\t\t\t\t\t\t\n\n\n_c_opt_value = 1\n\n\ndef _use_c"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn _use_c_impl.get().C_compiler\n\n # __unused__\n # __all__\n # __builtins__\n"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn (\n\t\t_use_c_impl == 'CUDA'\n\t\tor sys.platform == 'darwin'\n\t)\n\n\ndef _should_install_jpeg_opts():\n\t\"\"\"\n\tReturn a true value if we should install the Jpeg Image Compression\n\toptions, i.e.\n\t -DGLEW_ENABLE_GL = 0\tIf we're using GL and need to enable it.\n\t\"\"\"\n\treturn (_use_c_impl == 'CUDA'\n\t\t\tor _use_c_impl == 'CV-CUDA"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn not _use_c_impl() and \\\n\t\t_use_c_impl() in ('PYTHON', 'python', '')\n\n"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn not _get_python_encoding() == 'UTF-8'\n\ndef _use_c_impl():\n\t\"\"\"Return a tuple of (bool, encoding) if C is available.\"\"\"\n\tencoding = _get_python_encoding()\n\treturn _should_use_c_impl(), encoding\n\ndef _get_python_encoding():\n\t\"\"\"Return the system's Python encoding or None\"\"\"\n\tenv = os.environ.copy()\n\tpath = os.path.expanduser(env.get(ENV_VAR_PATH,"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tpure_sys = _useable_sys()\n\n\tif pure_sys in ('PYTHON_BUILD_SUITE', 'OSX'):\n\t\treturn True\n\telif pure_sys not in _useable_sys(add=False):\n\t\treturn False\n\t# These values are not available if Python is built with \"--use-c-optimizations\"\n\telif pure_sys!= 'PURE_PYTHON':\n\t\treturn True\n\telif sysconfig.get_config_var('CFLAGS')!= _get_c_comp"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn _use_c_impl and not _is_a_pure_python_environment()\n\n\t\n# FIXME: this is not a test\n#\n#\t\tTest if c_optimization_enabled is set to a value from the\n#\t\tenvironment variables: ``True``, ``False``, or the value None.\n#\t\tIf False, we consider the optimization to be successful if:\n#\t\t- we didn't have any C code generated\n#\n#\t\t- the C code (which is defined with the code generated by the\n#\t\tcompiler) actually compiled is compiled\n"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\timport _pure_python\n\n\treturn _pure_python.PURE_PYTHON and getenv(\"PURE_PYTHON\") == \"1\"\n\n\ndef _use_fast_math(opt):\n\t\"\"\"\n\tReturn a true value if the compiler will use the fast math library for some\n\tkind of floating point number evaluation.\t(Currently the only known\n\tlanguage in C++ that uses this optimization).\n\n\tA value of ``False`` indicates that it has no choice; that is, it will\n\tnot attempt to use the fast math library.\n\n\tA"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn (\n\t\tis_pure_python and\n\t\tnot _USE_PYTHON_IMPL and\n\t\thasattr(sys.modules[__name__].__dict__, '__c_version__') and\n\t\t(\n\t\t\t_USE_PYTHON_IMPL == 'never' or\n\t\t\t'PYTHONUNBUFFERED' == _USE_PYTHON_IMPL\n\t\t)\n\t)\n\ndef _is_c_on_our_system():\n\t\"\"\"\n\tReturn a true value if we are on CPython."}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tenv = os.environ.get('PURE_PYTHON', None)\n\treturn env is None or env == '1'\n"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn _c_optimizations_required() - _c_optimizations_ignored()\n\n\ndef _c_optimizations_ignored():\t\t\t\t\"\"\"  \"\"\"\n\treturn _c_optimizations_ignored_()\n\n# end of c_optimizations_ignored.py"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn _c_optimizations_required() - _c_optimizations_handled()\n\n# -----------------------------------------------------------------------------\n# -- SOURCE --------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t@staticmethod\n\tdef no_c_optimization():\t\"\"\"\n\tThe no C optimizations available.\"\"\"\n\t@staticmethod\n\tdef optimize(func=None):\t\"\"\"\n\tThe compiler does not take care about the parameters of `c_optimizations`\n\tand only looks at the first argument type that is not None.  This value\n\twill be used as the 'C_optimization' value.\t\n\t\"\"\"\n\tif func:\t\treturn func\n\n\n\t@staticmethod\n\tdef no_c_optimization():\t\"\"\"\n\tThe compiler does not take"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn not c_optimizations\n\n\n@c_optimizations\ndef makefunc_ignore_unwanted(f);\tfunc = func[:func.find(f)]\n\ndef func_ignore():\t\"\"\"\t\"\"\"\n\n_c_optimizations_required = _c_optimizations[:len(function_table)]\n_c_optimizations_ignored = _c_optimizations[len(function_table):]\n"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tfor p in ['_ct_optimizations']:\t_c_opt(p, \" -o\", p)\n\n\ndef _c_optimizations_required():\t\"\"\"\n\tThe array of c-optimizations to run.\"\"\"\n\ttry:\n\t\tfrom c_functions import *\n\t\tfrom c_types import *\n\texcept RuntimeError:\n\t\treturn # ignore -o errors for non-existing or\n\t\t        # built-in functions\n\t\t# XXX: not a big deal if there are more than one...\n\t\tc_opt.warn(\"Could not load c-"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t_c_types_additional = [c_int, c_uint, c_int, c_uint]\n\t_c_types_exceptions = _c_types_additional\n\t__all__ = [\n\t\t\"CAPI_ONLY\",\n\t\t\"CAPI_ONLY_SUGGESTED\",\n\t\t\"CAPI_ONLY_REQUIREMENTS\",\n\t\t\"CAPI_REQUIREMENTS_FALLBACK\",\n\t\t\"CAPI_REQUIREMENTS_REQUIRES_SUPPORT\",\n\t\t\"CAPI_REQUIREMENTS_"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn \n#}}}\n        for j in self.c_func.order_of_parameters()[1:-1]:\t\n\tif j == 0:\treturn []\n        \n    def __getattr__(self, attr_name):\n        \n        try:\treturn self.c_method.__getattr__(attr_name)\n        except AttributeError: pass\n\t\n        try:\treturn self._c_method.__getattr__(attr_name)\n        except AttributeError: pass\n\t\n        print(\"\"\"<No function"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tif c_flag:\n\t\treturn False\n\tif _c_optimizations_required() and c_option >= 3:\n\t\treturn True\n\treturn False\n\n\ndef _c_optimizations_required():\t\"\"\"\n\tThe same as `_c_optimization_required` except it returns the `~_c_optimization_required._c_check` flag.\n\t\"\"\"\n\treturn not _c_optimization_supported()\n\n\ndef _c_option():\t\"\"\"\n\tThe same as `_c_option` except it returns the `~_c_option"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn not _c_optimizations_required()\n\ndef __inp(x):\n\treturn x\n\n\ndef __nop(*x):\n\treturn x\n\n\ndef _nop(*x):\n\treturn x\n\n\ndef _nop1(*x):\n\treturn x\n"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tif not _has_c_optimizations(c_obj):\t\treturn _c_optimizations_required()\n\treturn _c_optimizations_required()\n\n\nclass _c_dictionary(object):\n\t\"\"\"class used internally to check dictionary type\"\"\"\n\t_c_dictionary_type = None\n\tdef _c_dictionary_type(self):\t\treturn self._c_dictionary_type or {}\n\n\n\tdef _c_dictionary(self, key_type):\n\t\t\"\"\"dict -> int\n\t\t\tthis method is used internally\n\t\t\"\"\"\n\t\tif"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _use_c_impl(PURE_PYTHON)\t\t\n\n@_c_optimizations_required\ndef _use_c_impl(P):\n\tc_impl = build_system_module(P)\n\t# c_impl.module = _c_module(c_impl.__file__, P, c_impl)\n\t_use_python_imports(c_impl, c_impl.__file__)\n\tp = P.__file__\t\t\n\n\t# c_impl.module.enable_c7_impl"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\timport _use_c_impl\n\tif PY3 and _use_c_impl.PURE_PYTHON:\n\t\treturn False\n\telif PY2 and not _use_c_impl.PURE_PYTHON:\n\t\treturn False\n\treturn True\n\ndef _use_tensor_c(context, T):\n\t\"\"\"Return whether Tensor contraction is supported by the Python backend.\n\n\tParameters\n\t----------\n\tcontext : Context\n\t\tAn instance of `Context` class\n\tT : Tensor\n\t\tA Tensor object\n"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tif _use_c_impl.PURE_PYTHON:\n\t\treturn True\n\treturn str(sys.version[:2]).lower() in [\"1\", \"2\"][:2]\n\nPLATFORM_USE_BOOLEAN_TO_BOOL = _use_c_impl.BOOL_TO_BOOL\nPLATFORM_USE_COMPLEX_TO_COMPLEX = _use_c_impl.COMPLEX_TO_COMPLEX\nPLATFORM_USE_FLOAT_TO_FLOAT = _use_c_"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tglobal _use_c\n\treturn _use_c\n\ndef _c_impl():\n\t\"\"\"\n\tReturn the implementation file path to `libPy_Use_C`.\n\t\"\"\"\n\tfrom pywrapcxx import API\n\treturn API.get_library_file_path('./c/libPy_Use_C.so')\n\ndef _use_c():\n\t\"\"\"\n\tReturn a dictionary of supported C optimizations available.\n\t\"\"\"\n\tfrom pywrapcxx.wrapper_config import config\n\treturn config.get_supported_c_configs"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn PY_PY_IMPORT\n\n"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn c_optilizations_present() and not pure\n\n\ndef _c_optimizations_enabled(config=_pyspect_config):\t\"\"\"\n\tReturn a true value if the C optimizations are enabled.\n\nThis uses the ``PURE_C_OPT_IMPLEMENTATIONS`` environment variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _use_c_impl(*_c_optimizations_args(config))\n\n\ndef has_c_optimization(config=_pyspect_config, use_c_option=True"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn get_flags() & _flags_use_c_impl\n\n"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn var_name in settings.env.c_impl_names;\t\n\n@_optimizations(PURE_PYTHON)\ndef __c_optimizations_not_required():\t\"\"\"\n\tReturn an explanatory set of keys to consider being enabled.\n\nThis uses the ``_pure_pp_options`` variable as documented here `env.c_impl_names`\nand `PY_COMPILER`, which must be a list.\n\"\"\"\n\treturn (env.cxx_compiler_type, env.cxx_compiler_name)"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn config['PURE_PYTHON']\n\n@_c_optimizations_required()\ndef _c_optimizations_enabled():\t\"\"\"Return a true value if we're able to use\n       the C optimizations enabled (i.e. not just ``env.CFLAGS``). This\n       should be ``False`` only if we're not generating C code from any other\n       code, and we're running in a virtual directory.\n\nThis uses the ``PURE_TGT`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn config['"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn sys.getrefcount(\n\t\t_pure_compressor_cache_name(\n\t\t\t'c-optimizations-enabled',\n\t\t\tc_modules.c_type.__name__\n\t\t)\n\t)\n\nif _c_optimizations_required():\n\tdef _c_support_errors(*, **):\n\t\t\"\"\"\n\t\tReturn the compiler command line option for c-optimizations\n\t\tenabled via ``OPTIONS``.\n\t\t\"\"\"\n\t\treturn _pure_compressor_cache_name(\n\t\t\t'c-optimizations-enabled',\n\t\t\tc_"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n\tself.__reset = True\n\tif self.__is_empty:\n\t\tself.__bins = {}\n\telse:\n\t\tself.__bins = None\n\t\tfor init in self.__values:\n\t\t\tself.__bins[init.time] = init\n\tself.__reset = False\n\n\n\tself.__num_bins = None\n\tself.__size = None\n\tself.__is_empty = False\n\n\n\tself.__clear_state()\n\n\t\n\n"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself._histo = dict()\n\tself._initial_value = self._make_empty_bins()\n\n\treturn self\n\n# -----------------------------------------------------------------------\nclass HistogramBins(object):\n\t\"\"\"\n\tA class implementing binning.\n\n\tA histogram has two types of bins: a bin which contains values\n\tall but the first (the lower) and a bin which contains values\n\tat the upper boundary (the upper).\n\n\tBinning can be given by providing a starting value at which to\n\tplace bins. \t\n\t\"\"\"\n\t\n\t# --------------------------------------------------------------------------------"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself._histfunc = _HistFunc(self._binner, [], [], self._fnvals)\n#----------------------------------------------------------------##\n\tdef reset_all(self):\n\t\tself.set_default(**(self.__dict__))\n#----------------------------------------------------------------##\n\tdef reset_default(self):\n\t\tself._fnvals = self.__dict__ \n\t\tsuper().reset_all()\n#----------------------------------------------------------------##\ndef test():\n\thistfunc_init = _HistFunc.HistFunc\n\tinit = np.array(histfunc_init([1.,   # make"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.__init()\n\n\tself.__reset()\n\n\t# reset all bins using bins_* calls (to make it look the same)\n\tfor varname,value in self.__values.items():\n\t\tfor bin in self.__bins:\n\t\t\tgetattr(bin,varname)(value)\n\n\tfor varname,value in self.__values.items():\n\t\tself.__bins[varname].start_sample(value)\n\ndef histogram(varname,value):\t\"\"\"\tCreate the histogram\n\nvarname can"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\ttry:\n\t\tself.reset_with_value()\n\texcept Exception:\n\t\tself.reset_with_string()\n"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tif __debug__:\n\t\tself.__debug__()\n\t\t\n\tBinFactory.BinFactory.reset(self, make_bins())\n\n\n\tdef bin(data, binsize=5,\n\t\t**kwargs):\t\t\n\t\t\"\"\"\n\t\tReturn the bin of the function.\n\n\t\t>>> bin(x).f\n\t\t\tBinFactory.BinFactory\n\t\t\n\t\t>>> bins = BinFactory(1)\n\t\t>>> bins.make_bins()\n\t\t\"\"\"\n\t\treturn BinFactory(binsize).__binhelper__(data, **kw"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself._bins = {}\n\tself.reset_bin(self._bins)\n\t\n\t\n__all__=[b'Histogram', b'init']\n\ndef init(*args):\n\t\"\"\"\n\tCreate the list containing initial values for the histogram.\n\t\"\"\"\n\t\n\tif len(args) == 1:\n\t\tinitial_value=args[0]\n\t\t\n\telse:\n\t\tinitial_value=None\n\t\t\n\tif initial_value:\n\t\treturn [initial_value]\n\telse: return None\n"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\treset = self.set('reset', reset)\n\t\n\treturn reset\n\ndef set_reset(self):\n\t\"\"\"\n\tSet the reset status of this histogram.\n\nThis context is set to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the cleanup).\t\n\t\"\"\"\n\tself.set('reset', set_reset)\n\t\n\treturn self\n\ndef set_no_reset(self):\n\t\"\"\"\n\tSet the reset status of this histogram to False."}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t# reset of the context has to be checked in any case\n\tif self.first is None:\n\t\traise HistHistError(\"Histogram does not have any bins\")\n\n\tfor k in self.BIN_ITEMS:\n\t\tself.BIN_ITEMS[k] = None\n\n\t\tif k in self.bins:\n\t\t\tself.bins[k] =''\n\t\telse:\n\t\t\tself.bins[k] = None\n\ndef _histogram_to_dictionary(histogram):\n\t\"\"\"Returns a dict of bin_values as a list"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.__curr_context.reset()\n\tfor key, (keystr, bins) in self.__bins.items():\n\t\tbins[key] = None\n#\tfor key in self.__inits:\n#\t\tbins[key] = None\n#\tself.__inits = None\n\n\tself.__hist.reset()\t\t# reinitialize\n\n#\n# Context\n#\n\nclass context(dict):\n\t\"\"\"\n\tSet the context which is used to collect histograms.\n\tUseful for keeping a context around (e."}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\traise IOError(\"Not implemented yet\")\n#endclass\n\n\ndef _get_graph_input(graph, source_value):\n\t\"\"\"\n  .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      This method is used in Python 2.x to get to_csv(separator=' ').\n\nConvert graph's points to CSV.\n\n*source_value* points are converted to string\n\nReturns the string converted to CSV.\n\n*graph* is the graph to be converted.\n\n\"\"\"\n\tret = str(source_value"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\nfrom cStringIO import StringIO\nfrom itertools import count\nfrom lena.evaluation import *\n\nclass Table(object):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 Table is a new implementation.\n      Instead use Tables.to_csv()\n\n\"\"\"\n\t\n\tdef __init__(self, filename=None):\n\t\tself.filename = filename\n\t\n\tdef to_csv(self, separator=\",\", header=None):\n\t\twith open(self.filename, 'w') as f:\n\t\t\tf.write"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tself._output = lena.output.ToCSV(\n\t\tseparator=separator,\n\t\theader=header)\n\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\ttry:\tdel to_csv.__to_csv\texcept:pass\n\texcept NameError: pass\n\tif header is None:\n\t\theader_str = \"\\n\".join(\"%d\"%i for i in range(self.num_vertices()))\n\telse:   header_str = str(header)\n\treturn to_csv.__to_csv(self, separator=separator, header=header_str)\n\n\n#\n#\tGraph's __geo method\n#\n\nclass Graph(BaseGraph):\n\t\"\"\"\n\t.. deprecated::"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\n\t\nif __name__ == '__main__':\n\tprint 'Lena is not used', __file__\n\timport sys\n\tsys.exit()"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tself.point_to_csv(separator=separator, header=header)\n\ndef make_tables(self):\n\t\"\"\"\n\tConvert graph to tables after processing it.\n\t\"\"\"\n\tself.point_to_csv()\n\n\tself.get()\n\ndef from_csv(in_f, headers=None, separator=\",\"):\n\t\"\"\"\n\tReads a CSV file, or an arbitrary sequence of CSV files.\n\tThe sequence allows the ``separator`` parameter\n\tto be replaced with the separator parameter value.\n\n\tA regular expression"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tdtype = self._dtype\n\n\timport cPickle\n\tfrom numpy import savez\n\tfmt = \"[{0}], {1}\".format\n\tfmt1 = \"{0}\", \"{1}\"\n\tsavez(str(separator)+\"{0}\", self.points, cPickle.load(open('graph{0}.pkl'.format(self.__hash__), 'rb')))\n\n\timport os\n\timport csv\n\tdata = []\n\n\tfor pt in self.points:\n\t\tdata.append(fmt1["}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t# FIXME: check if already converted and skip\n\tfor row in self:\n\t\tfor i in reversed(range(len(row))):\n\t\t\trow[i] = row[i].replace(\" \", separator, 1)\n\t\tfor val in row:\n\t\t\tval = val.replace(\" \", separator, 1)\n\t\t\trow[val] = row[val].replace(\" \", separator, 1)\n\t\tyield tuple([row.replace(\" \", separator, 1) for row in header])\t\n\ndef _to_csv(self, separator=\",\", header"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif header==None: header=[self.__name]\n\tcsv=[]\n\tfor val,name in enumerate(self.__name):\n\t\tif isinstance(self.__values[name], ndarray):\n\t\t\tself.__values[name][:]=repr(self.__values[name])\n\t\t\tif header[val]:\n\t\t\t\tcsv.extend([name+\"> \"+str(_v) for _v in self.__values[name]])\n\t\t\telse :\n\t\t\t\tcsv.extend([name+\"> \"+(str(_v)+repr(_v"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tfrom itertools import cycle\n\tcolnames = [\"X_{}\".format(i) for i in range(len(self))]\n\tdelimiters = [\"{}\".format(separator) for i in range(len(self))]\n\tif separator in colnames:\n\t\ts = '{}'.format(separator)\n\telse:\n\t\ts = ','\n\tfor i,g in enumerate(self):\n\t\tfirst = True\n\t\tfor c,e in g.edges():\n\t\t\tif first:\n\t\t\t\tfirst = False\n"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tranges = self.get_params()\n\terr_indices, _ = super(_RangeIndex, self)._get_err_indices(coord_name)\n\tif len(ranges) > 1:\n\t\traise NotImplementedError\n\tif len(ranges) == 1:\n\t\tassert ranges[0].__class__.__name__ == 'f'\n\t\tindices = np.repeat(range(len(ranges)), ranges[0])\n\telse:\n\t\tfor i in range(len(ranges)):\n\t\t\tindices = np"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tindices = []\n\tfor coord in Coord.objects.values(coord_name):\n\t\tindexes = ['(d2 < '+str(coords[i].value)+')' for i in range(len(coords))]\n\t\tindices.extend(['[' +','.join(i) for j in indexes] for i in coord.values()])\n\t\tindices.extend(['.','']) # we use a blank string for the last index\n\treturn indices\n\ndef _get_error_count(self):\n\t\"\"\"\n\tReturn"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif coord_name in ['x', 'y']:\n\t\tindices = set([])\n\telif coord_name == 'z':\n\t\tindices = set([0])\n\telse:\n\t\traise ValueError (\"Invalid coordinate name '{0}'\".format(coord_name))\n\treturn indices\t\t\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tall_errors = []\n\tall_errors.append(self._get_data_indices(coord_name, self.coord_dim))\n\treturn all_errors\n\ndef _get_data_indices(self, coord_name, dim):\n\tdata_indices = []\n\tfor d in dim:\n\t\tdata_indices.append(self.n_dimensions[d] - 1)\n\treturn data_indices\n\ndef _get_coord(name, shape, dim, coords):\n\tfor coordsi in coords:\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tnum_rows, num_cols = self._num_dims\n\tfor i in range(num_rows):\t\t# for each error location\n\t\trow_ixs = self._err_loc[i][0]\n\t\tfor j in range(1, num_cols):\t# for each row\n\t\t\tcol_ixs = self._err_loc[i][j]\n\t\t\tif coord_name in col_ixs:\n\t\t\t\tindices = list(zip(*col_ixs))\n\t\t\t\terr = list(np.sort(np.abs(indices)))"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\treturn super(_ErrorIndices, self)._get_err_indices(coord_name)\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t# Try getting from the coord_system name\n\treturn self._get_coord_system_name(coord_name)\n\ndef _get_indices(self, coord_name):\t\"\"\"\n\tGet indices corresponding to a coordinate.  Returns one-based.\n\t\"\"\"\n\t# Try getting from the coord_system name\n\treturn self._get_coord_system_name(coord_name[2 :])\t# We should never get no data here\n\ndef _get_data(self, coord_name):\t\"\"\"\n\tGet points of the coordinate.\n\t\"\"\""}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\traise Exception(\"Must be implemented by the subclass\")\n\t\ndef _make_targettuple(coord_names):\n\t\"\"\"\n\tHelper to make coordinate names from string names.\n\tTries to find a single coordinate that matches the passed name and returns\n\tthe name it matches.\n\t\n\tIf no such coordinates exist, returns the original list of names.\n\tThis list is useful, since it allows us to easily specify dimensions and\n\tnames of dimensions. (There should be no dimension that does not match)\n\t\"\"\"\n\t\n\tresult_indices = []\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tout = None\n\tif 'error_indices' not in self.__dict__:\n\t\tdummy, err_indices = super(CKP, self)._get_err_indices(coord_name)\n\t\tif dummy is not None:\n\t\t\tlog.error(\"Unable to get err_num:\", e)\n\t\t\tlog.error(\"Unable to get err_indices:\", e)\n\t\t\tlog.error(dummy)\n\t\t\tlog.error(\"Could not convert\")\n\t\t\tout = np.array([])\n\t\telse:\n\t\t\tout"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tlogger = logging.getLogger('pystask.get_err_indices')\n\twith self.env.env_lock:\n\t\ttry:\n\t\t\treturn [e.id for e in self.env.env[coord_name]]\n\t\texcept KeyError:\n\t\t\t# coordinate was not recorded\n\t\t\treturn []\n\n\tdef _get_coords(self):\n\t\t\"\"\"Return a list of (name, value) tuples for all records for which it\n\t\texists\n\t\t\"\"\"\n\t\treturn [\n\t\t\t(cname, v) for coord_name, v"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n\tpass\n\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t# This is a workaround to ensure that we create the context and not add it to the graph.\n\tgraph_obj = to_csphere(self.context)\n\tgraph_obj = graph_obj.context_manager.unwrap()\n\tsubc = to_string(context)\n\tgraph_obj.set_context(graph_obj.context_manager.unwrap(), subc)\n\n__metaclass__ = type"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tif context[\"value\"] is None:\n\t\t# we don't want to change the graph to nothing yet. This is\n\t\t# used by the 'value' attribute at a higher level.\n\t \tcontext[\"value\"] = None\n\tfor name, value in context[\"meta\"][ \"n_output\" ].items():\n\t\tif name == \"E\":\n\t\t\tcontinue = self.graph.get_value('t', tuple(value))\n\t\t\terror_name = \"error\"\n\t\t\tfor i, e in enumerate(value):\n\t\t\t\tif i == 2:\n\t\t\t\t\terror_"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor s in context.subset:\t\t\t\t# XXX\n\t\tself.update(s)\n\n__all__ = [\"_get_type\", \"graph\"]\n"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tif isinstance(context, dict):\n\t\tif context[\"error\"] is False:\n\t\t\tcontext[\"error\"] = {\"x_low\": {\"index\": context[\"x\"][0]}}\n\t\t\tcontext[\"error_E_low\"] = {\"x_low\": context[\"x\"][1]}\n\t\t#context[\"error\"] = {{\"x_low\": {\"index\": v[0]}} for v in context[\"E\"].itervalues()}\n\t\tfor error, sub_context in context[\"error\"].iteritems():\n\t\t\tfor e, x in sub_context."}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\ttry :\n\t\tif context.error is None :\n\t\t\treturn #nothing to do\n\t\telif isinstance(context.error, dict) :\n\t\t\tif context.error.has_key(\"error\"):\n\t\t\t\tif context.error.has_key(\"x_low\"):\n\t\t\t\t\tcontext.error[\"error_E_low\"][3] = context.value\n\t\t\t\tcontext.value = {**context.error, \"error_E_low\": context.value}\n\t\t\tcontext.error = None\n\t\telse :\n\t\t\traise TypeError(\"Cannot change properties of a graph"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n\timport re\n\t\n\tif not self.error:\n\t\tprint(\"No errors found.\")\n\t\treturn False\n\t\n\t# Remove any existing errors before the graph is destroyed\n\terrors = self.error.values()\n\tself.error.clear()\n\tresult = []\n\tfor x in errors:\n\t\tfor s in x.subs:\n\t\t\tmatch = re.search(r\"t_\", s.args[0])\n\t\t\tif match:\n\t\t\t\tresult.append(s.args[0])\n\t\t\telse:\n\t\t\t\tprint(\"Graph %r did"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tself.update( self._copyContext( context ) )\n\t# In order to avoid leaking of all the values, we \"undo\" those\n\t# additions\n\tfor subcontext in self._context.itervalues():\n\t\tsubcontext.setContext( context )\n\n"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tcontext.update_error_field(\"value\", self.value)\n\tfor subcontext in self.subcontexts:\n\t\tself._update_context(subcontext)\n\tif self._error_field:\n\t\tif \"x\" in self._error_field.keys():\n\t\t\tcontext.update_error_field(self.label_field[\"x\"],\n\t\t\t\tself._error_field[\"x\"])\n\t\tif \"y\" in self._error_field.keys() and \"t\" in self._error_field.keys():\n\t\t\tcontext.update_error_field("}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tpass\n\"\"\"\n\nclass NoSubcontext:\n\tdef __init__(self, context):\n\t\tself.context = context\n\n\tdef _update_context(self, context):\n\t\tpass\n\n\n\tdef _update_properties(self, graph):\n\t\tpass\n\n\tdef _update_subcontexts(self, graph):\n\t\tpass\n\"\"\"\n\ndef _format(graph, prefix, suffix=\"\", n_props=None, error=False):\n\t# format as a subgraph\n\tif n_props is not None:\n\t\tgraph.add_sub"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\treturn histogram(bins.bins, edges)\n\t\ndef maxi(bins):\n\t\"\"\"\n\tCompute the maximum value, returned by int from bin values.\n\n*bins* contain values and include the last bin.\t\"\"\"\n\t\n\treturn bins.bins[-1]\n\t\ndef int(bins):\n\t\"\"\"\n\tCompute the value of the intial (first bin) from bin values.\n\n*bins* contain values.\t\"\"\"\n\t\n\t# Get last bin\n\tbins_i = bins"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\tdef __call__(bins, edges):\n\t\th = [histogram(bins, edges), None]\n\t\th[-1] = None\n\t\treturn h\n\treturn __call__\n\nclass histogram:\n\t\"\"\" histogram(a, b) -> Histogram representing the histogram.\n\t\"\"\"\n\t\n\tdef __init__(self, values, edges):\n\t\tself.values = values\t## list of bins, the size of 'edges'\n\t\tself.edges = edges\t## the range for the integrand of 'edges"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tpass\n\n\n\n\nif __name__=='__main__':\n\ttry:\n\t\tinputfile=file(\"../integrator_test.xml\")\n\texcept IOError:\n\t\tpass\n\telse:\n\t\tprint(\"OK, starting test\")\n\t\tprint(\"Checking input: \" + str(inputfile))\n\t\tinputfile.close()\n\t\tresult=integral(open(inputfile),open(\".*\"),\"Bins\")\n\t\tprint(\"Integral: \"+ str(result))\n"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn bin_edges.integral(bins, edges)\n\ndef integral(bins,edges,kmax=100):\n\t\"\"\" Compute integral of a histogram.\n\t\n\tBins must be evenly spaced and no more than the number of bins.  Bins may be\n\tgiven as a (n,*) tuple where n > 1.  Edges as a (m,*) sequence\n\tmay be given as a (n,) tuple where n > 1.  If one (m,) is given, no\n\tn-th bin is computed.\n\t"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t## integral\n\tresult = self.integral(bins, edges)\n\t## result[self.size()-1] = 0\n\treturn result\n\n\t\n\t## integral method\n\t# returns the integral (scale for a histogram) of the *bins* and *edges* for\n\t# the given array\n\t# b = the original array\n\t# edges = the borders of `bins`\n\t# returns an array containing the integral values for each bin\n\t# b and edges can differ, but a diagonal integral is the case for\n\t# `bounds"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\thistogram = eval(eval('''\n\t\t\t\t\t\teval(`bins`) * eval(`edges`) \n\t\t\t\t\t''')')\n\treturn integ(1,(5+edges)) / (int(1) * int(histogram))\n\ndef bincentre(bins, edges):\t\"\"\"\"\"\"\n\treturn ((bins*edges) + bins) / 2.0\n\ndef integ(min=0, max=1.0e-10):\n\t\"\"\"\n\tCompute integral, for a 1D or 2D histogram."}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\tedges = [float(e) for e in edges]\n\tbins = [float(b) for b in bins]\n\n\tintegral = np.zeros((len(bins), len(edges)))\n\n\tfor i in range(len(bins)):\n\t\tintegral[i, :] = edge_scale(bins, edges[i], None, None)\n\n\treturn integral\n"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tintegral = 0.0\n\tfor v, val in zip(edges, bins):\n\t\tif val >= (val+0.0)/2.0:\n\t\t\tintegral += (v - val)/val\n\treturn integral\n\nfrom histogram_util.tables import bin_edges  # noqa\n\"\"\"\nThese are the useful functions for integrating over\nall possible bins, given by\n\n- *edge lengths*\n\ne.g.\n\n>>> bin_edges = lambda u: 0.5*(u+0.5)**2\n>>> f"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tif len(bins)!= len(edges):\traise ValueError, \\\n\t\t\t\t\t\t\t\"The length of bins and edges should be given\"\n\t# Create a new instance of the integrand to be integrated\n\tintegrand = integral_new()\n\t# Add a bin to the integral\n\tintegrand.addbin (bins[i], edges[i], bins[i+1]-bins[i])\n\treturn integrand\n# --------------------------------------------------------------------\n\n# --------------------------------------------------------------------\n\ndef integral_new():\n\t\"\"\"Create a new integral.\n\t\n\tThe returned object should"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tpass\n\n\ndef bin(bins, edges=(0,), format=\"%.2f\"):\t\"\"\"\n\tCompute bin value and scale for a set of bins.\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tpass\n\n\ndef getRanks(points, method=\"max\"):\n\t\"\"\"\n\tReturn the ranks for a sequence of points.\n\tSee: :func:`.ranks` for more information about what arguments are needed.\n"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif not isinstance(seq, S3Req) or is_source_element(seq.ele):\n\t\treturn False\n\tif not isinstance(seq.reqds, list):\n\t\treturn False\n\ti = 0\n\ts = Source(seq)\n\tif not s.is_source:\n\t\treturn False\n\tif s.requirements is not None:\n\t\treqs = s.requirements\n\t\ti = 0\n\t\twhile i < reqs.count():\n\t\t\tif is_fill_request(reqs[i]):\n\t\t\t\treturn False\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn (isinstance(seq, FillRequestSeq)\n\t\tand len(seq.sources) == 1)\n\n\ndef get_fill_request(**kwargs):\n\t\"\"\" Returns the FillRequest from a dict or list passed in *kwargs* \n\tin *format* and *filler_class* argument.  Returns None if the request\n\tis not possible.\n\t\"\"\"\n\t_format = kwargs.pop(u'format', None)\n\treturn kwargs.popf(*_format)\n\n\ndef is_source_seq(x):"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif type(seq)!= type(FillRequest):\n\t\traise IndexError('seq must be a FillRequest')\n\n\treturn (len(seq) >= 2 and\n\t\tisinstance(eq_to_fill_request(seq[0]), FillRequest) and\n\t\tnot (isinstance(eq_to_fill_reason(seq[0]), Source) or\n\t\t\tisinstance(eq_to_fill_reason(seq[0]), SourceReason) or\n\t\t\tisinstance(seq[0], Source))\n\t\t)\n\t\n\t\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif seq.__class__.__name__!= \"FillRequestSeq\"\\\n\tor len(seq.content) < 4 # min # of 4 = 1\n\tor\t(len(seq.content) == 0 and seqnotinclude(fillseq(seq)) and seqnotinclude(seq))\n\treturn True\n\ndef seqnotinclude(seq):\n\t\"\"\"\n\treturn true if sequence *seq* NOT include a fill request\n\nA non-empty sequence may or may not include a fill request\n\"\"\"\n\treturn (len(seq.content) > 0 and seqex"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif isinstance(seq, str, bytes):\n\t\traise TypeError(\"can't convert a {0}; it is an byte string\".format(type(seq)))\n\tif seq.find(1) >= 0:\n\t\treturn False\n\tif (not isa.isinstance(seq, (list, tuple))\n\tor\tisa.isinstance(seq, (tuple, list))\n\tor\thasattr(seq, \"__len__\")\n\tor\tisa.isinstance(seq, asn1.FullName)\n\tor\tnot isinstance(seq,"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif not is_fill_request_seq(seq):\n\t\traise ParseError, \\\n\t\t\t(\"Request sequence %s is invalid.  It must be a sequence.\",\n\t\t\tseq)\n\t\t\n\tfill_reqs = eval(seq)\n\tif len(fill_reqs) <= 0:\n\t\traise ParseError, \\\n\t\t\t(\"Not a sequence of fill requests: %s\", seq)\n\t\n\tif not isinstance(fill_reqs[0], (types.List, types.Tuple)):\n\t\traise ParseError, \\\n\t\t\t(\""}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tseq = sequence.SequenceElementWrapper(seq)\n\treturn seq.type==\"FillRequest\" and len(seq)>0 and not is_fill_req(seq[0])\n\ndef read_fillrequest_seq(seq):\n\t\"\"\"Read in a FillRequestSeq.\n\n\tA FillRequest can be read in exactly one fill request. \n\t\"\"\"\n\treturn sequence.ContainerElementWrapper(seq)[1][0][1].data\n\ndef write_fillrequest_seq(fillreq): #TODO\n\t\"\"\"Write a fill request.\n\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\n\tif not is_fill_request_elem(seq):\n\t\treturn False\n\t\t\n\tif len(seq.GetElem()) == 2 and type(seq.GetElem()[0]).__name__ == 'Source':\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef is_fill_request_msg(Msg):\n\t\"\"\"Test whether *Msg* can be converted to a FillRequestMsg.\n\t\tIt cannot be \"empty\" or a FillRequestMsg with no elements\"\"\"\n\t\n\tif Msg.GetNumElems() == 0:"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn sequence_is_request_sequence(seq, \"FillRequest\")\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn not seq is None and isinstance(seq, seq) and len(seq) > 0 and not isinstance(seq[0], seq)\\\n\tand ('source' in seq or ('target' in seq and seq['target'] not in SEQ_TARGETS)]\n\nclass FillRequest(SrcReqMsg):\n\t\"\"\"\n\tDefines a message used to fill the data set.\n\n\tThis class is used to make a new fill message from a set\n\tof source elements. This message can be used to fill\n\tmultiple source sets of the same data. There is only"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tobj = obj.__dict__.get('_obj', None)\n\treturn type(obj) and obj.__dict__.get('_fill_request_el', False)\n\ndef _findobj(obj):\n\treturn type(obj) in _list_types\n"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn obj.__class__ == fill_request.FillRequest\n"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn isinstance(obj, (FillRequest, Request))\n\n\ndef is_fillable(obj):\n\t\"\"\"\n\tObject is one of the following two classes:\n\n\t=========== ======================\n\tFillable\n\t=========== ======================\n\t\t:class:`FillRequest <twilio.rest.resources.resources.FillRequest>`\n\t\tA method that is not defined. This exception is always thrown for\n\t\tundefined values.\n\t=========== ======================\n\t\t:class:`Request <twilio.rest.resources.resources.Request>`\n\t\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif hasattr(obj, '__getbuffer__') or hasattr(obj, '__iter__'):\n\t\treturn True\n\telse:\n\t\treturn False\n\n# Object to read the content\n#\n# This object contains an executable method 'fill' and an executable method\n#'request' for fetch to a remote server.\n\t\nclass content(object):\n\tdef __init__(self, sock):\n\t\tself.sock = sock\n\n\tdef close(self):\n\t\tself.sock.close()\n#\n# A class to read the content\n#"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn obj['Method'] == 'fill' or obj['Method'] =='request'\n\n###\n### Class Functions\n###\nclass Exclude:\n\tdef __init__(self,method):\n\t\tself.method = method\n\t\t\n\tdef __eq__(self,other):\n\t\treturn self.method == other.method\n\t\n\tdef __str__(self):\n\t\treturn 'Exclude({!s}).{!s}'.format(self.method,repr(self))\n"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn isinstance(obj, FillRequest) and obj._fillish()\n\ndef all_fill_requests(obj):\t\"\"\"\tReturns all FillRequestes for the given object. \"\"\"\n\tout = []\n\tfor method in FillRequest.get_executable_methods(obj):\n\t\tif callable(method):\n\t\t\tout.append(method)\n\treturn out\n\ndef is_fill_request_el(obj):\t\"\"\"\tObject contains executable methods 'fill','request' \"\"\"\n\treturn any(is_fill_request_el(obj))\n\n\n"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif hasattr(obj, 'fill'):\n\t\treturn True\n\treturn False\t\n\n\ndef build_from_xml(*args, **kwargs):\n\t\"\"\"\n\tThis method returns new instance of form class from xml string.\n\tIt accepts form class name as argument, and also some optional arguments:\n\t\n\t* 'xml' argument takes form from'request' xml element with specified id and name (returns name of class)\n\t* 'form_name' argument takes form from'request' xml element with specified name (returns full name of class)\n\t* '"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif obj.__name__!= 'fill': return False\n\t\n\treturn True\n\ndef is_fill_reply_el(obj):\t\"\"\"\n\tObject contains executable methods'success' and 'error'.\t\"\"\"\n\tif obj.__name__!='success': return False\n\t\n\treturn True\n"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif hasattr(obj,'fill'): return True\n\tif obj not in fill_request:\treturn False\n\treturn True\n\n\ndef has_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and'request'. If\n\t'super' parameter is set, then obj will be a super of the current\n\tinstance of obj and will not be considered as a fill request to\n\tits super. This can happen whenever obj is a method object being\n\tused as an alternate interface for the object, for instance as in\n\tusing the Python built-in"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn isinstance(obj, (tuple, list, dict)) and \\\n\t\t\tissubclass(obj[1], (str,unicode))\n\nclass Request:\n\tdef fill(self, value):\n\t\treturn {'value':value}\n\nclass Response:\n\tdef fill(self, result, value):\n\t\tif result is None:\t\treturn result\n\t\tif isinstance(value, int) or isinstance(value, float):\n\t\t\tvalue = float(value)\n\t\telse:\n\t\t\tvalue = float(value)\n\t\tresult['value'] = value\n"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn (callable(obj) and obj() and isinstance(obj(), Function))\n\n# END functions definition\t\n\ndef function_run(callable_func):\n\t\"\"\"\n\tFunction to run as main program.\t\n\t\"\"\"\n\tdef run(obj):\n\t\tfunc = getattr(callable_func, \"run\", None)\n\t\tif not is_run_el(obj):\n\t\t\traise ValueError(\"func '%s' does not define run() method!\" % obj)\n\t\t\t\n\n\t\tif callable(func):\n\t\t\treturn func()\n\t\telse"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn callable(obj)()\n\n\ndef is_load_el(obj):\t\"\"\"\n\tObject contains executable method 'load'.\t\t\"\"\"\n\treturn callable(obj)\n\n\ndef is_get_el(obj):\t\"\"\"\n\tObject contains executable method 'get'.\t\"\"\"\n\treturn callable(obj)\n\n\ndef is_set_el(obj):\t\"\"\"\n\tObject contains executable method'set'.\t\t\"\"\"\n\treturn callable(obj)\n\n\ndef is_del_el(obj):\t\"\"\"\n\tObject contains executable method 'del'.\"\"\"\n\t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tprint('{:s} found!'.format(obj.__name__))\n\treturn True\n\ndef is_wait_el(obj):\t\"\"\"\t\n\tObject contains executable method 'wait'.\t\"\"\"\n\treturn True\n\ndef is_stop_el(obj):\t\"\"\"\t\n\tObject contains executable method'stop'.  \"\"\"\n\tif not hasattr(obj,'__call__') or not hasattr(obj, '__name__'):\n\t\treturn False\n\tprint('{:s} found!'.format(obj.__name__))\n\treturn True"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tp = obj.p\n\tif p.name == 'run':\n\t\treturn True\n\treturn False\n\nclass test(unittest.TestCase):\n\n\tdef test_run(self):\n\t\t\"\"\"\n\t\tCheck running and stopping the right test from the manager module\n\t\t\"\"\"\n\t\texec(\"\"\"\n\t\timport unittest\n\t\timport manager\n\n\t\tclass Tester(manager.Manager):\n\t\t\tclass RunTest(unittest.TestCase):\n\t\t\t\tpass\n\t\t\"\"\")\n\n\t\ttester = Tester()\n\t\ttester_method = tester.run."}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif type(obj) == type(\"\"):\treturn obj.run()\n\tif isinstance(obj, tuple):\tobj = obj[0]\n\treturn not isinstance(obj, (int, bool, str, object)) and callable(obj)\n\nclass Script(Module):\n\t\"\"\"\n\tScript object, containing the main function of the script\n\t\"\"\"\n\tdef get_name(self, index):\t\"\"\"\n\tGet the name of the object\"\"\"\n\tdef write(self, data):\t\"\"\"\n\tWrite data to the log\"\"\"\n\tdef __str"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn(getattr(obj, 'run', None)*10)\t\n\t\n#def is_flee(obj):\t\"\"\"\n#\tObject contains get_executable_class method 'flee'.\t\"\"\"\n#\treturn(getattr(obj, 'flee', None))\n\t#return obj\n\n\n#def is_stop(obj):\t\"\"\"\n#\tObject contains stop method'stop'.\t\"\"\"\n#\treturn(getattr(obj,'stop', None))\t\n#\treturn obj\n\t#return obj\n#def is_continue(obj):\t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn callable(obj) and hasattr(obj, 'run')\n\n\t\"\"\" Test suite execution by `obj` of the `run` attribute \"\"\"\n\tprint \"=======================\", obj\n\ttry:\n\t\tobj = 'hello world'\n\t\tprint 'called \"hello world\"', obj.run(5, 'hello world')\n\tfinally: pass\n\t\"\"\" Test suite execution by `obj` of the `run` attribute \"\"\"\n\tprint \"=======================\", obj\n\tobj = 'hello world'\n\tprint 'called \"hello world\"', obj.run(5"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif isinstance(obj, string_type):\n\t\treturn True\n\tif isinstance(obj, tuple) or isinstance(obj, list):\n\t\t# TODO: Implement this!\n\t\treturn False\n\tassert False, 'is_run_el error, can not call getattr on object, please add test!'\n\n@staticmethod\ndef is_run_as(obj):\t\"\"\"\n\tObject contains executable method 'run_as'.\t\"\"\"\n\tif isinstance(obj, string_type):\n\t\treturn True\n\tassert False, 'is_run_as error"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\n\treturn isinstance(obj, CMD)\n\n\n\ndef is_return_value(obj):\t\"\"\"\n\tObject contains method'return_value'.\t\"\"\"\n\treturn isinstance(obj, CMD) and (hasattr(obj, '__result__') or hasattr(obj, 'run_arg0'))\n\t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn obj.__flags__ & 0xffffffffffffffff\n\n\n"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn False\n\t\ndef get_is_fill_compute_el(obj):\t\"\"\"\t\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn obj.__class__ is FillCompute\n\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif isinstance(obj, str): return False\n\tfor name in [\"fill\", \"compute\"]:\n\t\tif hasattr(obj, name): return True\n\treturn False\n\ndef is_save_compute_el(obj):\t\"\"\"\n\tObject contains executable methods'save' and 'compute'.\t\"\"\"\n\tif isinstance(obj, str): return False\n\tfor name in [\"save\", \"compute\"]:\n\t\tif hasattr(obj, name): return True\n\treturn False\n\n\"\"\"\n>>> from gme.core import *\n\"\"\"\n\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif not obj.obj_type[0] in'suqj': return False\n\treturn obj.name.startswith(\"fill\") or obj.name == 'compute'\n\n#############################################################################\n# functions/classes for computing/validating properties:\n# we can compute one of them\n# (see above)\n#############################################################################\ndef get_compute_object():\n\t\"\"\"\n\tGet the object this function is used for computing.\n\t\"\"\"\n\treturn None\n\ndef get_validate_object():\n\t\"\"\"\n\tGet the object"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif hasattr(obj, 'fill'):\t\treturn 2\n\tif hasattr(obj, 'compute'):\treturn 2\n\treturn 0\n\n\n## Function 'is_filled_compute_el'\ndef is_filled_compute_el(obj):\n\tif hasattr(obj, 'fill'):\t\treturn 1\n\tif hasattr(obj, 'compute'):\treturn 1\n\treturn 0\n\n## Function 'is_filled'\ndef is_filled(obj):\t\treturn isinstance(obj, '__builtin__.property' or"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn type(obj).__name__ == 'Execute'\n\t       \t\t\n\n\n####################################################################################################\n#\n#  Generic class initalization\n#\n####################################################################################################\t\ndef get_global_class(cls):\n\tname = cls.__name__\n\tname = name[len(cls.__name__)+1:]\n\t\n\treturn type(name,'generic',globals(),locals())\n\n\n###########################################################################\t\n\"\"\"\nGeneral class initialization\n\"\"\"\t\ndef get_generic_class(klass):\n\tnewname"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn isdir(os.path.join(os.environ[\"HOME\"], \".computation\"))\n\ndef is_fill_executable(obj):\t\t\"\"\"\n\tObject is for 'fill' method and is executable.\t\"\"\"\n\treturn not is_fill_compute_el(obj) or has_compute(obj)\n\ndef _add_fill_methods(f):\n\t\"\"\"\n\tMethod `f` that adds execute and computes method for 'fill' command.\n\t\"\"\"\n\tdummy_init = partial(\n\t\t(fill, None,"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn isinstance(obj, FillBase) and hasattr(obj, 'fill')\n\ndef is_fill_compute(obj):\t\"\"\"\tReturn True if object has execute() function.\t\"\"\"\n\treturn isinstance(obj, ComputeBase)\n\ndef is_selectable(obj):\t\"\"\"\tReturn True if object is selectable.\t\"\"\"\n\treturn isinstance(obj, Selectable)\n\ndef is_drawable(obj):\t\"\"\"\tReturn True if object is drawable.\t\"\"\"\n\tif not is_drawable(obj):\treturn False\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn isattr(obj, 'compute' ) or isattr(obj, 'fill')\n\ndef is_fill_list(obj):\t\t\"\"\"\n\tObject contains executable methods 'fill' and 'list'.\t\"\"\"\n\treturn isattr(obj, 'compute' ) or isattr(obj, 'list')\n\ndef is_fill(obj):\t\t\"\"\"\n\tObject is callable/method, 'list' or 'compute'.\t\"\"\"\n\treturn is(obj) or (is_fill_list(obj) and is_fill_compute_el(obj"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn (isinstance(obj, Fill) or isinstance(obj, Fill))\n\n\n\n## def is_fill_compute_function(obj):\t\"\"\"\n##\tObject contains executable methods 'fill' and 'compute' that implement\n##\tthe required attribute 'funct' (ie, 'compute').\n##\t\"\"\"\n## \treturn isinstance(obj, Compute)==True\n\n\n__all__ = ['Filler']\t#???????????\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn obj.executable\n\n\nclass ObjectMethod(object):\n\t\"\"\"\n\tA object containing executable and non-executable methods.\n\t\"\"\"\n\n\tdef is_fill_compute_el(obj):\treturn True\t\t\t\t# return true if the callable is a compile method\n\t\n\tdef is_execute_compute_el(obj):\treturn 'compute' in dir(obj)\n\n\nclass ObjectGenerator(object):\n\t\"\"\"\n\tA class representing a function which generates Python and C code\n\tin one run.\n\t\n\tObjects and methods that"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\ttry:\n\t\treturn deepcopy(d1) if d1[-1] == d2[-1] else d1\n\texcept IndexError:\n\t\traise KeyError\n\ndef intersect(d1, d2, level=0):\n\t\"\"\"Return a dictionary with elements from *d1* not contained in *d2*.\n\nFor infinite recursion, a different value will be returned. For\nnon-infinite level 1, if a key is present both in *d1* and\n*d2*, if a key is present but a different value is required,\nit"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tdifference = difference_recurse(d1,d2,level=level)\n\tif depth_recurse(d1,d2,level=level+1):\n\t\tnew_difference = difference_recurse(d1,d2,level=level,deep = True)\n\t\tfor k in new_difference.keys():\n\t\t\tdifference[k] = new_difference[k]\n\t\t\tnew_difference.pop(k, None)\n\t\treturn new_difference\n\treturn difference\n\ndef intersection(d1, d2, level=-"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\tret = {key: list(difference(key, d1, level=level, key=key))\n\t      for key in list(d1) | list(d2)\n\t          if key in list(ret)\n\t          }\n\treturn ret\n\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tout = {}\n\tfor k1, v1 in d1.items():\n\t\tif level!= -1 and k1 not in d2:\n\t\t\tval1 = v1\n\t\telse:\n\t\t\tval1, val2 = v1\n\t\tif min(val1, val2) < 0:\n\t\t\traise ValueError('negative diff %r' % key)\n\t\tout[k1] = (val1, val2)\n\treturn out\n\ndifference = difference"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tout = kwargs.copy()\n\tfor key in d1:\n\t\tif level == -1 or key not in out or out[key]!= d1[key]:\n\t\t\tout.setdefault(key,d1[key])\n\t\t\tout[key] = difference(out[key], d2[k] if k in d2 else d1[k], level)\n\treturn out\n\ndef intersection(d1, d2, level=1):\n\t\"\"\"\n\tReturn a dict with items from d1 and d2 whose values are the difference\n\tbetween them"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tdiff = dict(d1)\n\tif level!= -1:\n\t\tdiff[level] = diff.pop(level+1)\n\tdiff[level+1] = difference(d1.get(key, 0), d2.get(key, 0), level+1)\n\tfor k, d in d1.items():\n\t\tif d == 0:\n\t\t\tcontinue\n\t\tif k in diff:\n\t\t\tdiff[k] = max(diff[k],diff[k+1])\n\t\telse:\n\t\t\tdiff[k] = diff[k+1]"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\tfrom.utils import difference\n\tresult = difference(d1, d2, level)\n\n\tdef f(x):\n\t\t# if d1 have another value\n\t\tif x in d2:\n\t\t\titem = (x, d2[x])\n\t\t\tresult[item] = f(item)\n\t\treturn result\n\tresult = f(d1)\n\tif level == -1 and isinstance(d1, dict) and'max_d1' in d1:\n\t\ttry:\n\t\t\tnew_result = d1['max_d1']\n\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif not isinstance(d1, dict):\n\t\treturn (d1,) + difference(d2, level)\n\tdiff = []\n\tif level == -1:\n\t\tfor x in sorted(d1):\n\t\t\tif not x in d2:\n\t\t\t\tdiff.append(x)\n\telse:\n\t\tfor x in sorted(d1):\n\t\t\tif xnot in d2:\n\t\t\t\tdiff.append(x)\n\treturn dict(diff)\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tret = difference(d1 if level>=0 else d2, d2)\n\treturn {} if not level else ret\n\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tret = {}\n\tfor key, val1 in d1.items():\n\t\tif val2 and value in val2:\n\t\t\tret[key] = val1\n\t\telif value not in val2:\n\t\t\tif level > -2:\n\t\t\t\tdifference = difference(val1, val2, level - 2)\n\t\t\t\tret[val1] = value\n\t\t\t\tret.update(difference)\n\t\telif level == -1:\n\t\t\treturn ret\n\treturn ret\n\t#def difference(d1, d2, level=0):\n\t#\treturn dictionary"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tpass\n# histogram function\nfor x in xrange(0, self.n):\n\ta = 0\n\tfor y in xrange(0, self.n):\n\t\tfor z in xrange(0, self.n):\n\t\t\tif abs(x - 1) + abs(y - 1) + abs(z - 1) == self.n:\n\t\t\t\ta = 1\n\t\t\t\tbreak\n\tfor i in xrange(0, self.n):\n\t\tfor j in xrange(0, self.n):\n\t\t\tfor k in xrange(0, self."}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif ((coords) or (hasattr(self.e, \"all\")) or (hasattr(self.b, \"all\"))):\n\t\tself.h += weight\n\n\tif ((coords) or (hasattr(self.b, \"all\")) or (hasattr(self.e, \"all\"))):\n\t\tself.h -= weight\n\n\tif ((coords) or (hasattr(self.e, \"all\")) or (hasattr(self.b, \"all\"))):\n\t\tself.h -= weight\n\n\n\t#if ((coords) or (hasattr"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.current_count += 1\n\tval, weight = self.__histogram[self.current_count-1][0] + coord, weight\n\tif weight > 0:\n\t\tval += self.values[0])\n\tself._get_histogram(val, weight, self.current_count-1)[0] = val\n\ndef normalize(dist, dtype=float):\n\t\"\"\"\n\tNormalize the histogram values.  The normalization is done by\n\ttaking the dot product of the histogram elements divided by the\n\tsum of the histogram"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\n\tpass\n\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tminval, maxval = coord\n\tif minval < 0 or maxval > 0:\n\t\tif self.ndim == 1:\n\t\t\tndim = 1\n\t\telif self.ndim == 2:\n\t\t\tndim = 2\n\t\telse:\n\t\t\tprint 'histogram_fill: wrong dimensions', self.ndim\n\t\t\tsys.exit()\n\t\tstart = self.size + self.ndim * self.filloffset % ndim\n\t\tend = start + self.size % ndim\n\t\tself.update(start, end, weight)\n\telse:\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif isinstance(coord, Real): \treturn\n\telif isinstance(coord, RealArray):\n\t\tif all(isinstance(v, real) for v in coord):  # ignore empty arrays\n\t\t\treturn\n\t\telif not isinstance(coord, RealArray):  # convert real to Real\n\t\t\ttry:\n\t\t\t\tcoord = Real(coord)\n\t\t\texcept (ValueError, IndexError):\n\t\t\t\tpass\n\tfor _ in range(len(self._hist)):\n\t\tif self._hist[coord] < coord: \t\t\t# we add the weight of the item"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\treturn self._hist._fill(self, coord, weight);\n\ndef clear(self, coord):\n\n\tself._hist._cursor = (self._hist._cursor + 1) % len(self._hist.keys());\n\n\treturn self._hist._clear(self, coord)\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tweight -= self.edges\n\n\tdelta = weight * self.width\n\tself.sum += delta\n\tself.sum2 += delta\n\tself.count += weight\n\n\tself.width += delta\n\n\t# Now we have to compute the mean and variance.\n\t# delta = sum(self.sum, 0)\n\t# self.sum += delta\n\t# self.sum2 += delta\n\t# delta = sum(self.sum2, 0)\n\t# self.sum2 += delta\n\t# self.sum2 += d2*d2\n\t# self"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif coord.x < 0. or coord.y < 0.:\n\t\traise TypeError, (\"'xy' must be an int or a float\")\n\tif coord.x >= self.xrange.x or coord.y >= self.yrange.x:\n\t\traise TypeError, (\"'xy' must be < %d %d, not %d %d\") % \\\n\t\t    (self.xrange.x, self.yrange.x, coord.x, coord.y)\n\tif coord.x >= self.xrange.y or coord.y >= self"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\treturn self._hist.fill(coord, weight)\n\t\n\t\ndef histogram_to_array(self):\n\t\"\"\"Converts a histogram to a 3- or 4-dimensional array\"\"\"\n\tif len(self._hist.size) == 2:\n\t\treturn [[self._hist[i,0], self._hist[i,1]] \n\t\t\tfor i in range(self._hist.size)]\n\telse:\n\t\t# 3D input, 4D output\n\t\treturn [[self._hist[i] for i in range(self._hist.size)] "}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tlbl_set = set(labels.keys())\n\tfor k in labels.keys():\n\t\tlbl_set.remove(k)\n\t\tfor vale in labels[k]:\n\t\t\tre_val = re.match(key=k,value=ves=val)\n\t\t\tassert re_val is not None, str(k)\n\t\t\tvalue_set = [v.lower() for v in ves]\n\t\t\tassert key in value_set, k\n\t\t\tassert re_val.group(1) in value_set, (k,re_val.group("}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tcheck_message = 'label keys and values do not match'\n\t_format_labels(labels)\n"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tpass\n\n\ndef _validate_metadata(metadata):\n\t\"\"\"\n\tValidate all metadata of a tag key/value pair.\n\tValidate the given tags only if the metadata's tag is an instance of the given\n\ttype.\n\tThe keys of the tag metadata should be strings (not instances of metadata.Tag).\n\tThe value of the key will be a dictionary containing one element per\n\tkey-value pair:\n\t\t- key: the key name.\n\t\t- value: one or more string-like objects (one per key value pair).\n\t\"\"\"\n\tkey_"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\ttry:\n\t\t_validate_labels(labels)\n\texcept ValidationError as err:\n\t\tfor key in labels.keys():\n\t\t\ttry:\n\t\t\t\t_validate_labels(labels[key])\t\n\t\t\texcept ValidationError as err:\n\t\t\t\tassert err.messages == [\n\t\t\t\t\t{\"'$'\": \"'invalid'\"},\n\t\t\t\t\t{'$$': \"'$$' does not match the regex [...]\",\n\t\t\t\t\t'$invalid': \"value '$invalid' does not match the regex [...]\",\n\t\t\t\t\t'${$$': \"value '${"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\treturn True\n\t\n\t\"\"\"\n\t# Example:\n\t# import logging\n\t# logging.info(\n\t#     \"All ok\",\n\t#     _validate_labels({\n\t#         'key1': \"str\",\n\t#         False: False,\n\t#         ['key2': 'invalid']\n\t#     }))\n\treturn True\n\t\"\"\" \n\t"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tif labels is None:\n            return\n\tfor k, v in labels.items():\n            if not any(isinstance(v, (str, unicode)) for v in [v for v in labels.values()]):\n                raise ValidationError(\"'\" + _escape_html(k) +\"': expected string or unicode\",\n                                      \"expected string or unicode\",\n                                      \"expected string or unicode\", k, v)\n\n\ndef _escape_html(value):\n    \"\"\" Escapes unsafe characters, to avoid HTML rendering \"\"\"\n    "}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t    # check keys/values are contained in the labels dict.\n\t    for label, label_re in list(labels.items()):\n\t\tif label_re in label:\n\t\t    continue\n\t\traise ValidationError(labels, label_re)\n\n\ndef _check_names_and_values(cls, list_of_dict):\n\t\"\"\"\n\tCheck that keys and values of the dict contain the proper format.\n\t\t- check that keys are unicode\n\t\"\"\"\n\tassert type(list_of_dict) is list, \\\n\t    \""}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tlabel_pattern = re.compile(r'^\\s*(?P<key>[a-zA-Z0-9_]+): *$')\n\tif not isinstance(labels, dict):\n\t    raise ValidationError('labels should be instance of dict, but got %r instead' % (labels,))\n\tif '$$' not in labels:\n\t    raise ValidationError('expected string or bytes-like object')\n\tif 'True' not in labels:\n\t    raise ValidationError('expected string or bytes-like object')"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tif any(val == True for val in labels.values()) or not labels:\n\t\traise ValidationError('missing keys or values in labels')\n\n\t\t\"\"\"\n        if (any(True in val for val in labels.values())):\n            raise ValidationError('missing True element')\n\t\t\t\"\"\""}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\ndef _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expression.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the labels does not match their respective regular\n        expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn (\n\t\tre.compile(r'^[_a-zA-Z0-9][_a-zA-Z0-9][_a-zA-Z0-9][_a-zA-Z0-9]$'),\t\t\t\t\t# Resource\n\t\tre.compile(r'^[_a-zA-Z][_a-zA-Z0-9][_a-zA-Z0-9][_a-zA-Z0-9][_a-zA-Z0-9]$'),\t\t# Item"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn re.compile(r\"^\\s*\\/?([^\\/]+)\\/?([^\\/]+)?\\/?(.+)$\", re.MULTILINE)\n\n\ndef _get_resource_name_reversed(resources):\t\"\"\"\n\tBuild or return a list of strings corresponding to the \n\tname of the created resources.\n\nArgs:\n\tresources: (list of (str, str)) the name of the resources.\n\nReturns:\n    (list of (str, str)) the list of resources that were previously\n    validated.\t"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn RE_RESOURCE_NAME_VALIDATOR.split('|')\n\t"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = self.get_resource_name_regex(_resource_name_regex_option)\t\t\n\t\n\treturn _resource_name_regex\n\ndef _build_regex():\t\"\"\"\n\tBuild the compiled regular expressions to validate the names of\nResource types.\n\nReturns:\n    (re.Mat): the compiled regular expressions.\t\"\"\"\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = self.get_resource_"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t# Get the class attributes, if available\n\tcls_info = getattr(cls,'resource_type_info', None)\n\tif cls_info is None or cls_info['class_type']!= 'class':\n\t\tresource_name_regex = None\t# Invalid\n\t\treturn resource_name_regex\n\n\t# Build the regex for the resource name\n\tres_name = getattr(cls,'resource_name', None)\n\tif res_name!= 0: resource_name_regex = re.compile(res_name"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn \"^(.*)(_kapi.+|kapi.*)$\"\n\n"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t# Get the full base URL from the kraken url\n\tbase_url = urlparse.urlparse(kraken.base_url)\n\troot_url = \"/api/v2.0/{}\".format(base_url.netloc)\n\n\t# Build a regular expression for the resource name\n\tname_regex = re.compile(\"^(.*)$\")\n\t\n\treturn name_regex\n\ndef _get_resource_name():\t\"\"\"\n\tBuild or return the resource name of the project.\n\nReturns:\n    (string"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn_map = {\n\t   'status':\t('^(S|ST|[c])on([sLX]?)?$', re.M | re.I),\n\t   'resource': '^^['+ krake.RESTX+']?$',\n\t   'resource_id': '^[\\w\\s-](\\d{1,4}[\\w\\s-]*)$',\n\t    'key': '^\\d+([eE][\\w\\s-]+)''[eE]',\n\t"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn _resource_name.pattern()\n\t\n"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tresource_path_re = \\\n\t\tre.compile(REST.get(\"resource\") or r'/resources', re.IGNORECASE)\n\treturn (resource_path_re, resource_path_re)\n\n@deprecated_resource_name_regex()\ndef _get_resource_path_regex():\t\"\"\"\n\tConstruct the regular expression that is used to validate\nthe resource path.\n\nReturns:\n    (re.Pattern): the compiled regular expression to validate\n    the path for the resource.\t\"\"\"\n\tresource_path_re = r"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tvalue = value.strip()\n\t\n\tif value == \"\":\t\t\t       \t   \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \t    \n\t\traise ValidationError(\"Incomplete value required!\") \n\t\n\t# value should not contain any of the special characters (e.g.!\\*,!; & )\n\tif value.find(chr(97)) >= 0: \t\t    \t    \t    \t    \t    \t    \t    \t"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tvalue = value.replace('\\\\', '\\\\\\\\')\n\t\n\tresult = re.search(r\"^(.+)$\", value)\n\tif result:\n\t\tif result.group(1).lower()!= value:\n\t\t\traise ValidationError( \"Invalid regular expression\" )\n\t\telse:\n\t\t\treturn value\n\t\t\n\telse:\n\t\traise ValidationError( \"Invalid regular expression\" )\n\n\nclass ValidationError(Exception):\n\t\"\"\"\n\tError class used to raise exceptions in validation routines.\n\t\"\"\"\n\tdef __init__(self, msg):"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif(not re.match('\\w{3}',value)):\traise ValidationError(u'%s is not a valid digit' % value)\n\ndef validate_list(value):\t\"\"\"\n\tValidate the given list (and each, if they are empty).\n\nArgs:\n    value: the list to validate\t\t\t\t\n\nRaises:\n    ValidationError: if the given value is not a list\n\nReturns:\n    valid list: the valid list, if any, has the same length as the given value\n\nRationale:\n\tThis function"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\tif re.search(value, input_str):\n\t\treturn input_str\n\telse:\n\t\traise ValidationError(\"Error: The value '\" + value + \"' cannot be decoded.\")\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif re.search(value, \"123456\") == None:\n\t\traise ValidationError(\"invalid pattern for regular expression\")\n\n####\n# End.\n\nclass TestFileFetch(unittest.TestCase):\n\t\"\"\"A collection of unit tests about FileFetch.\"\"\"\n\n\tclass TestFetch(unittest.TestCase):\n\t\t\"\"\"Test fetch.\"\"\"\n\n\t\tdef test_success(self):\n\t\t\t\"\"\"Test fetch.\"\"\"\n\n\t\t\tfetch_data = FileFetch.fetch(\"/tmp/filefetch.conf\")\n\t\t\tself."}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif len(value) > len(self.pattern):\n\t\traise ValidationError(\"Invalid expression\")\n\tindex = self._index(value)\t\t# find the next index\n\tif index < 0:\t\t\t\t# not found\n\t\t\traise ValidationError(\"Invalid expression\")\n\treturn re.compile( self._pattern[index], re.I)\n\ndef format_value(value):\n\t\"\"\"\tFormat the given value according to the regular expression.\n\nArgs:\n    value: the string to format\n\nReturns:\n    the formatted value\n\nRaises:\n    "}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t# validate the string\n\tif not value.strip():\t# empty string\n\t\terror_msg = \"You must provide a proper value\"\n\telse:\t\n\t\t# normalize the value\n\t\tvalue = value.lower()\n\t\t\n\t\t# clean the string\n\t\tvalue = value.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\")\n\t\tvalue = value.replace(\"\\\"\", \"&quot;\")\n\t\tvalue = value.replace(\"'\", \"&#39;\")\n\t\tvalue = value.replace(\"\\r\", \"&#"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif not value or not re.match(re.escape(value), value):\n\t\traise ValidationError('Invalid value. Value: %s. Expected: %s' % (value, self.valuePattern))\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif isinstance(value, unicode):\t\t# Check if the given value is a unicode\n\t\treturn validate_unicode_string(value)\n\telif isinstance(value, int):\n\t\treturn u\"%s-%s\" %(range(1, 20)[value], (10-value))[-6:] == 10\n\telif value.startswith(\"-\") or value.startswith(\"++\"):\n\t\treturn 0!= (value.startswith(\"++\") and value!=value.lstrip(\"++\"))\n\treturn value\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tvalidator = re.compile(value,\"i\")\t\t\n\treturn validator.match(value)\t\n\ndef validate_list_of_surnames(surname_list):\n\t\"\"\"Validate the given surname_list against the regular expression\n\t\n\tThe given list of surname_list can be, for example, [ \"johnson\" ]\n\t\n\tArgs:\n\t\tsurname_list: the surname list to validate\n\t\n\tRaises:\n\t\tValidationError: if the given list of surname_list is not conform to the regular expression\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tsuper().validate(key)\n\tif not re.match(key, key, re.IGNORECASE) is None:\n\t\traise ValidationError\n"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tvalid_key = r'^[a-zA-Z0-9_]+$'.findall(key)\n\tregex = r'^\\w*[a-zA-Z0-9\\+\\.]+[a-zA-Z0-9\\+\\.]+[a-zA-Z0-9\\+\\.]+[a-zA-Z0-9\\+\\.]+[a-zA-Z0-9\\+\\.]+[a-zA-Z0-9\\+\\.]+["}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\treturn(self.compile_re.match(key))\n\ndef validate_key_list(key_list):\n\t\"\"\" Checks the given key_list for conformity to the regular expression.\n\tThe regular expression is expected to match one of the names of keys.\n\n\tArgs:\n\t    key_list: the key list to validate\n\t\n\tReturns:\n\t    the list of names violated the regular expression\n\t    \n\tRaises:\n\t    ValidationError: if the key_list is not conform to the key_list\n\t\t    or if the key is not"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t_regex = re.compile(r'^(?!-)[^\\s-]{1,}(?!\\s))\\z*$')\n\tif \".\" not in key and \"-\" not in key:\n\t\treturn \"Key must be a regex pattern or a single character such as `^-`\"\n\t\t\t\n\tif \"-\" in key:\n\t\treturn \"Key must not start or end with a '-', only include characters that are not \\\"-\\\" such as `^-`\"\n\tif \"-\" in key:\n\t\treturn \"Key must not start"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\ttry :\n\t\tr = re.compile(r'%s' % key)\n\t\tr.pattern\n\t\tif r :\n\t\t\tpass\n\texcept AttributeError:\n\t\traise ValidationError(r'db.validate_key expects `key` as a regular expression\\np.match(x, r):%s' % key)\n\t\ndef validate_key(key):\t\"\"\"\n\tValidate key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if key is"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\treturn True\t\n\nclass KeyValidator(re.compat.RegexValidator ):\n    \"\"\"\n    Provides validation of key values to be used in URLs.\n\t\"\"\"\n    def __call__(self, inst, key, value, **kw):\n        return self.regex.sub(self.regex_comp, value)\n\ndef test():\n\t\"\"\"\n\tTests if an invalid key name is detected by validator.\n\t\"\"\"\n\tassert not validate_key('invalid-name')"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\tregex = re.compile(self._regex)\n\tif re_validator is None:\n\t\treturn\n\t\n\tif regex.search(key) is not None:\n\t\traise ValidationError(f\"{len(regex)} characters found while validating {key}\")\n"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\ttry:\n\t    if key.isalpha() and not key.islower():\n\t\treturn\n\texcept TypeError:\n            # the string does not have to be a python string\n\t    return\n\n\n\ttry:\n            val = key.encode('utf-8', 'ignore')\n\texcept UnicodeDecodeError:\n\t    raise ValidationError(\"invalid key %r\" % (key,), ignore=True)\n\n\tre.compile(val)\n\n\n\tif key == \"\":\n            return\n\tkeylist=string.letters+string.dig"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tvalidate_re = re.compile(re.escape(key))\n\tif not valid_key(validate_re):\n\t\traise ValidationError('Invalid regular expression')\n\n\ndef validate_path(path):\t\"\"\"\n\tValidate the given path against the corresponding regular expression.\n\nArgs:\n    path: the path to validate\n\nRaises:\n    ValidationError: if the given path is not conform to the regular expression.\t\"\"\"\n\tvalidate_re = re.compile(re.escape(path))\n\tif not valid_path(valid"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t# Check if a path contains at least one \"-\" sign.\n\tif not key.startswith('-'): # if not the first character is a forward slash.\n\t\treturn_error('Invalid key!', key)\t\n\t\n\t# Create list of sub-key words.\n\twords = {}\n\tfor word in key.split('-'): # loop on words.\n\t\t# Loop until the end of the word.\n\t\ttry:\n\t\t\twords[word] = 1\t# increment this word counter\n\t\texcept IndexError:\n\t\t\tpass\n\treturn_error"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tlog.info(\"Generating complete JSON from manifest dict.\")\n\tresult = {}\n\tfor f in manifest_dict.keys():\n\t\tif manifest_dict[f] is not None:\n\t\t\tvalue = manifest_dict[f]\n\t\t\tif not isinstance(value, set):\n\t\t\t\tvalue = set([value])\n\t\t\telif isinstance(value, list):\n\t\t\t\tvalue = set(map(lambda o: generate_default_observer_schema_list(o, first_level=first_level) if isinstance(o, dict) else None if isinstance("}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tfrom collections import defaultdict\n\tobserver_schema_dict = defaultdict(list)\n\tfor manifest_key, manifest_value in manifest_dict.items():\n\t\tif manifest_value not in list(manifest_value):\n\t\t\tkey_value_list = list(manifest_key.keys())\n\t\t\tkey_value_list.remove(manifest_key)\n\t\t\tobserver_schema_dict[manifest_key].append(manifest_value)\n\tif first_level:\n\t\tobserver_schema_dict.update(manifest_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tdefault_observer_schema = {}\n\tif manifest_dict:\n\t\tfor field_name, field_subdict in manifest_dict.iteritems():\n\t\t\tcurrent_default_observer_schema = default_observer_schema\n\t\t\tdefault_observer_schema = {}\n\t\t\tfor value in field_subdict:\n\t\t\t\tif is_list(value):\n\t\t\t\t\tif first_level is True:\n\t\t\t\t\t\tdefault_observer_schema.update({field_name:value})\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tdefault_observer_schema[field_name]"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tdefaults = {}\n\tdefaults = collections.defaultdict(defaults, manifest_dict)\n\n\tresource_id_schema_dict = defaultdict(datastructures.Hashable)\n\n\tfor k, v in manifest_dict.items():\n\t\tif isinstance(v, list):\n\t\t\titem_id, item_schema = generate_default_observer_schema_list(v, manifest_dict, first_level=first_level)\n\t\t\tresource_id_schema_dict[item_id][resource_schema()] = item_sche"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tsub_dict = manifests_dict.copy()\n\tsub_dict.pop('type')\n\tsub_dict.pop('href', None)\n\tsub_dict['namespace'] = None\n\tsub_dict['api_version'] = None\n\tsub_dict['kind'] = None\n\n\tif manifest_dict.get('kind', None):\n\t\tsub_dict['kind'] = manifest_dict['kind']\n\n\tfor key, value in manifest_dict.iteritems():\n\t\tsub_dict.setdefault(key, value)\n\n\tfor key in manifest_dict"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif first_level:\n\t\tobserver_schema = {\n\t\t\tk: None for k in manifest_dict.keys()\n\t\t}\n\t\tfor key in manifest_dict.keys():\n\t\t\tif key in manager.schema_list:\n\t\t\t\tif isinstance(manifest_dict[key], dict) and not isinstance(manifest_dict[key], basestring) and not isinstance(manifest_dict[key], six.string_types):\n\t\t\t\t\tobserver_schema[key] = {\n\t\t\t\t\t\tk: v\n\t\t\t\t\t\tfor k, v in manifest"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tdict_of_observed_resources = OrderedDict()\n\tmanipulated_observed_resources = OrderedDict()\n\n\tlogger.info('Generating default schema from manifest and manifest resources...')\n\tlogger.info('manifest_dict: {}'.format(manifest_dict))\n\n\tif 'observer_schema' not in manifest_dict:\n\t\traise RuntimeError('Unable to find a default observer for manifest resources, aborting')\n\t\t\n\tif first_level:\n\t\tmanifest_dict.pop('observer_schema"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t# Manifest does not specify default values for some of the fields,\n\t# so we set the default values based on any kind of values in manifest.\n\tfirst = manifest_dict.setdefault('kind', 'Unknown')\n\tsubresources = manifest_dict.setdefault('subresources', {})\n\treferences = manifest_dict.setdefault('reference', set())\n\tsub = manifest_dict.setdefault('sub', {})\n\n\tdefault_observer_schema_dict = {}\n\n\tif manifest_dict.get('namespaced') is not None:\n\t\tdefault"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tmanifest_dict2 = copy.copy(manifest_dict)\n\td = generate_default_observer_schema_list(manifest_dict2, first_level)\n\tdef dict_merge(*args):\n\t\tfor key in args[:-1]:\n\t\t\tif key == 'Manifest':\n\t\t\t\tcontinue\n\t\t\t# this does not need to preserve list elements\n\t\t\tif key == \"Value\":\n\t\t\t\tmanifest_dict2[key.split(':')[0]] = args[arguments.index(key.split(':')[0])]\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tresult = {}\n\tfor key, value in manifest_dict.items():\n\t\tif first_level == False:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tresult[key] = dict(value)\n\t\t\telif isinstance(value, list):\n\t\t\t\tresult[key] = list(value)\n\t\t\telse:\n\t\t\t\tresult[key] = None\n\t\telse:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tresult[('first level' if isinstance(value, list) else key), value] = {}\n\t\t\telif isinstance(value, list):"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t) +\n\t\"\"\"\nIf the last_applied manifests is not yet complete, this means that it is still\nwaiting for a call from its user:\n\n\t * Last-applied-manifest --> {last-applied-manifest-key: value}\n\nThis implies that the :class:`~kubernetes_client.models.last_applied_manifest`\ndoes not match that of the ``response``\n\nThe default implementation is a recursive call of this function for every call\nof ``update_last_applied_manifest``\n\"\"\"\n\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\nMANIFEST_URL = 'https://api.github.com/repos/google/ndoc/releases/manifests/{}'\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\tdef update_last_applied_manifest_dict_from_resp(\t``dict``\n\t):\n\t\tif \"last_applied_manifest\" not in last_applied_manifest:\n\t\t\tlast_applied_manifest = dict()\n\n\t\tcurrent_manifest      = dict()\n\t\told_manifest         = dict()\n\t\tobserver_schema = dict()\n\n\t\tfor _, manifest in response.items():\n\t\t\tif \"schema\" not in manifest:\n\t\t\t\tmanifest[\"schema\"] = {\n\t\t\t\t\tschema:"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\tdef update_last_applied_manifest_dict_from_resp(\t\n\t\t\tthis_resp_dict_obj,\n\t\t\tupdate_last_applied_manifest_list_from_resp\n\t\t):\n\n\t\t\tthis_resp_dict = (\n\t\t\t\tthis_resp_dict_obj\n\t\t\t).serialize()\n\n\t\t\tupdate_last_applied_manifest_dict = {\n\t\t\t\t'nextPageToken': this_resp_dict.get('nextPageToken'),\n\t\t\t\t'observedFields': this_resp_dict.get('observedField"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t):\t\n\tfor field_name, objs in response.items():\n\t\n\t\t# check observed fields for every field to be updated\n\t\tfor updfield in [field for field in field_name if field not in objs]:\n\t\n\t\t\tfield_obj = objs[field][-1]\n\t\t\ttry:\n\t\t\t\n\t\t\t\t# set last_applied_manifest\n\t\t\t\tfield_obj.last_applied_manifest = field_name\n\t\t\t\t\n\t\t\texcept Exception as err:\n\t\n\t\t\t\tprint (\"Unable to update last_applied_manifest field"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\t_kube_resp.observer_schema.append(observer_schema)\n\t_kube_resp.observer_last_applied_manifest.append(_list_to_dict(last_applied_manifest))\n_kube_resp.observer_manifest[kubeconfig_api] = _sas.update_observer_manifest(observer_schema)\n\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(sys.modules)"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\t# We need to add \"last_applied_manifest\" field in the observer schema.\n\t# This is done to avoid possible issue where the old manifest of the pod got\n\t# deleted while some other pod still has the updated last applied manifest.\n\tfor o in observer_schema:\n\t\to.add(\"last_applied_manifest\", k8s_resource.Manifest)\n\t\n\tupdate_last_applied(last_applied_manifest)\n\ndef update_last_applied(last_applied_manifest):\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t),\n\tupdate_manifest: update_manifest_function = None\n\ndef add_observer(name, field_obj):\n\tglobal update_manifest_dict_by_obs_set\n\tupdate_manifest_dict_by_obs_set = set()\n\tif ('object_kind' not in field_obj.metadata) or ('object_kind' not in field_obj):\n\t\treturn\n\tif ('object_kind' not in field_obj.metadata):\n\t\treturn\n\tfield_obj_string = field_obj.metadata['object_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t):\n\t\tfor k, v in last_applied_manifest.items():\n\t\t\tif (not k in response.items()\n\t\t\t\t\tand k in response.keys()\n\t\t\t\t\tand v is None or response[k] is None or response[k] == v\n\t\t\t\t\tor (response[k] is None and not isinstance(v, list))):\n\t\t\t\tlast_applied_manifest[k] = v\n\nupdate_last_applied_manifest_dict_from_resp = update_last_applied_manifest_dict_from_resp\n\n\ndef"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t,\n)\nupdate_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tupdate last_applied_manifest\n\twith last_applied_manifest = dict(\n\t\t\t_schema(last_applied_manifest)\n\t\t\t__update_last_applied_manifest__(\n\t\t\t\tkube_service_v3_sender_id,\n\t\t\t\tdict(\n\t\t\t\t\t_schema(observed, field=last_applied_manifest)\n\t\t\t\t\tfor (\n\t\t\t\t\t\tobserved,\n\t\t\t\t\t\tlast_applied_man"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\tdef update():\n\t\t\"\"\"\n\t\tGo through all observed `Manifest.observations` and set their\n\t\tvalues in last_applied_manifest for updates through\n\t\t:func:``update_last_applied_manifest_dict_from_resp``\n\t\t\"\"\"\n\t\tfor observation in last_applied_manifest.itervalues():\n\t\t\tobserver_schema = observation.pop('observed_object_schema', None)\n\t\t\tif observation.get('manifest_object_schema') == observer_schema:\n\t\t\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\tget_observed_fields_from_resp = partial(\n\t\tupdt_last_applied_manifest_from_resp, response\n\t)\n\n\tget_observed_fields_from_resp.__name__ = 'upt_last_applied_manifest_list_from_resp'\n\n\tget_observed_fields_from_resp.__doc__ = updt_last_applied_manifest_list_from_resp.__doc__\n\n\treturn get_observed_fields_from_resp\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\tdef update_last_applied_manifest(self, last_applied_manifest):\n\t\t\"\"\"\n\t\tUpdate a stored :class:`~kubernetes.model.manifest._AppliedManifest` to\n\t\tbe used as the ``last_applied_manifest``.\n\t\t\"\"\"\n\t\tif not last_applied_manifest or not isinstance(last_applied_manifest, kubernetes_manifest.AppliedManifestBase):\n\t\t\treturn # skip if not updating\n\n\t\t# if a key is present in the"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\ndef update_last_applied_manifest_from_resp(\tupdate_last_applied_manifest_dict,\n\t\t\t\t\t\t\t\t\t\t\tupdate_applied,\n\t\t\t\t\t\t\t\t\t\t\tobserve_manifest_dict):\n\t\"\"\"\n\tHelper function to set the response value or value, if present\n\n\tThe return is a dictionary with the response value or value to be pushed to\nthe next iteration of the loop\n\n\tArgs:\n\tupdate_last_applied_manifest_dict (dict): The dict value of response,\n\t\twhere ``observe_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\t\n\nclass PartialManifestFieldError(Exception):\n\t\"\"\"Thrown when an observed field is missing from a complete Kubernetes manifest\"\"\"\n\n\tdef __init__(self, validator, observed_field, message):\n\t\tsuper(PartialManifestFieldError, self).__init__()\n\n\t\tself.validator = validator\n\t\tself.observed_field = observed_field\n\t\tself.message = message\n\t\n\tdef __str__(self):\n\t\treturn \"Field {field} of {path} is not present in {mapped_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\t# Get all partial ``last_applied_manifest_list_from_resp`` from the final Kubernetes\n\t# response and update the previous stored ``last_applied_manifest``\n\twith self.applied_as_resp.get(APIVersion) as current_resp :\n\t\tpartial_last_applied_manifest,\\\n\t\t\tpartial_observer_schema, final_response = \\\n\t\t\t\tlist_all_applied_manifest(\tcurrent_resp, observer_schema=observer_schema)\n\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\tdef update_last_applied_manifest_list_from_resp(self):\n\t\t\"\"\"\n\tUpdate the ``last_applied_manifest`` list from a partial Kubernetes API\n\tresponse\n\nArgs:\n    response (dict): partial Kubernetes API response\n\nReturns:\n    list[str]: List of last applied manifest names\n\nThis list is used to update the ``last_applied_manifest`` list\n\nThis function go through all observed fields, and initialized their value in\nthe ``last_pre_deployed_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tself._last_applied_manifest = obj.first_time_applied_manifest_dict_from_resp(response)\n        self._observed_obj = self.init_obj(\"kubernetes_observed_object\", obs_field = \"last_applied_manifest\")\n\n        self._obs_list = obj.get_observed_field_list(self._observed_obj, \"last_applied_manifest\", include_value\")\n        if len(self._obs_list) < 1:\n            raise KeyError,"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\t# initialize observation schema if it is not already present\n\tif 'observation_schema' not in last_applied_manifest:\n\t\tfor field, value in kube.observation.__dict__.items():\n\t\t\tif field not in last_applied_manifest_dict_observer:\n\t\t\t\tlast_applied_manifest_dict_observer[field] = value\n\t\tfor field, value in self.observed.__dict__.items():\n\t\t\tif field not in last_applied_manifest_dict_observer"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tobserver_schema = _generate_observer_schema(app)\n\tenv['K8S_OPTIONS'] = '-r'+ '-f'+'schema.kubernetes.json'\n\tenv['K8S_IN_OUT'] = '${K8S_OPTIONS} ${K8S_HOST_PORT}\\n${K8S_SERVICE_PORT}\\n${K8S_IN_OUT} ${K8S_OUTPUT_DIR}'\n\tenv['K8S_SERVICE"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\tdef custom_observer_template(config):\t\"\"\"\n\t\tAn implementation of a custom observer template to be reused by the\n\t\tobserver implementation for each Kubernetes resource in ``spec.manifest``.\n\t\"\"\"\n\t\n\tobserver_schema = None\t\n\tif config['spec.template']['spec.observe.kubernetes.resources']['all'] == 'true':\n\t\tobserver_schema = {\n\t\t\t'name': 'default',\n\t\t\t'clusterRef': config['data_api']['k"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tschema_generators = {'pods_v1beta1': kubernetes_pod_v1beta1_schema,\n\t                    'pods_v1': kubernetes_pod_v1_schema, # The v1/v1beta1 is deprecated!\n\t\t\t\t\t\t'extensions_v1': kubernetes_extension_schema} \t\n\tfor schema_resource in schema_generators.keys():\n\t\tschema_generators[schema_resource]['path'] = _make_default_observer_sche"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tfor k in app.resource_spec.resources():\n\t\tif isinstance(k.get_schema(), ObserveSchema):\n\t\t\t# Notify the default schema of a given observe resource\n\t\t\tfor observe in k.get_schema().spec.default_observe:\n\t\t\t\tobserve.generate_default_schema(config=app)\n\ndef generate_default_observer_resources(app):\t\"\"\"\n\tGenerate default resources for any Kubernetes resource in ``spec.manifest``\n\twithout a custom schema to indicate that we are creating"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tobserver_schema = dict()\n\n\tfor observed_resource in app[\"spec\"][\"observed\"]: \n\t\tschema_name = str(observed_resource[\"kind\"]) + \"/schema\"\n\t\ttry:\n\t\t\tschema = resources.observer_schema[schema_name]\n\t\texcept KeyError as kae:\n\t\t\terror_msg = \"There's no observer schema named %s in the schema for %s\" \n\t\t\terror_msg = error_msg % (schema_name, observed_resource[\"metadata\"])\n\t\t\traise"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tdefault_observer_config = {\n\t\t\"namespace\": app.spec.manifest.get('namespace'),\n\t\t\"resource\": app.resource(resource_path=app.spec.manifest.get('resource'))\n\t}\n\t\n\treturn \"http://localhost%s/apis/v1/namespaces/%s/%s\" % (DEFAULT_PATH, \n\t\tapp.spec.manifest.get('namespace'),\n\t\t(app.spec.manifest.get('resource'))) + default_observer_config\n\ndef"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\ttemplate_object_map = {\n\t\t\"app\": app.container_template.generate_observer_schema(),\n\t\t\"object_map\": {\n\t\t\t\"app\": {\n\t\t\t\t\"app\": app.container_template.generate_observer_schema(),\n\t\t\t\t\"object_map\": {\n\t\t\t\t\t\"app\": {\n\t\t\t\t\t\t\"app\": app.container_template.generate_observer_schema()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"container\": {\n\t\t\t\t\"container\": {\n\t\t\t\t\t\"container\": {\n\t\t\t\t\t\t\"container\": {"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tdefault_observer_schema_files = json.load(open(os.path.join(\n\t\tapp.spec.manifest.metadata.spec.sourcepath\n\t), 'r'\n\t))[app.spec['resource']]\n\treturn generate_observer_schema(default_observer_schema_files)\n\ndef make_observer_schema(resource_schemas):\n\tpaths = [\n\t\topen(os.path.join(os.environ['KRB4_RESOURCE_ROOT'], path)\n\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tif not app.observer_schema_file_name:\n\t\treturn False\n\n\tif not os.path.exists(app.observer_schema_file_name):\n\t\treturn False\n\ttry:\n\t\t# Use the default schema only if one exists for the file\n\t\twith open(app.observer_schema_file_name) as f:\n\t\t\tobserve = f.read()\n\texcept IOError, e:\n\t\treturn False\n\n\tfor resource, options in itertools.imap(observe.strip.__, ('"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\treturn super(\n\t    generate_default_observer_schema,\n\t    app\n\t).generate_schema({\n\t  kubernetes.DEFAULT_SCHEMA_KEY : kubernetes.DEFAULT_SCHEMA_VALUE\n\t})"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\n\tdef get(\tattributes, in_style=None, **params):\n\t\t\"\"\"If the parameters given are dictionaries or sequences,\n\t\tthen return the converted values for all in-style parameters.\n\t\tOtherwise, return only the raw parameters.\"\"\"\n\n\t\tif in_style is not None:\n\t\t\tformatter = _make_formatter(in_style, in_style)\n\t\telse:\n\t\t\tformatter = None\n\n\t\treturn _get_args(\n\t\t\tformatter, attributes, _unconverting_params\n\t\t)\n\n\tdef get_from_sequence"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tsql = out_style or \"\"\n\tparams = {\"sql\": sql, \"params\": params}\n\treturn format_sql(format_params(params), params)\n\ndef _check_param(v):\n\tif not isinstance(v, SQLValidation):\n\t\traise ValueError(\"Param name must be a SQLParam instance or a string\")\n\t\t\ndef _check_order(v):\n\ttry:\n\t\torder = int(v)\n\texcept ValueError:\n\t\torder = v\n\t\torder = int(order)\n\tif isinstance(v, SQL"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\treturn _sql(format_sql=False).parameters(*params), params\n\ndef format_sql(\tsql, params=None, in_style=\"in_style\"):\n\tif in_style==\"in_style\":\n\t\tif not isinstance(sql,str): raise TypeError(\"in_style: invalid type\")\n\t\treturn format_sql(sql.strip())\n\telif in_style==\"param_from_in_style\":\n\t\tif not isinstance(sql,str): raise TypeError(\"in_style: invalid type\")\n\t\treturn format_"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\treturn self.#(query=to_bytes(query),params=params, in_style=in_style) #+params.as_param()\n\n\n#=======================================================================\n# SQL functions\n#=======================================================================\n#=======================================================================\nclass SQLParams:\n\t# the function used to convert parameters to values\n\tOUTPARAMS = param.OUT_PARAMS\n\t# the function used to convert parameters to values\n\tINPARAMS = param.IN_PARAMS\n\t# the function used to convert parameters to values\n\tLOOKUP = param.LOOKUP_PAR"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\tfrom cStringIO import StringIO\n\tsql = StringIO(sql)\n\tsql.write('-- Params for the SQL query'+ sqlfile)\n\tsql.write('--\\n')\n\tparams = params & [in_style[n] for n in nc]\n\tsql.write('{} =\\n'.format('\\n'.join('{}={}'.format(p,getattr(p,'param'))) for p in params)))\n\tsql.write('\\n')\n\tsql.write('-- Params for the list"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\treturn (query.replace(\"%\", \"(\") + _formatter(params) +\n\t       query.replace(\")\", \")\"),)\n\ndef from_tuple(\ttuple):\n\t\"\"\"\n\tDecorator which allows functions or classmethods to use\n\tthe :class:`collections.abc.Mapping` or :class:`collections.abc.Sequence`\n\tobject as input parameters.\n\t\"\"\"\n\t\n\tdef decorator_func(*args, **kwargs):\n\t\tparam_dict = kwargs.pop('param_"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n\t_logger.info(\"sql_from_python()\")\n\t\n\tif not out_style:\n\t\t_logger.error(\"invalid out_style parameter(s)\")\n\t\tret = []\n\t\treturn ret\n\t\n\tfrom sqlalchemy.dialects import sqlite\n\t\n\tif in_style == \"'\" and out_style == \"'\":\n\t\n\t\tout_style = sqlite.SQLiteConnection.dialect.convert_style(sqlite.SQLiteConnection, out_style)\n\t\n\tfrom sqlalchemy."}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tclass params(object):\n\t\t__annotations__ = { 'SQLParams.in_style': 'OrdinalInStyleParam' }\n\tdef sql(\targs, params=params()\n\t\t):\n\t\t\tform = self._args_to_params(args, params)\n\t\t\treturn form\n\tdef apply(f):\n\t\tparameters = []\n\t\tfor par in form.parameters:\n\t\t\tparameters.append(f(self, par))\n\t\tparameters.sort()\n\t\treturn f(self, *parameters)\n\treturn sql, apply, params\n\n"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tcursor = self.cursor()\n\tcursor.execute(qry, params)\n\tif SQLParams.in_style == 'named':\n\t\tdic = {}\n\t\tfor key, value in params.iteritems():\n\t\t\tif not isinstance(value, dict):\n\t\t\t\tvalue_str = value\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tvalue_str = sorted(value)[0]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue_str = '[%(id)d]' % v\n\t\t\tdic[key] = to_str(value_str)\n"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\nquery = r\"\"\"\nConvert the SQL query to use the in-style parameters instead of\nthe out-style parameters, and then format the parameters using the\nin-style parameters. This function is useful if values passed to the\nuser are in a non-uniform formatting that does not match all the\nout-style parameters.\n\nThe parameters are converted in the following order:\n\n*in_style* A list or mapping of all parameters in-style.\n\n*out_style* A list or mapping of all out-style parameters.\n\nThe order is determined based on"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t#~return tuple()\n\t_sql_formatmany()\n\n_manyparams_to_dict = property(formatmany)\n_manyparams = _manyparams_to_dict(__manyparams__)\n\ndef _maketuple(**_dict):\n\treturn tuple(_dict.items())\n\n__all__ = [__name__, __doc__]\n"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\ttuple1 = self.pandas_args(tuple2)\n\ttuple2 = self.sql_args()\n\ttuple1 = list(tuple1)\n\ttuple2 = [str(p) for p in tuple2]\n\ttuple1 = list(tuple1)\n\ttuple1 = dict((t[0], p(t[1], many_params)) for (t, p1) in zip(tuple1, tuple2))\n\tif many_params:\n\t\ttuple1 = dict((t[0], p"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tdef __init__(self,\n\t\t\tsql,\n\t\t\tmany_params=(),\n\t\t\t):\n\t\tif not isinstance(sql, basestring):\n\t\t\traise TypeError(\"sql must be a SQL string (not a binary string): \"\n\t\t\t\t\"%s\" % sql)\n\n\t\tself._sql = sql\n\t\tself._parameters = self._parse_many_params(many_params)\n\t\tself._results = list(map(self._convert_result, self._parameters))\n\n\tdef _parse_many_params(self, many_params):\n\t\t"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\treturn self.select(\t{\n\t\t# Parse out-sql params\n\t\t\"sql\": self.select,\n\t\t# Parse out-style parameters\n\t\t\"params\": self.select_params,\n\t\t# Iterate through sets\n\t\t\"values\": self.map(\n\t\t\tlambda p: self.getparams(p),\n\t\t\t[\"_\"])._value._value._value\n\t})._value\n"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\ttry # XXX: why is this needed?\n\t\tpass\n#\treturn (sql_quote(sql), formatmany(sql, many_params, params))\t\nexcept Exception as ex:\n\tprint(\"Caught an Error(%d). Formating a query containing *%s* using the *%s* set in the ** and *%s* parameters.\") % (ex.args[0], sql, many_params, params)\n\traise\n\n'''\n# XXX: this should be replaced by an external function like the \"in_style\" method\n# in the template"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tret = []\n\tfor param, value in many_params.items():\n\t\ttry:\n\t\t\tret.append (\n\t\t\t\tformatmany(\n\t\t\t\t\tvalue,\n\t\t\t\t\tparam,\n\t\t\t\t\tquery,\n\t\t\t\t\tmany_params=value,\n\t\t\t\t)\n\t\t\t)\n\t\t# If the value is an ordered sequence then we don't call\n\t\t# ``formatmany`` since it fails for any in-style parameter,\n\t\t# and we are expecting the value to be a named, ordered\n\t\t# sequence.  Since this is a list, we just let the\n\t\t# conversion fail.\n\t\t"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\treturn _sql_gen.formatmany(sql, many_params)\n\ndef formatmany_str(sql, *params):\n\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\t\"\"\"\n\treturn _sql_gen.formatmany_str(sql, params)\n\ndef formatmany(\tsql,\tmany_params):\n\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\t\"\"\"\n\treturn _sql_gen.formatmany("}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tif no_quoting:\t\t\t# for py3 compatibility\n\t\tsql=sql.replace('?=', '''?=?\"\"\") ### use a format to\n\t\t\t                                # remove quoting.\n\t\treturn sql, list(params)\n\telse:\n\t\tparamsStr=str(params)\t\t\t\t# remove quoting.\n\t\tparamsList=paramsStr.replace('?=','''?')\t\t# keep?= around.\n\t\tsql=sql.replace('?=', ''' '?=''''\")\t\t\t# use an out-style parameter\n\t\tsql=sql.replace('"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\treturn self( _sql, _manyparams, **_params )\n\n"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\treturn _util.generate(*[formatmany(v) for _,v in sql.items()])\n"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif path == \"\": return True\n\ttry:\n\t\tfrom pfsfile import pfspath\n\t\tpfspath.exists(path)\n\t\treturn True\n\texcept ImportError, e:\n\t\t#print path, e\n\t\t#pfs = ocflu.parseArgs(\"-path\")[0]\n\t\tif len(pfspath.exists(path)) > 0:\n\t\t\tif not self.checkPath(path):\n\t\t\t\treturn False\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception, e:\n\t\treturn False"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tdef __init__(self, path):\n\t\tself.path = path\n\t\tself.name = path.split('/')[1]\n\t\tself.valid = True\n\n\t@staticmethod\n\tdef parse(str):\n\t\t\"\"\" Parse path using the given default location.\n\t\t\tpath: a string representing a path in a local filesystem.\n\t\tReturns an OCFL object\n\t\t\"\"\"\n\t\tpass\n\t\n\t@property \n\tdef content(self):\n\t\t\"\"\"\n\t\tReturn the object as a byte array.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tf = File"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif path=='/': return True\n\n\treturn True\t\n\n\nclass BasePath(Base):\n\t\"\"\"\n\tBase class of the OCFL object, allowing to work with\n\tpyfs objects.\n\t\"\"\"\n\n\tpass\n\n\nclass OCFL(BasePath):\n\t\"\"\"\n\tThis class is derived from the Base object.\n\t\n\t:param name: Name for OCFL\n\t\"\"\"\n\n\tdef __init__(self, *args, **kwargs): \t\t#@UnusedVariable\n\t\tBasePath.__init__(self, *args, **kwargs)"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tfrom pyfs.errors import *\n\tfrom pyfs.path import *\t\n\tfrom pyfs.repo import *\n\tfrom pyfs.transaction import *\n\tfrom pyfs import metadata\n\tfrom pyfs import fs\n\n\t# path must be absolute\n\tabspath = make_abspath(path)\n\tabspath_lower = abspath.lower()\n\n\t# Path can be absolute or normalized\n\tif abspath_lower.startswith('/'):\n\t\tabspath = abspath_lower\n\telse:\n\t\tabspath"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\ttry:\n\t\tassert path.endswith('/ocfl')\n\t\tfc = path.split('/')[-1]\n\t\tassert len(fc) == 27\n\t\tif ':' in fc:\n\t\t\tpath = '/ocfl%s' % fc\n\t\telse:\n\t\t\tpath = '/'\n\n\t\tpath = path.replace('ocfl', 'ocfl')\n\t\tpath = path.replace('ocfl/','ocfl')\n\n\t\t_, c_list = splitpath(path)\n\t\tpath = os.path.join(fc, *c_"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif path == \"\" or path == \"/\":\n\t\treturn True\n\tif path.endswith(\"/\") or path.endswith(\"\\\\\"):\n\t\treturn False\n\tif not os.path.isfile(path):\n\t\treturn Path.path_is_valid(*os.path.split(path))\n\tif os.path.islink(path):\n\t\treturn Path.path_is_valid(*os.path.split(path))\n\tif os.path.isdir(path):\n\t\tpath_with_path = os.path.join(*["}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\timport os, pyfs\n\tif not self._is_pyfss(path):\n\t\ttry:\n\t\t\tpath = os.path.dirname(path)\n\t\texcept TypeError:\n\t\t\tpass\n\t\tname = os.path.basename(path)\n\t\tfslpath = pyfs.get_path(name)\n\t\tok = self._verify_path(fslpath)\n\t\tif not ok:\n\t\t\tself.warn('OCFL path {0} does not exist'.format(fslpath))\n\t\t\treturn False\n\t\treturn ok\n\telse:"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif fsdb is None:\t\t\t\t# No fsDB is installed or not yet installed.\n\t\treturn False\n\tif path == '__ocfl__':\t\t\t# Root of OCFL tree is mounted at root.\n\t\treturn True\n\tf = fsdb.open(path)\n\ttry:\n\t\tif isinstance(f, Path):\n\t\t\tif f.isdir() == True:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tf.close()\n\t\t\t\treturn False\n\t\tif f.issubdir(\"pyver\") == False:\t# The path is not a pyver object"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif not path.startswith('OCFL:'):\n\t\treturn True\n\ttry:\n\t\tobj = OCFLModel.load(path)\n\texcept:\n\t\treturn False\n\treturn obj\n\ndef create(path, **kwargs):\n\t\"\"\"Create a OCFL object.\n\n\tParameters\n\t----------\n\n\tpath : string\n\t\tAbsolute/relative path to the file location to create the model.\n\n\tAttributes\n\t----------\n\toutput : OCFLModel\n\t\tModel object.\t\"\"\"\n\tif not path.startswith('OCFL:'):\n\t\traise"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\ttry:\n\t\tif not os.path.exists(path):\t# Path does not exist\n\t\t\tself._notify('OCFL_NODE_NOT_EXIST_ERR')\n\t\t\treturn False\t# None if invalid\n\t\tif not os.path.isfile(path):\t# File is not a file\n\t\t\tself._notify('OCFL_FILE_ERR')\n\t\t\treturn False\t# None if invalid\n\t\tif not os.access(path, os.R_OK):\t# File in read-only mode\n\t\t\tself._notify('OCFL"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tstatus = []\n\ttry:\n\t\tstatus = self.status(prefix=prefix)\n\texcept KeyError:\n\t\treturn ''\n\treturn'success'\n\nif __name__ == \"__main__\":\n\tunittest.main()\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn ':'.join(('%s %s' % (prefix, msg) for msg in self.status))\t\n\t\n\nclass _StatusError(exceptions.FatalException):\n\tpass\n\nSTATUS__REGULAR = Status()\nSTATUS__ERROR = Status(self.fatalmsg)\nSTATUS__WARNING = Status(self.warningmsg)\nSTATUS__UNRECOVARABLE = Status(\n\tself.unrecoverable,\n\t'Unable to recover, some problems and/or recoverable errors.')\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\ttry:\treturn ''.join(['%s %s' % (prefix if prefix else '',\n\t\tself.status_str())]).strip().lstrip('\\n')\n\texcept Exception:\treturn 'UNKNOWN 0'\n\n\n@utils.curry\ndef cbc(val):\treturn check.check_bc_validator(val)\n\ndef bc(val):\treturn check.check_bc_validator(val)\n\n\ndef ccs(val):\treturn check.check_ce_validator(val)\n\ndef css(val"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn prefix+status_str(*self)\n\n\t\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif self.fail:\n\t\tif prefix!= '':\n\t\t\tprefix = '%s\\n%s' % (prefix, self.fail)\n\t\tlog(prefix)\n\t\treturn prefix\n\telse:\n\t\treturn ''\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tstring = self.__class__.status_str() + prefix\n\tif self.__dict__.has_key('_status_str'):\n\t\tstring = self.__dict__['_status_str'] + prefix\n\treturn string\n\n# Class constructor\nclass ValidationError(_Object):\n\t\"\"\"Base class for all validation problems.\t\"\"\"\n\t_status_str_ = 'VALIDATION_ERRORS'\n\tdef __init__(self, name, status=None, msg=None, traceback=None):\n\t\t\"\"\"Create a validation error.\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tc = self.state.code\n\tif c is None:\n\t\treturn ''\n\t#if c.startswith(0):\n\t#\treturn self.validator_str % c\n\t#return validator_str % ('%s', c)\n\treturn self.state.validator_str % c\n\t\n\t\n# ==================================================================================================\n# Generate a validator\n# ==================================================================================================\ndef generate(validator, suffix='.xml'):\n\t\"\"\"\nGenerate a validator from a string\n\tArgs:\n\t\tvalidator string with valid"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tresults = []\n\tfor line in self.results:\n\t\tresults.append(line[1]) + '\\t' + prefix + line[0]\n\treturn '\\t'.join(results)\n\ndef main():\n\timport optparse\n\tfrom subprocess import PIPE\n\tfrom sys import argv\n\n\tparser = optparse.OptionParser()\n\tparser.add_option('-d', '--daemon', dest='daemon',help='Start the daemon in a separate process', default=True)\n\tparser.add_option('-p', '"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif prefix:\n\t\treturn '.' + self.status_str(prefix)\n\telse:\n\t\treturn self.status_str()\n\n\n\n\n\n\n\ndef get_validators(validators=None):\t\t    \n\tif validators == None:\n\t\tvalidators = []\n\t\tif isinstance(validators, list):\n\t\t\tvalidators.insert(0, Validation(validators[0], 'validator'))\n\t\t\tfor i in range(1, len(validators)):\n\t\t\t\tvalidators.append(Validation(validators[i], '*valid"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn self.msg_str(status=True, prefix=prefix)\n\n\n\tdef status_int(self, index=''):\t\"\"\" \n\tValidate given index of status, if index!= None, return the status at the start, otherwise return None.\t\"\"\"\n\ttry:\n\t\treturn self.msg_int(status=True, index=index)\n\texcept IndexError:\n\t\treturn None\n\n    def status_string(self, status=None, index=''):\t\"\"\"\n\tReturn string of validator status, if status is not None, return the status and"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\n\treturn prefix if prefix else self.__str__"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn self.__str__() % (prefix + self.__class__.__name__)\n\nclass Status(object):\n\t\"\"\" Represents a status message, as generated by the validation system\"\"\"\n\n\tclass StatusError(Status):\n\t\t\"\"\"Error status\"\"\"\n\t\tdef __init__(self):\n\t\t\treturn_code = self.__class__.STATUS_ERROR\n\n\t\tdef __str__(self):\n\t\t\treturn'status \"%s\" failed' % self.__class__.__name__\n\n\tclass StatusWarning(Status):\n\t\t\"\"\"Warning status\"\"\"\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn ('%s: Valid from %s to %s' % \n\t\t(prefix, \n\t\t self.from_date() if self.valid_at_date() else 's', \n\t\t self.to_date() if self.valid_at_date() else '?'))\n\n\n# This is a placeholder for the logger object, so our validation objects will\n# be automatically instantiated instead. We do not need to instantiate the\n# classes here as they are contained by valid_class, and will be instantiated\n# when the program exits.\ndef _log_class"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn \"%s(%s)\" % (status_str, prefix)\n\n\n\n\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\ttry:\n\t\tif self and self.status:\n\t\t\treturn'%s'% (prefix + self.status)\n\t\telse:\n\t\t\treturn ''\n\texcept AttributeError:\n\t\treturn ''\n\n\ndef get_validation_time_str(self):\n\t\"\"\"Get the timestamp when validation was last performed.\n\tUsed by the server status log, and by the debug log.\n\t\"\"\"\n\treturn time_format(datetime.now(), '.%Lf')\n\ndef set_validation_time_str(self, value):\n\t\"\"\"Sets the"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tcalls = self.calls\n\tresults = []\n\n\tfor c in calls:\n\t\t(status, reason, log) = c\n\t\tif log:\n\t\t\tresults.append((prefix + status_str(status),\n\t\t\t\t\treason,\n\t\t\t\t\tformat_log(log))\n\t\t\t)\n\treturn ', '.join(results)\n\ndef status_str(i):\n\tret = (str(i)+\"\")\n\tif i == 0: ret = \"(valid)\"\n\tret = status_str(i)\n\treturn ret\n\ndef format_error(e):"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t# print(prefix)\t\n\tfor val in self.validations:\n\t\tstatus_str = prefix + \"\\t\"\n\t\tstatus_str += val.status_str()\n\t\tstatus_str += \"\\n\"\n\treturn status_str\n\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn \"%s<br/>\\n<dt>%s</dt><dd>%s</dd>\" % (prefix,\n\t                           self.status, self.desc)\n\n\n\t\n\t\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn '\\n'+'Validated by '+prefix+self.class_.__name__+', file = '+self.filename+'\\n'+'-----------------------------------------\\n\\n'+'\\t'+_str(self.timestamp)+'\\t'+self.descr+'\\t'+'\\t'+'\\t'.join(_str(val) for val in self.parameter)\n\t\n\tdef print_status(self):\n\t\tprint('Validated by '+self.class_.__name__+', file = '"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tstatus_str = re.sub(r'[^\\d\\W\\s]', '', str(self.data['status'][prefix])) +'\\n'\n\tstatus_str = re.sub(r'\\d{3}', '', str(self.data['status'][prefix])).strip()\n\tstatus_str = re.sub(r'$', '', status_str)\n\treturn status_str\n\n\t\ndef run(run_dir):\n\tfiles = glob.glob(\"{}/**.xarr\".format(run"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\t\ndef is_inbox(self, identifier):\n\treturn identifier == INBOX"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif not isinstance(identifier, Name) or not identifier.isidentifier():\n\t\treturn IdentifierNotValid(identifier)\n\treturn True\n\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif identifier in self.tokens:\n\t\treturn True\n\treturn False\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\t# This implementation makes all subclasses have this function return True.  If we implement all the implementations here, we would allow subclasses to override this method\t\"\"\"\n\n\tdef get_identifier_name(self, identifier):\n\t\t\t\n\t\"\"\"Return the (hopefully) unique name\"\"\"\t\t\n\treturn identifier\n\t\n# ----------------------------------------------------------------\n# Local variable definitions\n# ----------------------------------------------------------------\n\t_IDENTIFIER=None\n# ----------------------------------------------------------------\n\n\t\t\n# ----------------------------------------------------------------\n# Public static methods\n# ----------------------------------------------------------------\n\t#\n\t# @method is_valid(identifier):\n\t#   This method returns"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\ndef remove_from_stack(identifier):\n\tfor child in identifier.parents:\n\t\tif isinstance(child.value, Variable) and child.value.name == variable.name:\n\t\t\traise SyntaxError(\"Circular dependencies\")\n\treturn identifier\n\ndef get_ancestor_nodes_from_stack(identifier):\n\tfor parent in re(r'[^#]*') + ['->']:\n\t\tif getattr(parent.value, 'name', None) == identifier.name:\n\t\t\treturn [parent]\n\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True \n\t\n\t\ndef is_valid_ip(ip):\n\t\"\"\"\n\tReturn True if identifier is valid, False otherwise.\n\t\"\"\"\n\treturn True\n\t\n\t\ndef is_valid_mac(mac):\n\t\"\"\"\n        Return true is identifier is a valid mac\n        \"\"\"\n\treturn True\n\t\n\t\ndef is_valid_mac_in_ip(mac_in_ip):\n\t\"\"\"\n\t\tThis function will return true if mac in IP address is valid\n\t\"\"\"\n\treturn is_valid_mac(mac_in_"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\n#############################################################################\ndef get_resource_id(self):  # pylint: disable=unused-argument\t\"\"\"\n\t\"\"\"\n\tReturn the resource ID which uniquely identifies this node.  Only one of these\n\ttwo methods can be used in a single node.  The resource ID is the same as a\n\tshort name for the resource, e.g. `\"node_0\"`, `\"node_00\"`, `\"node_0_3\"`.\n\tThe resource ID is a string of two first-letter uppercase letters that uniquely"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n#----------------------------------------------\n\ndef is_mip_reserved(identifier): # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if the given identifier is mapped by MPI and is used for MPI computations etc.\t\"\"\"\n\treturn identifier == \"mip\" \n\ndef is_mq_reserved(identifier): # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if the given identifier is reserved by MQ\n\t\"\"\"\n\treturn False\n\ndef is_qmp_reserved(ident"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\n\tsuper().validate(inventory)\n\t\n\tif not (super().validate_type(inventory) == self.valid_type(inventory)):\n\t\traise ValueError('Specification version %(versionnum)s should be supported.' % {\n\t\t\t'versionnum': inventory.spec_version,\n\t\t})\n\t\n\tif type(self.valid_type(inventory)) == str:\n\t\textract_spec_version = True\n\telse: # it's string then extract the version number and verify\n\t\textract_spec_version = ('.'.join"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif not extract_spec_version:\n\t\tself.validate_latest_version()\n\tself.validate_version()\t\n\n# --------------------------------------------------------------------------------\ndef validate_latest_version():\n\t\"\"\"\n\tValidates the given version on our inventory.\n\n\tThere is no implementation of validate this method yet.  (The version will\n\tbe decided from the base specification)\n\t\"\"\"\n\tprint(get_version())\n\t\n# --------------------------------------------------------------------------------\ndef validate_version():\t\n\t\"\"\"\n\tValidates the latest version given on the inventory.\n\t\"\"\"\n\t(v"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tself.has_extract = True\n\n\tself.compare_type_versions(inventory._compare_type_versions())\n\tif self.compare_type_versions():\n\t\tself.spec_version, extracted = inventory._compare_mappings(extract_spec_version)\t\t\n\t\tif extracted:\n\t\t\tself.extract_to_root(inventory, extracted)\n\ndef test_inventory():\n\t\"\"\"Make sure that the inventory type is valid and test it more thoroughly\"\"\"\n\tinp = get_my_inventory('')\n\tprint inp"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\n\tvalid_versions = [\n\t\t'1.10',\n\t\t'1.13',\n\t\t'1.14'\n\t\t]\n\t\t\t\n\tif (inventory.get('type') not in valid_versions \n\t\tand inventory['type'] not in [\n\t\t\t\tNone,\n\t\t\t\t'',\n\t\t\t\t'CISV1.x'\n\t\t]):\n\t\tmsg = \"No valid type. \"\n\t\tmsg += 'Available types are: %s' % valid_versions\n\t\treturn False, msg\n\n\tversion = 0\n\tif 'type' in inventory['type"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tsuper(Version, self)._validate(inventory, extract_spec_version)\n\t# TODO: validate each of the child containers of the version object.\n\n\nclass VersionCollection(_VersionMixin):\n\tdef __init__(self, name=None):\n\t\tif name is None:\n\t\t\tself.name = [{'version': '1.0.0'}, {'version': '2.0.0'}]\n\t\telse:\n\t\t\tself.name = name.split('-')\n\n\t@property\n\tdef spec_version(self):\n\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\n\tdef validate(self, inventory):\n\t\t\"\"\"\n\t    Validate the inventory using the given set of test methods to determine the\n\t    specification version and to check for the presence of the required field if it is missing.\n\t    \"\"\"\n\t\t\n\t\t\"\"\"\n\t    Validate the inventory against the required parameters.\n\t    \"\"\"\n\t    result = True\n\t    for test in self.testlist:\n\t    \tis_invalid = test.does_not_match(inventory, extract_spec_version)\n\n\t    \tif is_invalid:\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tnew_inventory = dict(inventory.inventory)\n\tself.spec_version = get_inventoryversion(new_inventory)\n\tnew_inventory.update(self.inventory)\n\treturn new_inventory\n\n\t##\n\t# Get the version string from a given inventory.\n\t# @param Inventory Object\n\t# @param version String\n\t# @return version String\n\t# @raise inventory.InventoryException\n\t# @todo test the specific version\n\tdef get_inventoryversion(self, new_inventory):\n\t    version = [x for x in new_inventory"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\n\tif(inventory == '' or inventory == 'none'):\n\t\traise ValueError(\n\t\t'')\n\n\tself.inventory = inventory\n\ttry:\n\t\tself.spec_version = re.findall(\"(\\d.\\d[.])+\", inventory)[0]\n\texcept IndexError:\n\t\traise ValueError(\n\t\t'')\n\n\tif self.spec_version.endswith('.0'):\n\t\tself.spec_version = self.spec_version[:-2]\n\t\tself.spec_version = '.' + self.spec_version"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tlog.debug(\"validate()\")\n\n\t# Extract the inventory's specific type\n\tinventory[\"type\"] = self.type\n\tpayload.append(inventory)\n\n\t# Validate the inventory\n\terrors = self.validate_inventory(payload)\n\n\t# If an error occured validate the inventory but don't raise any exceptions\n\tif len(errors) > 0:\n\t\treturn errors\n\telse:\n\t\treturn validations.ValidationResult.VALID, None\n\n\t# Determine the inventory type\n\ttype = \"inventory\"\n\tinventoryType = inventory[\""}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tinventory = tuple(sorted(inventory))\n\tself.spec_version = extract_spec_version\n\tsuper(Version, self).validate(inventory, True)\n\ndef check(*args):\n\t\"\"\"Check our version.\n\n\tThe arguments are of form:\n\t\t-spec_version=1_234_567_abc\n\t\t-instruments=foo_bar\n\t\"\"\"\n\tif len(args) < 2:\n\t\traise NoValidValue(default=\"\")\n\t\n\tassert 1 <= len(args) <= 3, \"Arguments should be of form"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tdigests_used = set(digests_used)\n\n\t\t# Check if digests are present.\n\t\tfor digest in manifest_file.digests:\n\t\t\t# If the manifest digest refers to an object that is not part of the manifest\n\t\t\t# set it to False\n\t\t\tif not manifest_file.digest[digest.name]:\n\t\t\t\tdigests_used.remove(digest.name)\n\n\t\t# The digests present in the manifest file are used.\n\t\ttodo_used = digests_used | set("}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor f in manifest_files:\n\t\tfor key in digests_used:\n\t\t\tstatus = self.executeInventoryCheck('digest' + key, f, f)\n\t\t\tif not status['result']:\n\t\t\t\t_log.error('No digests are found where required for the host %-45s' % f)\n\n\t\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tused_for_update = False\n\tfor manifest_files in manifest_files:\n\t\tif manifest_files in digests_used:\n\t\t\tused_for_update = manifest_files\n\t\t\tbreak\n\n\treturn used_for_update\n\n##\n# Test harness for testing the digests_present_and_used() method.\n#\n# This script uses the checkdigestdir.py script to check that the digests\n# are properly collected in a temporary location.\n\ndef main():\t\t\"\"\"Main function.\"\"\"\n\t\n\tcheck_digests_present_and_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tdigests_present = {}\n\tused_digests = set(digests_used)\n\n\tfor manifest_file in manifest_files:\n\t    digests_used_file = set(os.path.split(manifest_file)\n                                   [len(os.path.split(manifest_file)[1]) - 1]\n                                   [1])\n\t    digests = manifest_file.split(os.path.sep)[1]\n\t    for digest in digests:\n\t\tdigest_parts = digest.split(os.path"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\td_files = []\n\td_used = set()\n\tfor f in manifest_files:\n\t\tif not os.path.isfile(f):\n\t\t\tcontinue\n\t\tfpath = os.path.realpath(f)\n\t\tif not os.path.isdir(fpath):\n\t\t\tcontinue\n\t\tif not os.path.isdir(fpath+'_refs'):\n\t\t\td_used.add(fpath)\n\t\t\tcontinue\n\n\t\tfor digest_url in self._get_digests_used(fpath):\t\t\t\t\n\t\t\tif os."}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\n\t\tif os.path.isfile(manifest_file):\t\t\n\t\t\tmanifest, md5s = self.read(manifest_file).split('\\n')\n\t\t\tnewManifest = True\n\t\t\t\n\t\t\tfor digest_string in digests_used:\n\t\t\t\tfound_digest = False\n\t\t\t\t\n\t\t\t\tfor digest in md5s:\n\t\t\t\t\tif isinstance(digest, str):\n\t\t\t\t\t\tif digest_string == digest.replace(digest_string, \"\"):\n\t\t\t\t\t\t\tfound_digest = True\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\n\tfor manifest_file in manifest_files:\n\t\tif not os.path.isfile(manifest_file):\n\t\t\tcontinue\n\t\t\t\n\t\tself._debug(\"Checking if file: %s is a digest: %s\" % (manifest_file, digests_used))\n\n\t\twith Timestamp(unix_time=True):\n\t\t\tis_digest = True\n\t\t\tfetch = self._digest_request(manifest_file, digests_used)\n\n\t\tdigests_used[manifest_file] = fetch\n\n\t\tif not fetch:\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest in manifest_files:\n\t\tself.check_digests_present(manifest)\n\t\tself.check_digest_present_used(manifest, digests_used)\n\t\tself.check_digest_is_available(manifest, digests_used)\n\t\tself.check_manifest_is_valid(manifest)\n\n\tdef update_manifest(self, manifest_files):\n\t\t\"\"\"Update the manifest used.\t\"\"\"\n\t\tself.update_digests(manifest_files)\n\t\tself.update_manifest_files("}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor fname in manifest_files:\n\t\tmanifest_fd = open(fname, 'rb')\n\t\n\t\tfor line in manifest_fd:\n\t\t\tparts = line.split()\n\t\t\t\n\t\t\tdigest_name = parts[8].lower()\n\t\t\t\n\t\t\tif fname in digests_used:\n\t\t\t\tdigest_used_value = digests_used[fname]\n\t\t\t\tdigest_in_manifest_line = parts[10].encode('ascii')\n\t\t\telse:\n\t\t\t\tdigest_in_manifest_line = parts["}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\t_, _, hash_value = os.path.split(manifest_file)\n\t\tfor digest in digests_used[hash_value]:\n\t\t\t_, used = os.path.split(digest)\n\t\t\tif used not in ['0']:\n\t\t\t\traise Exception(\"Digest missing '_'\")\n\n\t# Add to a list, so it can be checked after.\n\tself._digests_used = digests_used\n\nif __name__ == '__main__':\n\t\"\"\"\n\tThis function"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif not self.valid: raise ValueError(\"'as_prior_version' must be a function\")\n\n\tself.assert_internal_consistency()\n\n\t\t\n\t# Make a test version of 'before'.\n\t#\n\t# If two versions are identical (and are of the same object), use\n\t# the latest version.\n\t#\n\t# If the prior object is a VersionedInventory (e.g. a VersionedInventory\n\t# built from the parent) with an updated version that is less than/equal\n\t# to the current version, use"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif not (isinstance(prior, InventoryValidator) and (isinstance(self, Inventory) or isinstance(self.inventory, Inventory))):\n\t\traise TypeError(\"Validator type is expected to be an instance of Inventory and the prior Inventory or InventoryValidator object provided in the argument.\")\n\tver = prior.ver\n\tif ver == 3:\n\t\treturn (not valid_prior_version_3(prior, self, self.inventory))\n\treturn (not valid_prior_version_3(prior, self, prior.inventory))\n\n##\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tself.assertIsNot(prior, None)\n\t# check that prior version is the same as self\n\treturn prior.is_same_version(self)\n\ndef validate_for_test(self, name):\t\"\"\"\n\tCheck that the inventory object does not have internal consistency problems\n\tin some form, meaning that this inventory is in a particular state in the form\n\tof a set of inventories which are valid for the test instance, but are in a particular state\n\tof which there is some issue elsewhere.\t\n\nThis is useful when doing sanity tests, since this is"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tpriorIval = self.get_base()\n\tpriorIval.check_version(prior)\n\t\n\tif not prior.validate_as_prior_version(priorIval):\n\t\tself.set_exception_and_fail(errors.VersionMismatchError, \n\t\t\t'%s must be %s' % (self, prior))\n\t\n\nclass Inventory(BaseInventory):\n\t\"\"\"\n\tRepresents a single local inventory object.\n\t\"\"\"\n\t_name = 'inventory'\n\n\tdef __init__("}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tsuper(VersionOf, self).__init__(parent=prior)\n\tself.__doc__ += validate_as_prior_version.__doc__\n\tself.__doc__ += \" Returns the correct version number.\"\n\treturn\n\n    def _create(self, n, parent):\n\t\"\"\"Creates the object using the name n.  If its already exists,\n\tit returns self, otherwise it first checks to see if the\n\trequested version has already \"been seen\" (see below)\n\tNote: If it does not exist, the parent is returned\"\"\""}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif prior not in self and prior not in [V.as_validator(a) for a in self]:\n\t\tif prior and self.version not in prior.versions:\n\t\t\traise Exception(\"Invalid prior version of inventory \"+repr(prior))\n\t\telif not prior or (prior in [V.as_validator(a) for a in prior.versions]):\n\t\t\tif prior and self.version not in prior.versions:\n\t\t\t\traise Exception(\"Invalid prior version of inventory \"+repr(prior))\n\ndef as_prior_version(self):\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\n\tif not issubclass(type(prior), InventoryValidator):\n\t\traise TypeError, \"prior must be an InventoryValidator object.\"\n\t\n\tif not prior.is_valid:\n\t\traise ValueError, \"prior is not valid.\"\n\t\n\tif not self.is_valid:\n\t\traise ValueError, \"self inventory is not valid.\"\n\t\n\tself._validate_as(prior)\n\t\t\n\treturn prior\n\t\t\n\nclass InventoryVault:\n\t\"\"\"\n\tThe inventory vault object stores inventory of a given set"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t# Check invariants:\n\t#\n\t# For a prior that has no self inventory object, ensure its invariant\n\t# still holds.\n\tself_inv = self.inventory\n\tprior_inv = prior.inventory\n\tif self_inv is None: self_inv = prior_inv\n\telse: self_inv.is_internal = self_inv.is_internal or prior_inv.is_internal\n\tassert not self_inv.is_internal, \\\n\t    'invariant violation: self inventory is invariant'\n\tif not valid_as"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif (isinstance(prior, base.IInvPrev) and\n\t\t\t(self in prior or prior in self.bpy.data.materials)):\n\t\trv = base.IInvPrev(self, prior)\n\t\tif base.IInvValidator.Validator.validate(rv=rv)!= base.IInvValidator.Validator.Success:\n\t\t\traise base.IException(\"A valid prior must be used as a prior in a new inventory\")\n\n\tsuper(ObjectTypeEnum, self).validate()\n\ndef validate(self,"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif not (prior and Prior.validator(prior)):\n\t\traise SyntaxError(\"Fatal error: Invalid prior in inventory database\"+\\\n\t\t\" check for self consistency (%s, %s)\".format(self.inventory_key, prior))\n\n\n\n\n\n\n\n"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tstate_map, _tempory_map = create_state_map(inventory, version)\n\tcontent_files_in_state = set()\n\tfor logical_path, contents in state_map.items():\n\t\tcontent_files_in_state.add(contents)\n\t\tlogical_path_in_state = set((logical_path, 0))\n\t\t_tempory_map[logical_path] = contents\n\t\tif logical_path in _tempory_map:\n\t\t\t_tempory_map[logical_path].discard(contents"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_map = {}\n\tfor path_element in inventory:\n\t\tif hasattr(inventory[path_element], 'path_element_map'):\n\t\t\tif inventory[path_element].path_element_map['version'] == version:\n\t\t\t\tsub_tidy, sub_maps, sub_logical_path_map = sub_get_logical_path_map(inventory[path_element], version)\n\t\t\t\tsub_tidy_files = sub_maps.get(sub_maps['isEmpty'], set())\n\t\t\t\tfor logical_path in"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_paths = {}\n\tall_versions = {}\n\tfor artifact in inventory:\n\t\tlogical_paths[artifact] = set()\n\t\tall_versions[artifact] = set()\n\t\tfor dep in inventory.get_all_dependencies(artifact):\n\t\t\tall_versions[artifact].update(dep.get_in_version_map(version))\n\n\t# this is done to prevent double counting\n\tlogical_paths[None] = all_versions[None]\n\tfor key in logical_paths.keys():\n\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_in_state = {}\n\tcontent_files = set()\n\tfor (key,item,value) in list(inventory.items()):\n\t\tassert len(item) == 1, \"path should contain single entry, even if key is multipe entry [%s]\" % str(item)\n\t\tif len(item) == 1:\n\t\t\titem = item[0][0]\n\t\tpath = \"%s/%s\" % (item[0],value)\n\t\tset_add = content_files.add\n\t\tfor child in item:\n\t\t\tif"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tdef _logical_path_set_from_item(self):\n\t\tresult = {}\n\t\tfor name, item in self.logical_ref_tree.items():\n\t\t\tresult[name] = set(item)\n\t\treturn result\n\n\tlogical_path_in_state = property(fget=_logical_path_set_from_item)\n\tlogical_path = property(itemgetter('logical_path'))\n\t\"\"\"Map of logical path to path_info\"\"\"\n\tdef __repr__(self):\n\t\treturn ('%s"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tmap = collections.defaultdict(set)\n\tpaths = {}\n\tlogical_path_in_state = collections.defaultdict(set)\n\ttry:\n\t\tcursor = inventory.cursor()\n\t\tfor row in cursor.execute(_get_path):\n\t\t\tpaths.update({row.logical_path_in_state.get(row.path)})\n\texcept NotFoundError:\n\t\tpass\t# There may be errors in a previous version that still have\n\t\t# \treferences to files that have gone away.\t\n\t\tpaths ="}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tassert type(version) is str\n\tif isinstance(inventory, list):\n\t\tinventory = [inventory]\n\tlogical_path_to_content_files = {}\n\tcontent_files = list(inventory)\n\t# Add in any version_file_refs in the current folder\n\tfor version_file_ref in get_content_file_refs(version):\n\t\tversion_file_ref_location = version_file_ref.get_location(version)\n\t\tif (version_file_ref_location is not None):\n\t\t\troot_location = version"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogger.debug('get_logical_path_map')\n\tversions = list(inventory.version_ids)  # filter out previous items\n\tcached_paths = None\n\n\tif 'previous_versions' in inventory.params:\n\t\tversions.extend(filter(None,\n\t\t\t[\n\t\t\t\t(x.version_id, x.file) for x in inventory.version_ids\n\t\t\t\tif x.version_id not in versions\n\t\t\t]))\n\n\tcached_paths = cached_paths or get_logical_path_map(inventory"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tout = {}\n\tif not get_logical_path_map_by_version(inventory, version):\treturn {}\n\tfor lp in get_logical_path_map_by_version(inventory, version):\n\t\tfiles = out.setdefault(lp, set())\n\t\tfiles |= set(inventory.get_file_for_logical_path_in_state(\n\t\t\tlp, version_to_file_dict(inventory, version)))\n\treturn out\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tif version is None: version = ''\n\treturn dict([(item['source_path'], set(item['dst'])) for item in\\\n\t\t\t\t\tre.finditer('^(.*/)?(contents_dir)?([^/]+/)?([^/]+)', inventory)\\\n\t\t\t\t\tif 'contents_dir' in item])\n\n\ndef get_path_map(inventory, content, version, path_type, get_data):\t\"\"\"\n\tGet map of file locations in state to content_files for version in inventory.\n\nReturns a dictionary: file_"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfixity.split() # remove white space at beginning and ending spaces\n\tmanifest_file = fixity.get_filename()\n\t\n\tif manifest_file == '':\n\t\tprint \"No manifest file provided\"\n\telse:\n\t\t\n\t\tif fixity.endswith('txt') or fixity.endswith('json'):\n\t\t\tprint \"This is a txt or json file\"\n\t\telif fixity.endswith('json'):\n\t\t\tfixity.split()\n\t\t\tmanifest_file = fixity.get_filename()\n\t\t\tif"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\tvalid_manifest_files = sorted(manifest_files.split('|'))\n\n\tfor check_file in self.fixity_checkers:\n\t\tmanifest_file = manifest_files[check_file]\n\t\twith open(manifest_file) as manifest_f:\t\t\n\t\t\tfor line in manifest_f:\n\t\t\t\t\n\t\t\t\tline = line.strip()\n\t\n\t\t\t\tif len(line.split()) > 0: \n\t\t\t\t\tfiles_to_check = line.split(',',1 )[0].split('=' )\n\t\t\n\t\t\t\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tinvalids = set()\n\tfor path in os.listdir(fixity):\n\t\tinvalid = False\n\t\tfor manifest in manifest_files:\n\t\t\tpath_with_manifest = os.path.join(path, manifest)\n\t\t\ttry:\n\t\t\t\tos.stat(path_with_manifest)\n\t\t\t\tinvalid = True\n\t\t\texcept OSError:\n\t\t\t\t# If nothing in file, skip this check.\n\t\t\t\tcontinue\n\n\t\tif invalid:\n\t\t\tinvalids = invalids.union(zip((path,) + manifest_files))\n\treturn inval"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\tprint(self._('The fixity block must not contain unreferenced files.\\n'\n\t\t\t\t 'See the following information for more information:'))\n\t\tprint(self._('1. The manifest (manifest-fix.yaml) content.\\n'\n\t\t\t\t '2. The files that will result from this fixity block.\\n'\n\t\t\t\t '3. The files that have already been checked.\\n'))\n\t\tprint(self._('\\n'\n\t\t\t\t 'Note that the manifest contains information about which\\n'\n\t\t\t\t 'files contain which packages."}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tdef validate_manifest(manifest, manifestFile):\n\t\t\"\"\"\n\t\tValidate that all the files are referenced in the manifest.\n\t\t\"\"\"\n\t\tfiles = manifest[manifestFile].strip().split('|')\n\t\tpaths = [f.strip() for f in files]\n\t\tpairs = list(re.findall('[^\\s|\\|]+', os.listfiles(os.path.join(manifestFile, \n\t\t\t'.fixity'))))\n\t\tfor f in files:\n\t\t\tif os.path.isdir(os.path."}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfixity_block = manifest_files['fixity']\n\t# validate the contents of the fixity block\n\tfor line in fixity_block.file.getlines():\n\t\tline = line.strip()\n\t\tif not line:\n\t\t\t# end of manifest file\n\t\t\tbreak\n\t\t\n\t\tif line[0] in [ '#', '!' ]:\n\t\t\t# end of fixity line\n\t\t\tvalue = line[1:].split()\n\t\t\tif len(value) > 1 and value[0]:\n\t\t\t\t# manifest file containing 'value' line\n\t\t\t\tbreak\n\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tif fixity['kind'] == 'variable' or \\\n\t\t\tfixity['kind'] == 'object':\n        \tif not fixity['type'] in manifest_files:\n                \treturn (\"Missing manifest file, missing 'type' \" + \\\n                        \"from'manifest': [\" + fixity['kind'] + \"]\" + \\\n                        \".\")\n\n        return\n\n    def get_data(self, fixity, manifest_files):\n        \"\"\"\n        Get data for the fixity block based on a specified manifest list.\n        \"\"\"\n\tdata = {}\n\tfiles ="}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfiles = ''\n\tmatch = '^.*#%s'%fixity\n\tpath = ''\n\tfor f in manifest_files:\n\t\tprint \"Processing %s...\"%f\n\t\tcontent = None\t# this is our content\n\t\ttmp = re.match(match, f, re.M)\n\t\tif tmp:\n\t\t\ttmpfile = tmp.group(1)\n\t\t\tif tmpfile == '1':\n\t\t\t\tcontent = '1'\n\n\t\tif content:\n\t\t\tfiles += \"%s\\n\"%tmpfile\n\t\telse:\n\t\t\tprint \"File is"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tmissing_files_are_not_valid = self.check_fixity_file_present(fixity, manifest_files)\n\t\n\treturn missing_files_are_not_valid\n\t\ndef check_fixity_block_present(fixity, manifest_file):\n\tmissing_files_are_not_valid = False\n\t\n\t#check if file(s) in manifest are referenced\n\treferencing_files = {}\n\t#check if manifest file exists\n\tmanifests_file_regex = r'.*(?:\\+\\+)"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tmanifest = ''\n\ttry:\n\t\tfor f in manifest_files:\n\t\t\tmanifest = f\n\n\texcept NameError:\n\t\tprint '\\nERROR: No manifest file given. Cannot fixity: %s' % fixity\n\t\tsys.exit(1)\n\n\tman = {'/': '/',\n\t\t\t '~': '~'}\n\tallman = [m for m in manifest.split('/') if m]\n\n\tif not allman:\n\t\tprint 'ERROR: No manifest filenames given. Cannot fixity: %s' % fixity\n"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tpath = os.path.expanduser(path)\n\tfiles = os.listdir(path)\n\tfiles.sort()\n\treturn files\n\n\ndef files(path):\n\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tpath = os.path.expanduser(path)\n\tfiles = os.listdir(path)\n\tfiles.sort()\n\treturn files\n\n\ndef walk_dirs(path, file_filters):\n\t\"\"\"\n\tWalk `path` and `files_list(path)` in `[root]`"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tif path == '\\'':\n\t\treturn (['.','.'])\n\treturn path.split('/')\n\n\ndef make_file(relpath, name, contents):\n\t\"\"\"\n\n\t:type relpath: str\n\t:type name: str\n\t:type contents: str\n\t:rtype: str\n\t\"\"\"\n\tfile = '%s.txt' % (relpath[1:-8].replace('/','-').replace('.','-'))\n\twith open(file, 'w') as fin:\n\t\tfin.write(contents)\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\timport glob\n\tout = []\n\tf = glob.glob(f\"{path}/*\")\n\tfor i in f:\n\t\tout.append(os.path.join(f,i))\n\treturn out\n\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfobj = open(path)\n\tfiles = []\n\tfor line in fobj:\n\t\tline = line.strip()\n\t\tif line.startswith('#'):\n\t\t\tcontinue\n\t\tfiles.append(line)\n\treturn files\n\n##\n# Return the full file path of a file or directory.\n#\ndef file_name(path):\t\n\tfile = os.path.basename(path)\n\tfile = os.path.join(os.path.dirname(path), file)\n\treturn file\n\n##\n# The command_"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn open(path)\t\t\t# todo\n\n@bddi.when(exists, file_list)\t#\t1. Check if file exists or not\ndef read_with_content(path, content):\n\t\"\"\"\tRead content content from `path` \"\"\"\t\n\treturn open(path, 'r').read(content) # todo  remove\n        \n\"\"\"\nCreate a set of files created under `folder`, based on the following arguments.\nThis will check the folder hierarchy and if it find a file and checks against a regex.\nFiles matching `regex` are"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tlist = []\n\tfor fpath, f, dic in walk(path):\n\t\tlist.append(f)\n\treturn list\n\ndef file_name(path):\n\treturn path.split('/')[-1]\n\ndef file_path(path):\n\treturn path[6:-4]\n        \ndef test_path(self, path):\n\tp = path.split('/')\n        return p[-1]"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfiles = file_system.list_files(path)\n\treturn [f for f in files if not f.is_hidden()][:10]\n\ndef create_directory(path):\n\t\"\"\"\tCreate a directory at `path`\"\"\"\n\tfile_system = file_system_create(path)\n\tfile_system.mkdirs()\n\ndef list_all_repo_files(path):\n\t\"\"\"\tRetrieves the list of all the files in `path`\"\"\"\n\tfiles = file_system.list_files(path)\n\treturn ["}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t#print \"files_list\"\t\n\treturn [f for f in itertools.chain(os.listdir(path), []) if not f.startswith(\".\") and os.path.exists(os.path.join(path, f)) ]\n\n\ndef get_user_home():\n\treturn os.getenv(\"HOME\")\n\n\n\n\ndef log_level(log):\n\treturn None\n\tif log == \"DEBUG\":\n\t\treturn logging.DEBUG\n\telif log == \"INFO\":\n\t\treturn logging.INFO\n\telif log == \""}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tf = os.path.join(path, \"*\") # add the base directory (e.g. /usr/data)\n\t\n\treturn glob.glob(f + \"*\") # list all subfolders\n\n\n\ndef get_fixture_paths_to_install_from(cwd):\n\t\"\"\"Return absolute paths to the fixture files\n\tand directory on the source tree\n\t\"\"\"\t\n\treturn  cwd + os.sep + 'fixture'\n\ndef get_all_deps_files():\t\n\t\"\"\"Return the full list"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tf = open(path)\n\tret = f.read()\n\tf.close()\n\treturn ret\n\ndef list_dirs_and_files(path):\n\t\"\"\"List the directories in `path` and return a list\tof dictionaries\tkeys: file names and values:\t[file data].\n\nIf not a directory, the function will return a list containing just the last directory and a list with all the files.\"\"\"\"\n\td0 = []\n\td1 = []\n\td2 = []\n\td = []\n\tfor f in os.listdir"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tfiles_list = []\n\tfor f in files:\n\t\tfilepath = os.path.join(xmls, f.split('/')[-1])\t# remove xml directory\n\t\td = {'name': f,'file':filepath}\n\t\tfiles_list.append(d)\t\t# add the file information to the list\n\treturn {path : filelist for path, name, filelist in zip(files_list,xmls,files)}\t# return package information"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tpackage = zipfile.ZipFile(file(source)).read(xmls)\n\tpackage.close()\n\tpackages = [package.filename().split(\"/\")[:-1], package.filename().split(\"/\")[-1]]\n\tresult = {}\n\tfor p in packages:\n\t\tif p not in result:\n\t\t\tresult[p] = {\t\"name\": \"\"\t}\n\t\tfor f in files:\n\t\t\tresult[p][\"name\"] += f\n\n\treturn result\n\n# TODO: fix this\n# def _group_files_by_xml"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tdict = {}\n\tfor f in files:\n\t\tname = f\n\t\tmatch = []\n\t\tmatch_files = re.search(\n\t\t\t# Regular expression for all the files\n\t\t\t# name with path ending on '.' (no dot) or '*'\n\t\t\t'(?P<name>.{1,255})([^\\]\\.]+)'\n\t\t\t'((?P<extension>.{1,255})\"|\\*{0,1}(?P<extension_dot>\\W|\\*{0,1}.*))))',\n\t\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tsourceXMLName, sourceXMLFilename = zipstring_split(source, \\\n\t\t\t\t\t\t\t\txml_filename)\n\tsourceXML = get_file_contents(sourceXMLName)\n\tfilesMap = {sourceXMLName: sourceXML}\n\tresultList = []\n\tfor i in xrange(len(files)):\n\t\tfilesMap[xml_filename][i] = files[i]\n\t\t\t\n\tresult = {xml_filename:filesMap[xml_filename]}\n\treturn result\n\ndef _group_files_by_filename(source,"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tfiles_name = {}\n\tfor f in files:\n\t\tfiles_name[f.get('filename').split('/')[-1][4:]] = f\n\treturn files_name\n\ndef _remove_xml_filename(source, xmls, files_name):\n\tfor f, key in files_name.items():\n\t\tif f is not None and is_xml(f):\n\t\t\treturn _remove_xml_filename(f, xmls, files_name)\n\t\telse:\n\t\t\treturn\n\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\tdata = {}\n\tfilename = xbmcvfs.fn(xml_filename)\n\tfilelist = xbmcvfs.listdir(str(settings.PATH_XML_DATA)+\"/\")\n\tfilelist.sort()\n\tfor filename in filelist:\n\t\tif not os.path.isdir(filename):\n\t\t\tpkgname, full = os.path.splitext(os.path.basename(filename))\n\t\t\tfull = os.path.join(settings.PATH_XML_DATA, full)\n\t\t\tif not full in files:"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tkey = []\n\tvalue = dict()\n\tfor file in files:\n\t\tif file[3:].find(\".xml\")!= -1:\t\t\t\n\t\t\tif xmls.find(file) == -1:\n\t\t\t\tkey.append(file)\n\t\t\t\tvalue[file] = []\n\n\treturn value\n\n# Main function\ndef function_main():\n\t# Check parameters\n\t_test_folder = str()\n\t_test_zipfile = str()\n\t_test_dir = str()\n\t_test_files = str()\n\t_test_xmls"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\tprint('Groups files by XML basename')\n\t\n\txml_folder = xmls\t\n\tif (xml_folder is None):\t\n\t\txml_folder = os.path.join(xmls, '')\n\n\tnum_files = len(files)\n\n\t#print ('Total number of files:'+ str(num_files))\n\t\n\tfiles = sorted(files)\n\tif (len(files) < 1):\n\t\tfiles.append('')\n\n\toutfile = ''\n\n\tret = {}\n\toutfile = 'output' +"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\treturn sorted(files, key=_sort_xml, reverse=True)\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\ttry:\n\t\treturn dict([(os.path.split(filename)[1], files[filename]) for filename in xmls])\n\texcept KeyError as err:\n\t\traise KeyError('{0}: no XML files found in {1}'.format(err.message,\n\t\t\tpath.join(source, xml_filename)))\n"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n    for i in file_path.split('.'):\n        if i.startswith(prefix):\n\t\t\treturn True\n    return False\n## END #######################\n\n\ndef list1_by_prefix(prefix, file_names):\n\t\"\"\"\n\nIdentify if a `file_names` belong to a document package by a given prefix\t\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_names : list[str]\n    List of filenames from the packages\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\""}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tfile = open(file_path, 'r')\n\tfile_name = file.name\n\tprefix_length = prefix.split('/')\n\tlength = len(prefix_length)\n\tif length < length:\n\t\treturn False\n\telse:\n\t\treturn ('.'.join(prefix_length[-length:]) == file_name)[1]\n\nclass PackageLoader():\n\tdef __init__(self):\n\t\tpass\n\n\t@staticmethod\n\tdef get_files_by_prefix((pkg_name, prefix):\n\t\t\tfile_path = PackageLoader"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tpackage_path = file_path.rsplit(\"/\", 1)[-1]\n\tif package_path == \"package_\":\n\n\t\tpackage_match = True\n\t\tpkg_exists = True\n\t\tpkg_rels = []\n\n\t\tfor rel in package_path:\n\t\t\tpkg_path_rel = package_path.split(\"/\", 2)[-2]\n\n\t\t\tif rel[0]!= \"data\" or pkg_path_rel not in pkg_exists:\n\t\t\t\tpkg_exists = False\n\n\t\t\tpkg_rels.append(pkg_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tp = os.path.join(prefix, file_path) \n\tif os.path.isfile(p):\n\t\treturn True\n\treturn False\n\t  \ndef get_package_name(prefix, document):\n\t\"\"\"\n\tParse a `document` into\n\tpackage_name\n\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tdocument : str\n\t    filename with the package.\n\treturn : str\n\t\tpackage name\n\t\"\"\"\n\tif '.' in document :\n\t\tpackage_name = document.split('.')["}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\tfor entry in dir(file_module):\n    \t#get the filename for this entry\n    \tfilename = getattr(file_module, entry)\n\n    \tif filename.startswith(prefix):\n\t\t\tmatch_file = filename\n\t\t\t\n    \tif match_file == file_path:\n\t\t\t\t\treturn True\n\treturn False"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\tif len(prefix)<=0:\n\t\treturn False\n\n@match_file_by_prefix\ndef match_file_by_short_name(short_name, file_path):\n\tpass # This function has to be implemented to match a file path with a given short name\t\n\n\n\n\n\ndef create_document(filename, doc_file_name, package_name, doc_type, source_lang, \\\n\tlanguage_version, doc_version, package_name_ver_dir, version_dir, doc_data):\n\n\t# Initialize the"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\tfor pack in self._packages:\t# For every package\n\t\tif pack.prefix == prefix or pack.name == prefix:\n\t\t\tif file_path in os.listdir(self.package_folder + pack.prefix):\n\t\t\t\treturn True\n\treturn False\n\t\n\t\n\n\ndef filter_files(input_files_path, result_files_path):\n\t\"\"\"\n\tRemove a file that is not on any `list_of_files`\n\n\tArgs:\n\t\tinput_files_path : str\n\t\t\tFilename to test\n\t\tresult_files_path :"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tfilename = os.path.basename(file_path)\n\n\tif( filename == 'LecturaMedio' ):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef match_file_name(file_name, prefix=''): \t\"\"\"\n\tIdentify if a file is in your document path or name\n\n\tParameters\n\t----------\n\tfile_name : str\n\t    File name\n\n\tprefix : str, optional. default = ''\n\t\tFilename prefix\n\nRetorna `True` para documento pertencente ao campo `prefix`"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\ttry:\n        lines = codecs.open(file_path).readlines()\n\t\tlines = [line.rstrip('\\n') for line in lines]\n\texcept IOError:\n\t\treturn False\n\telse:\n\t\tfor line in lines:\n\t\t\tif line.startswith(prefix):\n\t\t\t\treturn True\n\n\treturn False\n\ndef is_document_in_package(doc_file):\t\"\"\"\n\tIdentify if a `doc_file` belongs to a document package.\n\n\tReturns\n\t-------\n\tbool\n\t\"\"\"\n\treturn(match_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tglobal PANDO_DOCS\n\t# Obtenem os numeros da string de prefix...\n\tif prefix.find(\".\") == 0:\n\t\tprefix_new = prefix[:-1] + unicode(int(prefix[-1:]))\n\t\tfor d in PANDO_DOCS:\n\t\t\tif prefix_new in d.docs_paths:\n\t\t\t\treturn True\n\treturn False\n\"\"\"\nif __name__ == '__main__':\n\t#pandos_p = match_file_by_prefix(\"pandos\", \"pandos."}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tr = {}\n\tindex = len(files) -1\n\tfor x in files:\n\t\tn = os.path.basename(x)\n\t\te = x.split(prefix)\n\t\tfor i in range(len(e)):\n\t\t\tif e[i] in r and r[e[i]]:\n\t\t\t\tbreak\n\t\tif index == len(e):\n\t\t\tr[e[i]] = True\n\t\t\tp = index + 1\n\t\t\tfor j in range(len(r)):\n\t\t\t\tn = os.path.basename(files[j])"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tlist = []\n\tfor x in files:\n\t\tif not self.file_exists(x): continue\n\t\tr = self.file_scanner.get(x, '*.html', start=60, duration=60, filter='*.html',\n\t\t    filter_type=self.filter_type)\n\t\tif not r: continue\n\t\telse:\n\t\t\tlist.append(r[0])\n\t\t\n\t\n\treturn list\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\tdef get_filepaths():\n\t\treturn [\"files/\" + os.path.basename(prefix+f) for f in files]\n\t\n\tfiles_filepaths = []\n\t\n\tfiles_filepaths.extend(get_filestore(prefix, list_files=True))\n\t\n\tfiles_filepaths.append(get_filestore(re.sub(\"^.*\"+os.path.basename(prefix)+\"$\",\"\", prefix), list_files=False))\n\t\n\tfiles_filepaths.append"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn glob.glob(prefix+\"/*\"+prefix+\"/**\", recursive=True)"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn select_filenames_by_prefix_list(prefix,files)\n\n\n#==================================================================================================\n# MAIN\n#==================================================================================================\n#@param files list\n#@return list\t\n#==================================================================================================\ndef main(files=None):\n\n\tif files is not None:\t# @see: if the parameter `files` is not None\n\t\tfor f in files:\t# @see: of `files` list a selecionar dos arquivos para obter o nome\n\t\t\toutfile = f+\".txt\"\n\t\t\ttry:\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfiles, files = split_path(files)\n\tfiles = remove_files(files)\n\tfiles = files.rsplit(prefix, 1)\n\tfiles = [ f for f in files if f!= '' ]\n\tfiles = [ f for f in files if f!= '' ]\n\tfiles, _ = merge_with_files(files)\n\treturn files\n\n\ndef select_filenames_by_extension(ext, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t# Get the paths which belong to a document package\n\t# Get paths of the files with that prefix in current_folder\n\t# Get paths of the files in the currentfolder that do not belong to\n\t# another file\n\t# Get the files basenames\n\tfiles = list()\n\tpaths = []\n\tfor file_path in files:\n\t\tif re.search('%s%s$' % (prefix, file_path), file_path) is None:\n\t\t\tfiles.append(file_path)\n\t\t\tif path.split('/')[-1]"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tres = np.ones(len(files), dtype=bool)\n\tfor i in range(len(files)):\n\t\tfile = os.path.join(pipeline.prefix, files[i])\n\t\tif os.path.split(file)[1] == prefix:\n\t\t\tres = res & np.ones(len(files), dtype=\"bool\")\n\treturn res\n\n\ndef select_dirs_by_prefix(prefix, directories):\t\"\"\"\n\tGet directories which belongs to a document package.\n\n\tParameters\n----------\nprefix : str\n    Filename"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\tresult = []\n\tfor filename in files:\n\t\tsplit = filename.rsplit('.', 1)\n\t\tif prefix == split[len(split) - 1]:\n\t\t\tresult.append(filename)\n\treturn result\n\ndef find_docu(files):\n\"\"\"\nRetorna ao meu documento quando for cadastrado se encontram os arquivos.\nEssa fun\ufffd\ufffdo ainda dojante, neste, ou seja um documento ou diferente.\n\n\nParameters\n----------\nfiles :"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfilenames_matched = []\n\t# Filenames to check\n\tfilenames_file = glob(prefix + str(files))\n\n\tif len(filenames_file) > 0:\n\t\t\treturn filenames_file\n\telse:\n\t\t\treturn []\n\n\tfilter_n = lambda f: (not f.startswith('.'))\n\tfiles_by_names = list(filter(filter_n, files))\n\n\tfor f in files_by_names:\n\t\tif os.path.basename(f) == prefix:\n\t\t\tfilen"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tdirPath = os.path.join(folder, 'packages')\n\tpNames = os.listdir(dirPath)\n\tdirNames = [x.replace(os.sep,'') for x in pNames]\n\tpFiles = [os.path.join(dirPath,x) for x in dirNames]\n\n\tdictName = {'name':'','version':''}\n\tdictPath = []\n\n\txmlPath = os.path.join(dirPath,'packages.xml')\n\n\txmlDict = xmltodict.parse(xml"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\ttry:\n\t\timport xlrd\n\texcept ImportError:\t\n\t\tfrom xlconf import xlconf\n\t\timport xlwt\t\n\t\tfrom xlutils import writepdf\n\n\t\texport_conf = xlconf()\n\t\tprint 'Export config is ', export_conf, '\\n'\n\t\t#export_conf.add_file_to_folder('test-data-folder-exporter', 'test-data-file-export') \t\t\n\t\texport_conf.add_file_to_folder('test-data-folder-exporter', 'test-"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tpackage = os.path.basename(folder).split(\"-\")[0]\n\tfiles = {}\n\tfor fname in os.listdir(folder):\n\t\tfile_name = fname.split(\".\")[0]\n\t\tif file_name.endswith(\".xml\"):\t\t\t\n\t\t\tfile_info = fname.split(\".\")[0]\n\t\t\tif file_info.startswith(\"v\"):\t\t\t\n\t\t\t\tsubfile = os.path.join(folder, file_name)\n\t\t\t\tprint(\"Getting data from \"+subfile)\n\t\t\t\tif"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tif folder == 'data-pack':\n\t\tfiles = os.listdir(folder)\n\telse:\n\t\tfile(os.path.join(folder, 'data-pack'), 'r', 0)\n\t\tdata = parse_folder(filepath=os.path.join(folder, 'data-pack'))\n\t\t\n\treturn data\n\ndef _create_package(folder, package_name):\n\t\"\"\"Add package in the package folder\"\"\"\n\tif folder == 'data-pack':\n\t\tif package_name == u'packages':\n\t\t\treturn package_name\n"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tret = {}\n\tfor dirname in _fpath_by_dir(folder, [\"root\"]):\n\t\tfor fname in list_files(dirname):\n\t\t\tif os.path.splitext(fname)[1] == \".xml\":\n\t\t\t\tfname, _ = os.path.splitext(fname)\n\t\t\t\tret[fname] = \"\\n\".join(_fname_by_file(folder, fname))\n\n\treturn ret\n\ndef _fpath_by_dir(folder, dir_names):\n\tret = []\n\tfor"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t# parse the package names in folder\n\tfor xml_file in os.listdir(folder):\n\t\tif xml_file.endswith('_xml.xml'):\n\t\t\t# create keyword dict mapping XML basename to full path\n\t\t\txml_file_dict = _get_full_name(xml_file)\t\t\n\t\t\t# create keyword dict\n\t\t\tkeyword_dict = dict((key, path) for path,key in xml_file_dict.items())\n\t\t\t# get packages' data from xml file\n\t\t\tpackages_data = _get_file_content(keyword"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\td = dict()\n\tfor file in sorted(os.listdir(folder)):\n\t\tbase, f = os.path.splitext(file)\n\t\td[base] = {}\n\t\tdatafile = os.path.join(folder, file)\n\t\twith open(datafile) as f:\n\t\t\tfor line in f.readlines():\n\t\t\t\tline = line.replace(']]0', '')\n\t\t\t\tline = line.replace(']]]', '')\n\t\t\t\t# line = line.replace('[/tag]', '')\n\t\t\t\tline = line.replace('"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\treturn {\"files\": {}}\n\n#\n# Functions\n#\n\n\ndef _folder_listing(folder):\n\tresult = []\n\tif os.path.isdir(folder):\n\t\tfiles = os.listdir(folder)\n\t\tfor f in files:\n\t\t\tif os.path.splitext(f)[1] == \".xml\":\n\t\t\t\tresult.append(f)\n\treturn result\n\n@retry(delay=10, tries=5, count=30, exc=OSError)\n@cache_if(\"retry\")\ndef _generate"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tlog.info('get package data from folder', folder)\n\tfiles = os.listdir(folder)\t\n\tfiles = dict(map(lambda p,f: (f,'\\t'+p.replace('_',' ')),files))\t\n\tfor key, (fname, flist) in files.iteritems():\n\t\tlog.info('Get data from [%s]',fname, key)\n\t\tres = []\t\n\t\tfor f in flist:\n\t\t\tfv = os.path.join(folder,f)\n\t\t\tx = f(f"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tmydata = []\n\tfolder = path + folder\n\tfor name, package in enumerate(os.listdir(folder)):\n\t\tif package.endswith('.xml'):\n\t\t\tdata = json.loads(os.path.join(folder, package))\n\t\t\tif data:\n\t\t\t\tmydata.append(\n\t\t\t\t\tdata.pop('items'))\n\t\tfolder = path + name+ folder\n\treturn mydata\n\n\nif __name__ == '__main__':\n\tfolder = 't_projects.html'\n\tlist_packages(folder)\n\n"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\treturn {}\n\ndef _parse_rtd_list(rtd_list_file):\n\t\"\"\"\n\tFun\u00e7\u00e3o para parsar os arquivos \u00e0 listas de rtd\n\n\tPar\u00e2metros:\n\t\t- raiz: raiz do arquivo de listas de rtd (lista \u00e0 lista na\n\t\t  m\u00e9trica de arqu\u00f5es)\n\t\t- pasta: pasta do arquivo\n\t\t- nome: nome do arquivo\n\t\t- fonte: nome do nome do arquivo\n\t\t-"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tfrom xml.etree import cElementTree as et\n\troot = et.parse(file_path)\n\telement = root.find(_id='%s/%s' % (prefix,file_path))\n\tif element is None:\n\t    log_error('missing `%s/%s`' % (prefix,file_path))\n\t    return{'packages' : {}}\n\tpackages = {\"file\" : \"Package from `%s`\" % (element.attrib.get(\"id\",file_path))}\n\tpackages.update(parse"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tpack1 = file_folder.split(\".pac\")[0]\n\tfname =  '/'.join(filename.split(\".pac\")) + \".png\"\n\n\tif pack1 in [\"asset\", \"rendition\"]:\n\t\tf = open(prefix + fname)\n\n\treturn {'id': f.name, \n\t       'filename': filename,\n\t      'macro1': '',\n\t    \t\t'filename1': fname,\n\t       'packages1': \"\",\n\t       'ext': ext}\n\n\n\n\ndef _check_file"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t# se a pasta est\u00e1 na funcao de funcionamentos de dicion\u00e1rio\n\t# e nao n\u00e3o \u00e9 um arquivo diferente de mais dos an\u00e1lises,\n\t# se crie os itens do arquivo\n\tnome_proj = os.path.basename(file_path).strip('\\n')\n\tnome_proj = nome_proj.split(\".\")[0]\n\tif prefix.upper() == \"ASSET\":\n\t\tsubkey = \"assets\"\n\t\tif \":\" in"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t#\n\tif filename[-1] == \"-\":\n\t\tfilename = filename[:-1]\n\t#\n\textension = filename[-4]\n\tassert extension in [\".xml\", \".xml.gz\"], \"extensao desconhecida\"\n\t#\n\tfile_path = os.path.join(file_folder, file_path)\n\t#\n\tif extension == \".xml\":\n\t\t#\n\t\tpackages = get_packages_from_xml(file_path)\n\t\t#\n\telif extension in [\".zip\", \".xz\""}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tpackage_file_path = os.path.join(file_folder, prefix + file_path)\n\tjson_data = read_file(package_file_path)\n\n\tpackages = json_data['packages']\n\tassets = json_data['assets']\n\tassets_name = json_data['assets_name']\n\tassets_path = json_data['assets_path']\n\tfor key in packages:\n\t\tpkg = packages[key]\n\t\tif filename.lower() == key.lower():\n\t\t\tprint(\"Efetuando carregamento do pacote"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\timport os\t \t\n\timport pandas as pd\n\tfrom os import listdir\n\tfrom shutil import copyfile\n\tdirname = os.path.dirname(os.path.splitext(file_path)[0])\n\troot, _files = os.path.split(file_path)\n\t_files = [f for f in _files if f.startswith('.')]\n\t\n\tif (type(prefix)!= str):\n\t\tfor f in _files:\n\t\t\tif '/' in f:\n\t\t\t\tprefix = f\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\tif filename.lower().endswith('.xml'):\n\t\tfull_file = filename\n\telif filename.lower().endswith('.xml.gz'):\n\t\tfull_file = filename\n\t\n\tfolder = os.path.dirname(filename)\n\t\n\tif folder not in packages:\n\t\tpackages[folder] = {file_folder:filename}\n\tpackages[folder].update(package['config']['dependencies'] or {'pkg':package['pkg']})\n\t\n\tfile_folder, filename = os.path.split"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tpkginfo = {}\n\tpkginfo['name'] = prefix\n\tpkginfo['version'] = __version__\n\tpkginfo['author'] = __author__\n\tpkginfo['url'] = '../../README.txt'\n\tpkginfo['description'] = 'TODO'\n\tpkginfo['keywords'] = 'TODO'\n\treturn pkginfo\n\ndef _read_from_text(content, lang='en'):\n\tout_str = __repo_type__"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\timport codecs\n\tcodecs.open(file_path, encoding = 'utf-8').read()\n\tpack = {}\n\tpack['pack'] = ''\n\tpack['version'], pack['extensions_package'], pack['package_name'] = str(prefix), filename, file_folder\n\tpack['name'] = package_name = ''\n\tpack['content_types'] = []\n\tpack['texts'] = []\n\tpack['mime_types'], pack['mimes'] = _eval_mime(pack, [''])\n\t"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tif lang not in self.allowed_languages: # no lang is specified\n\t\tlang = self.default_language\n\t\t\n\treturn u\"\"\"\n\t%(extension)s;\n\t<a href=\"%(lang)s%(path)s\">%(file)s</a>;\n\t\"\"\" % {'lang': lang, 'path': file_path, 'extension': os.path.splitext(file_path)[1]}\n\ndef add_video_plugin(plugin_type, title, content):\n\tfile_prefix = os.path"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tpath = os.path.join(self.root, lang, file_path)\n\tbase = os.path.dirname(file_path)\n\text = os.path.splitext(file_path)[1][1:]\n\n\t# file_name = os.path.basename(file_path).split('.')\n\t# filename = file_name[0]\n\t# return '_'.join((base[:-1], filename, ext))\n\treturn '_'.join((base, file_path))"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself.original = self.converter.convert(file_path)\n\tself.en = self.convert_to_english(file_path)\n\n\n\t# ---\n\t# get page\n\t# ---\n\tself.get_page()\n\t\n\tf = PdfFileReader(self.en)\n\tpage = f.getPage(1)\n\n\n\t# ---\n\t# get a writer\n\t# ---\n\tself.pw = PdfContentByteWriter()\n\tf = PdfFileReader(self.original)\n\tself"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\treturn self.__renditions.add_rendition(lang, file_path)\n\t\n\t\n\ndef copy_file(file_to_copy, file_to_rename, files_to_copy):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\n\t\"\"\"\n\treturn os.system(\"cp -R %s %s\" % (file_to_copy, file_to_rename))\t\n\t\n\t\n\t\ndef move_file("}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself.settings[\"artigo2\"][\"page_order\"] = {\n\t\t\"en\": \"pages\",\n\t\t\"tr\": \"pages\",\n\t\t\"de\": \"pages\",\n\t\t\"pl\": \"pages\",\n\t\t\"es\": \"pages\",\n        }\n\t\t\n\t# print file_path\n\t\n        f=open(file_path,\"w\")\n\tf.write(\"Content-type: application/pdf\\r\\n\\r\\n\")\n\tf.write(\"<?xml version=\\\"1.0\\\"?>\\r\\n\\r\\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\treturn '%s%s.pdf' % (lang, file_path)\n\ndef get_page_number():\t\"\"\"\n\t{\n    \"original\": 2,\n    \"en\": 2,\n}\t\"\"\"\n\treturn 'original'\n\t\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself.appendChild(self.DocumentType(\"en\"))\n\t\n\tself.appendChild(self.Text(\"Artigo inicial\"))\n\t\n\tself.appendChild(self.Text(\"Nome da imagem: \"))\n\tself.appendChild(self.Text(file_path))\n\n\tself.appendChild(self.Text(\"\"))\n\t\n\tself.appendChild(self.Text(\"\"))\n\t\n\tself.appendChild(self.Text(\"\"))\n\t\n\tself.appendChild(self.Text(\"\"))\n\t\n\tself.appendChild"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tbase_url = \"http://a.beijing.qzone.cn/xls/data/%s/%s\"\n\trv = \"\"\n\tpage_count = self.get_page_count()\n\tfor i in range(page_count):\n\t\trv += \"\\n<tr>\\n<td>%s</td>\\n<td>%s</td>\\n\" % (lang, file_path)\n\t\trv += \"\\n</tr>\\n\"\n\trv += \"\\n</tbody>\\n</table"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t# self.env[\"ir.attachment\"].update(\n\t#     ir_cronjob_attachment_wizard.get_rendition_templates(self.env, file_path, lang, \"res.partner.wizard\")\n\t\t)\n\treturn res_partner_wizard._render_template(self, \"addons/attachment/rendition_add.jinja\", lang or \"en\", file_path=file_path)\n\t\n\t\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself.doc_file_path = file_path\n\tfile_path = util.path_join(self.full_target_path, \"en\", self.doc_file_path)\n\tfile_path = util.path_join(self.full_target_path, lang, file_path)\t\n\treturn render.RenderResult()\n\t\t\n\t\ndef setup_logging(**config):\n\t\"\"\"setup module logger\"\"\"\n\timport logging\n\thandler_list = [logger.getLogger(\"general\")]\n\tfor key in config:\n\t\t"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\traise NotImplementedError\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t#TODO: get full path of this asset/directory from config\n\t\n\ttry:\n\t\tpath = os.path.join(self.config['paths']['assets'], basename)\n\texcept:\n\t\tpath = os.path.join(self.config['paths']['assets'], 'artigo02-gf03.tiff')\n\t\n\tlog.info(\"Adding asset: %s\" % path)\n\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\n\tmtime"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tgf03data = open(str(file_path), 'rb').read()\n\timgdata = gf03lib.get_image_data(gf03data, width=200, height=200)\n\t#imgdata = gf03lib.read_image_data(gf03data)\n\t#imgdata = imgdata.get_imagedata()\n\tresult = imutils.create_image(imgdata, [0, 0])\n\t#imutils.write_bitmap_image(imgdata, str(fh.name) + '."}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tprint \"In add_asset\"\n\tself.add_asset_string(basename, file_path)\n\n#\n# create_dir()\n#\n# # dir1 = cwd()\n# # if dir1 is NULL:\n# #    dir1 = \"/tmp/dir_add\"\n#\n# # print dir1\n# #\n# # dir1 = \"../dir1\"\n# #\n# # print dir1\n# #\n# #\n# # make_dir()\n# # make_dirs()\n# # get_file_"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\twith open(file_path, \"rb\") as f:\n\t\tif basename.startswith(\"artigo\"):\n\t\t\treturn StringIO(f.read())\n\t\telse:\t\n\t\t\treturn None\n\t\t\t\t\n\t\t\t\t\n\t\n\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\timport glob\n\tartigo = self.g.search_result(basename)\n\n\tif not artigo:\n\t\traise Exception(\"Unable to open file {0}\".format(file_path))\n\n\tif not os.path.isfile(os.path.basename(file_path)):\n\t\traise Exception(\"Path {0} is not a regular file\".format(file_path))\n\n\tfile = (os.path.basename(file_path))\n\n\t# remove empty file\n\tfile = file[file.find(\".tiff\"):file."}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tself.add_asset(fullpath=file_path)\n\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\treturn\n\n\n\n\t#\n\t# Return value\n\t#\n\tdef asset(self, basename, file_path):\n\t    self.basename = basename\n\t    if self.basename in sys.path:\n\t        return sys.path.index(self.basename)\n\t    elif os.path.exists(file_path):\n\t        return os.path.split(file_path)[1]\n\t    else:\n\t        return 0\n\n\n\t#\n\t# Return value\n\t#\n\tdef basename(self, value"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\treturn self.addAsset(\"artigo02-gf03.tiff\", file_path)\n\nif __name__ == '__main__':\n\t\n\t# Test 1 for gf03\n    test_module = globals()['test1']\n    test_module.runfile(globals())\n\n    # Test 2 for gf04\n    test_module = globals()['test2']\n    test_module.runfile(globals())\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tfile_path = os.path.join(self.path, basename + \".tiff\")\n\ttry:\n\t\tself.output['outputs']['asset'].append(file_path)\n\t\tself.output['outputs']['asset_type'].append(None)\t\n\texcept KeyError:\n\t\tself.output['outputs']['asset'] = []\n\t\tself.output['outputs']['asset_type'] = []\n\t\tself.output['outputs']['asset'].append(file_path"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\tzip_file.extractall(dir=os.getcwdu())\n\t# return a dict with keys by basename and values by path\n\n\ndef _explore_yaml_dir(dir_path):\n\tfor root, dirs, files in os.walk(dir_path):\n\t\tfor f in list(files):\n\t\t\tif f.endswith(\".yaml\") :\n\t\t\t\tfull_path = os.path.join(dir_path, root, f)"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tout = OrderedDict()\n\tzip_filename = os.path.basename(zip_path)\n\tzip_dir = os.path.dirname(zip_path)\n\trootDir = zip_path\n\tos.system(\"cd \"+rootDir+\" && zcat \"+zip_filename+\" |\"+\"zcat -o \"+zipdir+\"/\"+zip_filename+\" |\"+\"wc -l |grep \"+zipdir+\"/\"+zip_filename)\n\t\n\t# print(rootDir+\"/\",\"/\"+zip"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\txml_file = zip_file.readfile('c.xml')\n\txml_file = xml_file.read()\n\n\tdef get_single_item(item_dict, base_dict):\n\t\tkey = base_dict[item_dict['base']]\n\t\t#print(key)\n\t\tif key not in item_dict:\n\t\t\titem_dict[key] = ''\n\n\t\ttry:\n\t\t\tvalue = item_dict[key]\n\t\texcept:\n\t\t\titem_dict[key]"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t_data = {}\n\tzinfo = zipfile.ZipInfo()\n\t#first get the data from the first file (first)\n\tfor path, dirs, files in os.walk(zip_path):\n\t\tif files==[]:\n\t\t\tname = path[_data['basename'] + '.xml']\n\t\t\tfetch_cmd = 'zcat -Q'+ urllib.request.urlretrieve(os.path.join(path, name).replace('.xml','.zip'))[0]\n\t\t\tsubprocess.check_call(fetch_cmd"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tdata = {}\n\ttry:\n\t\tzip_filename = zipfile.ZipFile(zip_path).read('__metadata__')\n\t\tzip_dict = pytoml.decode(zip_filename)\n\t\tdata = json.load(zip_dict, object_hook=get_dict_from_json)\n\texcept zipfile.structures.ZipError:\n\t\tprint (\"\"\"\nError: zipfile package not opened\n\"\"\") # This will be fixed once Zipfile package is gone\n\nplt_dict = []\n\nfor item in listdir(DATA_DIR"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\"\"\"\n\t{\n\t\t\t'<path>': zip_path,\n\t\t\t'<files>': {\n\t\t\t\t '<path>': zip_path\n\t\t\t\t},\n\t\t\t'<files>': None\n\t\t}\n\t\"\"\"\n\t\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tdata = zf.infolist()\n\t\tfiles = {}\n\t\ttry:\n\t\t\tfor f in data:\n\t\t\t\tbasename = os.path.basename(f.filename)\n\t\t\t\t\n\t\t\t\tif basename in files:\n\t\t\t\t\tcontinue\n\t\t\t\tfiles"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tdict = {}\n\tfilename_list = glob.glob(zip_path+\"/*.*\")\t\n\tfor filename in filename_list:\n\t\tif os.path.isdir(filename):\n\t\t\tsubDirList = []\n\t\t\tif not os.path.isdir(filename):\n\t\t\t\tcontinue\n\t\t\tfor name in os.listdir(filename):\n\t\t\t\tfileName = os.path.join(filename, name)\n\t\t\t\tfileData = open(fileName,'rb').read()\n\t\t\t\tfileData = cStringIO.StringIO(fileData)\n\t\t\t\t(packageName"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tfile_list = []\n\tfile_list = os.listdir(zip_path)\n\tfor d in file_list:\n\t\ttry:\n\t\t\twith open(os.path.join(zip_path, d), 'r') as zip_file:\t\n\t\t\t\traw_data = zip_file.read()\n\t\t\t\tfile_list.append(raw_data)\n\t\texcept TypeError:\n\t\t\tif '<file>' in d:\t\n\t\t\t\twith open(os.path.join(zip_path, d), 'rb') as zip_file:\n\t\t\t\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tdict_list = {}\n\tfor f in os.listdir(zip_path):\n\t\tname = f[len(zip_path):].lstrip('~')\n\t\tdict_list[str(zip_path)] = name[6:]\n\treturn dict_list\n\ndef get_package_list(zipfile):\n\tpkg_list = []\n\tfor f in os.listdir(zipfile):\n\t\tname = f[len(zipfile):].lstrip('~')\n\t\tif f!= \"__init__\" and name!= \"__init__.py"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t# read from zip file\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\t\n\t# read all contained dirs\n\tdirectories = []\n\tfor dir_path in zip_file.namelist():\t\t\n\t\tdirectories.append(dir_path)\n\t\n\t# group files by their basename and output in dict format\n\tfileses = {}\n        # group files by their basename and output in dict format\n\tfor base, sub_fileses in zip(os.pathsep.split('/'), directories):\t\n"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\treturn [\n\t\tfile_ for _,\n\t\tf_fname, file_ in zip(files_list_from_zipfile(zip_path),\n\t\t\t\t\t\t\t\t\t\tfiles_list_from_zipfile(zip_path),\n\t\t\t\t\t\t\t\t\t\t['file'])\n\t]\n\t\n####################################################################\n# Test\n####################################################################\n\ndef test_files_list_list(): # Check for file structure with subfiles\n\tfiles_list=files_list_from_zipfile(zip_path)\n\tfor file_in_zip_path in files_list:\n\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tfiles = []\n\tfiles = files_list_from_zipfile( zip_path )\n\treturn files\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tdef get_filelist_zip(zip_path):\treturn zipfile.namelist(zip_path)\n\tzip_name, entry = next(get_filelist_zip(zip_path))\n\treturn zipfile.read(zip_name)[:-1]\n\n# from boto3.exceptions import ClientError\n# from boto3.resource import EC2Connection\n\n\"\"\"\nCheck all the files in `dir`\n\nIf the `expire_time` isn't None, then when the last known time is older than the expire_time, delete the file"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tfiles_dict=[]\n\tfp=None\n\tfor root, _, files in os.walk(unpack_path):\n\t\ttry:\n\t\t\tfor f in files:\n\t\t\t\t\n\t\t\t\tfp=zipfile.ZipFile(root+'/'+f)\n\t\t\t\tmtime=fp.file('mtime').read()\n\t\t\t\tif mtime!= '' and mtime!= None:\n\t\t\t\t\tfp.file('mtime').close()\n\t\t\t\t\n\t\t\t\t#files_dict.append((fp.file('name').read(), mtime))\n\t\t\t\tfiles_dict.append((fp."}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\treturn zip_files(zip_path)\n\n@task(name='files-from-zip-2')\ndef files_list_from_zip(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2517-0350-tinf-33-4009/2517-0350-tinf-33-4009.pdf',\n    '2517-0350-tinf-33-4009/2517-0350-tinf-33-4009.xml',\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\timport zipfile\n\t\n\tdirs = zipfile.zipfile(zip_path=zip_path,  zinfo=zipfile.ZIPFILE).namelist()\n\tfiles = [(d.split('/')[-1].split('-')[0], d) for d in dirs if d.split('/')[-1].split('-')[0]!='z' or d.split('/')[-1].split('-')[1]!='gf']\n\treturn [file[1] for file in files if os.path"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tresults=[]\n\tfor fpath in os.listdir(zip_path+('./files_list_root.','files_list_root.txt')):\n\t\ttry:\n\t\t\tpath,fpart=os.path.split(fpath)\n\t\t\tresult = fpart.split(\"_\")\n\t\t\tif (path.startswith(zip_path)):\n\t\t\t\tif (result[0]==\"files_list_root.\"):\n\t\t\t\t\tresults.append(fpath)\n\t\t\t\telif (result[0]==fpart and result[1]==\"files"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\twith ZipFile(zip_path, 'r') as f:\n\t\tret_files = {}\n\t\tnum = 15\n\t\tfor file_i in xrange(num):\n\t\t\tfile_path = f.read(file_i * 10)\n\t\t\tret_files[len(f.namelist()) + 1 - file_i] = file_path\n\t\treturn ret_files\n\n\"\"\"\nExample:\n\n```\n# from python 3\nfrom zipfile import ZipFile\n\nimport numpy as np\n\n# read files one by one\nfiles = ['2318-"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\treturn zip(glob.glob(zip_path))\n\n\n# Example:\n#\n# ```\n# zipname = sys.argv[1]\n# filename = sys.argv[2]\n#\n# with zipfile.ZipFile(zipname, \"r\") as zf:\n#     print(\"zf is \" + zf.filename)\n#\n# print(files_list_from_zipfile(zipname))\t# print the file names (no file names on the ZipFile object)\n# ```\n\nif __name__ =="}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tret = []\t\n\tif zip_path not in ['0.0.0.0/s', '0.0.0.0/p', '0.0.0.0/t', '0.0.0.0/dt']:\n\t\treturn ret\n\tif zip_path == '0.0.0.0/s':\n\t\tret = sorted(sorted(s) for s in os.listdir('./') if s.endswith('.srt'))\n\telse:\n\t\tret = sorted(sorted(s."}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tif re.search(\"^[^:;\\\"\\(\\)]*\\w+:[^;\\\"\\(\\)]*$\", content):\n\t\tpattern = re.search(\"^(:[^;\\\"\\(\\)]*?st-?[^;\\\"\\(\\)]*-?[^;\\\"\\(\\)]*-?[^;\\\"\\(\\)]*?st)?\\w*:[^;\\\"\\(\\)]*$\", content)\n\t\trepl_text = content\n\t\tcontent = pat_result = re.sub(pattern.group(0)[0], repl_text"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = re.sub(r'^<[^!?>]+?>?[^<]*(?<!<!.)</(?!<!.)[^>]*>?[^<]*', '', content, True)\n\treturn re.sub(r'^<\\!?>', 'w-xmpp-x', content)\n\n\t### XMPP\n\t\"\"\"\n\tReturns content that does what is expected for a\n\tXMPP 'name=value' fragment.\n\t\"\"\"\n\tdef xmpp_x(self, content):"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\ts = re.search(r'w:st=\"([^\"]*)\"', content, re.DOTALL)\n\tif s!= None:\n\t\ts, _ = s.span()\n\t\ts =  s.replace('w:st=\"\",', 'w:st=\"\\\"\")\n\t\ts = s.replace('w:st=\"\\n\\\",', 'w:st=\"\\n\\\",')\n\treturn s\n\t\n\t\n\t\ndef add_xmlfilter(filters, namespace, outf):\n\t\"\"\"\n\tTake a list of"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t# return'st=\"' + content + '\"'\n\tcontent = content.replace('@NAMESPACEPREFIX@', '@NAMESPACE@')\n\treturn content\n\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t##\tTODO: use a regexp to capture 'w:ns=\"'\n\t\n\tcontent = re.sub('w\\.?NS=\"', 'w-ns=\"', content)\n\treturn content\n\ndef fix_namespace(content):\n\t##\tTODO: use a regexp to capture 'ns=\"'\n\t\n\tcontent = re.sub('ns=\"', 'ns=\"' + config.namespace, content)\n\treturn content\n\t\ndef fix_type(content):\n\t##\tTODO: use a regexp to capture '"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.replace('w:st=wsjb', 'w-st=wsjb')\n\tcontent = content.replace('wsjb', 'wst=wsjb')\n\tcontent = content.replace('wst','st=wsjb')\n\treturn content\n\n\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn content\ndef fix_namespace_prefix_u(content):\t\"\"\"\n\tConvert text that defaults to 'u:st=\"' to 'u-st=\"'\t\"\"\"\n\treturn content\ndef fix_namespace_prefix_t(content):\t\"\"\"\n\tConvert text that defaults to 't:st=\"' to 't-st=\"'\t\"\"\"\n\treturn content\ndef fix_namespace_prefix_c(content):\t\"\"\"\n\tConvert text that defaults to 'c:st=\"' to 'c-st=\"'\t\"\"\"\n\treturn content"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.strip()\n\treturn content\n\ndef fix_namespace_prefix(content):\t\"\"\"\n\tConvert text that defaults to 'xmlns=\"' to 'xmlns'\n\t\"\"\"\n\tcontent = content.strip()\n\treturn content\n\ndef fix_namespace_with(content, wns_regex='.*xmlns=.*'):\n\t\"\"\"Extract attributes of namespace prefix from content,\n\tregex the ns prefixes.\n\n\tReturns: a pair (has_prefix, name)\"\"\"\n\n\tcontent_split = content.split(' xml"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tprefix = re.sub(r'{%s}%s' % (re.escape(namespaces.w), re.escape(namespaces.st)),\n\t\t\t\t\t  'rew-st=\"', content, 1)\n\treturn prefix"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn content.replace('w:st=\"', 'w-st=\"')\n"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\treturn next(match_single(node, pubdate_xpaths))\n\n# Returns the first match in the pubdate_xpaths list\t\"\"\"\ndef match_single(node, pubdate_xpaths):\t\"\"\"\t\n\tReturns a single match for the pubdate_xpaths list\t\"\"\"\n\tmatch = match_single_from_xpath(node, pubdate_xpaths)\n\n\treturn match\n\n# Takes a single match from the pubdate_xpaths list\t\ndef match_single_from_xpath(node, pubdate_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tmatch = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_str = node.xpath(pubdate_xpath).extract_first().text\n\t\tmatch = pubdate_str\n\t\tfor pattern_xpath in [ 'first_of_century', 'prev_of_century', 'last_of_century' ]:\n\t\t\tif pubdate_str.endswith(pattern_xpath):\n\t\t\t\tmatch = match + '_' + pattern_xpath\n\t\t\n\treturn match\t\t\n\t\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t# We have no idea how the original list is called.\n\tpubdate_list = node.xpath(pubdate_xpaths)\n\tmatch = None\n\tfor element in pubdate_list:\n\t\tif element.text and ((element.text == 'yes' or element.text == 'true')):\n\t\t\tmatch = element\n\treturn match  \n\n\ndef handle_pubdate(node, pubdate_xpaths, file_path):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tmatch = None\n\tfor"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif len(pubdate) > 0:\n\t\t\tmatch = re.search(\n\t\t\t\tr'\\bpubdate(\\d+)?',\n\t\t\t\tpubdate[0].text)\n\t\t\tif None  is not match:\n\t\t\t\treturn pubdate[0]\n\treturn {}\n\t\n\ndef convert_to_html(node, title, url, pubdate):\n\tpubdate = pubdate_to_html(pubdate)\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor p in pubdate_xpaths:\n\t\tmatch_text = False\n\t\tmatch_xml = False\n\t\tmatch_etree = True\n\t\tfor match in re.finditer(\"(?:<dt>|/dt>)\", node.text):\n\t\t\tif match.group(1):\n\t\t\t\tmatch_text = match.group()(1)\n\t\t\tif match.group(4):\n\t\t\t\tmatch_etree = match.group()(4)\n\t\tif match_text:\n\t\t\tfor match in re.finditer(\"(?:<dd>(.+?)</dd"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor node_xpath in node.xpath('''//{http://www.idpf.org/2007/v1.0}pubdate/text()'''):\n\t    pubdate_yield(node_xpath.text)\n\tif len(pubdate_xpaths) == 0:\n\t\traise InvalidArguments('The --pubdate was called with no arguments')\n\telse:\n\t\treturn pubdate_xpaths.pop(0)\n\t\ndef match_date(node, date_xpaths):\t\"\"\"\tReturns the value"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\t\t\t\t\t\n\t\tmatch = re.search(r'^\\s*(<pubdate>)(.*)$', pubdate_xpath)\n\t\tif match:\treturn match.groups()[0]\t\t\t\t\t\n\treturn None\n\t\n\ndef get_public_key(node):\t\t\t\t\n\t\"\"\"\n\tReturns the first match in the public_key_xpaths list\t\"\"\"\n\tfor pubkey_xpath in public_key_xpath_list:\t\t\t\t\t\n\t\tmatch = re.search(r'"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\txpath = pubdate_xpaths[0]\n\tif xpath == '':\n\t\treturn None\n\tpath = pubdate_xpaths[0]\n\tresult = re.search(xpath + '//*[@pubmodate]/@pubdate', node.getElementsByClassName('pubdate')[0].firstChild.data, re.MULTILINE)\n\tif result is None:\n\t\treturn None\n\t#print len(result.group(1))\n\treturn result.group(1).strip()\n\n# this method"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\n\t#if no match found\n\tif len(xpaths) == 0:\n\t\treturn \"\"\n\t#if match not found but no matches\n\tif len(pubdate_xpaths) == 0:\n\t\treturn \"\"\n\n\tfor pubdate in pubdate_xpaths:  # returns matching matches for all matching paths\n\t\tmatch = pubdate.evaluate(node, xpaths)\n\t\tif match is not None:\n\t\t\treturn match\n\n\t#check for one match only\n\tif len(xpaths)!= 1:\n\t\tprint_log(\"Could"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tpubdate_xpath = []\n\tfor index in range(0, len(pubdate_xpaths)):\n\t\tpubdate_xpath.append(pubdate_xpaths[index][0].xpathExpression(0))\n\t\n\txmllist = None\n\tmatch = None\n\tfor xpath in pubdate_xpath:\n\t\txmllist = xpath.xpath(pubdate_xpaths[0])\n\t\txmllist_len = len(xmllist)\n\t\tif xmllist_len"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# Extract number and suppl from the issue content\n\tcontent = issue.text\n\tcontent = content.split(' ') if not content else []\n\tfor i, t in enumerate(content):\n\t\tparts = t.split('#')\n\t\tif len(parts) == 1:\n\t\t\tcontinue\n\t\telif len(parts) == 3:\n\t\t\telements = parts\n\t\telse:\n\t\t\traise ValueError('Invalid element on issue: %s' % e)\n\t\tfor j in elems:\n\t\t\t# Extract the possible values of number and suppl\n\t\t\tif not parts"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# Get the possible values for the number\n\t# Extract only numbers from the contents\n\tnumber_value = _extract_number_and_suppl(issue)\n\treturn number_value\n\n\ndef _is_suppl_tag(tag):\n\t\"\"\"\n\tReturn True if element has only one allowed supplier\n\t\"\"\"\n\treturn len(tag.findall('.//suppl')) == 1\n\n\ndef _is_text_node_extracted(node_element):\n\t\"\"\"\n\tReturn True if element is a text Node of the type text\n\t\"\"\"\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber_and_suppl_xml =  issue['details']['suppl']\n\tsupplier_list = []\n\tfor supplier in number_and_suppl_xml.iter():\n\t\tif \"SaleSupplier-Supplier\" in supplier.text:\n\t\t\tsupplier = supplier.text[supplier.text.index( '<' ):]\n\t\t\tsupplier = re.compile(r'\\bSaleSupplier-Supplier\\b|\\bSupplier\\b\\s+:\\s+[\\s\\S]+').parse(supplier"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tissues = list(issue.elements)\n\tissue = issues[0]\n\tisNamed = False\n\tif isinstance(issue.body, list):\n\t\tissue.body = issue.body[0]\n\t\tisNamed = True\n\tif not isNamed:\n\t\t# TODO: need issue body to be a list\n\t\tissue.body = issue.body.name\n\t\tif issue.body.strip().endswith('<br>'):\n\t\t\tissue.body = issue.body[:-len('</br>')]\t# trim last line"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber = get_attr_value(issue, 'number')\n\tsupp = get_attr(issue,'supplement')\n\tissue.number_and_supplement = (number, supp)\n\n\treturn issue\n\t\"\"\".\n\t\"\"\"\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\n\tvalues = {}\n\t\n\tfor i in issue:\n\t\tif not i.find(u\"http://\"): values[i.find(u\"http://\").partition(u\" \")[1].strip()] = i.partition(u\" \")[2].strip()\n\n\treturn values\t\n\n\t\"\"\"\n\tTODO:\n\t\tadd in the specific conditions below for specific supplier ids for non existing supplier ids etc.\n\t\"\"\"\t\n\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tresult = extract(issue, \"Number\", \"Suppl\")\n\tfor line in result:\n\t\tif line.has_key('Suppl'):\n\t\t\tline['Summary'] = line['Description']\n\treturn result\n\ndef write_issue_list():\n\t\"\"\"\n\tFunction to write a sorted issuelist to a file.\n\t\"\"\"\n\twith open('issuelist.txt', 'w') as issuelist_object:\n\t\tissuelist = sorted(issue_list_object.readlines())\n\t\tprint(\"Found \", len(issuelist), \" issues and \", len("}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber = issue.content('n').replace('\\xc2\\xa1', '')\n\tsupp = issue.content('s').replace('\\xc2\\xa1', '')\n\treturn number, supp\n\ndef _extract_issue_element_and_source_and_issue_issue(issue_xml):\t\t\t\n\tissue_element = issue_xml.find('./issue')\n\tsupplement = issue_element.find('./supp').source()\n\tsource = issue_element.find('./source').content()\n\tissue_issue = issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tfor num, supp in issue.attrib.get_all(\"suppl\", default=0):\n\t\t\"\"\"if num is a string, we have it as our value\"\"\"\n\t\tint_value = int(num)\n\t\treturn {num: int_value, supp: int_value} # We only use the first\n\treturn None\n\n\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber, product = _extract_number_and_suppl(issue)\n\n\tvalue_str = next((v for v in issue.value.split() if v and v.strip()), 'None')\n\tvalue = ''\n\tif value_str == None: return None\n\n\tvalue = value_str.strip() \n\tvalue = value.replace('$', '0')\t\n\tvalue = value.split('/')[1]\n\tvalue = value.strip()\t\n\tsupp = None\n\tif value == 'None': return None\t\n\tif"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\treturn pformat(self)\n\nif __name__ == '__main__':\n\timport doctest\n\tdoctest.testmod()\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif not indent and not debug: return \"\"\n\tif debug:\n\t\tprint \"pretty:\"+ self.summary()\n\telse:\n\t\tresult = ''\n\t\tif self._debug:\n\t\t\tfrom pprint import pprint\n\t\t\tpprint(self.summary())\n\t\treturn result\n\n\tdef summary(self):\n\t\ts = '{0:<6} {1}'.format('','{0}'.format(self.__dict__))\n\t\tif \"parsing\" in dir(self):\n\t\t\ts +='- {0:<6}'.format(' '."}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif debug:\n\t\tprint \"pretty(<\", indent, \"<\", self, indent)\n\t\tprint \">\")\n\n\treturn pretty_list(iter(self))\n\n\n\tdef pretty(self, indent=0, debug=False):\n\t\t\"\"\"\n\t\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug:\n\t\t\tprint \"pretty(<\", indent, \"<\", self, indent)\n\t\t\tprint \">\")\n\t\t\t\n\t\t\t\n\t\treturn _generic.Generic.pretty(self, indent=indent, debug=debug)\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tout = StringIO.StringIO()\n\t\n\tindentStr = (''* indent)\n\t\n\tif debug:\n\t\tprint'  '+ indentStr + self.__class__.__name__\n\t\n\tif debug:\n\t\tfor i in xrange(len(self.__class__.__dict__)):\n\t\t\tval ='   '+ self.__class__.__dict__[i].__class__.__name__\n\t\t\t\n\t\t\tkey = i +'   '\n\t\t\tprint key + val\n\t\t\n\tdef __str__("}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\n\tdef dump_indent(self, indent, base_indent, is_func=True, level=0, end=False):\n\t\t\"\"\"\n\t\tSame as self.__str__.\n\n\t\tIf debug is True, pretty print each line of self.\n\t\t\"\"\"\n\n\t\tif end is True:\n\t\t\ttext = self.__str__()\n\t\telse:\n\t\t\tif is_func:\n\t\t\t\tif '__repr__' not in vars(self):\n\t\t\t\t\ttext = self.__str__()\n\n\t\t\telse:\n\t\t\t\tif '__repr__'"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tout = \"\"\n\tout = out + indent * \"  \" # pretty\n\tfor r in self.row:\n\t\tfor c in r:\n\t\t\tif (c == '&'):\n\t\t\t\tout += indent * \"&\"\n\t\t\telif (c == '<'):\n\t\t\t\tout += indent * \"<\"\n\t\t\telif (c == '>'):\n\t\t\t\tout += indent * \">\"\n\t\t\telif (c == '\"'):\n\t\t\t\tout += indent * '\"'\n\t\t\telif (c == '`'): # this is a special tag\n\t\t\t\tout +="}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tout = \"\"\n\tif self.pretty_headers:\n\t\tout += \"\\n\\n\"\n\tout += self.make_html()\n\tfor header in self.headers:\n\t\tif out: out += \"\\n\"\n\t\tout += header\n\tif out: out += \"\\n\"\n\treturn out\n\n\n# ------------------------------------------------------------------------\n\n\tdef check_content(self):\n\t\t\"\"\"\n\t\tCheck data's content so it satisfies the data format, and then\n\t\tprocess and return the result back by calling the.set_somahore method.\n\t\t\n\t\tNOTE: this"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif debug:\n\t\tpretty = Pretty(self)\n\tif indent > 0 & pretty:\n\t\treturn \"\\n%s\" % str(list(pretty))\n\treturn str(pretty)\n\n\ndef pretty(num, p=2, c=3, w=5, s=15):\n\t\"\"\"\n\tReturn a pretty form the number given in its argument.\n\t\"\"\"\n\tn = int(num)\n\tif n < 0:\n\t\tprint(\"Error: The number can't be negative.\", file=sys.stderr)\n\t\treturn\n\t\n\t#"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\treturn self.repr(0, debug)\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tdef __repr__(self):\n\t    if debug:\n\t\treturn '{:s} {}{:s}'.format(self.__class__.__name__, self.line, self.indented)\n\n\t  return '{:s}{:s}'.format(*([repr(x) for x in self.line] + list(self.indented))\n    def __str__(self):\n\treturn ''.join([repr(x) for x in self.line])\n\n\n"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif '\\(A | B\\)' in self.text:\n\t\treturn _absorb_args(self, args)\n\treturn self._normalize_abs(self._absor(args))\ndef _absorb_args(args, list=False):\n\tabs = set()\n\tfor arg in args:\n\t\tif '&' not in arg.text:\n\t\t\tabs.add(arg)\n\t\tif '&' and '&' not in arg.text:\n\t\t\tabs.add('&' + arg)\n\tabs = _absorb_abs("}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\treturn [exp(self, p) for p in args]\n\n\n@absorb\ndef diff(self, other):\t\"\"\"\n\tDifference between the two lists (or pairs of lists), if any.\n\nSee https://en.wikipedia.org/wiki/Difference_algorithm\"\"\"\n\treturn other([exp(self, p) for p in self])\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\targs = args[1:]\n\tif args[0].isRelational and args[0].value.isInteger:\n\t\traise RuntimeError('negative absorption not implemented')\n\telif args[0].isRelational:\n\t\tif args[0].value.isZero():\n\t\t\traise RuntimeError('negative absorption cannot be applied')\n\t\telif args[0].value.isNotZero():\n\t\t\treturn _absorb(args[0], args[1:])\n\t\telse:\n\t\t\treturn [ _absorb(arg.value, args[1:]) for arg in"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t# A was probably checked at the top of the program.\n\tif len(args)!= 2:\n\t\tprint \"A is not set.\"\n\t\treturn\n\tsarg0 = args[0]\n\tsarg1 = args[1]\n\tnews = []\n\tsarg0.preprocess(sarg1)\n\tif isinstance(sarg0.type, list):\n\t\toldtype = sarg0.type\n\t\tfor _ in range(0, len(sarg0.type)-1):\n\t\t\tsarg0 = sarg0 % sarg"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif len(args) > 16: return args\n\tabsmodels = []\n\tfor a in args[-21:-13]:\n\t\tabsmod = a.exp.absMod.absModExpr\n\t\tabsmodels.append(absmod)\n\treturn absmodels\n\nAbsorption.abs = lambda args: len(args) == 7 and Absorption.absor(args)\nNegativeAbsorption.abs = lambda args: len(args) == 7 and NegativeAbsorption.absorb(args)\n\"\"\"\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\treturn self._absorb.apply(args)\n\ndef remove(self, expr):\n\t\"\"\"Given an `expr` iterable, removes the expr from the list.\n\nNote that this method will raise no error if the `expr` cannot be applied to\neach element of the list, otherwise it will return a list that contains any\nelement that does not match one of the first arguments.\"\"\"\n\n\treturn self._remove.apply(expr)\n\ndef intersection(self, a, b):\n\t\"\"\"Given a `a` and `b` expression sequence, returns a `b` expression"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif len(args) < 2:\n\t\traise SyntaxError(\"expected 2 arguments.\")\n\n\tlhs, rhs = args\n\tif not isinstance(lhs, ast.Expr):\n\t\tlhs = ast.Node(lhs)\n\tif not isinstance(rhs, ast.Expr):\n\t\trhs = ast.Node(rhs)\n\n\tlhs_idx, rhs_idx = lexeme(lhs, rhs)\n\n\ta0 = rhs_idx.value\n\n\tassert a0\n\n\tabsrhs ="}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n\tif len(args) == 0:\n\t\traise InvalidParameterTypeException()\n\t\n\tfor arg_idx, arg in enumerate(args):\n\t\tif not isinstance(arg, Expression):\n\t\t\traise TypeError('args is not an Expression, %dth argument is expected at index %d, but is %d' % (len(args), arg_idx + 1, type(arg)))\n\t\t\n\t\tif arg!= UNBOUNDARY and isinstance(arg.arg, AbsorvableExpression):\n\t\t\traise NotImplementedError()\n\t\t\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif (len(args)&1==1:\n\t\tret = []\n\t\tfor x in args:\n\t\t\tv = x.absorb(args[:-1])\n\t\t\tret.append(Absorption())\n\t\t\tret[-1].append(abs(*v, v, v))\n\t\treturn ret\n\t\t\n\treturn Absorpture()\n\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif len(args) < 1 or len(args) > 2:\n\t\traise syntax_error(\"Absorption takes a single (A & B) expression\")\n\ta1 = list(args)\n\ta1.reverse()\n\ta2 = list()\n\tfor k in args:\n\t\tif isinstance(k, ast.Name):\n\t\t\tk = str(k)\n\t\ta2.append(k)\n\ta3 = \"\"\n\tif len(args) == 2:\n\t\ta2, a3 = a2 + a3\n\tif a3"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t# We don't need to make sure that the passed-in hook value doesn't\n\t# start with \".\" and end with \". \", because we have already checked\n\t# that the hook does not start with one of its parent's prefixes.\n\tcallbacks = self._hook_prefixes.get(hook)\n\tif callbacks:\n\t\tcallbacks.append(hook)\n\t\treturn hook\n\n\treturn self.add_hook(hook)\n\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tself.handlers.append(hook)\n\treturn hook\n\n\t\ndef on(*hooks):\t\"\"\"\n\tDecorator function to add an array of hooks to the registry.\n\nArgs:\n    hooks (list): array of HookType \n\nReturns:\n    callable: Decorator for registering callables for the specified \n    hooks.\t\"\"\"\n\tfor i in range(len(hooks)):\n\t\th =hooks[i]\n\t\ton(h)\n\n\treturn hooks \n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tself._hooks = self._hooks + [hook]\n\n\treturn hook\n\n\n# #### Initialization and API ####\ntypehooks = hooks._typehooks.copy()\t\ntypehooks['name'] = __name__\n\nclass TypeHookManager(SingletonRegistry):\t\n\t\"\"\"\n\tThis is a Singleton that is instantiated when the core is initialized.\n\t\"\"\"\n\tdef __init__(self):\t\t\t\n\t\tSingletonRegistry.__init__(self)\n\n\n\tdef __getattribute__(self, key):\n\t\t"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef decorator(wrapped):\n\t\tregistry[hook] = function(wrapped)\n\t\treturn wrapped\n\treturn decorator\n\nclass EventListener:\n\tdef __init__(self, hook):\n\t\tself.hook = hook\n\t\tself.registration = {\n\t\t\t'window:title-changed': self.window_title_changed,\n\t\t\t'window:focus': self._on_window_focus,\n\t\t\t'window:blur': self._on_window_blur\n\t\t}\n\n\tdef event_dispatch(self, source, eventName, event"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef call_callback(*args, **kwargs):\n\t\tregister = self.hooks[hook]\n\t\tfor idx, (self, args_raw, kwargs_raw, callback) in enumerate(register):\n\t\t\t# Call the handler.\n\t\t\tif callback():\n\t\t\t\t# Call the callback for each of the callback args only if\n\t\t\t\t# the callback already has had at least one call.\n\t\t\t\tif hasattr(callback(), 'nr_callbacks_found'):\n\t\t\t\t\t# If there are some callbacks remaining,\n\t\t\t\t\t# the listener should re-register itself\n\t\t\t\t\t"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn _registers[hook]\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn self.hooks[hook][\"call\"]\n\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tself._add_to_hook_manager(hook=hook)\n\n\treturn hook\n\ndef unregister_hook(*args, **kwargs):\n\t_HOOK_MANAGER.remove_hook(*args, **kwargs)\nclass AddonWrap(type):\n\t_REGISTRY_ADDING = 'hooks'\n\t_REGISTRY_REMOVE = 'hooks'\n\tHOOKS = AddonWrap._REGISTRY_ADDING + AddonWrap._REGISTRY_REMOVE\n\t__all__ = ['add"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef register_listener(self, *args, **kwargs):\t\"\"\"\n\tRegister listener for the hook specified in the 'hook' parameter.\n\n\"\"\"\n\t\treturn lambda **kwargs: self.register_hook(hook, *args, **kwargs)\n\t\n\t@register_listener.connect\n\tdef on(self, *args, **kwargs):\t\"\"\"\n\tListen on the specified hook for a specified set of arguments.\n\nArgs:\n    hook (HookType) : the name of the hook.\n    args (list): The arguments to be"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef callback(self, event):\n\t\tself.hooks.append(hook)\n\t\tself.hook = event\t\n\t\thook.func(self)\n\n\treturn callback\n\t\n\t\n\t"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = dict()\n\tconfig['user'] = user\n\tconfig['host'] = etcd_host\n\tconfig['port'] = etcd_port\n\treturn config"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = dict()\n\tconfig['dbuser'] = user\n\tconfig['dbuser2'] = user\n\tconfig['dbuser3'] = user\n\tconfig['dbuser4'] = user\n\tconfig['dbuser5'] = user\n\tconfig['host'] = etcd_host\n\tconfig['port'] = etcd_port\n\tconfig['name'] = \"\".join(['etcd', 'etcd_port', etcd_port])\n\tconfig['id'] = \"0\",\n\tconfig['config_defaults'] = '',\n\tconfig['t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\t\n    config = dict(user,\t   \n\t\t  config=\"localhost\",\n\t\t  dbHost=\"${ETCD_HOST}\",\n\t\t  dbPort=\"${ETCD_PORT}\",\n\t\t  dbUsername=\"${USER}\",\n\t\t  dbPassword=\"${PASSWORD}\",\n\t\t  dbClusterName=\"${CLUSTERNAME}\",\n\t\t  etcd_host=etcd_host,\n\t\t  etcd_port=etcd_port,\n\t\t  etcd_name=\"${ETCD_NAME}\")\n\treturn"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\ttry_user=user.lower()\n\tbasedb=json.loads('''\"{}\",\"{}\",{} {},{}\".format(\n\t\ttry_user,\n\t\tbase_config(user, the_host, the_port),\n\t\tthe_namespace,\n\t\tthe_db_backend,\n\t\tthe_db_type)'''\t).format(\n\ttry_user,\n\tthe_host,\n\tthe_port,\n\tthe_namespace,\n\t#we use the config values directly so they don't have to be converted"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {\n\t\t\t'project': {\n\t\t\t\t\"name\": project,\n\t\t\t\t\"kind\": \"project\",\n\t\t\t\t\"description\": \"The example configuration to test what is done in your environment\",\n\t\t\t\t\"metadata\": {},\n\t\t\t\t\"default\": {\n\t\t\t\t\t\"etcd_host\": etcd_host,\n\t\t\t\t\t\"etcd_port\": etcd_port\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"group_config\": {\n\t\t\t\t\"name\": \"test\",\n\t\t\t\t\"kind\": \"group\",\n\t\t\t\t\"description\": \"group for your environment\",\n\t\t\t\t\"metadata\": {},\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = {\n\t\t\"type\": \"etcd\",\n\t\t\"host\": etcd_host,\n\t\t\"port\": etcd_port,\n\t\t\"user\": user\n\t}\n\n\treturn config\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {'naming_scheme':'starmap_'+user+'_'+'{0}.json'.format(user),\n\t\t\t'global_kv_path':'/etc/starmap_{0}'.format(user),\n\t\t\t'naming_rules':{user : {'prefixes''':'%s_'%user,'name':'%s_'%user}}\n\t\t\t   }\n\t\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = {}\n\tconfig['auth_url']='https://' + user + '/store'\n\tconfig['public_api_endpoint']= user + '/v1/'\n\tconfig['public_api_key'] = ''\n\tconfig['base_url'] = user + '/v1/'\n\tconfig['public_url'] = url\n\tconfig['public_keys_url'] = user + '/v1/public_keys'\n\tconfig['public_keys'] = []\n\tconfig['public_url'].extend(['public/signer/sign"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\t\n\tconfig = {\"etcd_host\" : etcd_host, \"etcd_port\" : etcd_port}\n\t\n\treturn config\n\ndef build_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\n\tconfig = get_config(user)\n\t\n\treturn config\n\ndef get_config(user):\n\t\n\tprint \"Getting configuration\"\n\t\n\tconfig = {\n\t\t\"etcd_host\" : user,\n\t\t\"etcd_port\" : etcd_port,"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tbase_config = dict()\n\tbase_config['username'] = user\n\tbase_config['password'] = user\n\tbase_config['database_type'] = \"etcd\"\n\tbase_config['host'] = 'localhost'\n\tbase_config['port'] = etcd_port\n\treturn base_config\n\ndef configuration_from_file(config_file, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration based on a file.\n\nArgs:\n    config_file (str): the name of"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\tif isinstance(dt, basestring):\n\t\t# Assume datetime string\n\t\ttz = dt\n\t\tif tz is not '' and tz!= 'UTC':\n\t\t\timport pytz\n\t\t\tif tz.is_ambiguous():\n\t\t\t\tdt = pytz.utc.localize(dt)\n\t\t\telse:\n\t\t\t\tdt = pytz.UTC.localize(dt)\n\t\treturn dt\n\telse:\n\t\treturn super(_fromutc_mixed, self)._fromutc(dt)\n\n\n\n#------------------------------------------------------------------------\n#"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not self._tzb:\treturn dt\n\n\ttzname = dt.tzname()\n\ttry: return self._tzb.fromutc(dt, timezone=None, ambiguous=True)\n\texcept AttributeError:\n\t\t# TODO: use ambiguous to determine the state of ambiguities\n\t\tif tzobject.is_naive(tzname): return dt.replace(tzinfo=TZInfo(utcoffset=-timezone))\n\t\treturn None\n\ndef dt_to_tz(dttz):\n\t\"\"\"\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn self.__check('_fromutc',dt,tz.stdtime)\n\t\n\t\n\t# -------------------------------------------------------------------------\n\t\n\tdef __check(self, op, v, base):\t\t\t\"\"\"\n\tChecks the given operation, assuming the result is based on\n\tthe previous operation (if any).  Checks the given value and\n\tthe type and then delegates to the operation, if any.\n\t\"\"\"\n\ttry:\n\t\treturn op(v, base)\n\texcept Exception as e:\n\t\traise TypeError(e)\t\t\t\t\t  \n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn self._to_utc(dt, dt.tzinfo)\n\ndef _fromutc(self, dt):\n\treturn datetime(*(dt.utctimetuple()))\n\n\n\n\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif dt is None:\n\t\treturn None\n\tif not isinstance(dt, basestring):\n\t\traise ValueError(\"Date object must be a string\")\n\tif dt=='':\n\t\treturn None\n\tif dt.tzinfo is None:\n\t\tdt = datetime.datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute)\n\tdt = dt.replace(tzinfo=tzutc())\n\tdwc = datetime.datetime(dt.year, dt."}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\tif self._fold:\t\tself._foldedit = dt\n\telse:\t\t\tself._unfolded = dt\n\tself._unfolded = self._unfolded.astimezone(tz=self.tz)\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn datetime.combine(dt.date(), (dt.time() - _tzoffset(dt.tzname(), dt.utcoffset())) * tz.utcoffset(dt.tzname()))\n\ndef _tzoffset(tz_name):\n\t\"\"\"\n\tReturns the timezone offset in minutes for a timezone-name\n\tname.  e.g. tz_name = \"EDT\"; returns -11 (UTC + 11 minutes).\n\t\"\"\"\n\ttry:\n\t\treturn tz.daylight * (tz_name == \""}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif type(dt)!= datetime:\n\t\traise TypeError('expected a datetime; given %s%s' % (type(dt), dt))\n\n\t# Get rid of microseconds, as these all cause problems because\n\t# of daylight saving changes.\n\t#\n\t# Note: do not change anything about `localtime` by just reusing\n\t# it here. If we need to change that, we'll have to use a real\n\t# calendar system and avoid a \"dead line\" of using the `utc` system.\n\t#\n\t# Also,"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn self._tzinfo.utcoffset(dt).timetuple()[:3] + (dt.tzname(),)\n\n\t\n\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif dt.tzinfo is None:\treturn _fromutc(self, dt)\n\treturn dt.replace(tzinfo=_UTC()).replace(tzinfo=dt.tzinfo)\n\t#return dt.time()\n\t#else:\n\t#\treturn dt.astimezone(_UTC())\n\t#\t#return dt.replace(tzinfo=dt.tzinfo,\n\t#\t#\treplace_operator=\"+\")\n\n\ndef now():\n\treturn _fromutc(datetime.now())\n\ndef tomorrow():"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(self.days, self.hours, self.seconds)\n\n\ndef set_default(obj):\n\t\"\"\"\n\tDecorator that sets the default object value of an object.\n\t\"\"\"\n\tif obj is None:\n\t\treturn obj\n\telse:\n\t\treturn set_default_if_none(obj)\n\nr = relativedelta(days=20, hours=12)\nset_default(r)\nset_default(r.years)\nset_default(r.months)\n\n\"\"\"\n\n>>> date(2009, 8,"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(self._attr_relativedelta(name='days'))\n\ndef relativedelta_relative(self, other, name):\n\treturn relativedelta(self, other, name)\n\ndef relativedelta_fuzzy_diff(self, other, fuzziness=1, name='fraction'):\n\t\"\"\"\n\tFuzzy diff returns a fuzzy diff of this object represented in integer values\n\tfor the relative attribute using a specific fuzziness\n\n>>> relativedelta_fuzzy_diff(self=1..2, other=3"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(**locals())\n\n\ndef to_offset(value):\n\t\"\"\"This function is the same as using the strftime() function, given a standard output format string.\"\"\"\n\ttry:\n\t\td=dateutil.relativedelta.strptime(value, \"%Y-%m-%d\")\n\t\tsign=1\t# default is the correct sign\n\t\tif '-' in value:\n\t\t\tvalue_1 = value.replace('-', '')\n\t\t\td =  d - dateutil.relativedelta.relativedelta(sign"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(**dict(\n\t\t(k, self.__dict__[k]) for k in self.raw.keys()\n\t\t))\n\ndef relativedelta_from_delta(delta):\t\"\"\"\n\tReturn a :class:`relativedelta.relativedelta` object from a\n\t:class:`dateutil.relativedelta.relativedelta` object\n\t\"\"\"\n\treturn relativedelta(**delta.__dict__)\n\ndef relativedelta_from_date(dt):\t\"\"\"\n\tReturn a"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(**{np.attr: d for d in self.__dict__.itervalues() if d in ('_attr',)} if d == '_attr')\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(*[self.days, self.hours, self.minutes])\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn dateutil.relativedelta.relativedelta(**{self: self.normalized()})"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(**{self.name: float(norm)})\n\n\n\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(_year = 1, _month = 2, _day = 3)\n\n\n__all__ = ['relativedelta','relativedelta_timedelta',]\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(**self._attributes)\t\n\n\t# The attributes returned from methods should be sorted correctly,\n\t# as they may be the result of applying 'normalized' to other objects.\n\n\n# TODO: Make this class class, rather, like DateTime, so it can be called\n# with either a year/month/day/time (depending on the date) as a keyword argument\n# or by constructing a specific instance from the date argument.\n# Perhaps, this line should have this keyword argument also instead of a class,\n# which would allow the subclass to set the"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\timport sys\n\timport time\n\tif sys.version_info[0] == 2:\n\t\tnamefunc = namefunc.encode\t\t\n\tif namefunc == basestring:\n\t\treturn unicode\n\telse:\n\t\treturn str(namefunc)\n\ndef set_default_time(tzmethod):\n\t\"\"\"Sets the default timezone used to call the tzmethod() function.\n\t\n\tThe default timezone also becomes the display timezone if one is not\n\tpassed in.\n\t\"\"\"\n\tglobal default_timezone, default_tzname"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tdef _decode(s, encoding):\n\t\treturn s.decode(encoding)\n\treturn _decode\n\n\n# python 2.6 can't detect unicode in unicode objects when unicode characters\n# are used in python 3\nif sys.version_info < (2, 6):\n\tdef tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tdef _dec"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\treturn string.unicode(namefunc())\n\t\n#TZINFO\t\t\t\ndef tzinfo(namefunc=tzname_in_python2):\n\t\"\"\"Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\treturn byte.encode(bytestr(namefunc))\n"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tnamefunc_py = re.match(\"(?P<name>.)(?P<name_py>.)[\\w+]*\", namefunc)\n\tif not namefunc_py:\n\t    namefunc_py = re.match(\"(?P<name>.)(?P<name_py>.)[\\w+]*\", namefunc)\n\t\t\t\t  \n\tif namefunc_py:\n\t    namefunc_py = namefunc_py.groupdict().keys()[0]\n\t    namefunc_py = namefunc_py.en"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tdef newfunc(t):\n\t\treturn namefunc(t.tzname())\n\treturn newfunc\n\ndef mytzname(*args): # Deprecated.\n\t\"\"\"\n\tChange from Python 2 and in Python 3 to bytes.\n\t\treturn a unicode string\n\t\"\"\"\n\t#return namefunc(tzname(*args))\n\treturn namefunc(dill.dumps(args))\n\n##\n# Main entry point for loading C and Python DLLs\n##\n\n\ndef load_dll(lib_name, dll_func): \n\t\"\"\"\n\tGiven"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif hasattr(namefunc, '__call__'):\tnamefunc = str\n\treturn namefunc\n\ndef datetime_compare(datetime1, date1, datetime2, date2):\n\t\"\"\"Compare two datetimes by seconds, dateTime fields.\n\n\tDoes not affect timezones.\n\t\"\"\"\n\tdate2_as_dt = time_to_date(datetime2)\n\tdate1_as_dt = time_to_date(date1)\n\n\tif date1_as_dt.hour == date2_as_dt."}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tnamefunc=namefunc.encode()\n\treturn lambda x: bytes('unicode('+repr(x)+'), encoding='utf-8')[:-len(namefunc)]\n\n\t\n\t\n\t#\n#\n#\n\ndef tzdesc():\n\t\"\"\"\n\tget the current time zone\n\t\"\"\"\n\tstz=raw_input('Now (dd-MM-yyyy hh:mm:ss)')\n\tif stz==\"\":\n\t\traise KeyboardInterrupt()\n\n\tdt=stz.split(' ')\n\tstrd="}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\treturn namefunc(encode(name, \"iso-8859-1\"))\ntzname_in_python2 = tzname_in_python2\n\n\ndef translate(fromtextunc, tounicodefunc, context=\"system\"):\n\t\"Translate string from the current locale using from_utf8/from_current. Accepts the \"\n\t\"same form (i.e. from_utf8) as fromtext, tounicodefunc() or to_utf8func() to \"\n\t\"return unicode.\"\n\n\tfromlib = fromtextunc(from"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\toutput=u''\n\tvalue = namefunc()\n\tfor character in value:\n\t\tif isspace(character):\n\t\t\toutput = output+character\n\t\telse:\n\t\t \toutput = output + character\n\treturn output\ndef as_unicode(obj, encoding=None):\n\tif encoding:\n\t\treturn obj.encode(encoding,'replace')\n\treturn obj.encode('utf-8')\n\ndef to_str(obj):\n\treturn obj.encode('ascii','ignore')\n\ndef load_python_file(name):"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tdef _change(self, namefunc):\n\t\t\"\"\" Changes the result from unicode output 'namefunc' into bytestrings\n\n\t\tThe result can be either bytes in Python 2, or unicode strings.\n\t\t\"\"\"\n\t\tif isinstance(namefunc, unicode):\n\t\t\tnamefunc = namefunc.encode()\n\t\treturn namefunc\n\t\n\treturn _change\n\ndef _default_tz_decode_py3k(namefunc, *args):\n\t\"\"\" Decodes a tz aware method's default args\n\nDecodes the named argument of the built-in"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tif name == 'get_versions':\n\t\tresponse('get_versions', default)\n\t\treturn\n\ttry:\n\t\tif get_version() is False:\n\t\t\tresponse('get_versions', 'false')\n\t\telse:\n\t\t\tif name == 'get_versions':\treturn\n\texcept:\n\t\tresponse('get_versions', 'false', sys.exc_info())\n\t\n\tresponse('get_versions', default)\n\nif __name__ == \"__main__\":\n\tdefault = 'default'\n\tresponse = {}\n\tget_versions()\n\tif"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tresult = dict ()\n\topener = urllib2.build_opener()\n\topener.addheaders = [('User-agent', 'Mozilla/5.0 (X11; Linux x86_64) GNU/Linux Powermeter 3.17.2')]\n\tfor filepath in os.listdir ('/usr/share/openpm'):\n\t\ttry:\n\t\t\tfullpath = os.path.join ('/usr/share/openpm', filepath)\n\t\t\tfilecontent = open (fullpath)\n\t\t\tif filecontent is not None:"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tversions = {}\n\ttry:\n\t\twith open('/usr/share/python-install/versions.txt') as versions_file:\n\t\t\tfor line in versions_file.readlines():\n\t\t\t\ttry:\n\t\t\t\t\t(name, version) = line.split('=')\n\t\t\t\t\tversions[name.strip()] = version.strip()\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\texcept:\n\t\tpass\n\treturn versions\n\n# ==================================================\n# Main\n# ==================================================\n\nversion = get_versions()\nprint \"install_dir:             %s\" % install_"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tdef get_data(self):\ttry:\n\t\tself.version = self._version\n\texcept AttributeError:\n\t\treturn {\"version\": \"0+unknown\", \"full_version\": \"\"}\n\tdef get_id(self):\treturn self.get_dict(\"id\")\n\t\t\t\nClassifierModel = model.classifier_model()\nClassifierModel.__name__ = \"ClassifierModel\"\nclassifier_model = ClassifierModel()\nclassifier_model.start()\nclassifier_model.stop()\n\nclassifier_model.stop()\n"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tfrom urlparse import urlparse, SplitResult #, split_scheme\n\tv = dict()\n\tversions = get_root().get_versions()\n\tfor url,ver in versions.items():\n\t\tq = urlparse(url)\n\t\tfor p in SplitResult(url):\t# get version from website\n\t\t\tv[ p.path[1: ].rsplit('/',1)[0] ] = ver # and set\n\treturn v\n\n\ndef get_repos(version=None):\n\t\"\"\"\n\tGet versions by version if passed a value.\tIf"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tmy_path = 'api/'\n\tapi_contents = my_path + 'local_version.xml'\n\tf = open(api_contents)\n\thtml = f.read().replace('\\r\\n', '')\n\tf.close()\n\tif re.search('version:[\\d.*]', html):\n\t\treturn\n\telse:\n\t\treturn default\n\nif __name__ == \"__main__\":\n\tf = get_local_version(api_version)\n\tprint f,\n"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\tout, err = pcall([\"git\", \"rev-parse\", \"HEAD\"], stdout = subprocess.PIPE)\n\t\tif err:\n\t\t\tout, err = _get_output(out), err\n\t\treturn out\n\texcept Exception:\n\t\treturn None\t\t\n\n\ndef _get_output(rc, out='', err=''):\n\t\"Read stdout; return None if '-A' not found or '-An' not found.\"\n\tout = out or pd.as_str(_get_output_stream(rc))."}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tf = open(get_default(),\"rb\")\n\treturn(cPickle.load(f))\n\n# EOF.\n"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tif os.path.isfile(module_location + version_file):\n\t\ttry:\n\t\t\twith open(get_file(module_location + version_file)) as vs, open(module_location + version_file + \".r1\") as vs1, open(module_location + version_file + \".r2\") as vs2:\n\t\t\t\tfor part in vs:\n\t\t\t\t\tvs_r1 = part.split()[:2]\n\t\t\t\t\tvs_r2 = part.split()[2:5]\n\t\t\t\t\ty = 0\n\t\t\t\t\twhile y<len(vs_"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tf = open(\"version.json\", \"r\")\n\tmyversion = f.read()\n\tf.close()\n\tver = myversion.split(\" \")[0]\n\tif ver is not None and len(ver.strip()) < 9 : \n\t\tver = ver + \"%s%s\" % (self.distribution.version_info[0], ver.strip())\n\treturn ver\n\ndef get_download_url(version):\t\n  url = \"http://{0}/page/search-list?q={1}\".format(version.server"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"aligned\"]:\n\t\treturn render(pieces, \"ALREADY_ALIGNED\")\n\ndef render(pieces, style):\n\tif pieces[\"raw\"]:\n\t\treturn pieces[\"rendered\"]\n\n\tstyle = style or knighdots, KNIGHDOTPLATFORM\n\tif pieces[\"embedded\"]:\n\t\tstyle = style / \"embedded\"\n\t\tstyle = knighdots+style, KNIGHDOTPLATFORM\n\n\tstyle = knighdot+style, KNIGHDOTPLATFORM\n\n\tif style == \"rendered\":\n\t\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t# If we are initialising part, then initialize the whole version tree\n\tif not self.part: return\n\t# Render the part\n\tif self.part == 1:\n\t\t# We have to do this in a nasty way to have the same effect but I want to be certain\n\t\t# not to change the order, so that with respect to changes, it becomes possible if the\n\t\t# original order is different\n\t\tif style in ('sparc','sparcv8'):\n\t\t\twidth, height = 32, 16\n\t\telse:\n\t\t\twidth, height = 32, 16"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif style is None:\n\t\tstyle = self.style \t\t\n\t\ttry:\n\t\t\t_ = self.trans_type\n\t\texcept AttributeError:\n\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\t\n\t\t\t\treturn ( 'TEMPLATE \"{style}\" {version} {versionmark} {type}\"'.format(style=style,version=self.version,versionmark=self.versionMark(),type=self.trans_type));\n\n\t\t\nelse:\n\traise NotImplementedError(\"This method doesn't support {style}.\".format(style=self."}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\n\t# All the rendering takes place in the same thread\n\t# so there's no need to lock anything\n\t\n\tif not version:\n\t\treturn \"all black\"\n\t\n\tdef getcolor(indexcolor):\n\t\treturn (255, 254, 239)\n\t\n\tdef getpil(indexcolor):\n\t\tpilname = \"images/%s.PNG\" % os.path.basename(version[:-3])\n\t\tpilfile = open(pilname, \"rb\")\n\t\tpil = cimlib.ImagePIL(p"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tf = pieces.from_locale_string(version).to_locale_string()\n\ts = {\n\t    'font_size': {\n\t\t    'BaseFont': '2x',\n\t\t    'normal': {\n\t\t\t    'Normal': {\n\t\t\t\t\t'family': '',\n\t\t\t\t\t'featureLevel': 0,\n\t\t\t\t\t#'uniqueFontName': 'helvetica',\n\t\t\t\t\t#'stretch': []\n\t\t\t\t\t#'lineGap': 5\n\t\t\t\t\t#'capHeight': 1\n\t\t\t    }\n\t\t    },\n\t\t   'san-bold':"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"] or \"delay\" in pieces:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces[\"dirty\"],\n\t\t\t\"dirty\": pieces.get(\"dirty\"),\n\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\"date\": pieces.get(\"date\")}\n\tif not isinstance(style, dict):\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces[\"dirty\"],\n\t\t\t\"dirty\": pieces.get(\"dirty\"),\n\t\t\t\"error\": \"no style given\",\n\t\t\t\"date\": pieces.get"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t# If the version is already rendered, just return the current style\n\tif pieces[\"style\"] == \"auto\" and not pieces.tex_info.installed:\n\t\treturn pieces\n\n\tif \"<default>\" in style_map:\n\t\tstyle = style_map.replace(\"<default>\", \"\" + pieces[\"style\"])\n\n\t# Render the version\n\trendered = []\n\tfor piece in pieces[\"pieces\"]:\n\t\tif piece.startswith(\"p/\") == True:\n\t\t\trendered.append(\".\")\n\t\t\tcontinue\n\t\trendered.append(piece)\n\n\t# Apply"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t# Get the desired width\n\tif pieces[\"plaintext\"]:\n\t\twidth = pieces[\"width\"]\n\telse:\n\t\t# Add padding\n\t\twidth = \"{\" + str(len(os.linesep)) + \"}\"\n\t\twidth += \"  \"\n\t\twidth += \" \".join((\"#\" + name + \"\\n\" for name in os.linesep.rsplit(\" \", 1)))\n\t\twidth += (\"\\n#\" * len(os.linesep))\n\n\t# Get the outer space\n\tif len(width) > 0:\n\t\twidth = os"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\n\tdef prefix(pieces):\n\t\tresult = ''\n\t\tfor i in range(len(pieces), 0, -1):\n\t\t\tresult += pieces[i - 1]\n\t\treturn result\n\t\n\tdef suffix(pieces):\n\t\treturn ''\n\t\n\tfont = pieces.font\n\twhile len(font) > 0:\n\t\t\n\t\tspace = ''\n\t\tfor i in range(0, len(font)):\n\t\t\tspace +=''\n\t\tfont = suffix(font)\n\t\t\n\t\tstart = ''\n\t\tsuffix = ''\n\t\tfor i in range"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif not pieces:\n\t\treturn {\"version\": \"0+unknown\", \"full-revisionid\": unknown, \"dirty\": False,\n\t\t\t\"ancestry\": None,\t\"date\": None}\n\tif pieces[\"tick_hash\"] == unknown:\n\t\tpieces[\"tick_hash\"] = hash(render_version_pieces(pieces, style.object_type, style.pieces_by_hash))\n\tif pieces[\"branch\"] == \"unknown\":\n\t\tpieces[\"branch\"] = render_version_pieces(pieces, style.object_type, style.pieces_by"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[\"raw\"][0] == '+':\n\t\treturn pieces[\"raw\"]\n\telse:\n\t\treturn \"+\".join(pieces)\n\ndef render_pep440(pieces):\n\t\"\"\"\n\tReturn a version string for the given list of pieces\n\tin the pep440 format.\n\n\tExceptions:\n\t1: no version specified\n\t2: add \"0+\"\n\t3: add \"0+w154+\"\n\n\t\"\"\"\n\tif pieces[\"dirty\"]:\n\t\treturn \"dirty\"\n\tif pieces[\"error\"]:\n\t\treturn"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[\"closest-parent\"]!= \"\" and \".\" not in pieces[\"closest-parent\"]:\n\t\treturn {\"@add\": \".\"}\t\n\treturn pieces\ndef render_html(pieces, context):\n\t\"\"\"Render the given tags as HTML.\"\"\"\n\n\toutput = []\n\tfor k, v in six.iteritems(pieces):\n\t\tif v)\n\t\t\toutput.append((escape(k), escape(str(v))))\n\n\treturn \"\\n\\n\".join(output)\n"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif not pieces:\n\t\tplus_dot = True\n\telif pieces[-1] == \".\":\n\t\tplus_dot = True\n\telif pieces[0] == \".\":\n\t\tplus_dot = False\n\telse:\n\t\treturn  \".\"\n\tif plus_dot:\n\t\treturn \"..\"\n\telse:\n\t\treturn \".\"\n\n\tdef remove_dot_dot_commas(pieces):\n\t\t\"\"\"\n\t\tRemove the last dot in the sequence, and add comma, to keep it last\n\t\t\"\"\"\n\t\tpieces = pieces[:-2"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\treturn \"+\" + pieces[\"closest-tag\"]\n\treturn \".\" + pieces[\"closest-tag\"]\n\n\t\t\t\ndef render(pieces, style):\n\t\"\"\"Render the given style on the given HTML template.\"\"\"\n\n\twrapper_class = \"%s-widget-wrapper\" % style\n\tapp = None\n\n\t# TODO: Better implementation\n\t# This is using the \"if you like\" approach, which works on both\n\t# normal HTML pages and if-style pages, it works with a custom\n\t#"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[1] == \"\" then pieces[1] = plus_or_dot( pieces[0] ) end\n\tif pieces[1] == \".\") then pieces[1] = \".\" + pieces[1] + \".\" end\n\treturn pieces\n\ndef parse(inString):\n\t# Pull the function out of re-parse.js but only for the first fragment:\n\t#   re-parse.js seems to have an internal logic bug, there can't be only\n\t#   one argument to a function.\n\tif inString[0]!= \"#\":\n"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tplus = pieces[0]\n\tif plus_or_dot(pieces) == \".\":\n\t\treturn \".\"\n\tret = \"[\" + plus.lstrip(\"+\") + plus_or_dot(pieces) + \"]\"\n\treturn ret[1:]\n\n\n# Rename the q to q+ or q*.\ndef rename_q(item):\treturn \"+=\" if item else \"=\"\nq = re.compile(\"(?:<|>)?q.*?\\Z\")\ndef split_q(item):\tqitem = item.split(\"+\")\n"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif \"+\" in pieces[0] and pieces[1] == \"\", \"#\" not in pieces[0] and pieces[1]!= \"#\":\n\t\treturn \"+\" + pieces[0]\n\telif \"+\" in pieces[0] and pieces[1]!= \"#\":\n\t\tpieces[1] = pieces[1] + \"+\"\n\t\treturn pieces[0]\n\treturn pieces[0]\n\ndef get_peers():\n\t\"\"\"Retrieve the list of peers we are connected to.\"\"\"\n\t#\n\t# we use localNode so we can use the same"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[\"closest-tag\"] == \"\" and not pieces[\"raw\"]:\n\t\tif pieces[\"distance\"] < dst-1:\n\t\t\treturn \"+\" + pieces[\"tag\"] + \"-\" + pieces[\"short\"]\n\t\telse:\n\t\t\treturn \"+\" + pieces[\"tag\"] + \"=\" + pieces[\"short\"]\n\telse:\n\t\tif pieces[\"distance\"] < dst-1:\n\t\t\treturn pieces(\"tag\", \"short\") + add_or_dot(pieces) + \"-\" + pieces[\"short\"]\n\t\telse:\n\t\t\treturn pieces(\"tag\") + add_or"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif not pieces[\"notfound\"]:\n\t\tif pieces[\"src\"] == \"+\":\n\t\t\treturn \".\"\n\t\telse:\n\t\t\treturn \"..\"\n\telse:\n\t\treturn \".h\"\t\t\n\n\ndef plus_or_dot(pieces): \t\"\"\"\n\treturn the joined value or the default value if none is present \n\t\"\"\"\n\tif \"+\" in pieces[\"pieces\"]: \n\t\treturn pieces[\"pieces\"].replace(\"+\",\".\") \n\telse: \n\t\treturn pieces[\"pieces\"]\t\ndef is_executable(fstype): \t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif not pieces:\n\t\treturn \"+\"\n\tif len(pieces) == 1:\n\t\treturn pieces[0] + \".m\"\n\treturn pieces[0] + \".\" + plus_or_dot(pieces[1:])\n"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif type(commands) in [types.StringType, types.ListType]:\n\t\tcommands = [commands]\n\tfor cmd in commands:\n\t\tif type(cmd) is types.DictType:\n\t\t\tif env:\n\t\t\t\tlst = env.keys(cmd)\n\t\t\telse:\n\t\t\t\tlst = []\n\t\telse:\n\t\t\tlst = []\n\t\tlst.extend(cmd.keys())\n\t\ttry:\n\t\t\tlst.append(cmd[cmd])\n\t\texcept IndexError:\n\t\t\traise IndexError, 'The given command line"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmds = args\n\targs = [_to_args(x, cwd=cwd) for x in args]\n\t\n\tif env is None: env = {}\n\tif 'env' in commands:\n\t\tenv_env = cmds['env']\n\t\n\t(rc, stdout, stderr, err) = _execute_command(commands, args, cwd=cwd)\n\tif hide_stderr:\n\t\tstderr = None\n\tsys.stdout.write('\\r>> ')\n\tsys.stder"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcommands = (cmd.strip().split(\" \") for cmd in commands)\n\t\n\tif not isinstance(args, (list, tuple)):\n\t\targs = [args]\n\tif not isinstance(cwd, (str, unicode)):\n\t\tcwd = cwd or ''\n\tif not isinstance(verbose, int):\n\t\tverbose = verbose\n\tif not isinstance(env, (dict, unicode, str)):\n\t\ttry:\n\t\t\tenv = unicode(env or \"\").encode(\"utf-8\")\n\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif cwd is None:\n\t\tcwd = commands[0]\n\targs = self.split_args(args)\n\ttry:\n\t\tprocesses = self.cpus - self.cpus_map[cwd]\n\t\tif verbose:\n\t\t\tprint(\"Running command '%s' at '%s'\" % (commands[0], cwd))\n\t\t\tprint(\"  args: %s\" % args )\n\t\treturn Popen(commands, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmd = Cmd()\n\tif verbose:\n\t\tcmd.log_method_name = cmd.INFO\n\tf = None\n\tcommands = commands[:len(commands)-1]\t# remove a trailing command\n\ttry:\t\t# FIXME: do it more efficiently\n\t\tf = os.system(\"%s %s %s\" % (commands[-1], cmd.method_name, args))\n\texcept Exception, e:\n\t\tf = 1\t\t# FIXME: remove this\n\t\tif hide_stderr:\n\t\t\tsys.stder"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif cwd is not None:\n\t\tos.chdir(cwd)\n\n\t\tif verbose and env.has_key('PIP_COMMAND_VERBOSE') and env['PIP_COMMAND_VERBOSE']:\n\t\t\tprint 'Running pip command', \"%s with command line \"%s\"' % (commands,''.join(args))\n\t\n\t\trunPipCommand('-vvvv', '--verbose', '-vvvv',\n\t\t             '--no-color', '-vvvv', 'python',\n\t\t             '%s"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif not isinstance(commands, list):\tcommands = [commands]\n\tfor command in commands:\n\t\tif not callable(command):\t\tcontinue\n\t\tif not is_str(command):\t\t\tcontinue\n\t\tif verbose:\t\t\t\t\t\t\tcommand_output = \"\\n[-] \"\n\t\tif is_str(args):\t\t\t\t\t\tcommand_output = \" \".join(args)\n\t\telif is_str(args[0]):\t\t\t\tcommand_output = args\n\t\telse:\t\t\t\t\t\t\t\tcommand_output = \"unknown\"\n\t\tcommand_output = command_output.upper()"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif args == '': args='all'\n\targs=utils.normalize_values(args)\n\t\n\t# we get this information from the first command\n\t# we can just assume it is a command-e\n\tif not commands:\n\t\tcommands=args\n\n\t# execute the commands\n\treturn self._commands[commands](args, cwd, verbose, hide_stderr, env)\n\n\t\nclass BaseCommand:\n\t\"\"\"\n\tObject capable of executing a command. Will be instantiated and registered to the parent\n\tfor use in the root"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif cwd is None:\n\t\tcwd = sys.real_cwd()\n\t\n\tif verbose:\n\t\tprint((' '.join(commands)))\n\tout, err = exec_cmds(commands, args, hide_stderr)\n\tif hide_stderr and err is not None:\n\t\terr = out + err\n\tif env is not None:\n\t\t# XXX what happens here?\n\t\tfor k in env:\n\t\t\tos.environ[k] = args[k]\n\n\tif cwd!= None:\n\t\tos"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\targs, command_args, opts = parser.parse_known_args(args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\tcmd = subprocess.Popen(command_args, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n\t(stdout, stderr) = cmd.communicate()\n\treturn(stdout, stderr)\n\n\n@cmdarg(int, nargs='?')\ndef pw(p"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tcfg = cfg.Config()\n\n\t# Get/create options functions\n\tconfig_getopts = getopt.getopt_long\n\tconfig_getopt = getopts\n\tconfig_getconfig = getconf.getconfig\n\t\n\tcfg.OptionParser = optionparser\n\tcfg.option_getopts = config_getopts\n\tcfg.option_getconfig = config_getconfig\n\tcfg.option_parse = 'help'\n\n\tconfig_getopt(['logger='])\n\t\n\t# Define options of our own\n\tlog"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tfrom yorpc.config.plugins import base\n\treturn base.create_config(config=base.conf)\n\n\ndef get_version():\n\t\"\"\" Get the Versioneer version.\"\"\"\n\tconfig = get_config()\n\treturn config['version']\n\ndef get_parsestep():\n\t\"\"\" Get the parsedraftstep.\"\"\"\n\tconfig = get_config()\n\treturn config['parsestep']\n\ndef get_startdate():\n\t\"\"\" Get the parsedraftstartdate.\"\"\"\n\tconfig = get_config()\n\treturn config['start"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tmyvre =  Versioneer()\n\tmyvre.load_version( myvre.V_DEFAULT )\n\treturn myvre.config\n\n\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()\n\ndef get_exe_dir():\t\"\"\"\n\tGet the folder where the egg will end up, including the scripts\n\tand baksms subdirectories.\n\t\"\"\"\n\treturn str(os.environ[\"PATH\"]+os.environ[\"EXAMPLES_PATH\"])\n\n\ndef get_version(egg_path):\n\t\"\"\"\n\tTry to call versioneer.Vetting to determine the version number.\n\tAfter the call, the function returns the latest veer version.\n\t\"\"\"\n\ttry:\n\t\timport vetting\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tconfig = ConfigParser.ConfigParser()\n\n\tdef __getattr__(self, name):\n\t\tsetattr(self, name, lambda: None)\n\t\ttry:\n\t\t\treturn self.__dict__[name]\n\t\texcept KeyError:\n\t\t\traise AttributeError(name)\n\n\tdef read(self):\n\t\t\"\"\"\n\t\tRead in the VERSION file.\t\"\"\"\n\t\ttry:\n\t\t\tfp = open(VERSION_FILENAME)\n\t\t\t(pid, appver) = get_pid_appver(fp)\n\t\t\tconfig.readfp(fp)\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tc = vstruct.VStruct(solver.VersioneerConfig.vstruct_solver_config)\n\tc.vstruct_p = vstruct\n\tc.config_p = vstruct.VArray(vstring)\n\tc.last_status = solver.Status('running')\n\treturn c\n\n\ndef run(config):\t\n\t\n\tcc = solver.CalculateChainCount()\n\tsolver.CreateEvent(config)\n\tsolver.Start(tuple())\n\tsolver.SetSolverStatus(config)\n\ts"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\n\treturn versioneer.make_config()\n\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tconfig = vconfig.Vconfig(\n\t\t# main config options\n\t\tcfg = \"\"\"\n\t\t%s\n\t\t%s\n\t\t%s\n\t\t\"\"\" % (\n\t\t\tvconfig.find_file('peanutbrushing_vendor'),\n\t\t\tvconfig.find_file('peanutbrushing_homepage'),\n\t\t\tvconfig.find_file('peanutbrushing_download'),\n\t\t),\n\t\tversion=__version__\n\t)\n\n\tif __revision__:\n\t\tconfig.replace('VERSION', '%s-%s' % (__revision__"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"%s [options]\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n\t\t\t\t default=False, help=\"verbose output\")\n\tparser.add_argument(\"-q\", \"--quiet\", action=\"store_true\",\n\t\t\t\t default=False, help=\"quiet output\")\n\treturn parser.parse_args()\n\t\ndef main():\n\tglobal opts, run_command\n\topts = get_config().__dict__\n\trun_command = True\n\t"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(vcs_handler_method):\n\t\tvcs_handler_method.vcs_handler = decorator\t\t\t\t# decorator\t\t\t\t\t#\n\t\treturn vcs_handler_method\n\treturn decorator\t\t\t\t\t\t\t\t\t\t\t\t  #\t(return the decorator\n\n#===================================================================================================\n#\tRegister all global vcs commands.  This is more convenient in development.\n#\tCalled by cProfile and cVerbose.\ndef register_all():\n\t\"\"\"Register everything (all commands, VCS-specific modules, etc)\"\"\"\n\tfor (func, func_name"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(func):\n\t\tvcs_methods[method] = func\n\treturn decorator\n\t\ndef is_subproject(name):\n\t\"\"\"Is the named subproject directory?\t\"\"\"\n\treturn name.find(\"sub\") == 0\n\t\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tvcs.vcs.register(method, method_path=\"vcs\")\n\n\t\treturn method"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t    vcs.register(method)\n\n\tdef decorated_function_classification(func):\n\t    \"\"\"\n\t    The functions that do and are to be considered as handler methods.\n\t    \"\"\"\n\t    vcs.register(func)\n\t\n\treturn decorated_function_classification\n\n\ndef get_urls():\n    \"\"\"\n    Get all links from a web page.\n    \"\"\"\n\treturn \"\"\"\\\n    <a href=\"/user/list\">Home</a>\n    <a href=\"/user/create\">Add another user</a>\n    <a"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef real_method(command, options):\n\t\tdef method_wrapper(*args, **kwargs):\t\n\t\t\tdef method(*args, **kwargs):\t\n\t\t\t\treturn getattr(command, method)(*args, **kwargs)\t\n\t\t\treturn method_wrapper\n\t\treturn method_wrapper\n\tvcs.register_vcs_handler(vcs, method)  # decorator\t\"\"\"registry\"\"\"\n\treturn real_method(vcs, method)  \t\t\n\ndef call_vcs(cmd, cwd=None, options=None, args=[], **"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\n\tdef handle_args(args, vcs_method):\t  #args_obj is  methodobj\t\t\"\"\"\n\t\tprint(args[0], \"args object:\", args[1])\t\n\treturn handle_args\n\ndef setup(app):   # decorator\t\"\"\"Initializes the application\"\"\"\t\t\n\tapp.add_config_value(\"enable_vcs\", True, \"Boolean\")\n\tapp.add_config_value(\"create\", True, \"Boolean\")\n\tapp.add_config_value(\"checkout_url\", False, \"Boolean"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(func):\n\t\tif func.__name__ not in ('on', 'off'):\n\t\t\traise ValueError(\"VCS handler must have name 'on' or 'off'\")\n\t\tvcs_handlers[vcs] = func\n\t\treturn func\n\treturn decorator\n\t\n\t\n@register_vcs_handler('branch', 'git.commit_message')\n@register_vcs_handler('branch', 'git.checkout')\n@register_vcs_handler('branch', 'git.update')\ndef git_update"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(wrapped_func):\n\t\t@wraps(wrapped_func)\n\t\tdef wrapped_method(*args, **kwargs):\n\t\t\tif method not in [v for v in vcs.vcs_methods if vc.active]:\n\t\t\t\treturn wrapped_method(*args, **kwargs)\n\t\t\treturn NotImplemented\n\t\treturn wrapped_method\n\treturn decorator\n\nVCS = Vcs()"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(func):\t# function\t\"\"\"\n\t\treturn vcs_handle(func, method)\n\treturn decorator\n\nregister_vcs_handler(vim.RepositoryManager, 'configure')\n\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\thandler_map[vcs] = method\t\t\t\t\t\t\t\t\t# add_to_dict(vcs)\n\treturn decorator(method, method)\n\n\ndef add_to_vcs_handler(vcs, method):\n\t\"\"\"Add a new handler or update the default handler if another\n\tone has been set (no-op if the default handler was already\n\tset).\t\t\t\"\"\"\n\tif vcs in handler_map:\n\t\treturn handler_map[vcs]\t# if handler_map contains a vcs, return it directly\n\treturn handler_map[vcs] |"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tlog('**** validator.validate_hierarchy')\t\n\tif self.storage_root_hash is None:\n\t\traise Exception('Error: Storage root has not yet been set to the validation object.')\n\tnum_objects = 0\n\tgood_objects = 0\n\ttry:\n\t\tdummy_obj_1 = self.storage_root_hash[0:8]\n\t\tbins_before = []\n\t\tbins_after = []\n\n\t\tlog('Checking %s for validness...', dummy_obj_1)\n\t\tfor (dirpath,"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tself.validate_hierarchy_count(\"hierarchy\")\n\tnum_objects = 0\n\tgood_objects = []\n\tregex = self.validate_hierarchy_name\n\tif validate_objects:\n\t\tfor root in self.validate_hierarchy.roots:\n\t\t\tnum_obj = 0\n\t\t\tregex = self.validate_hierarchy_objects\n\t\t\tif check_digests:\n\t\t\t\tnum_obj = self.check_digests_count(root)\n\t\t\t\tgood_objects.append(num_obj)\n\t\t\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tif isinstance(self.hierarchy, dict):\n\t\tself.hierarchy = [ self.hierarchy ] * 0\n\t\n\tself.object_count = 0\n\tfor r in self.hierarchy:\n\t\tif isinstance(r, list) and len(r) > 0:\n\t\t\tfor obj in r:\n\t\t\t\tif not obj in self.hierarchy:\n\t\t\t\t\traise errors.HierarchyError(\"Hierarchy must contain valid objects\")\n\t\t\t\t\n\t\t\t\tobj_id = obj[0]\n\t\t\t\tobj_name = obj\n\t\telse"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\thierarchy, depth, num_objects = self.validate(False, False, False)\n\tif validate_objects:\n\t\tself.validate_objects(hierarchy, depth, num_objects, show_warnings)\t\n\n\tself.check_digests(check_digests)\n\treturn num_objects\n\n\ndef validate_objects(hierarchy, depth, num_objects, show_warnings):\n\tfor level in hierarchy:\n\t\troot = level[0]\t\n\t\tsorted_level = sorted(level)\t\n\t\t\n\t\tnew"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tif show_warnings:\n\t\tmsg = 'Validation failed. Check the following warning messages:'\n\t\tshowMessage(msg)\n\n\tnum_objects, obj_files = 0, 0\n\tvalidate_obj_tree = None\n\tfor obj, root_subtree in self.valid_hierarchy.iteritems():\n\t\tif root_subtree.is_container:\n\t\t\tnum_objects += 1\n\t\t\tvalid_obj = False\n\t\t\tif not obj.container_check('exists'):\n\t\t\t\tobj.container_create(obj.get_extended_property"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tif'storage_root' not in self:\n\t\traise ValueError('Storage root cannot be None')\n\tif not self['storage_root'].value():\n\t\traise ValueError('Storage root value cannot be None')\n\tif not self.hierarchy:\n\t\traise ValueError('No storage hierarchy found')\n\tif not self['hierarchy'].value():\n\t\traise ValueError('Storage hierarchy value cannot be empty')\n\n\tgood_objects, num_objects = 0, 0\n\tobj = 0\n\tfor objName in self['object_list']:\n\t\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\ttry:\n\t\tcheck_repos = self.check_repos\n\t\trepos_list = self.get_repos()\n\t \tnum_objects = check_repos(repos_list, validate_objects, allow_bad_checks=True, check_digests=check_digests, show_warnings=show_warnings)\n\texcept check_diges:\n\t\tnum_objects = check_repos(repos_list, validate_objects, allow_bad_checks=True, check_digests=check_digests, show_warn"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t# Get check status\n\tif check_digests:\n\t\tself._validate_digests(check_objects=check_objects)\n\tcheck_objects = False\t\n\tif validate_objects:\n\t\tself._validate_objects(check_objects=check_objects)\n\t\tcheck_objects = True\n\t\t\n\tobjlist = []\n\t# Check for items that do not exist within the check status\n\tif'substitutions' in self.dict['object_status']:\n\t\tfor v in self.dict['object_status']['substit"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tparent = False\n\tnum_objects = 0\n\tbad_objects, bad_objects_num = 0, 0\n\tfor obj in validate_objects(self.objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\tif obj.is_stored_root():\n\t\t\tif obj.parent is not None:\n\t\t\t\tprint \"*** Warning ***\"\n\t\t\t\tself.bad_objects += 1\n\t\t\t\tparent = True\n\t\t\tparent = False\n\t\t\ttry:\n\t\t\t\tretval = hierarchy_validator.validate_root_object("}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tpass\n\n\t\n\t\n"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tOCFLStorageRoot.initialize(self)\n\n\tdef init_config(self, configFile, hostname, username, password):\n\t\tself.configDir = configFile\n\t\tself.hostname = hostname\n\t\tself.username  = username\n\t\tself.password  = password\n\n"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = None\n\n\tself.parent = None\n\n\tself.storage = dict()\n\tself.configFileName = \"/tmp/ocfl.ini\"\n\tself.configFile = open(self.configFileName, \"w\")\n\n\tself._ocflInitParams()\n\nclass OCFLStorage:\n\tdef __init__(self, path):\n\t\tself.path = path\n\t\tself.root = self\n\n\t\tself.storage = dict()\n\t\tself._create_files()\n\t\n\tdef create(self):\n\t\t\n\t\tfor configFile"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t# Call the superclass method first to initialize the OCFL's internal structures\n\tself.super_init(self)\n\t\t\t\n\t# Construct the root, then create the storage root\n\tself.root = create_storage_root(self.config.root_dir)\n\n\t# Initialize the storage root's index cache\n\tself.disk_stats()\n\t\t\t\t\t\t\n\n\t# Add a default group of users\n\tself.storage_dir = os.path.join(self.root, 'data')\n\tself.add_default_group()\n\n\t# Add some users\n\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.storG = OCSGStorageRoot()\n\n\tself.storG.initStoragePath(\"OCSGFSTest\")\n\tself.storG.setStoragePath(\"OCSGFSTest\")\n"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.store = self.createNewFolder('OCFL')\n\tself[\"OCFLName\"].text = self.store,\n\tself.openNew()\n\tself.saved = False\n\t\n\t# self.openNew()\n\n\t# get the values for the settings of the server\n\tself.listo = [x.value for x in self.config.listo.keys()]\n\tself.listo2 = [x.value for x in self.config.listo2.keys()]\n\t\n\tindex = 0\n\tself.serverIndex"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.skeleton_ = None\n\tself.root = ocb.graphics.OCFLStorage(self.root_id)\n\tocb.graphics.OCFLStorage(self.root_id).update_root(self.root)\n\n\treturn self\n\n\ndef get_storage_id(id):\n\t'''\n\tGet the storage id for an OCFL.\n\t'''\n\tif not is_numeric(id):\n\t\treturn None\n\tif ocb.graphics.is_id(id):\n\t\treturn str(id)\n\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\n\tself.dirName = \"\"\t# Name of the storage directory\n\tself.rootDirPath = \"\"\t# Directory where we will store storage\n\t\n\tfor fname in [f for f in sys.argv if f.endswith(\".dat\")]:\n\t\t# Extract file name\n\t\tself.dirName = f.replace(\".\", \"\")\n\t\t#print fname\n\t\ttmpFileName = self.dirName+fname\t\t# Copy file in temp name\n\t\tf = open(tmpFileName, \"wb\");\n\t\tprint(\"Copying \" + f"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tocf_root = open(PREF+'/'+'ocfl', 'rb').read()\n\tself.root = OCFL(ocf_root)\n\tself.root.setpath('/')\n\t\t\n\tself.root.newf = lambda x: x\n\tself.root.delf = lambda x: x\n\tself.root.loadk = self.root.loadf\n\tself.root.loadkf = self.root.loadf\n\tself.rebuild_root()\n\t\n\t\n\tdef rebuild_root(self):"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\ttry :\n\t\tself._storage_root = StorageRoot(self._config_root)\t\n\t\tself._storage_root.initialize()\n\texcept :\n\t\tself._storage_root = None\n\t\traise\n\t\t\n\tdef update(self):\n\t\t\"\"\"\n\t\tUpdate storage object.\t\n\t\t\"\"\"\n\t\t# update the storage info of this storage root object\n\t\tif self._storage_root!= None:\n\t\t\tself._storage_root.update()\n\t\t\n\tdef getAllStorageInformation(self):\n\t\t\"\"\"\n\t\tReturn all the storage information of a storage root"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tsuper(OCFLStorageBase,self).initialize()\t\n\n\tself.root = ocfroot.Ocfroot(self.context, self.identity)\n\n\ndef initialize(context):\n\t\"\"\"\n\tConstructs a OCFL storage root for the given context.\n\t\"\"\"\n\tnewc = context.Ocfroot(context.Ocfcontext(), context.Ocfidentity())\n\tnewc.context = context\n\tfor newroot in context.Ocfroots():\n\t\tnewc.add_root(str(newroot))\t\n\treturn newc\n"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif not len(version):\n\t\tfrom django.core.urlresolvers import reverse\n\t\treturn reverse('manage:next_version')\n\treturn int(version[:-1])\n\n\n# The following should be defined for each system:\nmanage_base = manage.base_url(\"manage\")\nmanage_user_base = manage.user_url(\"manage_user\")\nmanage_admin_base = manage.admin_url(\"manage_admin\")\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tnext_idv = None\n\n\t# Try version 0.\n\tnext_idv = version.replace(\"0.\", \"\", 1)\n\tif next_idv and version and version.endswith(\".0\") and next_idv not in version:\n\t\tnext_idv = next_idv[:-2]\n\n\t\t# Re-fetch old version if it was zero-prefixed (but possibly not,\n\t\t# in which case will be returned the same).\n\t\tif version.startswith(\"0.\") and next_idv:\n\t\t\tnext_id"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\n\tif version is None:\n\t\traise Exception, \"Must have matching version.\"\n\t\t\n\tidx = 0\n\t#if is_zeropr():\n\t#\tidx = 1\n\t#else:\n\twhile version[idx]!= ord('\\0'):\n\t\tidx += 1\n\t\n\treturn version[0:idx]\n\ndef is_zeropr():\n\t\"\"\"\n\tDecompress prefix of a string that has the zero-prefixed pattern, \n\tif necessary.\n\t\n\tReturns False if the string does"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version[:3] == \"v\" and len(version) >= 4:\n\t\treturn version.split(\":\",1)[1].lstrip(\"v\")\n\telse:\n\t\treturn version\n\t\n\nif __name__ == \"__main__\":\n# If executed from the command line:\n\tif len(sys.argv) == 2:\n\t\t# Check if '--version' keyword is used (note a trailing zero)\n\t\tif sys.argv[1] and not sys.argv[1].startswith(\"--version\"): \t#"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version[0] is not 0:\treturn None # version needs to be zero-prefixed\n\t\t\t\t\t\t\t\n\tif version[1] is not 0:\t\t# just a prefix...\n\t\tif version[0] is 0:\n\t\t\treturn 0\n\t\treturn version[0] + 1\n\t\t\n\telse:\t\t\t\t\t\t# one is the whole version...\n\t\tif version[1] is not 0:\n\t\t\treturn 1\n\t\treturn version[1]\n\n\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif isinstance(version, bytes):\n\t\treturn '{}{}'.format(version, _UNRESERVED)\n\tif isinstance(version, unicode):\n\t\treturn str(version)\n\tif version == unichr(0) and not isinstance(version, str):\n\t\treturn str(version[0])\n\n\traise ValueError('Cannot handle version: \"{}\"'.format(repr(version)))"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\treturn next(v for v in Vers(version) if not v.startswith('-%0')) + '_' + version\n\n# version number of the current system\nCurrent ='stable'\n\ndef CurrentVersion():\n\t\"\"\"\n\tThe version number the current system is based on:'stable';\n\n\tNote that this differs from the Version method.\t\"\"\"\n\treturn Current\n\ndef get_installed_version():\n\t\"\"\"\n\tGet the most recent version of the Python Python library.\tIf\n\tthere's an installed version, assume its stable (including beta)"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version[0] == '0' or version[0] == '-' or version[0] in set(['0:','-']): \treturn \"NEXT_VERSION\"\n\treturn \"PREFIXED_VERSION\"\n\n####################################################################\n#\n#    The version number is used to identify multiple \n#    entries in a version control system.  It may be necessary\n#    to add some internal numbers in future versions.\n#\n####################################################################\ndef format_version_info(verinfo):\t\t\n\ts=\"%s-\"\n\tif ver"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tpattern = pattern_match\n\tif len(version) == 0 or version == \"0\":\n\t\tpattern = pattern_lookup_version0\n\t\tversion = None\n\telif version[-2:] == \"0\":\"0.\":\n\t\tpattern = pattern_lookup_version0\n\t\t# if the match to next version succeeds, call next_version\n\t\tfirst, last = 0, int(version.split('.')[0])\n\t\twhile first <= last:\n\t\t\ttry:\n\t\t\t\tnext_version = pattern[first](version)\n\t\t\t\tbreak\n\t\t\texcept NotDoneException"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tversion = '_'.join( (str(v) for v in version) )\n\tif (len(version)>0 and len(version)<= 2) or (version.upper()[0] == \"Z\") or (version.upper()[0] == \"_\"):\n\t\traise UiError(\"Invalid version of %s. Must be 0:0 or 0:1\\n\" % (version,))\n\tif version == \"0\": # Z is not supported\n\t\treturn version\n\telse:\n\t\tid = re.findall(r\"\\d\","}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor v in version_dirs:\n\t\ttry:\n\t\t\tself.inventory.verify_inventory(v)\n\t\texcept ValueError:\n\t\t\traise ContentLibError('Failed to initialize inventory database.  Invalid version directory.')\n\n\tfor v in version_dirs:\n\t\ttry:\n\t\t\tself.inventory.verify_inventory(v)\n\t\texcept ContentLibError:\n\t\t\t# If the same inventory has been already created, don't change it\n\t\t\tif self.inventory.inventory is not None:\n\t\t\t\traise\n\t\t\telse:\n\t\t\t\tbreak\n\t\telse:\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor dir in version_dirs:\n\t\tdir_list = os.listdir(dir)\n\t\ttry:\n\t\t\n\t\t\tfor ver in version_dirs:\n\t\t\t\t\n\t\t\t\tself.version_inventory_file = os.path.join(id('idc','version').replace('/', '_'), ver + '.npm')\n\t\t\t\tif os.path.isfile(self.version_inventory_file) and os.path.getsize(self.version_inventory_file)!= 0:\n\t\t\t\t\tself.valid = True\n\t\t\t\t\tbreak\n\t\t\t\t\n\t\t\tif"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\tfor vdir in version_dirs:\n\t\tfor vversion in (1,2,3,4,5,6,7):\n\t\t\tpv_dir = version_dirs[vdir]\n\t\t\tfor in_v, in_vldesc in v_and_verldescs.items():\n\t\t\t\tif vldir.startswith(pv_dir):\n\t\t\t\t\t\n\t\t\t\t\twith open(in_vldirectory + \"/\" + vver_desc_ext, 'rb') as fin:\n\t\t\t\t\t\tversion_verification = fin.readlines"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tself.write(\"/etc/yum.repos.d/yum-inventories\")\n\tfor d in version_dirs:\n\t\ttry:\n\t\t\tself.check_version(d)\n\t\texcept VersionMismatchError:\n\t\t\tpass\n\ndef valid_file(path):\n\t\"\"\"Returns true if the file at ``path`` exists.\"\"\"\n\treturn os.path.isfile(path)\n\ndef not_valid_file(path):\n\t\"\"\"Returns true if the file at ``path`` isn't a valid file.\"\"\"\n\treturn not"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tvalid_versions=[]\n\tvalid_versions_names=['0.0','1.0','2.0','3.0']\n\tfor version in version_dirs:\n\t\tv,vn=self.extract_version(version)\n\t\tif v in self.versions_cache:\n\t\t\tvalid_versions.append(v)\n\t\t\tvalid_versions_names.append(vn)\n\t\telse:\n\t\t\tif v in valid_versions_names:\n\t\t\t\tvalid_versions.append(v)\n\t\t\t\tvalid_versions_names.remove(v)\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t# For each inventory dir and version, get the newest item for\n\t# that version that is at or earlier than the root\n\tfor i in range(1, len(version_dirs)):\n\t\tnewest_item = -1\n\t\tfor j in range(0, i + 1):\n\t\t\tlatest_item = [v for v in version_dirs[j]\n\t\t\t\t\t\t\tif latest(v) >= newest_item][0]\n\t\t\tif latest_item!= -1: newest_item = latest_item\n\t\tassert(latest(latest_item) > -1)"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\t# keep track of the digests that were specified to trigger a validation\n\tvalid_versions = {}\n\n\t# Build the set of directories that are affected.\n\t# TODO: this is not good as we can't get the information\n\t# about the digests in the root inventory yet.\n\t# Note that this could get very slow if several inventories have\n\t# different sizes.\n\tinventories_dirs = set(dir for dir in version_dirs)\n\t# Iterate thru the version dirs, and check each of them for the\n\t# given"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\"\"\"\n\tReturns the digests of the contents of every content directory.\n\t\"\"\"\n\treturn self.__validate_digests(dirs=version_dirs)\n\n#-------------------------------------------------------------------------------\nimport os\n\ndef validate_versions_for_validation(versions):\n\t\"\"\"\n\tVersions should have content digest information in version 2/1/2/3s etc.\n\n\t(The argument is assumed to be strings.)\n\t\"\"\"\n\tvalidated = [v for v in versions if isinstance(v, str) and v.startswith('version/')]\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor vdir in version_dirs:\n\t\troot = InventoryContentDigest(vdir=vdir)\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tself.versions = {}\n\tdir = version_dirs[0]\n\tif not dir == '':\n\t\tfor vd in version_dirs[1:]:\n\t\t\t# only include items that still exist on disk\n\t\t\tif os.path.exists(os.path.join(dir, vd)):\n\t\t\t\tself.versions[(vvd,vvv)] = os.path.abspath(os.path.join(dir, vd))\n\ndef parse_content_digest(vdf, log=logging):\n\tc = {}\n"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\treturn type_from_path(path)\n\n######################################\n##  Create an OCFL object\n##\ndef create_ocfl_obj(path):\n\t\"Return an object from OCFL file at the given path.  Doesn't actually do anything!\"\n\treturn ocflobj(path, True, None)\n\n######################################\n##  Create an OCFL inventory object\n##\ndef create_ocfl_invobj(path):\n\t\"Return an invent object from OCFL files at the given path.\"\n\treturn ocflobj(path, False,"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif is_in_archive(path):\n\t\treturn \"root\"\n\tif is_in_contents(path):\n\t\treturn \"object\"\n\tif len(path) == 1 and is_filelike(path):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"root\"\n\n# =============================================================================\ndef find_path_type_and_file_type():\n\t\"\"\"\n\tReturns a tuple (path_type, file_type) where path_type is the type\n\tof thing at the given path and file_type is the type of thing at the"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tpath_type = 'root'\n\tif path.startswith(('0=.*', '0=', '0=*.')):\n\t\tpath_type = 'object'\n\telif path.startswith(('0=', '*.')):\n\t\tpath_type = 'file'\n\tif path_type == 'root':\n\t\tpath_type = 'object'\n\treturn path_type\n\ndef _test():\n\tfrom numpy.testing import TestCase # is used by pytest because it uses it\n\tmy_obj = OCFL"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t#FIXME! do this check for directories?\n\treturn path.split('=')[-1].upper()\n\ndef find_path(dir_path, dir_type, path_type='root'):\t\"\"\"\n\tReturns a tuple (path, result) where result will be None if path\n\tdoes not exist or is a type not compatible with dir_type.\tIf the\n\texact paths do exist, result will be the path with a correct result;\n\totherwise, it will be None (but will return an empty tuple).\tIf\n\tdir_type returns"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\ti = re.search(re.escape('0=*'), path)\n\tif i:\n\t\to = path.split(\"/\")\n\t\tfor s in o:\n\t\t\tif s[0] == '0': s = str(s)\n\t\t\telse: break\n\t\tif s[-1] == '*':\n\t\t\treturn ('file', 'file')\n\t\telif i.start() == 0:\n\t\t\treturn ('root', 'root')\n\t\telif s[-1] == '*':\n\t\t\treturn ('object', 'object')\n\t\telse:\n"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif path.find(\".\") > 0:\n\t\treturn 'file'\n\treturn \"root\"\n\t\ndef parse_path(path):\n\t\"\"\"\n\tParse a path into root, name and object parts.\t\"\"\"\t    \n\timport glob\n\tif glob.fnmatchcase(\"*.{}\", path): \n\t\tpath = path.replace(\"*.\", \"\")\t\n\treturn path\n\nclass OCFLDirectory(object):\n\t\"\"\" OCFL directory object, handles the ocfl files in this directory.\n\n\tThere are certain exceptions (\"*\" are objects)"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n\tif path.startswith('/') or path.startswith('\\\\'):\n\t\tpath = path.replace('\\\\', '/')\n\t\tif path.endswith('/'):\n\t\t\tpath += '/'\n\t\t\tpath += '/**'\n\t\telse:\n\t\t\tpath += '/'\n\t\tpath = path.replace('*', '//')\n\t\tif path[0] == ':':\n\t\t\tif len(path) > 1:\n\t\t\t\treturn path.split(':', 1)[1]\n\t\treturn path\n\n\ttry:\n\t\tfile = open"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tfor dt in self.D:\n\t\t\tif dt == '*':\n\t\t\t\treturn 'root'\n\t\t\tif '0' < dt <= '1':\n\t\t\t\tbreak\n\t\tif dt == '*' and not find_file(path):\n\t\t\t\ttry:\n\t\t\t\t\tdt = get_directory_type(path).lower()\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\t\t\tif dt == 'file':\n\t\t\t\t\treturn dt\n\t\t\tif dt == 'object':\n\t\t\t\t\treturn dt\n\nroot_dir = {}\nobject_dir = {}\n\nfound = []\n"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tpath=\"\"\n\tif path==\"\":\n\t\t# check for valid path\n\t\tif ocfl_is_file(strpath=strpath):\n\t\t\t# if a valid path exists, return as \"file\"\n\t\t\treturn \"file\"\n\t\telif ocfl_is_inventory(strpath=strpath):\n\t\t\t# the inventory root is the first thing after /, but we could end up\n\t\t\t# in the second directory, with /../some/inventory\n\t\t\tif strpath.endswith(\"/\"):\n\t\t\t\tpath=[strpath, \"./\"]\t# this"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\treturn (path.find('=') or 'root').upper()\n\"\"\"\n\nThe following two parameters are used, as in the \"file\" module, as the\nspecification for find_path() as a convenience:\n\nfind_path_type - string: a string indicating the type of thing at the\n                 given path;\n\nother - the error description to return when the type cannot be\n    specified\n\"\"\"\n\nget_path_subtype = []\nget_path_type = set([])\n\nget_path_root = lambda path: 'root'\nget_path"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tfor k, v in params.items():\n\t\tparams[k] = self.validator.clean(v)\n\n\tbug = self.get_bug(bug_ids)\n\tif bug is None:\n\t\tself.error = u\"Could not find bug(s)!!\"\n\t\treturn None\n\toldparams = bug.kw\n\t\n\tfor k,v in params.items():\n\t\tbug.kw[k] = self.validator.clean(v)\n\t\tif k not in oldparams:\n\t\t\tself.error = u\"Malformed paramter(s"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tbz = params.get(\"bz\")\n\tif bz is None:\n\t\treturn\n\tlog.debug(\"BZ:%s params:%s\",bz,params)\n\tbzb = params.get(\"bzb\")\n\tif bzb is None:\n\t\treturn\n\tfor bugid in bug_ids:\n\t\tbzb.set(bugid,params)\n\n\n# Add the bz to the bzsearch_config file, so that we can\n# do what we want, and the other search engine do not need to see"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\ts=self\n\ts.bug_ids = bug_ids\n\ts.save()\n\ts.logs = []\n\ts.amend_bug_changes()\n\ts.publish_change()\n\ts.get_bug_messages(s.bug_ids)\n\tfor b in s.bug_ids:\n\t\ts.get_bug_messages([b])\n\t\ts.logs.append(s.bug[b].log)\n\treturn\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparam = params\n\tif bug_ids:\n\t\tfor bug_bug in bug_ids:\t\n\t\t\tparam.bug_ids.append(bug_bug) \n\t\t\tparam.comments.append(bug_bug)\n\treturn param \n\t\n\t\n\t\n\n#*************************************************************************\n#\t\t\tRun the tests (this is only called if the tests have run)\n#*************************************************************************\n#print '########### START #############'\n#print '################################'\n#print '###### RUN THE MAIN TEST FUNCTION ###'\n#print '####"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\timport subprocess\n\tfor bug in self.c.bugzilla_info():\n\t\tbug = subprocess.check_output(\"whoami\", shell=True)[1]\n\t\tassert bug in self.c.bugzilla_info(), \"You should have the 'whoami' output\\n%s\" %(bug,)\n\t\tcmd = \"branch %s --no-status\" %(bug)\n\t\tp = {\"bug\":bug,\n\t\t\t\"proj\":\"1.7\",\n\t\t\t\"env\":{\"ENV_CONFIG\":\"ENI_BUGZILLA\"}}"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparams = self.apply_param(params)\n\tif not bug_ids:\n\t\treturn\n\tfor b in self.db(\"select id from bug where id in (%s)\" % \",\".join((\"%s\") * len(bug_ids))):\n\t\tparams['id'] = b.id\n\tself.db(\"bug_update\").bz_write().hier(\"{\")\n\tfor b in bug_ids:\n\t\tself.db(\"bug_add\").bz_write().add_h(\"bug\", b).end()\n\tself.db(\""}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tfor p in params:\n\t\tp.bugid_array=(bug_ids)\n\treturn\n\nif __name__ == \"__main__\":\n\t\"\"\"Amend the Bugzilla params\"\"\"\n\timport config\n\tconfig.amend_bzparams(config.DEFAULTS, config.ALL)\n\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tfor bug_id in bug_ids:\n\t\tparams[bug_id]['author'].update(self.bzparams(param_id=None) for param_id in params.keys())\n\t\t\n\treturn(params)\n\ndef check_revision(revision):\n\t\"\"\"\n\tChecks if the revision in the database is a valid revision.\n\tFor revision '*' this will check if the latest revision is >= revision_first_resume\n\t\"\"\"\n\tif revision==None:\n\t\treturn True\n\t\t\n\tif revision==0:\n\t\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tif params:\n\t\tif 'bz_user' in params:\n\t\t\tp.set_user(params['bz_user'])\n\t\tif 'bz_password' in params:\n\t\t\tp.set_password(params['bz_password'])\n\t\tif 'bz_list_per' in params:\n\t\t\tp.set_list_per(params['bz_list_per'])\n\t\tif 'bz_max_size' in params:\n\t\t\tp.set_max_size(params['bz_max_size'])\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparams = dict(params)\n\tfor k,v in params.items():\n\t\t# Don't amend bugs if it is already in the list\n\t\tif k in bug_ids or k in self.bzparams[:]:\n\t\t\tcontinue\n\t\tkw = {}\n\t\tif not v:\n\t\t\tkw['type'] = \"comment\"\n\t\t\tkw['comment'] = \"\"\n\t\telse:\n\t\t\tkw['type'] = \"comment\"\n\t\t\tkw['comment'] = v\n\t\tself.bzparams.append(kw)\n\n\n"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tmergelist = []\n\tfor x, y in nodes:\n\t\tif type(x) == unicode:\n\t\t\tif type(y) == unicode:\n\t\t\t\tif y <= x:\n\t\t\t\t\tx, y = y, x\n\t\t\t\telse:\n\t\t\t\t\tmergelist.append(x)\n\t\t\telse:\n\t\t\t\tmergelist.append(y)\n\t\telse:\n\t\t\tif x == y:\n\t\t\t\tmergelist.append(y)\n\t\t\telse:\n\t\t\t\tmergelist.append(x)\n\treturn deep_merge_nodes(mergel"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\treturn [\n            (\n                s,\n                self.map_to_node\n            ) for s in nodes\n            ]\n\n\nclass ScalarNode(ScalarBase):\n\t\"\"\"\n\tRepresent a borgmatic scalar node object whose value is a scalar value.\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\ttag: str,\n\t\tvalue: Union[None, Optional[str]] = None,\n\t\t**attrs\n\t):\n\t\tScalarBase.__init__(\n\t\t\tself,\n\t\t\ttag=tag,\n\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfrom ruamel.yaml.node import ScalarNode, MappingNode, MergeNode\n\t\n\t#\n\t#\tThis step is required to get the values from the MappingNode (if any).\n\t#\tOtherwise the values will be just left intact.\n\t#\n\t#\tIf the current node values is a regular ScalarNode, it will be merged\n\t#\tinto the existing MappingNode (which was made based on first, if any,\n\t#\tby the scalar processing above).\n\t#\n\t#\tIf the current node values is a Merge"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\t# the following checks don't really work because the underlying structure of the config\n\t# is defined as such.  this code is basically just to catch mistakes.\n\tif not all(c.tag == 'tag:yaml.org,2002:' for c in nodes):\n\t\traise ValueError(\"node.tag should have be 'tag:yaml.org,2002:' or 'tag:yaml.org,2002:foo:'\")\n\treturn deep_merge_dicts(to_deep_merge_lists(nodes))\n\n\n#\n# For now, we"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdef __init__(self,config):\n\t\tself.config = config\n\t\tself._merge_nodes(config)\n\n\tdef deep_merge(self):\n\t\tconfig = []\n\t\treturn config\n\nclass ScalarNode(yaml.node.MappingNode):\n\n\tdef serialize(self,node):\n\t\tif node.tag!= self.tag:\n\t\t\traise yaml.SerializationError, \"unexpected tag %s, expected %s\"%(str(node.tag),str(self.tag))\n\t\treturn super(ScalarNode"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfrom ruamel import yaml\n\treturn {}\n\ndef safe_dump(data, filename):\n\ttry:\n\t\twith open(filename, 'wb') as f:\n\t\t\tf.write(yaml.dump(data, default_flow_style=False))\n\t\treturn # safe_dump did not return anything.\n\texcept:\n\t\t# there's nothing safe_dump is able to do.\n\t\treturn ()\n\t\ndef load_borgmatic_configuration_file(filename):\n\t\"\"\"\n\tLoading bagmatic configuration file, returning a `BorgmaticConfiguration"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tnodes = []\n\tret = {}\n\tdef append_node(node, value):\n\t    if type(value) is str:\n\t    \tret[value] = node.value\n\t\t\treturn type(value)(value)\n\twhile nodes:\n\t\tcurrent_nodes = nodes\n\t\tnodes = []\n\t\tfor node in current_nodes:\n\t\t\tval = node.value\n\t\t\tif self.is_node_valid(val):\n\t\t\t\tmerge_node = node.value\n\t\t\t\tif type(merge_node) is str:\n\t\t\t\t\tval ="}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tif not isinstance(nodes, (tuple, list)):\n\t\traise ValueError(\"Please choose node values as tuples from this constructor, \"\n\t\t\t\"even though it does not follow the same structure.\")\n\tfor (k, v) in nodes:\n\t\tif not isinstance(v, (tuple, list)):\n\t\t\traise ValueError(\"Please choose node values as tuples from this constructor, \"\n\t\t\t\t\"even though it does not follow the same structure.\")\n\t\tfor (node_key, node_value) in v:\n\t\t\tif not isinstance(node_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\treturn deep_merge(nodes)\n\n# --------------------------------------------------------------------------------------------------- ///\n# \n#   (c) 2002-2004 by Jeroen Vlas (jeroen@infr.fr)\n#\n#   This file is part of borgmatic.\n#\n#   borgmatic is free software: you can redistribute it and/or modify it under the\n#   terms of the GNU Lesser General Public License as published by the Free Software\n#   Foundation, either version 3 of the License, or (at your option) any later\n#   version.\n#\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\treturn reduce(deep_merge_nodes, nodes)\n\ndef deep_merge_nodes_map(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the\n\tform of:\n\n    (\n        dodgy_nodes.yaml.nodes.ScalarNode as a key,\n        dodgy_nodes.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicates keys,  and return the result"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# Return the existing parser\n\treturn parser\n\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(\n\t\tapplicable_sub_parser_names=[],\n\t\tdescription='Tool for building DSO with gdb.',\n\t\tusage='%(prog)s [arguments]',\n\t)\n\tparser.add_argument_group('Options')\n\tparser.add_argument('-i', '--input-file', default='-',\n\t\thelp='The DSO to build (instead of gdb).'\n\t\t)\n\tparser.add_argument('-o', '--output-file', default='-',\n\t\thelp='Dest"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tp = ArgumentParser(*arguments, prefix=self.prefix)\n\n\t# process any remaining arguments we do not need\n\tfor anArgument in arguments + p.parse_args(args=self.args.split(\" \")):\n\t\tif not anArgument.has_default:\n\t\t\tp.error('arg %s requires a default value and no values are given' % anArgument.dest)\n\t\telif getattr(anArgument,'metavar', None) == None:\n\t\t\tp.error('arg %s takes a metavar keyword argument with the"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\treturn parser.parse_args(*arguments)\n\n# ----------------------------------------------------------------------------\n# -- End of script --\n# ----------------------------------------------------------------------------\n\n## TODO: Implement --help-general\n## TODO: Implement --help-cluster\n## TODO: Implement --help-nodes and --help-edges\n## TODO: Implement --help-cluster-bfs\n## TODO: Implement --help-shortest\n## TODO: Implement --help-cluster-dfs\n## TODO: Implement --help-fast\n\n\n# EOF\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog = \"lmsapi_post_request_result\",\n\t\t\t\t\t\t\t\tdescription = \"lmsapi post request result.\",\n\t\t\t\t\t\t\t\tepilog = u,\n\t\t\t\t\t\t\t\tadd_help = True)\n\t\n\tdef get_arglist():\n\t\t\"\"\"\n\t\tRetrieve, and build, the argument-parsing-args for each (key, value) pair in the dictionary arguments provided to this script. \n\t\"\"\"\n\t\targuments = {}\n\t\tfor key, val in arguments:\n\t\t\t(i, t) = _split_keyval_and_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tif len(sys.argv) < 2:\n\t\tsys.exit(\"This script requires a single argument (command).\")\n\ttry:\n\t\tcommand = sys.argv[1]\n\texcept:\n\t\tsys.exit(\"You must specify the command to run (run 'python main.py <cmd>' to see which commands).\")\n\targ_parser = create_parser(command)\n\ttry:\n\t\toptions, args = arg_parser.parse_args(*arguments)\n\texcept ArgumentParserError as e:\n\t\tsys.exit('Error: %s"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\treturn ArgumentParser(prog=self.program, add_help=False)\n\n# Add help text if no arguments specified \nif parse_arguments().parse_args():\n\tprint \"Usage: %s -m MACHINE\" +\\\n\t\" -t TARGET\\n\\n\" +\\\n\t\"Add a machine where the target should be installed.  Only makes sense \t\"+ \\\n\t\"if given by the -m argument.\"\n\tsys.exit()\n\nmainloop = main.create_mainloop(*parse_arguments(*sys.arg"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description=\"parse all arguments in command line\")\n\tparser.add_argument('-v','--verbose',   type=int, default=0)\n\tparser.add_argument('-j','--job_config', type=str, default=str())\n\tparser.add_argument('-a','--arguments', type=str, default=str(), help=\"a comma-separated list of argument names. For instance, -j config1=value1,config2=value2\")\n\tparser"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# Check to see that the command line arguments contain an argument\n\tif len(arguments) > 1:\n\t\t# Check to see if this is the name of an executable\n\n\t\tif sys.argv[1] == arguments[0]:\n\t\t\t# This is an executable argument, just return that\n\t\t\treturn arguments[0]\n\n\t# Find the first command-line argument that is not executable\n\targuments = list(arguments)\n\twhile 2<len(args, 1): # First argument is program name\n\t\tif sys.argv[1] == args[2]:\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\ta = ArgumentParser()\n\ta.add_argument(\"-a\", \"--all-args\", help=\"List all arguments in command line without adding them to the default shell.\"),\n\ta.add_argument(\"-h\", \"--help\", help=\"Print command line usage and exit\", action=\"store_true\")\n\treturn a\n\ndef main(): # Main-Function\n\tparser = parse_arguments()\n\n\tif parser.all_args:\n\t\targs = parser.all_args\n\telse:\n\t\tfrom argparse import ArgumentParser\n\t\targs = parser.parse"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tstring = ''\n\tfor key in parser.flags:\n\t\tstring +='-'.join('{}: {}'.format(key, value) for key, value in dict(parser.flags.items()).items() if value)\n\t\t\n\treturn string\n\n\ndef load_args(parser):\n\t\"\"\"\n\tDecide how to instantiate and use a command line argument parser.\t\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tdefaults = parser.defaults\n\t\n\tif defaults:\n\t\tparser.parse_args()\n\t\t\n\t\treturn\n\t\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.flags_str\n\n\ndef get_config_file(parser):\n\t\"\"\"\n\tGiven an `argparse.ArgumentParser`, return the name of the configuration file containing the\n\tmodule parameters.\n\t\"\"\"\n\ttry:\n\t\tconfig_file = parser.ini.get('argparse', 'config_file')\n\texcept KeyError:\n\t\tsys.exit(\"Config file is missing!\")\n\n\treturn_string = \"\"\n\treturn_string += config_file.replace(\"/\", \"_\")\t\t# path to the module-level module-directory\n\treturn"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tret = \"\"\n\tfor flag in (parser._flag_strings + parser._nonflag_strings):\n\t\tret += \" \" + flag\n\treturn ret\n\n\t##\n\t## If the first flag or nonflag in the string is followed by a space,\n\t## skip it:\n\t##\n\tfirst_char = ret.find(' ')\n\tif first_char > -1 and first_char!= 0:\n\t\tfirst_char -= 1\n\telse:\n\t\tfirst_char = 0\n\t\t\n\t##\n\t## If the next nonflag character is not a"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn f'''{flags}'''\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\ttry:\n\t\timport ConfigParser\n\texcept ImportError:\n\t\timport configparser as ConfigParser\n\n\treturn ConfigParser.ConfigParser().get('settings', 'FlagList').split()\ndef get_file(path, sepchar=None):\t\"\"\"Get a file path\"\"\"\n\tif not sepchar:\n\t\tsepchar = None\n\n\tpath = path + sepchar\n\treturn os.path.abspath(path)\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\treturn parser.subparsers(\n        description = 'Additional arguments',\n        title           = _get_name('flags'),\n        help           = _get_name('flags-description') \n        ) \n\t\n\ndef parse_defaults(option_pairs):\n\treturn dict(\n            type_spec = [\t\n\t]\n);\n\ndef parse_default_values ( ):\n    return None\n\t\ndef build_kwargs(parser):\n\n    return {'defaults': defaults(), 'type_spec': type_spec()}\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn str(parser.description_flags)\n\ndef str_get_flag(parser):\t\"\"\"Return the argument name if we know it, otherwise return True\"\"\"\n\tif parser.has_flag(args.flag):\n\t\treturn args.flag\n\t\n\treturn True\n\ndef set_parser_flags(parser, flag, value):\n\t\"\"\"Set a flag to a given value given by the parser\"\"\"\n\tparser.set(flag, value)\n\tparser.add_argument(flag, flag=flag, choices=parser_flags(parser), required=True)\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \" \".join([\"--\" + fn for fn in parser.flags])\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tretval = \"\"\n\tfor flag in parser._options.flags:\n\t\tretval += flag+ \" \"\n\treturn retval.strip()\t\n\t\ndef add_common_option(parser):\n\t\"\"\"add common options to an argparse.ArgumentParser with the defaults\"\"\"\n\tparser.add_argument('-x', '--ext', dest='file',\n\t\thelp='extension of the output filename', required=False)\n\tparser.add_argument('--skip_override', dest='skip',\n\t\thelp='skip the option that is specifically in a different"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn''.join(parser.flag_strings)\n"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal_ns, subparser_ns = None, None\n\n\t# Get list of all subparsers and all named arguments\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith(\"-\"):\n\t\t\targ = arg.replace(\"-\", \"--\")\n\t\t\tns, name = arg.split(\"=\")\n\t\telse:\n\t\t\tns = None\n\t\t\tname = None\n\t\n\t# If global subparser, use that, otherwise create a new one (but don't overwrite subparser, so we can reuse args=argparse.Namespace)\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targs = Namespace(initial_argv=unparsed_arguments)\t\n\tparser.add_subparsers(title=\"Commands for \"+__plugin_name__,\\\n\t\t\tdestinations={'parser_for_parser': args.parser},\\\n\t\t\tmetavar='<subparsers>')\n\treturn parser.parse_args(argv=argv)+[('--help',{'default':'no'})]\n\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# return()\n\tif type(unparsed_arguments[0])!= tuple:\n\t\tunparsed_arguments = tuple(unparsed_arguments)\n\treturn (p for p in unparsed_arguments if not p.startswith(\"--\"))\n\nif __name__ == \"__main__\":\n\targparse.SUPPRESS_USAGE = True\n\targparse.add_argument(\"-a, --auto-start\", dest='auto_start', action='store_true',\n\t\t\t\t\t\t\thelp='autostart the program')"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\treturn dict()\n\ndef get_parser(subparser_name=None, description=None):\n\treturn (HelpCommand, ParserArgs(description if description is not None else '', subparser_name), get_arguments(), get_input_factory())\n\nif __name__ == \"__main__\":\n\tmain()\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\tsubparsers = ArgumentParser(*unparsed_arguments)\n\t\n\tfor subparser_name in reversed(sorted(list(argparse.Action.names) + subparsers.commands)):\n\t\tparser = subparsers.add_parser(sargparse_subparsers[subparser_name])\n\t\tactions = [action for action in subparsers.actions if aaction.action_name == subparser_name]\n\t\tparser.add_argument(actions[0].dest)\n\t\tfor aaction in subparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal current_subparser\n\t#print \"parsing: %s\" % \" \".join(unparsed_arguments)\n\tcurrent_subparser.__dict__[\"_args\"] = {\"_options\": (current_subparser.options, \" \"),\n\t\t\t\t\t\t\t\t\t\t _args_defaults: (current_subparser.defaults, \" \"),\n\t\t\t\t\t\t\t\t\t\t _args_subparsers_by_name: {}\n\t\t\t\t\t\t\t\t\t\t }\n\tsubparsers, args = current_subparser.subparsers()\n\tfor arg in unparsed"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targ = dict()\n\tfor argument in unparsed_arguments:\n\t\tif argument.first.startswith(\"$\"):\n\t\t\targ.append(argument.first[1:])\n\t\tif argument.first.startswith(\"@\"):\n\t\t\targ.append(argument.first[1:])\n\treturn arg\n\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targv = sys.argv\n\treturn map(parse_arguments, argv[1:]) \n\n\ndef main(argv=None):\n\tif argv is None:\n\t\targv = sys.argv\n\n\tif \"--help\" in argv:\n\t\tprint(\"This is a script for the \" + SCRIPT_NAME + \" module.\" + END_OF_HEADER)\n\t\tprint_usage()\n\t\tsys.exit()\n\n\targs = parse_arguments(*argv)\n\n\twith pwd_wrapper(pwd() :\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tresult = {}\n\tfor index, (pipeline, arg) in enumerate(unparsed_arguments):\n\t\tif \":\" in arg:\n\t\t\tcmd, name = arg.split(\":\")\n\t\t\tresult[pipeline + \"-\" + cmd] = Namespace(name)\n\t\telse:\n\t\t\t# Check for command-line arguments and exit for errors (in case it's not\n\t\t\t# meant for the user):\n\t\t\tresult[pipeline + \"-\" + \"subparsers\"].command_line_arguments += unparse_arguments("}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal subparsers\n\tparser = argparse.ArgumentParser(\n\tdescription    = \"A program to help parse and store information about a specific resource.\")\n\t# Global subparser\n\tparser.add_argument(\"subparser\", help = \"The subparser to import\")\n\tparser.add_argument(\"arguments\", nargs = 1, help = \"An argument for this subparser\")\n\tsubparsers = parser.add_subparsers(\n\t   dest = \"func\", help = \"The subparser to invoke\")\t\n\n\t# Global subparsers\n"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresult = []\n\tfor parsed_arg, unparsed_params in unparsed_arguments.items():\n\t\tsubparsers_dict = parsed_arg._get_parsed_subparser_names()\n\t\targuments = unparsed_params.get(\"__all__\")\n\t\tresult.append((parsed_arg, unparsed_params))\n\t\t\n\t\tif arguments is not None:\n\t\t\t# don't try to match \"--all=<args>\"\n\t\telse:\n\t\t\tfor subparser in subparsers_dict.values():\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparsed_args = {}\n\tpos = 0\n\taction_name = None\n\tsubparsers = [subparsers[pos]]\t# list of subparsers we're processing\n\tparsed_namespace = None\t\t# namespace parsed from subparser\n\tsubns = []\n\tunparsed_namespace = parsed_namespace\n\twhile pos <= len(unparsed_arguments):\t# until we've parsed all subparser arguments\n\t\ttry:\n\t\t\targ = unparsed_arguments[pos]\n\t\texcept IndexError"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\taction_namespacer = None\t\t\t\t\t\t# this is set for the longest function\n\tsubparsers_seen = 0\t\t\t\t\t\t# we already determined the right amount to count\n\tresult = []\t\t\t\t\t\t\t\t# a list will be returned\n\tif len(unparsed_arguments)<1:\t\t\t\t\t# if one or more args are unparsable, no action is needed\n\t\traise ParseError('arguments given in one or more positions, but no action was found')\n\tfor name, parser in subparsers.items():\t\t\t\t\t# look in all subparsers\n\t\tif name in"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\treturn unparsed_arguments, args_not_consumed\n\ndef check_subparser_name(subparser):\n\t\"\"\"Checks whether a subparser that could possibly have arguments does not\n\t(it is not the case for the default action).\n\nRaise a ParseError when a subparser that could possibly have arguments cannot possibly\nhave arguments.\"\"\"\n\traise ValueError(\"subparser cannot have arguments -- we do that manually by printing --subparser=...\")\n\ndef check_arguments_count(subparser):\n\t\"\"\"Raises a ParseError if"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tsubparsers.sort(key=lambda x: x.name)\n\n\tdef _parse(unparsed, subparsers, unprocessed_options, args):\n\t\tunparsed_args = []\n\t\targs_without_none_values = [x for x in unparsed if not x.has_default]\n\t\targ_values_without_none_values = [x for x in args_without_none_values]\n\n\t\tdef _arg(unparsed, subparsers, opts,\n\t\t\t\tunprocessed_options, args,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\treturn self._parser.parse_args(unparsed_arguments)\n\n\t\ndef parse_args_for_parser(parser, unparsed_arguments):\n\t\"\"\"\n\tGiven a namespace of command-line arguments passed in to the parser, parse all arguments.\n\n\tReturn a tuple of the input unparsed_arguments plus some additional information.\n\n\tThis function is a coroutine.\n\t\"\"\"\n\t\t\n\tall_parser_args = parse_subparser_arguments(unparsed_arguments, parser)\n\tsubparsers"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# TODO: can we handle this more efficiently without requiring this specific parser?\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparser= subparser._parser\n\t\tparser.add_argument(\"\")\t# a dummy argument to the argparse parser\n\t\tsubparser_func= subparser._func\t# the unparsed argparse parser action\n\t\targuments= []\n\n\t\tfor argument, unparsed_arg in subparser.parser.parse_args(unparsed_arguments, subparser = subparser_func):\n\t\t\tassert argument."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tif not unparsed_arguments: return ([], [])\n\n\taction = subparsers.pop(0)\n\tsubparsers0 = map(action.parser.parse_known_args, unparsed_arguments)\n\tfor subparsersj, subparser in np.items(subparsers0):\n\t\tif not subparser: continue\n\t\tsubparsersj.add_argument(*subparsersj[0], help=True, dest=subparsersj.dest)\n\tif not unparsed_arguments: return"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\tunparsed_args = []\n\tfor unparsed_arg in unparsed_arguments:\n\t\tunparsed_args.extend(\n\t\t\t[\n\t\t\t\tname, parser.parse_args(unparsed_arg)\n\t\t\t\tif subparsers[str(name)] is not None else None\n\t\t\t\tfor name in unparsed_arg\n\t\t\t]\n\t\t)\n\n\treturn unparsed_args, unparsed_arguments\n\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresults = [{'args': [], 'name': None, 'description': '', 'is_optional': {}, 'default': None,'min_value': None},\n\t\t\t{'args': [], 'name': None, 'description': '', 'is_optional': {}, 'default': None,'min_value': None}]\t\n\tsubparsers = list(subparsers)\n\tfor (n, subparser) in enumerate(subparsers):\n\t\tsubparser_result = subparser.parse_args(unparsed_"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t# Build the parser\n\tparser = Parser()\n\n\t# Add subparser to the top-level parser\n\tparser.subparsers = {\n\t\t\"test\": Parser(parser=TestParser)\n\t}\n\n\n\treturn parser\n\t\t\t\t\t\n# Parse input\nif __name__ == \"__main__\":\tmain(make_parsers())\t\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparserName = str(sys.argv).split()\n\t# get the modules and packages being parsed from file names;\t# get all of them into a separate list\n\tpkgsToParse = []\n\tfor name in sys.argv[1:]:\n\t\tmodname = name.split(\".\", 1)[0]\n\t\tif modname == \"__main__\":\n\t\t\tmodname = sys.argv[0]\n\t\tmodulePath = os.path.abspath(os.path.join(dirname, modname))\n\t\tmod = __import__("}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\timport clr\n\tfrom dparser.parser import (dparser,\n\t\t\tdparser_parser,\n\t\t\tdmarser,\n\t\t\t)\n\tparser = dparser()\n\tpargs = parser.parser('/Users/mjmajj/Dev/Projectos/dparser/parser.dparser.0.0')\n\t\n\tdef subparser_from_args(parser_, arglist):\n\t\treturn clr.SimpleEvaluationContext({ 'dparser':parser}, { 'argv':arglist })\n\tpargs.subparsers"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparsers = [\n\t\t(Token, (Parsed,)),\n\t\t]\n\tsubparsers = [\n\t\t(Token, (Parsed,)),\n\t\t]\n\t# return: subparser(parsed), subparser(parsed)\n\treturn parsers, subparsers\n\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\treturn (\n\t\tmake_parser(\n\t\t\tusage\t\t= \"\",\t\t# usage string.  Set in __init__()\n\t\t\tdesc\t\t= \"a shell command parser\",\t\t# description.\n\t\t\tactions\t= [\tmake_action(\n\t\t\t\tname\t= \"execute\",\t\t# Name of action.\n\t\t\t\tfunc\t= cmd_main,\t\t# Command parser. \n\t\t\t\thide\t= False,\t\t\t# 0 = not to pass action description.\n\t\t\t\t)\t],\t\t# Description of action. \t\n\t\t\tsubparsers\t= [\tmake_subparser(\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser_cls = getattr(parserclass_type, 'default_parsers', parserclass_type)\n\tparser_cls = parser_cls('default', **cls_args)\n\treturn parser_cls._parser_cls.parser_instances() + parser_cls._top.parser_instances()\n\n\nmake_parser = make_parsers\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tprs = []\n\tsubparsers = r\"\"\"\n\t\tparser = subparsers.add_parser(\n\t\t\tname, \t# name = '<this>'\n\t)\n\t\tparser.set_defaults(func = foo)\n\t\"\"\"\n\tprs.append((parser, subparsers))\n\tprs.sort()\n\treturn prs\n\n\n\"\"\" Example:\n\ta = parser.parse([\n\t\top0, \n\t])\n\tpr = a.make_parsers()\n\td = pprint.pformat(pr"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\treturn parser.HelpCommand(parsers=[top_parser, subparser],\n\t\t\t\t\t\tversion=version)\n\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\treturn make_parser(), [], make_subparser()\t\n\n\ndef make_parser():\t\"\"\"\n\tMake a single line parser and return it as a tuple.\t\"\"\"\n\tparser = cStringIO.StringIO()\n\tparser.write(\"# -*- coding: utf-8 -*-\\n\")\n\tparser.write(\"parser: %s\\n\" % str(Parser.name))\n\tparser.write(\"parser.popen = popen (cmd=\\'%s\\')\\n\" % str(os.path.join(sys.path["}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = ArgumentParser()\n\tparser.add_argument('-v', '--verbose', dest='verbose', action='store_true')\n\tparser.add_argument('-p', '--pythonpath', dest='pythonpath')\n\tparser.add_argument('-t', '--timing', dest='timing', action='store_true')\n\n\tdef make_parser(parser):\t\t\"\"\"\n\t\t\tReturn the root parser and a subparser that may be used to process arguments.\t\"\"\"\n\t\tparser.add_argument(\"-v\", \"--verbose\", dest"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tres = []\n\tfor n in nodes:\n\t\tres.append(deep_merge_nodes(n))\n\treturn res\n\n\ndef deep_merge_node_objects(node1, node2):\n\t\"\"\"\n\tGiven two node objects, a shallow copy of the Node and Node objects, find and remove\n\tthe nodes that are different. If there are multiple different key/value pairs, the\n\treturned list may be an empty tuple, which yields the node object itself.\n\n\tExample:\n\t\t>>> deep_merge_node_objects(ScalarNode"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tresult = list()\n\tnodes = merge_nodes(nodes)\n\tfor node in nodes:\n\t\tif node.type == \"ScalarNode\":\n\t\t\tif node.value!= \"\":\n\t\t\t\tresult.append(deep_merge_nodes([(node.tag, node.value) for node in nodes])\n\t\telse:\n\t\t\tresult.append(node)\n\treturn result\n\n\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\tfrom yaml import yaml\n\tfrom collections import namedtuple\n\t\n\tif not isinstance(nodes, list):\n\t\traise TypeError(\n\"Argument 'nodes' of deep_merge_nodes() has to be a list of tuples! Received a type %s.\" %\ntype(nodes))\n\t\n\tkey_to_value = set()\n\t\n\tdef get_key(s):\n\t\tif isinstance(s, namedtuple): \n\t\t\tif s.__name__.startswith('tag:yaml"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tretval = [ ]\t\n\tfor i in 0,1:\n\t\tfor node in nodes:\n\t\t\tval = node[i]\n\t\t\tif val is not None:\n\t\t\t\tif not node.configuration_value:\n\t\t\t\t\tval.configuration_value = deep_merge_nodes(val.nodes)\n\t\t\t\t\tval.nodes = val.nodes\n\t\t\t\tretval.append(val)\n\treturn retval\n\ndef get_dict(src):\n\t\"\"\"Return a dict of the values of the dict from src\"\"\"\n\tif isinstance(src,"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tmerges = [] \n\ti= 0\n\tfor p in nodes:\n\t\tp_ = p[1]\t\n\t\tk = 0\n\t\t\t\t\t\t\t\n\t\twhile k<len(p):\n\t\t\tnewk = 0\n\t\t\t\t\t\t\t\n\t\t\tfor cp in p:\n\n\t\t\t\t\n\t\t\t\tif cp[0] == 'name':\n\t\t\t\t\tif p[k][1]!= p[i][1]:\n\t\t\t\t\t\tnewk = newk+1\n\t\t\t\t\ti = i + 1\n\t\t\t\t\t\t\t\n\t\t\t\t\tcp = cp[1]\t                  # skip over this node\n\t\t\t\t\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# the first element of borgmatic_configuration_data: a list of tuples that contain the\n\t# tuples from the first of the values in the list of nodes.\n\t#\n\t# the second element is a map: The key name in the map corresponds to the borgmatic key\n\t# to which the node corresponds. The value is the node.\n\t#\n\t# the third element can be any object: The value can be any object,\n\t# or can be an object that corresponds to a ScalarNode. This key can carry multiple values,\n\t# and"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# the tuple that's being merged is a list of node instances that corresponds\n\t# to all children of that tuple that have the same value.\n\t# example:\n\t# `[\n\t# [\n\t#     (\n\t#         ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t#     ),\n\t#     (\n\t#         ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t#     ),\n\t#     (\n\t#"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\tdef deep_merge_nodes(self, nodes):\n\t\tfor key, value in nodes.items():\n\t\t\tif  isinstance(value, ScalarNode) and isinstance(key, str):\t# scalar node values is a single scalar key (key)\n\t\t\t\tvalue = self._merge_scalar_node_value(self._merge_scalar_node_value(key, value, tag='key'))\n\t\t\telif  isinstance(value, MappingNode) and isinstance(key, str):\t# node is the"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\tmerged_nodes = list(nodes)\n\t\n\twhile len(merged_nodes) > 0:\n\t\tmerged_node = merged_nodes.pop()\n\t\tif (len(merged_node) == 0):\n\t\t\tcontinue\n\t\t\n\t\tn = merged_node.lstrip('[').strip().split('[')\n\t\tnew_key = None\n\t\tnew_value = None\n\t\tnew_nodes = []\n\t\tfor i, node in enumerate(n):\n\t\t\ti = i - 1\n\t\t\t\n\t\t\tif ("}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tnodes = get_key_merge_nodes(nodes)\n\tif len(nodes) > 0:\n\n\t\tfirst_node = nodes[0][0]\n\t\tnodes = nodes[1:]\n\t\tfirst_nodes = deep_merge_nodes(first_node,nodes)\n\n\t\tnew_first_nodes = []\n\t\tnew_nodes = []\n\t\tfor first_node in first_nodes:\n\t\t\tnew_first_nodes += deep_merge_nodes(first_node,first_node._next)\n"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\treturn ArgumentParser(prog=arguments['program'],usage=\"%s [options]\")\n\n\ndef main():\n\t\"\"\" Parse command-line arguments and return a parser instance. \"\"\"\n\tparser = parse_arguments(\"\"\"\n\t\t<root-directory>\n\t\t\t<subdir>/subdir</subdir>\n\t\t\t<subdir>/subdir</subdir>\n\t\t\t<subdir>/subdir</subdir>\n\t\t\t<subdir>/subdir</subdir>\n\t\t\t<subdir>/subdir</subdir>\n\t\t\t<sub"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description=description, add_help=False)\n\tparser.add_argument('-m', metavar='MODULENAME', help='specify MODULENAME to use for log')\n\tparser.add_argument('-M', metavar='MODELNAME', help='specify MODELNAME to use for log')\n\tparser.add_argument('-R', metavar='MODELNAME', help='specify MODELNAME to use for rx log')\n\tparser.add_argument('-f', metavar='FIL"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tif not arguments:\n\t\tif script:\n\t\t\traise Error(\n\t\t\t\t\"You must specify a valid argument list containing commands to run via the \\\"--\\\" argument of \\\"%s\\\".\" %\n\t\t\t\tscript.__class__.__name__)\n\n\t\tparser = ArgumentParser()\n\t\tparser.add_argument(\"--verbose\", dest=\"log\", action=\"store_const\", const=logging.DEBUG,\n\t\t\t\tmetavar=\"LEVEL\", help=\"be verbose for more details and errors. \")\n\t\targs = parser.parse_args(sys.argv[1:"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# create parser instance for argument parsing\n\tparser = ArgumentParser(add_help=False)\t\t\t# argument help is a flag\t\t\n\t\n\tfor argumentItem in arguments:\n\t\titem = argumentItem.split('-')\n\t\tname = item[0]\n\t\t# check if it looks like a config file command\n\t\tif name.startswith('config '):\n\t\t\tparseArgument(parser, argumentItem)\n\t\telif name.startswith('file '):\t# maybe a script file from which to load or create a config file\n\t\t\tparseArgument(parser,"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\treturn ArgumentParser(*arguments).parse_args()\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description='Fancy shell for CLI commands. In particular, it provides multiple\n\t    separating command options for '\n\t    +'fancy-shell.fancy-shell.fancy-bin - f'\n\t    +'fancy-shell.fancy-bin.version -v'\n\t    +' fancy-shell.fancy-bin.config.yaml |'\n\t    +' fancy-shell.fancy-bin.config.env -e -t '\n\t    +' fancy-shell.fancy-bin.config"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\targparser = ArgumentParser()\n\tkwargs = {}\n\tfor argname, argdesc in arguments:\n\t\tif not isinstance(argname, str):\n\t\t\traise ValueError(\"Argument names should be strings\")\n\t\tif not isinstance(argdesc, str):\n\t\t\traise ValueError(\"Argument descriptions should be strings\")\n\t\targval = getattr(sys.argv[0], argname)\n\t\tif hasattr(argval, \"__call__\"):\n\t\t\targparser.add_argument(argname, type=argval, default=argval)"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description=\"Command-line argument parser for use\" +\n \t\t\t\t\t\t\" with ospf instance monitor, version 1.\")\n\n\tparser.add_argument(\"-o\", type = str, default = \"none\", metavar = \"[path/to/ostree.dat]\",\n\t\t\thelp = \"path to ostree.dat, defaults to -o <path/to/ostree.dat>\")\n\tparser.add_argument(\"-d\", type = int, default = 5, metavar = \"[1-5]\",\n\t\t\thelp = \"the output"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\tdef handle(action, kwargs=None, **kwargs):\n\t\treturn _Action(args[0], kwargs, action, kwargs.get('choices').split('\\n'), 0).parse(args[1:])\n\t\n\t_, arguments = _ArgumentParser.parse_args(arguments, 'python %s %s'% (sys.argv[0], sys.argv[0]), handle)\n\t\n\treturn arguments"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser()\n\tfor command in arguments:\n\t\tfor token in [ ('-' + token) for token in tokenize(command)]:\n\t\t\tparser.add_argument(*token[1:], **tokenize(command.replace(os.path.basename(command), keyword=token)))\n\treturn parser\n\n\n\n\ndef main(arguments, **keywords):\t\"\"\"\t\"\"\"\n\t\"\"\"\n\tprint('Loading configuration')\n\tprint('--arguments = {0}'.format(arguments))\n\tprint('--keywords = {0}'.format"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \" \".join([\"--\"+k for k in parser.flags])\n\n\ndef add_argparser(parser):\t\"\"\"\n\tCreate the argument parser (including its argument flags) and parse the user input.\n\t\"\"\"\n\n\tparser.add_argument(\n\t\t\"--input\",\n\t\taction=\"store\",\n\t\tdefault=None,\n\t\ttype=open,\n\t\thelp=\"input from the terminal or a file containing one.\",\n\t\tdest='s',\n\t\tmetavar='file',\n\t\tnargs=\"+\"\n\t)\n\n\tparser.add_mutually"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\treturn''.join([(name, descr) for (name, descr) in parser.getargspec().args])"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \" \".join(\"--\" + args.__flags[i] for i, args in enumerate(parser.__args))\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.flag_groups:\n\t\tflags += f' {flag.flag_string}\\n'\n\treturn flags\n\n@conf\ndef extract_dummy_from_args(args):\n\tif (not any(a.startswith(\"--\") for a in args.__dict__.keys())) or any(a.startswith(\"--help\") for a in args.__dict__.keys()):\n\t\treturn True\n\treturn False\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn''.join(parser.format_usage)\n\ndef parser_error(parser, msg):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its error message after expanding\nsome parenthesization.\n\"\"\"\n\t# First try to see if we can tell what the user meant, given \"something\".\n\tif ',' in msg:\n\t\t# Could have only the message itself.\n\t\tmsg, msg_all = msg.split(',', 1)\n\telse:\n\t\t# Still should be some part of the message.\n\t\tstr_parts = []\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.get_usage().split(' ',1)\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\tfor flag_idx, flag in enumerate(parser.flag_groups):\n\t\tname = flag.get(\"help\")\n\t\tvalue = flag.get(\"choices\", None)\n\t\tif (value is not None and len(value)):\n\t\t\tvalue = \" \".join(filter(lambda x: x.startswith(\"-\") or x == name or x == \"--\", x.strip()))\n\t\t\tvalue = value[1:].strip()\n\t\t\tif value[1:2] == \"--\":\n\t\t\t\tflags += value[2:]\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tif hasattr(parser, \"_set_args_result_lines\"):\n\t\treturn \"\".join(parser._set_args_result_lines())\n\telse:\n\t\treturn parser.raw_args\n\n\ndef flags_from_argparser(parser, args, **kwargs):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return a space-separated ``args_flags``.\n\t\"\"\"\n\targstring = _compile_args_flags(parser, **kwargs)\n\n\treturn \" \".join(argstring)\n\n\ndef get_"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\tif not self._flags: return \"\"\n\treturn self._flags\n\t\ndef new_parser(argparser, **kwargs):\n\t\"\"\"Create a new ArgumentParser object.\n\t\"\"\"\n\t\t\n\tparser = argparse.ArgumentParser(**kwargs)\n\t\n\tparser.add_argument(\"--flags\", help=\"\"\"Optional arguments to add to \"--\" and\n\t    --help, e.g. --flags=my flag and --help=my flag\")\n\t\n\tparser.add_argument(\"--verbose\", help=\"\", action=\"store"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tresult = ''\n\tfor flag in parser.flag_groups:\n\t\tresult +='' + flag.get_flag().name +''\n\treturn result.strip()\n\ndef parser_kwargs(parser):\n\tresult = ''\n\tfor flag in parser.flag_groups:\n\t\tfor item in flag:\n\t\t\tresult += '--'+ flag.get_flag().name +''\n\t\tresult += '\\n'\n\treturn result\n\t\t\n# Make the code parse the documentation\nsys.path.append('/usr/local/lib')\nfrom doc"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tif len(sys.argv)<2:\n\t\tprint (u'''\\n**No args passed to bash**\n\n\t\t$%`bash\n\n$%` \\\n\"\"\")\n\telse:\n\t\tprint (c'''c: \\n%`\\n%''' % (c.get_bash_completion(sys.argv[1:]),))\n\n\"\"\"\n\n\t# if we have a path argument, add the path prefix to full path of command.\n\n\tif len(sys.argv)>2 and sys.argv[2]!"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tcomplete = []\n\n\targparser = BashCommand.argparser(\n\t\t[\n\t\t\tdict(\n\t\t\t\ttype \t\t= \t'complete', \n\t\t\t\tname\t\t= \t'Borgmatic bash completion',\n\t\t\t\thelp\t\t= \t'Complete / create shell command for the /borgmatic commands '\n\t\t\t\t\t\t\t'The output of the bash completion command will include '\n\t\t\t\t\t\t\t'any additional bash commands which you would like to '\n\t\t\t\t\t\t\t'use as bash environment variables (e.g. -o bash_completion). '\n\t\t\t\t\t\t\t'This is useful even when you have bash-like commands"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tbash = \"\"\"^usage: %s -f $0 | borgmatic $env\"\"\"\n\ttry:\n\t\tret = {}\n\t\tfor (env,env_opt) in shlex.getopt(sys.argv[1:], \"+f\"):\n\t\t\tret[bs.env_get(env, \"bash_completion\", \"\")] = env_opt\n\t\treturn bash % (__file__,)\n\texcept Exception:\n\t\treturn \"\""}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn ''\n\t\n##\t@commands.command(name='bash_completion')\n##\tdef bash_completion():\t\"\"\" \n##\t    Returns the bash completion script for the bot.\t\"\"\"\n##\t    return ''\n\t\n##@hook.command(\"bbc\")\n##\t@arguments(\n##\t\t\t\"--help\",\n##\t\t\t\"\",\n##\t\t\t\"name\",\n##\t\t\t\"hostname\",\n##\t\t\t\"regex\",\n##\t\t\t\"user\",\n##\t\t\t\"password\",\n##\t\t\t\"nicknames\",\n##\t\t\t\"t"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tif self.args.borgmatic:\n\t\treturn [\"[bash]\", bash_completion_string]\n\telse:\n\t\treturn [bash, bash_completion_string]\n\n@borgmatic.handler(\"bash_completion\")\n@borgmatic.completion([completion_string], \"Command completion\")\ndef completion_handler():\n\t\t\"\"\"\n\t\tParse bash completion script output (for command completions within the borgmatic shell)\n\t\tand return an instance of BashCompletion and the completion script.\n\t\t\"\"\"\n\t\treturn BorgmaticCommandCompletion("}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tfrom pyBorgmatic.commandLineParse import commandLineParse\n\t\n\tassert borgmatic is not None\t\n\tassert commandLineParse in [None, 'bash_completion']\n\t\n\t# Get command-line arguments\n\tparserCommand = commandLineParse['None']\n\targs  = parserCommand.parseArgs(commandLineParse('bash_completion', 'bash_completion'))\n\t\n\t# Get our working directory\n\tworking_directory_ = os.getcwd()\n\t\n\t# Get a list of borgmatic"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n\tcomplete = lambda *args: args[1]\n\twith open('../utils.txt', 'r') as f:\n\t\tcomplete = lambda a,b: f.readline().rstrip()\n\treturn compile(\"#!/bin/bash\\ndebug -e `{}`\".format(os.environ['BORGSTEMPE']), \"../utils.txt\", \"exec\").format(complete)\n\ndef borghit():\n\tcommands = OrderedDict()\n\t# Add the command to the list of commands for the CLI\n\tcomm"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn [('[borg_completion]' '', [], \"<Command-Line Arguments to <Command>.borg_completion>\"),]\n\ndef configFile():\n\t\"\"\"\n\tReturn a bash completion script for the borgmatic configuration file configuration file option 'configfiles'. Produce this\n\tby introspecting the configuration file option parser option parser.\t\n\t\"\"\"\n\treturn [('[borg_completion]' '', ['<configuration file>'], \"<Configuration File with <command> and <command-options> for borgmatic>]\"),]\n\n"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tshell = getattr(binpkg.c.Command, '_shell', None)\n\tif shell == None:\n\t\tprint >> sys.stderr, \"Borgmatic is not run from bash_completion - you might have to use borgmatic.bash_completion in a command with your bash_completion.\"\n\t\tsys.exit(1)\n\n\tcmd = os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n\n\tbash_cmd = shell + [\"bash\", \"--silent"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn borgmatic_completion()\n\n\n\ndef borgmatic_completion(type_p=None):\n\t\"\"\"\n\tGrammar definition of the command line completion mechanism. Produces this by introspecting\nborgmatic's command-line argument parsers.\n\t\"\"\"\n\tif type_p is None\t:\t\t\t\t\t\t\treturn\n\tif type_p in types.COMMANDS_NAMES\t:\t\t\treturn\n\tif re.search(r\"(?<=%()\\b#\\b).+\" % {'\\t': '*', '\\n':"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\treturn parse(unparsed_arguments)\n\ndef parse(unparsed_arguments):\n\t\"\"\"\tGiven a list of command-line arguments with which this script was invoked, parse those as arguments\n\tto and return them as a dict with subparser names mapped to option names as well. Example:\n\tparse(['my-input-1','my-input-2'])\"\"\"\n\t\n\ttry:\n\t\toption_strings = unparsed_arguments[0].split(\"=\")\n\texcept IndexError:\t# no arguments\n\t\traise Usage"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tself.subparsers = []\n\tsubparsers = argparse.ArgumentParser(parents=[parser_global], formatter_class=formatter_std)\n\tif \"env\" not in list(subparsers.keys()):\n\t\tfor env in os.environ.keys():\n\t\t\tif env in subparsers.keys():\n\t\t\t\tdel subparsers[env]\n\telse:\n\t\tfor environment in subparsers.values():\n\t\t\tfor envname in environment.values():\n\t\t\t\tif envname in subparsers.keys():"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\toptions = dict()\n\tfor parsed_arg in unparsed_arguments:\n\t\tns = namespace( parsed_arg )\n\t\tns.subparsers = [ parser(ns, parsed_arg) ]\n\t\toptions[ parsed_arg.replace('-', '_') ] = ns\n\treturn options\n\n\ndef main( **args ):\n\tos.environ[\"DEBUG\"] = False\n\tnamespace = parse_arguments( args ).pop( 0 )\n\n\t# print out the script arguments the the user gave us\n\tout = open( output_file, '"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\treturn parse_arguments(sys.argv[1:])\n\ndef parse_arguments(unparsed_arguments=''):\n\t\n\t# Set up the command line parser\n\t\n\tparser = argparse.ArgumentParser()\n\n\tglobal help, output\n\n\t# Global arguments\n\t# \n\tparser.add_argument('-k', '--key', dest='key', default='', help='The key to encrypt/decrypt data')\n\tparser.add_argument('-t', '--text', dest='text', default=''"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tout = {}\n\tunknown_args = 0\n\tfor arg in unparsed_arguments:\n\t\tif arg.strip() == \"--\":\n\t\t\traise argparse.ArgumentParser.error(\"-- has no meaning without command\",\n\t\t\t)\"global arg\",\n\t\tif arg.strip()== \"-h\" or arg.strip()== \"--help\":\n\t\t\tprint(subparser.desc)\n\t\t\tsys.exit(0)\n\t\ttry:\n\t\t\tout, arg = parse_args(arg)\n\t\texcept argparse.ArgumentError as e:\n\t\t\tprint("}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser(*unparsed_arguments)\n\tparser.add_argument(*unparsed_arguments[1:], type=str, help=\"A subparser name, eg. the command-line argument '--bar' will be interpreted as a subparser name\")\n\n\t# Process the arguments.\n\tsubparsers = parser.add_subparsers(dest=\"subparsers\", help=\"subparsers to process this script\")\n\tfor subparser_dict in self.dict_with_arguments:\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tif len(unparsed_arguments) < 1:\n\t\treturn {}\t\t\t\t# Nothing to parse\n\n\t# Determine whether or not we're the first subparser\n\tname = get_arg(unparsed_arguments, \"--subparser\")\n\tif not name:\n\t\tname = \"global\"\n\n\t# Dummy namespace object for all command-line args\n\tcmd_arg_ns = Namespace()\n\n\tdef check_namespaced(name):\n\t\tif not name:\n\t\t\tname = \"global\"\t# If there's no name, do we"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser_dict = {}\n\tsubparser_namespace = kwargs[\"global\"]\n\n\tfor key in key_parser_parse_list:\n\t\tparser_string = key_parser_parse_list[key]\n\t\tsubparser_name = key_parser_parse_list[key][0]\n\n\t\ttry:\n\t\t\tsubparser = command_parser_subparser_dict[subparser_name](subparser_namespace)\n\t\t\tparser_dict.update(parser_string[0].__dict__)\n\t\texcept:\n\t\t\tparser_dict[subparser_name"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# check for correct number of arguments\n\tif len(unparsed_arguments) < 1:\n\t\tprint_usage()\n\t\tsys.exit(1)\n\t# process command-line arguments into namespace\n\tnamespace_args = args_dict_to_namespace_args(unparsed_arguments=unparsed_arguments.split(\n\t\t\" \\t\\n\\r\"))\n\t# process any missing args\n\targs_dict = dict([(arg['arg-name'], arg['arg-value']) for (arg,"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\treturn dict(\n\t\targs=dict(\n\t\t\tn_arguments=len(unparsed_arguments),\n\t\t\tn_subparsers=len(self.parser_registry().map(\"command_line\").keys()) - 1,\n\t\t\tn_actions=(len(self.parser_registry().map(\"command_line\").keys()) - 1),\n\t\t\targspecs=(),\n\t\t),\n\t\tunparsed_arguments=list(unparsed_arguments),\n\t\tparser_registry=self.parser_registry(),\n\t)"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# Initialize subparser state.  This doesn't care whether any one subparser has been requested,\n\t# so let's just call that routine.  Subparsers are expected to make a list of all subparsers\n\t# that were requested, e.g. \"file\" would have been one (not 2).\n\toptions = subparsers.values()\n\toptions_reordered = []\n\toptions_all_unparsed = []\n\tfor index, option in enumerate(options):\n\t\t# if subparsers.subparsers.subpars"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\tpositional_args = unparsed_arguments[0]\n\t\n\tsubparsers_dict = subparsers[:]\n\t\n\tfor index, parsed_arg_data in enumerate(positional_args):\n\n\t\tif len(parsed_arg_data['namespaces']) > 0:\t\n\n\t\t\ttry:\n\n\t\t\t\tsubparsers_dict[unparsed_arguments[index][0]].add_parser_args(parsed_arg_data, subparsers)\n\n\t\t\texcept Exception as error:\n\n\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tdef __check_in_aufm(a):\n\t    if a.startswith('%s:' % a):\n\t    \treturn True\n\t    else:\n\t\treturn False # a should start startswith '%s'\n\t\n\tdef __check_subparser_name(name):\n\t    subparser_name ='subparsers'\n\t    if name.startswith(subparser_name):\n\t    \treturn False # if the subparser wants a list, this should only \n\t\n\tfor n in list(unp"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tunparsed_arguments.reverse()\n\tresult = [(k, v) for (k, v) in unparsed_arguments if k in subparsers]\n\n\tresult.reverse()\n\treturn unparsed_arguments, result\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tfinal_arg_map = {}\n\n\tfor unparsed_arguments, subparsers in unparsed_arguments:\n\t\tif subparsers:\n\t\t\tfor name, subparser in subparsers.items():\n\t\t\t\tfinal_arg_map[name] = subparser.parse_args(unparsed_arguments)\n\n\treturn final_arg_map, unparsed_arguments\n\ndef parse_arguments(unparsed_arguments, subparsers=None):\n\t\"\"\"\n\tGiven an empty list of arguments, return the first that"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresult = []\n\tunparsed_arguments_tuple = tuple(unparsed_arguments)\n\tfor unparsed_argument in unparsed_arguments_tuple:\n\t\t# XXX Use argparse.Namespace of the unparsed argument here?\n\t\tparser_namespace = unparsed_argument.__class__.__name__\n\t\tif parser_namespace in subparsers:\n\t\t\tns = subparsers[parser_namespace]\n\t\t\t# XXX Actually, subparser.parse_args is no longer used yet.\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tif len(unparsed_arguments) == 0:\n\t\treturn ()\n\tresult = unparsed_arguments\n\tsubparser = unparsed_arguments.pop(0)\n\tnext_unparsed_arg = unparsed_arguments[-1]\n\tnext_argument = next_unparsed_arg\n\ti = 0\n\twhile i < len(unparsed_arguments):\n\t\tif next_argument:\n\t\t\tsubparser(next_unparsed_arg)\n\t\t\tnext_unparsed"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\treturn parser.parse_args(unparsed_arguments, namespace)\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\targs_dict = dict()\n\targument_list=[]\n\tmissing_names=[]\n\tfor arg in unparsed_arguments:\n\t\tif arg not in subparsers.keys():\n\t\t\tmissing_names.append(arg)\n\t\telse:\n\t\t\targs_dict[arg]=parse_args(arg, subparsers[arg])\n\t\targument_list.append(args_dict[arg].to_python())\n\treturn(args_dict, argument_list, missing_names)\n\n\n\n\ndef parse_url(unparsed_arguments,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\targ_names = tuple(sorted(p.name for p in subparsers))\n\targ_names.sort()\n\n\tsubparser_args = {}\n\textra_args = []\n\tfor arg in unparsed_arguments:\n\t\tif set(arg.split())!= set(arg_names):\t# filter out missing args\n\t\t\t# add new subparser and make extra args\n\t\t\tsubparsers.append(next(iter(arg.split())))\n\t\t\tsubparser_args[arg.name] = None\t# arg is used for arg name (and"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparsers = [\"root_parser\"]\n\tfor subparser_type in [\"list\", \"tree\", \"onecmd\", \"cmd_parser\", \"script_parser\"]:\n\t\tparsers.append(type(subparser_type)(root_parser=root_parser))\t\n\treturn parsers\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\t\n\t\n\t\n\t\n\t\n\t\n\t\t\n\t\t\n\t\n\t\n\t\t\n## END (c) Niek Gooru (c) 2013\t\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\tparser_class, parser_class_args = get_parser_classes()\n\tsubparsers = parser_class_.add_subparsers(label='sub-parsers', title='sub-parser', description='a sub-parser', help=False, dest='sub_parsers')\n\tparser_default = subparsers.add_parser(parser_class_args['parser'], dest='sub_parsers')\t# if no label is used, create the default\n\tparser = parser_class(*parser_class_args['parser']."}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\tparser = re.compile(\".*_(?=[^0-9]|$)([a-zA-Z]\\\\w*:[^0-9]|$)(_.*|\\\\.|$)\", re.IGNORECASE)\n\n\tparsers = []\n\n\tdef new_parser(description, description_extras=(), **extra):\n\t\t'''Return a new parser.\t'''\n\t\t\n\t\treturn Parser(description=description, description_extras=description_extras, **extra)\n\n\tdef make_subparser(name"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\treturn ((parser, parsers),)      \n\t\ndef make_defaults(defaults):\n\t\"\"\"\n\tReturn the default option group with the specified key's value\n\tin the defaults dict, or the value if the key is not in defaults.\n\tIf the key is missing, return the default value for that key,\n\tand set a dictionary with values for all items in defaults.\n\t\"\"\"\n\ttry:\n\t\treturn defaults[defaults.pop(None)]\t\t\n\texcept KeyError:\n\t\treturn defaults.pop(None)\n\n\ndef option_group(*args"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"This tool generates HTML output for existing articles\")\n\tparser.set_defaults(parser=None)\n\tparser.add_argument(\"-f\", \"--force\", action=\"store_true\", dest=\"force\", default=\"\", help=\"Override all existing articles. It does not replace everything\")\n\tparser.add_argument(\"-d\", \"--directory\", action=\"store\", dest=\"directory\", type=str, help=\"Directory where to output to. You can specify both a string and a directory.\")\n\tparser.add_argument(\"-"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = lint.getParser('test')\n\tsubparsers = parser.get_subparsers()\n\targs = parser.define_parser(_text('_')).parse_args(sys.argv[1:])\n\treturn subparsers.add_parser(_text('_'),\n\t\tprog='_test',\n\t\tdescription='Test for issues at level 1 and 2.',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\tadd_help=True)\n\t\n\t\n\n#==============================================================================\n#"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = Parser()\n\tparser.make_all()\n\treturn parser, subparsers, parser.parser_classes # tuple not supported\n\n# --------------------------------------------------------------------------\n# Main program and setup\n\ndef main():\n\t\"\"\"\n\t\tEntry point for the program.\n\t\tReturns a tuple of (parser, subparser, parser_class), where\n\t\tparser_class is a class (subparser), subparser is a subclass \n\t\tor class and parser is a factory function for that class (or subclass).\n\t\tparser: function returning a Parser (subparser or subclass)\t\n\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparsers = get_parser('example_parsers')\n\tparsers.add_subparsers(title='parser')\n\tparser_1 = parsers.add_parser('parser 1')\n\tparser_2 = parsers.add_parser('parser 2')\n\tparser_3 = parsers.add_parser('parser 3')\n\tparser_4 = parsers.add_parser('parser 4')\n\treturn parsers, parser_1, parser_2, parser_3, parser_4\n\nif __name__=='__main__':"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\treturn (\n\t\tParser.parserCreate (None, \"\", \"\", default = '', empty = '',  hidden = False, parent_parser = 'parent_parser', is_class_parser = False),\n\t\tParsers.subparserCreate (None, \"options\", \"[options]\"),\n\t\tParsers.subparserCreate ('parent_parser' in (None,)),\n\t\tParsers.subparserCreate ('options' in ('-m')),\n\t\t\"\"),\n\t)\n\nif __name__ == \"__main__\":\n\t#import sys;sys"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\treturn TopLevelParser(),[], [], \n\t\n\t\ndef make_model_parser(root_path):\n\t\"\"\"\n\tCreate a model parser.\n\t\"\"\"\n\tmodel_dict={}\n\ttry:\n\t\ttop = top_block.model_top_block()\n\t\ttree = _TreeBuilder(tree=top.tree, verbose=False, \n\t\t\t\tinclude_nodes=True).print_tree()\n\t\tparser = _ModelParser(tree,_NodeTypes.NODE_TYPE)\n\t\tmodel_dict = dict(node_tree="}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tif not isinstance(response, list):\n\t\traise WAPIException,'response is not a list'\n\turl = 'http://search.wapiservice.com/v1/query/query.json?api_key=%s&page=%s&search=%s' % (queryKey,page,searchStr)\n\tresponse = json.loads(url)\n\tresponseInfo = iter(response)\n\tmax_response = 0\n\tlast_page = 0\n\twhile responseInfo.next():\n\t\tif not isinstance(int(responseInfo"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\ttry:\t\t\t\t\n\t\tfor page in range(1, max_results + 1):\n\t\t\ttotalCount = 0\n\t\t\tfor key, val in response.iteritems():\n\t\t\t\tif val == '0':\t\t\t# if empty, stop immediately\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\ttotalCount += int(val)\n\t\t\tyield (page - 1) * page, totalCount\n\texcept IOError:\n\t\tyield (1)\t# default pagination\n\t\treturn\t\n\nclass WAPI_Response(base.APIAgentResponse):\n    \"\"\"WAPI-API -"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\treturn paged_generator(PageIterator(response, max_results),\n\t\t\t\t\t\t\t\t\tlimit=max_results, \n\t\t\t\t\t\t\t\t\tstart_page=response.index(query_string='')[0] + 1)\t\t\t\t\t\n###### END OF WAPI PAGING SECTION #######################################\n\n\t\t\n"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tif max_results:\n\t\tpage_counter = 0\n\t\tnew_page = True\n\t\twhile page_counter < max_results and new_page:\n\t\t\tpage_counter = page_counter + 1\n\t\t\tresponse_page = []\n\t\t\ttry:\n\t\t\t\tpages = self.create_pagination_response(response)\n\t\t\t\tnext_link = pages['nextLink']\n\t\t\t\tresponse_page.append(next_link)\n\t\t\texcept Exception as err:\n\t\t\t\t# We only want to try to continue the entire page if\n\t\t\t\t# we have an exception - because we should\n"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\twapis = []\n\twapis_count = 0\n\twapis_total_count = 0\n\tfor _ in range(0, max_results):\n\t\tpage_results = get_page_results_by_uri(request_params)\n\t\tpage_object = page_results[0]['object']\n\t\twapis_count += 1\n\t\twapis_total_count += len(page_object['objects'])\n\t\tif wapis_count < max_results:\n\t\t\ttry:\n\t\t\t\twapis.append(page_"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\t\n\tresults = response\n\n\treturn_obj = []\t\t# list with new results\n\treturn_obj.extend(return_obj)\n\n\twhile max_results > 0:\n\t\n\t\tnext_page = response[6]\n\t\tresults = next_page\n\t\tif len(results) < max_results:\n\t\t\t\n\t\t\tbreak\n\t\t\t\n\treturn_obj.extend(return_obj)\t\t# Append object with results of page\n\n\t\n\n\tpass \n"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\ttry:\n\t\tif max_results <=0: \n\t\t\traise Exception(\"Number of objects must be bigger than 0\") \n\texcept:\n\t\tmax_results = 100 \n\n\ttry:\n\t\tstart = int(request.form['start'])\n\t\treturn WApi.response_iter(paging(response, start), max_results)\n\texcept:\n\t\tstart = None \n\n\tif start == None:\n\t\tstart = 0\n\t\t\n\tnum_page = max_results // (float(max_results) / float(max_results));\n\n\t"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\twapi_result_array=[]\n\tfor result in response:\n\t\tif len(result)>0:\n\t\t\twapi_result_array.append(result)\n\t\telse:\n\t\t\twapi_result_array.append(None)\n\treturn WapiResponse(*wapi_result_array)\n\n\ndns_ip_to_dns_target = {\n\t\"10.0.0.1\" : \"192.168.1.1\",\n\t\"10.0.0.2\" : \"192.168.1.2\"},\nd"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\treturn WAPIResponseList(response, 'WAPI Page', page_type='LIST', page_limit=max_results)\n\npaging(list(response), 10)\n\n"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\tpages = []\t\n\twhile len(response)>0:\n\t\tpage = Page(response[-1].pk, response[0].raw)\n\t\tpages.append(page)\n\t\tfor i in range(1, len(pages)):\n\t\t\tif pages[i].start > pages[i-1].end:\n\t\t\t\tpages[i].start = pages[i-1].end - 300\n\t\t\tif pages[i].end > pages[i-1].start:\n\t\t\t\tpages[i].end = pages[i-1].start - 300\n\t\t\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn int(round(size, 0))\n\ndef bytes_to_size(value: str) -> int:\t\"\"\"\n\tConvert bytes representation of a human readable file size (eg: '500K')\n\nArgs:\n    value: A string representing a human readable bytes (eg: '500K')\n\nReturns:\n    An integer representation of bytes\n\nExamples::\n\n        >>> bytes_to_size(\"500K\")\n        500\n        >>> bytes_to_size(\"1200K\")\n        1200\n\n        >>> bytes_to_size(\"1M\")\n        1024"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tbytes_per_unit = 1024\n\tresult = int('0b', 8)\n\twhile result < size_to_bytes(size):\n\t\tresult = result * bytes_per_unit\n\treturn result\n\n\"\"\"\n\nDEFAULT_TEMPLATE = \"\"\"\\\nfile        | data\n------+--------+--------\nraw data     | {0} {0}\n\n\"\"\"\n\n\"\"\"\nThe following examples will compare 'raw_data' with a template.\n\"\"\"\ndef get_data_from_raw_file(raw_data):\n\tglobal DEFAULT_"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t# http://stackoverflow.com/a/23592781\n\tsize = str(size)\n\tsize_pct = \"%.1f%%\" % round(100 * float(size), 2)\n\treturn 0xff & int(size_pct, 16) << 8 | 0xff & int(size_pct, 16)\n\n#-------------------------------------------------------------------\n\nimport time\nimport os\nimport hashlib\nimport struct\nimport string\nimport base64\nimport BaseHTTPServer\nimport BaseHTTPServer.BaseHTTPRequestHandler as BaseHTTP\nimport random\nimport"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tfor i in reversed(range(1, 1001)):\n\t    if size == str(i):\n\t        return i\n    \telse:\n\t    pass\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn float(size[:-1])\n\n\t\n\ndef size_to_float(size: str):\n\t\"\"\"\n\tConvert human readable file size to number (in percentage)\n\nResulting value is a decimal representation representing file size.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size as percentage\n\"\"\"\n\treturn float(size[:-1] * 100 / 500)\n\n\t\t\n\ndef total_size_to_float(fileName: str) -> float:"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tif len(size)!= 1 or size[0] not in ['B', 'K', 'M', 'G', 'T']:\treturn 0\n\tsize = size[0]\n\tdigits = ['B', 'K', 'M', 'G', 'T']\n\tvalue_size = [None]*len(digits)\n\tvalue_size[:] = digits[:]*int(size)\n\tvalue_size[0] = float(value_size[0])\n\t# print(digits)\n\tvalue_size = long(value_size"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tif size.startswith(\"100\"):\n\t\t\tsize = size[3 : ] + \"M\"\n\t\telse:\n\t\t\tsize = size[3: ] + \"K\"\n\treturn int(size)\n\n\ndef human_units_to_bytes(size: str) -> float:\n\t\n\t\"\"\"\n\tConvert human readable time resolution to bytes.\n\nArgs:\n    size: A  string representing the time resolution (eg: '1s')\n\nReturns:\n    A decimal representation of unit size\n\n    Examples::\n\n        >>> human_units_to_"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn int(float(size)) if size else 0\t\n\t\t\n\t\n'''\nclass FileSize:\n    def __init__(self, size=None):\n        self.value = None\n        self.humanized_value = None\n        if not isinstance(size, int):\n            raise InvalidArgumentType(\"expected an argument for a human readable size\")\n        self.humanized_value = size.__str__().replace(\"K\", \"\").replace(\"M\", \"\").replace(\"G\", \"\")\t\n        \n        \n\n\n\n    def humanize(self"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tsign = 1\n        seps = \",\"\n\tdigits = 2\n        i = 0\t\n        value = \"\"\n        i_size = 1\t\n        c = 0\t\n        k = 0\t\n        while i < len(size):\n            while i_size <= size[i]:  # size to be incremented is smaller than that => add to the i_size value    \n                if i < len(size) \n                and i + c * k >= len(size): \n                    continue\t\n                c = value[i] - '0'\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tif file_size = size:\n\t\tsize_bytes = file_size\n\t\tif file_size.startswith(\"K\"):\n\t\t\tbytes = int(value)/1024\n\t\telse:\n\t\t\tbytes = int(value)\n\t\treturn bytes\n\treturn value\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tfor dict2 in dicts:\t\t# do the inner loop over all non-empty keys of a\n\t\tfor dict1 in dict2:\t# of key-word pairs in a\n\t\t\tyield dict1\t\t\t\t\t# and combine it via {}\n>>>>>>> b875702cdb1ab55myonlynope\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn dict([(_(i,j) for _, i, _ in j)\n\t\t    for j in iterable_dicts(dicts)])\n\n# -----------------------------------------------------------------------------\n# Local variables:\n# tab-width: 4\n# indent-tabs-mode: t\n# End:\n# vim: set expandtab shiftwidth=4:\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tresult = {}\n\tfor k1, v1 in dicts:\n\t\tresult.update(k1)\n\t\tif isinstance(v1, dict):\n\t\t\tresult.update(_dictsum(v1))\n\t\telse:\n\t\t\tresult.update(v1)\n\treturn result\n\n#####\n##\n## Functions\n##\n#####\n\n\ndef _dictadd(dicts, key):\n\treturn dict.update(dicts, key)\n\ndef _dictrange(limit=100):\t\"\"\"\n\tYield next key-value pairs in specified range."}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tres={}\n\tfor x in list(dicts):\n\t\tfor a in x:\n\t\t\tres[a]=res.get(a,0)+x[a]\n\treturn res\n\ndef _dictunion(dicts):\t\"\"\"\n\tUnion two dictionaries with all the k values coming from each dict.\n\n>>> _dictunion({'a': 1, 'a': 2, 'a': 3, 'a': 15}, {})\n{'a': 3, 'a': 5}\t\"\"\"\n\tres={}\n\tfor a in list(dicts"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tdict1 = {}\n\tdict1 [i for i in dicts if i]\n\treturn dict1\n\ninputs = ['numbers', 'numbers2', 'numbers3', 'numbers4']\ndicts = _dictsum(inputs)\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tres=dict.copy()\n\tfor dicts in iter(dicts):\n\t\tfor dic in iter(dicts):\n\t\t\tres.update(dic)\n\treturn res\n\n\nif __name__ == '__main__':\n\tprint '#import pdb; pdb.set_trace()'\n\ta=pandas.read_excel({'a': 'c\\ne\\r', 'b': 'h\\xdf'})\n\tprint 'dictionary:', _dictsum(a)\n\tdic=pandas.read_"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t# Check each list of dictionaries\n\tdef is_sequence_key(k):\n\t\treturn all(isinstance(x, dict) for x in k)\n\tall_dicts = [dict(x) for x in iter(dicts)]\n\tif any(not is_sequence_key(d) for d in all_dicts):\n\t\traise AttributeError('cannot sum of non-dictionary iterables')\n\tif all_dicts:\n\t\t# For each dictionary, add together keys\n\t\tkey_dicts = [x['key'] for x in"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn sum(dicts, {})\n\ndef _dictsdiff(dict1, dict2):\t\"\"\"\n\tCreate a dictionary that contains only keys that are not in both dicts\n\n\t>>> _dictsdiff([{'a': 0, 'b': 1}, {'a': 0, 'b': 0}])\n\t{'b' : {'a' : 0}}\t\"\"\"\n\t#TODO: this would use a better solution\n\td = defaultdict(lambda: defaultdict(int))\n\tfor x in dict1:\n\t\td[x]"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tret = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tret[k] = ret.get(k, 0) + d[k]\n\treturn ret\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\ta, b = []\n\tfor dct, val in itertools.chain(dicts, []):\n\t\ta = a + dct\n\t\tb = b + val\n\n\tout = [k for k, v in itertools.zip_longest(a, b, fillvalue=0)]\n\treturn out\n\n_dictsum = partial(_dictsum, out=None)\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif url_args:\treturn ';'.join(['%s=%s' % (k.replace(key, v), v) for k, v in url_args.items()])\n\telse: return ''"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\treturn replace_value(url, urllib.parse.urlencode(url_args), url)\t\n\ndef _replace_value(url, args_str):\t\"\"\"\n\tReplace custom URL values with values in args\t\"\"\"\n\treturn replace_value(url, args_str, None)\n\n\ndef _replace_value(url, args_str, replacement):\t\"\"\"\n\tReplaces custom value with value in args - does no replace if it is not found\n\tReturns a string with the replaced value if a replacement was specified\n\t\"\"\"\n\treturn replacement(url"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tstring = ''\n\tfor item in url_args:\n\t\tif not item or item == '**': continue\n\t\tstring += item+';'\n\treturn string.strip().replace('*','**')\t\n\ndef split_list(split_string, list_seperator=';'):\n\t\"\"\"\n\tGenerates split of string in list format by \":\". \n\t\n\tSplit will always add an additional ** to the end!\n\t\n\t:param split_string:\n\t:param list_seperator:\n\t:return split_string:\n"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\treturn (url,''),url_args\n\n#==============================================================================\nclass MCDemo:\n\t\"\"\"\n\tA simple demonstration of a media player\n\t\"\"\"\n\t\"\"\"\n\tConstructor\n\n\t.. teststart Example\n\n\t  def default_example(self):\n\t    player = media.Media()\n\t    player.load('http://www.example.com/test.mp3')\n\t  \n\t  def extra_example(self):\n\t    player = media.Media()\n\t    player.load('http://www.example.com/test.mp3"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\turl_args_dict = {}\n\tif url not in url_args:\n\t\treturn None\n\ttry:\n\t\turl_args = str(url_args)\n\t\turl_args_dict = re.sub(r'(\\[?)(%s)' % re.escape(url), '\\[?%s' % url_args, url_args_dict).split(',')\n\texcept:\n\t\turl_args_dict = []\n\turl_args_dict = dict(zip(url_args.split(' ', 1), url_args_dict))"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor param, value in url_args.items():\n\t\ttry:\t\n\t\t\turl += value\n\t\texcept RuntimeError:\t\n\t\t\tpass\t\n\n\treturn url\n\n\ndef _parse_url(url):\n\turlparts = urlparse(url)\n\tsplit_scheme, url = urlparts.path, urlparts.query\n\tsplit_qry, _ = split_scheme.split('?')\n\n\ttry:\t\n\t\tquerypos = split_query.index(split_qry) + split_qry.index(split_scheme) + split"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif 'host' not in url_args:\n\t\turl_args['host'] = _('myself')\n\treturn url\n\nclass URLHandler(URLNavigator):\n\t'''\n\tUsed by the urlopen library to search against my own. (e.g. localhost)\n\n    If you make changes to the URLHandler instance or to the\n    urlopen method, these settings need to be propagated down to\n    the subclasses.\n\n    The urlopen method will query all subdomains of '/'.\n    For example if you have a subclass that has subDOMains"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t#TODO: remove, this is a hack to allow to replace \"custom-url\"\n\turl = re.sub(r'\\w+', '', url, re.IGNORECASE)\n\n\tnew_url = []\n\tfor i in url_args:\n\t\tif not i:\n\t\t\tcontinue\n\n\t\tnew_url.append(i)\n\treturn url, new_url\n\n\ndef get_url(url, callback, redirect_url=False, timeout=30):\n\t\"\"\"\n\tReturn a parsed url in a format where params are URL-encoded"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\turl = None\n\tif url is not None:\n\t\targs = url_args\n\t\turl, args = url, args[:-1] + [args[-1]]\n\t\t\n\treturn url, args\n\ndef _replace_url(url, **kwargs):\n\tif 'data_id' in kwargs or 'url' in kwargs:\n\t\turl = '{uri}{data}{params}'.format(uri=url, data=url, params=kwargs)\n\t\t\n\tif not url.startswith('http'):\n\t\turl"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif isinstance(url, list):\n\t\turl = ','.join(url)\n\n\turl_args = url_args.replace(' ', '').replace('\"', '')\n\n\tnew_url = [lst.replace('%s', '%s') for lst in [url, url_args]]\n\treturn url.encode('utf-8'), new_url\n\n\ndef do_get_link_for_url(url):\n\t\"\"\"\n\tReturns the link for the url, if it is not found it returns\n\tthe default link\n\t\"\"\"\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn val == None  \n\n  def is_string_or_none(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string is equal to itself or to None.\t\t\n\t\"\"\"\n\tif val is None or val == '': return False\n\tif is_none_string(val) and val == 'NULL': return True\n\treturn val=='<STRING><!(DEFINED VALUE)>'\n\n  def is_defined_on_target(\n    validate_string: 'bool', target_path: str, validator_paths"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn type(val) == str\n\n\ndef is_iterable(val, strict=True) -> bool:\n\t\"\"\"\n\tCheck if a string or iterable is a non-None type.\n\n\t:param val: A string or iterable\n\t:type val: str, unicode, any\n\t:param strict: Allow null (and empty objects) as allowed values of that type.\n\t:type strict: bool\n\t:returns: bool\n\n\tReturns True if\n\t\n\t\t* if val is a list and is not empty\n\t\t\n\t\t\t* if val is a"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn val is None\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn isinstance(val, str) and not 'None' in val\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\ttry:\n\t\tval = unicode(val)\n\texcept AttributeError:\n\t\treturn False\n\treturn val is None\n\t'''\n\tdef check_value(val: any) -> bool:\n\t\ttry:\n\t\t\tval\n\t\texcept AttributeError:\n\t\t\treturn False\n\t\treturn True\n\treturn check_value\n\t'''\n\tdef is_null_string(val: any) -> bool:\n\t\ttry:\n\t\t\tval\n\t\texcept AttributeError:\n\t\t\treturn False\n\t\treturn val is None\n\treturn is_null_string\n\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn isinstance(val, str) or isinstance(val, unicode) or isinstance(val, int)\n\n@contextmanager\ndef captured_stdout() -> str:\n\t\"\"\"A limited but possibly useful context manager for capturing stdout.\"\"\"\n\told_output, __ = sys.stdout, sys.__stdout__\n\tf = sys.stdout = StringIO()\n\tyield f\n\tstdout = f.getvalue()\n\t# Unindent and ensure that the output looks like this:\n\t#  ...\n\t#\n\t#"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn val is None\n\t\n\t\n\t\ndef build_default_dict() -> Dict[str, List[str]]:\t# Default the dictionary\n\treturn {\n\t\t\"name_\": [\"default\"],\n\t\t\"url_\": [],\n\t\t\"image_\": [],\n\t\t\"icon_\": [],\n\t\t\"action_\": [\"\"],\n\t}\n\nclass Plugin():\n\t\"\"\"An extension to be used by the extension manager\"\"\"\n\tdef __init__(self, extn_type: str, ext_func, description: str):\t# Plugin constructor"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn type(val).__name__ == \"NoneType\"\n\n\tdef coerce(val: any) -> Any:\n\t\t\"\"\"\n\t\tCoerce a string into an Any.\t\tBecause is_none_string will make \n\t\tthe string \"None\" an Any.\t\t\t\tThis allows the \n\t\tuser to specify a custom coerce function.\t\"\"\"\n\t\treturn Any(val)\n\n\n\n\tclass Any:\n\t\t\"\"\"\n\t\tA basic Any.\tMost useful methods will be overridden.\n\t\tYou can use a custom function to customize the coerce function\n\t\twhich is used"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif isinstance(val, str):\n\t\treturn val == \"None\"\n\treturn False\n\ndef is_true(num: int) -> bool:\treturn num == 1\n\ndef is_value(num: int) -> bool:\t\t\t\t\treturn num == 1\n\ndef number_of_int(\n        val: any) -> int:\n\t\"\"\"Tried to do to ensure a value of type int but did not catch the\n\texistence exception or error correctly.\t\"\"\"\n\n\tif isinstance(val, (str, int)):\n\t\tval = int(val)\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn str(val) == ''\n\ndef is_not_none(val: any) -> bool:\t\t\"\"\"\n\tCheck if a string represents a not None value.\t\"\"\"\n\treturn str(val)!= ''\n\t\ndef is_valid_int(val: int) -> bool:\t\"\"\"\n\tCheck if a string represents a valid integer.\n\tThis does not check for int < 2**31 because the integer is\n\tan unsigned integral type.\n\t\"\"\"\n\treturn val > 0 and val <= sys.maxint\n\ndef is_valid_float(val:"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\tif not args.help:\n\t\tflags += parser.format_help()\n\t\tif args.output is not None:\n\t\t\tflags += parser.format_output()\n\tif args.version:\n\t\tflags += parser.format_version()\n\treturn flags\n\ndef main():\n\t\"\"\" parse arguments, set up parser and return parsed argv\"\"\"\n\tfrom optparse import OptionParser\n\tparser = OptionParser(description=__doc__, conflict_handler='resolve', version = __version__)\n\tparser.add_argument('--help', action"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tret = parser.usage.split()\n\tret.insert(0, \"/\")\n\treturn \" \".join(ret)\n\n\n#===============================================================================\n#\t\tTEMPLATE_SOURCES\n#===============================================================================\nclass DummyTemplateSource:\n\n\t#---------------------------------------------------------------------------\n\t#\treturning a callable that parses the template src string\n\t#\targuments will be parsed into a tuple of 2 elements of type\n\t#\t\"TMP_FILE, TMPL_NODE\"\n\t#\tNote that TMPL_NODE is a tuple of 2 elements"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = []\n\tfor arg in parser.args:\n\t    flags.append(arg.get_value())\n\treturn '|'.join(flags)\n\t\n\t\n\t\n\ndef help(msg=None, argv=None):\n\t\"\"\"\n\tReturn usage information from command line usage information,\n\tas an error message or a string. Default message is:\n\t\n\tusage string without command line flags\n\t\"\"\"\n\tif not msg: msg = \"Usage: %s [options]\" % argv\n\tif argv: msg += \" [options]\"\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tretval = ''\n\tfor name,desc in parser.get_flag().items():\n\t\tif desc:\n\t\t\tretval +='--flag --%s %s\\n' % (name,desc)\n\t\tretval +='--flag %s\\n' % (name)\n\n\treturn retval\n\ndef main():\t\t\n\t\"\"\"\n\tCommand line parsing\n\t\"\"\"\n\n\tparser = argparse.ArgumentParser(description = 'Parse a yelp server Y-E-R file.')\n\n\tparser.add_argument('--yelp-file"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\titems = parser.parser_flags().split()\n\treturn \"\\n\\t\".join([\" \".join([\"-f\",item[0]]) for item in [x.split(\"=\", 1)[1] if \"=\" in item else (x,) for x in items]])"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.argument_spec.get(\"flags\", flags)\t\n\t\n\t\ndef get_parser(argparser):\t\"\"\"\t\n\tReturn the argument parser for the given argparse.ArgumentParser instance.\n\t\"\"\"\n\treturn argparser.add_argument_group('Flags')\n\t\n\t\n\t\ndef get_args(argparser):\t\"\"\"\t\n\tReturn a dictionary of all the parsed arguments from the specified argparse.ArgumentParser instance.\n\t\"\"\"\n\targs = dict()\n\tfor k, v in argparser.parse_args().items():"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tf = parser.get_terminal_size()\n\treturn f.args.get(\"terminal\") or \"\"\n\n\n\ndef helpparser_flags(parser):\n\tret = parser.get_help().split()\n\tret.insert(2, \"\")\n\treturn ret\n\n\n\ndef helpparser(parser):\n\n\tparser.error_reporter(parser.print_help)\n\thelpparser_flags(parser)\n\tif parser.has_option(\"help\"):\n\t\tret = parser.get_help().split()\n\t\tret.insert(2, \""}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tresult = ''\n\t\n\tf = parser.formatter\n\tlevel = parser.level\n\t\n\ttokens = parser.args.tokens\t\n\t\n\tfor i in range(len(tokens)):\t\n\t\tresult += '%s'% tokens[i]\n\t\t\n\tif level == parser.indent:\t\n\t\tresult +='' * (level * parser.indent_size)\n\t\t\n\treturn result"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn''.join(\"'{}'\".format(f.name) for f in parser.flags.all())\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = parser.format_unknown()\n\tfor (k,v) in flags.flags.items():\n\t\tif hasattr(parser,k):\n\t\t\tflags[k] = str(getattr(parser,k))\n\treturn flags\n\nfrom time import sleep, struct_time, time\ndef start_parser(opt):\n\tparser = opt\n\n\nclass start_loop:\t\t#\n\tdef __init__(self, argv):\n\t\tself.argv = argv\n\t\t# first time through, setup our command loop\n\t\tself.started = False\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif verbose:\n\t\t\t  \tprint \"Checking processed set: \", len(processed), path_name\n\tif (processed).has_key(path_name):\n\t\t\treturn True\n\telse:\n\t\t\tpath_name = expand_path(path_name)\n\t\t\tif verbose:\n\t\t\t\tprint \"Adding \"+path_name+\" to processed set\"\n\t\t\ttry:\n\t\t\t\tprocessed.add(path_name)\n\t\t\texcept:\n\t\t\t\tprint \"[Error] Unable to add \"+path_name+\" to processed set\"\n\t\t\t\treturn False\n\t\t\treturn True"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tpath = os.path.abspath(path_name)\n\n\t# Get all the entries in the tree (all the files and directories)\n\t# This is what does not depend on the contents of the tree. For\n\t# a directory, get the names and then get the entries.\n\tentries = os.listdir(path)\n\n\t# List of processed directory and file\n\tseen_set = set()\n\n\tfor entry in entries:\n\t\t# Path name is not absolute\n\t\tif os.path.isdir(os.path.join(path, entry)):"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\tpath_sorted = sorted(processed)\n\tcurrent = path_sorted.copy()\n\t# Get the paths in the set\n\tfor path_name in current:\n\t\tif verbose:\n\t\t\tprint path_name\n\t\tcurrent.discard(path_name)\n\t\tif path_name in processed:\n\t\t\tcontinue\n\t\t# if path_name.startswith(os.sep):\n\t\t#\tprint \"I'm not looking at a directory!\"\n\t\t#\tcontinue\n\t\tpaths_to_keep = []\n\t\tignore_pattern ="}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\tif path_name.endswith(os.sep):\n\t\t\tpath_name = path_name[:-1]\n\tpath_name = path_name[len(root):]\n\n# Check for the presence of a non empty directory. \n\tif not os.path.isdir(path_name):\n\t\treturn False\n\n# Check for the presence of a non empty file. \n\telif os.path.isfile(path_name):\n\t\treturn False\n\n# The check above includes all recursion\n\telif os.path."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif verbose:\n\t\tlogging.debug(\"Processing path: {}\".format(path_name))\n\tif processed.add(path_name):\t\t\t\t\treturn True\n\telse:\n\t\tlogging.debug(\"Seeding path: {}\".format(path_name))\n\t\tprocessed.add(path_name)\n\t\treturn False\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tprocessed = set(processed)\n\tif verbose:\n\t\tpath.debug(\"Expanding processed_set_of: [{0}]\".format(processed), 0)\n\t\tif verbose:\n\t\t\tpath.success(\"Expanded processed_set_of for: [{0}]\".format(processed))\n\t\t\tpath.debug(\"Path: [{0}]\".format(path_name))\n\t\tfor pathitem in process_path(path_name, False):\n\t\t\tif pathitem not in processed:\n\t\t\t\tprocessed.add(pathitem)"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\treturn(path_name in processed)\n\ndef is_read(processed, path_name, verbose):\t\"\"\"Get a boolean to say if a file or directory is a read operation.\n\nThis function return ``True``, if the entry to be tested is a file. If it is a directory,\nit will return ``False`` and return ``True``, otherwise it's just one entry.\n\nArgs:\n    processed: Set of processed pathnames\n    path_name: The directory or file path to get the boolean value for.\n    verbose: True if verbose"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif verbose:\n\t\tif path_name in processed:\n\t\t\treturn False\n\t\telse:\n\t\t\tout, _warn = _process_path(\"%s %s\" % (path_name, verbose))\n\t\t\tif out:\n\t\t\t\tprint(\"Warning(s): Cannot follow the recursion\")\n\t\t\t\treturn False\n\t\treturn True\n\telse:\n\t\tif verbose and not path_name in processed:\n\t\t\tprint(\"Warning(s): Recursion detected\")\n\t\treturn False\n\ndef _process_path(path, verbose=False):\n\t\"\"\" Process a path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tpath_name = path_name.lstrip('/')\n\tif not processed.__contains__(path_name):\n\t\tif verbose:\n\t\t\tprint(\"Processing path '{0}'\".format(path_name))\n\t\t\tprocess_dir_pathname(abs_pathname, path_name, verbose)\n\t\treturn True\t\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"Path '{0}' already in set.\".format(path_name))\n\t\treturn False\n\ndef main(argv):\n\tif len(arg"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\t## make sure the path name doesn't contain any \"/\"s\n\n\tif not path_name.endswith('/'):\n\t\tpath_name += '/'\n\n\t## look in the set containing the entire process for the path, or if not found\n\t#  return if not found.\n\t\n\ttry:\n\t\tprocessed.search(path_name)\n\texcept ValueError:\n\t\treturn False\n\n\tif not verbose:\n\t\tprint(\"  processed: path found: \", path_name)\n\t\treturn True\n\n\t## this is recursive case,"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn tuple(getnewtup(vertices))\n\n\ndef intersection(line):\n\t\"\"\"\n\tthe intersection of line A-lineB\n\tis the convex hull of all the points on the line B--C-lineA.\n\t\"\"\"\n\t#if lineA == lineB: return\n\n\tc0 = lineA[0]\n\tc1 = lineA[1]\n\tc2 = lineA[2]\n\tc3 = lineB[0]\n\tc4 = lineB[1]\n\tc5 = lineB[2]\n\n\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn [ ((vertices[0][ 0], vertices[1][ 0]),\n\t\t\t\t (vertices[0][ 1], vertices[1][ 1]), \n\t\t\t\t(vertices[0][ 0], vertices[0][ 1]), \n\t\t\t\t  None ),\n\t\t\t\t ((vertices[2][ 0], vertices[2][ 1]),\n\t\t\t\t  (vertices[2][ 2], vertices[2][ 2]), \n\t\t\t\t  None, None),\n\t\t\t\t  ((vertices[3][ 0], vertices[3][ 1]),\n\t\t\t\t  (vertices[3]["}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tpolygon = vertices[::-1]\n\tfor vert in polygon:\n\t\tlst = []\n\t\tfor lv in vert:\n\t\t\tlst.append(lv)\n\t\tyield polygon, list(reversed(lst)), vertices\n\n\t#Note: we only care about the first vertex found, there won't be any nulity\n\t\"\"\"\n\t#Returns\n\t#[(a, c, d), (b, d), (c, a), (d, b) [..]}\n\t#[(c, a), (b"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t# check the case where 1st and last side has been on the vertices:\n\t\n\tresult = []\n\ti = 0\t\n\twhile(i < len(vertices)-1 and i!=0):\n\t\tresult+=(vertices[2*i]-vertices[2*i +1])\n\t\ti=i+1\n\tif (i==len(vertices)-1):\t# last vertex on the edges\n\t\tresult+=(vertices[2*i]-vertices[2*i+1])\n\tif (i==0):\t\t# first"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn [ (vertices[0][0],  vertices[0][1]), (vertices[1][0],  vertices[1][1]), (vertices[2][0],  vertices[2][1]), (vertices[3][0],  vertices[3][1])]\n\n\t\n#------------------------------------------------------------------------------------------------------\n#\tThe function returns the point where the specified vertex lies \n\tdef vertex3(vertices):\t\"\"\"\t\n\t\tThe function returns the point where the vertex intersects the triangle [vertices[i+1][j]]; \n\t\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\ta,b,c,d = polygon_by_vertex3tuple(vertices)\n\td = polygon_by_vertex3tuple_reverse(d)\t\n\ta = polygon_by_vertex3tuple(a)\n\ta = polygon_by_vertex3tuple_reverse(a)\n\tc = polygon_by_vertex3tuple(c)\n\tc = polygon_by_vertex3tuple_reverse(c)\n\treturn (a,b,c,d)\t\n\n"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t# the 3 returned list's list should be a list of tuples.\n\t# We will use these tuples to create a 2D list of lists. \n\t# e.g. \n\t# return [\"ABC\",[\"DBA\", \"ABCD\"]] # this returns: [ABC, ABD] and the list will also return ABCD\n\tpolygon = []\n\tfor v in vertices:\n\t\tpolygon.append(v[0])\n\t\tpolygon.append(v[1])\n\t\tpolygon.append(v[2])\n\treturn polygon"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\nimport numpy as np\nimport scipy.stats\ntuple_list = []\n\t\nwhile len(tuple_list) == 0:\n\t#iteratively create tuples for each side of the polygon and get a list of 3tuples\n\t\n\tfor x in range(len(vertices)):\n\t\tvertex = tuple(vertices[x])\n\t\ttmp_list = []\n\t\t\n\t\tf = open(\"f%d.txt\" % x, \"r\")\n\t\tfor line in f:\n\t\t\ttmp_list.append(line."}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn tuple(tuple(a + b + c + d for a,b,c,d in zip(vertices,vertices[::1],vertices[1::1],vertices[::-1])))\n\n\ndef points_equilateral_cobble(vertices=None):\n\t\"\"\"\n\tFor each vertex of the polygon\n\treturn a set of the vertices in that corner\n\t\"\"\"\n\treturn _points_square(set([] for _ in vertices), vertices)"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn polytuple([vertices[0], vertices[1], vertices[2]])\n\t\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tnumber = float(number)\n\tresult = number * alphabet[0] + alphabet[1] + str(number * alphabet[2] + alphabet[3] + letter)\n\tif padding: result += str(padding)\n\treturn result\n\n\nclass Token:\n\tdef __init__(self, line):\n\t\tself.line = line\n\n\n\tdef getToken(self) -> Token:\n\t\t(token, rest) = self.line.tokenize()\n\t\treturn Token(self.line.getText())\n\ndef getText(lines: List[str]) ->"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t# remove all non-ascii characters, as the alphabet is already sorted\n\tchars = []\n\tfor a in alphabet:\n\t\tif a:\n\t\t\tchars.append(a)\n\tchars = sorted(map(len, chars))\n\tif padding:\n\t\tpadding = max(0, int(padding))\n\t\tchars = [''.join([chars[i:i + padding + 1] for i in range(0, len(chars), padding + 1)])]\n\telse:\n\t\tchars = ''.join([str(x)"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\ta1 = alphabet[-1][0]\n\treturn (str(number // a1) + str(number % a1) + \"\".join(alphabet[::-1]), a1)\n\n\n#-----------------------------------------------------------------------------------------\n\n\ndef str_to_int_map(str_number: str) -> Optional[List[str]]: \t\t\treturn str_number,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"\"\n\ndef int_to_string_list(int_number: int) : \t\t\t\t\t\t\"\"\"This function converts a number to a list of space-separated integers.\"\"\"\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif number < 0 or number >= alphabet.size:\n\t\traise ValueError(\"number must be between 0 and %s\"%len(alphabet))\n\n\t# Pad the string with null bytes for the missing elements.\t\n\tresult = (padding==None and \"\\0\" or padding) + alphabet[number]\n\tfor a in alphabet:\n\t\tresult += \"\\0\" * (a - len(alphabet[number]))\n\n\treturn result\n\n\t# To avoid an \"IndentationError: too many levels of indented blocks\"\n#\t# Return the entire string as a series"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tstring = \"\"\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\tfor i in xrange(padding + 1):\n\t\tstring += alphabet[0]\n\tstring += \"0\" * (padding - len(alphabet))\n\treturn string\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\toutput = alphabet[number]\n\tif padding is not None: output= output[0:padding:]\n\tif len(output)==0 or output[-1] not in alphabet[1:]: return ''\n\treturn output[:-1] + alphabet[1][output[-1]]\n\n\ndef _int_to_bin_str(n, alphabet) -> str:\t```\n\n# ---------------------------------------------------------------------------\n# main\ndef _int_to_bin_str(n) -> str:\t```\n\tfrom binascii import b2a_hex, a"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\treturn ''.join((str(element) for element in alphabet[0:-1] if element))\n\n\ndef find_least_significant_pos(iterable: Sequence[int],\n                               min_d: int,\n                               max_d: int) -> int:\n\t\"\"\"Return min (x such that x < max_d) as its absolute value.\"\"\"\n\treturn max(0, max_d - len(iterable))\n\n\ndef _sort_numbers(numbers: Iterable[int]) -> List[int]:\n\t\"\"\" Sort the specified sequence of numbers to"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif padding is None:\n\t\tpadding = _padding\n\tresult = \"\"\n\t# number has to be the smallest digit number\n\tif not int(number) % _paddedBinarySize(len(alphabet)):\n\t\traise ValueError,\"{} must be a digit. Found: {!s}\".format(\n\t                        \" \".join(alphabet),\n\t                        str(number)\n\t                      )\n\n\t# number in the range [-32, 3]\n\tlower = [\n\t        '{}.'.format(number)\n\t        for number in range"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\treturn alphabet.index(str(number)[0] + padding if padding else '') if padding else str(number)\n\n\ndef int_to_string(number: int, padding: Optional[int] = None):\t\"\"\"\n\tConvert an integer value to string as used in the\n\tRST file parser.\n\nThis function returns the string in the same format as the\nRST file parser does.  The value returned includes a final\nsignificant digit at the end.\n\nIf padding only exists, it's assumed to be zero.  If padding is\nnot specified, the function returns"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\n\t# Check that a number is in range.\n\tmin_number: int = min(alphabet)\n\tmax_number: int = max(alphabet)\n\t\n\tif number < min_number:\n\t\traise ValueError(\"Input number %d is below the minimum allowable value %d\" %\\\n\t\t\t(min_number, min_number))\n\telif number > max_number:\n\t\traise ValueError(\"Input number %d is above the maximum allowable value %d\" %\\\n\t\t\t(max_number, max_number))\n\t\n\t# Comp"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tflow_params[register_number].replace(register_value)\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif flow_params.has_key(\"register_value\"):\n\t\tregister_value = flow_params[\"register_value\"]\n\t\tflow_params[\"register_value\"] = register_value[register_number]\n\treturn flow_params\n\ndef _get_flows_from_param_as_dict():\n\n\tfor flow_param in os.popen(\"ls flow/*\").readlines():\n\t\tsplitted = flow_param.split()\n\t\tflow_params = {}\n\t\tflow_params_key = splitted[0]\n\t\tflow_params[\"flow_param"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\treturn {register_value: register_number}   \n\t\ndef _copy_registers_flow(to_create):\t\"\"\"\n\tCopy flows from the flow_params to those flows of to_create object.\n\nTo_create object has flow_params as the first key\n\nReturns:\n    A dictionary with two keys:\n\tflow_params[register_number]\n\tTo_create object will have flow_params in these keys\n\nNote:\n\t\"The flow_params dictionary is automatically copy, if flow_params is used\n\tthe flow_params object is used"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif \"register_value\" in flow_params:\n\t    if flow_params['register_value'] in registers:\n\t\tflow_params['register_value'] = register_number\n\t\tself.set_flows(flowparams=flow_params, key=\"register_value\")\n\t\n\treturn register_value"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\tflow_params[('_registers'+register_number, register_value)] = register_value\n\t\nclass Flow(object):\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\t\n\tdef __call__(self, func):\n\t\t# Get a name corresponding to registered func instance\n\t\tname = None\n\t\t\n\t\tfor name, value in _register_registry[self.name]:\n\t\t\tif func == value:\n\t\t\t\tbreak\n\t\t\t\n\t\tif name is None:\n\t\t\traise ValueError(\"Can not find name for"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\treturn flow_params.get(register_number, register_value)\n\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor key, item in flow_params.iteritems():\n\t\tif item =='register_value':\n\t\t\tflow_params[key] = register_value\n\n\ndef _replace_defaults(flow_params, defaults):\n\t\"\"\"Replace 'default' keyword parameter in all flows with 'default'\"\"\"\n\tfor name, value in defaults.iteritems():\n\t\tflow_params[name] = value\n\n\ndef _calculate_value(flow_name, values_dict):\n\t\"\"\"Calculate the value of all given flow parameters\n\nThe flow is given by a"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\tglobal flow_params\n\tif not isinstance(flow_params, dict):\n\t\tflow_params = {\"key0\":flow_params}\n\treg_f = flow_params[\"key0\"].get_flow(register_number)\n\tfor key in reg_f.keys():\t#iterate through register values\n\t\treg_f.keys()[1] = register_value\n\t\tif key == register_value:\n\t\t\treturn reg_f\n\traise Exception(\"Register {} was not found in flow\".format(register_value))\n\n\ndef _get_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tcurrent_flow_val = {}\t\n\tfor key in flow_params:\n\t\tcurrent_flow_val[key] = flow_params[key]\n\t\n\t# Look for given value\n\t# Look for given registration number in the dictionary\n\t# Replace value from one that has been found\n\tfor k in register_value.keys():\t\n\t\tflow_params[k] = register_value[k]\n\t\n\treturn current_flow_val\n\t\t\ndef process_flow():\t\n\t\"\"\"\n\tReturn the dict representing 'flow_params' to be stored"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif flow_params is None:\n\t\traise Exception(\"Flow param(s) should not be None in _replace_register function\")\n\tif register_value is None:\n\t\traise Exception(\"Register value should be not None in _replace_register function\")\t\n\tfor key, value in list(flow_params.items()):\n\t\tflow_params[str(register_number)].update(value, key) \n\ndef _get_register_value(flow_params, register_name):\n\t\"\"\"Get flow value for given register_name\"\"\"\n\tfor flow_param in"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\treturn value.rstrip() + \"%\" + arg + \"\".rstrip()\n\nclass Test_ParseConfigFile(unittest.TestCase):\n\t\"\"\"Test case for the parse_config_file(...) function\"\"\"\n\n\tdef testInit(self):\t\"\"\"Test case to prevent infinite loops\"\"\"\n\t\tfile_name = self.getRandomName()\n\t\tvalue = \" \".join([\"foo.xml\", \"bar\", \"baz\", \"qux\"])\n\n\t\targs = []\n\t\targs.append([\"foo\", \"bar\"])\n\t\targs.append("}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\targ = arg.replace('.', '')\n\treturn [value, arg]\n\n#\n# Private stuff\n#\ndef strip_dots(value):\n\treturn '.'.join(x.strip() for x in value.strip('.').split(\".\"))\n\n#\n# Public stuff\n#\ndef get_ip_address(name_or_ip):\n\ttry:\n\t\treturn socket.inet_aton(name_or_ip).decode().split(\".\")[0].strip(\"$\").replace(\".\", \"_\")\n\texcept Exception:\n\t\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\n\tfor d in value.split(\".\"):\n\t\targ = arg.replace(d,\"\")\n\t\n\treturn arg\n\n\n\n\n\n\ndef build_numpy(func, *args, **kwargs):\n\tdef decorator(f):\n\t\t\"\"\" Returns function decorator for numpy.\"\"\"\n\n\t\t@wraps(f)\n\t\tdef fg(self):\n\t\t\t\"\"\" Executes the function if not function called.\"\"\"\n\t\t\treturn f(self, *args, **kwargs)\n\t\treturn fg\n\n\treturn decorator\n\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tnew = []\n\tm = re.fullmatch(r'\\.([a-z]\\w|\\b)\\b(.*)', value)\n\tn = len(value) - len(m.group(2))\n\n\tif n!= 0 and m.group(2) == m.group(3):\n\t\treturn new\n\tp = m.group(2).rpartition(n)\n\t\t\n\ttmp = new\n\tfor v in p:\n\t\ttmp.append(arg + v)\n\t\t\n\tfor b in tmp:\n\t\treturn b"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif value == '.':\traise ParseError(\"Incorrect use of '.' in substitution\")\n\tif isinstance(value, (str, unicode)):\treturn value.replace(\".\",\"\\\\\")\n\telif isinstance(value, int):\treturn value\n\telse:\n\t\traise ParseError(\"'{}' does not a type of {}\").format(value, object)\n\treturn value\n\n\ndef replace_numbers(value, arg):\n\tvalue = re.sub(r'[^\\.(\\w|)]+' + arg + r'"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tresult = value.replace(\".\", \"\")\n\treturn result\n\n"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tfor k, v in value.iteritems():\n\t\tif k.startswith('.'):\n\t\t\tvalue[k] = arg\n\treturn value\n## end\n"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tfor v in value.split(', '):\n\t\ttmp = re.sub('((.+))', '\\1' + v, arg)\n\t\tvalue = re.sub('\\.', '.' + v, value)\n\t\tvalue = re.sub('((\\*(\\[(.*)>\\]\\+?)\\)*)\\s(.*)', '\\1\\2', tmp)\n\n\treturn value\n\ndef find_first_value(value, arg):\n\t\"\"\"\n\tFind the first value of a given string that is equal to the given\n\tstring argument"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif value == '':\n\t\treturn arg\n\telse:\n\t\tnew_value = value.split('.')\n\t\tfor entry in new_value:\n\t\t\tentry = entry.split('_')\n\t\t\tnew_arg = arg.split('.')\n\t\t\tif len(entry)!= 0:\n\t\t\t\ttry:\n\t\t\t\t\tif len(new_arg) == 1:\n\t\t\t\t\t\treturn value.replace(entry[0], new_arg[0])\n\t\t\t\t\telse:\n\t\t\t\t\t\tres = ''\n\t\t\t\t\t\tif len(new_arg) > 1:\n\t\t\t\t\t\t\tres = '%s."}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\ts = value.split('.')\n\tret = \"\"\n\tfor i in range(len(s)):\n\t\tret += s[i] + '.'\n\treturn ret\nfor p in sys.argv:\n    o = {\n\t\"__doc__\" : p,\n\t#\"test_class\" : Test,\n\t\"__init__\" : __init__,\n\t#\"__init__\" : __init__,\n\t\"__init__\" : __init__,\n\t\"create\" : create,\n\t\"remove\" : remove,"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tseen=[]\n\tfor x in cls.__subclasses__():\n\t\tif x not in seen:\n\t\t\tseen.append(x)\n\t\t\tyield x\n\nif __name__=='__main__':\n\timport sys\n\tif len(sys.argv)>2:\n\t\tcls=None\n\t\tfor arg in sys.argv[1:]:\n\t\t\ttry:\n\t\t\t\tcls=getattr(sys.modules[arg],arg)\n\t\t\t\tif cls is None:\n\t\t\t\t\traise ImportError, 'class %s has no method %s"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\ts= cls.__subclasses__()\n\tif s:\n\t\treturn s\n\ts= []\n\tif not cls.__bases__:\n\t\treturn s\n\tfor b in cls.__bases__:\n\t\ts = subclasses(_wrap_bases(_subclass(b),s))\n\tif not s:\n\t\treturn s\t\n\tp= cls.__bases__[0]\n\treturn subclasses(p.__bases__) + s\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\treturn filter(lambda x: hasattr(x, '__subclasses__'), cls.__subclasses__())\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tsubclasses = []\n\tsubclasses.append(cls)\n\tsubclasses.extend(cls.subclasses())\n\treturn subclasses\n\n\ndef get_func_args(func):\n\targs = []\n\targv = list(func.func_code.co_varnames)\n\tfor name in argv:\n\t\targ = func.func_code.co_argcount[name]\n\t\tif isinstance(arg, (type(''), type(''), type(type('default'))-type(void), type(u'default'"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tdef _extend(cls):\n\t\tfor attr in dir(cls):\n\t\t\tif attr.startswith(\"__\"):\n\t\t\t\tcontinue\n\t\t\tc = getattr(cls,attr)\n\t\t\tif callable(c):\n\t\t\t\tsubclasses(c)\n\t\t\t\tc._extend(c)\n\t_extend(cls)\n\treturn cls\n\ndef _isMethod(f):\t\treturn callable(f) and f.__func__ in {'__call__', '__call_method__' }\ndef _isMethod2"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tcls = dir(cls)\n\n\tsubclasses = []\n\tfor name in cls:\n\t\tobj = getattr(cls, name, None)\n\t\tif ( not issubclass(obj, cls) ) and ( not obj.__name__.startswith((\"get_\" + obj.__name__)) ):\n\t\t\tsubclasses.append(obj)\n\treturn subclasses\n\nclass CustomException(Exception):\n\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\n\tdef __str__(self):\n\t\treturn"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\ta_cls = cls\n\tnew_cls = []\n\twhile True:\n\t\tnew_cls += a_cls.get_subclasses()\n\t\tif not new_cls: break\n\t\ta_cls = new_cls[0]\n\treturn new_cls\n\ndef main(): \t\n\td_path = \"/home_seattle_gens_data_courses_l2_lunarg\"\n\n\ttree = get_tree(root=\"/home_seattle_gens_data_courses_l2_"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tsubclasses = [ getattr(cls, a) for a in dir(cls) if not ( a[0] == '_' and a.startswith('__') ) ]\n\treturn subclasses\n\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tret=[cls]\n\tif cls not in ret:\tret.append(cls)\t\n\tfor x in cls.__subclasses__(): ret.append(x)\n\treturn ret\n\n\ndef get_superclass(cls):\n\t\"\"\"\n\tReturn the superclass class of a class\n\t\"\"\"\n\treturn getattr(cls, super_binder.__name__, None)\n\n\ndef get_class_desc(cls):\n\t\"\"\"\n\tReturn the class description if any, otherwise return the class itself\n\t\"\"\"\n\tif"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tsubclasses = set()\n\tsubclasses.update(cls.__subclasses__())\n\treturn [s for s in subclasses if not isinstance(s, cls)]\n\n\ndef instance_method(cls, method_name, *args, **kw):\n\t\"\"\"Return a method within a class matching a certain method name\"\"\"\n\tm = getattr(cls, method_name)\n\ttry:\n\t\treturn object.__getattribute__(m,*args, **kw)\n\texcept AttributeError:\n\t\traise get_method_error_if"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n\tdef _string_to_int(input: str, alphabet: List[str]) -> int:\n\t\t\"\"\"Convert a string to a number, using the given alphabet.\t\"\"\"\n\t\t\n\t\tp = set(input)\n\t\td = len(p)\n\t\tret = 0\n\n\t\t\n\t\tfor i in range(len(p)):\n\t\t\tif p[i] not in alphabet:\n\t\t\t\td = i\n\t\t\t\tbreak\n\t\t\n\t\tfor i in range(d):\n\t\t\tfor j in range(p[i]):\n\t\t\t\tret = ret * 10 +"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\"\"\"return int(alphabet[str_to_int(string, alphabet[0])])\"\"\"\n\ttry:\n\t\treturn int(string, 10)\n\t\treturn 0\n\texcept ValueError:\n\t\treturn alphabet[0][0]\n\treturn 0\n\n\n\r\n\r\n\r\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tif string is None:\n\t\treturn None\n\ttry:\treturn int(string, 10)\n\texcept ValueError as e:\traise ValueError(\n\t\t\"\"\"String %r is not an integer\"\"\" % string)\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tnumber = 0\n\tfor symbol in string.upper():\t\n\t\tnumber = 1 << (alphabet.index(symbol) + 1)\n\t\tif number > 1:\n\t\t\tbreak\t\n\treturn number"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\talphabet = alphabet or string.upper()\t\t# alphabet must be a string, not a list\n\toutput = 0\n\twhile len(alphabet) > 1:\n\t\tif string[-1] in alphabet: \t\t\t\t\t# add one-digit number to end\n\t\t\toutput_by_one = 0\n\t\t\tfor ch in alphabet[0:1]:\n\t\t\t\tif ch in string[:-1]:\n\t\t\t\t\tif output % 10 == 0:\n\t\t\t\t\t\toutput_by_one += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput_by_one += 1\t\t\t\t\t# not the last digit! or it's"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tbits_sent = 0 # bits_sent is the amount of bits that can be sent to the server.\n\tnum_bits = 0 # num_bits will contain the number of bits to allocate on the server.\n\t# print 'convert', string, num_bits\n\tfor i in range(2):\n\t\tif len(string) >= string[-i]+1:\n\t\t\tprefix = string[-i]\n\t\t\tif alphabet[prefix] == prefix:\n\t\t\t\tnum_bits += 1 + alphabet[prefix]\n\t\t\t\tprefix = 'L'\n\n\t\t\tprefix = 'P'\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\talphabet = alphabet[:]\n\tdigits = {int(digit) for digit in string}\n\tval = 0\n\twhile digits:\n\t\tval = val * 10 + digits.pop()\n\treturn val\n\ndef int_to_string(val: int, alphabet: List[str])->str:\n\t\"\"\"\n\tConvert an integer to a string, using the given alphabet.\n\t\"\"\"\n\talphabet = alphabet[:]\n\tstr = \"\"\n\tfor digit in val:\n\t\tstr += alphabet.index(digit)\n\treturn str\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tresult = 0\n\tfor symbol in alphabet:\n\t\tresult = int(string[len(string)-1] + string[:-1], alphab) * alphab[symbol]\n\t\tassert result >= 0\n\treturn result\n\t\ndef str2float(string: str) -> float:\n\t\"\"\"\n\tConvert a string into a float.\t\n\n\tstring2float(<str>) -> float\n\t\"\"\"\n    return float(string)\n\ndef str2int(string:str) -> int:\n\t\"\"\"\n\tConvert a string into an int"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tchar_index = 0\n\ttotal = 1\n\twhile char_index < len(string):\n\t\tchar = string[char_index]\n\t\tcurrent_len = len(alphabet)\n\t\tif char in alphabet:\n\t\t\ttotal += 1\n\t\telse:\n\t\t\tbreak\n\t\tfor i in range(current_len):\n\t\t\talphabet[char] = alphabet[alphabet[char]]\n\t\tchar_index -= 1\n\treturn total\n\ndef int_to_string(number: int) -> str:\n\t\"\"\"\n\tConvert an integer to a string, using"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\talphabet = alphabet[0] + alphabet[-1]\n\n\tpos = 0  # start at zero\n\tlength = len(string)\n\n\tinteger = 0\n\tletter = string[pos:pos+len(alphabet)]\n\twhile len(letter) > 0:\n\t\tfirst_digit = letter[0]\n\t\tposition = 0\n\t\twhile letterset_get(alphabet, first_digit)!= first_digit:\n\t\t\tletter.pop(0)\n\t\t\tletter.append(first_digit)\n\n\t\tletter = letter[:]\n\t\tletter.pop(0"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tpath,filename = os.path.split(destination_path)\n\tif not path:\n\t\treturn \"path doesnt exist\"\t\t\n\treturn os.path.join(path,filename))\n\t"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\ttar = tarfile.open(fileobj=fileobj)\n\tfor tarinfo in tar:\n\t\tif tarinfo.name == 'desc' :\n\t\t\ttar.extract(tarinfo)\n\n\treturn os.path.join(destination_path, 'desc.tar')\n\n\"\"\"\nGiven a source content path and a destination path that contains src path:\n\n1) Get tar from source content and parse the.tar.gz\n2) get each tar file with.tar.gz extension and extract it to destination path\n3) parse and extract and save directory with tar data\n"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tpass\n\t\"\"\"\n\nfrom pathlib import Path\nimport subprocess\nimport urllib.request\nimport gzip\n\n\ndef extract(url, destination_path):\n\tprint(\"Downloading {} to {}\".format(url, destination_path))\n\treq = urllib.request.Request(url)\n\tr = urllib.request.urlopen(req).read()\n\n\td = Path(destination_path)\n\td.mkdir(exist_ok=True, parents=True)\n\twith d.open('wb') as f:\n\t\t"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t# TODO: Figure out how to do this correctly, there are two different approaches. \n\t# \tThe first method will use a specific set of packages (which has to be there) in order\n\t# \tto be able to work for it. It also might be possible to use some sort of directory scanning;\n\t# \tin which case we might want to use something more powerful, but this may be a matter of\n\t# \tfact regarding memory usage.\n\t\n\t# TODO: Make this better as an external command or something. \n\t\n\t# TOD"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t# Download the URL and get the archive\n\tresp = requests.get(url)\t\n\t# Extract all the archive\n\twith open(destination_path, mode='wb+') as arch:\n\t\twith gzip.GzipFile(fileobj=resp.content) as f:\n\t\t\tfor i in iter(lambda: f.readline(), b''):\n\t\t\t\tarch.write((i.encode('utf-8')))\n\n\"\"\" ########################################################################### #\n\nA helper method which helps us write to a temp file. \n\nArgs:\n    file"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tprint(\"Downloading tar.gz archives.\"), \n\tprint('Downloaded from {}...'.format(url))\n\n\tr = requests.get(url)\n\tprint('Request took %.3f seconds' % (r.elapsed() * 1000))\n\twith open(destination_path,'wb') as f:\n\t\tf.write(r.content)\n\tprint(\"Archive successfully downloaded.\")\n\tprint(\"Tar.gz extraction took %.3f seconds\" % (r.elapsed() * 1000))\n\n\ndef extract_desc_tarfile(dest_"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tr = requests.get(url)\n\tresponse = r.content\n\ttar = tarfile.TarFile.open(mode=\"rb\")\n\tarchive = tar.extractall(path=destination_path)\n\ttar.close()\n\tfor path, d in archive:\n\t\tif str(d).name == 'desc':\n\t\t\tos.remove(path)\n\treturn os.path.realpath(destination_path)"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tr = requests.get(url)\n\tr.raise_for_status()\n\tdataPath = r.json()[\"raw\"]\n\twith open(dataPath, 'wb') as g:\n\t\tg.write(w.open(\".tar.gz\").getvalue().encode('utf-8'))\n\t\t\ndef download_archive(url, destination_path):\n\tr = requests.get(url)\n\tr.raise_for_status()\n\tr.json().update({'raw': w.open(\".tar.gz\").getvalue()})"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tif not os.path.exists(destination_path): os.makedirs(destination_path)\n\tprint(\"[INFO] Downloading '{url}' at '{destination}'.\".format(**locals()))\n\tcurl_request = 'curl {url} --header \"Referer: https://archive.org/index.htm?desc={desc}\"' \\\n\t\t.format(**locals())\n\tfile_destinations = [destination_path, 'archive']\n\tfor file_destination in file_destinations:"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tr = requests.get(url).json()\n\tdir = os.path.dirname(os.path.expanduser(destination_path))\n\ttry:\n\t\tos.mkdir(dir)\t\t\t\n\texcept OSError as e:\t\n\t\tif e.errno!= 17:\t\t\n\t\t\traise\t\t\n\tfor pkg in r['packages']:\n\t\tpkg['name'] = pkg['name'].split('/', 1)[-1]\t# get the package name\n\t\tfor repo in r['repos']:\n\t\t\tif repo"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\treturn os.system(\"uname -n\") == \"Darwin\"\n\n\nif __name__ == \"__main__\":\n\tmain()"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tlibs = os.environ.get(\"LD_LIBRARY_PATH\").split(\":\")\n\tsys.modules[\"os\"] = os\n\tif 'MACH_SUBTYPE_X86_64' in libs:\n\t\tenabled = False\n\telse:\n\t\tenabled = True\n\treturn enabled\n\n# Check if the mac os is installed\n@ignore_case\ndef isMac():\n\treturn check_env('MACOSX')"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tglobal __VERSION__\n\tif not len(sys.argv) < 2 : return False\n\tversion = getenv('HOME') + os.sep + sys.argv[1] + os.sep + __VERSION__\n\n\tif sys.platform == 'darwin' : return False\n\tif os.path.exists(version) : return True\n\treturn False\n\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\tif False not in (os.name == 'nt' or os.name == 'posix'):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef check_macos_unix(filename):\n\t\"\"\"\n\tChecks if the file is a MAC OS UNIX file\n\n:return: True if file is MAC OS UNIX file\n\t:rtype: bool\t\"\"\"\n\n\tif os.name == 'nt':\n\t\treturn True\n\telif os.name == 'posix':\n\t\treturn True\n\telse:\n\t\treturn False\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\ttry:\n\t\tif sys.platform=='win32':\n\t\t\treturn True\n\t\t\t\n\t\telse:\n\t\t\treturn False\n\n\nclass LinuxNix(NativeApp):\n\t\n\tdef init(self):\n\t\tprint(\"LinuxNix init\")\n\n\tdef getDefaultAppPath(self):\n\t\tpath = \"\"\n\t\treturn path\n\n\tdef checkPath(self):\n\t\tpath = \"\"\n\t\treturn path\n\n\tdef run(self):\n\t\t\"\"\"\n\t\t:return: True if success, False if failed\n\t\t:rtype: bool\t\"\"\"\n\t\ttry:\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif sys.platform == 'darwin':\n\t\tf = os.popen(\"uname -b\")\n\t\tif len(str(f.readline()).strip()):\n\t\t\treturn True\t\n\t\treturn False\n\treturn False\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif os.system(COMMAND)!= 0:\n\t\tprint \"Mac OS is apparently not installed, and you don't have sufficient privileges.\\n\"\n\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn os.system('/usr/bin/which kivy --platform sdl 2.0') == 0\n\n##\n\"\"\"\nCheck if we can run OSX and Mac OS X\n\n:return: True if the OS is Mac OS or not\n:rtype: bool\t\"\"\"\n\ttry:\n\t\timport subprocess\n\t\tisMacosx = True\n\texcept ImportError:\n\t\tprint('ImportError: could not import Subprocess module')\n\nelse:\n\tprint('OS not MACOS')\n\tisMacosx = False\n\ntry:\n\tfrom"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn sys.platform=='darwin'\n\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\tif platform.system().lower() == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef check_output(commandline):\n\t\"\"\"\n\tRuns a CommandLine-like executable and checks for errors. Returns output\n\n:param commandline: CommandLine to be able to call an executable\n:type commandline: list\n:return: Output of the commandline command\n:rtype: string\n\t\"\"\"\n\t\n\tlog.debug(commandline)\n\t\n\tcmd_output = None\n\tif is_mac:\n\t\tcmd"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\trule_list, re_patterns = build_rules(item)\n\trule_list = '|'.join(rule_list)\n\n\tif(not re_patterns):\n\t\tregexes = {}\n\t\treturn_dict.append(rule_list)\n\t\trule_key_list = list()\n\t\treturn_dict.append(rule_key_list)\n\n\telse:\n\t\twith open(resolve_dict_file, 'r') as f_in_temp:\n\t\t\twith open(resolve_dict_file, 'w') as f_out"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\treturn item\n\ndef regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to their corresponding regex keys.\n\n        Args:\n\t    a: dict where the keys are all filenames with wildcards, convert only\n        the keys into equivalent regexes and leave the values intact.\n\tReturns:\n\t    dic with keys converted to regexes\n\tArgs:\n\t    a: dict where the keys are all filenames with wildcards, convert only\n\t    the keys into equivalent regexes and left with '*' to indicate \"all\" wildcards\n\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\t\n\t\n\t\t\t\n\t\n\tconverters = dict()\n\t\t\t\t\n\t\n\tfor key in value.keys():\n        \n\t\tif not key.endswith(conversion_extension_str):\n\t\t\tkey = key[0:-1] + conversion_extension_str\n\t\telif key.endswith(conversion_extension_str):\n\t\t\tkey = key[0:-2] + conversion_extension_str\n\n\t\tconverters[key] = value[key]\n\t\t\t\n\treturn (converters)\t\n\t\n\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t# Get regular expressions from the dict.\n\tregex_res = [r'^{}*$'.format(re.escape(k)) for k in item.keys()\n\t\t\tif re.search(r'[^\\.]*$', k)]\n\n\t# Convert regular expressions.\n\tfor r in regex_res:\n\t\titem[re.escape(r)] = r \n\n\t# Create rules dict from converted dict.\n\trules = {}\n\tfor k in item.keys():\n\t\tkeys = regex_keys.get(re.escape(k), None)\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\treturn_dict = []\n\tregex_fns = []\n\tfor key in item:\n\t\tregex = re.compile(key)\n\t\tresult = regex_keys[key]\n\t\treturn_dict.append((key, result))\n\t\tregex_fns.append(re.compile(result))\n\treturn return_dict, regex_fns\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tdict_to_list = list()\n\tfor key in item:\n\t\tlist_ = item[key]\n\t\tdict_to_list.append(dict_to_regex(list_))\t# convert only those strings\n\treturn dict_to_list\n\"\"\"\ndict = {\n    '*.cpp':\n['arf', 'bark', 'coo'],\n    '*.h':\n['help'],\n}\nregex_keys = regex_dict(dict)\n\"\"\"\n\ndef dictionary_to_list(item):\n\t\"\"\"Convert dict to"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\t# build the key -> text pairs\n\t\n\ttexts = listdir(item['path'])\n\tregexes = dict()\n\tfor filename in texts:\n\n\t\t# check the extension, can happen with.cpp files\n\t\t\n\t\tfile_extension = filename[fos(filename).lower().endswith('.cpp')].lower()\n\t\t\t\n\t\tkey = (file_extension,os.path.basename(filename))\t\n\t\tregexes[key] = text\n\n\treturn (regexes, rule_dict)\n\n\n\nif"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\treordered =  set()\n\tfor file_ in sorted(item:item.keys()):\n\t\treordered.add(re.escape(regex_keys[file_].replace('*', '.*')))\n\trules = {\n\t\trule:dict(keys=reordered)\n                for key, value in rules.items()\n\t}\n        return rules\n\t\n\t\"\"\"\n\tConvert *.css keys to regex keys\n\nArgs:\n    item: dict with keys\nReturns:\n    list of all *keys* converted from regex keys\n\nExample:\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\treturn_dict=False\n\tfor key in item:\n\t\tif key in rules:\n\t\t\treturn_dict=True\n\t\t\tregex= rules[key]\n\t\t\tfor val in regex:\n\t\t\t\tkey= key.replace(\"\\\\.\",\"\\\\\\\\\")\n\t\t\t\tre_value=re.compile(val)\n\t\t\t\tre_keys = re_value.findall(val)\n\t\t\t\titem[key]= re_keys\n\t\t\treturn_dict=True\n\t\t\tcontinue\t\n\n\ndef rule_dict_to_regex(dict_to_match):\n\n\tre_keys  = []"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tc = re.compile('(?<=[^\\/\\\\]|\\/){0,1}(?=[^\\/\\\\]|\\/){0,1}(?=[^\\/\\\\]|\\/)', flags = re.MULTILINE | re.UNICODE)\n\trules = {}\n\tfor k,v in item.items():\n\t\trules[re.split('{',k + '|}', flags = re.UNICODE)[-1]] = v\n\treturn rules, c.sub(re.sub('{0,1}(?=["}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tname = re.sub(r\"\\\"\", \"\", name, flags=re.UNICODE | re.MULTILINE | re.IGNORECASE)\n\ndef GetNameFromURL(url):\n\t\"\"\"Converts a URL to a valid file name.\n\n\t@type url: str\n\t@param url: A URI or a local file on the server\n\t@return: The file name as a string\n\t\"\"\"\n\tif re.search(r\"'\", url):\n\t\treturn url\n\telse:\n\t\treturn os.path.basename(urllib"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tret = re.sub(quote, '', name)\n\treturn ret\n\n##############################################################################\n##########################   Global Setup   ########################\n\t\"\"\"(Set up the global variables, such as\n\t\t`use_loglevel` and `enable_loglevel`).\n\t\"\"\"\n\n\tuse_loglevel = False\n\tenable_loglevel = False\n\n\t# We could use these to figure out whether to use loglevel.\n\t# TODO: Should support multiple ways...\n\timport loglevel as loglevel\n\tuse_logle"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef __init__(self, line):\n\t\tre_remove_quote = re.compile(\"(\\\\'|\\/|\\\\.|\\\\$|:|\\,|-|'|\\/|\\\\|\\\\-|\\\\!|\\\\[|\\\\]|\\\\{|\\\\^|~|;|~|\\\").*$\")\n\t\t\n\t\tself.match = re_remove_quote.finditer(line)\n\t\tself.removed = set()\n\t\tself.re_find = re.compile('\\[(?<=\\\\\\')((\\$"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n\tname=name.replace('\"','')\n\n\n\t\"\"\"\n\tif isinstance(name,bytes):\n\t\tif re.match(regex,name.decode()):\n\t\t\tname = name.replace(regex,'')\n\n\t\treturn name\n\telse:\n\t\tif re.match(regex,name):\n\t\t\tname = name.replace(regex,'')\n\t\treturn name\n\n\ndef _stripQuotes(input):\n\tinput=input.replace(\"\\'\",\"''\").replace('\"','')\n\treturn(input)\n\n\ndef"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tfrom os import chdir\t\"\"\"\n\tif not (filename):\n\t\tprint \"Unable to detect the name file!\"\n\t\treturn False\n\tfilename = chdir(os.path.abspath(filename))\t\"\"\"\n\t#remove quotes if the name starts with the \"\\\"\n\tif name.startswith('\"') and name.endswith('\"'):\n\t\tname = re.sub(\"^\\\"|\\\"$\",\"\"\",name)\n\t\tname = name.strip()\"\"\"\n\t\n\tfilename = os.rename(filename,filename+\".untranslated"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tn = re.compile(r'{0,N}?[-+.*]?['+n2d+']*'.format(n), re.I)"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n\treturn r.sub(\"-\", s)\n\t\ndef split(string): #This one is a little weird\n\t\"\"\"Split an input string by a delimiter.\n\n\tReturns a tuple of the split string and each sub-string.\t\"\"\"\n\t\n\ttokens = re.split(r\",\")\n\ttokens = list(tokens)\n\treturn tokens\n\nif __name__ == '__main__':\n\tfor tt in sys.argv[1:]:\n\t\tif len(re.split(r\",+\", tt))"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tname = Name.strip_quotes(tokenize(name))\n\treturn name.split(\"#\")[0].split(\"@\")[0]\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n\ts = t0\n\ts = re.sub(\"'\", '', s)\n\tfor r in re.findall(r\"'\", s): s = s.replace(r,\"\")\n\tres = re.sub(r\"![0-9]\", r\"\", res)\n\tres = re.sub(r\"![a-z]\", r\"\", res)\n\tres = re.sub(r\"_|-|\\.|[:;]|\\.|\\\\.\", \"\", res)\n\treturn (s + \"!\")\t\n\nif __"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tif aClass.getName() == \"unnamed\":\n\t\treturn \" \"\n\n\tif re.match(\"a\\b\", name):\n\t\t# It is assumed the name is already stripped, so just remove the\n\t\t# leading back to avoid some of the name matching magic\n\t\tname = name[1:]\n\t\treturn \"_\" + name\n\n\tif re.match(\"^\\b\"+name+\"\\b$\", aClass.getName(), re.I):\n\t\treturn None\t\t\t\t# don't give up if the class name has\n\t\t\t\t\t\t\t\t\t# a leading \"*\" on a"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\t# Command line split.\n\tline_splitter = re.compile(re_line_splitter, flags=re.MULTILINE)\n\n\tif argv:\n\t\t# Command line string.\n\t\tcommand =''.join(argv)\n\telse:\n\t\t# Default command (shell).\n\t\tcommand ='sh'\n\n\t# Get platform.\n\tplatform = platform.system()\n\tif platform == 'Windows':\n\t\tcommand = platform + '/' + command\n\t# Try to find command in list of commands.\n\tfor c in commands"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\t# Return an equivalent list\n\tsplit = (platforms,re.compile('['+lst+']'),re.compile(''))\n\tres=\"(\"+','.join(split)\n\treturn res\n\t\ndef get_cwd():\n\t\"\"\"\n\t@return: CWD\n\t@rtype: str\t\"\"\"\n\timport os\t\"\"\" Get CWD\n\treturn os.path.realpath(os.curdir).replace('/','\\\\') + '\\n'\n\ndef find_files(dst_dir,ext_"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tdef shlex_split(line):\n\t\treturn re.split(pattern, line)\t\"\"\"line\n\n\tdef run(cmd):\n\t\t\"\"\"Run command\"\"\"\n\t\tp = subprocess.Popen([cmd, \" \"], stdout=open(os.devnull, \"w\"), stderr=subprocess.PIPE)\n\t\tp.wait()\t\t\t\"\"\"wait for the process to finish\"\"\"\n\t\tsts = p.terminate()\t\"\"\"stop the process from waiting on the output pipes\"\"\"\n\n\tshlex = list(shlex_split(input))\t# parse the"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\tcmd = re.compile(cmd_str,re.MULTILINE)\n\t#re.compiled = re.COMMODELED\n\t#if not re.match:\tre.compiled = None # we dont want to use the re compiled\n\t\n\tcmd_len = len(cmd)\n\t# for each command line piece of text in the string, split it at the separator\n\toutput = []\n\t\n\tcurrent_cmd = cmd[0:-cmd_len]\n\t# for each piece of text in the command line with separator matching"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tcmd = re.split(command_regex, cstr)\n\treturn cmd\n\ndef _get_name_from_cmdline(cmdline):\n\t\"\"\"\n\tHelper to call cmdline method with fast and slow re.split().\n\n\tReturns 1 if name is invalid.\n\tReturns 0 if name doesn't match command regex.\n\n\tReturns 0,0 if name matches and cmdline doesn't.\n\n\t\"\"\"\n\n\ttry:\n\t\treturn re.match(command_regex, cmdline).span()[1]\n\texcept AttributeError:\n\t\treturn match_"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tversion = int(os.getenv('PYTHONVOTE_VERSION') or 'UNKNOWN')\n\n\n\tdef _do_split(self, line, _dont_split=True):\n\t\tif not _dont_split:\n\t\t\tp = re.compile(r\"(^|)\\|\\|\")\n\t\t\tl = map(p.sub, re.split(r\"(^|\\\\|)|(\\b(!)|\\b(^|\\\\|))\", line))\n\t\t\treturn [i+l for i in (self[key] for key in list"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\treturn shlex.split(s)\t\"\"\"\n"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tif not re.match(r'([a-zA-Z0-9]+(?: -[a-zA-Z0-9]+)*)', shell):\n\t    raise ValueError, \"unrecognized shell spec: %s\" % shell\n\t(cmd, shell) = re.match(r'([a-zA-Z0-9]+)(?:\\s+([\\w_\\*+\\.\\(\\)\\-@:%~]+))?((?:this)? -[a-zA-Z0-9]*)', shell).groups()"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tp = re.compile(line, flags)\n\tfor l in source:\n\t\tp = re.compile(line, flags)\n\t\tif p.search(l):\n\t\t\tparts.append(p.split()) # TODO move this function to regex\n\treturn parts"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tfrom os.path import join\n\n\tplatforms = {'this' : r'(this|%s)' % platforms}\n\n\t# Regexp for finding the name\n\t# This is a slightly more restrictive version of the regexp\n\t# This regexp ignores '%' character\n\tnamere = Re.compile(\"\"\"\n\t^([A-Z])([0-9]+)$                                         # platform #0-4\n\t+([A-Z][0-9]+) +: {platform} {platforms}                       # 0-5\n\t("}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t# If this process fails, we'll try next step.\n\t# Note: on failure, this just calls the original method.\n\tdef is_archive_path(archive_path):\n\t\treturn True\n\n\t# Compress the given archive.  If an archive doesn't exist, return nothing.  If we find this is an invalid format, we'll go on to the next step.\n\tdef compress(archive_path) -> subprocess.CalledProcessOutput:\n\t\ttry:\n\t\t\tsubprocess.call([path.join(zip, f\"{zip}{archive_path}\")."}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tfile_repo_url = \"\"\n\tif archive_path==\"\" or os.path.isdir(archive_path)==False:\n\t\tfile_repo_url = \"pypi/\" + url+\".git\"\n\telse:\n\t\tfile_repo_url = archive_path\n\treturn file_repo_url\n\ndef check_file_repo_url(file_repo_url):\n\t\"\"\"\n\tIf the input value is not a file repo url, throws an exception to alert the user why. \n\tOtherwise, returns the file"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tfrom glob import glob\n\tfrom io import FileIO\n\tfrom os import mkdir;\n\tfrom os.path import exists, basename\n\timport zlib\n\timport bz2\n\n\tarchive_path = None\n\tf = None\n\t# if it exists, it must be a file system, so check its name\n\tif exists(archive_path):\n\t\tbio = FileIO(archive_path,[\"]\")\n\t\tbio.close()\n\t\tbz2.BZ2File = file.read\n\n\t\tf = open(archive_path, \"r+"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\twith ctx.migration.transaction() as current_trans:\n\t\n\t\tdef to_path(remote):\n\t\t\treturn os.path.join(remote_path, os.path.basename(remote))\n\t\t\n\t\t# the archive contents file is always in /var/www/Archive/Archive.txt\n\t\tcurrent_path = to_path(current_trans.archive_path)\n\t\tcurrent_trans.archive_src = [current_path]\n\t\n\t\tcmd = [ 'git', 'config', 'dirstore' ]\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t# FIXME: implement an interface!\n\n\t# this is where we call the archive to un-archive\n\tsub_cmd = None\n\tsub_cmd_params = None\n\tsub_cmd_result = None\n\n\tsub_cmd_params = {}\n\tsub_cmd_params['archive_path'] = archive_path\n\tsub_cmd_params['archive_path_exists'] = subprocess.call(cmd1, shell=True, cwd=path_exists, # FIXME: should this be executed by the process?\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprocess_"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tuncompressed_path=subprocess.output  # will be the archive path\n\tif os.path.exists(uncompressed_path)  == True:\n\t\tprint('Found a new uncompression')\n\t\treturn uncompressed_path\n\telse:  \t\t\t\t\t\t# If an existing archive does not exist, then return None\n\t\treturn None\ndef uncompress_archive(archive_path):\n\t#Uncompress a plain archive path.\n\tcmd=[\"unzip\",archive_path,\"-cr\",]\n\t#cmd=[\"unzip\",archive_path,\"-"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tarchive_root = os.path.join(archive_path, \"..\")\n\tif not os.path.isdir(archive_root):\n\t\tshutil.rmtree(archive_root)\n\tcmd = \"gunzip -q '\"+archive_path+\"' | tron --extract -q --format json \"\n\tp = subprocess.Popen(cmd,stdout=subprocess.PIPE)\n\tp.wait()\n\tif not os.path.isdir(archive_root):\n\t\tp.terminate()\n\t\tsys."}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tarchive_path = file_path_to_url(archive_path)\n\tif not os.path.isfile(archive_path):\n\t\traise ArchiveDoesNotExistError(archive_path)\n\n\trepo_path = os.path.join(path, repo_name)\n\ttry:\n\t\tstatus, output = subprocess.call([\"zsh\", \"archive\", \"-\" + archive_path, repo_path], stdout=subprocess.PIPE)\n\t\tassert(status == 0)\n\t\tlog.debug(\"Found repo at: %s\""}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tdef run(cmd):\n\t\tcmd = subprocess.list2cmdline(cmd)\n\t\treturn subprocess.check_output(cmd.split(' '), stderr=subprocess.STDOUT, shell=True)\n\tdef run2(cmd):\n\t\tcmd = subprocess.list2cmdline(cmd)\n\t\treturn subprocess.check_output(cmd.split(' '), stderr=subprocess.STDOUT, shell=True)\n\ttry:\n\t\tout = run(['git', 'clean', '-f', 'HEAD'])\n\t\tout = run"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tif (archive_path, output_file) not in data: # This has to be done before it can be used as the output URL\n\t\traise ArchiveNotPresentError(\"The archive_path ({0}) does not have an associated {1}\".format(archive_path, output_file))\n\tdata[archive_path][\"file_name\"] = output_file # Used as the file_name (used for the URL)\n\t#print(data[archive_path])\n\t#output_url = None\n\t#output_size = None\n\t#with zipfile."}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tfname = fname.split(',') if fname else list()\n\tdirs = [item[0] for item in fdir.files() if item.startswith('.')]\t# use the same dir as'main' for'main'\n\tdirs = [dirs[0]]\n\tsubprocess.call(['git', 'ls-tree'], stdout=subprocess.PIPE)\n\ttree = subprocess.PIPE\n\tfor name in dirs:\n\t\tretry_after = \"20000\"\n\t\tf = open"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\t# Run the git command\n\twith subprocess.Popen(cmd_list + [\"status\"], stdout=subprocess.PIPE) as process:\t\n\t\t#print(\"List of files\\n\", process.stdout.readlines())\n\t\tprocess.stdout.close()\n\t\tstdout = process.stdout\n\t\treturn stdout, process.returncode\t\t\t\n\ndef get_all_files(git_out):\n\t\"\"\" Takes a list of stdout from an already known git command that return only the files. \n\tReturns an array of (filename"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tignored = ['__*', '__built-in', '__built-in.h', '__built-in.inc', '__cplusplus', '__cplusplus.h', '__declspec.attribute', '__asm', '__asmf.inc', '__asmf.pch', '__include.inc', '__math_fwd', '__math_fwd.inc', '__objcopy.h', '__math_fwd.h', '__fwd.map']\n\n\t# Execute the command.  \n\tcommand = '"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tfmt = \"cat %s& %s\"\t# Create a new list for files to ignore\n\tignore = [\".git\"]\t# List of filename files not to ignore\n\tignore_regex = re.compile(\".gitignore\")\n\n\tfor filename in ignore:\n\t\tfilename = os.path.basename(filename)\t# Take only basename\n\t\ttry:\n\t\t\tnew_list = subprocess.check_output([\"git\", \"ls-files\", \"-z\", filename], cwd=dir).splitlines()\n\t\t\tnew_list = [new_list"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tlog.info(\"Starting the git command for '%s'\", subcmd)\n\tfiles = list()\n\tfiles.append(\"~/.git\")\n\tfiles = sorted(files)\n\tfor fn in files:\n\t\tlog.verbose(\"Scanning '%s'\", fn)\n\t\tif not fn.endswith(\".git\"):\n\t\t\tlog.debug(\"Path has '.' as the extension - skipping\")\t\t\t\n\t\t\tcontinue\n\t\tfn = fn[:-3] #remove extension extension from path\n\t\tlog.debug(\"Found file %s\" % (fn))"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t#print(r\"\"\"\n\t#import csv\n\t#import os\n\t#subprocess.call(\"$(git fetch && git rev-parse HEAD:1)\", shell=True)\n\t#file, dir = file + dir, file + \"/ignored_files/\"\n\t#print(\",\".join(dir.split(\"/\")) + \",\".join(dir.split(\"/\")[0:-1]))\n\t#file, dir = file + dir, file + \"/ignored_files\" + dir\n\t#print(\",\".join(dir.split(\"/\""}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t# check if it's in repository, if it is not, do not use the 'git help'\n\tif not os.path.exists(repository[0]+\"/.git\"):\n\t\tcommand = \"git --version\"\n\t\tif platform.system() == \"Windows\":\n\t\t\tcwd = os.path.split(__file__)[0]\n\t\t\tcommand = command.replace(\"\\\\\", \"/\")\n\t\t\tcommand = command.replace(\" \", \"\")\n\t\t\texecfile(cwd+\"/repository.txt\", {'repository' : command },"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tfilenames = [\"{0}.log\".format(f) for f in os.listdir(\"./logs/\") if \"log\" not in f] # list directory and make glob for ignored files\n\tfilenames.sort(axis=1)\n\treturn filenames\n\ndef merge_logs(base_dir):\n\t\"\"\"\"\"\"\n\tfor f in os.listdir(\"{0}/logs\".format(base_dir)):\n\t\tif os.path.basename(f).split(\".log\")[0]!= \"cmd-"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\t\n\tdummy = False\n\t\n\tif opts.bom:\n\t\tdummy = False\n\t\tcwd = os.getcwd()\n\t\tsys.path.insert(0,cwd+'/git/git')\n\t\n\tgitClient = Git(git_url=opts.git_url, username=opts.git_username, password=opts.git_password, \n\t\tworktree=opts.work_tree, dummy=dummy)\n\t\n\tpipeline = Pipeline(opts.taskdir)\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\targlist = getarguments()\n\tret =  arglist[1]\n\tret = getargstring(ret)\n\t\n\t# return the file names as a single string with each filename, separated by the ','\n\tretlist = ret.split('\\n')\n\treturn  retlist[0], [retlist[1], retlist[2][:-1]]\n\n\t\t\t\n\nif __name__ == '__main__':\n\tmain()\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\treturn (filename.upper() in ('MULTIROTOR.FMT',\n\t\t\t\t\t\t\t\t\t\t\t'MULTIROTOR.FMT.DAT'))\n\n@pytest.fixture(params=os.all_files())\ndef os_module(request):\n\t\"\"\"Return a test module based on a single test file.\"\"\"\n\t\n\treturn {'path': os.path.join(os.path.dirname(request.module.__file__), 'testfiles', request.param)}\n\n@pytest.fixture(params=MULT"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif  fileExists(filename):\n\t\treturn True\n\telse:\n\t\treturn False\n\n##########################\n##### MISC UTILS\n##########################\n\ndef generateID():\n\treturn 'a' + str(1000000000 * random.randrange(100000000000))[2:]\n\ndef getDir(path):\n\treturn os.path.splitdrive(path)[1].split('/')[0]\n\ndef openURI(uri):\n\ttry:\n\t\timport urllib_request\n\t\t#print urllib_request.Request()."}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t# Match for path, dir and file\n\t\n\treturn filename!= 'not_this_module' and filename.endswith('.py')\n\t# No match\n\ndef getContent(filename):\n\t\"\"\"Get contents of a Python file \n\nArgs:\n\t   Filename: Filename to load content for\nReturns:\n\t    Contents(String): Content of file or nothing if unknown\"\"\"\n\t\n\ttry:\n\t\tf = open(filename, 'r')\t# Read from file\n\t\treturn f.readlines()\t# Read content of file\t\n\texcept"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\tif not os.path.isdir(filename)\t:\n\t\treturn False\n\t\n\tfor file in list(os.listdir(filename)):\n\t\tif not file.startswith('_') and not file.endswith('/') and ('.dat' in file or '.html' in file):\n\t\t\treturn True\n\treturn False\n\ndef getSubmodules(filename):\n\t\"\"\"\n\tGet List of submodules\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    Submodule list of file\n\t\"\"\"\n\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\tif os.path.isdir( filename ):\n                ext = os.path.splitext(filename)[1]\n                if extension!= \"\" and extension in self.EXTENSIONS:\n                  return True\n\t\telif ext == \".h\":\n\t\t        return True\n\t\telse:\n\t\t        return False\n\telse:\n                if extension!= \"\" and extension in self.EXTENSIONS:\n                          return True\n\t        else:\n                          return False\n#enddef\n\n\ndef compile(cFile):\n\t\"\"\"Convert a file to a string that can be compiled"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif not os.path.isfile(filename):\n\t\treturn False\t\n\telif '.' in os.path.basename(filename):\n\t\treturn True\n\telse:\n\t\treturn False\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tcheck = 0\t\t\"\"\"Does this file type implement this interface?\n\t\"\"\"\n\n\n\tif support:\n\t\tif (check == 0 and module.isType(filename)) \n\t\t\treturn True\n\t\tif (check == 1 and not module.isType(filename)) \n\t\t\treturn False\n\t\treturn None\n\telse:\n\t\traise ValueError\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\tif sys.platform.startswith('java'):\n\t\tif os.path.isfile(filename):\n\t\t\tif filename.endswith(\".class\") or filename.endswith(\".java\"):\n\t\t\t\treturn True # supported\n\telse:\n\t\tif filename.endswith(\".class\") or filename.endswith(\".jar\"):\n\t\t\treturn True # supported\n\t\n\treturn False # Not a supported extension\n\n#------------------------------------------------------------------------\t\ndef run_cmd( args ):\n\t\"\"\"Run a bash command asynchronously in a separate thread\"\"\""}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\t\"\"\"\n\tcheck_is_type(*[filename.id for filename in filenames])\n\t\"\"\"\n\ndef check_is_type(*filenames):\n\t\"\"\"Check if filename is a type that this module supports\"\"\"\n\t\n\t\"\"\"\n\tCheck filename length\n\t\"\"\"\n\t\t\n\tfile_num = len(filenames)\n\n\t\"\"\"\n\tCheck filename length\n\t\"\"\"\n\t\n\tif len(filenames) < 6:\n\t\traise AttributeError(\"not enough files for this module. Check the number of filenames for the modules"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\t\n\tif os.path.isfile(filename):\n\t\t\n\t\tpath = os.path.dirname(filename)\n\t\t\n\t\tif not path == \"\":\n\t\t\t\n\t\t\tif path not in modules:\n\t\t\t\t\n\t\t\t\treturn(False)\n\t\t\t\t\n\t\telse:\n\t\t\treturn(True)\n\t\t\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tunits = None\n\tf = time.strptime(frequency)\n\tdelta = f.tm_wday * (60 * 60) + f.tm_mday * (60 * 60) + f.tm_year * (24 * 60 * 60) + f.tm_hour * (60) + f.tm_min * (60) + f.tm_sec\n\tif units == \"never\":\n\t\ttry:\n\t\t\tif not units or units == \"infinite\":\n\t\t\t\tdelta -= datetime.calendar.rrule.infinite\n\t\texcept"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t# http://docs.python.org/lib/datetime.html#duration-formatting\n\tf = {\n\t\t\"Y\": \"years\",\n\t\t\"M\": \"months\",\n\t\t\"W\": \"weeks\",\n\t\t\"D\": \"days\",\n\t\t\"H\": \"hours\",\n\t\t\"M\": \"minutes\",\n\t\t\"S\": \"seconds\",\n\t}\n\tif self.frequency in f.keys():\n\t\treturn datetime.timedelta(**time_dict[self.frequency](self)[2:])\n\telse:\n\t  "}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif (frequency is None) or (frequency.lower() == 'never') or (frequency == 'always'):\n\t\treturn None\n\tif (frequency.lower() in self.freqs) and self.freqs[frequency.lower():]:\n\t\tif self.unitless:\n\t\t\tfreq = self.freqs[frequency.lower()]\t\t\t\n\t\telse:\n\t\t\tfreq = self.freqs[frequency.lower()][0]\n\t\treturn datetime.timedelta(weeks=freq)/7.0\t# make a delta of one week, or the"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif freq_start is None:\n\t\tfreq_start=0\n\tif freq_end is None:\n\t\tfreq_end=0\n\ttry:\n\t\tdt=parser.time(str(freq_start), str(freq_end))   \n\t\treturn datetime.timedelta(dt)\n\texcept ValueError as ve:\n\t\traise ValueError(str(ve))\n\n\ndef _weekday(freq_start=None, freq_end=None):\n\t\"\"\"\n\tGiven an integer starting index of a weekday, return an"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif frequency not in \"MdyHhms\":\n\t\traise ValueError(\"You may not provide a %s\" % (frequency,))\n\tif not formatin:\n\t\treturn None\t \t\n\tval = getattr(cal, frequency)()\n\tif val == None:\n\t\treturn None\t\t \t\n\tfmt = val.strftime(formatin)\n\tif fmt == \"\":\n\t\treturn datetime.timedelta(0)\n\telse:\t\t \t\n\t\treturn datetime.timedelta(fmt)\t\n\t\ndef weekday"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif freq==None:\n\t\traise ValueError(\"frequency must not be None.\")\n\tif freq not in self._valid_freq_ids:\n\t\traise ValueError(\"frequency %r not included in the list\" % freq)\n\tt=None\n\ttry:\n\t\tt=float(freq)\n\texcept:\n\t\traise ValueError(\"invalid value for frequency %r\" % freq)\n\treturn datetime.timedelta(**{self._valid_freq_keys[freq]:t})\n\ndef from_iso8601("}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\ttry:\n\t\tf = int(frequency)\n\texcept ValueError:\n\t\traise ValueError(\"frequency must be an integer\")\n\treturn timedelta((f - 1) * 24 * 3600 * 1000 # (every day,)\n\t               + datetime.timedelta(weeks = f / 24 * 7))\n\t\t\t\t\n\ndef get_first_free_weekday(freq=None, unit_of_time=None):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return the\ndatetime.weekday of the first week before"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t# TODO: add unit tests to this function\n\tif frequency.isdigit():\n\t\tf0 = frequency[0]\n\t\tf1 = frequency[1]\n\t\tf2 = frequency[2:]\n\t\tfraction = frequency.index(\"/\") + 1\n\t\tbase = frequency.index(\".\")\n\t\tif fraction == 0:\n\t\t\tif int(f1) == 10:\n\t\t\t\tfraction = 1\n\t\t\telse:\n\t\t\t\tfraction = 0\n\t\tif f1 == '*':\n\t\t\tfor v in [1,2,3]:\n\t\t\t\tvalue = int"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\treturn None if ((frequency is None) or (isinstance(frequency, basestring))) else datetime.timedelta(**frequency)\n\ndef frequency_string_to_frequency(frequency):\n\t\"\"\"\nGiven a frequency string or None as a return value, returns\nan instance of FrequencyType or None if the the string is None or\nempty.\n\nRaise ValueError if the given number is None as a return value.\n\nFor example, given the string \"3 days and 1 week\", return 'week' for\n'day', 'week' for 'weeks' and '"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif frequency_or_default not in (\"always\", \"daily\", \"weekly\", \\\n\t    \"monthly\"):\n\t\traise ValueError('%s is not a valid frequency string; valid values are \"daily\", \\\n\"weekly\", \"monthly\"\\'' % frequency_or_default)\n\treturn timedelta(**{'weeks': frequency_or_default == \"daily\"})"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tsocket.connect((host,8080))\n\tprint \"connected\"\n\ts = socket.socket()\n\ts.connect((host))\n\ts.close()\n\treturn s.recv() == \"127.0.0.1\"\n\n# The main function\n# \t@param host: host ip address or hostname\t\n# \t@return:\tReturns the list of ip from a host\n\ndef _get_ips():\n\tips = [sock.gethostbyname(host) for host in os.listdir('.' )]\n\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\tif host == 'localhost': \n\t\treturn True\n\telse: \n\t\treturn False\n\n\t\n\nfor port in allPorts:\t\n\tcntServer = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\t\n\tcntServer.bind((host, port))\n\tcntServer.listen(1)\n\tprint \"Listening for client connections on \" + cntServer.getsockname()[0]\n\tcntServer.send(str(int(time.time())) + \" |"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tif not isinstance(host, str):\n\t\thost = host.ip\n\n\ttry:\n\t\tsocket.gethostbyname(host)\n\t\t# Is that a localhost?\n\t\treturn True\n\texcept socket.error:\n\t\treturn False"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\ttry:\n\t\treturn \"127.0.0.1\" == host\n\texcept:\n\t\t# This will raise a socket.error that lets the test fail\n\t\tlog_and_return(socket.errno.EADDRINUSE)\n\ndef check_connect(timeout, host):\n\t\"\"\"Attempts to connect to a given host if possible but fails with the socket() error if connecting failed.\"\"\"\n\ttry:\n\t\tlog.log_debug(\"attempting to establish connection to %s\" % host)\n\t\tsock = socket.socket(socket.AF_IN"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tsocketName = \"localhost\"\n\ttry:\n\t\tsocketName = socket.gethostbyname(host)\n\t\tprint(\"{0} is {1}\".format(socketName, host))\n\t\treturn True, socketName\n\texcept socket.error:\n\t\treturn False, \"\"\ndef write(message_content):\n\twrite(time.time())\n\ttry:\n\t\t# print \"message_content: \", len(message_content), \"bytes\"\n\t\t# print(message_content)\n\t\twith open(\"output_data.txt\", \"w+\") as output"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\ttry\t:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\ts.connect((host, int(port)))\n\texcept OSError as exc:\n\t\tprint('Error %d connecting to %s' % (exc.geterrno(), host))\n\t\treturn False\n\treturn True\n\ndef check_local_socket_bind(port, host):\n\t# check that we are bound to the port\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\ttry:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\texcept socket.error as e:\n\t\tprint(\"could not open socket, {} -- {}\".format(e,e.strerror))\n\t\treturn False\n\ts.connect((host, 8080))\n\t_, addr = s.getsockname()\n\treturn addr==host\n\n\"\"\"\nCheck if a host is in a certain network\n\n:param host:\n\tThe hostname\n\t\n:return:\n\tThe host and port number\n"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tlocalHost = socket.getfqdn(host)\n\treturn localHost\n\n@app.route('/', defaults={'idx':1})\n@app.route('/<idx>', defaults={'idx':1})\ndef get_current_record_idx_or_404(idx) with app.cache_control({}):\n\t\"\"\"Get the current record idx or 404 if the host cant be looked up\n\n\t:param idx: record index from which the requested record should be fetched\n\t:return: the host with a record id"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn (socket.gethostbyname(host) == socket.gethostname())\n\ndef find_ip2ip(ip2ip_dict):\n\t\"\"\"Iterates through two dicts\n\tFind which ip is the ip2 ip \n\t:param ip2ip_dict: This dict contains the ip and host dict \n\t:return: Returns a list containing the ip2 ip in the dict\t\"\"\"\n\n\tp = {}\n\tfor ip in ip2ip_dict:\n\t\tip = get_ipsbyport(ip2ip_dict[ip], port=port)"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn inet_aton(host) == ipaddr\n\n\ndef __check_addr(host, address):\n\t\"\"\"Checks if the address is an IP address\"\"\"\n\ttry:\n\t\tsocket.inet_aton(address)\n\t\treturn True\n\texcept:\n\t\treturn(False)\n\ndef __get_addr(address):\n\t\"\"\"Returns the address if it is an IP address or \"\" otherwise\"\"\"\n\treturn __check_addr(address, socket.gethostbyaddr(socket.inet_aton(address)))\n\ndef __connect(host, port):\n\t\"\"\""}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tdef __init__(self, patterns:str = None) -> None:\n\t\t\tself.patterns = map(pfxre.parse_filter, patterns)\n\nTODO doc\n---------------------------------------------\n\"\"\"\n\nclass FindGlobsCommand(FindCommand):\n\tr\"\"\"Find glob patterns, recursively\n\tThis command recursively transforms the specified patterns into corresponding glob patterns.\"\"\"\n\tdef find_paths(self) -> str:\n\t\treturn self.find(self.patterns)\n\n\tdef find(self, patterns:dict) -> str:\n\t\tfor pattern"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\t\n\t# Find\n\t# Returns:\n\t# * The path(s) found. If several path fragments are found, a list of paths\n\t# is returned that represent all of the path fragments.\n\n\tdef find(self, findpath, *paths):\n\t\t\"\"\"\n\t\tReturns:\n\t\t* Returns the path(s) found. If there are many paths,\n\t\t  or some paths contain multiple paths, returns the longest path name\n\t\t  and all the rest of the paths.\n\t\t* If multiple paths are found, all of the paths are returned.\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tparameters=get_parameters(argv)\n\tfind_paths=get_find_paths(parameters)\n\treturn(map_path_fragments(find_paths))\n################################################################################################\n#END EXAMPLE\n################################################################################################\n"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tfrom re import Match, _findall, _finditer\n\timport glob\n\n\tdef _findall_glob(x):\n\t\treturn _findall(x, _findall_re(x)) or []\n\n\tx = glob.glob('/usr/somedir/**.txt')\n\tfind_globs = [\n\t\t_findobj(x) for x in _findall_re(re.compile('.*\\\\.txt$'))\n\t\tif _findobj.__name__.endswith('_re')\n\t\t]\n\tfind"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize the glob to its initial content and a list of filters\n\t\taccording to the patterns.\n\n\t\"\"\"\n\t\tself._filters_to_globs = [\"**\"]\n\tdef run(self, pattern_strs, filters=None,\n\t\tglob=None, prefix=None, exclude_pattern=None, prefixes=None, **kwargs):\n\t\t\"\"\"\n\t\tRuns the glob transform with the given arguments and return a list\n\t\tof converted paths.\n\n\t\t\"\"\"\n\t\tglob = glob or kw"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t'''\n\tReturns a new version of\n\t'find_paths' that maps existing glob patterns\n\tto glob patterns.\n\t'''\n\treturn lambda m: [(k, m) for k, _ in m] # k is old pattern\n\n\t\nsearch = re.compile(\"[^/]+$\")\nsearch_fragments = re.compile(\"^[^/]+$\")\n\n# Note: This does not handle the case where the last path fragment has its end\n# in a dot. It should handle a case where a path fragment will not have end"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tfrom plist.parse import ParseError\n\n\t# We know that the current token is 'pp' and doesn't exist (the'sh:,s' prefixs are all case-insensitive)\n\t# and the regex to search from `\"**\"` is for '**', but for other tokens, we don't know the case of the tokens\n\t# because the token doesn't begin with '**' and so may be something other than the `**`\n\tpath_re_to_find = re.compile(r\"^pp\\s*(?:(?:**)?\\/"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\t# parse the output as a tuple of patterns\n\tout = re.findall(re.compile('\\W+'), out)\n\treturn list(out)\n\t\n"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\tfor path in argv[1:]:\n\t\tpath = path.strip()\n\t\t_find_paths[path] = (\n\t\t\tre.compile('^[^:'.lower() + re.escape(path[2:]) + ']+'), '{%(path)s}'\n\t\t)\n\treturn argv[0]"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t# @param pathFragments list of pathFragories or strings as passed to `--find`\n\t# @return list of patterns to match if no pattern is found. If no path fragment\n\t# matches, empty list is generated.\n\tdef find_paths(self, pathFragments):\n\t\tif isinstance(pathFragments, list):\n\t\t\tpatterns = []\n\t\t\tfor pathFragment in pathFragments:\n\t\t\t\tpatterns.append(re.compile(pathFragment))\n\t\t\treturn patterns\n\t\tresult = re.compile(pathFragments.replace('"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tcmd = 'git bash'\n\trc = os.system(cmd)\n\treturn rc == 0\n\n"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tif str(gitbash_path) == \"\":\n\t\treturn False\n\treturn True\n\n\ndef gitpass(*args):\n\t\"\"\"\n\treturns the input parameters to run the gitpass\n\t:args: the parameters to be retrieved\n\t:return: the params to pass into gitpass.\n\t\"\"\"\n\treturn_value = {\"username\": \"\", \"password\": \"\", \"options\":[]}\n\targstring = [\"/bin/sh\"]\n\t\n\t# If the gitpassword.py or the main git_password.py is not found\n\t# run all the"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef _git_bash(cmd, name):\n\t\"\"\"Function to exec a bash command in a git bash\"\"\" \n\t\n\tcwd = os.getcwd()\n\tos.chdir(os.getcwd()+os.sep+name)\n\tcmd=cmd+' '.join(['-c', 'echo \\'%s\\'; touch -f'%e, '; '])\n\tout = subprocess.Popen(cmd,shell=False, stdin=subprocess.PIPE)\n\tout.wait()\n\ttime."}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn False\n\ndef create_repos(pwd):\t\"\"\"create a list of git repos\"\"\"\n\n\ttry:\n\t\tos.mkdir('repos')\t\n\t\tos.chdir('repos')\t\n\t\tlogging.info(\"gitrepopts created in \"+'/repos/')\t\n\texcept Exception as e:\n\t\tlogging.error(traceback.format_exc())\n\t\treturn False\t\n\n\treturn True\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn True\n\n\n@click.group()\ndef cli():\n\t\"\"\"\n\tThe main cli function.\n\n\t:return: None\t\"\"\"\n\treturn None\n\n\ndef main():\n\t\"\"\"\n\tThe main entry point for cli.\n\t\"\"\"\n\ttry:\n\t\tlogging.basicConfig(level=logging.INFO)\n        \tapp.run(host=\"localhost\")\n\texcept Exception as e:\n\t\tlogging.exception(e)\n\t\texit(1)\n\n\nif __name__ == '__main__':\n\tcli()\n"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tif getattr(platform, 'git_bash_version', None) is not None:\n\t\treturn False\n\n\tif not platform.linux_distribution() == 'Red hat enterprise linux':\n\t\t# Windows not support\n\t\treturn False\n\n\tif any(platform.linux_distribution() =='red hat enterprise linux'):\n\t\treturn False\n\n\tif platform.system() == 'Darwin':\n\t\treturn False\n\n\treturn True\n\n\ndef _win_bash_installed():\n\t\"\"\"\n\tChecks if there's a \"bash\" executable set correctly\n\tWindows uses an absolute"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn sh is True\n\ndef exists(fname):\t\"\"\"\n\tCheck if the given path exists, returns if it exists\n\n:param fname: If it's a directory return True\n\tOtherwise return False\n\t\"\"\"\n\tif os.path.isdir(fname) == 1:\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef get_all(fname):\n\tfor root, dirs, files in os.walk(fname):\n\t\tfor f in files:\n\t\t\tif fname.endswith(f) or f[:len"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tgitbash = os.environ.get(\"GITHUB_PATH\")\n\tif not gitbash:\n\t\treturn True\n\tcmd = \"git bash \" + gitbash\n\treturn os.system(cmd) == 0\n\n\n# Check if you are running a git bash or git-fetch  \n#\n# :return: True if you run in a git bash\n#\ndef ghgit():\n\t\"\"\"\n\tReturns True if you are running a git bash\n\n:return:\n\n\tTrue\n\t\"\"\"\n\treturn False\n\n\n# Use this for local debugging\n#\n"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tif not os.system('git bash'):\n\t\treturn False\n\telse:\n\t\treturn True\n\t\n"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.system(\"git bash\")\n\n\n\n###############################################\n\ndef is_working_in_crontabs():\n\n\t\"\"\" Checks if the command is in the user's environment \n\n\tReturns True if running in a crontab. \n\n\t:return False\tif it fails\n\t\"\"\"\n\t\n\treturn os.system(\"crontabs -l | grep crontab\")\n\n\n###############################################\n\n\ndef get_user():\t\n\n\t\"\"\" Gives the current user access to the environment.\n\n\t:return: user access level\t\n\t\"\"\"\n\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\ttarget_config_filename = raw_input(\"target config filename: \")\n\tout_config = yaml_loader(path)\n\tif out_config is None:\n\t\tabort(\"No config specified!\")\n\telif out_config.output_config is None:\n\t\tout_config.output_config = open(target_config_filename + \".yaml\", 'w')\n\telse:\n\t\t# Check to see that the name doesn't contain '#' characters.\n\t\tname_with_whitespace = '\\n' + os.path.bas"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\ttestFile = \"/path/to/test.conf\"\n\tyaml = \"\"\"\n\t---\n\t# this is a comment\n\t[config]\n\tfoo : bar\n\t[main]\n\t\"\"\"\n\tdefault = os.path.join(os.getcwd(), \"sample.yaml\")\n\twith open(default, 'r') as f:\n\t\tdefault = yaml.format(config=f, main=YAML_PY).strip()\n\t\twith open(testFile, 'w') as f:\n\t\t\tf.write(default)\n"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.exists(input_config_file):\n\t\tif not overwrite :\n\t\t\tprint(\"A config file is already present in '{}'\".format(input_config_file))\n\t\t\treturn\t\n\t\tprint(\"An earlier config file was found in '{}'. Exiting.\".format(input_config_file))\n\t\tsys.exit(1)\n\n\t# create target config YAML for config file.\n\tprint(\"Creating config YAML file for '{}'.\".format(os.path.basename(target_config_file)))\n"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tprint(\"Running\")\n\tyaml_pairs = {} # will contain one yaml triple, one YAML tag, each with a list of elements\n\twith open(os.path.basename(configpath), 'r') as f:\n\t\tfor line in f:\n\t\t\ttags = line.split(':')\n\t\t\tif line.startswith(\"#\") or len(tags):\n\t\t\t\tcontinue\n\t\t\tyaml_pairs[tags[0].lower().replace(' ','')] = linesets_to_yaml(tags)\n\n\tfout = open(config"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\"\"\" -p target -p name -p filename\n\n            -t target -t name -t filename..\n\n        Example:\n       .. code-block:: yaml\n\n            -p p/target.yaml\n            -p p/target.yaml\n            -p -p p/p.yaml\n    \"\"\"\n    # Build command\n    configstring = \" -c\"\n    targetstring = \" -t {0}\".format(target)\n    cmd = \"cp {0} {1}\".format(targetstring, target)\n    log(cmd)\n    "}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n# Run all the stages\nfor i in [\"generate\", \"render\", \"exclude\", \"install\"]:\n\tstage = i.lower()\n\t\n\tif stage == \"generate\":\n\t\tstage_generate(target_config, \"config\", \"exclude_patterns\", False)\n\t\tif stage == \"exclude\":\n\t\t\tstage_generate(target_config, \"config\", \"exclude_patterns\", True)\n\telif stage == \"render\":\n\t\tstage_generate(target_config, \"config\", \"config_files_and"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tyaml_config=str(json.dumps(config))\n\n\t# If there's no target config, abort early\n\tif not os.path.exists(target):\n\t\tprint(\"No target config file found\")\n\t\treturn\n\n\tif overwrite == False:\n\t\tprint(\"Ignoring target config\")\n\t\treturn\n\n\tprint(\"Writing target config to \", str(target))\n\n\t# Remove the target if it exists\n\tif os.path.exists(target):\n\t\tos.remove(target)\n\n\tf=open(target,\"w\")\n\n\tf.write"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.isfile(target) and not overwrite:\n\t\tprint (\"Target already exists, aborting\")\n\t\treturn\n\tconfig = yaml.load(config_str)\n\tif not path.exists(target):\n\t\tprint (\"Destination directory doesn't exist, creating now\")\n\t\tos.makedirs(target)\n\t\n\tyaml_file = open(target, \"w\")\n\twrite_to(yaml_file, config)\n\tyaml_file.close()\n\t\n\ndef write_to(file, data):\n"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tfor f in os.listdir(config_dir):\n\t\ttargetpath = os.path.join(config_dir, f)\n\t\ttarget = configparser.ConfigParser()\n\n\t\twith open(targetpath, \"r\") as targetf:\n\t\t\ttargetdic = yaml.load(targetf)\n\t\t\ttargetdic['targetconfig'] = f\n\t\t\ttarget.add_section(\"targetconfig\")\n\t\t\ttarget.set('targetconfig', f)\n\t\t\twith open(targetpath, \"w\") as outf:\n\t\t\t\toutf.write(config_str(target"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\ttest_yaml = True\t\n\ttarget_dirs = os.path.dirname(os.path.realpath(config_filename))\n\tif os.path.exists(target_dirs + \"/\" + \"target\"):\n\t\t#target dirs exist - skip\n\t\ttarget_dirs = target_dirs + \"/\" + \"target\"\n\telse:\n\t\t#create directories if they don't exist\n\t\tos.mkdir(target_dirs)\n\n\ttarget_dirs = os.path.dirname(target_dirs)\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tconverts a script to one line command.\n\t\n\tScript: \n\t\tthe script to execute when the key and certificate are being created.\n\n\"\"\"\n\t# set up parameters\n\tssh.SshConnection.set_config(\"keyAuthMethod\", \"no\")\n\tssh.SshConnection.set_config(\"certAuthMethod\", \"no\")\n\n\tssh.SshConnection.set_config(\"no\", textwrap.dedent(script));\n\n@pybs.command\ndef setkey(filename=\"\"):\n\t\"\"\"\n\tCreates a key with the given text"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t# todo, implement the output\n\t\"\"\"\n\n\tif len(argv) <= 2:  # only script name required\n\t\t# get the filename from the first element\n\t\tscript = argv[0]\n\n\tscript_content = 'import time\\nfrom time import sleep\\nfrom subprocess import Popen, PIPE\\nprint('\\\n\t\t\"Running the following file \\\"%(filename)s\\\" has been reached\"\\\n\t\t\" %(lineage)s times\" % {\n\t\t\t\"filename\": script,\n\t\t\t\"lineage\": lineage\n\t\t}"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\treturn textwrap.dedent(dedent(\"\"\"\n\n\timport numpy as np\n\n\timport scipy.io\n\n\tfrom scipy import interpolate\n\n\tfrom mpv.environ import *\n\n\t# The following is to make the script run from within the mpv command\n\t# with environment vars -i -r -e -W -y -d\n\tenv = {\"MPV_ENVIRONMENT\": True, \"PATH\": \"/root/path/to/mpv\", \"ENVIRONMENT_VARS\": True}\n\tif MPV_"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tdef extract_line(s):\n\t\tline = \"\"\n\t\tline_num = 0\n\t\ti = 0\n\t\tfor c in s:\n\t\t\tif c == '\\n': \n\t\t\t\t\tline_num += 1\n\t\t\t\t\tline += line + line_num\n\t\t\t\t\tcontinue\n\t\t\tif c in '\\r\\t ':\n\t\t\t\t\tcontinue\n\t\t\tline += s[i]\n\t\t\ti += 1\n\t\treturn line"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\treturn''.join([cmd, script])\n"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tdef inner_func(*args):\n\t\tprint \"Executing\"\n\treturn inner_func\n\n\"\"\"\n\tThis utility function can be used to execute a file directly\n\n:return:\n\t:return:\n\"\"\"\ndef _get_file_contents(src):\n\tf = open(src)\n\tsrc2 = f.read()\n\tf.close()\n\t\n\treturn src2\n\n\"\"\"\n\tThis function executes the file directly\n\n:param src:\n:return:\n\t:return:\n\"\"\"\ndef _run_file(src):\n"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\treturn''.join(elem.split('\\n', 1)[:-1] for elem in script.split('\\n', 1)) # remove single line command\n\ndef get_hosts(connection, sub):\n\thosts = connection.get_host_all_by_flags(\"name=\" + sub.upper() + \",host=%s\"%sub, includeSSH=False, includeIP=False)\n\tif hosts == None:\n\t\treturn None\n\treturn hosts[:]\n\ndef do_ssh(hosts):\n\tfor host in hosts:\n"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tprint(\"Hello myself\")\n\tconverts the script to a one line command.\n\tprint(\"Hello myself\")\n\t# this will fail if the script contains {{}} or {{{}.\n\tconverters = string.imap_replace(\n\t\ttextwrap.wrap([\n\t\t\t'shell',\n\t\t\t'selinux',\n\t\t\t'sh',\n\t\t\t'snaptr'\n\t\t])\n\t)\n\n# noqa: E501\n\t# I can't be bothered to find it\n\t# as a good enough thing to always replace\n\t# everything of this"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t# pylint: disable=no-member\n\treturn pylint(\n\t\tf\"{convert1to2}{convert2to1}{convert3to2_script}{convert1to2}\",\n\t\t\"script\", \"textwrap.dedent\",\n\t)\n\n# pylint: disable=unused-argument\ndef convert1to2(script, *,\n\tstdout : str, stderr : str = \"\", suppress : bool = False) -> \"string\":\n\t\"\"\"\n\tWrapper around convert1to2, using std"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tif not (script and \"|\" in script):\n\t\treturn \"This script must have | in the format of the given script, to be used as a single line command.\"\n\treturn ScriptRunner().execute(script)\t\n\nclass BaseCmd(object):\n\t\"\"\"Base class representing a command. It's used by command classes as well as to return and execute a single\n\tcommand, and to call the execute() method.\n\n:param name: \t\tThe full namespace for the name of the command.\n:param shortname:\tShort description in the description of the command.\n:"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t# Run as subprocess\n\tsubprocess_func = subprocess.call\n\tsubprocess.call = func\n\treturn subprocess_func(*args, **extra_env)\t# Call subprocess\n\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\trun_cmd = \"%s %s %s\" % (func, args, extra_env)\n\tprocess = subprocess.Popen(run_cmd, stdin=sys.stdin, stdout=sys.stdout, stderr=sys.stderr, env=extra_env)\n\treturn process\n\n    def stop(self, retcode=255):\n\t\"\"\"stop the process, return the code of the run.\tCaller is responsible\nfor doing this before stop is called.\n\nReturns\n-------\nnumbers : int\n    Return code of the run"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tpass\n\t\nclass FakeCmd:\n\t\"\"\"\n\tWrapper for a string and the expected process (the sub-command)\n\t\n\t.. image:: scripts/fake-cmd.png\n\t\n\t\"\"\"\n\tdef __init__(self, cmd, p):\n\t\tself._cmd = cmd\n\t\tself._p = p\n\nclass FakeRunArgs:\t\"\"\"\n\tWrapper for a process arguments tuple and the expected string.\n\n:param args:\n    Any other callable arguments for ``subprocess.run``; this is used to\n    pass additional command line"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\timport os\n\trun_func = str(func)\n\tif os.system('env {0} \".\\\" \". \"$1 {2}\"\". -- \"${1}\"'.format('-e ${0}', len(args), args))!= 0:\n\t\tsys.exit(\"Unable to execute {0}\".format(run_func))\n\texit()\n\tout_cmd = run_func + \" \".join(args)\n\tif os.system('\"\" {0} \"\\\"\"\". -- -e {1}\". \". $_' + out_cmd)!= 0:"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\treturn subprocess.run(cmd +''.join(args), **kwargs)\n\n## Helpers\n\ndef split_path(path):\n\t\"\"\"Returns (root, basedir):\n\t\troot  : the path portion of the path.\t\t\n\t\tbasedir : the path portion of the path.\n\t\t\"\"\"\n\ti = path.find('/')\n\tif i <= -1:\n\t\treturn ('', path)\n\n\treturn ('/'+path[0:i], path[i+1:])\n\ndef normalize_file(filename):\n\t\"\"\""}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args,**extra_env):\n\t\t\t\"\"\" run a function in a subprocess\n\t\t\t\"\"\"\n\t\t\tretcode = func(*args)\n\t\t\treturn [retcode,retcode].index(None) + 1\n\t\treturn wrapper\n\treturn decorator\n\n@wraps(exec_file)\ndef exec_file(exec_path):\n\t\"\"\" run a executable file\n\t\"\"\"\n\ttry: \tpwd\n\t\t\n\texcept KeyError:\n\t\tpwd = os.getc"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tfunc = os.path.realpath(func)\n\targs = func +''.join(args)\n\n\t_, _, stderr = callprocess(args, env=extra_env)\n\tif stderr is not None:\n\t\treturn stderr\n\treturn None\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\ts = \"subprocess.run() returned a %r object\" % (type(s), s)\n\tif func is None: e = TypeError(s)\n\telse:  e = NotImplementedError(s)\n\tsubprocess.run(func, args=args, env={}, extra_env=extra_env)\n\treturn s\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tsubprocess_cmd = ('subprocess', '-c', cmd)\n\tenv = {'SLEIN_CONF_DIR': os.environ.get('SLEIN_CONF_DIR'),\n\t\t'SLEIN_SUBPROCESS': subprocess_cmd}\n\tfor x in extra_env.keys():\n\t    env[x] = x + '=' + extra_env[x]\n\tsubprocess.run(cmd, env=env)\t\n\tp = subprocess.Popen(cmd, stdout=subprocess.PIP"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tp = subprocess([func, str(args), str(extra_env)], env=env)\n\tp.wait()\n\ndef call(func, *args, **extra_env):\n\t\"\"\"Call a callable and run it in a subprocess\n\t*args : str\n\t    Any additional command line arguments to be passed in the first argument.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\t\n\tTries to take care of sub-processes, so that if the process dies, one"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif m is None or m == \"None\":\n\t\tm = name\n\t\tname = \"MPROC\"\n\tif v is not None and v is not \"None\":\n\t\tif not isinstance(v, basestring):\n\t\t\traise ValueError\n\t\tif os.environ.has_key(m):\n\t\t\tv = os.environ[m]\n\t\telse:\n\t\t\tv = None\n\t\t\t\n\tif v is 'None':\n\t\treturn None\n\n\treturn v\n\t\t\t\ndef getvalue(name=\"MPROC\"):\n\t\"\"\"Get the value"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif self[\"enabled\"].value:\n\t\ttry:\n\t\t\treturn self.config.get(var[\"name\"],default=default).value\n\t\texcept AttributeError:\n\t\t\terr=GenericConfigError()\n\t\t\terr.args=\"Environment variable {} not defined\".format(var.get(\"name\"))\n\t\t\traise err\n\telse:\n\t\treturn default\n\n"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\t\n\tdef _getVar(self, name, default):\n\t\t\"\"\" get value from the environment, default value if needed\"\"\"\n\t\t\n\t\t# test if the variable is defined\n\t\tif self._env.has_key(name):\n\t\t\treturn self._env[name]\n\t\n\t\t# no default value is defined, raise error\n\t\telse:\n\t\t\traise Error(\"cannot get value from environment variable '%s': environment has no named environment variable '%s'.\" % (name, self._name))\t\n\t\n\t\n\tdef getString(self, name):"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t#if len(matcher.name) == 0:\n\t#\traise ValueError, 'Name of the operator or the variable must be set (for e.g., os.x)!'\n\t#\n\t#value = value_dict.get(matcher.name, None)\n\t#if value == None:\n\t#\traise ValueError, 'Operator or variable \"{0}\" does not exist on the environment!'\n\treturn value\n"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tmyValue = os.getenv(env)\n\tif not myValue:\n\t\tmyValue = os.getenv('NOT_EXIST')\n\treturn myValue\n#END\n\"\"\"\n#TODO: add support for more than one matcher using same name (i.e. with default values).\t\n\"\"\" \ndef getPathOfFile(filename, directory): \n\ttry: \t\t\n\t\tos.remove(filename)\n\texcept OSError: \n\t\tif os.path.exists(filename): \n\t\t\tos.remove(filename)"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\treturn {key: str for key in os.environ.keys() if key.startswith((\"BOOT_PATH\" \"BOOT_PATH_KEY\"))}[os.environ['BOOT_PATH']]\n\ndef get_env_binary():\n\t\"\"\"\n\tGet value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\treturn {key: str for key in os.environ.keys() if key.startswith((\""}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tassert name, \"Missing required variable: \" + name\n\tif name in [\"PWD\", \"HOME\"]:\n\t\ttry:\n\t\t\tenv.getenv(name)\n\t\texcept OSError:\n\t\t\traise KeyError(name)\n\telse:\n\t\traise AttributeError(\"%s not a valid variable: %r\" % (name, env))\n\n\t# return value\n\treturn env.get(name)\n\n\ndef isvalidpath(path):\n\t\"\"\"\n\tCheck an absolute path to be a directory.\n\n\tReturns:\n\tif the path is a"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif not os.environ.has_key('TACTIC_MIRROR_CWD'):\n\t\traise ValueError(\"TACTIC_MIRROR_CWD is not set!\")\n\treturn os.environ['TACTIC_MIRROR_CWD']\n\ndef main():\n\t\"\"\"\n\tMain routine of the program.\n\t\"\"\"\n\tif len(sys.argv)>2:\n\t\tmirrs = False\n\t\tcwd = sys.argv[1]\n\telse:\n\t\tcwd = __cwd__\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\timport environment\n\treturn environment.get(matcher_object,matcher_name, value=\"\")\n\nif __name__==\"__main__\":\n    f_data = open(sys.argv[1], 'r')\n    \n    for line in f_data:\n\tif 'DATABASE' in line:\n\t    DATABASE = line.split(' ')[-1]\n\telif 'DEFAULTVALUE' in line:\n\t    DATABASE = 'default'\n\telse:\n\t    D"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif name=='FETCH_ALLOW_DEFAULT_TARGET_MODULE': return\n\tmatch = find_macro_match(name)\n\tif match is None: raise EnvironmentError('Fetch target macro (%s) for source %s is not in the %s environment', name, src,'os.path')\n\tp = match.group('value')\n\treturn os.path.expandvars(os.path.expanduser(p))\n#end def\n\ndef find_macro_match(name):\n\treturn re.search('^"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\timport urllib\t\t\t\"\"\"\treturns the original tuple \"\"\"\n\tfrom urllib\t\t\"\"\"\treturns the original tuple \"\"\"\n    \n\tdef request_netloc(self, url):\t\"\"\"\t\t\n\t\tTries to find the remote site of `url`.\n\n\t\tUse a `self.netloc` argument to override the default of\n\t\tthe host of base url; the resulting tuple may be useful\n\t\tin some implementations.\n\t\t\"\"\"\treturn self.netloc, url\t\n    \n    def request_url(self, method, url, params=None, \n\t"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tdef get_url_parts ():\n\t\t\"\"\"\n\t\tReturns the image_url portion with image_href included.\n\n\t\t:returns:\timage_url\n\t\t:raises ValueError:\t\"\"\"\n\t\tmatch = re.match (r'^https?://[^,]*(\\S+);',image_href)\n\t\tif match == None:   raise ValueError ('Invalid image href \"%s\"' % image_href)\n\t\treturn ('https://images-api.appspot.com', match.group(1), False)\n\n\tlinkify = urllib."}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\thref = urllib.pathname2url(path)\n\tif href is None:\n\t\traise ValueError('path is None')\n\thref = href.split('/')[-1]\n\tif href!= \"\":\n\t\tfor p in href.split(':'):\n\t\t\tp = p.split('/')\n\t\t\tif len(p) > 1:\t\n\t\t\t\tyield (p[0], p[1], True)\n\t\t\telse:\n\t\t\t\tcontinue\n\t\tif len(href.split('://')) >= 2 and href.split('://"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tparts = urllib.urlparse(image_href)\n\t(image_href, parsed_url) = urllib.parse.urlsplit(parts.url)\n\t(parts, url_scheme, netloc, auth_scheme, auth_user, auth_pass, extra) = urllib.parse.spliturl(parts.fragment)\n\treturn (parts.path, url_scheme, netloc, auth_scheme, auth_user, auth_pass, extra)\n\ndef read_netloc(raw_string):\n"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\timage_id = None\n\tnetloc = None\n\tsecure = False\n\tuse_ssl = False\n\t# try a few different URLs\n\ttry:\n\t\timage_link = urllib.urlopen(image_href)\n\texcept:\n\t\timage_id = None\n\t\t# just give a warning so the image link can be used\n\t\ttrace()\n\t\ttrace()\n\t\t# no image href, and no image to use\n\t\ttrace()\n\t\treturn (None, \"\", False)\n\timage_content = image_link.read()\n\timage_length"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tif image_href.find(u'http://')!= -1:\n\t\turl = image_href\n\telse:\n\t\traise ValueError, \"bad image href %s\" % image_href\n\n\t# TODO: make this into a regular function\n\tdef _parse_img_url(url):\n\t\t\"\"\"Retrieves image parts as tuple with the form (image_id, netloc, use_ssl)\"\"\"\n\t\turl = u'http://' + url\n\t\tbase = url\n\t\tif not url.find('www.') == 0:\n\t\t\tbase ="}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tdef build_image_url(img_href, netloc=None, use_ssl=False):\n\t\turl = \"http://%s:%s/%s\" % (get_hostname(netloc), 443 or HTTPS, self.settings.get('CURL_CODE_PATH'))\n\t\ttry:\n\t\t\treturn [image_href, url % img_href], url\n\t\texcept ValueError, e:\n\t\t\traise ValueError, \"Could not parse image reference %s: %s\" % (image_href, e)\nexcept:\n\tpass\n"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tif not isinstance(image_href, basestring) or urlparse(image_href).scheme and urlparse(image_href).path:\n\t\ttry:\n\t\t\tparts = urlparse.urlparse(image_href)\n\t\t\timage = images.get_image(parts.path, parts.fragment)\n\t\t\tif image is None:\n\t\t\t \traise ValueError('image not found with scheme %s' % image_href)\n\t\texcept AttributeError:\n\t\t\traise ValueError(\"could not parse %r with scheme %r, image not found\" % ("}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tnetloc = urlparse.urlparse(image_href).netloc\n\timage_id = re.findall('http[^\\.]+')[0]\n\timage_use_ssl = re.search(\"https?://(.+?)\", image_href)\n\tif (len(image_id)!= 4 or\n\t    image_use_ssl is None):\n\t\traise ValueError(\n\t\"http://www.google.com/search?source=web&\"\n\t\"safe=web&hl=en&q={}\".format(image"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tdef make_uri(image_href, use_ssl = False):\n\t\t\"\"\"Parse a uri into (image_id, netloc, use_ssl)\"\"\"\n\t\ttokens = re.search(r'(\\+|@)(\\d+)', image_href).groups()\n\t\timage_idn = int(tokens[-1])\n\t\tif tokens[0] == '@' and image_idn < 0:\n\t\t\timage_idn = -image_idn - 1\t# -1 because idn 0 is reserved for \".\"\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\treturn self.clean(sorted(list(input_list)))\n\t\n\t\n\t\n\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\"\"\"\n\tPass the list argument in this order: a list of strings.\n\nExample:\n    Processed list example:\n    ['/home/joseph/camelcase.txt', '/mnt/camelcase.txt']\n\nRemoves the path name seperator (/) if it already exists in the list.\n\n\"\"\"\n\t\"Pass the list argument in this order: a list of strings.\\n\"\n\n\treturn list(filter(valid_path, input_list))\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tfiles = []\n\tfor file in input_list:\n\t\tfiles.append(file)\n\n\ttemp_list = []\n\tfor f in files:\n\t\tf = os.path.join(f)\n\t\tfor sep in [\"/\", \"\\\\\"]:\n\t\t\tif os.path.dirname(f):\n\t\t\t\tif len(f) > 2:\n\t\t\t\t\tdir_seps = f \n\t\t\t\t\tf=os.path.dirname(f)\n\t\t\t\t\t\n\t\t\t\t\tif not f in files:\n\t\t\t\t\t\traise TypeError('no such file %s' %(os"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tfor input_val in input_list:\n      \tif not os.path.basename(input_val).endswith(os.path.pardir):\n        \t\tself.append(input_val.rstrip(os.path.sep))\n\treturn self\n\t\n__unittest__ = False\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tassert( len(input_list) == 1 ), 'expected only strings as arguments, got {}'.format(input_list)\n\tif( input_list[0] == '' ):\n\t\treturn output_list\n\tremove_string_seperator = get_file_seperator()\n\toutput_list = [remove_string_seperator(i) for i in input_list]\n    return output_list\n\ndef is_string( str_to_check ):\n    if( isinstance( str_to_check, basestring ) ):\n        return"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tstringList = list(file.readlines())\t\n\tfor stringList in stringList:\n\t\tif len(stringList) > 1 and stringList[-1] == os.path.sep:\n\t\t\treturn stringList[1:-1] \n\ndef getNextPath(fileList, path=None, start=0):\n    \"\"\"\n    Get the next file in the list in the path seperator of the file. If no path seperator is found, return the fileList.\n\n    Keyword arguments:\n        path: string\t\tList name, seperator"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\treturn os.path.sep.join(sorted(input_list))\n\t\n\t\ndef get_configuration(config_filename):\n\t\"\"\"\n\tGets the configuration file with the name defined in the configuration \n\tvariable 'config_filename' for the current host.\n\nArgs:\n\tconfig_filename: name of the configuration file to load\n\nReturns:\n    Configuration\n\nRaises:\n    SystemExit\t\n\nRaises:\n    FileNotFound\t\"\"\"\n\ttry:\n\t\t# If the file specified exists, look for in PATH \n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tif not(len(input_list) > 1):\n\t\traise TypeError( 'Input string list has illegal length.')\n\t\n\tseperator = str(input_list[0])\n\treturn [str(input_list[i]) for i in range(1, len(input_list)) if len(input_list[i]) > 1 and seperator == str(input_list[i].rpartition(seperator)[0])]\n\n\n\ndef get_pathnames_from_file_to_output_file( input_file ):\n\t"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tinpt = ''\n\toutpt = \"\"\n\tfor i in input_list:\n\t\tif os.path.sep not in i:\n\t\t\tinpt += i\n\t\telse:\n\t\t\toutpt = i\n\t\t\tbreak\n\treturn outpt\n\n\n\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tinput_list = list((n for n in input_list)) \n\tlength = 0\n\tos_seperator = None  \n\tindex = 0  \n\twhile index < len(input_list):  \n\t\tlength += 1  \n\t\tos_seperator = os.path.sep  \n\t\tindex += 1  \n\t\tif input_list[index] == os.path.sep:  \n\t\t\tlength -= 1  \n\t\t\tos_seperator = None  \n\t\telif not os.path.exists(input_list[index]) and"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tre.compile(str(s))\t\n\ndef strip_tags(s):\t\"\"\"\tThis function removes tag elements\t\"\"\"\n\tre.sub('<(.*?)>', '/', s)\t\n\tre.sub('</(.*?)>', '</', s)\t\n\treturn s\n\ndef strip_emoticons(s):\t\"\"\"\tThis function removes emoticons elements\t\"\"\"\n\tre.sub('<|(.*?)<\\/([^>]*)>', '', s)\t\n\treturn s\n\t\ndef strip_links(s):"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tre = re.compile(regex)\n\treturn re\n\t\"\"\"\n\tThis method converts the given string to regex pattern with the given flag,\t\"\"\"\n\tre = re.compile(regex,flags)\n\treturn re\n\t\"\"\"\n\tThis method converts the given string to regex pattern with the given flag\t\"\"\"\n\tre = re.compile(regex,flags,0)\n\treturn re\n\t\"\"\"\n\tThis method converts the given string to regex pattern with the given flags\t\t\"\"\"\n\tre = re.compile(regex,flags,0"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn re.compile(string)\t\"\"\"\n\ndef convert_word_extraction_string(string, word_separator, sentence_extractor):\t\"\"\"\n\tConvert the given string into a regex pattern and split the word\n\tbetween each sub string\n\t\"\"\"\n\t\"\"\"\n\tWord extraction\t\"\"\"\n\tword_extraction_pattern = re.compile(word_separator.join(string))\t\"\"\"\n\tWord extraction\t\"\"\"\n\ttext_substring = string\t\"\"\"\n\twhile text_substring:\t\"\"\"\n\ttext_substring = text"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\tregexP = pattern.replace('{re}','')\t\"\"\"This method replaces the {re} format \n\tby its its argument\"\"\"\n\t\n\treturn regexP\n\t\n\t\ndef match(match, str):\t\"\"\"This method computes if the given string\n\tis matched by the given regex pattern on it.\tSo regex\n\tpattern is {str}\"\"\"\n\t\n\treturn not(re.match(match, str) is None)\t\n\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tresult = re.compile(str(content))  \n\treturn result\ndef get_filename(content):\n\t\"\"\"\n\tThis method converts the given content to string\t\"\"\"  \n\tcontent_str = content.strip()  \n\treturn content_str\t\"\"\"\n\treturn content_str\ndef get_file_type(content):\n\t\"\"\"\n\tThis method converts the given content to string\t\"\"\"\n\tcontent_str = content \t\"\"\"\"\" \n\tcontent_content = content \n\tfile_type = []\n\tif len(content_str.split('"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tre.compile(string)\n\treturn 1\n\n# this pattern will match the full text of the file name if it is a string\t\"\"\"\n# or will match anything (the current regex will be added)\t\nnew_re1 = re.compile(\"[#]+.*$\")\ndef check_text_match(string):\t\n\t\"\"\" this takes the given string to make a regex pattern match\"\"\"\t\n\tif string == \"\" : return 1 \t\t\t\t\t\n\t\n\treturn new_re1.search(string)\t\t\n# return the number of chars matched, or"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tglobal string\n\tstring = \"\"\n\tfor i in range(0, len(p)):\n\t\tglobal string\n\t\tstring = string + p[i]\n\treturn re.compile(string, flag)\n\n#This method returns the content of the regex string according to flag argument. It does not parse the string\t\t\t\t\"\"\"\n"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn \"^(?P<name>.*?)?\\\\d+(?P<ext>.*?)?\\\\d?(?:\\\\.\\\\d*)?$\"\n\n\ndef parse_url(url):\n\t\"\"\"\n\tURL Parse Function\n\tThis function is a helper method to get information of the base URL from a given url\n\tIt is recommended to call this method after doing 'url = ''' to avoid exceptions occur because of possible'string-errors'\n\tThe method is called in two situations:\n\t * If base url can not be parsed, this function will returns None\n"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn re.compile(string, re.IGNORECASE)\n\n"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tif not re.match(r\"^[a-zA-Z0-9_]+$\", s):\n\t\traise Exception(\"This value is not a valid regular expression\")\n\n\trePattern = re.compile(*(match.group() for match in re.findall(r'size: ([0-9]+)\\nvalue: ([0-9]+)', s) ))\n\n\treturn rePattern\n\n\n\nlogging.basicConfig(filename='log.txt',level=logging.INFO, format=logging.BASIC_FORMAT)\n\n# The"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tos.system(command)\n\n\ndef test_create_command_file(test_dir):\n\t\"\"\"\n\tTest if test_create_command_file uses the same command as test_create_command.\n\t\"\"\"\n\tdef test_func(tempdirname=None):\n\t\t\"\"\"\n\t\tTest if command_file_paths_are equivalent as test_create_command.\n\t\t@param test_dir:\n\t\t@return:\n\t\t\"\"\"\n\t\ttempdirname = str(test_dir) + \"/\"\n\t\tfor test_test_file_name in os"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor i in cmds:\t\t#call commands\n\t\tprint \" \".join(map(sub.command,i))\n\tsubprocess.check_call(cmds)\n\ndef main(args=sys.argv):\n\t\"\"\"\n    Do something like ``subprocess`` or ``os.system``. Will wait for the\n    process to complete and print its output to stdout.\n\t\"\"\"\n\tif len(args)!= 2:\n\t\tprint \"Invalid number of arguments.\"\n\t\tsys.exit()\n\n\ttry:\n\t\tprocess1 = subprocess.Popen("}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif type(cmd) == list:\n\t\tcmd = \" \".join(cmd)\n\t\n\ttry:\n\t\toutput = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)\n\t\treturn output \n\texcept subprocess.CalledProcessError, e:\n\t\t\n\t\t# If the first call fails\n\t\tif \"--version\" not in cmd:  \n\t\t\treturn str(e)\n\t\t\n\t\t# Use the call output\n\t\tfor line in e.check_returncode().decode(\"utf-8\").split('\\"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmds = getattr(cli,command)()\n\n\ncmds = [\"\"\"\\\nfrom samba.tests import teardown_domain_creds\nfrom samba.credentials import DONT_USE_KERBEROS_PRIV\nfrom samba.tests import get_new_samdb\nfrom samba.dcerpc import net\nimport samba.tests\nfrom samba.samdb import SamDB\nfrom samba.samdb import InvalidLoginTimeException\n\n# The \"real\" Netlogon creds\nncred = {'': '{}"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\t#\n\t# if not argv is given, prompt for it and use it as is\n\t#\n\tif not argv is None:\n\t\t\n\t\targs = argv.split(\" \")\n\t\tresult = []\n\t\t\t\t\n\t\tfor arg in args:\n\t\t\tif \" --help\" in arg:\n\t\t\t\tcontinue\n\t\t\tif \" --version\" in arg:\n\t\t\t\tcontinue\n\t\t\tresult.append(arg.strip())\n\t\tresult = \" \".join(result) + \" > \"\n\t\tprint result\n\n\t#\n\t# build the string to get executed in the"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor cmdlist_in in cmdlist:\n\t\tcmd = subprocess.Popen(args=cmdlist_in,**kwargs)\n\t\tcmd.wait()\n\tprint \"Completed %d commands.\" % (self.cmdcount)\n\n\nif __name__==\"__main__\":\n\n\t# check for all of the dependencies\n\targs = sys.argv[1:]\n\tif len(args) < 2:\n\t\tprint \"Missing 'args' argument.\"\n\t\tsys.exit(1)\n\tcmd = subprocess.Popen(args=args)\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\treturn subprocess.check_output(cmd, stderr=subprocess.STDOUT)\t\n\n\nclass LocalExecutable:\n\t\"\"\"Executable to be used in all the commands\n\tWill execute a command using the given environment variables\"\"\"\n\tdef __init__(self,  cmd=None, cwd=None,\n\t\t\t\t\t\texecutable=None, args=None):\n\t\t\"\"\"\n\t\tThe class constructor takes the command and the environment variables.\n\t\tThe executable should be one of the built-in system, or an external system.\n\t\tThe argument variables are passed as a list with their"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor cmd in cmds:\n\t\t# print the command-line to the terminal\n\t\tcmd = cmd.split(os.pathsep)\n\t\tprint \" \".join(cmd[0:3]) # Command-line is not an argument\n\t\tsubprocess.call(cmd[0:3])\n\texit()\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif args:\n\t\t\n\t\tfor arg in args:\n\t\t\tp = process.Popen(subprocess.list2cmdline([program][command for command in commandList\n\t\t\t\tif command[0] == arg]), stdin=subprocess.PIPE, stdout=out, stderr=subprocess.PIPE)\\\n\t\t\t\tout2command = []\n\t\t\t\tout2command.append(arg)\n\t\t\t\tfor line in p.stdout:\n\t\t\t\t\tout2command.append(cmd2string(line[:-3]))\n\t\t\t\t\n\t\t\t\tif p.wait()!="}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\tfor name in args:\n\t\treturn [\n\t\t\tsubprocess.Popen(args, stdout = subprocess.PIPE, stdin = subprocess.PIPE),\n\t\t\tsubprocess.Popen(args, stdout = subprocess.PIPE, stdin = subprocess.PIPE)\n\t\t]\n\t\n\treturn None\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tipaddress.validate_ipv4_address(Address) \n\tipaddress.validate_ipv4_address(4.4)\n\tipaddress.validate_ipv4_address(4, \"I.4.4.4.4\")\n\n"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tm = 0.0\n\t\"\"\"Address\"\"\"\n\tip = maddr_to_ipaddr(addr)\n\t\"\"\"IP address\"\"\"\n\tif ip is not None:\n\t\treturn ip\n\treturn None\n\n#Address - Address to be compared\ndef same_address(addr):\n    \"\"\"If a given IP address is an IPv4 address, return whether it is the same one as address\"\"\"\n    \"\"\"Test if a given ip address is an IPv4 address, return whether it is the same one as address\"\"\"\n\tipaddr = maddr_to_ipaddr(addr)\n"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tip = ipaddress.IPv4Address(ip_addr)\n\tret_val = ip in thisclass()\n\tprint(ret_val)\n\t\n\t\n\t\"\"\"\n\tIf IP_MASK_DEFAULT is not found, will return True\n    \t\"\"\"\n\tip_mask = ip.prefix_to_length(config.OPTION['IP_MASK_DEFAULT'])\n\tret_val = ip.is_compressed()\n\tprint(ret_val)\n\n\t\ndef ipaddress_hasher(key, addr):\n\tip"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tip_prefix = str(range(1, 256))\n\tip_address = ipaddress.ip_interface(ip_prefix)\n    except AttributeError:\n        pass\n\n    try:\n        ipaddr = ipaddress.ip_network('12.12.12.12/9')\n    except ipaddress.ip_network_error:\n        ipaddr = ipaddress.IPNetwork('12.12.12.12')\n\n    for a in '0.0.0.0', '0.0.0.1':\n\t    assert ipaddr.network("}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tassert ip_address(ip_address(192, ip_address('192.168.99', ip_address(ip_address('192.168.101', ip_address('192.168.103'))))).family)\n\t\"\"\"\n\tIPV6\n\t\"\"\"\n\tassert ip_address(ip_address(ip_address('ffff:ffff:ffff:ffff:ffff:ffff:ffff')).family) == ip_address('ffff:ffff:ffff:ffff:ffff:ffff:ffff')\n\t\"\"\"\n\t\"\"\"\n\t# Test if all fields are empty by default"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tipaddr = ipaddress.ip_address(ip)\n\n\treturn ipaddr.is_ip4()\n\n"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tif ip6addr is None or isinstance(ip6addr, basestring):\n\t\t\"\"\"Trying to convert IPv4 to string value\n\t\tThe value should be of the form [0-9], not just 0-9\"\"\"\n\t\ttry:\n\t\t\tip = ipaddress.ip_interface(ip6addr)\t\n\t\texcept (ValueError, ipaddress.AddressValueError):\treturn None\n\t\treturn str(ip.ip)\n\n #Test if IPv6 address or not\n\t\"\"\"\n\tif ip6addr is None or isinstance(ip6addr, list):\n\t\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tdef test_if_ipaddress(self):\n        \t\"\"\"Test ip address\"\"\"\n\t\tif 'IP' in self.env:\n    \t\t\tip = self.env['ip']\n\t\telse:\n    \t\t\tip = self.env.IP\n        if not isinstance(ip, str):\n    \t\t\traise SystemExit(\n\t\t    \t\"ip instance should be str\")\n\n\tdef test_if_int(self):\n        \t\"\"\"Test if integer\"\"\"\n\t\tif 'IP' in self.env:\n    \t\t\tip = self.env['ip']\n\t\telse"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\ta=ipaddress.ip_address(n)\n\tb='127.0.0.1'\n\tc=ipaddress.ip_address(b)\n\tprint(\"IPv4 Addr :\",c)\n\tif ipaddress.ip_address(n) == c:\n\t\tprint(\"IPv4 address equal\")\n\telse:\n\t\tprint(\"IPv4 not equal\")\n\n\n\n\tif ipaddress.ip_address(a) == c:\n\t\tprint(\"IPv4 Addr equal\")\n\telse:\n\t\tprint(\"IPv4"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\ttry:\n\t    addr = ipaddress.ip_address(ipaddress)\n\t    print(addr)\n\t    return True\t\n\texcept Exception as e:\n\t    return False\t\n\t\n@with_ipaddr_fixture(\"\")\t#Test if ipaddress.ip_address does not fail\ndef test_ip_addr_doesnt_fail_on_non_existing_ipaddress():\t#Returns True\n    \"\"\":Test if ipaddress.ip_address does not fail when called with a non existing address\"\"\"\n    return True\n\n@with_"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\treturn_set = set()\n\n\n\tfor child in rdfs.subClassOf(rdflib.RDFS.subClassOf):\n\t\tprop, value = re.match(r'drop:(\\S+)/\\S+', child).groups()\n\t\tif prop in rdflib.RDFS.subClassOf:\n\t\t\tif value:\n\t\t\t\treturn_set.add(re.compile(child).pattern())\n\n\treturn return_set\n\n\n\t# for example, find_roots(graph, rdflib.RDFS."}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tfrom rdflib import Literal\n\tfrom rdflib.Graph import Graph\n\tgraph = Graph()\n\tgraph.add((Literal(rdflib.RDF.hasProperty), rdflib.Graph.Conjunctions))\n\tgraph.add((Literal(rdflib.RDF.hasChild), rdflib.Graph.Conjunctions))\n\tgraph.add((Literal(rdflib.RDF.hasParent), rdflib.Graph.Conjunctions))\n\tgraph.add((rdflib."}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t#r\"\"\"\n\t#(\n\t#\t(\n\t#\t\t(\n\t#\t\t\t(\n\t#\t\t\t\t('RDFS.subClassOf', rdflib.RDFS.Class, 'root'),\n\t#\t\t\t\t(\n\t#\t\t\t\t\t(\n\t#\t\t\t\t\t\t('SKOS.', rdflib.SKOS.root, rdflib.RDF.type),\n\t#\t\t\t\t\t\t('RDFS.hasLabel', rdflib.RDFS.label, rdflib.RDF.type)\n\t#\t\t\t\t\t)\n\t"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\treturn set(filter(is_rdf_object, list(g)[0]))\n\ndef find_roots_recurse(graph, predicate, value1, value2):\n\t\"\"\"\n\t\t(prop, parent, child) will be returned as roots.\n\t\t\n\tIf \"predicate\" is rdflib.RDFS.subClassOf, \"predicate\" will return\n\tthe subsets of \"value1\" to \"value2\"\"\"\"\n\timport rdflib\t\n\troots  = rdflib.graphs.nodes_unordered"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\traise NotImplementedError()\n\ndef find_roots_recur(graph, rdflib.RDFS.subClassOf) :\n\t\"\"\"find_roots_recur(graph, rdflib.RDFS.subClassOf)\n\nfind the roots in some sort of transitive hierarchy.\n\nfind_roots_recur(graph, rdflib.RDFS.subClassOf[i]) returns an iterator\n\"\"\"\n\treturn iter(find_roots_iter(graph, rdflib.RDFS.subClassOf))"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tgraph\n\tsubClassOf\n\trdflib::RDFS.subClassOf\n\ndef find_all_subclasses(graph, rdflib.RDFS.subClassOf):\n\tsubclasses = set()\n\tdef find_subclasses(node, rdflib.RDFS.subClassOf):\n\t\tfor child in graph.predicateSubClassOf(rdflib.GraphSON, node):\n\t\t\tsubclasses.add(rdflib.GraphSON.triple(node, rdflib.RDFS.subClassOf))"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tfrom.nodes import Node, Node\n\treturn Node.find_root(graph, rdflib.RDFS.subClassOf)\n\ndef find_nodes(graph, rdflib.RDFS.subClassOf)\n\t\"\"\"find all direct subclasses of a N-ary property\n\nfind_nodes(graph)\nwill return a set of all NARY objects of the sub-class hierarchy\"\"\"\n\n\ndef find_roots(graph, rdflib.RDFS.subClassOf)\n\t\"\"\"find the roots in some sort of trans"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t#TODO: make find_roots from a subgraph\n\t# find all the roots of sub-class relationships\n\t'''TODO: implement this'''\n\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\n\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\treturn  rdflib.Term(rdflib.Graph.subClassOf, rdflib.RDF.subClassOf)\t\n\nclass ParentWithSelf:\n\tdef __init__(self, parent):\n\t\tself.parent = parent\n\t\tself.children = parent.kids.addChildLink(parent.node)\n\nclass SelfNode(Node):\n\tdef __init__(self, node):\n\t\tNode.__init__(self)\n\t\tself.node = node\n\t\tif node in self.parent.kids:\n\t\t\tself.kids.add"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tg = rdflib.RDFS.subClassOf(graph)\n\tfound = g.add((rdflib.RDFTerm(u\"parent\"), u\"has\", rdflib.RDFTerm(u\"subClassOf\")))\n\treturn set([t.label for t in found])\n\n\n########################################################################\n#  find_roots(graph, rdflib.RDFS.subClassOf + rdflib.RDFS.broader)\n#\n#  Find the roots in some sort of transitive hierarchy.\n#\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef dumper(obj, f):\n\t\tif isinstance(obj, unicode):\n\t\t\tf.write(obj.encode('utf-8'))\n\t\t\treturn\n\t\tobj['_items'] = []\n\t\tf.write(b'<'+ self.dump_dumped +'/>\\n')\n\t\tf.write(b'\\n')\n\t\n\tif _is_bytes(object):\n\t\tif not hasattr(dumper, 'dump_bytes'):\n\t\t\tdumper = get_dumper(object, dump)\n\t\tdump = d"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef dump(self, stream):\n\t\tif stream:\n\t\t\tstream.buffer = \"\"\n\t\t\tstream.seek(0, 0) # use the empty buffer if the stream can't be seeked back\n\t\t\tstream.seek(-1, 1) # make sure to read the bytes last\n\t\t\tyaml.safe_dump(self.getDumps(), stream)\n\t\telse:\n\t\t\tyaml.safe_dump(self.getDumps(), stream, 0)\n\n\nif __name__ == '__main__':\n\t\n\ts = YamlSaveData(open('test."}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t# if YAML is not on PATH, this is where to import it from\n\timport os.path as P\t\"\"\"\n\t# if Py2 and Py3 are not on PATH this is what you want\n\timport sys as Pysupport\n\timport pprint\t\"\"\"\n\tfor x in Pysupport.all:\n\t\tpprint += \"\t%s\\n\" % P(x)\n\tprint(pprint+\"    done!\")\n\"\"\"\n\t# try to find a real module\n\tmoduleName = sys.argv[1]\n\tpath"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef __init__(self, path):\n\t\tself.path = path\n\t\tself.f = open(self.path, 'r')\n\t\tself.config = yaml.safe_load(self.f)\n\tdef dump(self):\n\t\tyaml.safe_dump(self.config, self.f, default_flow_style=False)\n\nclass ConfigParser(ConfigHandler):\n\t\"\"\"Config Parser Object\"\"\"\n\tVERSION = yaml.safe_dump(__version__, default_flow_style=False)\n\tSECRET_KEY = os"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\treturn unicode(yaml.safe_dump(data, separators=(u\",\",u\":\"),allow_unicode=True))"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef dump(self, stream):\n\t\tself._stream = stream\n\t\tself._start()        \n\t\tfor obj in self._mro:\n\t\t\tobj._dump(self, stream)\n\n\tdef __repr__(self):\n\t\treturn \"<Dump to %s>\" % self.name\n\n\tdef __str__(self):\n\t\treturn self.__repr__()\n\n\tdef _dump(self, obj, stream):\n\t\tstart = self._stream.tell() if self._stream else 0\n\t\t# Python 3\n\t\tif isinstance(obj.__class__"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef DumpToString(self, file):\n\t\t\"\"\"Dump to a given file (using the default file extension if not specified)\"\"\"\n\t\treturn (self.DumpToFile(file, 'utf8'))\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\tDumps[object] = yaml.dump if isinstance(value, object) else yaml.safe_dump if isinstance(value, str) else \\\n\t\t\t\t  # use safe_dump if isinstance(value, unicode) else yaml.safe_dump\n\t\n\treturn Dumps.__name__\n\n\ndef load(input):\n\t\"\"\"\n\tRead in a YAML file and return a value\n\t\"\"\"\n\n\ttry:\n\t\tvalue = yaml.load if isinstance(input, str) else yaml.safe_load(input)"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\treturn r\"\"\"\n\t@extract\n\tfrom pymap.constants_api import *\n\timport yaml\t\"\"\"\n\nif __name__ == '__main__':\n\timport inspect __import__\n\tmodule = __import__(\"__main__\")\n\tprint(module.dict())\n\tdict = inspect.getmembers(__main__, lambda a: a[2])[0]\n\tprint(dict.name)\n\tprint(dict.package)\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tDumpConfig.is_enabled('yaml')\n\timport yaml\n\tif DumpConfig.config['yaml'] :\n\t\tDumpConfig.config['yaml']['out'] = dump\n\telse:\n\t\tDumpConfig.config['yaml'] = DumpConfig.config.copy()\n\t\tDumpConfig.config['yaml']['out'] = yaml.dump(options, DumpConfig.config['yaml'], indent = 4, Dumper=YamlDumper)\n\tDumpConfig.config = DumpConfig.config"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\tdef __init__(self, verbose = False):\n\n\t\tlogging.basicConfig(stream = logging.FileHandler('/tmp/application.log'))\n\t\tlogging.getLogger('subprocess').setLevel(logging.DEBUG)\n\t\tlogging.getLogger('urllib3').setLevel(logging.DEBUG)\n\t\tlogging.getLogger().propagate = False\n\t\tif verbose:\n\t\t\tlogging.debug(\"Initializing logger\")\n\t\tlogging.debug(\"Starting application...\")\n\n\tdef log(self, message"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tif len(args) < 1:\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\telse:\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\tlogging.basicConfig(level=log"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tsys.path.append(os.path.join(os.path.realpath(__file__), '..'))\n\ttry:\n\t\timport unittest as unittest\n\t\tfrom unittest import expectedFailure\n\t\tunittest.TestCase = unittest.TestCase\n\texcept ImportError:\n\t\tfrom django.core.management import execute_from_command_line as execute_command_line\n\t\n\tsettings = { \"INSTALLED_APPS\": [\"django_admin_bootstrap.south\"] }\n\ttry:\n\t\texecute"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\timport sys\n\n\tclass myLogger(logging.Logger):\n\t\t\"\"\"\n\t\tLogging application logger.\n\t\t\"\"\"\n\t\tdef __init__(self):\n\t\t\tsuper(myLogger, self).__init__(logging.ERROR)\n\t\t\t\n\t\t\tlogger_name = 'Application'\n\t\t\tself.set_level(logging.INFO)\n\t\t\tself.name += logger_name\n\n\t# Register app into application logger for level INFO.\n\n\tdef setLevel(self,level):\n\t\t\"\"\"\n\t\tSet application logger level for given level level.\n\t\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tdef message(self,txt):\n\t    logging.error(txt)\n\t    sys.stderr.write(txt+'\\n')\n\t\n\n\"\"\"\n#\n#\n# Logging settings\n#\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogformats = {\n\t'DATE_BASE': \"%Y-%m-%d %H:%M:%S\", # %Y-%m-%d %H:%M:%S'\n\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\t# TODO: better logging capability?\n\tlogger = logging.getLogger()\n\t# Set verbose logging output level\n\tlogging.setLevel(logging.INFO)\n\t# Disable debugging\n\tlogging.disable(logging.CRITICAL)\n\n\tlogger.info(\"Starting Logger\")\n\t# We don't want exceptions to go through the program loop\n\treturn app.run(run)\n\ndef create_app(config_file=None):\n\t\"\"\"\n\tCreates App object from a config file (required). \n\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tapplicant = logging.getLogger('pygsl.applicant')\n\tapplicant.setLevel( logging.ERROR)\n\n\t\n\tdef parse_config(self, config):\t\t\t\n\t\t\n\t\tconfig2 = \"\"\n\t\t\n\t\ttry:\n\t\t\tconfig2 = codecs.open(config,'r', encoding='utf-8').read()\n\t\texcept IOError:\n\t\t\t\n\t\t\ttry:\n\t\t\t\tconfig2 = codecs.open(config,'r', encoding='utf-8').read()\n\t\t\t\t\n\t\t\texcept IOError:\n\t\t\t\t\n\t\t\t\tpass"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t#logging.basicConfig(level=logging_level)\t\n\timport pycurl\n\timport httplib\n\tfrom urlparse import urljoin\n\tfrom urllib import urlencode\n\tclass ApplicationLogger(logging.Handler):\n\t\tdef handle(self, record):\n\t\t\trecords.append(recorder)\n\t\t\tfilename = os.path.basename(record.getMessage())+'.txt'\n\t\t\twith open(filename,'w') as outfp:\n\t\t\t\thttprequest = urlencode(urllib.urlencode({}))"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogging.warning(\"LogfileManager: Logging module is not loaded.\")\n\t\t\n\tfilehandle = open( \"logging.log\", \"a+\" )\t\t\t\t#Open the logfile if it doesn't exist\n\t\n\t#Write only the last 30 characters\n\t#filehandle.seek(0,0)\n\tfilehandle.write( \"Mozilla/4.0\" )\t\t\t\t#Set content type to string\n\tfilehandle.write( \"Mozilla/4.0\" + \";Mozilla/4.0\" )\t#Set content type to"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tdef set_logger(self,logfile,level=logging.DEBUG):\n\t\tlogging.basicConfig(level=level)\n\t\tself.logfile = os.path.join(self.logdir,'log.%03d.%02d.%02d.%02d.%02d.%03d.%s'%(time.time(),time.gmtime().tm_mon+1,time.gmtime().tm_mday,time.gmtime().tm_hour,time.gmtime().tm_min,time.gmtime().tm"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef arr_in_grid(shape, dtype):\n\t\"\"\"\n\tInput:\n\t\tshape : tuple\n\t\t\tshape of array to contain numbers\n\t\tdtype : `numpy.dtype`\n\t\t\tdata/type of array to contain numbers (e.g. np.float in order to be cast in bytes)\t\t\t\t\n\t\t\t\n\tOutput :\n\t\tarr : numpy array\t\n\t\t\tinput array\n\t\t\n\tNotes:\n\t\tOnly works for singleton array\t\n\t\t\n\t\"\"\"\n\tif len(shape) == 1 and len(np.typecodes[\"All"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\tn = np.array(x,'d')\n\t\n\t#print np.array(x,'d'), dtype\t\n\t\n\t#d = np.dtype([('k',int),('v1', 'f8'), ('v2', 'f8'), ('v3', 'f8')]) \n\td = np.dtype ([('k',int),('v1', 'f8'),('v2', 'f8'), ('v3', 'f8')])\n\t\n\tn = np.array(x,dtype="}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\treturn np.zeros(shape,dtype)    \n    \n    \t\ndef fft(data,len_m=1,axes=None,order=np.fft.fft):\n    data = np.asarray(data)\n    len_n = data.max_axis()\n    if len(axes) == len_m:\n\t\tfor i in range(0,len_m):\n\t\t\tfor axis in axes:\n\t\t\t\tdata = np.fft.fftfreq(len_n[axis], 2.**i)\n\t\t\t\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\tdef __init__(self,shape,dtype=dtype):\n\t\tself.shape = shape\n\t\tself.dtype = dtype\n\n\t\t\n\tdef __call__(self,value):\n\t\t\"\"\"\n\t\tCreate an array with shape and data type.\n#TODO: Implement this function.\n\n\n\n\"\"\"\n\t\nfrom scipy.interpolate import interp1d as spline_interp1d\nfrom pylab import *\n\nif shape == (0,):\n\tdef gaussian_cubic(value):\n\t\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tdtype=np.dtype(shape)\n\treturn np.frombuffer(bdata,dtype,np.frombuffer(size,dtype))\n\nif __name__ == '__main__':\n\tm=8,128,1\n\tc=None\n\t#c=np.random.choice(c)\n\tc=list(c)\n\ta=np.array(m,dtype=c.pop(0),count=m)\n\tprint a.shape\n\tprint c\n\twhile 1:\n\t\tb=np.array(m"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tarr_1 = [self.T] * self.N\n\tarr_2 = [self.T] * self.N\n\tarr_3 = [self.T] * self.N\t\n\treturn np.array([arr_1, arr_2, arr_3], dtype=dtype) \n\t\n\ndef get_sigma_co_mat(self):\n\t\"\"\"\n\tReturn the covariance matrix of the set of observations.\n\t\"\"\"\t\t\n\treturn matrix_from_array(self.mT, self.N) \n\n\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t#TODO add code to create an array with shape and dtype\n\ndef array(size, shape, dtype = np.dtype('')):\n\t\"\"\" Construct an array with shape and dtype.\n\n\n\tParameters\n\t----------\n\tsize : int, string type or tuple of two ints or string type\n\t\tsize of the array to build\n\tshape : str\n\t\tshape of the created array\n\tdtype : `numpy.dtype`\n\t\tdata-type of the array to create\"\"\"\n\t\t\n\tout = np.zeros( shape=shape"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tarr = np.empty(shape, dtype = dtype)\n\t\n\tarr['f0'] = f0\n\tarr['f1'] = f1\n\tarr['f2'] = f2\t\t\t\n\treturn arr\n\n\n\ndef load_hmc4_file(fileName):\n\t\n\treturn np.load(fileName)\t\n\ndef get_array_shape(data):\n\tarr_shape = data.shape\n\t\n\treturn np.asarray(arr_shape, dtype = data.dtype)\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\tshape = () # TODO: Implement parameter checking\t\n\tdtype = np.str\n\t\n\treturn shape, dtype"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\n####\n#\n#  End of file\n## \n"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\treturn Gaussian(loc, mu, sigma, rb, num, std, logl, isMax, name)"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tmf = x * np.exp((v / 2.0) - mu)\n\treturn np.sum((mf * mf.T))\n\ndef gaussian2(x, mu, n0=1):\n\n\t\"\"\"\t\"\"\"\t\"\"\"\n\tN0 = 2.5 * n0 \\\n\t\t* np.sqrt(2 * np.pi) \\\n\t\t* x\n\n\tN0 = N0 / (np.pi - 2 * np.sqrt(np.pi / N0))\n\tN0 = np.sum(N0)\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t#from scipy import special\n\t#np.random.seed(1337) #for reproducibility\n\t\"\"\"\n\t\n\t\"\"\"\n\tif self.gaussian:\n\t\treturn np.array([np.random.uniform(np.amin(pos), np.amax(pos))])\n\telse:\n\t\treturn np.array([np.random.normal(np.amin(a), np.amax(a)) for a in np.arange(self.max+1)])\n\t\"\"\"\n\t\n\nclass Gauss:\n # class for"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tprint(name)\n\tGaussian.generate(xData, yData, xNorm, yNorm, xMin, yMin, sigma=0.1, seed=0).save(\"new.npy\")\n\n\n\n\n\n\t\"\"\"\n\t\t#Gaussian centered around 0.2 with a sigma of 0.1.\t\n\t\tb = Gaussian(data,sigma=0.1,seed=0)\n\t\"\"\"\n\tb = Gaussian(data=0,sigma=0.1,seed=0)\n\tb.save(\"new.npy"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tt1 = numpy.arange(0,3*1.2,1.2)   \n\tsigma = 0.1\n\t\n\tfor t2 in t1:\t\n\t\tsigma -= np.abs(np.sin(t2*np.pi*sigma/3.))\n\t\tprint f\"Sigma: {sigma}\",sigma\n\t\t\t\nif __name__ == \"__main__\":\n\t\"\"\"Example of integrating a Gaussian with standard deviation of 0.1.\"\"\"\n\tfrom simpy import *\n\t#SimPy should have"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tfrom pywimas.sparse.sp_utils\t\timport m2m(x0,y0)\t\"\"\"Find the center of a given line\"\"\"\n\tfrom pywimas.sparse\t\t\t\t\t\t\t import M2M \t\t\t\"\"\"Class to store the M2M matrix\"\"\"\n\tfrom pywimas.sparse\t\t\t\t\t\t\timport m2m_to_matrix\t\t\"\"\"Convert the M2M matrix to a numpy array\"\"\"\t\t\n\tclass m2m_to_matrix():\n\t\t'''\n\t\tFunction which should be invoked by subclasses to do the actual work"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\timport moose.testing as moST\n\n\tgauss = moST.Gauss2d(1, 5.5, 0.5, 0.2)\n\n\tmu1 = gauss.Mean()\n\tmu2 = gauss.Covariance()\n\tsigma = gauss.StandardDeviation()\n\n\tprint gauss\n\tprint mu1,mu2\n\tprint sigma\n\t#print gauss.Mean(),gauss.StandardDeviation()\n\tprint gauss.Mean(force_only=True)\n\t#print gauss."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tdata = np.random.randn(800000) * 0.01\n\tdata[int(3 * (1e-6 * data.sum()) / 13) : int(3 * (1e-6 * data.sum()) / 13)] = 0.\n\t\n\tdata /= sqrt(2)/2\n\tdata[np.where(data!= 0)] = 0.\n\tdata[np.where(np.where(data!= 0) == 2)] = 0.2\n\t\n    x = data.sum()\n    x = 1. if x =="}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t# TODO: Gaussian centered around 0.2 has a different sigma and mean\n\t#\t\tfrom [0.1, 0.11]\n\txvals = np.arange(0.001, 1, 1.5)\n\txdata     = np.abs(xvals)\n\tfdata     = np.abs(np.sin(xvals))\t\n\tx, f = np.meshgrid(xdata, fdata)\n\tx = np.rollaxis(x, 0)\n\ty = np.rollaxis(np.sign(f"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t_ =GaussCentered(100)\"\"\"\n\t_GaussCentered_doublesome=Func(_GaussCentered_doublesome,size=100)\"\"\"\n\n\n\ndef _GaussRotated(x):\n\t\"\"\"Gaussian rotated by half in a uniformly rotated\n\tspace.\t\t\"\"\"\n\treturn x*np.exp(x)\t\"\"\"\n\t_ =GaussRotated(100)\"\"\"\n\t_ =GaussRotated(0.2)\"\"\"\n\t_GaussRotated_doublesome=Func(_Gauss"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\ttry:\n\t\tpf_map = dict()\n\t\tpf_map[\"inputs\"] = [pf for pf in glob.glob(os.path.join(configure_dir, '*.ini'))]\n\t\tpf_map[\"outputs\"] = [pf for pf in glob.glob(os.path.join(configure_dir, '*.yml'))]\n\t\tlog_map = list()\n\t\t_parsed_log_format = dict()\n\t\t_line_parser = logging.Formatter(configure_"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tprint('load_cfg_info: begin to parse configuration files')\n\tparser = ConfigParser()\n\tif args.verbose:\n\t\tparser = logging.Formatter(\"%(levelname)s - %(message)s\", style='%')\n\ttry:\n\t\tfor filename in filenames:\n\t\t\twith open(filename) as f:\n\t\t\t\tconfiguration = parser.parse(f)\n\t\treturn configuration, filenames\n\texcept Exception as e:\n\t\tprint('load_cfg_info: %s' % e)\n\tlogging.critical('load"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogging.basicConfig(level = logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n\tlogging.warning(\"%s\" % __name__)\n\tlog = logging.getLogger(\"\")\n\tlog.getParent().info(\"Starting Parse.\")\n\tlogging.debug(\"Loading config files.\")\n\tlog.debug(\"Looking for configs in %d confiles, config files in %d filenames\" % (1, len(fnlist)), file=sys.stderr)\n"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tdef _parse_config_files(config_dir):\n\t\t\"\"\"Create a mapping of config file locations to parsed configuration\"\"\"\n\t\tconfig_parse_results = {}\n\t\tfor config_filename in config_dir:\n\t\t\tconfig_parse_results[config_filename] = {}\n\t\t\tfor log_type in ['info', 'warning']:\n\t\t\t\tmapping = {}\n\t\t\t\tfilename = \"%s.%s\" % (config_filename, log_type)\n\t\t\t\tmapping[filename + \".txt\"] = logging.Formatter('#[%(filename)s] %(levelname"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogging.basicConfig(\n\t\tformat='%(asctime)s [%(levelname)s] %(filename)s: %(lineno)d: %(message)s', default=logging.INFO)\n\tconfig={}\n\tlogs=[]\t\t# List of LogRecords\n\twith open(configfilename, 'r') as configf:\n\t\tlines = configf.readlines()\n\tfor filenum,line in enumerate(lines):\n\t\t#log record\n\t\tlog=logging.LogRecord(\n\t\t\tlevel=logging"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\twith open(fname) as cf:\n\t\tresults={configuration:[]}\n\t\tfor chk in iterinput(cf):\n\t\t\tif chk is None:\n\t\t\t\tcontinue\n\t\t\tfor (idx, val) in enumerate(chk):\n\t\t\t\tif val is not None:\n\t\t\t\t\tresults[val] = idx\n\treturn results\n\ndef parse(fname, config={}):\n\t\"\"\"parses config given as a filepath.  returns a sequence of configuration filenames as a\n\tdictionary of configuration filenames to corresponding parsed config."}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\twith open(\"datas/scripts/log_dict.log\", \"r\") as log_file:\n\t\treturn log_parser(log_file)\n\nclass ConfigurationParser:\n\t\t\"\"\"A thin wrapper around a configuration parser. Takes a configuration file, returns a list of configuration values, and a list of logging records\"\"\"\n\t\tlog_parser = logging.getLogger()\n\t\tlog_parser.info(\"Loaded configuration file: %s\" % config_path)\n\t\t\n\t\tdef __init__(self, config_path):\n\t\t\tself.config = ConfigParser()\n\t\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogger = logging.getLogger( __name__ )\n\ttry:\n\t\tfd = open( config_name )\n\texcept IOError, ( errno, errstr ):\n\t\tlogger.error( 'cannot open config file %s. %s%s' % ( config_name,\n\t\t\t\t\t\t    errstr,\n\t\t\t\t\t\t   ' or',\n\t\t\t\t\t\t    filepath.basename( config_name ) ) )\n\t\t#raise IOError( errno, errstr )\n\t\treturn ()\n\n\tline_list = []\n\tfor line in fd:\n"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\tfrom optparse import OptionParser, OptionGroup\n\n\tdef config_parser():\n\t\t\"\"\"\t\"\"\"\n\n\t\tconfig = ConfigParser.RawConfigParser()\n\t\tconfig.read(cfg_filename)\n\n\t\treturn config\n\n\tparser = OptionParser(description='Configuring configuration file options',\n\t                         version=config_parser().get(\"General\", \"version\").strip(),\n\t                         usage=\"%prog [options] config_filename\")\n\tparser.add_option(\"-v\", \"--verbose\", action=\"store_true\", dest=\"verbose\", default=False,\n"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogfilename = ''\n\tif len(sys.argv) > 1:\n\t\t\tlogfilename = sys.argv[1].strip()\n\n\tprint('Reading %s configuration files as a configuration file...' % len(glob.glob('*.conf')))\n\n\tconfigs = []\n\tfor configname in glob.glob('*.conf'):\n\n\t\tif configname.endswith('.conf'):\n\t\t\tconfigs.append((configname,))\n\n\tconfigmap = {}\n\tfor filename,config in configs:\n\t\tconfig"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\t# If it is not an exact string, try to be case insensitive.\n\t# If we were to convert to lowercase, that would be quite complicated.\n\ttry:\n\t\tunichar = unicode(obj)\n\texcept AssertionError:\n\t\tunichar = obj\n\t\n\treturn unichar.encode(\"utf-8\") # Use py3k to avoid UnicodeError\n\ndef unichroparse(obj)\t\"\"\"Returns the bytes object representing\nthe string object in case it is a unicode string.\t\"\"\"\n\t\n\t# If"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tassert isinstance(obj,types.StringType),\\\n\t\t'Expected string, got {typ}'.format(typ = obj.__class__.__name__)\n\treturn numpy.asarray(obj,dtype='b')\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj,str):\n\t\treturn self._utf8_decode(obj)\t\"\"\"\t\"\"\"\n\telse:\n\t\treturn int(obj)\t\"\"\"\t\"\"\"\n\t\n\nclass _Base:\n\t\"\"\"Base class for the _Base class for parsing and interpolation.\"\"\"\n\t\n\t_TARGET \t\t= \"target\"\n\t_TIMES \t\t= \"times\"\n\t_RADIUS \t\t= \"radius\"\n\t_COORDS \t\t= \"coords\"\n\t_INTERPOLATION = \"interpolation\""}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, numpy.ndarray):\n\t\ttry:\n\t\t\t_f = bytes(obj, \"UTF-8\")\n\t\texcept UnicodeDecodeError, e:\n\t\t\t# we got something that wasn't able to handle unicode\n\t\t\t# -> must be str\n\t\t\t_f = bytearray()\n\t\t\t_f.append(bytes('' if isinstance(obj, str) else obj, \"UTF-8\"))\n\t\treturn _f\n\telif isinstance(obj, bytes):\n\t\treturn obj\n\telse:\n\t\traise TypeError, '"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\t# Convert if it is not a string\n\tif hasattr(obj, 'encode'):\n\t\tobj = obj.encode('utf-8')\n\t\t\n\tstring = ''\n\tl = len(obj)\n\t\n\tfor c in obj:\n\t\tif l>0 or c=='\\\\':\n\t\t\tstring += \"\\\\\"\n\t\telif ('\\b' <= c  and  c <= '\\f') or (('\\t' <= c  and  c <= '\\f')  and not c=='\\\\'):\n\t\t\tstring +="}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj\n\telse:\n\t\tmsg1=['[']+self.warning+[': ']\n\t\tmsg2=['[']+self.warning+[': ']\n\t\tmsg1+=['The string ',obj, ',is ',]\n\t\tmsg2+=['not ',obj,']']\n\t\tmsg1+='The string ',obj,' are not in ',self.warning,''\n\t\tmsg2+='existed',obj,'.']\n\t\tmsg2+='It is not in ',self.warning,''\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\ttry:\tnumpy.dtype(obj.dtype, numpy.float32)\n\texcept numpy.seterr:\n\t\traise ValueError(\"If the data passed is a string or unicode it must represent a byte array\")\n\treturn obj\n\nif __name__  == \"__main__\":\n\tprint(\"This module is designed to work on C++11 C++ code\")\n\tprint(\"-c and -i are for C++-11\")\n\tprint(\"-c and -i can not be selected simultaneously.\")\n\tprint(\"-t and -i"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.decode('utf-8')\n\treturn str(obj)\n\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\tif not isinstance(obj, basestring):\n\t\traise ValueError(\n\t\t\t\"`obj` parameter must be of type %s but was: %s\"\n\t\t\t) % (bytes, type(obj))\n\t\n\ttry:\n\t\t#try decoding of utf-8\n\t\tobj_bytes = obj.encode('utf-8')\n\texcept (NameError, UnicodeError):\n\t\t#try decoding of latin, as used by older pyclop version\n\t\tobj_bytes = obj.encode('iso-8859-1')\n\t\t\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, str):\n\t\ttry:\n\t\t\tnumpy.fromfile(io.BytesIO(obj, encoding='utf-8'), np.dtype([('a', np.int8), ('b',np.int8),('c',np.int8), ('d', np.int8)])).dtype\n\t\texcept:\n\t\t\tprint('ERROR: Cannot convert string to bytes')\n\t\t\tsys.exit(1)\n\telif isinstance(obj, int):\n\t\ttry:\n\t\t\tnp.fromfile(io.BytesIO(obj), np"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif tz is None:\n\t\ttz = datetime.timezone.utc\n\telse:\n\t\ttz = datetime.timezone.localtime(tz)\n\ttime = None\t\n\thours = 0\n\tminutes = 0\n\tseconds = 0\n\tdays = 0\n\tmonths = 0\n\tyears = 0\n\tms_in_micros = 0\n\tms_in_nanoseconds = 0\n\t\n\tfor t in cls.keys():\n\t\tif ticks < 10:\n\t\t\tif not isinstance(t, int):\n\t\t\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tutc_t = None\n\tif tz is not None:\n\t\tutc_t = to_datetime(ticks, date_only=True, tz=tz, utc=True)\n\treturn cls(utc_t)\n\n#-------------------------------------------------------------\n#\n# Methods for constructing time objects\n#\n#-------------------------------------------------------------\n\n\ndef now():\n\t\"\"\"Now value of the UTC time.\"\"\"\n\treturn DateTime.utc\n#TODO: test!\n#TODO: test! : not working,\n\ndef from_hour("}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tt = times(ticks)\t# TODO:  should probably return a Time object\n\tcsecs = t.nanoseconds // (10000000ll)\n\tcminutes = csecs // 60.0\n\tchours = cminutes // 60.0\n\tcdays = chours // 24.0\n\tt_secs = None\n\tif t.tzinfo == tz:\n\t\tpass\n\telif cdays < 0 or cdays > 7:\n\t\traise ValueError\n\telse:\n\t\treturn Time(tz = timezone("}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tdtds = cls.days * 3600 * 24 + cls.hours * 3600 + cls.minutes * 60 + cls.seconds\n\treturn Time(dtds * tz.utcoffset(datetime.min),\n\t\t\tticks // dtds,\n\t\t\tticks % dtds,\n\t\t\ttz)\n\n#: class representing a regular minute\n_mins = from_ticks(60)\n\n#: class representing an hour\n_hrs = from_ticks(3600)\n\n#: class representing a day\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif ticks < 0:\traise ValueError(\"ticks must be >= 0\")\n\tif ticks < 86400000000000:\tticks = 7200000000 \n\tif ticks > 1000000000:\tticks = 600000000 - ticks\n\tif tz is None:\ttz = utc\n\treturn Tick(ticks, tz=tz)\n\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\ttry:\n\t\treturn cls._ticks.from_ticks(ticks, tz=tz)\n\texcept ValueError:\n\t\traise ValueError(\"ticks must be within [0,86400000000000] or None.\")\n\nclass Time(BaseTime):\n\t\"\"\"\n\tCreate a time from epoch.\n\n:param epoch: epoch of current time\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if epoch is out of bounds \n    (0 <="}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif not isinstance(ticks, int) or ticks < 0 or ticks > 8630000000000000:\n\t\traise ValueError(\"ticks must be an integer between 0 and 8630000000000000\")\n\t# Create a time from ticks\n\tt = cls.from_time(ticks)\n\t# If timezone information is available, convert it to a UTC time\n\tif tz!= None:\n\t\ttz = tz.tzname(tz)\n\treturn t\n\ndef from_microseconds(ticks, unit=None):\n\t\"\"\" Create a Time from ticks ("}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif (tz is None) or (tz.utcoffset(None) is None):\n\t\ttz = datetime.timezone(tz if tz is not None\n\t\t\t\telse get_timezone(tz))\t\n\treturn Time(ticks, tz=tz)\n\n\n\n\nimport operator\t# noqa\nfrom pandas import to_timestamp\nfrom pandas.tslib.base import _isna, _infer_dtype\n\n\nclass TimeSeries(object):\n\t\"\"\"\n\tWrapper around a time series.\n\n\tReturns the time at which the series"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif not isinstance(ticks, int) or ticks < 0 or ticks >= 8192:\n\t\traise ValueError, \"Tick value out of bounds\"\n\tif tz is None:\n\t\ttz = gettz(tzname(None) if tzname(None) is None else tz)\n\treturn cls(ticks, tz)\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\treturn Time.from_ticks(ticks, tz)\n\n"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif isinstance(protocol_version, tuple):\n\t\tif len(protocol_version)!= 2:\n\t\t\traise TypeError(\"protocol version tuple must have two elements\")\n\t\tprotocol_version, handler_class = protocol_version\n\t\tif protocol_version == (None,):\n\t\t\tpass\n\t\telif protocol_version[0] < 3:\n\t\t\traise TypeError(\"protocol_version cannot be a tuple containing \"\n\t\t\t           \"less than 3 elements\")\n\t\tif handler_class == None:\n\t\t\traise TypeError(\"handler class not passed\")\n\t\tif"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not isinstance(protocol_version, tuple):\traise TypeError, \\\n\t  \"Expected tuple of (integer,) but got (%s) instead.\" % \\\n\t  type(protocol_version)\n\tif len(protocol_version) > 1:\n\t\traise TypeError, \"Expected tuple of (integer,) but got (%s) instead.\" % \\\n\t\t  type(protocol_version)\n\tprotocol_version = protocol_version[0]\t# Take a key-only version\n\tfor k in sorted([x for x in v if"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\thandlers = {}\n\tprotocols = {}\n\tif protocol_version:\n\t\thandlers = {} # Dictionary of handler classes for this version\n\t\tfor protocol_version in protocol_version:\n\t\t\tprotocols['', protocol_version] = Class()\n\t\t\tprotocols[protocol_version[0], protocol_version[1]] = _load_protocol()\n\n__all__ = [ \t'load_protocols',\t'Class', 'protocol_handlers']\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\treturn {}\n\telif type(protocol_version) is not tuple:\n\t\traise TypeError(\"parameter protocol_version must be a tuple\")\n\tfor version in protocol_version:\n\t\tif version is None or (not isinstance(version, int) and\n\t\t\t\t\t\t\t\t not version.startswith(\"0.1\")):\n\t\t\tcontinue\n\t\tv_lower = version.lower()\n\t\tfor klass in cls.protocol_handlers():\n\t\t\tif klass.has_protocol(v_lower):\n\t\t\t\treturn"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\thandler_classes = {}\n\tprotocol = protocol_version if isinstance(protocol_version, tuple) else None\n\tfor name, handler in cls.__protocol__.__mro__.values():\n\t\tprotocol = handler.BOLT_PROTOCOL_HANDLER if protocol else None\n\t\tif protocol:\n\t\t\thandler_classes[protocol] = handler\n\tif protocol:\n\t\treturn {protocol: handler_classes[protocol]}\n\telif protocol:\n\t\traise TypeError(\"{} is a BOLT protocol but is"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is not None:\n\t\tprotocol_version = tuple(protocol_version)\n\tif protocol_version is None:\n\t\treturn {}\n\treturn {}\n\n@classmethod\ndef all_protocol_handlers(cls):\t\"\"\"Return a tuple of all supported\n    protocol handlers.\n\n    The tuple will contain a single element, the Bolt protocol\n    handler of type PB. This is the same as the protocol's\n    _proto_consts attribute.\n    \"\"\"\n\treturn tuple(PB)\n\n\t\ndef _handler_factory"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocols = {}\n\tprotocol_versions = []\n\t\n\t# Check the version against all supported protocol versions\n\tfor version in cls.protocolVersions:\n\t\ttry:\n\t\t\tprotocols[version] = cls\n\t\t\tprotocol_versions.append(version)\n\t\texcept Exception:\n\t\t\tcontinue\n\ndef register(protocol_cls):\n\t\"\"\"Register protocol. Supports version 0, 3 or 4.\"\"\"\n\tif isinstance(version, tuple):\n\t\tversion = (version,)\n\telse:\n\t\tif version is None:\n\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (3,5)\n\tfor (ver, handler) in protocol_handlers().iteritems():\n\t\tif ver == protocol_version:\n\t\t\treturn handler\n\traise TypeError(\"No Bolt protocol handler of version %s was \\\nfound\" % protocol_version)\n\t\t\n\t\nclass BoltProtocolHandler(protocol_protocolv):\n\t\"\"\"Protocol handler of a Bolt version (e.g. 3.5)\"\"\"\n\tprotocol_version = (3, 5)\n\ndef serialize"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_versions = {None: None}\n\telse:\n\t\tprotocol_versions.update({v: cls for v in protocol_version})\n\treturn protocol_versions\n\n\ndef get_client_protocol(protocol_version):\n\t\"\"\"\n\tGiven a specific protocol version, returns the client-side protocol\n\thandler class for the supplied one or None if the protocol is not\n\tsupported. If a specific protocol has not been initialized,\n\treturns the global default handler class, None.\n\t:param protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\tprotocol_version_set = set(protocol_version)\n\tversions = [x[:2] for x in protocol_version_set]\n\tversions = map(lambda x: tuple(x), versions)\n\treturn {v: h for v, h in itertools.izip(versions,\n\t                                          protocol_handlers(protocol_version=v))}\n\n\n\t\n\n"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\n\tcls.__bookmarks__ = values # We never want to create the class directly\n\treturn cls\t\n\t\n\t\n\t\ndef _from_raw_values(cls, source):\n\t\"\"\"Take a source object (a raw string) and convert to a Bookmarks object.\n\nThis method is used if you need to read a list of raw bookmark strings from\na different format than your bookmarks.\n\n:param source: raw bookmarks\n:type source: Iterable[str]\n:return: Bookmarks\n:rtype: Bookmarks\n\t\"\"\""}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn Bookmarks._create_from_raw_values(cls(), values)\t\n\t\n\t\n# TODO: Support lists\n# TODO: Support arrays\n\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tBookmarks.from_raw_values(values)\n\treturn cls(values)"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn cls(values)\n\n\t# Bookmarks object methods\n\t# ------------------------------------------------------------------------\n\t@property\n\tdef uri(self):\n\t\t\"\"\"\n\t\tGet the URI of a bookmark.\n\nThis method returns the unique URL to the bookmark, which is a path to a PDF file.\n\n:rtype: str\t\n\tThe URI for a bookmark.\n\t\"\"\"\n\t\treturn self.values.pop(0)\t\n\n\t@property\n\tdef title(self):\t\n\t\t\"\"\"\n\t\tGet the title.\n\nThis method returns the title of a bookmark.\n\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn cls()\t\n\n\t\ndef __iter__(cls):\n\t\"\"\"Returns an iterator over Bookmarks object objects.\"\"\"\n\treturn cls()\n\t\n\n\nclass EnumerationBookmarks:\n\t\"\"\"A class to enumerate Bookmarks objects.\"\"\"\n\n\t__members__ = ('__type__', '__bookmarks', '__values')\n\n\tclass Iterable(EnumerationBookmarks):\n\t\t\"\"\"An Iterable class for enumerating Bookmarks objects.\"\"\"\n\n\n\t\t@classmethod\n\t\tdef enumerate_from_raw_values(cls, values"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn Bookmarks(values)\n\n\ndef from_pickle(cls, serialized_dict):\t\"\"\"\n\tCreate a Bookmarks object from a pickled Bookmarks object.\n\n:param serialized_dict: the serialized bookmarks object as a pickled bytes object\n:type serialized_dict: Iterable[str]\t\"\"\"\n\treturn Bookmarks(serialized_dict)\n\n\ndef from_mongo_json(cls, raw_bookmarks):\n\treturn Bookmarks(raw_bookmarks)\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn Bookmarks{values}\n\n__all__ = [from_raw_values]\n\nimport _bookmarks as bm\n\n#\n# bookmarks\n#\n\nclass Bookmarks(bm.Bookmarks):\n\tdef __init__(self, values):\t# Bookmarks(values)\n\t\tbm.Bookmarks.__init__(self, values)\n\n\tdef get(self, key):\n\t\treturn self.__get(key)\n\n\tdef __get(self, key):\n\t\tval = self.__values[key]\n\t\tif not isinstance("}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tif not isinstance(values, Iterable[str]):\n\t\traise Argument(\"expecting to be iterable; got {type} {value}\".format(type=type(values), value=values))\n\n\tvalues = list(values)\n\n\tif len(values) < 1:\n\t\traise Argument(\"number of values must not be zero\")\n\n\t# split values into names and values\n\tvalues = list(values)\n\tnames = []\n\tvalues = list(values)\n\tfor val in values:\n\t\tnames.append(val)\n\t\tvalues.remove(val"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn cls(list(values))"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\n\timport bookmarks\n\treturn bookmarks.from_raw_values(values)\n\t\t\n\n\n\n\n\n\n\n\n\n\n"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif bufsize is None:\treturn _get_seq_with_type_and_bufsize(seq,None)\n\treturn _get_seq_with_type_and_bufsize(seq,bufsize)\n\ndef _get_seq_with_type_and_bufsize(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\n\tIf *seq* is a sequence type, then sequence is a tuple;\n\tOtherwise sequence is a string.\t\t\n\tIf *seq* is not a sequence type then return a tuple ("}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif not seq:\n\t\treturn [], type(seq)\n\tif bufsize is None:\n\t\tbufsize = MAXBUF\n\twhile True:\n\t\tl = len(seq)\n\t\tfor i in xrange(l):\n\t\t\tif i in (ord(x) for x in seq[i:i+bufsize]):\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn seq, seq[i-l:i]\n\treturn seq, seq[0]\n\ndef _print_seq_with_type(seq, indent, **kwargs):\n\tseq ="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n\tif bufsize is None:\tbufsize = 1\n\n\treturn ((seq, seq.type), bufsize)\t#\n\ndef _get_types_with_seq(seq, bufsize=None):              #\n\t\"\"\"\n\t\tReturn a (types, sequence) pair.\n\tSequence is derived from *seq*\n\t(if it is of a type)\n\t\tor\n\t\tif the sequence is of a sequence type\n\t\tthe type of the sequence.\n\n\t\"\"\"\n\tif bufsize is None:\tbufsize = 1\n\n\treturn ((seq, seq."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\ttype_ = type(seq)\n\tif seq and type_ in (type, SequenceOf):\n\t\treturn seq, type_\n\telif type_ is not None and\n\t\tisinstance(seq, (List, Set, Tuple, Iterator, type, Range)):\t# list, set, list[], map, list()\n\t\treturn seq, type_\n\telse:\t# str or unicode\n\t\treturn seq\n\t\ndef _get_type_str(data):\n\t\"\"\"Given a data, return its type name, raising a ValueError if\n\tthere"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tseqid = seq.type_id\n\tassert type(seqid)==str \n\t# If seq is of type list, return sequence of that type\n\tif not isinstance(seq, type(Numpy.array())):\n\t\treturn (_cast(seq,Numpy.array),)\n\telse:\n\t\treturn (_cast(seq,'sequence'),)\ndef _array_to_sequence(_seq, bufsize=None):\n\t\"\"\"\n\tConvert sequence into a sequence (array), in case ``.ndim!= 1``.\n\tIf ``.ndim!="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif seq is not None:\n\t\tif isinstance(seq, bytes):\n\t\t\treturn seq, bytes\n\t\telse:\n\t\t\treturn seq, _parse_string_literal(seq)\n\telif not seq:\n\t\tseq = 0\n\tn = bufsize\n\tif bufsize is not None:\n\t\tassert isinstance(bufsize, int)\n\t\tn = bufsize\n\treturn seq, len(seq), n\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif seq.__class__ is not str:\n\t\treturn _get_seq_with_type(seq, bufsize)\n\telse:\n\t\treturn _get_str_with_type(seq)\t\n\n################\n# Generic helpers\n################\n\ndef get_default_parser(_doc='Parser'):\n\tparser = get_parser()\n\tparser_kwargs = {\t\"name\": 'Parser 1.0' }\t\n\treturn parser._subparser(parser_kwargs)\n\ndef _get_doc(cls):\n\tdoc = cls."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tseq.validate() # this method is supposed to help if the sequence has no type!\n\treturn _get_seq_iter(seq, bufsize=bufsize)\n\t\ndef _get_seq_iter(seq, bufsize=None):\n\ttry:\n\t\treturn iter(seq), type(seq)\n\texcept TypeError:\n\t\treturn iter(seq), tuple(map(str, seq))\n\t\t\ndef _dump(seq, type, file):\n\t\"\"\"For use in Python 2 only.\"\"\"\n\ttry:\n\t\tfile.write(\"%s %"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif seq is None: return (None, None)\n\tif type(seq) in (list, tuple): return (list(seq), type(seq))\n\telse: return (seq, type(seq))\n\ndef _make_seq_out_of_type(seq, outtype, bufsize, error=None):\t\"\"\"\n\tCreate a sequence out of the sequence and the output\n\ttype, according to optional error function.\n\tIf the sequence has already a type that is not of the type specified\n\tby *outtype*, create outtype (*outtype*) with the"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif not bufsize:\n\t\tbufsize = len(seq)\n\n\t# Create a sequence:\n\tcurr = list(seq)\n\tl = 0\n\twhile curr:\n\t\t# Find the last element that is of a type\n\t\tlast = -1\n\t\tfor i in range(0,len(curr)):\n\t\t\tif isinstance(curr[i],int):\n\t\t\t\tif i!= l and curr[i] <= bufsize:\n\t\t\t\t\tlast = i\n\t\t\t\tbreak\n\t\t# Create this sequence if necessary\n\t\tif last == -1:\n"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None: # use the current value of self and use that\n\t\tother = self[:] # so scalar values just go at the end\n\tif other < 0.0:\n\t\traise LenaValueError(\"No scale provided\")\n\tif other > 1.0:\n\t\traise ValueError(\"Unsupported scale: %f > 1\" %other)\n\tif recompute:\n\t\tself._scale = other # compute new scale for self\n\treturn self._scale # will not recomputef the scale anymore\n\ndef isinteger(obj):\n\t\"\"\"\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif isinstance(other, Real) and len(self['values'])!= len(other['values']):\n\t\t\traise ValueError, \"Histogram's rank mismatch\"\n\n\tdelta = self.domain.max - self.domain.min\n\tm = delta.value\n\n\tif other is None:\n\t\t# Compute (scale * sqrt(delta / sqrt (delta.value * self['count'])**2)) ** 0.5\n\t\treturn math.sqrt(delta.value / math.sqrt(delta.value * self['count']"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tval = None\n\tself.lock.acquire()\n\tif recompute:\n\t\tif val is not None:\n\t\t\tif val.scale == 0.:\n\t\t\t\traise LenaValueError()\n\t\t\tif other is None:\n\t\t\t\treturn val.scale\n\n\telif other is not None:\n\t\tval = self.scale(other)\n\n\tself.lock.release()\n\treturn val\n\ndef _solve_for_other(self, other, recheck=False, solve_diags=SOLVE_DIAGAMS, max_solve=None"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\n\t# normalize all axis\n\tif self.axis == 0xFF: # for float\n\t\tscale = 1./sum(self)\n\telse: # for int\n\t\tscale = sum(self[:, self.axis[0]])/sum(self)\n\t\n\t# check input\n\tif not isinstance(other, (Ntuple, t.Ntuple)):\n\t\traise TypeError(\"Scale factor must be a Ntuple, got \"+str(type(other)))\n\telif other is None:\n\t\tother = ()\n\t\t\n\t#"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is not None:\n\t\tother = float(other)\n\n\t\tself.is_valid_scale = other and other < self.max\n\t\tif self.is_valid_scale and not other:\n\t\t\traise TypeError\t(\"{} cannot be zero\".format(self.name))\n\n\t\tif recompute:\n\t\t\tself.recompute_scale()\n\t\telse:\n\t\t\tself.scale = other\n\nclass LenaHistogram(Histogram):\n\t\"\"\"A :class:`.Histogram` that uses Lena's ``NumbaScale``.\"\"\"\n\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif isinstance(other, intc):\n\t\tif other < 0:\n\t\t\traise ValueError('other cannot be negative')\n\t\treturn sg.integrate(other, None, recompute)\n\telif isinstance(other, sg.integral) and isinstance(self, sg.integral):\n\t\treturn self.scale(scal=other)\n\telif not any(isinstance(other, sg.integer) and (0. <= other) <= self):\n\t\traise ValueError('other must be 0 to itself')\n\telse:\n\t\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif not isinstance(other, Scalar):\n\t\traise ValueError(\"other must be a Scalar\")\n\tif not recompute:\n\t    if scale!= 0:\n\t\t    return self.get_scale()\n\t\tself.__calculated_scale()\n\t\tself.__makedescr()\n\t\tself.recompute_scale()\n\telse:\n\t    recompute = True\n\t    d = self.hist(recompute)\n\t    if self.scale!= 0:\n\t\td = d.__makedescr()\n\t    "}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tself._validate_scale_integral()\n\tscale = self.histogram.scale\n\tif other is None:\n\t\tif recompute:\n\t\t\treturn scale\n\t\telse:\n\t\t\treturn scale_int(self.min(), self.sum(), self.bin_width())\n\telse:\n\t\tif recompute:\n\t\t\treturn self.__scale(other)\n\t\telse:\n\t\t\treturn self.__scale_int(other.scale(self.min().log), 1, self.bin_width())\n\nclass _Histogram2d(Histogram2D):"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other == None:\n\t\treturn self.scale(* self.size, recompute=recompute)\n\telse:\n\t\tself.scale(*float(other))\n\n\treturn self\n"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t# XXX: should support float instead?\n\t# maybe add a method: get_scale() to make this better.\n\tif isinstance(self, LenaColorHistogram):\n\t\treturn\n\tif self.scaling == 1.0 or (recompute is False not in ((1,2),(3,4)):\n\t\treturn\n\t\n\tscaled = 0\n\twhile scaled<other:\n\t\tscaled = self.scale*2.0**scaled\n\t\t\n\t\tif self.scaling == 4.0 or (recompute is False not in"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\n\tif other == None:\n\t\treturn self.x * self.y\n\telse:\n\t\treturn other**2.\n\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\n\tif other==None:\n\t\tif self.scale(axis=-1) == []:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(other,int):\n\t\tif (self.scale(axis=other) == []) and self.is_contig:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\t\n\t\n\t\n#\n#\n#\n\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\n\tif other==None:\n\t\treturn self.scale_X(other_coord=None)\n\telse:\n\t\treturn self.scale(other)\t"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\ttry:\tres = self.rescaled()\n\texcept:\n\t\tif other is None:\n\t\t\tres = self._rescale()\n\t\telse:\n\t\t\traise\n\treturn res\n\ndef get_geometry(*pos):\t\"\"\"\n\tReturn the geometry, which is determined by\n\t*pos* which is a string or an array of coordinates.\n\t\"\"\"\n\tif isinstance(pos, (list, tuple)):\n\t\treturn pos[-1]\n\tif len(pos) >= 1:\n\t\tif isinstance(pos[-1], (list,"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif other is None:\n\t\treturn self.scalar\n\n\n\tw = self.width\n\th = self.height\n\tif other is None or not isinstance(other, Base):\n\t\tif other:\n\t\t\t# scalar case: assume everything is a scalar\n\t\t\t#\n\t\t\t# i.e., *x*, *y*, etc.\n\t\t\t#\n\t\t\t# in other case, assume that w*h*h == 1\n\t\t\tif w: w = 1. / w\n\t\t\tif h: h = 1. / h\n\t\telse:\n\t\t\treturn w"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\treturn self.get_v_tagger().coords_.scale(*(self.coord(other)))\n\ndef scale(point):\n\treturn point.x, point.y, point.z\n\ndef zoom(point):\n\treturn (np.clip(point.z, 1 - 0.1, 20),\n\t          np.clip(point.y, 1 - 0.1, 20),\n           \t\t    (zoom,))\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif other == '**':\n\t\treturn self.__scale(1)\n\telif other is None:\n\t\treturn self.__scale(self.units)\n\telif hasattr(other, 'value'):\n\t\tif type(other).__name__ =='scale':\n\t\t\treturn other.units\t# can just access field directly\n\t\telse:\n\t\t\tif other in self.fields:\n\t\t\t\treturn self.fields[other]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tif other < 0:\n\t\t\t\t\t\tdef zero_scaled(val):\n\t\t\t\t\t\t\treturn lambda:"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\n\tdef _scale_coords(self):\n\t\tif self.x!= 0:\n\t\t\t# avoid 0 division\n\t\t\treturn self.x,\n\t\t\n\t\t# TODO: should this be:\n\t\t# if self.y!= 0:\n\t\t# return self.y\n\t\t\n\t\treturn 0\n\t\ndef get_bbox(rgraph):\n\t\"\"\"Get `(lower_left, upper_right)` coordiantes of points\n\tinside the bounding box of a graph.\n\tIf the edges are parallel, the upper-left coordinate is\n\t("}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tpass #__delitem__\n\t\ndef show(plot, cmap=\"Greys\", zscale=None):\n\t\"\"\"\n\tShow the graph or show the whole dataframe.\n\nIf *plot* is a string, or if it does not start with ``+``, then\nshow the plot. Otherwise, show its associated image.\n\n\t\t>>> print(show('x'))\n\t\tz|\n\t\t------\n\t\t  _\n\t\n\tCan also have ``zscale=None``, which will result in showing the scale.\n\nWill show the given figure if there's a"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\n\t\n\tif self.name not in (self.width, self.height, self.x, self.y, self.z,): \n\t\tg = copy.deepcopy(self)\n\t\tscale = 1.0\n\t\tif self.z not in (0,):\n\t\t\tscale = 100.0 / self.z\n\t\tg.scale(-scale, -scale)\n\t\treturn g\n\t\n\t\n\t#other.name must be a field name.\n\tif other not in self.fields:\"bad field name\"\n\tscale, other = self.fields"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tname = field_names(hist, hist_field_name, \"graph\")\n\tdataroot = field_root(hist, hist_type, \"graph\")\n\tif not make_value:\n\t\thist_values = {}\n\telse:\n\t\thist_values = make_value(hist)\n\n\tsigma_values = []\n\tsigma_hist, error_values = get_sigma_and_error_of_bin(hist, get_coordinate=get_coordinate)\n\tfor sigma in sigma_hist:\n\t\tfor value, error"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tgraph_params = graph_params or {}\n\tgraph_params.update({\"label\":make_value, \"axes\":get_coordinate})\n\n\tdef get(key):\n\t\t# TODO: make key a function of param and name\n\t\tret = hist_data[key]\n\t\tret.remove(key)\n\t\treturn ret\n\n\treturn hist_to_graph(\n\t\tget(make_value) if isinstance(make_value,tuple) else hist,\n\t\tget(has_context), get(get_coordinate), get(field_names),"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t)\n\treturn bin_to_graph(hist, hist, make_value, get_coordinate, field_names, scale, hist, hist_to_graph)\n\nhist_to_graph.__doc__ = hist.__doc__\n\n\ndef plot_graph(data, axes=None, scale=True, hist=True, color_axis=True,\n\tmake_value=None, get_coordinate=\"left\", hist_to_graph=None,\n\tmake_errorbars=None, error_bar_width=0.5, error_bar_color"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\treturn graphify(hist, get_coordinate=get_coordinate,\tdicts=dicts,\n\t\tmake_value=make_value,\n\t\tfield_names=dicts[\"field_names\"],\n\t\tscale=dicts[\"scale\"])\n\t\n\t\n\t\t\n\ndef histogram_to_graph(hist, mean, count,   *args, **kwargs):\n\t\"\"\"Convert :class:`histogram` to a graph.\n\nWe have several options for returning a graph,\nthe first one is the same as ``hist``.\nThe second"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t)\n\nhist.help(make_value=make_value,\tget_coordinate=get_coordinate,\t\"histogram\")\n\n#\n# histogram as visualisation\n#\n\nhist.histogram_output_format = \"png\"\nhist.help(make_value=make_value,\tget_coordinate=get_coordinate,\t\"histogram\",\n\t\t\t    )\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tdic = h5py.File(self.fn_input, \"r\")\n\tcoord = dic.open(\"get_coordinate\")\n\tif not isinstance(coord,h5py.dims):\n\t\traise ValueError(\"coordinate is not a variable!\")\n\n\tcoord = dic['get_coordinate']\n\tcoord.value = get_coordinate\n\tdel dic\n\tdic.close()\n\tdic.close()\n\n\tif make_value is not None:\n\t\tmake_value(dic)\n\t\n\tf ="}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\treturn make_graph(hist, make_value, get_coordinate,\n'(x, y_mean, y_mean_error, x_mean, x_mean_error)')\n\nclass graph_from_hist(object):\n\tdef __init__(self, hist, x_coordinate):\n\n\t\tself.hist = hist\n\t\tself.x_coordinate = x_coordinate\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tEvaluate histogram.\n\t\t\"\"\"\n\t\tfor _, bins in self.hist.iter_bins():"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tfrom matplotlib.pyplot import rcParams\n\trcParams[\"lines.linewidth\"] = 0.5\n\trcParams[\"lines.antialiased\"] = True\n\trcParams[\"lines.markersize\"] = 0.5\n\trcParams[\"lines.color\"] = \"#ffa0f0\"\n\trcParams[\"lines.alpha\"] = 0.8\n\ty, x = plot_with_histo()\n\treturn y\n\n#\n#   plot_with_histo()\n#\ndef plot_with_"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\treturn {'graph':graph.graph(hist, make_value, get_coordinate, field_names, scale, hist_to_graph.__module__, hist_to_graph.__name__)}, hist_to_graph.__name__"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\treturn Histogram2D(\"Histogram\", hist, make_value=make_value,\n\t# get_coordinate can't be a str, it needn't be\n\t# \"left\", \"middle\" or \"right\".\n\tget_coordinate=get_coordinate, field_names=field_names,\n\tscale=scale, hist=hist)\n\ndef hist_2d(hist):\n\t\"\"\"\n\tReturn a new :class:`.Histogram2D` histogram\n\n\tReturn the resulting histogram.\t\"\"\"\n\treturn Histogram2D(\"Hist"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative:\n\t\tzope.interface.provide(iface, candidate, True, True)\n\t\treturn True\n\tzope.interface.provides(iface, candidate)\n\tzope.interface.provider(iface, candidate)\n\tif not candidate.providedBy(iface):\n\t\tzope.interface.implements(iface, candidate)\n\treturn False\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif not (iface and candidate.providedBy(candidate.__class__)):\n\t\tfor error in _verify_errors(iface, candidate):\n\t\t\tyield error\t\n\t\treturn True\n\terror = 'The interface %s was automatically generated by IPython'\n\tif not all([iface.providedBy(candidate.__class__),\n\t\tcandidate.providedBy(iface.__class__)\n\t\tfor error, cand in zip(\n\t\t\t_verify_errors(iface, iface.providedBy(candidate.__class__)),"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\ttry:\n\t\tif iface is not None:\n\t\t\tif not any(vtype is t for t in iface.providedBy(candidate.__class__)):\n\t\t\t\tif candidate.__class__.providedBy(iface):\n\t\t\t\t\treturn candidate.__class__ == iface\n\t\t\t\tif isinstance(iface, zope.interface.interfaces.IIdentity):\n\t\t\t\t\t# We should never reach this with IIdentity\n\t\t\t\t\treturn candidate.providedBy(iface.providedBy(iface))\n\t\tcandidate.providedBy(candidate.__class__"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\ttentative = (not tentative) and 'tentative' in config._getOptions()\n\tcandidate.verify(iface, tentative=tentative)\n\ndef get_tentative(candidate=None):\n\t\"\"\"Return the candidate, or the default zope.interface implementation\n\tthat is required to satisfy the interface. It should either be the\n\tdefault for a `zope.interface.implementation` in the configuration\n\tor a ``zope.interface.implementation`` directive, or a `zope.interface`\n\tclassdef as documented by"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\treturn _verify.__bool__(iface, candidate, tentative, vtype)\n\ndef _verifyNestedMeth(iface, candidate, vtype=None):\"\"\"\n\tVerify that *candidate* might correctly provide *iface*\n\twith method or attribute ``meth``.\n\n\t:param iface: The interface to verify.\n\t:type iface: zope.interface.Interface\n\n\t:param candidate: The candidate interface to verify.\n\t:type candidate: zope.interface.Interface\n\n\t:param vtype: If not `None"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\treturn True\n\n\n##\n## Importing other implementations\n##\nfrom. import ztupletree\n\n\n# The _compare methods were never used by ZODB\n##\nztupletree.compare = ztupletree._compare\n\n\n##\n## The _compareFrom implementation contains the actual methods.\n##\nfrom. import _compareFrom\n_compareFrom._compareFrom = _compareFrom._tupleFrom\n\n\n##\n## The _tupleFrom implementation contains the actual methods.\n##\nfrom. import _tupleFrom\n_tuple"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif iface.providedBy(candidate):\n\t\treturn True\n\tif (candidate, tentative) not in iface.providedBy(candidate):\n\t\tpassed = True\n\telse:\n\t\tpassed = False\n\tfor method in Candidate.interfaceExtensions(candidate):\n\t\ttry:\n\t\t\tif type(method)!= MethodType:\n\t\t\t\tmethod = method.implementedBy\n\t\t\t\tif not type(method) == type:\n\t\t\t\t\tcontinue\n\t\texcept AttributeError:\n\t\t\tpass\n\t\tif abs(method(iface).__dict__.get"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tdef _check(candidate, iface):\n\t\tcheck = candidate\n\t\tif 'providedBy' not in iface.providedBy.attrs:\n\t\t\tif iface in check.providedBy.attrs:\n\t\t\t\tcheck = None\n\t\tif not isinstance(check, Candidate):\n\t\t\tif type(check) is type and type(iface) is type and check:\n\t\t\t\tcandidate = check\n\t\tif candidate:\n\t\t\treturn candidate._verify(iface, tentative=tentative)\n\t\treturn False\n\n\tlog.doError('trying to verify failed:"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tiftype = _gettype(iface, tentative=tentative)\n\n\tdef check(cand):\n\t\t\"\"\"Check that *cand* is defined by this interface.\"\"\"\n\t\t# Check class\n\t\tcand.implementedByClass()\n\n\t\t# Check interface\n\t\tif iftype.providedBy:\n\t\t\treturn iface.providedBy(cand)\n\n\t\t# Check interface\n\t\telse:\n\t\t\treturn iface.providedBy(cand) == iftype\n\n\n\treturn _verifyIface(iface, tentative=tentative)\n\ndef _verifyIface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif not any(_verifyChecker(iface, candidate,\n\t\t\t\t  tentative=tentative, vtype=vtype)):\n\t\t\t# no other checks could pass\n\t\t\treturn False\n\treturn True\n\n\ndef _verifyChecker(iface, candidate, vtype=None, v_attributes=None):\n\t\"\"\"Returns True or False depending on if *vtype* (type of\n\tinterface) differs from *iface*.\n\n\tFor **v_attributes**, *candidate* is a tuple.\n\n\tThe type of *v_att"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif not candidate.providedBy(iface):\n\t\terrors.append(zope.interface.Invalid(\"No candidate provided by %s\" % (candidate)))\n\t\treturn False\n\tif not candidate.doesImplement(iface):\n\t\terrors.append(zope.interface.Invalid(\"No candidate implements interface %s\" % (candidate)))\n\t\treturn False\n\tif iface.providedBy(candidate):\n\t\terrors.append(zope.interface.Invalid(\"The candidate is already provided by %s\" % (iface)))\n\t\treturn False\n\treturn True"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif not candidate or not Candidate.providedBy(candidate):\n\t\treturn False\n\n\t# Now, the only step required: Make sure the candidate has a __provides__\n\t# method, which will tell us what it claims is.\n\n\t# FIXME It's questionable if this step is a best fit if there are\n\t# more than one __provide__s (e.g. you do not specify or provide the\n\t# default interface):\n\t# Make sure we get at least the default implementation (if provided)\n\t# This is checked by ``zcprovider.Z"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\t# Make sure the candidate provides the interface of the interface\n\tif candidate.providedBy(iface):\n\t\treturn True\n\t\t\n\t# Make sure all its required methods have the same signature\n\tmethods = candidate.methods()\n\tif len(methods)!= Candidate.methodsWithDifferentSignature(candidate):\n\t\traise zope.interface.Invalid(\n\t\t\t\"The interface has different interface methods and/or \" + \n\t\t\t\"signature for this interface!\")\n\t\n\t# Skip the tentative checks if any of the previous conditions\n\t# do"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\treturn not tentative and bool(candidate.providedBy(iface))\n\ndef hasDependency(ifaces, interface, dep):\n\t\"\"\"\n\tGet the set of interfaces where candidates depend on *interface*\n\tand *dep* is a zero base or more interface.\n\n\tThis provides the set of interfaces, as well as\n\tthe dependencies of methods and attributes, which were used for\n\tbuilding this iface, together.\n\n\t::\n\n\t\tifaces = ['MyInterface'].union([('MyInterface2', 'MyInterface2'],\n\t\t  ['MyInterface2"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# We use the \"candidate\" object directly on its bases, so that\n\t# we can skip the class verification if it is not provided by\n\t# the candidate.\n\tfor interface in iface.providedBy(candidate):\n\t\ttry:\n\t\t\t# The candidate's 'providedBy' method will use\n\t\t\t# zope.interface.implementer -- if not the same, it's\n\t\t\t# an alternative to checking what can be looked up.\n\t\t\tisImplementerType = type(iface.implementedBy(interface))\n\t\t\tiface.providedBy("}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# pylint: disable-msg=E0611\n\t# pylint: disable-msg=W0622\n\tcandidate = Candidate(candidate)\n\tif candidate.providedBy is None:\n\t\traise errors.Invalid(\"Can't verify object %r claims interface '%s'.\"\n\t\t                 % (candidate, candidate.interface.__name__))\n\tif iface.providedBy is None:\n\t\traise errors.Invalid(\"Can't verify object %r claims interface '%s'.\"\n\t\t                 % (candidate, candidate.interface.__name__))\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tfrom zope.interface.declarations import interface\n\tfrom zope.interface.declarations import implements\n\n\tifaceTypes = []\n\tif'methods' in candidate:\n\t\tassert all(x.isMethod() for x in candidate['methods'])\n\telse:\n\t\tcandidate['methods'] = candidate['providedBy'](ifaceTypes)[]\n\n\tfor ifaceDecl in candidate['providedBy']:\n\t\tassert ifaceDecl is iface\n\tfor method in candidate['methods']:\n\t\tif tentative and not ifaceDecl.providedBy:"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\tresult = (iface.providedBy(candidate) or \n\t\t (candidate.providedBy(iface) and not tentative))\n\tif result:\n\t\treturn iface\n\telse:\n\t\traise zope.interface.Invalid( iface, candidate, result[0] )\n\t\t\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\timport zope.interface\n\treturn zope.interface.providedBy(iface, candidate)\n\n\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif not(isinstance(iface, zope.interface.Interface) and\n\t        iface.providedBy(candidate)):\n\t    # (Actually, we could do this to avoid looking at any existing\n\t    # implementations for this candidate, but it would introduce\n\t    # problems with \"implementing <zope.components.interfaces.IImplementer>\",\n\t    # which can't be tested by the caller).\n\t    raise zope.interface.Invalid(\n\t\t'There is nothing that could provide the class \"%s\"' % candidate)\n\n"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t# This method checks first that there has been something\n\t# at all of the interfaces passed.  If there has been nothing\n\t# at all then the candidate will be None.  This is to be really\n\t# an assert and never to test things in a way that won't work.\n\t#\n\t# *candidate* will not be None at this point unless\n\t# some further testing (say, just a typo) passes for some non\n\t# interfaces.\n\n\ttry:\n\t\tif not (candidate and candidate->isSubClassOf(iface)):\n\t\t\traise"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not candidate.isGeneric:\n\t\treturn True\n\tassert iface.isGeneric\n\tname = CandidateClass(iface).getName(candidate)\n\tif name == candidate.name:\n\t\treturn True\n\treturn False\n\n__all__ = [\"verifyClass\",\"VerifyType\"]\n\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tifaces, interfaces = getInheritedInterfaces(candidate)\n\tfor interface in ifaces:\n\t\tif not determineIface(iface, candidate, tentative):\n\t\t\treturn False\n\t\tif candidate.get(interface):\n\t\t\tcontinue\n\t\tcontinue\n\tiface = candidate.pop(ifaces.keys()[0])\n\tif determineIface(iface, candidate, tentative):\n\t\treturn True\n\treturn False\n\n# Get interface(s) of a callable, starting with the current\n# interface.\ndef getCallArgsIndex(call, ifaces"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not candidate.is_subclass(iface, tentative=tentative):\n\t\traise RuntimeError(\"Class %s does not provide '%s' interface\" % (candidate.__module__, candidate.__name__))\n\nclass UnimplementedClass(type(baseclass.type)):\n\t\"\"\"\n\tProvides unimplemented methods\n\t\"\"\"\n\tdef __init__(self, name=None, bases=(baseclass,)):\n\t\tif name:\n\t\t\tself.__name = name\n\t\tif bases:\n\t\t\tself.__bases = bases"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif (candidate.is_base_interface or candidate.is_parent_interface):\n\t\traise Exception(\"You try to use a class that already provides interfaces!\")\n\n\t# We have a non-base class\n\tifaceClass = candidate\n\n\treturn True\n\nclass Class(Interface):\n\t\"\"\"\n\tGeneric class which provides a set of methods that will return\n\teither None or the appropriate type\n\t\"\"\"\n\tdef verifyClass(self, candidate, tentative=False):\n\t\t\"\"\"\n\t\tVerify that the candidate is a valid *self*.\n\t\t\"\"\"\n\t\tif ("}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not candidate:\n\t\treturn False\n\n\t# FIXME: this should be in a base class.  i18n\n\tif not getattr(getattr(candidate, 'interface_provided','False'), iface):\n\t\treturn False\n\timport warnings\n\n\t# FIXME: This should probably eventually be a decorator.  i18n\n\tif (candidate._deps.get(iface)\n\t\tand not candidate._deps.get(iface, (True,False))[0]):\n\t\twarnings.warn(\"%s is a dependency of %"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\n\tclass_vars = inspect.getmembers(iface, lambda s: inspect.isfunction(s) and not inspect.ismethod(s))\n\tfor cvar in class_vars:\n\t\tif cvar[0] in candidate and not cvar[1] in iface:\n\t\t\t# if you have to, don't do that one! - it's not guaranteed a method can return that function, if a function is passed to an interface twice, the\n\t\t\t# interface must get called only once.\n\t\t\tprint('The class %s is not supported on this interface"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not candidate.is(iface):\n\t\treturn candidate\t\n\tret1 = False\n\tret2 = False\n\tif candidate.is(tentative):\n\t\tret2 = True\n\t\tret1 = True\n\n\t\tif not candidate.isInterface:\n\t\t\tfor i in range(0, len(candidate) - 1):\n\t\t\t\tif candidate[i].is(tentative):\n\t\t    \t  ret2 = candidate[i]!= candidate[i+1]\n\t\t\t\t    if ret2:\n\t\t\t\t      break\n\t\t\t\t    if not candidate[i+1"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tiface = candidate[0]\n\tcandidates = candidate[1:]\n\t\n\t# Look through all candidates to see if any one can provide *iface'.\t\n\tfor c,p in candidates:\n\t\tifp = p['if'].split('#')[1]\n\t\tif not ifp in ifaces:\n\t\t    ifp = ifaces[ifaces.index( iface )]\n\t\tif ifp not in ifaces:\n\t\t\tif not tentative:\n\t\t\t\treturn False\n\t\t\telif ifp._tries and p['tries'] >= ifp"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not candidate:\n\t\treturn True\n\tif not cls.have_interface(iface):\n\t\tif not tentative:\n\t\t\treturn True\n\t\tfor candidate in ifaces.itervalues():\n\t\t\tif not cls.areElementsEqual(iface, candidate):\n\t\t\t\treturn False\n\treturn True\n\ndef areElementsEqual(a, b):\n\tr = True\n\treturn ((r and cls.areElementsEqual(a, b)) or\n\t\t(not r and not cls.areElementsEqual(b, a"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t# See PEP461 for explanation\n\t# Note: it may be a good idea to avoid using both explicit_mc and metaclass altogether if you're\n\t# setting up or testing for the presence of python-2.4\n\tif not explicit_mc and __has_metaclass__:\n\t\treturn '_metaclass'\n\telse:\n\t\tif len(bases) > 1 and bases[-1] == (object, object):\n\t\t\t# this catches both.__metaclass__ and.__mro__\n\t\t\tbases = bases[:-1]"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t# if explicit_mc is None and bases is not None and not set():\n\t# \tprint('WARNING: no explicit_mc nor bases given')\n\t# # add explicit_mc to bases if given\n\t# print('Looking for {}...'.format(';'.join(sorted(bases))))\n\tif len(bases) > 0:\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = 'Metaclass'\n\t\treturn types.ModuleType, bases[0], explicit_mc\n\t\n\tmet_cls = None\n\tfor cl"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif bases and not explicit_mc:\n\t\texplicit_mc = None\n\t#print(\"determining metaclass\")\n\tif bases and isinstance(bases[0], (type, types.ModuleType)):\n\t\tmetaclass = explicit_mc\n\telif getattr(bases, '__bases__', None):\t\t\t# assume classes\n\t\tmetaclass = bases.__bases__[-1]\n\telif hasattr(bases[0], '__bases__'):\n\t\tmetaclass = baseclasses[bases["}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc or len(bases)==1 or len(bases)==0:\n\t\treturn explicit_mc or bases[0].__class__\n\telse:\n\t\treturn bases[0].__class__\n\n\n\n\nclass Logger(object):\n\t\"\"\"\n\tClass which handles logging of messages to disk.\n\t\"\"\"\n\tdef __init__(self, logger_name=None):\n\t\t\"\"\"\n\t\tClass constructor\n\t\tParameters\n\t\t----------\n\t\tlogger_name\toptional\tname of an existing logger which will be\n\t\tcreated with this class\n\t\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\n\tif not bases:\n\t\t# implicit __metaclass__\n\t\tif explicit_mc:\n\t\t\treturn explicit_mc\n\t\telse:\n\t\t\t# try for __metaclass__ if available\n\t\t\tif hasattr(getMetaClass(), '__metaclass__'):\n\t\t\t\treturn getMetaClass().__metaclass__\n\t\telse:\n\t\t\t# try for __metaclass__ when not explicit\n\t\t\treturn bases\t\n\telse:\t\n\t\t# find __metaclass__ if available\n\t\tif hasattr(getMetaClass(), '__metaclass__"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tfoundDMetaclass = False\n\tfor base in bases:\n\t\ttry:\n\t\t\tif issubclass(base, metaclass.instance) and issubclass(base, metaclass):\n\t\t\t\treturn bases\n\t\texcept TypeError:\n\t\t\tpass\n\tfor base in reversed(bases):\n\t\tif issubclass(base, metaclass.instance) or issubclass(base, metaclass):\n\t\t\tfoundDMetaclass = True\n\treturn classes.no_metaclass if foundDMetaclass else classes.metaclass\n\t\ndef"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tdef default():\n\t\ttry:\n\t\t\treturn bases[0].__dict__['__metaclass__']\n\t\texcept:\n\t\t\treturn bases[0].__dict__['__metaclass__']\n\tif '__metaclass__' in bases:\n\t\treturn explicit_mc or default()\n\tif '__dict__' not in bases:\n\t\treturn default()\n\tmetaclassName = None\n\tfor name, obj in bases[0].__dict__.iteritems():\n\t\tif name.startswith('__metaclass__'):\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc:\treturn explicit_mc\n\tif not bases:\treturn None\n\tif bases[0] is None:\treturn None\t\t# Must be a subclass\n\tm = bases[0].__metaclass__\n\tif not m:\t\treturn None\n\twhile bases:\n\t\tbases = bases[1:]\n\t\tm, = b.__dict__['__metaclass__'].__dict__['__slots__']\n\t\tif not all([sub.__class__.__name__ in m for sub in m]):\n\t\t\treturn m"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tsimp = []\n\tcur = bases[0]\n\n\tif len(bases) > 1:\n\t\tfor parent in bases[1:]:\n\t\t\tif cur is parent:\n\t\t\t\tsimp.append(cur.__name__)\n\t\t\telse:\n\t\t\t\tsimp.append(parent.__name__)\n\telse:\n\t\tsimp[0] = 'object'\n\tcur.__metaclass__ = ExplicitMetaclass\n\tif explicit_mc and cur.__name__ in explicit_mc:\n\t\tmt = explicit_mc[cur.__name"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif bases is None:\n\t\treturn None\n\n\texplicit_mc = explicit_mc or {}\n\tsubclasses, subclass_dict = set(), {}\n\tfor base in bases:\n\t\tfor base_type in bases[base]:\n\t\t\tsubclasses, subclass_dict = set(subclasses), set(subclasses)\n\t\t\tsubclasses_in_type = subclasses.union(reversed(list(subclasses_dict.values())))\n\t\t\t#print(\"subclasses in \" + base_type.__name__ + \", \" + str(subclasses_in_type))"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tv = self.__find(key)\n\tif v is __marker:\n\t\traise KeyError(key)\n\treturn v.pop(default)\t# v can be None if default not given\n\ndef popitem(self):\n\ttry:\n\t\treturn self.__find('').popitem()\n\texcept KeyError:\n\t\tpass\n\ndef setdefault(self, key, default=None):\n\tif default is None:\n\t\tdefault = self # make set default\n\tif key in self:\n\t\treturn self.__find(key) # key"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tself.checkStack(1)\n\tval = self._stack_.pop(key)\n\tif val == __marker:\n\t\tself._stack_.pop(key)\n\t\tval = self._stack_(key, default)\n\treturn val\n#\t\treturn key, default\n#\tdel key, default\n#\treturn key\n\t\ndef pop(self, *keys):\n\t\"\"\"Remove elements using a *first* argument: key if given,\notherwise element at end of stack with first key.\"\"\"\n\tself.checkStack(len(keys))\n\ttmp_"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif self is None:\n\t\treturn default\n\telif hasattr(self, \"__weakref__\"):\n\t\tself = self.__weakref__()\n\tif key in self:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn value\n\telse:\n\t\treturn default\n\nclass NotAnObject(NamedTuple):\n\t\"\"\"not object type\"\"\"\n\n\tnotarealshape = ()\n\t\"\"\"The Not-A-Shape object shape, which is the (2,)\"\"\"\n\n\t# N.B. NotAnObject isn"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif default is obj = __marker:\treturn self.pop(key)\n\telse:\n\t\tif key in self:\n\t\t\tret = self.pop(key)\n\t\t\tdel self[key]\n\t\t\treturn ret\n\t\telse:\n\t\t\treturn default\n\t\n\nclass DictProxy:\n\t\"\"\"Proxy that extends the Dict class so that it allows user-defined accessors\"\"\"\n\tdef __init__(self, kwargs = None):\n\t\tif kwargs is None:\t\tkwargs = {}\n\t\tself._kwargs = kwargs\n\tdef"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif key in self.keys:\n\t\ttry:\n\t\t\treturn self.__get(key)\n\t\texcept KeyError:\n\t\t\treturn default\n\telif self._find(key) is not None:\n\t\traise KeyError(\"Can't find key: %s\" % key)\n\telse:\n\t\treturn default\n\n\t\ndef dict_keys(dict):\n\t\"\"\"Returns all keys in a dictionary.\"\"\"\n\tkeys = {}\n\tfor k in dict.keys():\n\t\tif k in keys:\n\t\t\tkeys[k].add(jid)\n\t\telse:\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif key in self:\n\t\tif default is __marker:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tdel self[key]\n\telse:\n\t\treturn default\n\n# A very simple function (no use of global namespace) that calls a\n# function in the scope of a variable. Useful in the context of a C:\n#  x = x+y/z\nimport time\ndef barler(a=(2, 3)):\n\t\"\"\"Print out a string: returns the sum of the inputs\"\"\"\n\tprint a[0]+\"+\"+a["}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif default is __marker:\n\t\tif key in self:\n\t\t\ti = self.__class__.index(key)\n\t\t\tdel self[key]\n\t\t\tself.__length -= 1\n\t\t\tif i == 0:\n\t\t\t\tself.__free = None\n\t\t\treturn self[key]\n\t\telse:\n\t\t\traise KeyError, key\n\telse:\n\t\treturn default\n\ndef setlistdict(d, v, init={}, merge_keys=None, free=None, update=None):\n\t'''\n\tSet list of values for a dictionary object."}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif default is __marker:\n\t\treturn key\n\telse:\n\t\tself._update()\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept IndexError:\n\t\t\treturn default\n\t\n\tdef popitem(self):\n\t\tself._update()\n\t\ttry:\n\t\t\treturn self.__popitem()\n\t\texcept IndexError:\n\t\t\ttry:\n\t\t\t\tkey = next(self.keys())\n\t\t\t\tself[key] = self.pop(key)\n\t\t\texcept StopIteration:\n\t\t\t\traise KeyError('popitem(): dictionary is empty again')\n\t\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\treturn super(OrderedDict, self).pop(key, default)\n\texcept KeyError:\n\t\treturn default\n\t\n\t\n#\n# for compat with 2.x compatibility\n#\n\nprint type(OrderedDict) \n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif default is __marker:\t\treturn key\n\n\tind = self.__findkey(key)\n\n\tif ind == -1:\n\t\treturn default\n\n\tdel self[ind]\n\n\treturn self[ind]\n\t#@@ to make it efficient, just need to do it right... XXX\n\ndef set(self, key, value):\n\t\"\"\"set(self,key,value) -> None..\"\"\"\n\tind = self.__findkey(key)\n\tif ind!= -1:\n\t\tdel self[ind]\n\t\tself[ind] = value\n\n"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\ttry:\n\t\tdel self._items[key]\n\texcept KeyError:\n\t\traise KeyError(key)\t\t\n\treturn key, value\n\n\n\ndef iteritems(self):\n\t\"\"\"Iterate over all `(key, value)` pairs, in sorted order.\n\n\tIf the given mapping is a tree, then this will be an iterator\n\twith an infinite amount of pairs to read.  The first entry of \n\tthe pair is always returned, so as to speed up finding the one most often.\n\n\t\"\"\"  \n\n\tfor value in self._items.iterv"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tl := len(self.items)\n\tif l == 0:\n\t\treturn self.items\n\to := self.items[0]\n\tdel(self.items, 0, l-1)\n\treturn o\n\n\ndef updatelist(l, v):\n\tif l < len(v):\n\t\treturn updatelist(len(v), v[:l])\n\tfor i, x in enumerate(v):\n\t\tv[i] = x for x in l[:i+1]\n\treturn v\n\n\nclass Dict(list):\n\tdef"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\n\tkey = self.popitem()\t\t\n\t\n\tif len(self) > 0 :\n\t\treturn key\n\t\t\n\treturn None"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tcount = len(self)\n\n\t\t# Remove by frequency with smallest probability.\n\tif (count == 0):\n\t\treturn (None, None)\n\t\t\n\t\t# Remove by probability with minimal frequency.\n\twhile (self)!= [] and (count-1) > self.frequency:\n\t\tt2, r = self[(self.frequency - 1)]\n\t\tif r <= 0.75:\n\t\t\tbreak\n\t\tcount -= 1\n\t\tself.popitem()\t# Unwrap it\n\n\telse:\n\t\treturn (None, None)\n\n\nbasket = Basket("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tpos_key, pos_value = self._popitem()\n\treturn (pos_key, pos_value)\n\n\t\n\t\ndef popitem(self):\n\t\"\"\"\n\tRemove and return the most frequent position and value pair.\n\t\n\tThe element is removed to avoid having duplicates. (See the documentation\n\tfor get).\n\t\n\tIf no values are available, returns (None, None) where the position doesn't exist and None.\n\t\n\tIf the element is deleted, it's remove() is called. This may result in a call to a\n\tdelete"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn list(self.__dict__.pop(self.__key__,None))\n\n\nclass dict(list):\n\t\"\"\"\n\tImplements a doubly-linked list.  The 'values' attribute of the object is\n\ta dictionary of one-time-frequently used key-value pairs that is stored\n\tin the first element of entries in the entry set.  In order to store\n\tadditional data, the values attribute may not be changed; set the values\n\tattribute only if you're sure your data will never change; the values\n\tattribute of an object cannot"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn self.popitem(0)\n\ndef peekitem():\t\t\"\"\"Read the next most frequent item from the Queue.\"\"\"\n\treturn self.peek(0)\n\ndef push(val):\t\"\"\"Push val onto the Queue and then return the popped value\"\"\"\n\treturn self.push(val)\n\ndef pop(val):\t\t\t\"\"\"Peek value off the Queue and return it\"\"\"\n\treturn self.pop(0)\n\ndef peek(val):\t\t\"\"\"Peek value off the Queue and return it\"\"\"\n\treturn self.peek(0)"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\n\tmax_usage = 0\n\twhile True:\n\t\tcurr = self.popitem()\n\t\tif not curr:\n\t\t\tbreak\n\t\tfor key, value in curr:\n\t\t\tif value*3 <= max_usage:\n\t\t\t\tm = len(self)\n\t\t\t\tself.remove(curr)\n\t\t\t\tdel self[key]\n\t\t\t\tdel self[key]\n\t\t\t\t#print key, min(m, 5)\n\t\t\t\tif m == 5:\n\t\t\t\t\tbreak\n\t\tmax_usage = curr[1]\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn self.tree_iter()()[0]\n\n\nclass tree2d(tree_iter): #(tree_iter, tree_iter)):\n\t\"\"\"\n\tFind the `tree2d`_ at root of the passed tree.\n\n\t.. _tree2d: http://en.wikipedia.org/wiki/Tree2D\n\t\"\"\"\n\tdepth_method = lambda : (np.diff(sorted(t)) for t in sorted(t))\n\n\tdef popitem(self):\n\t\treturn (t, self.tree_iter()()[1])\n\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn self[min(self.counter, self.length - 1)]\n\n\t\ndef insert(arr, val):\t\"\"\"\n\tInsert val into arr sorted by increasing order of count.\n\t\n\tWarning: \tThis uses an `OOM`, see http://stackoverflow.com/a/442654.\n\tWarning:\tIt only works if the values are increasing.\n\t\n\tWarning:\tIt is not currently safe to call this on arrays that are not sorted by count.\n\t\n\t\n\t>>> arr = [1,2,3,4]\n\t"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn self.list[0]\n\ndef makelist():\t\n\treturn l = []\n\tfor i in xrange(16,0,-1):\n\t\tl.append(i)\n\ndef randlist(i=2):\t\n\tl=makelist()\n\tj=0; i+=(j<<1) # the number of times the list to shuffle\n\tt=i*i*i*i; t+=(j<<2) # the number of times the list to shuffle\n\tl1=makelist();l1.extend(l);l1"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tnval, minval = self\n\tif nval:\n\t\tassert nval == minval\n\t\tdel self[minval]\n\t\treturn(self.popitem())\n\telse:\n\t\treturn None\n\n\t# return the key most recently used, a copy (self, None) if none\n\t\ndef isprime(n):\n\t# this method will only work if n is a prime number\n\t# but it has the possibility to see a very low answer\n\ttry:\n\t\treturn (1 if ((n & (n - 1)) == 0) else True)\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t# pylint thinks this return is safe but will crash on my machines\n\treturn(self.__dict__['_cache'].pop(self.key) or None)\n\t\n\n\nif __name__ == '__main__':\n\t#\n    ## main(verbosity -- int, 0 = info; 1 = error\n\t##\n\tpass\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tif self._tail:\n\t\tp = self._tail.split()\n\t\titem = self._head.split()\n\t\tif item[:len(p)] == p and item[-len(p):] == p:\n\t\t\tp2 = p[:len(p2)]\n\t\t\tif p2 == p2:\n\t\t\t\tself._head = self._tail = None\n\t\t\t\treturn item[-len(p):]\n\t\t\telse:\n\t\t\t\tself._tail = self._tail[:-len(p2) - 1]\n\t\t\t\treturn item[-len(p2):]"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\ttry:\n\t\tdel self._key_cache[self.best]\n\texcept KeyError:\n\t\traise KeyError('Key not stored')\n\treturn self._key_cache.pop(self.best)\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tlst = self.iteritems()\n\tlst.reverse()\n\treturn lst.next()\n\t\n\n\n\nclass Dict(object):\n\t\"\"\"A dictionary.\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\t\tArguments:\n\t\t\t\n\t\t\t   *args\t\t\t\t\tThe arguments to initialize the dictionary with.\n\t\t\t   \n\t\t\t   *kwargs\t\t\t\tThe keyword arguments to initialize the dictionary with.\n\n\t\t\tExample:\n\t\t\t\n\t\t\t>>> from datahub.common.databindings import Dict\n\t\t\t>>> d ="}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\n\tif self.key: # If our key is non-empty.\n\t\tif self.val is not None:\n\t\t\tdelete(self.val) # We delete the associated data.\n\t\tdel self.key, self.val\n\treturn self.key\n\n\t\nclass RedirectedList:\n\t\"\"\"Class representing an in-memory linked list of objects.\"\"\"\n\n\tdef __init__(self, capacity = None):\n\t\t\"\"\"Creates a new List object.\tCapacity is in number of items.\t\"\"\"\n\t\t\n\t\tif capacity:\n\t\t\tself.capacity ="}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tlst = self.listData()\n\tk, v = lst[-1]\n\tdel lst[-1]\n\treturn k, v\n\t\n\t\n\t\nclass ListData():\n\tr\"\"\"\n\tA tuple of keys, values, and total size, as two lists.\n\t\"\"\"\n\tdef __init__(self):\t\t# Initialize with length 0.\n\t\tself.length = 0\n\t\tself.suspect = \"\"\n\t\t\n\t\t\n\tdef __len__(self):\t# Return current total size. \n\t\treturn self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\n\tif self._max:\n\t\tself._max -= 1\n\t\treturn self._item\n\n\treturn None\n\n@util.memo()\ndef add(self, obj):\n\t\"\"\"Add obj to the queue.\n\tReturns a new instance whose attributes are copied from obj;\n\n\tNote: Does not use Queue as it is often more efficient to copy obj than this method.\"\"\"\n\tnew = type( self, type(obj)() ) # copy object, not reference\n\tnew._max = self._max + 1\n\tnew._queue = self._queue + [ new"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn self._map.pop(max(self._map.keys(), key=self._map.get), ())\n\n\t\ndef popitemlist(self):\n\t\"\"\"\n\tRemove and return (key, value) pairs from a list\n\tor other collection of lists.\n\t\n\tThe function will return only the *most-recent* `key` pair, oldest first.\"\"\"\n\titems = self._map.items()\n\titems.sort()\n\treturn [(key, value) for (key, value) in items if value > self._map[key]]\n\n\t\ndef"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif self.items:\n\t\to = self.items.pop(0)\n\t\treturn o\n\t\t#logger.debug(str(o))\n\treturn None\n\nif __name__ == \"__main__\":\tmain()\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\ts = self.getitem(last=True)\n\tdel self[s]\n\treturn s\n\n################\n##\n##\n##\n##\nclass Node(object):\n\tdef __init__(self, value=None):\n\t\tself.value = value\n\n\t@property\n\tdef next(self):\t\"\"\"Return the next node in the link traversal\n\t\tsequence given by the current child node\"\"\"\n\t\trval = self.next   # TODO check this\n\t\tif rval is None:\n\t\t\trval = Node()\n\t\telse:\n\t\t\trval"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\td = self\n\n\tif d.keys:\n\t\t(key, value) = d.keys[-1]\n\t\td.keys.pop()\n\t\td.vals.pop()\n\t\tif not d.keys:\n\t\t\td.vals = []\n\t\t\td.keys = { }\n\t\treturn (key, value)\n\n\nclass MemcacheStore(pymongo.collection.MultiMongodbCollection):\n\t\"\"\"Store the objects in a single DB\"\"\"\n\n\tdef __init__(self, name=\"memcachedclient\"):\n\t\tpymongo.collection."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\n\treturn self._dict.popitem(last=False)\n\t\n\t\n\nclass Bunch(Dict):\n\t\"\"\"Maintains a mapping, iterating over it is possible, so a Dict.view() method can\n\tbe used.\t\n\t\n\t.. warning::\n\t\tThis Bunch class is not meant to be used by anyone.\"\"\"\n\t\n\tdef __init__(self, **kwargs):\n\t\tsuper(Bunch,self).__init__(**kwargs)\n\t\n\tdef __getitem__(self, key):\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif len(self._store) == 0: return (None, None)\n\t# first element is always the most recently used\n\titem = self._store.popitem(last=1)\n\titem[2] = None\n\treturn item\n\nif __name__ == \"__main__\":\n\t# print \"STARTING WITH COOKIE TESTS FOR COOKIEKEY:\", self._cookie_test['key1']\n\t# if self._cookie_test['key1']!= \"value1\" or self._cookie_test['key2']!= \"value"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\ts, key = self.list.popitem(last=True)\n\tdel self.list[key]\t\n\n\treturn key, s\t\n\t\n\t\nif __name__=='__main__':\t\n\timport csv\n\trouteconf=CSV()\n\twith open(\"../log_config.csv\") as csvfile:\n\t    reader = csv.reader(csvfile)\n\t    for row in reader:\n\t        routeconf.set(row[0], row[1], 1)\n\twith open('"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\treturn self.__dict_dict.pop(self.selfindex)\n\n\t\n#===============================================================================\ndef pop(self, key='value'):\n\n\tif(key=='value'):\n\t\treturn self.__dict_dict.pop(self.selfindex)\n\telif(key=='index'):\treturn self.selfindex\n\telif(type(key) == type(0)))\treturn self.__dict_dict.pop(key)\n\telse\t\t\treturn self.__dict_dict[key]\n\treturn self\n\t\ndef popitems"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\ti, j, p = self.__tree.find(key = self._h)\t\n\tdel self._h._parent[p]\n\tdel self._h\n\treturn self._h\n\n\t\n\nclass BaseHashtree:\n\t__max_size = 64 \n\thashmax = 0\t\n\t_h = {}\n\t_h.update = update\n\t_h.__repr__ = _repr\n\tdef __init__(self):\n\t\tself.__tree = self   ## initialize tree\n\t\tself.__count = 0\t\t## sum of"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\treturn self.pop()\n\t\n\t\ndef popvalue(self):\t\"\"\"Gets the `(key, value)` most recently used.\n\t\n\tReturns:  the `(key, value)` most recently used, if any,\n\t\tor None if none.\n\t\"\"\"\n\tif self.has_key(self._name + '_more') and self.has_key(self.popkey):\n\t\treturn self.popkey\n\treturn None\n\n\n\n\n# Global cache\n#\n# This isn't very good, but it's the fastest way to work"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tvalue, key = self[:].popitem(False)\n\treturn (key,value)\n\n\n\ndef append(s, key, value):\t\"\"\".\n\tAppend a string to the end of a sequence, returning the last item\n\tthat was appended (not overwriting the last or first value).\n\t\"\"\"\n\tif not s:\n\t\traise ValueError, 'cannot append \"%s\": sequence is empty' % key\n\tlast = [key]\n\tlast.append(value)\n\tif not last:\n\t\tvalue, last = last[0"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn ( self[self.random.randrange(len(self))]\n\t\t\t\t\t, self[self.random.randrange(len(self))] )\n\n\n\t \n####################\n\nclass RandomList():\n\t\"\"\" A list of random elements. A list is built once, based upon a seed. \"\"\"\n\tdef __init__(self):\n\t\tself.list  = []\n\t\tself.current  = []\n\t\t\n\tdef add(self, element, seed=None):\n\t\tif not seed == None:\n\t\t\tcurrent = \"\"\n\t\t\tfor i in range("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tself._count = len(self._data)\n\treturn self._data.popitem()\n\n\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tn = len(self)\n\tif n == 0: return ()\n\tindex = randrange(n)\n\treturn (self[index], self[index + 1])\n\n# Test code to make tests run\nfrom Tkinter import *\ndef main(*args, **kwargs):\n    root = TkRoot(width=320, height=480, **kwargs)\n    frame = Tkinter.Frame(root)\n    frame.pack()\n    window = Tkinter.Text(frame)\n    window.pack(side=LEFT)\n    win"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tresult = self.popitem()\n\treturn result \n\n\ndef poplist(self, size=1):\t\"\"\"\n\tRemove and return a list of `size` random `(key, value)` pairs.\n\t\"\"\"\n\tresult = self.poplist()[:size]\n\treturn result\n\n\ndef pop(self, key):\t\"\"\"\n\tRemove and return `key` pair.\t\"\"\"\n\treturn self.pop(key)\n\n\ndef exists(self):\t\"\"\"\n\tCheck whether a key is present in a list.\n\t\"\"\"\n\tself.keys"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tfor el in self._list.items():\n\t\t(key, value) = el\n\t\tdel self._list[key]\n\treturn (key, value)\t\n\ndef add(key, val):\n\t\"\"\" Add a new `val` to set of value for `key`\n\t\tKey is used for debugging and error-reporting\"\"\"\n\tself._list[key] = val\n\t\n\t\t\ndef set(key, val):\n\t\"\"\" Set a value for key, return `True` for success\"\"\"\n\tself._list[key] = val\n\treturn True\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tif len(self)==0: return None\n\treturn self[-1]\t\n\n\n\nclass ListNode(object):\n\t'''\n\tA node in a List.\n\t'''\n\tdef __init__(self,key,value):\n\t\tself.key = key\n\t\tself.val = value\n\n\tdef __str__(self):\n\t\treturn str(self.key) + \" = \" + str(self.val)\n\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tidx = random.randrange(len(self))\n\titem = self.items[idx]\n\tdel self.items[idx]\n\treturn item\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tfor i in xrange(len(self)):\n\t\tif self[i] is not None:\n\t\t\tself.delitem(i)\n\t\t\tbreak\n\tif not self:\n\t\treturn None\n\ttry:\n\t\treturn self[random.randrange(0,len(self)-1)]\n\texcept IndexError:\n\t\treturn None"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\ttry:\n\t\treturn self._itertok\n\texcept AttributeError:\n\t\tself._itertok = self._itertok[self._popitemindex]\n\t\treturn self._itertok[-1]\n\nclass dictmap(dict):\n\t\"\"\"Dummy dictionary subclass with one item of type string which\n\tpoints to a string argument. Such a dummy class is useful\n\tfor the map function, which maps the string *keys* to another\n\tdictionary of of type string\"\"\"\n\nclass numpydict(dict):\n\t\"\"\"Dummy dictionary subclass with one item"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\ttry:\n\t\tif self.has_pop: self.popitem()\n\t\telse:\n\t\t\traise KeyError\n\texcept KeyError:\n\t\treturn (None, 'empty')\n\n\nif __name__ == '__main__':\n\tclass Item: pass\n\n\timport argparse\n\n\t# parse command line arguments\n\tparser = argparse.ArgumentParser(description = \"\"\"\n\tRemove an arbitrary number of items from\n\tlist, using dict as database.\n\n\tIf multiple arguments are provided,\n\tall the arguments are removed.\n\n\tIf the key is missing, remove it"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(re.escape(\"\\.\"), re.IGNORECASE)\n\ndef _replace_string(self, s: str) -> str:\n\treturn s\n\ndef _parse_parameter_string(self) -> int:\t\n\t\"\"\"\n\tParse the inputstring (as a string) into a (possibly nested) list.\n\n:rtype: list of :class:`unicode`\n\n\t.. note::\n\n\t\tIf given a string, its result will be an array [1,2,3,4], and so on. In"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(self['param'])\t\n\n\t# Note: this is a bit a hack.  We need the function create_regex_from_repos\n\t# in order not to get the full match.  So in that case, we will use the\n\t# function _create_regex_receives_to_look_at;\n\t# TODO: this is too complex.  We should create the regular pattern only\n\t# once.\n\t#\n\t# TODO: remove it.\n\t# import sys\n\t# sys.stder"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn self._in_re.re_compile(self._in_string_regex, -1)\n\n################################################################################\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(b\"\\\\p{In}\", re.IGNORECASE)\n\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile('?in?([\\w|\\.|\\n]*)/(?!\\/)', re.MULTILINE)\t\t##<![^/]\n\n\n\nclass _S(Pattern):\n\t\"\"\"\n\t:class:`_S` class.\n\t\"\"\"\t\t\n\t_repr_patterns += [_create_out_regex(self)]:\n\t\t\n\tdef _get_in(self, in_):\t\t\t\t\n\t\treturn _getattr(_create_in_regex(self), in_)(\n\t\t\tself)\t\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(self.parameter_regex)\n\ndef _create_out_regex(self) -> Pattern:\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(self.output_regex)\n\ndef _create_param_regex(self) -> Pattern:\t\"\"\"\"\"\"\n__all__ = [\"IN_PATTERN\",\n\t\t\"OUT_PATTERN\",\n\t\t\"PARAM_P"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(r'\\b\\d+%')\n\ndef _check_integer_range(self) -> bool:\n\t\"\"\"Check if the given range is valid.\n\n\tIt is a no-op if the given range was not found in the input string.\n\t\"\"\"\n\treturn len(re.match(_check_regexp, self).groups()) % 1 == 0\n\n\ndef _create_mapping_from_regex(re_parameter, in_string):\n\t\"\"\"Create a mapping function from the input string to the given regex.\"\"\"\n\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(self.value)\n\nclass Parameter(_PatternBase):\n\t\"\"\"\n\tThe :class:`Parameter` object represents the object.\n\tThe following attributes are available:\n\n\t   :ivar name: the parameter's name\n\n\t   :ivar value: the parameter's value, it should be a :class:`bytes` of the same\n\t   length as the :class:`bytes` represented by the parameter's value.\n\n\t   :ivar get_value: returns the value of the parameter.\n\t\"\"\"\n\tdef __init__("}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn type(self).pattern(\".*\")\n\n\ndef _create_out_regex(self) -> Pattern:\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn type(self).pattern(\".*\")\n\ndef _create_empty_regex(self) -> Pattern:\t\"\"\"\n\tCreate an empty regular expression.\n\nReturns a re.Pattern (:class:`re.Pattern`).\t\"\"\"\n\treturn type(self).pattern(''"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\n\treturn re.compile(self.param, re.IGNORECASE | re.MULTILINE)\t\n\n\nclass QueryParam(ConstantParam):\n\n\t\"\"\"Describes a parameter that is a query parameter by using the\n\t:func:`ParameterFactory.create` method to create one or more\n\t:class:`~jython.datasource.dataproc.QueryParam` objects.\n\t\n\t:Param:name,\n\t:Param:value\n\t:Param:description,\n\t:Param:required,\n\t:Param:default,"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn [self._create_option('convert'), self._create_option('style'), self._create_option('style'), self._create_option('style'), self._create_option('style'),\n\t\tself._create_option('label', 'Style'), self._create_option('style'), self._create_option('style'), self._create_option('style'), self._create_option('style'), self._create_option('label', 'Labeling')]\n\n\ndef _create_option(my_type:typing.Any, my_default=False) -> typing.Any"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn self.copy()\t\n\nclass _converting(_converting.Base):\t\n\t'''\n\tConvert parameters from a list to a :class:`._converting.ArgumentConverter`.\n\n\tThis is essentially a wrapper class for the :class:`._converting._ArgumentConverter`,\n\tbut it does *not* create a specific converter per class instance (and thus creates\n\ta new one), and it takes the parameters stored in its constructor as an argument.\n\t'''\n\tdef __init__(self, *args, **kwargs"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn self\n\tconverter = property(_get_converter)\n\ndef _get_converter(self) -> _converting.DefaultStyleConverter:\t\"\"\"\n\tGet the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting.DefaultStyleConverter`).\t\"\"\"\n\t# the converter can be called without initializing it (in which case nothing is initialised), so we should\n\t# check which type was given before we call to initialize the converter.\n\ttry:\n\t\tself.style == self.param"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tfrom coda.utils import _parse_dict_converter_map, _parse_dict_converter\n\n\treturn self._ConverterFactory.create(self._parse_map, self._parse_dict)\n\nclass _ParameterConverter:\t\"\"\"\n\tBase type base for parameter style converters.\n\n\"\"\"\n\n\t_is_parameter_converter: bool\n\n\t@_internal_method\n\tdef _is_parameter_converter(self) -> bool:\n\t\t\"\"\"\n\t\tThis method describes how parameters are converted.  If a"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tif self._converted_value == None:\n\t\treturn _converting._Converter(_value)\n\tif self._converted_value == 0:\treturn _converting._Converter(_value)\n\tif self._converted_value == True: return _converting._TrueConverter(_value)\n\tif self._converted_value == False: return _converting._FalseConverter(_value)\n\traise ValueError(\"Parameter 'value' must be 0, true, false, or [0, 1, 2].\" % self)\n\nclass"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tif self.is_convertible(None) == False:\n\t\tpass\n\telif self.type(None) == 'int':\n\t\tif isinstance(self.value(None), int):\n\t\t\treturn self.convertible_type(None, 'int')\n\t\telse:\n\t\t\treturn self.convertible_type(None,'str')\n\telse:\n\t\treturn self.convertible_type(None,'str')\n\n\n\tdef _is_convertible(self) -> bool:\n\t\t\"\"\"\n\t\tCheck if the value of this"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tif self.converter is None:\t\t\t# Use the global ParameterStyleConverter\n\t\treturn super(_converting, self)._create_converter()\t# Return the instance directly for speed\n\telse:\t\t\t\t\t\t\t\t# Else check if converter\n\t\tfor item in self.converter:\t# Loop for each converter\n\t\t\tconverter = item(self).get_converter()\n\t\t\tif converter is not None:\treturn converter\t# Else use the converter. If any error occurs, return None.\n\treturn None\n\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._Converter(self)\n\n\tdef _make_input_dict(self) -> Dict:\t\"\"\"\n\tReturn a dictionary with the input parameters.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn Dict(_input_dict)"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tif isinstance(self.params[self.method].value, str):\n\t\traise TypeError('conversion is a function call, parameters must be plain strings for the call, got {}'.format(type_str(self.params[self.method].value)))\n\t\n\tif len(self.params)!= len(_converters) and _converters:\n\t\traise ValueError(\"conversion call accepts only callable parameters; got {}\".format(_converters.keys()))\n\t\n\tclass_ = type(self.params[self.method].value)\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn [str, float, _tuple2int]\n\n\ndef _convert_method(self, **kwargs):\n\t\"\"\"\n\tReturn the parameter style converter given this converter.\n\n\t:param paramstyle: the parameter style of the constructor\n\t:type paramstyle: :class:`~paramstyle.ParagraphStyle`\n\n\tThis function can be used to override the default from\n\t:class:`~paramstyle.ParagraphStyle`. It can be used to\n\tprovide user-defined style values to the argument.\n\n\tThis will return the parameter style converter given the"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\treturn parse_iso8601(dt_str)\n\ndef _parse_date(dt_str):\n\t\"\"\"\n\tParse a :class:`datetime.datetime` into a :class:`dateutil.tz.tzoffset`.\n\n\tA :class:`datetime.timedelta` will be returned as the fractional\n\tsecond, or as the whole `dateutil.relativedelta`.\n\n\t.. versionadded:: 2.7.0 - Added to support `dateutil.relativedelta`\n\t\n\t\"\"\"\n\n\tdt_str ="}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\ttry:\n\t\tstr_dt = utils.to_datetime(dt_str)\n\t\tif dt_str!= str_dt.strftime(dt_str_format) and str_dt.tz:\n\t\t\tdttmp = str_dt.astimezone(tzutc())\n\n\t\t\t# We can't use isoparse since this is a python/dateutil library -\n\t\t\t# however this was deprecated because of the change in the string.\n\t\t\tdtstr = (dt_str, dttmp.strftime(dt_str_format))\n\t\telse"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tif isinstance(dt_str, str):\n\t\tdt_str_parts = dt_str.split(':')\n\t\ti = 0\n\t\tcomponents_len = len(dt_str_parts)\n\t\tif components_len > 1:\n\t\t\tdt_str = \"\".join(dt_str_parts[:i]+[dt_str_parts[i]]+[\"T\"]*(components_len-i-1))[:-1]\n\t\telse:\n\t\t\tdt_str = dt_str_parts[0].upper()+dt_str"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tmatch = isoparse.search(dt_str)\n\n\tif match is not None:\n\t\treturn match.group()\n\telse:\n\t\t# In the future use something like the parse function to\n\t\t# parse the datetime string, not just the compiled pattern.\n\t\ttry:\n\t\t\treturn isoparse.match(dt_str).group()\n\t\texcept TypeError:\n\t\t\t# In 2.7.0 there was a bug with regular expressions that\n\t\t\t# wasn't catching all of these when they are compiled with\n\t\t\t# some additional flags.\n\t\t\traise Value"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tif isinstance(dt_str, text_type):\n\t\tif 'T' in dt_str:\n\t\t\tisoparse(dt_str + 'T')\n\t\telif 'I' in dt_str:\n\t\t\tisoparse(dt_str + '.00000000')\n\t\telif 'i' in dt_str:\n\t\t\tisoparse(dt_str + '.00000000', False)\n\t\telif '-' in dt_str:\n\t\t\tif isoparse(dt_str) == isoparse(dt_str[:3]):\n"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\ttry:\n  \t\t\t# parse into datetime\n  \t\t\tdt = dateutil.parser.parse(dt_str)\n\texcept Exception, e:\n  \t\t\tprint \"invalid date-string '%s' encountered in isoparse\" % dt_str\n  \t\t\tif e.args[0] in (dateutil.parser.ParserError, dateutil.parser.ParseError,\n  \t\t\t\t\t  dateutil.parser.ParseOtherError):\n  \t\t\t\t\traise e.args[0]\n  \t\t\telse:\n  \t\t\t\treturn dateutil.parser.parser_ex"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\treturn self.parse(dt_str)\n\n    def __eq__(self, other):\n        \"\"\"Compare two datetimes for equality.\nThis is only true of the two datetimes have the same numeric and\ntime components, and the equality operator is stable (i.e.,\nall but the first element) across different time zone offsets\nfor the same time zone offset date.\n\n        :param other:\n            Otherwise: a date or a datetime tuple\n        :return:\n            True if the two are equivalent, False otherwise\n        \"\"\"\n        if isinstance(other, datetime."}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\ttry:\n\t\tt = datetime_type(dt_str)\n\t\tv = t.strftime(self.TZ_PATTERN)\n\t\tif hasattr(v, \"strftime\"):\n\t\t\tt.strftime(self.TZ_PATTERN)\n\t\treturn v\n\texcept Exception:\n\t\tif dt_str.endswith(\"+00:00\"):\n\t\t\treturn datetime.datetime(1970, 1, 1)\n\t\telse:\n\t\t\tif '-' in self.TZ_PATTERN:\n\t\t\t\tt = dat"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\treturn IsoFormat(dt_str).get_date()\n\n\t\n\t\n# =============================================================================\nclass IsoFormat(object):\n\t\"\"\"\n\tImplementation of the `ISO 8601 date format <http://en.wikipedia.org/wiki/ISO_8601>`_ parser algorithm.\n\n\t.. versionchanged::\n\t\t2014-07-29\n\t\t:class:`IsoFormat` has been renamed *in* 1.9.0\n\n\tUsage in Python:\n\n\t>>> try:\n\t>>>\tIsoFormat()\n\t... except ParseError, e:"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tdt_str = datetime.strptime(dt_str, '%m/%d/%Y %H:%M:%S')\n\treturn dt_str\n\ndef from_parsed_iso8601(dt_str):\t\"\"\"\n\tCreate a :class:`dateutil.iso8601.Base` from the parsed ISO-8601 datetime.\n\n\t:param dt_str:\n\t\tISO-8601 datetime string to be parsed.\n\n\t:return:\n\t\tAn instance of :class:`dateutil.iso8601"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tself.parse(timestr, default, **kwargs)\n\n\nif datetime == datetime_module:\n    from datetime import tzinfo as tzinfo\n    from dateutil.parser import parsetime as parsetime_module\n    from dateparser import parse as parse_module  # this is faster\n\n\n\n"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\treturn _parser(timestr, default=default, ignoretz=ignoretz, **kwargs)\n\ndef _parser(timestr, default=None, ignoretz=True, **kwargs):\n\t\"\"\"Parses a string to produce a :class:`datetime.datetime` object.\"\"\"\n\tm = _parse_re(r'''(?x)(^|[^0-9:+\\-])([0-9]{4}:)?(?:([0-9]{1,2}):)?(?:(?P<fuzz"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tself._fuzzy_with_tokens = True\n\tself.timestr = timestr\n\tif self._parse_tzid:\n\t\t# FIXME: can't use the same in both parse_tzid and parse\n\t\tself.parse_tzid = parse_tzid\n\tself.raw = raw\n\ttry:\n\t\tself._parse()\n\t\treturn self\n\texcept OverflowError:\n\t\tpass\n\texcept Exception:\n\t\traise ParserError(\"Invalid datetime string, \\\n\t\tcan't parse date/format, can't parse '"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tdatetime = _parse(\n\t\t\t   default, timestr, ignoretz=ignoretz, ignoreunc=ignoreunc,\n\t\t\t   timestampFunc=timestampFunc, **kwargs)\n\n\t# handle a naive timezone default that was not set in the\n\t# constructor, but with a timestamp fallback\n\twarnings.filterwarnings(\"ignore\", deprecation.DeprecationWarning)\n\n\tif timestampFunc is None:\n\t\tdatetime.strptime(timestr), datetime\n\telse:\n\t\tdatetime.strptime(tim"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\ttimestr = unicode(timestr)\n\ttimestr = re.sub(r\"\\s\", \" \", timestr)\n\t\n\tres = _parse(str(timestr), fuzzy_with_tokens=True, **kwargs)\n\tif res is None:\n\t\traise ParserError(\"Could not parse datetime: %r\" % timestr)\n\telse:\n\t\treturn res\n"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\treturn self.__parser__(timestr).astimezone(default or self.__defaults__)\n\ndef _tzformat(tzone):\n\t\"\"\"\n\tReturn a formatted string for a specific ``tzinfo`` or ``tzinfo``\n\tparameter.\n\n\tThe `tzone` argument can be a time zone name as string or a :class:`tzinfo`\n\tand/or ``tzonefile`` value, or a\n\t:class:`datetime.tzinfo` object.\n\n\tThe result will be a format string as specified in\n\t:"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\treturn _parse(*args, **kwargs)(timestr, default = default, ignoretz = ignoretz,\n                                      tzinfos = tzinfos)\n\n# Local imports\n# ~~~~~~~~~~~~~~\n\n\n__all__ = (\n    'parse',\t# parse\n    'tzinfo',\t# tzinfo\n    'format_date',\t# format_date\n    '_parse'\t# parse\n)\n\n"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\ttry:\n\t\ttzinfo\n\t\tformat = timestr.format()\n\t\tif format == \"\":\n\t\t\traise TypeError\n\t\tformat = format.lower()\n\t\tif not format in (\"%b %%b, %A%%b, LT\", \"%B, BE, %A, LT\"):\n\t\t\traise ValueError;\n\t\tformat = format.lower().replace(\"%.f\", \"%X\")\n\t\tif format not in (\"%b %b\", \"%b %B\", \"%b %B %p\"):\n\t\t\traise TypeError;\n\t\tformat = format.replace(\"%"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tdate = None\n\ttzname = None\n\ttzoffset = None\n\tif _stk and timestr in _stk:\n\t\tdate = _stk.pop()\n\telif _tokens:\n\t\tdate, tzname, tzoffset = _parse_tokens(timestr, timestr.encode('utf-8'), _tokens, _date_parser)\n\telse:\n\t\t#\n\t\t# XXX if the string begins with \"~<offset>\", ignore tzinfo\n\t\t# when parsing strings containing only offset"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tdatestr = timestr\n\tif datestr.count(' ', 0, 1) > 1:\n\t\traise ParserError(\"Invalid string formatting arguments. Use the format(s) and time(s) keyword arguments when input a string.\")\t\n\t# Check if we are parsing a string that has time or offset fields\n\tif (timestr is None and default is None and ignoretz is False):\n\t\traise OverflowError(\"\"\"Invalid string formatting arguments. Use the format(s) and time(s) keyword arguments when input a string.\"\"\")\t\n\t# Check if"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not isinstance(dt, datetime.datetime):\n\t\traise TypeError(\"fromutc() can only be used with datetimes\")\n\t\t#raise ValueError(\"fromutc() can only be used with datetimes\")\n\n\tttime = dt.replace(tzinfo=timezone.utc())\n\tif ttime.tzinfo is not None:\n\t\ttslst = sorted(timezone.localtime(ttime))\n\t\ttslst.reverse()\n\t\ttzoffset = tslst[-1]\n\telse:\n\t\ttslst ="}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn iso_time(dt)\t# return naive datetime\n\n###############################################################\n### Utilities with datetime objects\nclass iso_time(object):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tdef __init__(self, dt):\n\t\t\tself.dt = dt\n\t\t\tself.timezone = 'UTC'\t"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tfrom datetime import datetime\n\tdt = datetime.combine(dt.timetz(), dt.timetuple())\n\treturn(dt.astimezone(timezone(str(int(dt.tzinfo)))))\n"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn self._dtimefmt.fromutc(dt)\n\nclass dateutil.relativedelta(object):\n\t\"\"\"\n\tA class which represents a relative date/timestamp\n\t(i.e. a date or a timestamp in the past), and gives a\n\tfunction which takes ``utcnow()`` and ``relativedelta`` as arguments, and\n\treturns the datetime representing that same time.\n\t\n\tThe difference between this class and relativedelta is that\n\t:class:`datetime.timedelta` can be used, which"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif isinstance(dt, pytz.AmbiguousTimeError):\n\t\t_, (d, t) = dt.datetime, dt.time\t\t\t\t\t\t# try and guess next time\n\t\ttimedelta = timedelta(seconds=-1)\t\t\t\t# add the next second of uncertainty\n\t\traise _ambiguousToTimedelta(d, t, timedelta)\t\t# return if next time is ambiguous\n\tassert isinstance(dt, datetime)\n\treturn datetime.combine(dt.date(), dt.time())\t\t# ensure we got a datetime object!\n"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t# if ambiguous, start from the day past.\n\tfirst = dt.time().day\n\treturn self.fromutcfinal(dt).date().set_trunc(first)\n\n\t\n\t# get the day after it's day and day of month.\n\tlast = dt.time().day   - 1\n\tdow = dt.time()  .day\n\t\t\n\treturn self.fromutcfinal(dt, last, dow)\t\n\n\ndef isafter(self, other):\n\t\"\"\"\n\t\tIf the other date is after this date"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif self.timezone == dt.tzinfo.utcoffset(dt):\n\t\treturn dt\n\n\t# The following code handles ambiguous times that occur\n\t# (i.e. two consecutive datetimes in different timezones) with a\n\t# second occurrence of datetime 'dt', so we keep track of which\n\t# occurrence of this ambiguous datetime we think it's occurring\n\t# (but don't change it yet), and whether we are \"folding\".\n\t#\n\t## First, note that we have to use 'time' as a method of this object"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\ttry:\n\t\treturn datetime.combine(dt, self.value())\n\texcept AttributeError:\n\t\tif self.fold(dt):\n\t\t\tself.value()\n\t\telse:\n\t\t\traise self.failure(\"Expected a datetime within a timezone, but timedelta has type 'datetime'\").get_failure(&datetime.datetime)\n\texcept ValueError:\n\t\traise self.failure(\"Expected a datetime within a timezone, but date/time object has type 'date'\").get_failure(&datetime.datetime)"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tdt = self.toiso(dt)\n\t\n\t# Convert microseconds so we can calculate a date in a timezone aware\n\t# date. \n\tminutes = abs(dt.microsecond // 100)\n\tif minutes > 0:\n\t\tdt = dt.replace(microsecond=dt.microsecond-minutes*100)\t\t\n\n\t# We need to calculate a date in the appropriate timezone in order to\n\t# preserve date/time fields.\n\ttz, tzh = utc_tz.look(dt)\n\tdt = tz."}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t## make this case insensitive\n\treturn datetime(*dt.timetuple()[:3]) \t## timestamp is stored as (year,month,day)\n\n\ndef fromutc_fast(self, dts):\t\"\"\"\t\n\tGiven a timezone-aware sequence of datetime objects, and a list\n\tof times, does absolutely nothing.\n\n:param dts:\n    A timezone-aware sequence of datetime.timetuple objects.\t\"\"\"\n\t## make this case insensitive\n\tdts = [dts[t].timetuple() for d"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif (dt.tzinfo is not None) and (dt.tzinfo.utcoffset(dt) is not None):\n\t\treturn tzinfo()+dt.utcoffset(dt).total_seconds()\n\telse:\n\t\treturn tzinfo(dt)\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif isinstance(dt, datetime.datetime):\n\t    dt = dt.replace(tzinfo=tzinfo)\n\telse:\n\t    try:\n\t\tdt = tzinfo.tzisaware(dt)\n\t    except AttributeError:\n\t\tpass\n\tif dt is not None:\n\t    return datetime.combine(dt, tzinfo.utcoffset(dt))\n\telse:\n\t    return dt\n\n# =============================================================================\ndef timedelta(value):\n    \"\"\"\n    Returns the timedelta object indicated by"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif isinstance(dt, type(tspan)):\n\t    return dflt_tz.localize(dt, is_dst=False).astimezone(tzinfo)\n\telse: return ts, tzinfo\t\t# TODO: do not rely on datetime.tzinfo and return ts\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\n\tlocal = tzinfo is dflt_tz\n\tif local:\n\t\ttzrule = tzoffset(dt.tzname)\n\t\tif local == True: # local datetimes are always in standard time zone\n\t\t\treturn local_to_naive(dt).replace(hour=12)\n\n\treturn dt.replace(tzinfo=tzrule)\n\ndef local_to_naive(dt):\n\t\"\"\"\n\tConverts a naive input datetime for a specific standard time zone\n\tto"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n\treturn datetime.fromtimestamp(int(dt.tm_hour*24.0+dt.tm_min*60.0+dt.tm_sec)).replace(tzinfo=tzinfo)\n\t\n\t\n\n# --------------------------------- #\n\n__all__ = ['default', 'naive']\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif not callable(dt):\n\t\tdt = datetime.utcfromtimetuple(dt.timetuple())\n\tif dt.tzinfo is None:\n\t\tdt = default_tzinfo(dt)\n\treturn dt\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\treturn dt.replace(tzinfo=tzinfo)\n\ndef localize(dt):\n\t'''\n\tDetermine the local timezone based off the date's time\n\n\t:see: http://stackoverflow.com/a/4449097/150188\n\t:note\n\t\tIn this driver, the result will be\n\t\t(:py:class:`~dateutil.tz.tzlocal`)\n\t\tThe time is in localtime.\n\t\tWe want to use the timezone inferred from the date's time if no\n\t\ttimezone is specified;"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif not isinstance(dt, datetime.datetime):\n            dt = parse(dt)\n\treturn dt.replace(tzinfo=default_tzinfo(dt, tzinfo))\n\n\n\ndef str2datetime(s):\n    \"\"\"\n\tConvert a string representation of data in UTC into a datetime\n\n\t.. doctest::\n\n        >>> print data_convertor, str2datetime(\"2014-06-15 12:30:00-05:00\")\n        2014-06-15 12:30:00+00:00\n        2014-"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\treturn dt.replace(tzinfo=tzinfo)"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif tzinfo is None:\n\t\treturn dt\n\ttzp = _get_timezone_info(dt=dt, tzinfo=tzinfo, default=False)\n\tif tzp is None:\n\t\traise ValueError(\"No timezone information was found for: {}\".format(dt.format(tzinfo.tzid)))\n\treturn tzp.aware(dt)\n"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.cut_chars = before + before.replace(b' ', b'') + after\n\n\nclass FileInfo(io.FileIO):\n\tdef is_open(self) -> bool:\t\treturn True\n\tdef __enter__(self) -> 'FileInfo':\n\t\treturn self\n\n\tdef __exit__(self, type, value, traceback) -> None:\n\t\tif type == io.ERROR:\t\traise value\n\t\tfile.close(self)\n\t\tdel self.__closed\n\ndef main():\n\t# Check command line arguments\n\timport argparse"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n\tfor i in range(3, len(before)):\n\t\n\t\tif i == 0:\n\t\t\tbefore.remove(before[-3:])\n\t\t\tbefore.insert(0, before[0])\n\t\telse:\t\t\n\t\t\t\n\t\t\tbefore.remove(before[-2:])\n\t\t\tbefore.insert(0, before[0])\n\t\t\tbefore.remove(before[-1:])\t\n\t\t\t\n\t\tbefore.insert(0, after[i-1:])\n\t\t\t\n\tfor i in range(3, len(after) ):\n\t\n\t\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.CuttingChars = [ before ] + [ after ]\n\tself.delim = bytes(before) + after\n\t\n\n\t\n\n\ndef set_fractional(fract: int) -> None:\n\t\"\"\"\n\tSet the offset of the start delimiter.\n    \n\tArgs:\n    \tfract: The fractional offset.\t\"\"\"\n\t#fract = (2 * (fract/1000.0)) / 25.4  # 0.15\n        \n\t#self.SetDelimiterFraction = list(fract)\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n\tself.cut_chars[before:after].set(str(self.file))\n\n\n\n\treturn self.__class__\t\n\n\n\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tpass\n\ndef trim(input: str) -> str: self.delim = None\ndef trim_space(input: str) -> str:\tself.delim = None\n\ndef check_cut_chars(input: str) -> None:\n\t\"\"\"Check if a string can be split by cut chars.\"\"\"\n\tpass\n\nif __name__ == '__main__':\n\tmain()\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.cut_start = before\n\tself.cut_end = after\n\n\n\t\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n\n\tself.f = open(self.before, 'rb')\n\tself.len = os.path.getsize(after)\n\tself.f.seek(0)\n\n\tself.f2 = open(self.after, 'rb')\n\tself.len2 = os.path.getsize(before)\n\n\tself.start = self.f.tell()\n\tself.end = self.f2.tell()\n\t\n\tself.f.seek(0,0)\n\tself."}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t# TODO : implement\n\t#self.set_cut_chars(before, after)\n\n\n\t\ndef __main__():\n\t# create a new test case\n\tfrom pypro import model\n\t\n\t# read in and run the application\n\timport sys\n\timport random\n\targv = sys.argv\n\tparser = argv[0]\n\tmodel.setup_testing(parser, '1f10')\n\tapp = argv[1]\n\tsys.exit(app(parser))\n\nif __name__ == '__main__':"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.cut_chars = bytearray([before, after])\t"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif isinstance(request, MessageType):\n\t\tif request.pub_id in Diaspora.mock_message_dict.keys() and \\\n\t\t\trequest.pub_date >= Diaspora.last_diaspora_message_pub_date and \\\n\t\t\trequest.pub_date <= Diaspora.last_diaspora_message_pub_date:\n\t\t\treturn True, Diaspora.mock_message_dict[request.pub_id]\n\n\ttry:\n\t\treturn Diaspora.identify_request(request.to_message())\n\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif hasattr(request, 'payload') and type(request.payload) == int:\n\t\tif request.payload == '1':\n\t\t\treturn True\n\t\tif request.payload == '0':\n\t\t\treturn False\n\t\treturn False\n\treturn False\n\n#\n#\n#\n#\tdef unify_message(request: 'MessageTypeType') -> MessageType:\n#\t\tif isinstance(request, 'MessageTypeType') and isinstance(request.content, MessageTypes.Message):\n#\t\t\tmessage: type(request.content.content)\n#"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\ttry:\n\t\tif request.from_address == \"anonymous\":\n\t\t\t# If the message is a request (legacy) and from address is anonymous\n\t\t\treturn True\n\t\tif request.from_address == \"\" or request.from_address is None:\n\t\t\t# Anonymous message\n\t\t\t_logger.warn(\"Invalid anonymous message from address (received: '{}')\".format(request.from_address))\n\t\t\treturn False\n\n\t\treturn True # Private message\n\texcept KeyError:\n\t\treturn False # Private message, not a valid request\n"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif not isinstance(request, RequestType):\n\t\traise TypeError(\"Parameter request is not a type of RequestType\")\n\n\tif request.payload.starts_with(\"<\") and request.payload.ends_with(\">\"):\n\t\treturn False\n\n\treturn True\n\n"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tfrom diaspaces.message import DiasporaMessage\n\tfrom diaspaces.message import DiasporaPublishRequest\n\tif isinstance(request, DiasporaMessage):\n\t\treturn True\n\telif isinstance(request, DiasporaPublishRequest):\n\t\treturn True\n\tmsg = str(request).replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(u\"\\uFF1B1\", u\"<br>\")\n\treturn msg!=\"<font color=\"red\">\" + u\"Please, make sure that you"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\ttry:\n\t\tpublic_message = request.publicMessage\n\texcept AttributeError:\n\t\traw_input(\"Public message not set in request.\\nClick on for another message...\")\n\t\tpublic_message = raw_input(\"Are you able to confirm access?\")\n\t\n\tif public_message == True:\n\t\t#If public message is set, check if message is private\n\n\t\t#User should be able send private message.\n\t\t\n\t\tprivate_message = False\n\t\tif request.privateMessage:\n\t\t\t#User should be able send private message.\n\t\t\t\n\t\tprivate"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request.is_public():\n\t\ttry:\n\t\t\tresponse = request.call(message_to_pub_key(request))\n\t\texcept KeyError:\n\t\t\traise InvalidInputError(\"Unexpected response of message to pubkey\")\n\t\ttry:\n\t\t\tif isinstance(response, dict):\n\t\t\t\treturn True\n\t\texcept AttributeError:\n\t\t\tpass\n\t\treturn False\n\telif request.is_private():\n\t\ttry:\n\t\t\tmessage = request.call(message_to_pub_key(request))\n\t\t\tif isinstance(message, dict):\n\t\t\t\treturn"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\n\t#print(\"Identifying request for:\", request.path())\n\t\n\tis_diaspora = False\n\tif \"diaspora\" in request.path():\n\t\tis_diaspora = True\n\t\tstatus_code = 200\n\telse:\n\t\tstatus_code = 404\n\t\n\treturn is_diaspora\t\n\t\ndef check_request(response: HttpResponse):\t\"\"\"\n\tCheck if the response is the origin of a request.\n\n\tIf the response redirects, return False to indicate that the request is going\n\tthrough Dias"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\n\ttry:\n\t\n\t\tif \"public_to_private\" in request.args[u\"payload\"]: # Public to Private\t\n\t\t\tif \"public_msgs_by_id\" in request.args[u\"payload\"]: # Public to Private (2) message\t\n\t\t\t\tif\trequest.kwargs[u\"id\"] in request.args[u\"payload\"]: # Public to Private (1) message\t\n\t\t\t\t\tpublish_request = {\"payload\": request.args[u\"payload\"], \"id\": request.kwargs[u"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif any([request.payload.get('message_id')=='0', request.payload.get('from_username')=='diaspora']) + any([(isinstance(r, DiasporaMessageType) or r.get('from_status') is 6) for r in request.payload.get('requests', [])]):\n\t\tpayload = request.payload\n\t\tmessage_by_from = message_by_from_username(payload['from_username'])\n\t\tstatus = -1\n\t\trequested_thread = None\n"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn bool(request.method == \"matrix\")\n\n\ndef parse_result(res: str) -> ResultType:\n\t\"\"\"\n\tParse the response body, look for a matrix and return a Result object \n\t\"\"\"\n\treturn ResultType(str_to_float(res), res)\n\n\ndef str_to_float(s: str) -> float:\n\t\"\"\"\n\tConvert a string into a number, return NaN if something fails\n\t\"\"\"\n\treturn float(s) if is_string(s, \":\", 1, 3, 3) else"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tmatrix_id = request.get('matrixId', None)\n\tif matrix_id is None:\n\t\treturn False\n\telse:\n\t\treturn True\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tif request.method == \"PATCH\":\n\t\treturn False\n\telif request.method == \"POST\":\n\t\treturn False\n\telif request.method == \"PIPE\" or request.method == \"POST\":\n\t\treturn False\n\telif request.method == \"POWERSET\" or request.method == \"POST\":\n\t\treturn False\n\telif request.method == \"OPTIONS\":\n\t\treturn False\n\n\treturn True\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn (request.params.get(\"resource\") == \"matrix\")\n\n\ndef parse_querystring(querystring: str, url: str = None):\n\t\"\"\"\n\tParse a querystring back into an object\n\n\tParameters\n\t----------\n\tquerystring : string\n\t\tthe querystring as a string\n\turl : string\n\t\toptional alternative url\n\t\"\"\"\n\tquery = Query.parse(querystring, url)\n\treturn query\n\n\nclass RequestValidator(Validator):\n\n\t\"\"\"\n\tThe Validator class must override the method \n\t\n\t\"\"\"\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn request.kind in \"Matrices\"\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\ttry:\n\t\tif isinstance(request, basestring):\n\t\t\tif'matrix_url' not in request:\n\t\t\t\timport uxl.utils\n\t\t\t\t# import uxl.client.Client\n\t\t\t\t# request = uxl.client.Client(url)\n\t\t\t\ttry:\n\t\t\t\t\trequest = urlfetch._requests_from_response(request)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\turl = urlfetch._base_uri + request[4]\n\t\t\tm = MATRIX_URL.match(url)\n\t\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn request.method == \"POST\"\n\n\t\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tpass\n\ndef classify_request(request: RequestType) -> list:\n\t\"\"\"\n\tClassify a request a give it its version and type\n\t\"\"\"\n\treturn [ \n\t\t#\tRequest Version\t\n\t\t(\"2\",'req_req3', None, \"Matrix request\", 0),\n\t\t#\tRequest Type\t\t\n\t\t(\"1\",'req_int', None, \"Integrity check\", 1),\n\t\t(\"req_int\",'req_int', None, \"Integrity check (without type)\", 1),\n\t\t(\"2\",'req_int', None,"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tif request.method == \"GET\":\n\t\tif request.url.endswith(\"/courses/list\") :\n\t\t\treturn True\t\t\t\n\treturn False\n\t\t\t\ndef do_request(client: Client, url: str) -> RequestType:\n\t\"\"\"\n\tDo any necessary processing\t\n\t\"\"\"\n\tresponse = None\t\n\ttry:\t\n\t\tresponse = client.get(url)\n\texcept ClientResponseError as e:\n\t\tprint(\"Error while requesting {} : {}\".format(url, e.request['message']))\n\t\traise e\n\texcept Client"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn request.url.startswith('http://matrix.voting') or request.url.startswith('http://matrix.votes') == True\n\n\ndef create_matrix_client(app: ApplicationType, params: ParamatersType, params_obj: Optional[ApiParamType] = None, **kwargs_obj: Any) -> ClientType:\t\"\"\"\n\tCreates a new Matrix client object for usage as API client\n\t:param app: Matrices Application\n\t:param params: Parameters container\n\t:param api_params: API parameters for Mat"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tdt = dt.strftime(\"%Y-%m-%d_%H:%M:%S\")\n\treturn datetime.strftime(dt, dT)\n\n\ndef get_dt_from_hex(d, s):\n\t\"\"\"\tReturn the datetime in the format parsed by parseDt.\"\"\"\n\t\n\tdt = d \n\tdt = dt.strftime(\"%Y-%m-%d_%H:%M:%S%z\")\n    \ts += \"+00:00\"\n\tdt = dt."}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tm,s,y,z = dt.date(), dt.microseconds / 1000, dt.year, dt.month\n\treturn '%02i-%02i-%04iT%02i:%02i:%02i.%03iZ' % (\n\t\tm, n, y, z, yordinal_to_daylight(y), ctime((s,y,z)), dt.microsecond / 1000)\n\n\n\ndef nordb_parse_dt(d):\t\"\"\"\n\tParse a datetime in the"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\trv = \"\"\n\ttry:\n\t\trv = dt.dayofweek\n  \texcept:\n\t\treturn \"\"\n  \tif not \"-\" in rv:\n \t\trv = \"+'+rv\n\t\t\n\treturn rv\n\t\ndef format_d(dt):\n\t\"\"\"\n\tFormat a date.\t\t\t\n\t\"\"\"\n\trv = \"\"\n  \ttry:\n\t  \trv = dt.month + dt.year\t\t\n\t  \tif not \"-\" in rv:\n\t\t\trv = dt."}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif dt < 0:\treturn '-%s' % dt\n\treturn dt\n\nclass BaseNode(object):\n\t\"\"\"\n\tBase of all nodes in the BPD tree.\t\n\t\"\"\"\n\n\tdef __init__(self, dt=None):\n\t\tself.dt = dt\n\t\tself.parent = []\n\t\tself._depth = 0\n\t\tself.children_dict = {}\n\n\t@property\n\tdef name(self):\n\t\t\"\"\"\n\t\tThe name of the node, which is used in the BPD visual output.\t\n\t\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tdt_value = dt.weekday()\n\tdt_mon_str = (\"Mon\" if dt.month == 1 else \"june\" if dt.month < 9 else dt_mon_str if dt.month > 0 else \"Tue\" if dt.month < 10 else dt_mon_str) + (\" \" if dt.first_day_of_year == 53 else \"\"\n\tdst = dt_mon_str + (\" \" if dt.second < 10 else \" \")\n\tif dt.month!= 1: # TOD"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\timport datetime\n\n\tdt = datetime.datetime(*dt.timetuple())\n\treturn dt.strftime(self.format_fmt)\n\nclass D*Node(object):\n\t\"\"\"This class defines the actual implementation of a D* node.  This class does not\n\toverride any of the node interface functions.  The only thing that this class\n\tshould be derived from does not define any node attributes.  It just defines the\n\tmethods used by the D object.  When created, the initializer method is called on the\n\tparent of the nodes"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif type(dt) not in ('datetime', 'time', 'datetime.time', 'datetime.date','str'):\n\t\tif type(dt) == str:\n\t\t\tdt = datetime(dt.split('x'), dt.split('x')[0])\n\t\telse: raise ValueError(\"Datetime expects string or datetime.strftime() format.\")\n\tif type(dt) not in ('datetime', 'time', 'datetime.time','str'):\n\t\timport calendar\n\t\tif type(dt) == int:\n\t\t\tdt ="}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.strftime(\"%s\")\n\n"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tlst = []\n\tif dt == None:\n\t\treturn None\n\tfrom datetime import date\n\tfrom datetime import time\n\tlst.append(str(dt.day))\n\tlst.append(str(dt.year))\n\tlst.append(str(dt.month))\n\tlst.append(str(dt.month))\n\tlst.append(str(dt.hour))\n\tlst.append(str(dt.minute))\n\treturn ''.join(map(str, lst))\n\n\ndef add_dt"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tdt_format = \"%Y%m%d%H%M%S\"\t\n\treturn \"\".join([\"%02d\" % dt.day, \"%02d\" % dt.month, \"%02d\" % dt.year, dt_format % dt])\n\t\ndef get_dt_list(root_dir):\n\t\"\"\"\n\tGet a list of all dt present in a tree, and return its root node\n\t\"\"\"\n\tdt_list = []\n\tfor key in os.listdir(root_dir) :\n\t\tdir"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttags = set()\t\t# all tags found in current content\n\tto_del = set()\t\t# tags removed from current content\n\tfor token in todo(text):\n\t\tif not replacer is None:\n\t\t\ttag, result = replacer(token, '')\n\t\telse:\n\t\t\ttag, result = str(tag), str(result)\n\t\tif not is_tag(tag) and not is_tag(result):\n\t\t\tif is_tag(result) and is_tag(tag):\n\t\t\t\ttags.add(tag)\n\t\t\t\tif is_blank(result):"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif replacer is None:\n\t\treturn parse_tags(text, default=None) # parses normal words\n\telse:\n\t\treturn parse_tags(text, replacer=replacer)\t\t# parses words with special tag words\n\ndef find_tags(text: str, replace: dict = None, default=None) -> list:\n\t\"\"\"\n\tFind all the tags in the text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replace\", will also replace the tag with the result\nof the replace function called with the tag"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t# Set up search results for the string to be translated\n\ts = re.compile(ur'<(?:[^<]+|<(?:)?)?>', re.MULTILINE)\n\t# Regex search for tags\n\ttags = re.compile('<(?P<tag>.*?)>(?P<repl>.*?)(?<!<)', RE_TAGS)\n\n\t# Find the tags to be added to the text (without duplicates)\n#\ttags = re.findall(ur'(?:[^<]+|<"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\n\tfor tagname in tagmap:\n\t\tif tagname in text:\n\t\t\ttags=tagmap[tagname]\n\t\t\tnewtext=replacer('<b>%s</b>' % (tagname)).replace(tagname, text).strip()\n\t\t\treturn set(tags),newtext\n\treturn set(),text\n\n@hf.check_config('-pw') # Only consider words found in the text.\ndef find_words(f):\n\treturn (nltk.caseless_tokenize(\"\" if (getvalue(f)==''"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\treplaced = []\n\tnonreplaced = {}\n\ttags = re.match(r'(?i)\\W+\\{(.*?)\\Z', text) or []\n\tif replacer:\n\t\tfor tag in tags:\n\t\t\trepl = replacer\n\n\t\t\tdef extract_key(match):\n\t\t\t\treturn match.group(2)\n\t\t\treplaced.append((extract_key, extract_key(tag)))\n\n\t\t\tdef extract_value(match):\n\t\t\t\treturn match.group(1)\n\n\t\t\ttag = match.group(1)\n\t\t\ttext = re."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif replacer is None:\n\t\treturn find_tags(text, tokenize(text))[0]\n\telse:\n\t\treturn find_tags(text, replacer=replacer)\n\ndef find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to return tag words.\n\nOptionally, if passed a \"replacer\", will also replace the tag words with the result\nof the replacer function called with the tag words.\n\nReturns a set of tags"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t# Try a direct replacement first.\n\tif replacer is None:\n\t\treturn re.findall(text,text)\n\t# Now try a modified version using our custom REPLACE_MAP.\n\tresult, text = re.sub(replacer,text,text), text\n\t# Use the modified set with any \"REPLACE_MAP\".\n\treturn find_tags(text, REPLACE_MAP)\n\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif replacer is None:\n\t\treplacer = ''.join(i.strip('{}') for i in text.split())\n\tif replacer is not None and not isinstance(replacer, unicode):\n\t\treplacer = replacer.decode('iso-8859-1')\n\ttags = {i.lower(): i[:i.index(max(0, len(i)-len(i.strip('{}'))-1)])\n\t\tfor i in text.splitlines()}\n\tfor tag, text in tags.items():\n\t\tfor"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttags : Set = set()\n\ttext : str\n\treturn  set(re.findall(regexp, text))\n\nclass Plugin:\n\t\n\tdef __init__(self, name: str, directory: str, text: str = ''):\n\t\tself.directory = directory\n\t\tself.name = name\n\t\tself.filename = f'{self.name}_' + str(time.time()).split('.')[0]\n\t\tself.text = text\n\t\tself.config = Config()\n\t\tself.current_tag = set()\n\t\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\n\toutput = set()\n\tpunct = re.compile(r'^([^()]*)(?=[\\w,])')\n\t\n\tfor i, line in enumerate(text.split('\\n')):\n\t\tif i % 2 == 1:\n\t\t\ttext = line.replace(\".\", \"&#8203;\").replace(\"(\", \"[^()]\")\n\t\telse:\n\t\t\ttext = line.replace(\".\", \"&#8203;\").replace(\"(\", \"[^()]\") \\\n\t\t\t\t\t+ punct.sub(\" \", replacer.__"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\timport re\n\tfor link in re.findall(r'\\<a\\s*=\\s*(?:(@)\\s*=(.*?)\\)(&?)(?:http)(?:@)(?:\\w)\\d+\\)', text):\n\t\ttext = re.sub(r'(?<=\\s>\\s *<a[^>]*(?:>(.*?)\\1(?:&?(?:http)?)':'', text, match_group(link)[2]) + '\\n', text)\n\t\tif re.search(r'&lt;"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext = text.replace('\\r\\n','')\n\n\t# Process links\n\tif \"|\" in text:\n\t\tlines = text.split(\"|\")\n\t\tlines = lines[:]\n\t\ttags = re.findall(\"[a-z\\-]+::?(?:link\\)\", text)\n\t\t# Use the first in_link tag\n\t\ttags = tags[0].split()\n\t\tlines = \"| \".join(lines)\n\n\t\ttext.replace(\"| \", \" \")\n\t\tfor i, tag in enumerate(tags):\n\t\t\tinlink ="}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tprocessed_text=[]\n\n\tfor l in text.split(':'):\n\t\ttry:\n\t\t\tlink=linkify_url_and_target(l)\n\t\t\tif link not in processed_text:\n\t\t\t\tprocessed_text.append( link )\n\t\texcept (ValueError,TypeError):\n\t\t\tpass # TODO: handle some kind of message from the regex\n\treturn [ process_text_link(l,link) for (l,link) in processed_text]\n"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\n\ttext = text.replace(\"!\", \"&amp;\")\t\n\ttext = text.replace(\"&\", \"&amp;\")\t\n\ttext = text.replace(\"'\", \"&apos;\")\t\n\ttext = text.replace(\"\\\"\", \"&quot;\")\t\n\ttext = text.replace(\"/\", \"&#47;\")\t\n\ttext = text.strip()\n\ttext = text.strip(\"<![CDATA[\")\n\t\n\treturn text\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext1 = text.__rpartition('</b>')[0].encode('utf-8')\n\ttext1 = text1.replace('&lt;', '<')\n\ttext1 = text1.replace('&gt;', '>')\n\ttext1 = text1.replace('&shy;', '-')\n\ttext1 = text1.replace('<br /><br />', '<br />').replace('<br/>', '</br>')\t\n\t\n\ttext2 = text.replace('<hr /><br /><"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tlinks = []\n\tdef process_link(link):\n\t\tif ('rel' in link or 'type' in link or 'href' in link):\t\t# if the link is some other attribute or an actual link\n\t\t\t\tif ('rel' in link or 'type' in link):\n\t\t\t\t\tlink = link['rel'] \t\t\t\t\t\t\t\t\t\n\t\t\t\tif ('href' in link):\n\t\t\t\t\tlink = link['href'] \n\tfor link in text.get('links','link'):\t# find all links\n\t\tprocess_link(link)\n\t\n\tlinks = links + list(text"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\toutput = \"http://www.siggler.net/\"\n\tfor link in text.split(\";\"):\n\t\tlink = re.sub(r\"^(.*)(;\\s)(.*)\", r\"\\1\", link).strip(\" \");\n\t\tlink = re.sub(r\"\\](.*)=(.*) [\\n\\s]\", r\"\\1<a  href=\\\"http://siggler.net/\"+link, link)\n\t\tlink = re.sub(r\"<\\/a\\[href],\\s\", \"\\n\", link, flags=re"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tl = [(\"\", False)]\n\treversedL = list(text.symbols[:])\t# strip the header words\n\tif not reversedL:\n\t\treversedL.append(text.text)\n\t\tl.append((\"_\", False))\n\tfor line in reversedL:\n\t\tnewL = str(line)\n\t\tif len(newL) == 3:\n\t\t\t# newL is a triple (sym, ln_len, ln_len)\n\t\t\tl.append(newL)\n\t\t\tl.append((text.sy"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext = re.sub(r'[\\-a-zA-Z0-9\\:\\/\\\\\\.\\*\\?\"<>\\(\\)\\?\\.,\\^\\{\\}\\@\\|]', ',', text)\n\ttext = re.sub(r'^\\s*<\\//\\s*$', '\\n', text) # don't include <html> lines\n\ttext = re.sub(r'\\:\\d+?\\s([^]*)', r'\\1<a href=\"\\2\" title=\"\\2\"><>', text)"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\timport urllib\n\n\tlink = text.find(link_tag)\n\twhile link < 0:\n\t\tlink = text.find(link_tag, link + 1)\n\n\tif link is not None:\n\t\tattrs\t= {'href':attrs_html_attributes,'rel':rel_html_attributes}\n\t\ttext = extract_link(text, attrs)\n\n\tif 'javascript:' in text:\n\t\treturn text\n\n\treturn extract_text_links(text)\n\n\t\ndef extract_text_links(text):\n\t''"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tfull_url = \"https:{}/{}/tree?recursive=1\".format(\n\t\turl.split(\"|\")[7], url.split(\"|\")[3])\n\tcontent_type, resp = fetch(full_url)\n\treturn content_type\n\ndef fetch(url: str) -> Tuple[str, List[str]]:\n\t\"\"\"\n\tFetch the specified URL and parse the response.\n\t\"\"\"\n\ttry:\n\t\treturn fetch_content_type(url)\n\texcept requests.exceptions.HTTPError:\n\t\treturn"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\treturn self.getURL(url, 'HEAD').text.split('/')[2].strip().lower()\n\n\t\ndef fetch_page(url: str, page: int=0) -> bool:\n\t\"\"\"\n\tTry to fetch a web page, return True if successful.\n\tReturn False if the request times out.  \n\n\tTo get the page type of the remote url, use the page attribute.\n    \"\"\"\n\tif os.path.exists('/tmp/%d/%d/%d' % (url, page, len(url.split('/"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tif url is None:\n\t\traise Exception(\"URL should not be None.\")\n\t\n\treturn RemoteContentType.to_str(get_response.getheader('content-type', None))\n\t\n#==============================================================================\n#==============================================================================\n#==============================================================================\n\ndef download(url: Anystr, filename: str, target_output_path: Optional[str], filename_extension: Optional[str]) -> Tuple[Optional[str], Optional[str]]:\n\t\"\"\"\n\tImplements the _download function for fetch_content_type.\n\tSee fetch"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tfetch_url = url\n\n\ttry:\n\t\tresp = requests.head(fetch_url)\n\t\tprint(f'{fetch_url} --> resp : {resp.status_code}')\n\t\tsoup = bs4.BeautifulSoup(resp.content, \"html.parser\")\n\t\treturns = soup('meta')\n\n\t\tfor content in returns:\n\t\t\tif content['name'] == 'content':\n\t\t\t\treturn content.get('content')\n\t\treturn None\n\texcept requests.exceptions.ConnectionError:\n\t\treturn \"no content of this"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\timport urlparse, cookielib\n\timport cgi\n\treturn urlparse.urlparse(url)[1].split('/')[0].split('./')[0]\n\n\ndef get(method, url: str, **kwargs):\n\t\"\"\"\n\tGiven an URL, make an httplib opener that adds `COOKIE` header on subsequent requests.\n\t\"\"\"\n\topener = urllib2.build_opener(URLErrorHandler(), urllib2.HTTPCookieProcessor(cookielib.CookieJar()))\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\treturn FetchContentType(url)\n\n\n\t\n\t\nclass _Request(BaseRequest):\n    \n    def __init__(self, url, query: str, content_type: Optional[str]=None, params: Optional[dict] = None, data: Any = None, headers: Optional[Dict[str, str]]=None, **kw):\n        \n        BaseRequest.__init__(self, content_type or self.content_type, params)\n        \n        \n    def _fetch(self, content_type=None, params=None,"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\ttry:\n\t\tresponse = session.get(url)\n\texcept urllib2.URLError as exc:\n\t\t# if we get an error, the request will return an error response code,\n\t\t# and you have access to its body.  We want to distinguish between\n\t\t# a successful response, and an error response code.\n\t\t#\n\t\t# To be robust, we rely on the fact that the response is never\n\t\t# something the remote server will return.  For example, if you POST\n\t\t# an invalid url, the expected response code will be 200.  "}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\treturn fetch(url).json()[\"url\"]\n\n\ndef get_file_url(local_url: str) -> Dict[str, Any]:\t\"\"\"\n\n\tFetch the content of the url provided as the second argument.\t\"\"\"\n\t# https://docs.python.org/3/library/urllib.html#urllib.urlparse.parse_qsl\n\tq = urllib.parse.parse_qsl(urllib.parse.urlparse(local_url).query)\n\n\tcontent_type = None\n\tfilename = None\n\n\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\treturn remote_url.fetch(\"head\", url).content\n#\tprint(\"Downloaded \\\"{}\\\" to {}\".format(url, path))\n\ndef download_url(remote_url: \"URL\",\n\t\t\t\t local_url: str = None) -> Optional[str]:\n\t\"\"\"Fetch the remote url to download it into a temporary file.\t\"\"\"\n\tpath = tempfile.mkdtemp()\n\t\"\"\"\n\tAdd the content type of the downloaded file to the list in `path`\n\tThis will help us make sure the content type is unique.\n\t\"\"\""}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\turl = url.split('/')\n\turl = url[-2]\n\tcontent_type = None\n\tif (url.startswith('http://')) or (url.startswith('https://')):\n\t\ttry:\n\t\t\turl = urlparse(url).path\n\t\texcept:\n\t\t\tpass\t\n\t\tcontent_type = urlparse(url).get('content-type', '')\n\treturn content_type or content_type\n"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\ttag_str = tag.text\n\ttag_word = re.search(r\"[A-Za-z- ]+\", tag_str)\n\tif tag_word.group(0).lower() not in [\"a\", \"z\", \"e\", \"l\", \"t\", \"y\", \"r\", \"i\"]:\n\t\treturn False\n\treturn True\n\ndef test() -> None:\n\t\"\"\"\n\tTests the word recognition rules.\n\t\"\"\"\n\tword_recognition = WordRecognition()\n\tif not word_recognition.test_words"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tfor word in ('-g', 'G', 'f', 'F', 'p', 'pB', 'a', 'A'):\n\t\tif isinstance(tag, word):\n\t\t\treturn True\n\treturn False\n\n\ndef test_tags() -> dict:\n\t\"\"\"\n\tTest whether tags are accepted.\n\t\"\"\"\n\treturn {\n\t\t\t'+a':True,\n\t\t\t'+s':True\n\t}\n\n\ndef main() -> int:\n\t\"\"\"\n\tDefine the program's main entry point, this runs the test suite and then prints an exit value to"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\ttry:\t\t\n\t\ttag_string = tag + \".\"\n\texcept RuntimeError:\n\t\traise unittest.SkipTest\n\tif \".\" in tag_string:\n\t\tresult = tag_string.count(\".\")\n\t\tif result<1:\n\t\t\traise unittest.SkipTest\n\t\treturn result\n\n\treturn True\n\ndef test() -> None:\n\tos.system(sys.executable+\"./tag\")\n\tpass\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif tag in u'abcde':\t\treturn True\n\tif tag in u'cd':\t\t\treturn False\n\treturn False\n\n\ndef test_word() -> bool:\n\t\"\"\"Test word acceptance.\t\"\"\"\n\tword = ['I', 'he', 'did', 'that'][random.randint(1, 3)]\n\tt = TestTag(tag=tag(word), success=test_true)\n\tt.word()\n\treturn t._done\n\n\nif __name__ == '__main__':\n\timport doctest\n\tdoctest.test"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\taccept = []\n\tfor ch in tag:\n\t\tif ch not in tags:\n\t\t\tprint(\"invalid word:\", tag)\n\t\t\treturn False\n\n\t\tif ch not in accept:\n\t\t\taccept.append(ch)\n\treturn True\n\n\ndef main():\n\tdef run(tag1, tag2 = []):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\taccept = []\n\t\tfor ch in tag1:\n\t\t\tif tag2:\n\t\t\t\tif ch in tag2 and ch not in accept:\n\t\t\t\t\taccept.append(ch)\n\t\t\telse:\n\t\t\t\tif ch not in accept"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\ttag = tag.lower()\n\tfor item in [\"ad\", \"an\", \"ae\", \"an\", \"ad\", \"are\", \"at\", \"be\", \"be\", \n\t\t\t\t\"can\", \"could\", \"do\"]:\n\t\tif tag == item:\n\t\t\treturn True\t\n\treturn False\n\ndef is_letter(char):\n\ttry:\n\t\tch = CharacterClasses.objects.get(id=char)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\t\n\treturn False \n\t\t\ndef test_is_tag(tag: str"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn tag in valid_tags\n\ndef valid_tags_all(tag: str):\t\"\"\"\n\tTest all valid tags.\t\t\"\"\"\n\treturn (tag, ) + tuple(valid_tags())\n\t\n\n\n# --------------------------------------------------\n# MAIN \t\n# --------------------------------------------------\nclass TestCase(unittest.TestCase):\n\t\"\"\"\n\tTest all of the functions to run TodoDB tests.\n\t\"\"\"\n\n\tdef test_words(self) -> bool:\t\"\"\"\n\tTest whether we can create a bunch of words from an id.\n\t\"\"\"\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn tag in ('and', 'or')\n\n\n\ndef test_set_all(tag: str) -> tuple:\t\"\"\"\n\tTest a word whether it can be selected as a tag, whether the word has any words within it,\nand whether it is a conjunct.\t\"\"\"\n\treturn (tag in ('and', 'or', '=', '!') and not tags_with_words.get(tag, False))\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tpass\n\t\n\nif __name__ == \"__main__\":\n\ttag(None, True).join().print_()"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn os.path.exists(repr(os.path.join(os.getcwd(), \"test\", \"test.txt\")))\n"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\td = {}\n\tfor childnode in node.getchildren():\n\t\td[childnode.tag] = xml_children_as_dict(childnode)\n\treturn d\n\n\ndef all_parsed_xml_children_as_dict(node):\n\t\"\"\"\n\tTurn all children of node 'node' into a dict with tag_children=True.\n\nThe dictionary is in the same form as from _xml.get_text or from\n_xml.extract_tree.\n\nWARNING: this method doesn't fully parse the XML, thus not all tags found in"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif node.tag_name == \"item\":\n\t\tchild_dict = node.attrib\n\t\tchild_dict[\"id\"] = node.attrib.get(\"id\", \"\")\n\t\tchild_dict[\"title\"] = node.attrib.get(\"title\", \"\")\n\t\tchild_dict[\"text\"] = node.text\n\telse:\n\t\tchild_dict = {\"parent\": node.tag_name, \"attrib\": {\"}, \"children\": [], \"text\": []}\n\t\tfor cur_child in node:\n\t\t\tchild_dict[\"parent\"] = cur_child"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tret = {}\n\n\tif node is None:\n\t\treturn ret\n\n\tfor child in node:\n\t\tret[child.tag] = xml_children_as_dict(child)\n\n\treturn ret\n\n\n\n\n\n\n\n\n\n\n\t\nclass _NodeToHierarchy(object):\n\tdef __init__(self, root, childOrder):\n\t\t\"\"\" Build a hierarchy from a node. \"\"\"\n\t\tself.root = root\n\t\tself.childOrder = childOrder\n\t\tself.parent = None\n\n\n\tdef getChild(self, tag):\n\t\t\"\"\" Return the node containing"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\ttry:\n\t\tnode.children\n\t\tnode.children = [xml_children_as_dict(x) for x in node.children]\n\texcept AttributeError:\n\t\tnode.children = [children_as_dict(x) for x in node.children]\n\tfrom xml.etree import ElementTree\n\t\n\treturn ElementTree(node).getroot().items()\n\ndef xml_as_string(node):\n\t\"\"\"Convert a \"tag\" to a string using the XML string format, if possible\"\"\"\n\n\treturn (u'''\"%s\"''"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif hasattr(node, 'xml_kids'):\n\t\tchildnodes = node.xml_kids\n\telse:\n\t\tchildnodes = node.child_nodes\n\n\tret = {}\n\tfor n in childnodes:\n\t\tret[n.tag] = xml_children_as_dict(n)\t\t\n\t\t\n\treturn ret\n\ndef get_xml_text_node(node):\n\tif node.__class__ == etree.XML:\n\t\treturn node.get('text')\n\telse:\n\t\treturn etree.tost"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tchildren = {}\n\ti = 0\n\tnode_nodes = []\n\tfor child in node.getchildren():\n\t\t# Make the first empty node\n\t\tif is_empty_node(child):\n\t\t\tnode_nodes.append(nodes.Node(\"data\"))\n\t\t\ti = 0\n\t\t\tcontinue\n\t\t# We have a child node\n\t\tchild_dict = xml_children(child)\n\t\tnode_nodes[i] = node_nodes[i].append(xml_children_as_dict(child))\n\t\tnode_nodes[i].append"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tchildren = xml_children(node)\n\tif len(children) == 0:\n\t\treturn None\n\telse:\n\t\treturn dict([(n, xml_children_as_dict(n)) for n in children])\n\ndef get_last_child(node):\n\t_child = None\n\twhile _child is None and node!= None:\n\t\t_child = xml_children(node)\n\t\tnode = xml_parent(node)\n\treturn _child\t\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tdict = {}\n\tfor i in range(len(node.contents){}):\n\t\tdict[node.contents[i]['tag']] = node.contents[i].xml_children_as_dict(node)\n\treturn dict"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\td = {}\n\tfor c in node.findall(\".//\" + node.tag):\n\t\td[c.tag] = c.text\n\treturn d\n\n# -------------------------------------------------------------------------\n#\n##  This is derived from Python from the XML Python module:\n##    http://www.python.org/dev/peps/pep-0338/\n##\n#  It does not require that XMLBeets be the main API. If XMLBeets is\n#  defined here, then it will still need to be installed to install\n#  PyXMLBeets."}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t# We have to do this because it's going to get called from inside recursing!\n\tparent = node.parent\n\tif parent.xml_children_as_dict:\n\t\treturn dict( parent.xml_children_as_dict(node)  )\n\telse:\n\t# It's a simple node, just use the parent's child dict\n\t\treturn { parent.xml_element: { node.xml_element : {}} }\n\n\n\n\ndef _write_xml(name, text):\n\t\"\"\"Write an XML with any parent tags.\t\"\"\"\n\tparent ="}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif(sender_handle == None) or (entity_handle == None):\n\t\treturn False\n\tif(sender_handle == object_sender_handle_to_check):\n\t\treturn True\n\tif(sender_handle.split('/')[0].split('.')[0] == entity_handle.split('/')[0].split('.')[0]):\n\t\treturn True\n\treturn False\n\ndef is_authorized_for_sender_and_entity_handle(sender_handle, entity_handle, auth_token):\n\t\"\"\""}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\ttry :\n\t\treturn\n\texcept :\n\t\treturn False\n\n\n### Message ##############################################################################################################\n\nmsg = { \"class\" : \"MsgId\", \"body\" : \"A message...\" }\n\ndef check_msg_match(msg):\n\treturn str(msg)[:7] == str(msg)\n\n\ndef check_msg_match2(msg):\n\treturn bool(msg) and str(msg)[:7] == str(msg)\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif sender_handle == sender:\treturn True\n\n\tif entity_handle in entity_hashes:\treturn True\n\tif entity_handle in entity_hashes_for_sender:\n\t\treturn True\n\n\n\treturn False\n\n\n#################\n###\tSender data helpers\n#################\n\n\ndef encode_senders_in_charset(senders, charset=True):\n\t\"\"\"\n\tEncode a dictionary of senders in the given `charset`\n\n\tcharset=False if you want the `SenderData` to not use\n\t\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t# if the sender_handle is the same, return\n\tif sender_handle!= sender_handle_exists(sender_handle):\n\t\t# don't know where the real sender is - don't verify \n\t\treturn\n\t# if the entity handle is the same, return\n\tif entity_handle == entity_handle_exists(entity_handle):\n\t\t# don't know where the real entity is - don't verify\n\t\treturn\n\n\t# Verify that sender/entity handles match.\n\tsender = dicom.Sender(sender_handle_exists("}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\theaders = self.get_headers(sender_handle, entity_handle)\n\tif headers[SENDER_HANDLE]!= headers[SENDER_ENTITY_HANDLE]:\n\t\traise InvalidClientException(\n\t\t\t'{} is trying to set sender and entity handles on different client!'.format(sender_handle)\n\t\t)\n\n\n#------------------------------------------------------------------------------\n\n\nclass InvalidClientException(Exception):\n\tdef __init__(self, what):\n\t\tself.what = what)\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tassert entity is not None\n\tif sender_handle in (sender_handle_match[0], sender_handle_match[1]):\t\t# It's us, good. Let's check by looking for entity and sender handle.\n\t\tentity_ref_type = \"type_id\" if entity is not None else \"entity_id\"\n\t\trefs = entity.find_reference_type(entity_ref_type)\n\t\tif refs:\n\t\t\tref = refs[0]\n\t\telse:\n\t\t\t##print \"No Entity Type Reference in the Body!\"\n\t\t\treturn"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif sender_handle!= entity_handle:\n\t\traise Exception(\"Can't use sender_handle to check the match.\")\n\t\n\tpayloads = senders.get(sender_handle)\n\tif len(payloads) == 0:\n\t\traise Exception(\"Can't find sender_handle in the list of senders.\")\n\tif entity_handle not in payloads:\n\t\traise Exception(\"The payloads are not for the sender in question.\")"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif sender_handle == sender_handle:\n\t\treturn s.diaspora.check_sender_and_entity_handle(\n\t\t\t\ts.payload.body,\n\t\t\t\ts.payload.headers.get('sender').get(s.sender_property))\n\telse:\t# If something's different then we can send ourselves the correct one\n\t\treturn s.diaspora.check_sender_and_entity_handle(\n\t\t\t\ts.payload.body,\n\t\t\t\ts.payload.headers.get('sender'))\n\n\ndef main():"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n\tif (sender_handle!= entity_handle) and (sender_handle.entity_id!= entity_handle.entity_id):\t\n\t\tprint \"Trying to overwrite sender\", sender_handle, \"with an entity\", entity_handle\t\n\t\t\n\t\treturn sender_handle.entity_id - entity_handle.entity_id\t\n\t\t\n\treturn None"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\treturn sender_handle == sender_entity_handle\n"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tdoc = {\n\t\t\"url\" : url,\n\t\t\"document_path\" : document_path\n\t#\t\"document_path\" : \"example.com\"\n\t} \n\treturn doc\n\ndef generate_well_known_document(url, document_data_list=None):\t\"\"\"\n\tGenerate a NodeInfo.well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_data_list: list of node information"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\turl = fix_protocolo(url)\n\tdoc = requests.get(base_path + url)\n\tif document_path is not None:\n\t\tdoc['file']=document_path\n\telif 'file' in doc:\n\t\tdoc['file'] = doc['file']\n\treturn doc\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tdoc_dict = {}\n\tif not document_path is None:\n\t\turl_part = document_path.split(\"/\")\n\t\turl_part_slice = slice(0, len(url_part))\n\t\turl = \"/\".join(url_part)\n\t\turl_part[-1] = url\n\t\tdoc_dict[\"url\"] = url\n\tdoc = \"<!DOCTYPE document SYSTEM \\\"%(dtdUrl)s\\\">\" % doc_dict\t\t\n\tf = urllib_request.urlopen(url)\n\tif f.code"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tparsed = utils.get_parsed_url(url)\n\treturn(utils.get_dict(parsed, document_path))\n\n\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif document_path is None:\n\t\tdirectory = './'\n\telse:\n\t\tdirectory = os.path.abspath(document_path)\n\n\tname = 'WellKnown Documents' + url[1:]\n\tif document_path is None:\n\t\tdoc = self.client.docs.get(url)\n\telse:\n\t\t# If we have custom document, ask for the user with the filename\n\t\ttry:\n\t\t\tdoc = self.client.docs.get_file(url, filename=document_path)\n\t\texcept ValueError:\n\t\t\tmsg = '"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\timport json\n\tfrom django.core.exceptions import *\n\ttry:\n\t\tmime = file(\n            document_path)\n\n\t\tmime.seek(0)\n\t\t# The document is ascii so load and write into binary format.\n\t\tif hasattr(mime,'read') and hasattr(mime.read,'read'):\n\t\t\tdoc = mime.read().replace('utf-8', 'utf-16le').replace('\\n', '')\n\t\t\tdoc = json.loads(doc)\n\t\telse:\n\t\t\t# There"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tdocument_path = document_path or url\n\n\t# Get the current node info\n\tinfo_raw = info()\n\tinfo_raw.fetch({'path' : document_path})\n\tinfo = json.loads(info_raw)\n\n\t# Prepare the document\n\treturn {'version': info['currentVersion'], 'name': info['name'], 'nodeType': info['nodeType'], 'timestamp': str(\n\t\tinfo['timestamp']), 'fileSize': str(info['fileSize'])}\n\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tdoc_doc = None\n\ttry:\n\t\tif document_path is None:\n\t\t\tdoc_doc = dia_util.file_resolve(url)\n\t\telse:\n\t\t\tdoc_doc = dia_util.file_resolve(document_path)\n\texcept:\n\t\tdoc_doc = dia_util.file_resolve(url)\n\n\treturn node.get_document(doc_doc)\n\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\theaders = []\n\tresponse = urlopen(url)\n\tdoc_string = ''\n\tfor line in response.readlines():\n\t\tline = line.strip()\n\t\tif line[0] == \"<\" and line[-1] > \">\" and line[-1]!= \"<\" and line[-2] == \">\" and line[\n\t\t\t\t\t-2]!= \"<\" and line[-3:] == \">\":\n\t\t\t# we have just two lines.\n\t\t\tdoc_string += line\n\t\telif line:\n\t\t\t# we have just a single line\n\t\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn nodepath_to_doc(url, document_path)\n\n# Get the NodeInfo document as bytes\ndef get_nodeinfo_document(document_path=None):\t\"\"\"\n\tGenerate a NodeInfo.well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: str\t\"\"\"\n\treturn document_to_byte(document_path)\n"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tpubkey = public_key.encode('utf8')\n\t\n\treturn self._dht.verify_relayable_signature(pubkey, doc, signature)\n\n#===============================================================================\n#\tMessage\n#===============================================================================\ndef get_message(doc):\n\tdoc = doc.replace(\"\\n\", \"\")\n\tdoc = doc.replace(\"\\r\", \"\")\n\tdoc = doc.replace('(', '').replace(')', '').replace(' ', '').replace('\"', '')\n\tdoc = doc.split(\"msg\")"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\n\treturn public_key, doc, signature.verify(doc)\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tresult = verify_signature(public_key, doc)\n\treturn result.satisfied_with()\n\nclass XMLSignature(Signature):\n\tdef __init__(self, public_key, doc, signature):\n\t\tself._public_key = public_key\n\t\tself._doc = doc\n\t\tself._signature = signature\n\n\t@property\n\tdef public_key(self):\n\t\treturn self._public_key\n\n\t@property\n\tdef doc(self):\n\t\treturn self._doc\n\n\tdef verify(self, signing_cert=None,"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\ttry:\n\t\tm = xml.dom.minidom.parse(doc)\n\t\tpk = m.getElementsByTagName(\n\t\t\t\t\t\"pubkey\")[0].firstChild.data\n\t\tdsig = m.getElementsByTagName(\n\t\t\t\t\t\"ds:signature\")[0].firstChild.data\n\t\th = m.getElementsByTagName(\n\t\t\t\t\t\"h:signature\")[0].firstChild.data\n\t\texpected_hash = base64.urlsafe_b64decode(\n\t\t\t\t\tpublic_key).split(\"|"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\ttry:\n\t\treturn ctypes.c_ubyte(ctypes.string_at(ctypes.addressof(doc), 10))\n\texcept:\n\t\tpass\n\n\treturn ''\n\n\ndef verify_relayable_signature(public_key, doc, signature, time=None):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\ttry:\n\t\treturn ctypes.c_ubyte(ctypes.string_at(ctypes.addressof(doc), 10))"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\ttry:\n\t\thash = hashlib.sha256(x_encoded_xml_blob(doc)).hexdigest()\n\texcept:\n\t\treturn False\n\tif hash[-4:] == sig: #signature is trusted\n\t\treturn True\n\telse:\n\t\treturn False\n\tif not verify_relayable_signature(public_key, doc, signature): return False\t#need to ensure the signature is signed\n\treturn True\n\n\t\ndef verify_authority(public_key):\t#check signature from server by public key\n\tcrc32 ="}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tdoc = doc.replace(\"<xml:signature />\", signature)\n\tdoc = doc.replace(\"<xml:signature\", signature)\n\tdoc = doc.replace(\"<xml:signature />\", signature)\n\tfor elem in elem.elements:\n\t\tif \"xml:verify-sig\" in elem.name or \"xml:sign-sig\" in elem.name:\n\t\t\tassert elem.elementType == \"xsi/signature\"\n\t\t\txsi_elem = elem.elements[0]\n\t\t\tassert \""}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tpublic_key = str(public_key)\n\tsignature = str(signature)\n\tassert( isinstance(public_key, bytes) ) and isinstance(signature,bytes)\n\tassert( isinstance(doc, unicode) )\n\tassert( isinstance(doc, text_type) )\n\n\tdoc, signature = doc, signature\n\tretro = []\n\tretro.append(\"|SigPub|\")\n\tretro.append( public_key )\n\tretro.append(\"|SigExp|\")\n\tretro.append"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tverifier = HashSignature(doc, signature, public_key)\n\tverified = verifier.verifies(doc)\n\tassert hasattr(verified, \"confidence\"), \"No confidence, verify didn't detect signature authenticity.\"\n\treturn verified\n\t\ndef is_relayable_message(doc, signature, message):\ttry:\n\tis_relayable(doc, signature, message)\n\treturn 1\n\ndef is_nonrelayable(doc, sign):\ttry:\n\tis_nonrelayable(doc, sign)\n\treturn 1\n\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\n\t# TODO: verify public key or signature\n\t\n\t# TODO: verify signature\n\t\n\tdocument = etree.XML(doc)\n\tsubject_list = document.xpath('/signature/sender/receiver/sender/x:SigConf/@Type/text').extract_first()\n\t# TODO: more verification\n\t\n\treturn public_key in subject_list\n"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://github.com/diaspora/diaspora_federation\n\n[DEPRECATED] You can use this function through :obj:`url_diaspora_webfinger` if you don't require XRD (XRD)\n\nParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n\tArgs:\n\t\tdocument: A string (new) of JSON document, or the original document (XRD"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tres = {}\n\turl_pattern = r'<([^<]+);'\n\twith open(document, 'r') as f:\n\t\tfor line in f:\n\t\t\tfor match in re.finditer(url_pattern, line):\n\t\t\t\turl = match.group(1)\n\t\t\t\tf.seek(match.start())\n\t\t\t\ttry:\n\t\t\t\t\tres[url]\n\t\t\t\t\tcontinue\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tres[url] = {\n\t\t\t\t\t'url': url,\n\t\t\t\t\t'info': line,\n\t\t\t\t\t'digest': None\n\t\t\t\t}"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t# todo: add to documentation\n\treturn {}\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif '\\r\\n' in document:\n\t\tnew_document = dict(new=True)\n\t\tdocument = document.split('\\r\\n')[0]\n\telse:\n\t\tnew_document = dict(new=False)\n\tlines = document.split('\\n')\n\td = dict()\n\tfor line in lines:\n\t\tif line.startswith('name: '):\n\t\t\tline = line.partition('name: ')[2]\n\t\t\td['new_name'] = line.strip().replace('&', '-')\n\t\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdocument = json.loads(document)\n\n\tdiacpora_url = 'https://api.diaspora.io'\n\tdiaspora_api = 'https://api.diaspora.io/v1'\n\n\tfor k in document['messages']:\n\n\t\tmessage = document['messages'][k]\n\n\t\twebfinger = {}\n\t\twebfinger['url'] = diacpora_url + '/' + diaspora_api\n\n\t\tfor v in message['attachments']:\n\n\t\t\t# extract only the \"data\" key\n\t\t\ttry:\n\t\t\t\tcontent"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora_version = document.get('version')\n\tif diaspora_version and diaspora_version!= '':\n\t\twith open('webfinger.json', 'rb') as jsonfh:\n\t\t\tdiaspora_jhf = json.load(jsonfh)\n\n\t\t# Parse JSON format\n\t\tdiaspora_list = []\n\t\tfor j in range(1000):\n\t\t\tdiaspora_jhf = diaspora_jhf['files'][0]\n\t\t\tdata = diaspora_jhf['data']["}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif document not in Diaos.webfinger:\n\t\treturn {'result': False,\n\t\t\t\t'error':\"There was a problem with this document :-( }\",\n\t\t\t\t'html':'',\n\t\t\t\t'desired_version':None}\n\telse:\n\t\tcontent = Diaos.webfinger[document]\n\t\tif content[3]['result']:\n\t\t\tDiaos.webfinger_json_format = True\n\t\telse:\n\t\t\tDiaos.webfinger_json_format = False\n\t\t\tif content[3]['error']:\n\t\t\t\tD"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\ttry:\n\t\tresults = _DiasporaResults()\n\t\t_DiasporaWebfingerResults = results.parse_diaspora_webfinger(document)\n\t\treturn {i: _DiasporaWebfingerResults.values()[i] for i in _DiasporaWebfingerResults.keys()}\n\texcept AttributeError:\n\t\treturn {}\n\n\n\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora = {\n\t\t'webfinger_id':  {\n\t\t\t'url': document\n\t\t}\n\t} \n\tdata = json.loads(dedent(JSON.load(urllib.request.urlopen(config.gettext(u'request'))))).get(u'url')\n\n\t# parse\n\tdiaspora = parseDOM(data, {\n\t\t'content':\n\t\t\tlambda match: {'url': match['url'],'metadata_xml': ''},\n\t\t'structure':\n\t\t\tlambda match: {\n\t\t\t\t'extension"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora_data = json.loads(document)\n\tdiaspora_data.pop('page').pop('link')\n\tdiaspora_data_new = diaspora_data\n\tif 'pageInfo' in diaspora_data:\n\t\tif 'diasporaVersion' in diaspora_data['pageInfo']:\n\t\t\tdiaspora_data_new['pageInfo']['diasporaVersion'] = diaspora_data['pageInfo']['diasporaVersion']\n\t\telse:\n\t\t\tdiaspora_data_new['"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\treturn pymongo.connection.find_one({\"Handle\": handle}).get(\"Document\").get(\"Body\")\n\n\t\ndef build_url_string(base_url: str, **overrides):\n\t\"\"\"\n\tBuild a url string from a base url and a list of kwargs\n\t\n\tArgs:\n\t\tbase_url {str}: url base. May contain scheme, hostname, port number, etc.\n\t\toverrides {dict(str) -> str}: any keyword args will be applied to the resulting\n\t\t\tURL (not URLs, for performance). Over"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\th = WebfingerRetriever(handle)\n\t\treturn h.retrieve()\n\texcept WebFingerError as wfe:\n\t\treturn wfe.webfinger_url\n\texcept Exception:\n\t\treturn None\n\n\ndef check_for_webfinger_errors(handle: str) -> None:\t\t\t\t\t\t\n\turl = try_retrieve_webfinger_document(handle)\n\n\tif not url:\n\t\treturn\n\telse:\n\t\tprint('{}'.format(url))\n\n\t\t\ndef start_retrieval(directory: str) -> int"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\turlPath = urlPath(handle)\n\t\turl = URL.parse(urlPath)\n\t\tif \"://\" in urlPath:\n\t\t\turlPath = urlPath.split(\"://\", 1)[1]\n\t\telse:\n\t\t\traise\n\texcept UrlParseError as e:\n\t\tlogging.print_error(\"Failed to parse url %s\" % urlPath,\n\t\t\tlogging.DEBUG)\n\t\treturn None\n\n\ttry:\n\t\turlDoc = json.loads(url.read())\n\t\tlogging.print_error(\"JSON:\","}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tf = requests.post('https://webfinger-detripper.readthedocs.io/fetch_from_webfinger/',\n\t\t\t{\n\t\t\t\t'url': handle,\n\t\t\t\t'format': 'json',\n\t\t\t})\n\tif f.status_code == 200:\n\t\treturn f.json()\n\telse:\n\t\treturn None"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\treturn self._client.webFinger.requestDocument(handle)\n\n\tdef check_retrieval_success(handle: str) -> bool:\n\t\t\"\"\"\n\t\tCheck if a record was retrieved from the webfinger database. Does not raise an exception if it fails.\n\t\t\"\"\"\n\t\treturn self._client.webFinger.recordStatus(handle) == 0\n\n\tdef check_retrieve_success(handle: str) -> bool:\n\t\t\"\"\"\n\t\tCheck if a record was retrieved from the webfinger database. \n\t\tDoes not raise an exception if it fails.\n\t\t\"\"\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\treturn None\n\n\n\tdef try_get_webfinger_url(handle: str) -> Optional[str]:\t\n\t\t\"\"\"\n\t\tGet the URL of the webfinger data.\n\t\tThis should give a URL if there is a problem extracting the data.\n\t\t\"\"\"\n\t\treturn None\n\n\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\n\t# Initialize an empty dictionary. If the caller wishes to retrieve a body and the URL is empty,\n\t# they only need one dictionary. The caller will make the same dictionary if more than one\n\t# webfinger request succeeds.\n\tresults = {}\n\t\n\t# Make an empty list\n\turl_list = []\n\t\n\t# If the URL was used in the first URL request, then initialize.\n\tif(strutils.string_has_utf8_utf16_characters(handle)):\n\t\thandle = strutils.utf8(handle).replace(\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tresult = handle.sendRequest(\"http://www.rfc-editor.com/trac/ticket/1701#rfc_7033_authorization\", \"GET\")\n\tif result.status == 204:\n\t\treturn None\n\telse:\n\t\treturn result.body\t\n\n\ndef retrieve_web_finger(webfinger_handle: str) -> str:\t\"\"\"\n\tGet the result of an RFC7033 webfinger (http POST or GET). Should be used within the API.\n\t\n\tArgs:\n\t\twebfinger_handle (str):\n\t\"\"\"\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tif not handle:\t\treturn None \n\ttry:\t\t\t\t(response, _, body) = HTTP(HTTP_GET, \"http://webfinger-api.org/v1/records\", handle=handle)\t\t\n\t\tbody = body.read()\n\texcept:\n\t\treturn None\n\tresult = re.search(\"<rst>\\n<record type=\\\"webfinger\\\"><webfinger></webfinger>\", body)\n\treturn result.group(1)\n\t\ndef try_retrieve_webfinger_metadata_document(handle: Optional[str] = None"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\n\tpath_to_file = 'webfinger_files/' + handle\n\ttry:\n\t\twith open(path_to_file, 'r') as f:\n\t\t\tdoc = f.read()\n\t\treturn doc\n\texcept:\n\t\tpass\n\n\treturn \"\"\n\t\nif __name__ == \"__main__\":\n\t_main(sys.argv)"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\trequest = request_dict()\n\tresponse = parse_diaspora_webfinger_response(handle, request)\n\tresponse_dict = dict(zip(response.keys(), response[0]))\n\treturn response_dict\n\n\ndef parse_diaspora_webfinger_response(handle, request):\t\"\"\"\n\tParse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:kwarg request:  JSON request body\n:returns: dictionnay\t\"\"\"\n\ttry:\n\t\tresponse = parse_diaspora_web"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn parse_diaspora_webfinger(handle_url(handle))[1]\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t#TODO Make this less verbose\n\turl = 'http://diaspora.com/api/rest/2/get/' \\\n\t\t\t\t\t'/?handle={}&timestamp={}&' \\\n\t\t\t\t\t'message_id={}&' \\\n\t\t\t\t\t'date={}'\n\tmessage_id = ''\n\tdate = ''\n\tbody = ''\n\ttry:\n\t\tbody, message_id, date, \\\n\t\turl = handle.retrieveFile('webfinger').split('|')\n\texcept:\n\t\tpass\n\ttry:\n\t\tif"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tdata = urlparse(handle).path + \"/\" # +path as we start from 1\n\n\trequest = requests.get(data)\t\n\tdiaspora_webfinger_doc = diaspora_webfinger_doc(request.content)\n\t\n\tif diaspora_webfinger_doc:\n\t\treturn diaspora_webfinger_doc.update(download=False)\n\ndef get_urls(file_list):\n\t\"\"\"\n\tReturn list of download and url of the files from the Diaspora urls.\n\n\t:param file_list:"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\timport gzip\n\timport urllib.request\n\timport requests\n\tfrom requests.models import Response\n\tfrom requests.exceptions import ConnectionError\t\n\tfrom requests.utils import parse_options\n\timport urllib\n\ndef retrieve(file, webfinger, handle):\n\t\"\"\"Retrieve a and parse a document using a remote Diaspora webfinger\"\"\"\n\tr = requests.get(filepath=file)\n\tif r.status_code == 200:\n\t\tdoc = urllib.parse.urlencode({'data':r.content, 'we"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\thandle = _diaspora_protocall(handle)\n\tdata = handle.fetch_document_by_handle(handle)\n\treturn _parse_diaspora_webfinger(data)\n\ndef _diaspora_protocall(handle):\n\t\"\"\"\n\tWrapper around the remote Diaspora protocol calls API which returns the dictionary\n\trepresenting a Diaspora remote document.\n\n:arg handle: Remote handle to query\n:returns: dict\t\"\"\"\n\treturn handle.fetch_document(handle)\n\ndef _retrieve_and"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\td = dict(\n\t\tname=None,\n\t\tip=None,\n\t\tip_domain=None,\n\t\tdiaspora_server=None,\n\t\tdiaspora_username=None,\n\t\tdiaspora_password=None,\n\t\tdiaspora_api=None,\n\t\tdiaspora_token=None\n\t)\t\t\n\n\theaders = {\n\t\t\"X-Diaspora-Access-Token\": \"diaspora\",\n\t\t\"Accept\": \"application/json\"\n\t}\n\n\tif not iswindows:\n\t\tbody, e = retrieve_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tdiaspora = []\n\tif os.path.exists(handle):\n\t\tdiaspora = open(handle).readlines()\n\t\tdiaspora = [d for d in diaspora if len(d) == 5]\n\treturn dict(diaspora=diaspora)\n\n\nclass RemoteHandler(tornado.web.RequestHandler):\n\t\"\"\"HTTP request handler.\"\"\"\n\tdef __init__(self, server):\n\t\tsuper(RemoteHandler, self).__init__()\n\t\tself.server = server\n\t\tself.body = ''\n\n\tdef"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn dict()\n\ndef retrieve_and_parse_diaspora_webfinger_content(handle, contentHandler):\t\"\"\"\n\tSend a and parse a remote Diaspora webfinger document.\n\n:arg handle: the id of the remote handle\n:arg contentHandler: the HttpResponse object to respond to the http get request\n:returns: dictionary\n\n:Example:\n\t\tfrom pypi.pypitranslation import retrieve_and_parse_diaspora_webfinger_content\n\t\tmyHandle = 'http://localhost/pypi.pypit"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn_dict = {\"content\"\t\t: \"\" }\n\theaders\t\t= {\"webfinger\"\t\t: \"\" }\n\tret\t\t\t= webfinger.grab(handle, headers, return_dict)\n\tresponse_dict\t= return_dict[\"daspora\"]\n\treturn {\"content\"\t\t: response_dict[0],\n\t\t\"daspora\"\t\t\t: response_dict[1],\n\t\t\"extendedlink\"\t: response_dict[2] }\n\n\ndef retrieve_and_parse_diaspora_html5(handle):\t\"\"\"Retrieve a and parse a"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn XRD.XRD(self.service_url+'/xrd/v1/download/meta/',auth_token=host.auth_token)  # auth_token not used on this server yet\n\n\n# Retrieve meta documents. Does not return results.\n@api(apiLevel=0)\ndef retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n## TODO:\n##"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\"\"\"xrd_get_host_descriptor does not exist\n\t\"\"\"\n\tremote_host_metas = {}\n\txrd = xrd_get_host_descriptor(host)\n\tif xrd is None:\n\t\traise InvalidRequest(\"no xrd for %s\"%(host.label))\n\t\treturn \"none\"\t# return all meta information for this host\n\twhile 1:\n\t\tremote_host_metas[xrd.description.get(\"host_description\", None)] = xrd # add host_description to meta\n\t\tremote_"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\trq = \"XRD \" + host + \".meta\"\n\tret = xrd(rq)\n\tif ret is None: raise ValueError(\"Host %s not found for XRD\" % host)\n\treturn ret\n\n\n@retry(exceptions=(HTTPError, HTTPServerError), run_afterwards=True)\n@deco(retry=exceptions=ValueError, retry_msg=\"Retryable error\")\ndef download_diaspora_files(host, diaspora, dirs=\"\"):\t\"\"\"\tDownload a list of files from a D"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn http.get(host + '/meta', follow_redirects=False)\n\n\ndef retrieve_diaspora_host_user_token(host):\t\"\"\"\n\tRetrieve a User-Token key from the remote Host\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn http.get(host + '/user_token', follow_redirects=False)\n\n\ndef initialize_diaspora():\n\t\"\"\"\n\tInitialize the diaspora database and start the sync process.\n\t\"\"\"\n\tif config"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn xrd(\n\t\t\"http://api.diaspora.com/1/diaspora/_connect?api_key=%s&host=%s\" % (CFG_DIAXIOREGION_KEY, host)\n\t)\ndef get_diaspora_host_uri(diaspora_data, host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn xrd(\"http://api.diaspora"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn Xrd(f\"https://{host}/{_api.meta}\").json_or_raise()\n\n\ndef retrieve_diaspora_version(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora version documents.\n\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\t\"\"\"\n\treturn Xrd(f\"https://{host}/v{_api.version}\").json_or_raise()\n\n\ndef update_diaspora_meta(host):\n\t\"\"\"\n\tUpdate an existing Diaspora"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tresponse_dict = remote(host).request.return_value\n\trequest_dict = response_dict.info\n\thost_meta = None\n\tfor field in request_dict.keys():\n\t\tif request_dict[field].endswith(\"Meta\"):\n\t\t\tif field == \"content_type\":\n\t\t\t\thost_meta = field\n\t\t\telse:\n\t\t\t\n\t\t\t\tif field == \"name\":\n\t\t\t\t\thost_meta = field\n\t\t\t\telif field == \"content_type\":\n\t\t\t\t\thost_meta = field\n\t\n\t\t\t\tif host_meta!= None and type_ex"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\twith open(host, 'rb') as f:\n\t\treturn xrd.from_remote(f, host, 'XRD', \"metadata/\", False)\n\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\ttry:\n\t\tresp = urllib.request.urlopen('https://api.diaspora.com/v1/host-m/' + str(host) + '/meta')\n\t\treturn XRD(resp)\n\texcept urlopen.HTTPError:\n\t\traise Exception('Remote API error:'+ resp.getcode())\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn self._meta(host)._get('XRD')\t"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\ttry:\n\t\trequest = urllib.request.Request(url, data=data)\n\t\tprint(\"Sending request:  {}\".format(request))\n\t\trequest.add_headers('Accept-Encoding', 'gzip')\n\t\treturn requests.post(request,timeout=timeout, **kwargs)\n\texcept Exception as e:\n\t\traise Exception(\"Error attempting to connect to url {}, data: {}\".format(url, data))\n\ndef handle_exception(exctype, value, traceback):\n\t\"\"\"\n\tWrapper around ``requests"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\ttry:\n\t\trequests.post(url, data, *args, **kwargs)\n\texcept requests.exceptions.RequestException as e:\n\t\treturn (None, e)\n\treturn (500, None)\n\ndef get_object_via_url(obj, url):\tdict_to_object(url, obj)\n\n\"\"\"\n\tFor internal use, just return the url, which is the document url.\n\"\"\"\ndef object_via_url(obj, url):\n\treturn obj.url.rstrip('/\\\\*')\n\n\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\theaders = {\"Content-Type\": \"application/json\"}\n\tif isinstance(data, basestring):\n\t\tdata = { \"document\": data }\n\theaders[\"Content-Length\"] = str(data.pop(\"Content-Length\"))\n\tdata = json.dumps(data) if isinstance(data, basestring) else data\n\turl = url + \"/\" + method\n\tresponse = urllib.request.urlopen(url, data, headers=headers, timeout=timeout)\n\tstatus_code = response.status\n\terror = None\n\tif"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn send_document(_prepare_url(url), data, method, *args, **kwargs)\n\ndef send_document_in_queue(url=None, data=None, callback=None, *args, **kwargs):\n\t\"\"\"\n\tHelper method to send a document via POST and POST queue.\n\n\tAdds ``callback`` to the callback on success of the request.  This also gets\n\tthe status code from the response.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\n\tstatus, headers = None, None\n\t\n\t# Attempt to use ``requests``.\n\ttry:\n\t\tresponse = requests.post(url, data=data, **kwargs)\n\t\tstatus, headers = response.headers, response.text\n\texcept requests.exceptions.MaxRetriesError:\n\t\t# Unfortunately ``requests.get`` is an optional mechanism to get an HTTP response for multipart HTTP requests, so we can't handle that properly\n\t\tpass\n\texcept requests.exceptions.BaseHTTPError:\n\t\t# Probably a 403 Forbidden on HTTP 1."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\ttry:\n\t\tstatus_code = 200\n\t\terror = None\n\n\t\tif method == \"delete\":\n\t\t\tstatus_code = 204\n\n\t\tif args:\n\t\t\tdata = b'\\n'.join(map(str, args))\n\n\t\tif data:\n\t\t\tdata = b'\\n'.join((data, C2HTTPConstants.CRN))\n\n\t\thttp_data = b\"%s\\n%s\" % (url, data)\n\t\treq = requests.post(url, headers=C2HTTPConstants.JSON_HEADER, data=http_data,"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\n\nclass Server:\n\t\"\"\"\n\tServer implementation for using PIL in web servers.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor. Instantiate a ``Server`` object to do HTTP GET requests\n\t\tto a PIL server. The PIL Server has nothing to do.\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_http_headers(self, request):\n\t\t\"\"\"\n\t\tAdd standard HTTP headers to the response object.\n\n\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn requests.api_request(method=method, *args, **kwargs) >> data\n\ndef _send_file(url, data, timeout=10):\t\"\"\"Helper method to upload/sending a local file through POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: a file object, see `python/fileupload`\n:arg timeout: Seconds to wait for response (defaults to 10)\n:returns: T"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif isinstance(data, dict):\n\t\t# Form-encoded\n\t\ttry:\n\t\t\tdata = urllib.parse.urlencode(data)\n\t\texcept ValueError:\n\t\t\treturn (None, 'data must be bytes or unicode')\n\n\t\treturn send(url, data, timeout=timeout, method=method, *args, **kwargs)\n\telif isinstance(data, str):\n\t\t# bytes or file-like object\n\t\ttry:\n\t\t\tf = tempfile.NamedTemporaryFile(delete=False)\n\t\t\tf.close()\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\ttry:\n\t\trv = requests.post(url, data, *args, **kwargs)\t\n\t\tif rv.status_code == 200:\n\t\t\treturn (None, None)\n\t\telse:\n\t\t\terr = RequestError()\n\t\t\terr.error_code = rv.status_code\t\n\t\t\terr.error_description = rv.reason\t\n\t\t\treturn (rv.status_code, err)\t\n\texcept Exception as e:\n\t\treturn (e, None)"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tdic = OrderedDict()\n\tfor k in keys:\n\t\t# print \"Before: {}\".format(k)\n\t\t# for subkeys in k.split('.'):\n\t\t# \tprint \"After: {}\".format(subkeys)\n\t\tdic[k] = val\n\t# print \"After:\"\n\t# for k in dic.keys():\n\t# \tprint \"After: {}\".format(k)\n\tdic.update(dict_insert(dic, val, *keys))\n\treturn dic\n\ndef dict_"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tdic[key.split('.')[-1]].append(val)\n\ndef dict_append(dic, val, key, *keys):\t\"\"\"\t\n\tappend two values to a dictionary\n\nto append the values for a key and corresponding sub keys, all ancestor\nkeys should be given as method's arguments\n\nexample:\n  dict_append(dic, 'val', 'key1.key2'.split('.'), 'key3'.split('.'))\n\n:param dic: the current dictionary object\n:param val: an arbitrary value\n:param key"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor key in keys:\n\t\tdic[key[0]] = val\n\treturn dic\n\n\ndef dict_delete_key(dic, key, key1, key2, *keys):\t\"\"\"\n\tdelete a key by a given key.  in a key, a subkey should be the key1\n\tvalue.\n\n\tall ancestor keys except the given subkey should be stored as\n\tmethod's arguments\n\nto delete a key, all ancestor keys should be given as method's arguments\n\nexample:\n\tdict_delete_key({'key_1':val1"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\ttmp = dic.get(key, None)\n\tdic.set(key, val)\n\ttry:\n\t\tif type(tmp) == list:\n\t\t\tfor i in range(0, len(tmp)):\n\t\t\t\tdict_insert(dic, val, tmp[i], *keys)\n\t\telse:\n\t\t\tdict_insert(dic, val, keys and keys[0] or tmp, *keys)\t\t\t\n\texcept Exception, e:\n\t\t# raise exception\n\t\treturn dic, val\n\treturn None, val\n\n\t\n# dictionary_sort"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tnitems = 1 #nested key count\n\tfor k in map(getattr, keys, []):\n\t\tif k.startswith('.'):\n\t\t\tnitems += 1 #the given key is a nested key\n\t\t\tprint \"Using nested dict_insert with nested key\", k.split('.')[-1]\n\t\t\tdict_insert(dic, val, getattr(dic, k), *keys)\n\t\telse:\n\t\t\tif nitems <= nitems_total:\n\t\t\t\tk = dic[k]\n\t\t\telse:\n\t\t\t\tprint \"The number\", nit"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t_keys = list(key)\n\tfor k in keys:\n\t\tif type(k[0]) is str:\n\t\t\t_keys.remove(k) # remove the deepest key from the _keys list\n\t\telse:\n\t\t\tif type(val) is str:\n\t\t\t\t# key.key2.key3 or key.a\n\t\t\t\tif len(_keys) > 0:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t_values = [v for k,v in dic.items() if k.startswith(k)] # get all childs\n\t\t\t\t\t\t_insert = _values[k]"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t_dict={}\n\tfor key in keys:\n\t\t_dict[key]=dict_insert(dic,val,key,*keys)\n\treturn _dict\ndef dict_delete(dic, key, *keys):   \"\"\"\n\tdelete the elements of a nested key from a dictionary\n\nto delete a key value of a nested key, all ancestry keys should be given as\nmethod's arguments\n\nexample:\n  dict_delete(dict_delete({}, 'key1.key2'.split('.')), 'key1.key2')\n\n:param keys: a"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif isinstance(dic, dict):\n\n\t\tfor k in keys:\n\t\t\tif k not in dic:\n\t\t\t\tdic[k] = val\n\t\t\t\tv = dic[k]\n\n\telif isinstance(dic, list):\n\n\t\tif all(isinstance(v, dict) for v in dic):\n\t\t\t[dict_insert(dic, key, v) for v in dic]\n\ndef method_append(dic, value):\t\"\"\"\n\tappend a value to a hash\n\nexample:\n  dict_append({}, 'val"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n\tkeys=keys[0]\t# remove first key\n\tif dic is None:\n\t\tdic=dict()\n\t\tdic['val']=val\n\telse:\n\t\tif len(keys)!=1\t:  return\n\t\tdic=dic.copy()\n\t\tdic['__parent__'] = dic\n\t\tdic=dic[keys[0]]\n\tfor key in keys[1:]:\n\t\tdic=dict_insert(dic, val, key, *keys)\n\treturn dic\n\ndef dict_find"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tkeys = list(keys)\n\n\tkeys = values_chain(keys)\n\n\t#if it is the key.key1.key2, make new key.key1.key2.key2 and key.key1.key2.key1\n\tif key in dic:\n\t\tif dic[key].__dict_iter__ == keys:\n\t\t\t#set it as a dictionary key\n\t\t\tdict_del(dic, key)\n\t\t\tdic[key] = val\n\n\t\t\t#get_depth=list(set(dic[key].keys()))\n\t\t\t"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tini_type = {\n\t\t\t'file_name': 'file', \n\t\t\t'path': 'path',\n\t\t\t'folder_tree': 'folder_tree',\n\t\t\t'files': 'files',\n\t\t}.get(spec_option,'files')\n\tif ini_type is not None:\n\t\treturn INI_TYPE_MAP[ini_type['path']](ini_type['file_name'], name = True).get(spec_option)\n\telif spec_option == \"folder_tree\":\n\t\treturn INI_TYPE_MAP[\"folder_tree"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tini_types = {}\n\tini_types_key = {}\n\tfor s in spec_option:\n\t\tini_types_key[\"ini_types.{}\".format(s)] = \"\"\n\tfor s in settings_dirs:\n\t\tif not os.path.exists(s):\n\t\t\tcontinue\n\t\tfnames = os.listdir(s)\n\t\tfor a1 in fnames:\n\t\t\tif stat.S_IXUSR & os.stat(a1).st_mode == 0:\n\t\t\t\tcontinue\n\t\t\tif re.match(r\"^"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tif spec_option == 0:\tvalue = 'List'\n\telif spec_option == 1:\tvalue = 'Create'\n\telif spec_option == 2:\tvalue = 'ListOfFile'\n\n\treturn value\t\n\ndef print_dictionary(config_file_name, dict_name):\n\t\"\"\"\n\tPrints out a dictionary that contains the keys in the dictionary with the \n\tfollowing format:\n\t\tfilename:list{key}_value\n\t\"\"\"\n\tfile_reader = open(config_file_name)\n\tfile_reader"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tini = Ini()\t\t\n\tini.set(settings_dirs[0], spec_option, ini_type)\n\tfor d in ini.list_of_file_names() :\n\t\tprint '%s -> %s %d:%s' % (settings_dirs[0], spec_option, ini.get_option('id'), d)\t\t\n\tini.close()\t\n\t\ndef ini_type(s):\t\t\n\treturn '%s\"\\n\\t%s' % (s.replace(\"[\",\"\"), '"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\tif not spec_option:\n\t\tspec_option = settings_dirs[1]\n\tif not spec_option:\n\t\tsettings_dirs = None\n\t\tspec_option = ''\n\tspec_option += settings_dirs[2]\n\treturn specs['INIFile'].IniTree(settings_dirs).\\\n\t\tgetSection(spec_option + '_file_list')\n#\n#\n# =============================================================================\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\treturn make_ini_class(spec_option)\n\n\n\tdef make_ini_class(spec_option):\t\"\"\"\n\t\tCreate a new IniType complex type\t\"\"\"\n\timport iniparse.core\n\tini_dict=self.get_ini_class_dict(spec_option)\n\t\n\t# build the ini_class\n\tini_class=IniType()\n\tini_class.name=str(ini_dict['ini_class_name'])\n\tini_class.file_names=str(ini_dict['ini_class"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tif spec_option == \"list\":\n\t    # create dictionary based on the spec_option\n\t    ini = dict()\n\t    for ini_name in os.listdir(settings_dirs):\n\t        # get path for this file\n\t        # ini_path = os.path.join(\n\t        ini_path = os.path.join(settings_dirs, \n\t\t\t\t\t  re.sub(\n\t\t\t\t\"^[^\\\\.]*$.\", \n\t\t\t\t\"\\\\1.\", \n\t\t\t\tre.sub(\n\t\t\t\t\"^\", \"\\\\"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\tinifile = '{0}{1}/inifilestore.xml'.format(settings_dirs,'')\n\tlog.info('creating new IniFile {0}'.format(inifile))\n\t\n\treturn IniType(inifile)\t\n\t\n\n\n#===============================================================================\n\n\n# If no arguments provided, exit\t\nif len(sys.argv) < 2:\n\tprint 'Arguments not provided. Exiting with code'\n\tsys.exit()\n\ndef get_settings_dir_names(args):"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tglobal options\n\t\n\t# build the complete path to the config file\n\tconfigdir = globals.configurationDirectory\n\tconfigfile = os.path.join(configdir, \"inipy/%s\" % spec_option.lower())\n\tif not os.path.exists(configfile):\n\t\traise IniError, \"Could not find '<%s>' in paths\" % spec_option\n\n\tfull_path_fmt = \"<{0}@{1}>\".format(spec_option, spec_option)\n\tini_type"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tfile_names = {}\n\t\n\tfor spec_key, spec_value in settings_dirs.items():\n\t\tif spec_key == spec_option: continue\n \n\t\tfor filename in spec_value.items():\n\t\t\tfile_name = spec_key + \"/\" + filename\n\t\t\tif not os.path.isfile(file_name): continue\n\t\t\tfile_names[file_name] = True\n\t\n\treturn list(file_names) # Return list of IniType objects\n\t\t\n\t\n\n \n\n\t\t\n\n\ndef read_settings_file("}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tif hasattr(cls, '_ansible_config_manager'):\n\t\treturn cls._ansible_config_manager()\n\t# noinspection PyClassHasNoCtor\n\tdef __init__(self):\n\t\tcls._ansible_config_manager()\n\t\treturn\n\tcls.__init__ = __init__\n\n\tdef build_config_string(self, section, item, data=None):\n\t\tif item in self.CNF.items('ansible_ini'):\n\t\t\t# If an initialise the section (if not already"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn config_manager_cg(**cls.__dict__)\n\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls.__manager\n\nclass ModuleManager(object):\n\t\t\n\tdef __init__(self, *args, **kwargs):\n\t\tself.module_args = kwargs.pop('module_args', None)\n\t\tself.module = None\n\t\tself.path = None\n\t\tself.manager = cls(*args, **kwargs)\n\t\n\tdef __getattr__(self, name):\n\t\tif name == \"name\":\n\t\t\treturn self.module_args.get('name')\n\t\tif self.module is not None:\n\t\t\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tprint 'getting module', cls, 'an instance of ansible config manager'\n\ttry:\n\t\tansible_ansible_config_manager = cls(class_name='ansible.plugins.config.manager.AnsibleConfigManager')\n\texcept Exception, e:\n\t\traise AnsibleImportError(\"Unable to get config manager: %s\" %  e)\n\n\treturn ansible_ansible_config_manager\t\n\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls.load(p, load_path=cls.__file__)\n\n@classmethod\ndef load_config_file(cls):\n\t\"\"\"\n\tLoads a file from the ansible configuration manager\n\t\"\"\"\n\tconfig_manager = AnsibleConfigManager()\n\tconfig_manager.load(filepath=cls.__file__)\n\treturn config_manager\n\n\ndef is_file_valid(file_path):\n\t\"\"\"\n\tTests the existence of a valid file, or raises AnsibleError if there is an error.\n\n\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls('ansible-config', cls=True)\n\t\ndef get_playbook_dir(**kwargs):\n\ttry:\n\t\treturn playbooks.get_playbook(**kwargs)\n\texcept:\n\t\treturn 'anadb'\ndef get_config(cls, **kwargs):\n\timport ansible_ssh_to_host\n\timport ansible_config_manager\n\treturn ansible.AnsibleConfigManager.anadb(**kwargs)\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls.__instance\n\t\n\t\ndef load_config_manager():\n\t\"\"\"\n\tCreates or retrieves an ansible config manager object. \n\t\"\"\"\n\tcls = instance_manager()\t\t\n\tif cls == None: \n\t\traise AnsibleLookupError(gettext('Could not find ansible config manager'))\n\tif not isinstance(cls, type):\n\t\tcls = type(cls)\n\treturn cls\n\t\ndef get_cluster_type(cluster):\n\t\"\"\"\n\tGets the"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tdef __get_ansible_config_manager(cls):\n\t\t# Check if the module has been loaded\n\t\tif not hasattr(cls, 'ansible_config_manager'):\n\t\t\tcls.load_cfg()\n\t\t\t# Load the module\n\t\t\tcls.ansible_config_manager = AnsibleConfigManager()\n\t\t\treturn cls.ansible_config_manager\n\t\telse:\n\t\t\treturn cls.ansible_config_manager\n\n\tdef load_cfg():\n\t\tcls.load_ansible_conf(cls.ansible_"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls.__manager\n\n\n\nclass TestMongodb(ModuleTestCase):\n\t\n\t\"\"\"\n\tThe mongodb module tests\n\t\"\"\"\n\tdef test_mongo(self):\n\t\t\"\"\"\n\t\tRun all database tests using the mongodb\n\t\t\"\"\"\n\t\tcls.mongo = C.__mongo\n\t\tcls.test_database = C.__database\n\t\t\n\t\ttry:\n\t\t\tcls.run_tests()\n\t\texcept Exception as e:\n\t\t\tself.fail(str(e)) \n\t\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn __salt__['config.option_merge']\n\n\tconfig_manager('ansible_config_manager',\n\t\t\t\tstate='include',\n\t\t\t\tdescription='config manager used to do Ansible config management')\n\ndef ansible_file(cls):\t\"\"\"\n\tGets the ansible ansible.\t\"\"\"\n\treturn __salt__['file.files_glob']('ansible*')\n\n\tfile('ansible.cfg', 'w')\n\t# file('ansible.cfg', 'w',\n\t#    encoding='utf-8')\n"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls.get_manager(cls)\n\t\n\t\n\n@attr.s\nclass WorkspaceManager(object):\t\t\n\t\"\"\"\n\tWorkspace manager singleton used by all workspace managers in the\n\tplugin's tree.\n\t\n\tThe workspace manager instance is not necessary to be instantiated.\t\n\t\"\"\"\n\t__instance = None\n\t\n\t\n\tdef __init__(self, parent_workspace):\n\t\t\"\"\"\n\t\tCreates the workspace manager instance.\n\t\t\n\t\t:param parent_workspace: the parent workspace\n\t\t:type parent"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn _c.workspace_manager(unsafe_str(cls.this))\n\ndef get_data(param0):\t\"\"\"\n\tGets the data.\t\n\t\n\tReturns:\n\tstring:\tThe data\n\tinteger:\tThe number of items\n\t\"\"\"\n\treturn _c.get_data(param0)\n\ndef set_data(param0, param1):\t\"\"\"\n\tSets the data.\t\n\t\n\tReturns:\n\tint:\tThe number of values\n\t\"\"\"\n\treturn _c.set_data("}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tcls.__workspace_manager = cls._workspace_manager\n\treturn cls.__workspace_manager\n\n@property\ndef _workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager.\tDoes not return anything.\n\t\"\"\"\n\treturn cls._workspace_manager\n\n\n# TODO: get rid of this class and its default __new__ signature for 3 or more values, and have that override\n# the default __class__.\nclass ValueObject(object):\n\t\"\"\"\n\tA generic object, useful as the root"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tcls.rsp.sendMime(\"/workspace/workspace_manager\", cls.rootdir)\n\tm = cls.rsp.readAllStandardOutput()\n\treturn m\n\ndef get_ws_for_client(cl, username):\n\t\"\"\"\n\tReturns a WS object associated with the client\n\t\"\"\"\n\tcls = cls.ws_cls\n\tcl_name = cls.__name__\n\tc = cl.wsObjects.first()\n\twhile c is not None:\n\t\tif c.get_client():"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn weakref.ref(cls._instance)\n\ndef create_workspace_manager(cls):\t\"\"\"\n\tCreates a shared workspace manager.\t\"\"\"\n\tif not cls._instance:\n\t\tcls._instance = cls()\n\treturn cls._instance\t\n"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn _workspace_manager.instance()\n\ndef workspace_manager(*args):\n\t\"\"\"\n\tGets the workspace manager, with arg 0 being the name of the workspace.\n\t\n\targs \n\t        All arguments are passed to the workspace.\n\t\n\tReturns\n\t-------\n\twsManager : workspace_manager\t\n\t\n\tRaises\n\t------\n\tAttributeError : When arg 0 is not a workspace.\n\t\"\"\"\n\tif len(args) >  0:\n\t\treturn getattr(_workspace_manager.instance(), args[0])\n\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tcls.workspace_manager = cls()\n\tcls.workspace_manager.logger = logger\n\t\n\tcls.workspace_manager.set_workspace(\n\t\tconfig = config,\n\t\tworkspace_manager_list = workspace_manager_list\n\t)\n\t\n\tcls.workspace_manager.start()\n\treturn cls.workspace_manager\n\t\n\t\n\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tcls.__WorkspaceManager = ClassVar(WorkspaceManager(cls))\n\treturn cls.__WorkspaceManager.get_instance()\n\n\t\nclass WorkspaceManager(object):\n\t\"\"\"\n\tGets the instance of the workspace manager.\n\t\"\"\"\n\t__instance = False\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreates the workspace manager instance.\n\t\t\"\"\"\n\t\tself.__instance = True\n\t\t\n\tdef get_instance(self):\n\t\t\"\"\"\n\t\tGets the instance of the workspace manager.\n\t\tReturns"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls.mgr_c.get_property(\"WorkspaceManager\", cls.mgr)\n\t\n@property_context()\ndef workspace_manager_context(cls):\t\"\"\"\n\tReturns the current workspace manager implementation context.\t\"\"\"\n\treturn cls.mgr_c.get_property(\"WorkspaceManagerContext\", cls.mgr)\n\n@property_context()\ndef session(cls):\t\"\"\"\n\tReturns the current session.\t\"\"\"\n\treturn cls.mgr_c.get_property(\"Session\", cls.m"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls.__dict__[cls._key]()\n\n\t\n"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tcls.plugins = list(plugins.get_plugin_classes())\n\n\t# Set plugin manager's default plugins.\n\tcls.plugins.sort()\n\tcls.plugins.insert(0, pls.PluginPlugin())\n\tcls.plugins.insert(1, pls.PluginGroup())\n\tcls.plugins.insert(2, pls.PluginItem())\n\tcls.plugins.append(pls.DynamicPlugin())\n\n\treturn cls.plugins\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls.__class__.__dict__['plugins_manager']\n\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls()\n\n\n@pluginmanager.register\ndef pluginmanager():\n\t\"\"\"\n\tRegister the plugin manager.\n\n\tNote: A plugin manager will *always* be available!\n\n\t@return: Plugin manager instance\n\t@rtype: plugins.manager.pluginmanager.PluginManager\n\t\"\"\"\n\treturn plugins.manager.pluginmanager(_pluginmanager, _plugins, plugins_manager)\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn getattr(cls, \"get_plugin_manager\", None)\nclass PluginManager(cls, plugins_manager):\n\t\"\"\"\n\tThe manager for this package.\t\"\"\"\n\tdef __init__(self, plugindir):\n\t\tplugins_manager.__init__(self)\n\t\tself.plugindir = plugindir\n\t\tself.plugins = dict()\n\t\n\tdef __getitem__(self, name):\n\t\tret = self.plugins[name]\n\t\tif not isinstance(ret, Plugin):\n\t\t\tself.plugins[name] = Plugin("}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn _system.loadModule('Plugins/Manager')\n\ndef plugins(cls):\t\"\"\"\t\"\"\"\n\treturn cls.findPlugin('Plugins')\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn plugins_manager()[0]\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls.get_setting(\"plugins_manager\", \"plugin_manager\")\n\n\n@override(\"plugins.manager\", method_args=[\"hostname\"], methods=[\"GET\", \"POST\"])\ndef load_manager_settings(cls):\t\"\"\"Load the manager settings page.\"\"\"\n\tsuper(cls, plugins_manager).load_manager_settings(cls.plugins_manager, cls)\n\tcls.settings.setdefault(\"plugins\", {})\n\tcls.settings.setdefault(\"host\", True)\n\n# Plugin loading\n@override"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls\n\ndef get_plugin(obj):\n\t\"\"\"\tGets a plugin from the active plugin manager.\n\t\n\tReturns: (instance) If the object is not currently a plugin object returns the object itself.\n\t\"\"\"\n\tglobal active_plugin\n\tif obj is None:\treturn None\n\tif isinstance(obj, PluginManager):\n\t\treturn obj\n\t\n\tif cls is not None:\n\t\tplugin = getattr(active_plugin._plugins_mgr, cls.__name__.lower())\n\t\tif plugin:\n\t\t\treturn plugin"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls._modules_dicts.get(pkg, None)\n\n\t\ndef plugin_add_module(module, plugin, path=None, parent=None):\n\t\"\"\"\n\tAdds the module to the list of plugin modules.\n\n\tReturns a pointer to the plugin module.\n\t\"\"\"\n\t\n\t\n\t# Check if the list already contain this module\n\tif module in cls._modules_dicts:\n\t\t\n\t\t# Already in the list - we are OK\n\t\treturn module\n\t\t\n\tplugin.loaded.add(pkg)\n\n\t# Module not yet"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls.__plugin_manager\n\n\n\t\ndef setup_class(cls):\n\t\"\"\"\tSetup the plugin\"\"\"\t\n\tprint \"Setup class: \"+plugin_name\n\tplugin_manager = get_plugin_manager(plugin_name)\n\tcls.__plugin_manager = plugin_manager\n\tprint \"Setup class done\"\n\n#\tprint plugin_manager.plugin_dict\t\n"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\t# load data\n\tdata = dict()\n\tfor name in cls.config.data:\n\t\t# set values\n\t\tvalue = getattr(cls.config,name)\n\t\tif value is None:\n\t\t\tdata[name] = \"NULL\"\n\t\telif value == \"\" or value==None:\n\t\t\tdata[name] = \"NULL\"\n\t\telse:\n\t\t\tdata[name] = \"\" + value\n\t\t\n\tif not cls.config._read_from_yaml(data):\n\t\traise IRValidatorException(\"Missing mandatory field in spec"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t# Check if one of mandatory values (YAML) is missing\n\tif 'YAML_TEMP_FILE' not in spec_content or not spec_content['YAML_TEMP_FILE']:\n\t\tcls.raise_validation_error('YAML_TEMP_FILE is missing')\n\n\ts1 = parse_and_match(spec_content, [ 'YAML_FILE' ])\n\n\t# If file doesn't exist\n\tif not s1:\n\t\tcls.raise_validation_error('File does not exist')\n\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\ttry:\n\t\tif not spec_content: \n\t\t\traise IRValidatorException(\"spec_content variable required. Check the code\")\n\n\texcept IRValidatorException:\n\t\tprint(\"\\n\\n******* Invalid spec content *******\")\n\t\tprint(\"\\n\\nError line 1: missing required data: {0}\".format(spec_content))\n\n\t# create a list to store data\n\tlist_data = [\"\", \"\"]\n\n\t# initialize with no data\n\tlist_data[:0] = [\"\"]\n\n\tresult = {}\n\n\t# loop through the"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content is None:\n\t\treturn {}\n\tvalid_fields = []\n\tall_fields = set()\n\tfor field in cls.fields.field:\n\t\tif field.data_type in ('file'):\n\t\t\tif field.data_load_func is not None:\n\t\t\t\tvalid_fields.append(field)\n\t\t\telse:\n\t\t\t\tall_fields.add(field.name)\n\tfor key in all_fields:\n\t\ttry:\n\t\t\tloaded_field = getattr(spec_content, key)\n\t\t\tif loaded_field is not None"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\ttry:\n\t\tif not spec_content or spec_content!= 'file':\n\t\t\traise IRValidatorException('spec must be file')\n\t\tfor f in cls.files:\n\t\t\tif not cls.file_content[f]:\n\t\t\t\traise IRValidatorException('{0} is missing data for {1}'.format('file', f))\n\t\treturn {'content': cls.file_content}\n\texcept Exception as e:\n\t\treturn {'content': str(e)}\n\n\t\n\t\ndef validate_content(content, module):\n\t\"\"\"\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif not cls.check_specs_file(spec_content):\n\t\traise IRValidatorException( 'validation_failed', spec_content, cls.name,\n\t\t\t\"The spec file doesn't seem to be properly formatted\" )\n\t\treturn {\"validations\":False,\n\t\t\t\"message\": \"Spec file format error\"}\n\tcls.data = {}\n\tcls.data[\"spec_content\"] = spec_content\n\treturn True\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\treturn self._validate(spec_content, 'from')\n\t\n\n\ndef is_factory_content(cls, spec_content=None):\t\"returns true if factory data from spec is present and valid\"\n\tvalidator = super(cls, cls).is_factory_content(spec_content)\n\n\tspec_content = super(cls, cls).load_spec(spec_content)\t\t\n\t\n\treturn validator and not spec_content.errors\n\n\t\t\t\t\t\t\t\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif not spec_content and not spec_content.startswith(\"@\"):\traise IRValidatorException(\"Spec file does not start with '@' [%s]\" % spec_content)\n\tif len(spec_content)!= 1024:\t\t\traise IRValidatorException(\"Spec file must not be larger than 1024 characters [%s]\" % spec_content\n\t\tif not len(spec_content) >= 1024:\traise IRValidatorException(\"Spec filename must not be longer than 1024 characters [%s]\" % spec_content)\n\tresult = {}\n\tfor key in [\""}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif not spec_content:\n\t\treturn {\"spec_content\": \"\",\n\t\t\t\t\t \"spec_base\": {\"value\": \"yaml\"}}\n\ttry:\n\t\tdoc = open(spec_content).read()\n\texcept IOError:\n\t\t# missing specs.xml\n\t\traise IRValidatorException( \"No valid specs.xml specified. Is it in path or directory?\")\n\tif not doc.startswith(\"###\"):\n\t\traise IRValidatorException(\"Spec file not formatted correctly. Check your specs.xml file\")\n\n\t# collect everything in the yaml structure"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tassert not is_empty_content(spec_content), 'A specification file must not be empty'\n\td = lre_validator(spec_content, spec_name=cls.name, description=cls.description)\n\td['from yaml'] = ''\n\tfor f in d['from']:\td.update(lre_validator(f, spec_name=cls.name, description=''))\n\treturn d\n\n\nclass IRepository(models.Model):\n\t\"\"\"Repository model\"\"\"\n\tname = models.CharField("}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif yaml_file is not None:\t\n\t\tcls.yaml_file = open(yaml_file)\n\telse:\tcls.yaml_file = None\n\treturn cls.data\n\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tresult = dict()\n\tif isinstance(yaml_file, basestring):\n\t\twith open(yaml_file) as f:\n\t\t\tresult = yaml.load(f)\n\telif isinstance(yaml_file, pytree.Node):\n\t\tresult= yaml.load(yaml_file)\n\telse:\n\t\traise IRValidatorException(\n\t\t\t\t\"Incorrect parameter type(s) for 'from_file': {}\".format(\n\t\t\t\t\ttype(yaml_file)))\n\treturn result"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\n\ttry:\n\t\tif yaml_file is None:\n\t\t\traise Exception(\"Requires a YAML file as argument\")\n\texcept Exception as e:\n\t\traise IRValidatorException(e)\n\t\n\tresults = {}\n\twith yaml_file as f:\n\t\td = f.read()\n\t\n\tif not d:\n\t\traise IRValidatorException(\"No YAML file found\")\n\t\n\ttry:\n\t\t\n\t\tresults = dict([ (x,y) for x,y in yaml_file.iter_objects()])\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif not os.path.exists(yaml_file):\n\t\traise IRValidatorException(\"Missing yaml file: {}\" % yaml_file)\t\n\t\t# print(\"Missing YAML file: {}\". format(yaml_file))\t\n\t\t\n\tif len(glob(yaml_file)) > 1:\n\t\tparser_instance = yaml.load(yaml_file)\n\t\tfor missing_element in parser_instance['missing_fields']:\n\t\t\tfor error in sorted(parser_instance['missing_elements']):\n\t\t\t\tif missing"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif yaml_file is None:\n\t\tyaml_file = cls.YAML_FILE\n\tdata = cls.load_file(yaml_file)\n\tif'metadata' in data:\n\t\tmetadata = data['metadata']\n\telse:\n\t\tmetadata = {}\n\terrors = []\n\ttry:\n\t\tcls.validate(metadata, data, errors)\n\t\tcls.validate_to_file(cls.PATHS.CWD_TO_YAML1, yaml_file)\n\texcept IRValidator"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tyaml_file = os.path.abspath(yaml_file)\n\t\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"{0} is not a valid file\".format(yaml_file))\n\n\td = { \"name\" : \"\", \"description\" : \"\", \"tags\" : [] }\n\td.update(yaml.load(open(yaml_file),\"tree\"))\n\t#d.update = d.update(d.get('name'))\n\tfor k in (\"name"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tvalid_file = yaml_file or cls.config.get(\"validation.yaml-file\")\n\tvalid_file = valid_file or os.path.join(cls.config.get(\"validation.yaml-path\"), \"validation.yaml\")\n\t\n\tif not os.path.exists(valid_file):\n\t\tmsg = \"Cannot find 'yaml' file in 'files' module!\"\n\t\traise IRValidatorException(msg)\n\n\ttry:\n\t\tvalid = cls.validate(os.path.join("}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tfrom yaml import assert_strict_yaml as valid\n\tvalid = valid.get_valid(yaml_file=yaml_file)\n\tvalid.assert_required(\n\t\trequired='missing'\n\t)\n\tvalid.assert_values(\n\t\tproperties=['name','state', 'date', 'type'], \n\t\tmissing='missing'\n\t)\n\tvalid.validate_with_datetime(\n\t\tfields='date')\n\treturn valid\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif not yaml_file:\n\t\treturn {}\t\n\n\n\t# Load the YAML object\n\tyaml = yaml.load(yaml_file)\n\n\t# Check the YAML object\n\tif not yaml.get(\"data\", None):\n\t\traise IRValidatorException(\"No data in 'data' section\")\n\n\t# Extract data\n\ttry:\n\t\t# Validate mandatory data elements\n\t\tdata = yaml.pop(\"data\", None)\n\n\t\t# Validate mandatory data names\n\t\tif not yaml.get(\"name\", None) in data"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\ttry:\n\t\tif not yaml_file:\n\t\t\tfrom yaml.util import validator\n\t\t\tvalidator.validate_from_file(cls.MODULE, yaml_file=yaml_file)\n\t\t\treturn cls.MODULE.YAML_PARSER.load(yaml_file=yaml_file)\n\t\telse:\n\t\t\tfrom yaml.loader import Loader\n\t\t\tdef validator_factory(f):\n\t\t\t\treturn Loader(YAML_PARSER, load_yaml=False)\n\t\t\tLoader.register_"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tnew_includes = {}\n\tfor section in self.spec.itertools('include', 'group'):\n\t\tif section['include'] or section['group']:\n\t\t\tnew_includes[section['name']] = [include for include in section['include']] + [include for include in section['group']\n\t\telse:\n\t\t\tnew_includes['default'] = [include for include in section['include']] \n\t\t\n\treturn new_includes\n\n\nclass OptionParser(optparse.OptionParser):\n\tdef optparse(self, option_list):\n\t\tif"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tinclude_dict= self.dummy.get_reserved('include_dict', parser_dict)\n\n\tif (len(include_dict) == 0) or (not_include_dict(include_dict)):\n\t\treturn True\n\telse:\n\t\treturn False\n# ============================================================================\n"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tgroup_dir = self.args.get(\"output_group_dir\")\n\tif parser_dict.has_key(group_dir):\n\t    parser_dict[group_dir].append(parser_dict[group_dir].keywords)\n\ndef _include_dirs(self, parser_dict):\t\"\"\"\t\"\"\"\n\tResolves the include dir directive in the spec files.\t\"\"\"\n\tdir_list = self.args.get(\"output_include_dir\", self.args.get(\"include_depends\"))\n\tfor dir in dir_list:\n"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tgroup_dict = {}\t# { group: group_include }, see spec.py\n\n\tfor name, key in parser_dict.items():\n\t\tif key['include'] is True:\n\t\t\tgroup_dict[key['group']] = { 'include': True }\n\n\treturn group_dict\n\ndef _include_group(group, group_parser):\n\t\"\"\"\n\tResolves the group directive in the spec file.\n\tGroup directives start at the start of the path, so the\n\targument may be None.\n\tGroup directives are terminated by a semicolon.\n"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif self.root:\n\t\treturn\n\tparser_dict[self.root] = dict()\n\tinclude_groups = self.parser.include_groups if self.parser else [\"<config>\"]\n\tfor group in parser_dict[self.root].keys():\n\t\tif group in include_groups:\n\t\t\tparser_dict[self.root][group] = parser_dict[self.root][group]._include_groups(parser_dict)\n\t# if not ('config' in parser_dict and parser_dict['config'] in include_groups):\n\t#\t"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser = parser_dict.get('parser')\n\n\tif not parser:\n\t\tif parser_dict.get('files'):\n\t\t\tparser = parser_dict.get('files')\n\t\telse:\n\t\t\tparser = self\n\telif isinstance(parser, basestring):\n\t\tparser = self.lookupDependency(parser)\n\tif isinstance(parser, basestring):\tparser = self.lookupDependency(parser)\n\n\tincludeFile = parser.get('include_file')\n\tif includeFile and includeFile.strip():\n\t\tinclude"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tfor group_name in self.include:\n\t\tparser_dict[group_name] = []\n\t\tgroup_dict = self.read_group(group_name)\n\n\t\tfor dir_name in group_dict:\n\t\t\t# If the group exists,\n\t\t\tif dir_name not in parser_dict:\n\t\t\t\tparser_dict[dir_name] = group_dict[dir_name]\n\t\t\t# If the group does not exist,\n\t\t\telse:\n\t\t\t\tparser_dict[dir_name].extend(group_dict[dir_name])\n\t\t\n\n\treturn parser"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tinclude = parser_dict['include']\n\tfor group in include.get(INCLUDE_GROUPS):\n\n\t\tdef _include(groupname=group):\n\t\t\tsubparser = parser_dict['include_%s' % group]\n\t\t\tsubparser.add_argument('--include-%s' % group, action=SubparserAction()(include), default=subparser)\n\n\t\t_include(group)\n\n\t# add the include-group directives to the include dict\n\tfor group in include.get(INCLUDE_GROUP):\n\t\tparser_dict['"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif 'include' in parser_dict:\n\t\ttry:\n\t\t\tself.global_dict['include'] = \\\n\t\t\t\tparser_dict['include']['include']\n\t\t\tparser_dict['include'] = self.parse_list(parser_dict['include']['include'], include_prefixes=[\"'include'\"])\n\t\texcept Exception, e:\n\t\t\tprint e\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n\nparser = parse_conf('include', _include_groups, default_config=self.global_dict,\\\n\t\t\tinclude_path"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tfor include in parser_dict['includes']:\n\t\tinclude['dir'] = parser_dict['dir'] + include['dir']\n\t\tinclude['dir'] = parser_dict['include_dir'] + '|' + include['dir']\n\t\tinclude['dir'] = parser_dict['dir'] + include['dir']\n\t\tinclude['dir'] += '/'\n\n\ndef _find_includes_args(self, include_dir, spec):\n\t\"\"\"\n\tTries to find includes arguments in the spec file.\tIf includes are not\n\tincluded, or if the spec file"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tlog.dbg(self.s)\n\tif not self.n_args:\n\t\tself.n_args = - self.n_args\n\t\tself.args     = \"\"\n\tself.out        = self.output_spec.parent.one_off_all\n\tself.overlap    = self.overlap_spec\n\tself.args      = \"\"\n\tself.name      = self.task_name\n\tself.overlap.defaults(self)\n\t\n\tif self.in1 is None:\n\t\tself.in1 = self."}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\targ_defs = [{ 'kind' : \tself.arg_KIND, \t\t\t'names' : ['nofind']\t\t   },\n\t\t\t\t   \t{ 'kind' :\t   \t'arg',\t\t\t\t\t\t'label' : self.arg_LABEL,\t'examples' : self.arg_EXAMPLES }, ]\n\tfor k in self.arg_KIND:\n\t\targ_defs.append({ 'kind':\tself.ARG_KIND[k], 'names':\tself.ARG_NAMES[k] })\n"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tdefaults = self.spec.get('defaults')\n\tif not defaults:\n\t\tdefaults = {}\n\t\tif hasattr(defaults, '__call__'):\n\t\t\targs = self.build_args(*defaults)\n\t\t\tsetattr(self, 'add_defaults', args)\n\n\treturn defaults\n\ndef get_spec_validation(defaults):\n\t\"\"\"\n\tGet a tuple with only sensible arguments for the 'validate' method:\n\t* key must be a string\n\t* key values are strings\n\t* must be a callable\n"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tspecs = {\t\n\t\t'base': 'Base URI',\n\t\t'base_root': 'Base root URI',\n\t\t'path': 'Base path',\n\t\t'default_path': 'Default path',\n\t\t'query_params': 'URL query params',\n\t\t'anchor': 'Spec'\n\t}\n\ts = self.get_argparser().parse_args()\t\n\treturn specs.get(s.spec, s.basepath+s.defbase)\n\n\ndef get_argument_parser(**kwargs):\n\t\"\"\"\n\tGet"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tself.base_dir = args.cwd+self.path[0] #path to the directory \n\tself.path = args.cwd+self.path #path to any specified file or folder.\n\tself.url = args.cwd+self.path[0] #url to a specified file or folder.\n\n\tif not os.path.isfile(self.path): #if the file specified by path is not a file, resolve it.\n\t\t#get path relative to current directory and resolve it again.\n\t\tfor i in range(0, len(self"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tresult=ParameterRegistry() #create the result\n\t# Get the default value (arguments should have the same length)\t\n\tfor arg in range(len(self.arguments_default)):\n\t\tresult.add_value(self.arguments_default[arg],self.args[arg])\t\n\t# Get any values from the specification\t\n\tfor arg in self.args.keys():\n\t\tif arg in args_to_be_included: #add argument\n\t\t\tresult.add_value(self.args[arg],self.args[arg])\t\n"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t# resolve arguments\n\tself._spec.args = self._spec.extras or []\n\t# resolve command line arguments\n\tself._args = self._args or []\n\t# resolve additional flags\n\tself._flags = self._flags or []\n\t# set command line flags\n\tself._spec.flags.extend(self._flags)\n\n\t# set the spec on myself if there are args to supply\n\tif (len(self._spec.args) > 0):\n\t\treturn self._spec\n\n\treturn self._spec\n\n"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tif self.arguments._args:\n\t\treturn\t{self._argdefault: self.arguments._args[0]}\n\telse:\n\t\treturn {}\n\n\ndef run(filename, args, kwargs):\n\t\"\"\"\n\tRuns the code, returning a return value, a (possibly empty) list of\n\timmediately evaluated and failed expressions (returned by eval()).\n\t\"\"\"\n\t# Get the output from `file`.\n\tfout = open(filename, 'r')\n\t# The result from executing `file` is the output from `file``,"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tfor k in self._keys:\n\t\tk = self._namespace + k\n\t\tif getattr(self, k) is None:\n\t\t\traise NameError, \"Spec's value for '%s' missing\" % k\n\t\tsetattr(self, k, get_attr(self.__dict__.get(k, None),'spec'),\n\t\t\t\t\t\tself)\n\t\t\t\t\t\n\treturn self\n"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tif has_spec_arg(self.params_type) and not has_spec_arg(self.arg):\t# noqa: E501\n\t\treturn self.get_arg(self.param_keys, [self.params_type.SPEC])\n\tif has_param_arg(self.params_type) and not has_param_arg(self.arg):\t# noqa: E501\n\t\treturn self.get_arg(self.param_keys, [self.params_type.PARAM])\n\tif has_param_list_arg(self."}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdeprecated_str = {\n        'list_deployments': 'Get list of deployments in project',\n        'is_list': 'Is list of deployments',\n        }\n\treturn deprecated_str\n\t\n\nclass ListDeploymentsCommand(BaseCommand):\t\n\t\"\"\"\n\tList the deployments of a project for a given project\n\t\"\"\"\n\tdef get_description(self):\n\t\t\"\"\"\n\t\tDescription of list_deployments.\n\t\t\"\"\"\n\t\treturn 'List deployments'\n\n\tdef get_help(self):\n\t\t\"\"\"\n\t\tHelp of list_de"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\targs = super():\t\n\t\t\t\t\n\treturn args\n\n\n"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdeprecations = get_deprecated_options(self.config)\t\n\treturn deprecations\n\ndef get_option_path(config, option):\n\t\"\"\"\n\n\tReturn path(string), to given config option.\n\t\"\"\"\n\treturn config.substitute(option)\n\ndef get_option_values(config, option):\n\t\"\"\"\n\n\tReturn tuples which contains key and value of given config option.\n\t\"\"\"\n\treturn config.substitute(option).split(',')\n\ndef get_deprecated_options(config):\n\t\"\"\""}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\targs = {}\n\tdef remove(name,value,deprecated=False):\n\t\tif not deprecated and name in self.args.keys():\n\t\t\targs[name]=value\n\tif hasattr(self,\"deprecated_args\"):\n\t\tremove(*self.deprecated_args.keys(),False)\n\t\twith self.deprecated_args as vars_and_values:\n\t\t\t\tfor key in vars_and_values:\n\t\t\t\t\tif key in self.base_args:\n\t\t\t\t\t\tdel self.base_args[key]\n\treturn args\n\ndef default_get"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t# Default depends on the version of the Python library\n\tdeprecated_args = {}\n\n\t# Get the major version of the Python library\n\tpython_version = self.py_version\n\tpython_version = tuple(map(int, [python_version] + sys.version_info[:2]))\n\n\t# If it is a package we have to see the deps of the package\n\t# and the build path (it can be None, so we don't want a dict)\n\tif sys.version_info[0] == 0:\n\t\ttry:\n\t\t\tdep"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn {\"verbose_deprecations\": True}\n\n\nclass CommandSet(BaseCmd):\n\tdef _execute(self):\n\t\t\"\"\"\n\t\tCommand that is called when command execution is complete.\n\t\t\"\"\"\n\t\tprint(\"Called.\")\n\n\nclass Command(BaseCmd):\n\tdef _execute(self):\n\t\t\"\"\"\n\t\tCommand that is called when command execution is complete.\n\t\t\"\"\"\n\t\tprint(\"Called.\")\n\t\tos.system(\"sudo sed -i's/^#define __has_cray__ /!defined __has_cray__/' /"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\ttry:\n\t    return self._deprecated_args\n\t   except:\n\t\tself._deprecated_args={}\n\t   return self._deprecated_args\n\n\n\tdef get_params(self,**params):\n\t   for param in params.keys():\n\t\tself.logmsg(\"param: %s\" % param)\n\t\tif (not self._deprecated_args.has_key(param)):\n\t\t   self.logmsg(\"param not deprecated\")\n\t\t   self._deprecated_args[param]= params[param]\n\n\tdef get_kw"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tresult = dict()\n\tfor key in self.__get_dict().keys():\n\t\tkwargs = dict()\n\t\tfor name, value in self.__get_dict()[key].items():\n\t\t\tif type(value) == type([]):\n\t\t\t\tvalue = value[0]\n\t\t\tkwargs[name] = value\n\t\tresult.update(kwargs)\n\treturn result;\n\n\nclass PluginList:\n\t\"\"\"\n\t\tA list of plugins which have been registered with this class.\n\t\"\"\"\t\n\tdef __init__(self, **kwargs):\n"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdict = {}\n\t# dict[\"deprecated\"] = {\"description\": \"option for deprecated\", \"opt_deprecation\": ((\"False\", \"False\")), \"opt_default_deprecation\": ((\"False\", \"False\"))}\n\tdict[\"deprecated\"] = dict(\n\t\tdescription=\"option for deprecated\", opt_deprecation=((\"False\", \"False\")), opt_default_deprecation=((\"False\", \"False\")),\n\t\topt_deprecation_status='deprecated'\n\t\t)\n\n\treturn dict"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn { \n\t\t'use_deprecated(**options)':\tself.use_deprecated() \n\t}\n\n"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tif (cli_args.get('verbose_mode')!= None) and (cli_args.get('verbose_mode')!= False):\n\n\t\tif cli_args['verbose_mode'] == True:\t\n\t\t\t# validate_verbose_mode(cli_args, answer_file_args['verbose_mode'])\n\t\telse:\n\n\t\t\t# validate_verbose_mode(cli_args, False)\n\t\t\t# print('Warning, no verbose mode was set for this command')\n\ndef validate_deprecation(cli_args, answer_file"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\n\tfor command in answer_file_args[\"deprecated\"]:\n\t\tif not os.path.exists(command + \".conf\"):\n\t\t\tprint(\"Invalid answer_file argument -- cannot find --deprecation-file=\\\"%s\\\"\" % command)\n\t\t\texit(1)\n\n\t\twith open(answer_file_args[\"deprecation_file\"] + \".conf\", \"r\") as fd:\n\t\t\tconf = ConfigParser.ConfigParser()\n\t\t\tconf.readfp(fd)\n\t\t\tfor variable in conf.options(command):\n\t\t\t\tif not variable in"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tfor arg in answer_file_args:\n\t\tif not isinstance(cli_args, cli_help.CLIArgumentDict):\n\t\t\traise Exception('%s must be of type %s'%(error_str, cli_help.CLIArgumentDict))\n\t\tif not CLIArgumentDict.is_valid_for_cli(getattr(cli_args, arg)):\n\t\t\traise Exception('expected to be able to process argument on the command line, but instead it was %s' % str(getattr(cli_args"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\timport clint.textui as tui\n\tanswer = tui.input_text('<', tui.input_title('validating args (deprecated)','Warning: this argument will be removed in favor \t\t  of passing in the input text. Do not use it until all tests pass ',' 'and then the code will re-validate the arguments!'), help=False)\n\tif answer!= 'exit' and not answer.startswith('exit'):\n\t\tprint('< ')\n\t\tprint_deprecation(cli_args, 'deprecated arguments ("}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\n\tif '--yes' in cli_args:\n\t\tanswer = True\n\telse:\n\t\tanswer = False\n\t\t\n\targ_deprecated = cli_args['-d']\n\told_arg1 = None\t\n\told_arg2 = None\n\tif arg_deprecated:\n\t\t#old_arg1 has the old value, old_arg2 must have the new value\n\t\tif not self.arg1 or not self.arg2:\n\t\t\traise Exception('you need either an argument --d or a list of arguments')\n\t\tfor key in"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\targs = {}\n\tfor arg in cli_args:\n\t\tif os.path.isfile(os.path.expanduser(answer_file_args[arg])) and not (os.path.isfile(os.path.expanduser(answer_file_args[arg])) and arg not in cli_args):\n\t\t\tvalue = self.get_default(answer_file_args[arg])\n\t\t\tif value is not None:\n\t\t\t\targs[arg] = value\n\t\telse:\n\t\t\targs[arg] = self.get_default(answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tfor deprecation_arg in cli_args:\n\t\targ_in_file = f\"{deprecation_arg}=<{\n\t\t\t\tf\"{os.environ[\n\t\t\t\t\t\"__module__\"]}.\",\n\t\t\t\t}\".replace(\" \", \"\").replace(\"/\", \"\\\\\")}\"\n\t)\n\n\t\ttry:\n\t\t\twith open(answer_file_args[deprecation_arg][\"filename\"]) as f:\n\t\t\t\tanswer_output = f.read()\n\n\t\texcept:\n\t\t\tprint line_map[1], \"Error:\", arg_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tif '-h' in cli_args and not self.check_deprecated_args(cli_args):\n\t\tself.deprecated_args_printed = True\n\t\tself.display_deprecation_message(cli_args)\n\t\tself.print_deprecation_msg(answer_file_args)\n\t\tself.print_prompt()\n\telse:\n\t\tself.print_prompt()\n\treturn\n\n\ndef show_parser_commands():\n\t\"\"\"\n\tPrints a text version of all of the top level commands in this module.\n\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tpass\n\ndef get_answers(answer_file_args):\n\tanswer_files = get_dict_from_files(answer_file_args)\n\n\tfor answer in answer_files:\n\t\tanswer = get_arg(answers=answer_files, dict_type=dict, answer_filename=answer)\n\t\tprint(f\"answers: {answer['answer']}\")\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\n\tprint('')\n\tprint(cls('').format(**ANSWER_FILE_OPTIONS, **cls.DEFAULT_OPTIONS))\n"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn self.command_group.get_option_specs(command_name)\n\n\n\ndef run_command(command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\t\"\"\"\n\tcommands = CommandNameParser(command_name)\n\treturn commands.get_options()"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\topts = self.shellparser.parser_option_specs(command_name)\n\treturn opts\n\ndef get_parser_option_values(self, subcommand):\t\"\"\"\n\tGets the values of the specified command\n\n:param subcommand: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\topts = self.get_parser_option_specs(subcommand)\n\tif opts is None:\n\t\treturn []\n\treturn []\n\t\n\ndef help("}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tcommand_opts = ['--config-only']\n\t\n\tif command_name =='main':\n\t\tcommand_opts += list_common_options(['-c'])\n\telif command_name == 'virsh':\n\t\tcommand_opts += list_common_options(['-c', '--conf'] + extra_arguments('-F'))\n\telif command_name == 'opcdir':\n\t\tcommand_opts += list_common_options(['-c', '-o', 'opcdir:d'])\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn VSCONFIG._get_parser_option_specs(self, command_name)[:self.options_count+1]\n\ndef get_parser_arg_option(option):\n\t\"\"\"\n \tGets the parser option as a list of values\n \t\n\tThis is done internally by calling the\n\toption.get_parser_arg_value and passing in the option index\n\t\"\"\"\n\t# NOTE this function is too hacky. I did the wrong thing first to\n\t# get the right value and then add it here so this is not \n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tresult = []\n\tresult = set(v.name.split('_', 1)[0] for v in self.vars[\"all_commands\"])\n\tresult.remove(command_name)\n\treturn result"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn self.options\n\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn get_parser_option_specs(self._opts, command_name, self._possible_commands)\n\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn self._options('').items()\n\n# TODO: deprecate and remove as soon as we have more option specrs\ndef get_default_option_specs(command_name):\n\t\"\"\"gets the default command args for the command\"\"\"\n\treturn get_parser_option_specs(command_name)\n\ndef get_option_specs(option_list):\n\t\"\"\"Gets all command options in a list\"\"\"\n\treturn get_parser_option_specs('')\n\ndef get_default_option_specs(option_list):"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn self.config_file.get_parser_option_specs(command_name)\n\n  def add_option(self, key, default=True):\n    self.config_file.add_option(key, default)\n\n# vim:ts=4:sw=4:et\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn self._get_parser_command_option_specs(command_name)\n\n\n\t\n#\n# Command options and option flags\n# =============================================================================\ndef get_basic_options():\t\t\"\"\"\n\tCreates the basic options dict for the parser\t\n\n\t:return: a dictionnry of parser options\n\t\"\"\"\n\tparser_option_specs = {\n\t\t\"--help\": (False, '', _('print help')),\n\t\t\"--version\": (False, '', _('print version information')),\n\t\t\"--verbose-stdout-for-"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn [argument_name]\n\n\t\ndef get_option_argument_spec(command_name,\n\toption_name,\n\targuments_name = None,\n\tdescription_name = None):\n\treturn [\n    # -------------------------\n    \n    # -------------------------\n    \n    '<argument>'        : { 'name' : 'Argument name', 'help' : '<description>',\n                                'required': True, 'type': argtype, 'optional': True,\n            'alias': False },\n    \n    '<arg>'"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn None\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif self.spec is None:\n\t\treturn\n\n\tif argument_name == \"--\":\n\t\t# default\n\t\treturn self.spec\n\n\n\tif self.spec.get(argument_name):\n\t\treturn self.spec[argument_name]\n\n\n\traise OptionError(\"{} option already defined: {}\".format(command_name, argument_name))\n\n\ndef add_input_spec(self, *args):\n\t\"\"\"\n\tAdds an input spec\n\t:param args: arguments list\n\t:type args[]: string\n\t\"\"\"\n\tif args is None"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\n\t\t\"\"\"\n\t\tBuilds the option specification for the command named by\n\t\tthe name and argument, which is named as a string containing a list\n\t\tof strings, like this:\n\t\t\n\t\tOption command_name=\"do_something()\"\n\t\t\n\t\treturns\n\t\t\tthe option specification for 'command_name'\n\t\n\t\tCommand examples:\n\t\t\t$ echo > /foo/bar\n\t\t\t\techo \"I have the bar\"\n\t\t\t$ $ echo \"I would have the bar\"\n\t\t\"\"\"\n\t\n\t\t\"\"\"\n\t\tGet the fully-qualified path to the specified command."}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn self.option_descriptions.get(command_name, None)\n\t\t            \n\tdef set_option(self, argument_name, value):\n\t\tif value is None:\n\t\t\treturn\n\t\tself.options[argument_name] = value\n\t\tself.send_options()\n\n\tdef get_arguments(self):\n\t\tself.send_options()\n\t\treturn self.config.get('global', 'arguments', [])\n\n\tdef set_arguments(self, name):\n\t\tif name is None:\n\t\t\treturn\n\t\tif '='."}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\ttry:\n\t\toptnum = self.vars[argument_name]\n\telif argument_name in self.options:\n\t\toptnum = self.options[argument_name]\n\telse:\n\t\traise ValueError()\n\treturn self.__options[optnum][2]\n\n\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn get_opt_spec(self.module, command_name, argument_name)\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\n\t# FIXME: Handle the exception here\n\t\t\ndef process_command_arguments_args(args, d):\n\t\"\"\"\n\tProcess and invoke command arguments.\t\n\t\"\"\"\n\targs = args[:]\n\td = d[:]\t\n\treturn command_arguments(get_command_name(args))[:]+args+d\t\n\ndef run_command(args, command_args, exit_on_error=False):\n\t\"\"\"\n\tRun a command and return the exit status.\t\n\t\"\"\"\n\tcmd_arg_args = command"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\n\tself.__option_dict.setdefault(command_name, {})\n\tself.__option_dict[command_name][argument_name] = argument_name\n\n\t# XXX check for unknown options (like /verb)\n\n\n\tdef _get_options(self):\n\t\t\"\"\"Return the list of command object options.\"\"\"\n\t\treturn [item for item in self.__options.values()] + self.__option_dict.keys()\n\n\tdef _get_command_options(self, command):\n\t\t\"\"\"Return the list of command object options of a particular"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn _nfft.get_option_spec(self, command_name, argument_name)\n\n\f\n\nif __name__ == '__main__':\n\tprint \"Hello, World!\"\n\tnumpy.set_printoptions(max_lines_per_line=80, linewidth=30, precision=2,\n\t\t\tsuppress=True)\n"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilence_pattern = re.compile(\"silence[0-9][0-9]-[0-9]\")\n\treturn [l.group() for l in silence_pattern.findAll(args)]\n\ndef list_silence_args(args):\n\treturn [args[i] for i in range(len(args)) if args[i] =='silence']\n\ndef get_silent_pattern(args):\n\treturn re.compile(\"silence[0-9][0-9]-[0-9]\")\n\ndef is_"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn [key for key in args if key not in self.silenced[0]]\n\n\nclass Argument():\n\t\"\"\"\n\tBasic class for the arguments.\n\t\"\"\"\n\n\tdef __init__(self, silent_args, name, value):\n\t\t\"\"\"\n\t\t\"\"\"\n\n\t\tself.silenced = []\n\t\tself.value = value\n\t\tself.name = name\n\t\tself.silent_args = silent_args\n\n\tdef get_silenced_args(self, args):\n\t\t\"\"\"\n\t\tList of silenced argument\n\t\t:return: list, silenced argument names."}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\n\tsilenced = [(\"silence_frequency\",\"silence_frequency\"),(\"silence_length\",\"silence\")]\n\treturn silenced\n\n@py_config()\ndef get_silent_options(args):\t\"\"\"\n    list of silenced option\n\n:param args: The received options.\n:return: list, slied option names\t\"\"\"\n\t\n\tsilenced = [(\"silence_frequency\",\"silence_frequency\"),(\"silence_length\",\"silence\")]\n\treturn silenced\n\n@py_config()\ndef get_silent_time("}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn [ arg for arg in args if arg not in self.silent]"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn [arg for arg in args if not arg.contains_silent()]\n\n\ndef count_silent_args(self, args):\t\"\"\"\n\tReturns number of argument passed to this method\"\"\"\n\treturn len([arg for arg in args if arg.contains_silent()])\n\ndef is_arg_name_silent(self, name):\n\t\"\"\"\n\tReturns `True`, if argument is defined and it's not empty, `False`, if argument is not defined.\n\t:param name: The name of the argument to check.\n\t:return:"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tif isinstance(args, list):\n\t\tret = []\n\t\tfor _, _, args, _ in args:\n\t\t \tif args is None:\n\t\t\t\tpass\n\t\t \telse:\n\t\t\t\tret.append(_.get_silent_args(args))\n\t\treturn ret\n\telse:\n\t\treturn [_.get_silent_args(args)]\n\n\nclass _Base(object):\n\t\"\"\"base class for all class related classes.\n\t\"\"\"\n\n\t_cache = {}\n\n\tdef __init__(self, _key):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tlst = []\n\tfor i in args:\n\t\tif i not in self.silenced_set:\n\t\t\tlst.append(i)\n\tlst = list(set(lst))\n\tlst.sort()\n\treturn lst\n\ndef get_argument(self, args, kwargs):\n\t\"\"\"\n\tGet arguments from the received arguments\n\t:param args:\n\t:param kwargs:\n\t:return: the received argument\n\t\"\"\"\n\tif not len(args) > 0:\n\t\treturn None\n\t\n\tif"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilenced = []\n\ttry:\n\t\tsilenced = args\t\n\texcept Exception as e:\n\t\tpass\t\t\t\n\t\t\t\t\n\treturn silenced\t\t\n\t\n\n"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\targs.sort()\t\n\treturn [arg for arg in args if arg not in [self.silent, self.silent_list]+self.silent_list]"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tresult = []\n\tfor i in range(len(args)):\n\t\tif args[i] not in self.restrict_silent:\n\t\t\tresult.append(str(args[i]))\n\treturn result\n\ndef silent(args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, sliked argument names\t\"\"\"\n\treturn list(args)\n"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tself.valid = True\n\tfor k in args['requires']:\n\t\tif not (hasattr(args['requires'][k], '__call__') or hasattr(args['requires'][k], '__doc__')):\n\t\t\tself.valid = False\n\t\t\tself.err_message = \"%s requires a function or class whose function is expected. Could be %s. %s\"\n\t\tif not (hasattr(self.requirement, k) or hasattr(self.requirement, '__doc__')):\n\t\t\tself.valid = False\n\t\t\tself."}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\treturn len(self.args)+len(self.required_args)==len(set(args))\n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\trequired_args = [(self.required, 'the required arguments')]\n\tres = []\n\tfor arg in args:\n\t\tif not arg in required_args:\n\t\t\tres.append(False)\n\t\t\tres.append((arg, 'Missing required argument'))\n\tif not sum(res):\n\t\targs[args.index(self.required)] = True\n\treturn args\n\nclass ArgValueValidation(ValueValidation) :\n\t\"\"\"Validates and displays whether or not a specific argument value is appropriate.\t\"\"\"\n\tdef __init__(self):\n\t\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\treturn ( not args.has_key(\"app_type\") or args[\"app_type\"]!= \"server\" )\n\nclass StartUp(ServerProgram):\n#\tdef get_args(self):\n#\t\treturn([\"port=\"+str(self.run_port),\n#\t\t\t\"app_type=\"+str(self.run_app_type),\n#\t\t\t\"debug=\"+str(self.run_debug)] )\n\n\tdef get_args(self):\n\t\treturn ([\"port=\"+str(self.run_port),\n\t\t\t\t  \"app_type"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor arg in args:\t\n\t\tif arg not in self.requires:\t\n\t\t\treturn 'Invalid argument required: %s (expected values are %s)' % (str(arg),self.requires)\n\treturn True\n\n\nif __name__ == '__main__':\n\tprint 'Checking whether to test to validate input'\n\tif validate_input(['caffe', '--repro']) == True:\n\t\tprint '\\nTest passed!\\n'\n\telse:\n\t\tprint 'Test FAILED! (failed: %s)' % str(valid"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\treturn_code = 0\n\targ_list = [] \n\t\n\tfor x in args: \n\t\targ_list.append( x )\t\n\t\n\twhile (return_code == 0):\n\t\treturn_code = commandHandler.get_command(arg_list)\n\n\tif (return_code!= 0):\n\t\traise ValidationError('Invalid inputs')\n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\treturn check_all_arguments(self, args)\n\n\ndef make_arg_parser(parser, description):\n\tparser.add_argument('--help', '-h', action=\"store_true\", default=False, help=description)\t\n\tparser.add_argument('--version', action=\"store_true\", default=False, help=description)\t\n\tparser.add_argument('--debug', '-d', action=\"store_true\", default=False, help=description)\n\ndef main():\n\tparser = argparse.ArgumentParser() \n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\treturn all([arg.required for arg in self.__all_attrs])\t\n\n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\terrors = []\n\thas_required_args = False\n\tfor k, v in args.items():\n\t\tif v is not None:\n\t\t\thas_required_args = True\n\t\tif k.startswith('--'):\n\t\t\treturn errors\n\t\terrors.append(\"Invalid argument: \" + str(len(k)) + \"th \"\n\t\t\t\t\t\t + k)\n\tif not has_required_args:\n\t\treturn []\n\n\treturn errors\n\ndef main():\n\tusage = \"Usage: %prog [options]\"\n\tversion = \"%prog 2.0"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\trequired = self.requires\n\tfor ix in xrange(len(args)):\n\t    if required[ix] not in args[ix]:\n\t\treturn True\n\treturn False\n\n\n\n\n# If this line below is uncommented then the module will become a tool in a new script\n#\n# ## If this line is uncommented then the module will become a tool in a new script\n#\n# #### TODO \n# #### This line is an attempt to see if this script would produce an error in the\n# ####   main loop with no arguments.  If so"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t)\n\n\tdef _get_optional_args(self, command_name, options_spec, \"\"\"\n\tList only optional arguments to be used in ``condition`` argument.\n\n\tThe optional conditions are in the same form as in ``arg_spec``.\n\tList the names of available optional arguments.\n\tList if the required arguments to be used in ``condition`` matches.\n\tList if the optional arguments to be used in ``condition`` doesn't match\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of option specification.\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tpass\n\n# Return command output text.\ndef get_command_output_text(self, command_name, output_string, **options_spec__):\n\t\"\"\"Return the command output text.\"\"\"\n\tline = []\n\tfor command_arguments in output_string.split(command_name, 1):\n\t\tif command_arguments is None:\n\t\t\t# return empty string if argument is None\n\t\t\tyield ''\n\t\telse:\n\t\t\tyield ''\n\t\t\t\n\t\t\targs = command_arguments\t\n\t\t\trequired = not re.match(r'^\\"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n\treturn_names=[\"keyword\"]\n\t\n\tif (command_name in [\"list\" or '']):\n\t\t\"\"\"List all arguments.\n\t\tExample: `--list list`\"\"\"\n\t\treturn_names+=[\"arg\"]\n\t\tdel args[2:]\n\t\tdel command_name\n\telif (command_name in [\"arg\" or '']):\n\t\t\"\"\"Check if arguments list has `required' key for the given command and match the spec provided by the argument.\n\t\t\tExample: `--arg required:=abc`\"\"\"\n\n\t\tfor arg in args:\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tlist = options_spec\n\n\treturn [\n\t\t\targument[\"name\"]\n\t\t\tfor argument in list\n\t\t\tif (argument[\"name\"] in self.required_when)\n\t\t\t\t]\n\n\ndef _get_conditionally_optional_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``optional_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``optional_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tsorted_names = []\n\tfor arg_def in sorted_arg_defs[command_name].get(options_spec):\n\t\t_, _s = self.split_arg_id(arg_def)\n\t\tnames = sorted(list(set(_s)))\n\t\tnames_with_condition = []\n\t\tfor name in names:\n\t\t\tif arg_def.get(name, None) == _s:\n\t\t\t\tnames_with_condition.append('true')\n\t\t\telif _s is not None and name == _s:\n\t\t\t\tnames_with_condition"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\treturn [arg for arg in args if options_spec.issubset(arg) and arg in cmd.required_args]\ndef _get_conditionally_optional_args(self, command_name, options_spec,\n\t\t\t\t\t\t\t\t  **kwargs):\n\treturn [arg for arg in args if options_spec.issubset(arg) and int(arg.required)]\n# pylint: enable = line-too-long\n# pylint: disable=no-member\ndef _get_optional_args(opt_obj, args, opts=[]):\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\treturn _get_args_func_args(command_name, options_spec,\n\t[\"[i]\" if args[0] in [\"\" for _ in range(command_name)] else args])\n\n_REQUIRED_ARGS = _get_conditionally_required_args\n\n\ndef process_options(options_spec, args=[], optparser=None, option_order=None):\n\t\"\"\"\n\tProcesses the command-line arguments list.\n\n\t:param options_spec: a string or list of options spec as\n        arguments.\n\t:param args"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t)\n\t\n\toptionalargs = \"\".join([arg for arg, _ in options_spec if self.is_optional(\"required_when\"))\n\treturn optionalargs, command_name, options_spec, args\n\t\n\t\n################################################################################################\n\nclass get_options_spec_required_when(getattr):\n\t\"\"\"\n\tReturn an ``opt.OPTS`` instance with option ``required_when`` matching its value for\n\tthe given specified list of arguments.\n\t\n\t:param options_spec: string, list of arguments to be checked\n\t:param args"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\treturn [\"--require_%s=%s\" % (arg, arg) for arg in options_spec] # convert options_spec into args\n\t_check_list_arguments_with_when(command_name, options_spec,\t\"required_when\")\n\treturn _get_conditionally_required_args\n\t\ndef _get_optional_args(self, command_name, options_spec,\t\"\"\"\n\tList options with 'optional_when' condition matched.\n\n\t:param command: the command name.\n\t:param options_spec: the list of"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\targs = self._remove_arguments(command_name, options_spec, args)\n\n\tif len(args) == 0 :\n        \treturn []\n\t\n\n\tself._logger.debug(\"%s: find %d arguments with ``required_when``\", \n\toptions_spec, len(args))\n\n\treturn self._find_arguments(args, required_when)\n\t\n\t"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) > self.length:\n\t\traise ValidationError(self.message +'Value cannot exceed'+ self.length)\n\t\t\n\treturn args\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif not \"value\" in args or len2(args[\"value\"]) >= len(self.data):\n\t\terrmess(\"Value is not longer than the specified length.\")\n\n\n\t\n\t\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif self._valid == \"long\":\treturn args > self._length - len(self._values)\n\telse:\t\t\t\t\t\t\t\t\treturn args < 0\n\t\n\treturn False\n\n\n\nclass MessageFactory:\n\t\n\t\"\"\"\tRepresents the factory object in which to implement some method.\n\t\t\t\t\n\t:param args: The type of arguments the method needs.\t\t\t\n\t\t\t\n\t:param desc: The description of the method need a result, or it was supposed to.\n\t\t\t\t\t\n\t:param func: The function where the result is expected.\t\"\"\"\n\t\n\tdef __init__"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tmin_len = len(args)\n\targ_num = 0\n\tmax_len = 2\n\tif args is not None:\t\t\n\t\targ_num += 1\n\t\tif arg_num > max_len:\t\n\t\t\treturn Validation.Error(len_error='',\n\t\t\t\t\t\t\t\t\t error_message=\"Must be a list of at least \" + \\\n\t\t\t\t\t\t\t\t\t\t\t'month and year.',\n\t\t\t\t\t\t\t\t\t args=args)\n\t\tif args.__len__() <= 0:\n\t\t\treturn Validation.Error(len_error='', error_message=\"Must be a"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif (not len(args)):\n\t\treturn\n\tlength = max(len(str(arg)), 1)\n\tfor key in args:\n\t\tif not args[key]:\n\t\t\treturn\n\t\ttmp_len = min(len(str(arg)), length)\n\t\tstr_arg = str(arg[:tmp_len][0:tmp_len])\n\t\tnew_arg = list(str_arg)\n\t\tfor index in range(tmp_len - str_len(str(arg[:-1]))[1]):\n\t\t\tnew_arg.append(0"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\treturn (args[0] is not None)\n\ndef validate_is_valid(self):\n\t\"\"\"validate - check if input is valid with given requirements.\"\"\"\n\tif\t(not validate_length_args(self.length)):\n\t\terror_message = u'Invalid length %s for given %s!'\\\n\t\t\t\t% (self.length,'object_')\n\t\traise ValidationError(error_message)\n"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\ttry:\n\t\targs_length = arglen(args)\n\texcept ValueError:\n\t\targ_errors = \"Unable to find length of arguments for validate_length_args\"\n\t\traise ValueError(arg_errors)\n\n\t# Check lenght of valid arguments if argument are too long\n\tif args_length > len(args):\n\t\targ_errors = \"Value %s is too long for the argument '%s'\" % \\\n\t\t(args, args[0])\n\t\traise LengthRequired(arg_errors)\n\n\t# else the last element is valid\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\n\tif len(args) > self.max_length:\tprint \"Received more args than allowed.\"\n\treturn\t\t\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif ((len(args) < self.min_length) or (len(args) > self.max_length)): \n\t\traise ValueError\n\t\n\n\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif not type(args) == types.ListType:\n\t\traise TypeError(\"argument 'args' must be a list/tuple.\")\n\n\tvalue_len = len(args)\n\tif  value_len > self.__len_check_value__:\n\t\traise ValueError(\"argument value '%s' is beyond length specified (%s/%s)\" % (str(args), self.__len_check_value__, self.__len_check_value__))\n\ndef validate_min_args(args):\t\"\"\"\n\n:param args: The received arguments"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tfor choice_arg in self.choices.keys():\n\t\tif choice_arg not in args:\n\t\t\treturn False\n\n\treturn True\n\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\targs[0] = str(self.choices[args[0]])\n\treturn args\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif not args: return False\n\tchoices = self.choices\n\tfor opt in args:\n\t\tif opt in choices: return True\n\treturn False\t\t\n\t\n\"\"\"Function used by validations to find out when a list of choices may be used.\"\"\"\t\ndef validations_check_choices_is_all_available_choices(self, args):\t\"\"\"\n\n:return: true if all available choices exist in argument list, false if some are not.\n\t\n\tValidations: none, no_value, number, string.\t\n\"\"\"\n\n\n\"\"\""}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tfor i in range(len(args)):\n\t\tchoices_args=self.value.choices_args[i]\n\t\tif  choices_args.value!=choices_args.value:\n\t\t\treturn False\n\t\t\t\n\treturn True\t\n\nclass MultiSlider(Slider):\n\t\"\"\"Sub class for handling the multiplot selection of a slider with more than one label and multiple values\"\"\"\n\tdef __init__(self, value=None, **args):\n\t\tSlider.__init__(self, value, **args)\n\t\tself."}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(list(map(str, args))) > 1:\n\t\tmessage = \"Expected valid list of string arguments.\"\n\t\traise ValueError(message)\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif not isinstance(args, list):\n\t\treturn\n\tfor arg in args:\t\n\t\tif self.choices.get(isinstance(arg, self.types.choice), None):\n\t\t\treturn\n\n\treturn True\n\n\nclass Validation(object):\n\t\"\"\"\n\tValidates a given value against a list of choices.\n\n:param args: The received arguments.\t\n\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"\n\t\tInits the object with the given arguments.\n\t\t\"\"\"\n\t\tif not validate_choices_args"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif args:\n\t\tif len(args) > 1 and args[0] in self.choices():\n\t\t\treturn True\n\t\treturn False\n\n\treturn False\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tassert(isinstance(args, (list, tuple)))\n\tif args[0] in self.choice_list:\n\t\tchoices = self.choice_list[args[0]]\n\t\tfor idx in range(1, len(args)):\n\t\t\tif args[idx][1] in choices:\n\t\t\t\tv = args[idx][0]\n\t\t\t\tassert(isinstance(v, (str, basestring)))\n\t\t\t\treturn v\n\treturn None\n\n\tdef validate(self, val):\n\t\treturn False\n\n\tdef validate_choices(self, v"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\ttry:\n\t\tfor i in range(len(self.choices)):\n\t\t\tself.choices[i]._validate_args(args)\n\n\texcept:\n\t\tlog = cLog()\n\t\tlog.debug('Value check failed. Exiting')\n\t\tlog.error('Bad input: {!r}'.format(args))\n\n\t\traise\n\n\treturn []\n\n\ndef validate_choices(choices):\t\"\"\"\n\tValidates that no more than one value is given for the choice. Returns the\n\tvalues that were validated.\n\n\t:param choices: An"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\treturn False\n\n\n"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tself.args = args\n\tif isinstance(args, tuple):\n\t\tself.minimum = args[0]\n\t\tself.maximum = args[1]\n\telif isinstance(args, int):\n\t\tself.minimum = args\n\t\tself.maximum = args\n\n\telse:\n\t\tself.minimum = None\n\t\tself.maximum = None\n\treturn\n\n\nclass ValueConstraint(ValueType):\n\t'''\n\tA ValueType that checks if a given value exceeds or is smaller than a specific value.\n\t\n\tArguments:\n\t\tvalue\t-"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tself.validate(min(args), max(args))\n\n\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tself.assert_(\n\t\tisinstance(args, dict), 'An argument must be instance'\n\t)\n\ttry:\n\t\tvalidations = args.get('validations')\n\t\treturn int(self.validate(validations))\n\texcept Exception:\n\t\treturn 0\n\n\n\t\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\treturn len(args) >= 2 and args >= self.min_value and args <= self.max_value"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args[0] < args[1]:\n\t\treturn \"-2147483648  -2147483648\"\n\telse:\t\t\n\t\treturn args[0] \t\t\t\t\n\t\n\ndef validate_args(args):\t\"\"\"\n\tCheck if argument list is complete.\n\n:param args: The received arguments.\t\"\"\" \n\tif len(args) < len(args):\n\t\t\treturn \"Too few arguments. Max should be:\" + str(args[len(args)]) \n\t\t\t\n\ndef validation(args):\t\"\"\"\n\tMain function. Try"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t# pylint: disable=arguments-differ\ndef get_min_max(value):\n\t\"\"\"\n\tGet minimum and maximum values.\n\n:param value: The value which will be received.\t\"\"\"\n\tlow = float(\"-inf\")\n\thigh = float(\"inf\")\n\tif value < low:\n\t\tlow = float(\"-inf\")\n\tif value > high:\n\t\thigh = float(\"inf\")\n\treturn low, high\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args is not None:\n\t\tfor key in args:\n\t\t\tvalue = args[key]\n\t\t\tkey = key.lower()\n\t\t\tif values_match(value, values_list(self.min, self.max), default=''):\n\t\t\t\treturn True\n\t\treturn False\n\n\treturn True\n\n\ndef validate_min_max(function):\n\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param function: The received function.\t\"\"\"\n\tdef _validate(args):\n\n\t\tmin = None\n\t\tmax = None\n\n\t\tif"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.minimum:\n\t\tif args.value < args.minimum:\n\t\t\treturn (\"Please input lower limit value of %(min)s < %(max)s!\" % {'min': args.minimum,'max':args.value})\n\t\telif args.value > args.maximum:\n\t\t\treturn (\"Please input higher limit value of %(min)s < %(max)s!\" % {'min': args.value,'max':args.maximum})\n\telif args.maximum:\n\t\tif args.value > args.maximum:\n\t\t\treturn (\"Please"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args is None:\targs = self.cfg.args\n\tmin = self.cfg.min\n\tmax = self.cfg.max\n\tif args and not (args <= min)\\ and not (args >= max):\n\t\tmax,arg = self.to_integer(args)\n\t\treturn max,arg\n\tif args is None:\n\t\treturn self.cfg.min\n\telse:\n\t\treturn self.to_integer(args)\n\ndef to_integer(value):\n\t\"\"\"Converts a string value to an integer if not already one\"\"\"\n\ttry:"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\ttry:\n\t\targs = args.split(',')\n\n\t\tmin = args[0].split('%')[0].strip(' ')\n\t\tmin = float(min)\n\t\tmax = args[1].split('%')[0].strip(' ')\n\t\tmax = float(max)\n\n\t\tif min > max:\n\t\t\terror = 'Error setting arguments.'\n\t\t\traise error\n\n\t\tif min <= 0 or max <= 0:\n\t\t\terror = 'Error setting arguments.'\n\t\t\traise error\n\n\texcept:\n\t\terror = 'Error setting arguments.'\n\t\traise error\n"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t):\n\n\t\tfor c in self.complex_argumet_type.get('c'):\n\t\t\tif not c.get('p'):\n\t\t\t\tp_type_name = self.complex_argumet_type.get_typename(c, self.complex_argumet_type.get_type(c), option_name)\n\t\t\telse:\n\t\t\t\tp_type_name = c['p']\n\n\t\t\topt_spec = c.get('p', None)\n\t\t\tif opt_spec is not None:\n\t\t\t\topt_list = set(opt_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\treturn self.get_arguments(subcommand) + [\n\ttype_name,\n\toption_name] \n\n#######################################################################\n\nclass create_complex_group_type(arguments_factory):\n\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param spec: the complex type specification\n:return: the complex type group instance\n\t\"\"\" \n\tdef __init__(self, spec):\n\t\tself.spec = spec\n\n\tdef get_arguments(self, subcommand):\n\t\treturn self.spec"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t)\n\tspecification = self.get_argument(sig = spec_option, default = None);\n\tif (specification == None or \\\n\t\t not type_name in specialization.items()):\n\t\tmismatch = 'The complex argument (%s) is expected to be in one of the following types : %s.\\n'\\\n\t\t\t\t% (spec_option, str(specification))\n\t\tself.log(mismatch);\n\n\tspecification.add_item(type_name, option_name);\n\n# ============================================================================\n\n\tif (sig"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t)\n\tif subcommand == get_command_name():\n\t\t# we're done processing the command\n\t\tsys.exit(0)\n\t# build the optional argument\n\targ_subcommand = subcommand + \"_arg\" + option_name + \"\\t\" + str(type_name) + \"\\t\" + str(option_name) + \"\\t\" + str(type_name)\n\treturn (arg_subcommand, \"\"\")\" + suboption_description + \":\" + str(type_name) + ( arg_subcommand)\n\n\n\n# command processing function"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t)\n{\n\treturn eval(\n\t\tsubcommand + \"#define\" + option_name\n\t+ \"Type_t\" + type_name\n\tf\"_{type_name}_type = {type_name}\" + option_name\n\t+ \"Class_t\"\n\tf\"_{type_name}_class_p = pthread_self()\",\n\t)\n}\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t)\n\t@staticmethod\n\tdef build_complex_argumet_type(type_name, option_name,  special_options_array):\n\t\tspec_options = []\n\t\tspec_options.append(\"complex_arg\")\n\t\tspeclist = \"\", [spec_options]\n\t\t# add the spec options\n\t\tfor speclist_el, value in\tspecial_options_array:\n\t\t\tspeclist.append(str(specilist_el.val))\n\t\tspeclist.append(str(type_name))\n\t\tspeclist"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t)\n\ttype_name = self._build_complex_type_name_string(type_name)\n\tspec_options, self._spec_inits_type = None, []\n\tif type_name in option_name:\n\t\tself.parser.reset()\n\t\toption_name = option_name[len(type_name) :]\n\t\tspec_option = self.get_string(\"-\" + option_name)\n\t\twhile(self.parser.get_next_flag()):\n\t\t\tif self.parser.get_flag() == \"=\" or self."}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t)\n\tself.add_completer_subcommand(subcommand, spec_option, type_name,\n\t\t\t\tfunction = self.build_complex_argumet_type\n\t\t\t)\n\ndef getargs_by_completer(args):\n\t\"\"\"\n\tBuilds the complex arguments from the argument list.\n\n\t:param args: the argument list\n\t:return: the complex arguments\n\t\"\"\"\n\tsimple_argument_types = [\n\t\tComplexArguments.Type(a.type, a.default_value, a.default_name)\n\t\t\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t)\n\treturn complex_argumet\n\n\t\n\t\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t    return SpecificationArgType(specification_parser, type_name, option_name, subcommand, self.options.create),\n\t    \"\"\"\n\n.. |create_complex_argumet_type| replace::\n\n\tCreate the complex argument type\n\n.. _`create_complex_argumet_type`: https://github.com/girsh/ghc/blob/master/examples/ghc/create_complex_argumet_type.rst\n\n\tCreate the complex argument type\n\"\"\"\n\n\t)\n\t\nparser_options.add"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\targs_t = []\n\tcontrol = \"control\"\n\tnested = \"nested\"\n\tcustom = \"custom\"\n\n\tfor arg in args.split(\",\"):\n\n\t\t# check for nested\n\t\tif arg == nested:\n\n\t\t\targs_t.append( arg )\n\n\t\t# check for the custom, if present, replace it with n_custom\n\t\tif arg.replace(\" \", \"\").startswith(custom)':\n\t\t\targs_t.append( arg.replace(\" \", \"\") )\t\n\n\treturn (args_t, args_t.replace(control"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\treturn (args,args)\n\ndef get_custom_args(func):\n\t\"\"\" returns the custom object for this function, by reading the config\n\t:param func: the function\n\t:return: the configured object in the spec yml\n\t\"\"\"\n\treturn func(get_nested_custom_args)\n\ndef check_for_control_args(args_map):\n\t\"\"\" Checks if control args should be passed to nested_args.\n\n\tThis is a simplified version of py_utils._get_nested_variables_map.\n\n\t:param args_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tdef is_string_or_list(string):\n\t\tif not isinstance(string, basestring) and type(string) == list:\n\t\t\treturn True\n\t\treturn string and (is_string_or_list(string[0]) or is_string_or_list(string[1]))\n\n\t# this is to allow control to control nested or a single arg.\n\t# this is required because of the way we are passing the args above\n\t# in the spec file. this makes it more easy to switch between the\n\t# two\n\tcontrol_args = {}"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = []\n\tnested_args = []\n\n\t# Create a flat list of args\n\tflat_args = []\n\tfor (arg, arg_data) in args:\n\t\tif hasattr(arg_data, 'get'):\t# is a list\n\t\t\tflat_args.append(arg)\n\t\telse: \t\t\t# is not a list\n\t\t\tfor key in sorted(arg_data):\n\t\t\t\tflat_args.append(key)\n\n\t\t\tfor key in sorted(arg_data):\n\t\t\t\tif not key.startswith('-'):\t#"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = set([i['type'] for i in self.args])\n\trest_args = list(self.args)\n\n\tnested_args = set([i for i in self.args if i not in args])\n\n\targs = [i for i in args if i not in control_args]\n\targs.extend(rest_args)\n\n\tif not len(args):\n\t\treturn (args, nested_args)\n\n\tall_args = list(args)\n\tall_args.append(self.args[-1])\n\n\treturn (all"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_and_nested_args =  {}\n\tnested_and_control_args =  {}\n\tfor arg in args:\n\t\tif \"control_and_nested\" in arg:\n\t\t\tcontrol_and_nested_args['control'] = \"yes\"\n\t\t\tnested_and_control_args['and'] = arg\n\t\telif \"control\" in arg:\n\t\t\tcontrol_and_nested_args['control'] = \"no\"\n\t\t\tnested_and_control_args['and'] = arg\n\t\telif \"nested"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t# if control_args:\n\t#     _control_args = {}\n\t#     for arg in args:\n\t#         #if  arg == '-c':\n\t#         #     _control_args['__control__'] = True\n\t#         if not arg.startswith('--'):\n\t#             # TODO maybe use custom config options instead\n\t#             _control_args[arg] = (True, True)\n\t# if nested_args:\n\t#     #if not('-C' in nested_args):\n\t#     "}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t# The 'control_args' and 'nested_args' are used below\n\t# to put the ansible data in the spec yml file\n\tcontrol_args = {}\n\tnested_args = {}\n\t#\n\t# Check if the 'args' contain nested variables\n\targs = self.validate_args(args)\n\t#\n\t# if args[0]:   # if the args are not the default\n\t#\t\t# the first argument needs to be checked\n\t#\t\t# if there wasn't already a custom control argument\n\t#\t\t# and the"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol = args[0]\n\tnested = args[1][0]\n\n\treturn control, nest\n\nclass CliCommand(object):\n\t\"\"\"Generic class representing a command to be executed in a playbooks \"\"\"\n\n\t@property\n\tdef name(self):\n\t\traise NotImplementedError()\n\n\tdef get_help(self, opts):\n\t\traise NotImplementedError()\n\n\n\t@property\n\tdef short_help(self):\n\t\traise NotImplementedError()\n\n\t@property\n\tdef full_help(self):\n\t\traise"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\t# remove control args and nested args\n\targs = filter (lambda x: '_' not in x, args)\n\tnested_args = [k for k in args if not k.startswith('control_')]\n\tcontrol_args = [k for k in args if k.startswith('control_')]\n\n\n\treturn nested_args, control_args\n"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor key, value in vars_dict.items():\n\t\tif extra_vars:\n\t\t\tvars_dict[key]=extra_vars\n\t\telse:\n\t\t\tvars_dict[key] = value\n\n\n\ndef merge(params, env=None):\n\t\"\"\"\n\tMerge dicts with env keys\n\n\t>>> params={'env1':'value1', 'env2':'value2', 'env2_plus':'value3'}\n\t>>> env=Env(params)\n\t>>> params_2={}\n\t>>> env.mer"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif vars_dict.get('__vars_attrs', False):\n\t\tvars_dict.update(extra_vars)\n\treturn vars_dict\n\ndef extend_vars_dict(vars_dict, extra_vars=None):\t\"\"\"\n\tCopy existing vars_dict into new vars_dict, merging ``extra-vars``\n\tor applying vars in vars_dict\n\n:param vars_dict: Dictionary to merge other vars (ex <name>_<class>) into\n:param extra_vars:"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\n\t# Get the merge object which should be extended.\n\tmerge = vars_dict.get(u'merge')\n\t\n\t# Check if we should overwrite existing vars_dict, \n\t# where case is different.\n\toverwrite = True\n\tif vars_dict and merge and not extra_vars:\n\t\toverwrite = False\n\t\n\t# Overwrite if we should.\n\tif overwrite:\n\t\n\t\t# Merge object in new vars_dict but not existing.\n\t\tfor var in vars_dict.keys():\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor key in extra_vars:\n\t\tif key in vars_dict.keys():\n\t\t\tvars_dict[key] = vars_dict[key] + extra_vars[vars_dict.index(key)]\n\treturn vars_dict\n\n\ndef get_request_body_string(request):\t\"\"\"\n\tExtend ``request`` and return the body string\n\n:param request: Request object \t\"\"\"\n\trequest_value=request.get(CONF_REQUEST_BODY)\n\tbody_string=''\n\tif request_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif not vars_dict:\n\t\treturn vars_dict\n\tif not extra_vars:\n\t\textra_vars = vars_dict.keys()\n\tfor var in extra_vars:\n\t\tif not var in vars_dict:\n\t\t\tvars_dict[var] = vars_dict.get(var)\n\treturn vars_dict\n\ndef add_config_variable(vars_dict, name, values):\t\"\"\" \n\tAdds a config variable to dict, keyed by name\n\t(and using a dot operator).\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif not extra_vars:\n\t\textra_vars = []\n\tif extra_vars:\n\t\tvars_dict.update(extra_vars)\t\n\treturn vars_dict\n\n\ndef merge_vars_dicts(source_dict, extra_dict, kw_dict={}):\n\t\"\"\"\n\tMerge all ``extra-vars``, ``source-vars`` and ``variable_dict``\n\tfrom `source_dict` to `extra_dict` and `kw_dict`, using both methods\n\tas defined in `copy"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif not extra_vars:\n\t\textra_vars = vars_dict\n\telif extra_vars:\n\t\tvars_dict.update( extra_vars)\n\n#=============================================================================#\ndef get_data_frame(params):\n#=============================================================================#\n\tdata_frame = pd.DataFrame(params)\t# Extract out each data values\n\treturn data_frame\n#=============================================================================#\ndef remove_columns_in_dataframe(data_frame):\n#=============================================================================#\n\tnon_empty_cols = data_frame."}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars and extra_vars!= vars_dict:\n\t\tvars_dict.update(extra_vars)\n\treturn vars_dict\n\ndef merge_vars_dicts(additional_vars=None,\n\t\t\t\t\t  default_var=None,\n\t\t\t\t\t  all_vars=None,\n\t\t\t\t\t  merge_extra_vars=True,\n\t\t\t\t\t  extra_vars=None):\n\t\"\"\"\n\tExtend all ``additional_vars`` and ``default_var`` to add extra-vars, add extra"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars:\n\t\tif extra_vars is None or len(extra_vars) == 0:\n\t\t\treturn vars_dict\n\t\telse:\n\t\t\tfor i in extra_vars:\n\t\t\t\tvars_dict['extra'].append(i)\n\t\t\treturn vars_dict\n\t\t\n\n\n\treturn vars_dict"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars:\n\t\tfor key, value in extra_vars:\n\t\t\tvars_dict.setdefault(key, value)\n\treturn vars_dict\n\n\t\ndef getDefaultArgs(obj):\n\t\"\"\"\n\tHelper function for getting the default args and return it by\n\tdefault from a given constructor or class using getDefaultArgs\n(which returns object instance of the constructor or class)\n\n:param obj: The class, constructor or object that should be used to set\nthe default args.  The default arg object is a tuple of the\n`value` object"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\n\t# This is a little odd, but the way everything in ansible uses Ansible workspaces\n\t# has become harder to read, because the Ansible Playbook command has to run\n\t# as well as execute commands.  We must figure out how to add that functionality\n\t# to Ansible before we can write out something into the playbook.  At all times\n\t# there is a small chance that the playbook will be called with an extra argument\n\t# specified, eg: \"--extra-vars\" will call Ansible's version 2 of the method, but\n\t# that"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t) -> None:\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\tExecutes the given playbook.\n\n:return: The stdout from the Ansible playbook executed.\n\nTodo >> What does the main Ansible function do?\n\n:since: 1.3\n:side_effect: Does not return output unless it is needed.\n\n\"\"\"\n\t_, playbook_output = execute_playbook(ir_workspace, ir_plugin, playbook_path, extra_vars, ansible_args)\n\treturn playbook_output\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\tif verbose is None:\n\t\tverbose = get_options(default=False)\n\t\n\tfrom ansible.callbacks import vv\n\tfrom ansible import __version__\n\tfrom ansible.playbook.local import LocalPlaybook\n\t\n\tp = vv.VirtualWlp(verbose)\n\tp.init(ir_workspace, ir_plugin, playbook_path, extra_vars, ansible_args)\n\tp.run()\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\t\n\t# Make sure we have a working workspace first unless one was given\n\tif not ir_workspace:\n\t    ir_workspace = IRWorkspace()\n\t\tir_plugin = IRPlugin()\n\t\tir_workspace.add_plugin_path(ir_plugin)\n\t\n\t# Convert to JSON\n\tplaybook_path_json, playbook_args = playbook_path.replace('%ANSIBLE_PLAYBOOKS_PATH%', 'json')\n\t\t\n\t# Check the verbose level\n\tif verbose:\n\t\tlog_verbose"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\timport pipes\n\timport os\n\tfrom random import randint\n\n\t# TODO: remove all this\n\tif verbose:\n\t\tverbose = \"verbose\"\n\textra_vars = extra_vars.copy()\n\textra_vars[\"playbook_path\"] = playbook_path\n\textra_vars[\"input_fds\"] = [pipes.list2cmdline(os.popen(f).read().splitlines()).split()[2]]\n\textra_vars[\"output_file\"] = \"stdout_\"+str(rand"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\treturn playbook_executor.execute(ir_plugin, playbook_path, verbose, extra_vars, ansible_args)\n\nclass PlaybookRunner( object ):\n\n\tdef __init__( self, plugin_name, playbook_class, verbose=None, ansible_args=None, extra_vars=None, ):\n\t\tself.plugin_name = plugin_name\n\t\tself.playbook_class = playbook_class\t\n\t\tself.verbose = verbose\n\t\tself.ansible_args = ansible_args\n\t\tself.extra_v"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t) -> None:\n\t\"\"\"Wraps the 'ansible-playbook' CLI if the 'ansible-playbook' is being used\n\tas the base command\"\"\"\n\n\t# Run the 'ansible-playbook' command\n\t# See https://ansible.readthedocs.org/en/master/reference/cli_general.html for details on the parameters passed to Ansible\n\targs = {\n\t\t'module_dir':  '/etc/ansible',\n\t\t'playbook':    os.path.realpath(os.path.join(ir_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t) :\n##\tdef print_help(parser):\n##\t\t\"\"\"Helper method to show help for the playbook command\"\"\"\n##\t\tfor arg in parser.parse_args():\n##\t\t\tprint '-'*30 + '\\n\\n' + '{{} \\n\\n'\n##\t\t\tfor key, value in  extra_vars.items():\n##\t\t\t\tprint '{{}}\\n\\n${}\\n\\n'\n##\t\t\t\tsys.stdout.write(' '.encode('utf-8') + key +'=\"'+value+'\"\\n')\n##"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t)\n\t\"\"\"\n\n\t# create the new play file\n\tplay = playbook.play(playbook_path, verbose=verbose)\n\n\t# load the remote configuration\n\tir_plugin.load_remote_configuration(play.config)\n\n\t# load the virtual host plugin\n\tir_plugin.add_virtual_host(play.vhost_list)\n\n\t# set the default configuration\n\tdefaults = dict(\n\t\thost_list=ir_plugin.get_virtual_hosts()\n\t)\n\n\t# add the current remote configuration settings back\n\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t) -> Tuple:\n\t\treturn (\n\t\t\tAnsible.create_workflow('ansible-playbook', ir_workspace, ir_plugin, playbook_path, verbose=verbose,\n\t\t\t\t\t\t\t\t\t\t\t  extra_vars=extra_vars, ansible_args=ansible_args):\n\n\t\t\t# create the Workflow object to hold the parsed result of Ansible\n\t\t\treturn (workflow_obj, pipeline_obj)\n\n\t# END ANSIBLE PLUGIN PLUGIN-NAME PLUGIN-VERSION\n\n\n# END __main__\n\n"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# Loads infra_vars.yml\n\t# loads variables and facts for that file to use as cli parameters\n\t# loads facts from vars_dict to use as cli parameters\n\t# loads plugin facts from vars_dict to use as cli parameters\n\t# loads playbook variables to use as cli parameters\n\n\treturn run(cli_args, vars_dict, ir_workspace, ir_plugin)\n\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tself._run(cli_args, vars_dict, ir_workspace, ir_plugin)\n\"\"\"\n\tdef _run(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\t\"\"\"\n\t\tRuns ansible cli with vars dict\n\t\t:param cli_args: The list of command line arguments to use\n\t\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t\t:param ir_workspace: An InfraredWorkspace object represents the active\n\t\t\t workspace\n\t\t:param ir_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n\t# Load user variables, and return them\n\t_user_vars_dict = cli_args.get(\"user_vars\",dict())\n\t\n\t# check whether the user can use this program, i.e. has a\n\t# -m command line argument in the command line\n\tif _user_vars_dict and __name__ == \"__main__\":\n\t\t_user_vars_dict.update(vars_dict)\n\t# set the values in the variables. if\n\tvars_to_set = _user_vars"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\timport os\n\n\toutput = \"\"\n\toutput += \"\\n# This script relies on ansible>=3.5.0\\n\"\n\t_execute_as_user(cli_args)\n\t_print_output_file(cli_args)\n\n\tvars_dict['_playbook_args'] = cli_args\n\toutput += \"/vars_dict: %s\\n\" % vars_dict\n\n\t_run_playbook_command(cli_args, output, ir_workspace, False)\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# Create empty vars_dict\n\tvars_dict['host_name'] = 'hostname' \t\n\n\t# Check input\n\tif 'hostname' not in vars_dict:\n\t\treturn [dict(stdout=\"This parameter is required for the ansible cli toolkit.\")]\n\n\n\t# Check the inputs to run the command\n\tif 'checkpathname' not in vars_dict:\n\t\treturn [dict(stdout=\"This parameter is required for the Ansible cli toolkit.\")]\n\n\n\t# Check the input to connect ssh and ip"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\ttry:\n\t\tplaybook = lh.lhrunviz2(cli_args)\n\t\tplaybook.vars = vars_dict\n\t\tansible_cli_vars = playbook.get_cli_vars()\n\t\tlogger.info(\"Running Playbook with CliVars on %s\" % vars_dict)\n\t\tlogger.info(\"ansible_var_file = %s\" % ansible_cli_vars['ANSIBLE_VAR_FILE'])\n\t\tlogger.info(\"ansible_play_vars = %"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n\t#check for vars\n\tif not hasattr(cli_args, 'vars'):\n\t\traise AnsibleError('vars attribute absent ')\n\tif not hasattr(vars, '__iter__'):\n\t\traise AnsibleError ('vars attribute is required')\n\ttry:\n\t\treturn_results = vars_dict\n\t\treturn_results['vars'] = vars\n\texcept TypeError:  # Python < 2.6\n\t\tvalues = vars  # pylint: disable=consider-using-delegation\n\t\treturn"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\ttry:\n\t\tansible_list = cli(vars_dict,\n\t\t\t\tir_workspace.run_remote_command('ansible-playbook', **{\n\t\t\t\t\t'vars' : cli_args,\n\t\t\t\t\t'worker_thread': False,\n\t\t\t\t}),\n\t\t\t\tcli=True,\n\t\t\t\tverbose=False # This is why we need to call it verbose\n\t\t\t)\t\t# Don't do verbose when using ansible-playbook\n\texcept TypeError, e:\n\t\timport traceback\n\t\t#print traceback.format_exc()\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# Create a cli result\n\tresult = {'changed': False, 'failed': False,'msg': '', 'rc': 0}\n\n\targs = (('_ansible_playbook', cli_args, vars_dict))\n\n\t# Run the ansible playbook (and capture output)\n\ttry:\n\t\tresult['msg'] = run_ansible(args)\n\t\tresult['failed'] = False\n\texcept Exception as e:\n\t\tresult['failed'] = True\n\t\tresult['err_msg'] = str(e)\n\n\treturn result"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tprint \"Running Playbook...\"\n\n\t# Call Ansible-API to run playbook\n\tif kafka_version > 0:\n\t\tplaybook_file = \"playbook-v0.1\"\n\telse:\n\t\tplaybook_file = \"playbook_basic-v1.1\"\n\n\t# create and read playbook config from file, and then parse to generate actual \n\t# plays\n\tplaybook = playbook_cfg(playbook_file, ir_workspace)\n\n\tprint \"Playing plays from file...\"\n\trunner_playbooks = playbook[\"runner_"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tstr_values = values_dict['value']\n\tif not isinstance(str_values, dict):\n\t\tstr_values = dict(values_dict)\n\n\tnew_func_name = \"{0}_{1}\".format(parser_name, values_dict['name'])\n\tnew_func = getattr(self.commands, new_func_name)\n\tif str_values is not None:\n\t\treturn_type = str\n\t\treturn_args = tuple() if not isinstance(str_values, tuple) else str_values\n\telse:"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tparser = self._get_parser(parser_name, values_dict)\n\toutput = self._convert(parser)\n\tparser.print_help()\n\tsys.exit(output)\n\t\n\t\ndef _convert(parser):\n\t\"\"\" Convert the argument parser to the same types as str\n\t\"\"\"\n\tparser.error('Incompatible argument type')\n\n\ndef _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t# The string type, so need to cast first\n\tvalues_dict = values_dict.copy()\n\tvalues_dict[\"command\"] = values_dict[parser_name]\n\tvalues_dict['version'] = 'qemu'\n\treturn values_dict\n\t\nclass VirshBackend(object):\n\t\"\"\"\n\tThe class represents the different API backends required of a\n\tVirsh daemon.  It is subclassed using the :class:`VirshDaemon` class,\n\tso that it needs to return backends from the appropriate methods.\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tdef _substitute_arg(self, arg):\n\t\t\n\t\tif isinstance(arg, str):\n\t\t\tif isinstance(values_dict, bytes):\n\t\t\t\treturn arg.decode('utf-8')\n\t\t\telse:\n\t\t\t\treturn'\"'.decode('utf-8') + str(values_dict) + \"'\"\n\t\t\t\n\t\tif isinstance(arg, bytes):\n\t\t\treturn arg.decode('utf-8')\n\t\t\t\n\t\traise ValueError('Invalid ArgType: %s' % arg)\n\t\n\treturn _substitute_arg"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif values_dict:\n\t\tif not isinstance(values_dict, dict) :\n\t\t\tvalues_dict = { }\n\t\tif not isinstance(values_dict, dict) :\n\t\t\tvalues_dict = { }\n\t\tfor k in values_dict:\n\t\t\tv = values_dict[k]\n\t\t\tif not v.__class__.__name__ == parser_name :\n\t\t\t\tvalues_dict[k] = str(v) \n\treturn values_dict\n\t\n\t\n\t\ndef convert_cli_args(values_dict=None):\t\"\"\""}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\treturn values_dict\n\n\nclass ArgumentError(BaseException):\n\t\"\"\"Raised if an unrecognized argument to an action is supplied\n\tin the CLI\"\"\"\n\tdef __init__(self):\n\t\tsuper(ArgumentError, self).__init__()\n\n\nclass OptionError(BaseException):\n\t\"\"\"Raised if an unrecognized command is issued\"\"\"\n\tdef __init__(self):\n\t\tsuper(OptionError, self).__init__()\n\n\nclass OptionGroupError(InvalidConfigError):\n\t\"\"\"A command group specified for an invalid option was supplied\n\n\tIn"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tres = values_dict\n\tfor val in values_dict.items():\n\t\tif type(val[0]) == str:\n\t\t\tres[parser_name] = str(val[0]) # cast strings to str\n\t\telif type(val[0]) == unicode:\n\t\t\tres[parser_name] = unicode(val[0]) # cast unicode to unicode\n\t\telse:\n\t\t\tprint('value %s is not a string or unicode, cannot coerce %s to %s' %\n\t\t\t      (val[0], parser_name, str("}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\treturn {parser_name: str(v) for parser_name, v in values_dict.items()\n\t\t\tif not self.check_option(v)}\n\n\t\ndef _convert_optionals(values_dict,\n\t\t\t\t\t parser_name,\n                    value_converter = _convert_string,\n                    default = ''):\n\tresult = _convert_string(value_converter,\n\t\t\t\t\t\tvalue_dict = values_dict.get(\n\t\t\t\t\t\tparser_name),\n\t\t\t\t\t  default = default)\n\treturn result\n\t\n\ndef main():"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif parser_name in values_dict:\n\t\tfor k, v in values_dict.items():\n\t\t\tif not self.verify(k, v) or not self.verify(parser_name, v):\n\t\t\t\t# Check is no argument present\n\t\t\t\traise CommandError(\"%s : value for '%s', arg '%s' is not legal\"\n\t\t\t\t\t\t\t\t % (parser_name, k, v))\n\telse:\n\t\traise ValueError(\"Unrecognized argument '%s'.\" % (\n\t\t\tparser_name + '=' + str(values_dict)))\n\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor key,value in sorted(values_dict.items()):\n\t\tif isinstance(value, list):\n\t\t\tnew_value = [self._convert_non_cli_args(parser_name, {key:value}) for value in value]\n\t\t\tvalues_dict.pop(key)\n\t\telse:\n\t\t\tvalues_dict[key] = self._convert_non_cli_args(parser_name, {key:value})\n\t\tdel values_dict[key]\n\ndef command_init(self, c=None):\n\t\"\"\" Initializes the command"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tresult = {}\n\tnames = os.listdir(plugin_dir)\n\tfile_name = \"plugin_spec_flatten.yaml\"\n\twith open(plugin_dir +\"/\"+ file_name) as spec_file:\n\t\tfor line in spec_file:\n\t\t\tname = line.strip()\n\t\t\tif name == \"type\":\n\t\t\t\tresult[\"type\"] = line.strip()\n\t\t\telif name == \"name\":\n\t\t\t\tresult[\"name\"] = line.strip()\n\t\t\telif name == \"version\":\n\t\t\t\tresult[\"version\"] = line"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\timport yaml\n\tpaths = os.listdir(plugin_dir)\n\tflattened_dict = {}\n\tfor path in paths:\n\t\tif path.endswith('.yaml'):\n\t\t\tflattened = yaml.load(open(plugin_dir+path))\n\t\t\tflattened_dict.update(flattened)\n\treturn flattened_dict\n\n\ndef get_plugin_spec_flatten_list(plugin_dir):\t\"\"\"\n\tCreates a list from the plugin's dir\n\n:param plugin_dir: A path to"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\treturn {p: p.spec.dict for p in os.listdir(plugin_dir)]\n\ndef get_plugin_spec_get_prop_str(plugin_dir): \t\t# Gets property str from plugin with given name\n\n\treturn os.listdir(plugin_dir)[os.listdir(plugin_dir)[-1]].split('.')[-1]\t\t# Extract the plugin property name\n\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tflatten_dict = {}\n\n\ttry:\n\t\tos.makedirs(\"plugin_spec\", exist_ok=False)\n\texcept OSError:\n\t\tpass\n\tdir_name = os.path.dirname(plugin_dir)\n\n\tfor file in os.listdir(\"%s/plugins\" % dir_name):\n\n\t\tif os.path.isfile(file):\n\t\t\tbase = basename(os.path.splitext(file)[0])\n\t\t\tplugin_name = \"%s/plugins/%s\" % (dir_name, base"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\treturn dict(flatten(plugin.flatten_dict()))\n\n@pytest.fixture(scope = 'function')\ndef plugin_dir():\n\treturn os.path.join(plugin_dir_path, \"plugins\")\n\n@pytest.fixture(scope = 'function')\nclass MockRequest:\n\tdef __init__(self, status_code, request_body):\n\t\tself.status_code = status_code\n\t\tself.request_body = request_body\n\n@pytest.fixture()\ndef mock_get_plugin_spec"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tret = {}\n\ttry:\n\t\tfor prop in [\"pluginId\", \"parentId\", \"name\", \"description\",\n\t\t\t\t\t\"source\"]:\n\t\t\tret [prop] = get_flatten_parameters ( plugin_dir, prop)\n\texcept Exception as e:\n\t\treturn ret\n\treturn ret\n\n\ndef get_plugin_spec_unflatten_dict(plugin_dir):\t\"\"\"\"\n\tCreates a dict of unflatten plugin properties\"\"\"\n\tflatten_pdict = get_plugin_spec_flatten_dict ( plugin_dir)\n\tret"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\treturn flatten_dict(plugin_spec_flatten_dict(open(os.path.join(plugin_dir, 'plugin-spec.yaml'))))\n\t\n\t\ndef get_plugin_spec_flatten_file(plugin_dir):\t\"\"\"\n\tCreates a flat dictionary from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: The plugin's spec dict\n\n:throws: FileNotFoundError, if the file doesn't exist\n:raises: Exception if any other exception is raised\n\"\"\"\n\tplugin"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tflatten_dict = {}\n\tflatten_dir = plugin_dir + '/flatten'\n\tflatten_results = os.listdir(flatten_dir)\n\tfor f in flatten_results:\n\t\tflatten_dct = flatten_dict = {}\n\t\tflatten_dct = {}\n\t\tflatten_keys = ['module', 'function', 'name']\n\t\tflatten_dict = {}\n\t\tflatten_dict[f[0] + '_' + f[1]} = f[2:]\n\t\tflatten_key = f"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n\tflatten_dict = dict()\n\t\n\tfor config in os.listdir(plugin_dir):\n\t\tif \".yaml\" in config:\n\t\t\tcontinue\n\t\t\t\n\t\ttry:\n\t\t\tconfig_value = get_config_content(config)\n\t\t\t\n\t\t\tif isinstance(config_value, list):\n\t\t\t\tflatten_dict[config] = flatten_dict.get(config, []) + flatten_dict.get(config, []) + config_value\n\t\t\telse:\n\t\t\t\tflatten_dict[config] = flatten_dict."}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\ttry:\n\t\tf = open(os.path.join(plugin_dir, 'props'), 'r')\n\t\tspec = json.loads(f.read())\n\t\tf.close()\n\t\treturn spec\n\texcept:\n\t\treturn {}\n\n\ndef get_plugin_config(plugin_path):\t\"\"\"\n\tA quick way to get the config for a plugin\n\n\t:param plugin_path: The plugin path we're looking for\n\t:return: A dict mapping properties to values for the plugin\n\t\"\"\"\n\tf = open(plugin_path, 'r')\n"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.env_isTrue(\"COMMON_CONFIG\"):\n\t\tself.env_setenv(\"COMMON_CONFIG\", self.env_path())\n\t\tprint \"The value of this environment variable is : %s\" %  self.env_value(\"COMMON_CONFIG\")\n\treturn\n\t\ndef get_config():\n\treturn self.ini_path()"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tenv_value = os.environ.get('CONFIG', '')\t\t\t##########\n\tenv_value = env_value.rstrip('$')\t\t\t############################\n\tif env_value[1:]:\n\t\treturn env_value\n\tenv_value = \"\"\t\t\t\t\t\t############################\n\treturn env_value\n\t#--------------------------------------------------------------------------\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tglobal config_path\n\tif config_path == None:\n\t\t\n\t\tglobal CONFIG_PATH\n\t\tconfig_path = CONFIG_PATH\n\t\tprint \"config_path\"\n\t\t\n\tself.config_path = config_path\n\t\t\n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tlogging.debug('Setting config path for $SNATB', file=sys.stderr)\n\tif not self.config_path:\n\t\tself.config_path = \"./etc/snatb.cfg\";\n\ndef main():\n\t\"\"\"\n\t\"\"\"\n\tsnatb_conf_path = '/etc/snatb.cfg'\n\tconfig_dir = configparser.ConfigParser()\n\tconfig_dir.read(snatb_conf_path)\n\tlogging.debug(config_dir.parser.sections())\n\tlogging."}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tglobal configPath\n\t\n\t# The default is to use $CONFIG_DIR\n\tif not configPath in DEFAULT_CONF_DIRS and not self.is_windows():\n\t\t# The default is to use $CONFIG_DIR for linux, to allow.bash to set $CONFIG_DIR\n\t\tconfigPath = confdirs.confdir_globals.conf_dir[0]\n\t\t\n\t# Now we do:\n\tif not configPath:\n\t\tif self.is_windows(): \t\t\t\n\t\t\tconfigPath = os.path.join("}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tself.__configPath = self.config.get('general', 'configPath', '.')\n\tself.__configPath = os.path.expandvars('$HOME/{0}'.format(self.__configPath))\n\tself.__sysPath = self.config.get('general','sysPath', '')\n\tself.__sysPath = os.path.expandvars('$HOME/{0}'.format(self.__sysPath))\n\tself.__sitePath = self.config.get('general','sitePath', ''"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif os.environ.get(\"CONFIGPATH\") == None:\n\t\tos.environ[\"CONFIGPATH\"] = \".\"\n\n\treturn True\n\n\n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t_config_path = self.get(\"config:path\", \"./config\").expand(lambda x: x)\n\tif _config_path!= \"\":\n\t    _env_variables[\"CONFIG_PATH\"] = _config_path\n\treturn\n\n\t\n\t\n\n####################################################################################################################################################\n# class: Config\n####################################################################################################################################################"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif not CONFIG_PATH:\n\t\tself.env('CONFIG_PATH', os.getenv('PATH'))\n\t\tlog.debug('Path: %s', os.getenv('PATH'))\n\telse:\t\t\n\t\tlog.debug('Path: %s', CONFIG_PATH)\n\t\tlog.debug('Define PATH: %s', self.env('CONFIG_PATH', ''))\n\t\tCONFIG_PATH = self.env('CONFIG_PATH', '')\n\t\tlog.debug('Config path: %s', CONFIG_PATH)\n\t\tlog.debug(\""}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tglobal config_path, environment, DEBUG\n\tif DEBUG == \"False\":\n\t\tconfig_path = self.config_path.getvalue()\n\t\t#self.__write_envn()\n\t\tif config_path is False:\n\t\t\tconfig_path = []\n\t\tenvironment = os.environ.copy()\n\t\tif DEBUG in [\"True\", \"False\"] and DEBUG == \"False\":\n\t\t\t#print(\"Config Path: %s\" % config_path)\n\t\telse:\n\t\t\tenvironment = os.environ\n\telse:\n\t\tconfig_path = self.config_"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\n\tparser_name = root_subparsers.__name__\t\n\tmain_parser = self.add_parser(parser_name, description=self.summary, \n\t   formatter_class=argparse.ArgumentDefaultsHelpFormatter, \n\t   description=self.summary, prog=self, \n\t   formatter_class=self.formatter, \n\t   conflict_handler=self.resolve_conflict)\n\t\n\tmain_parser.add_argument('-s','--shell',\n\t   type=str"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tdef add_cli(self, arg):\n\t\t\"\"\"\n\t\tAdds command-line options to the subparser\n\t\t:param arg: the option's path\n\t\t:type arg: :class:`Argument` class\n\t\t\"\"\"\n\t\tdef cls_init():\n\t\t\tsetattr(root_subparsers, arg.name, cli_opts(arg))\n\t\treturn cls_init\n\nCLI_OPTS = add_cli(cl)\n\nCLI = CLI_OPTS()\nCLI.register('update', update_cli)\nCLI"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tself._root = self.root_subparsers.add_parser(name=\"spec\", add_help=False, parents=[root_subparsers])\n\tfor opt,desc in self.add_cli_options:\n\t\tself._root.add_argument(\n\t\t\tdescription=desc,\n\t\t\ttype=optType,\n\t\t\trequired=True,\n\t\t)\n\ndef extend_cli_plugins(root_subparser):\n\t\"\"\"\n\tAdds the plugins cli options to to the main entry point.\n\t\"\"\"\n\tplugin_clinishars = ["}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tself.getcliparser(\n\t\tsubparser=subparser,\n\t\tparser_args=root_subparsers.add_parser_argument_group('cli'))\n\ndef get_defaults():\n\treturn {\n\t}\t\n\ndef getcliparser(\n\tsubparser=None,\n\tparser_args=None\n):\n\treturn clipparser_common.Clipparser_default(None, subparser, parser_args)\n\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tfrom subcmd.spec import CommandLineParser\n\n\treturn CommandLineParser(root_subparsers).setup_opt(self.values)\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tfor name, short, description in self.options.items():\n\n\t\tdesc = self.cli_option_desc(name)\n\t\t\n\t\tdef extend():\n\t\t\tsubcmd = subparser.add_parser(\n\t\t\t\t\tname,\n\t\t\t\t\tformatter_class=specs_cli,\n\t\t\t\t\thelp=description)\n\t\t\tdesc = self.cli_option_desc(name)\n\t\t\t\n\t\t\tsubparser.set_defaults(subcmd=extend, desc=desc)\n\t\t\n\t\textend()  \n\n\tfor name in self.config.get('plugins.speccli',"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\troot_parser = root_subparsers.add_parser(\n\t   'cli',\n\t   help='the cli for all the clis')\n\troot_parser.set_defaults(extend=extend_cli)\n\troot_subparsers.add_parser(\n\t   'config',\n\t   help='config files')\n\troot_subparsers.add_parser(\n\t   'env',\n\t   help='environment files')\"\"\"\n\tprint('Running cli in the parent'+ str(self))\t\n\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tself._subparser_factory.extend(root_subparsers, parent=self,\n\t\t\t\t\tcommand=\"SpecCli\")\n\n\nclass AddSpecOpts(cmd.CmdOpts):\n\t\"\"\"\n\tAdds the spec options to the main entry point.\n\n:param str command: the command to execute (must start with'spec')\n\"\"\"\n\tdef add_subparsers(self, **kwargs):\n\t\tself.add_subparsers(**kwargs)\n\n\ncli = AddSpecCli(\n\tcmd.ArgumentParser(),"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\troot_subparser = root_subparsers.add_parser(\n\t\toption_class=self.opts.get(\"speccli\", \"Opts\"),\n\t\tdescription=self.opts.get(\"speccli\", \"Description\"),\n\t\thelp=self.opts.get(\"speccli\", \"Help\"),\n\t\tusage=self.opts.get(\"speccli\", \"Spec\"),\n\t\tdest=self.speccli_opts)\n\tc = root_subparser.add_argument_group(\"cli options\", **self.opts)\n\tc.add"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\n\t#\n\t# Register the options for this argparse.\n\t#\n\tfor o, w in {'config_file':  '-c',\n\t\t\t  'config_dir':   '-d'.split() \n\t           }.items():\n\t\tif o not in self.__dict__.keys():\n\t\t\traise argparse.ArgumentTypeError(\"{0} option not recognized!\".format(o))\n\t\toption = SubparserWithHelp(w)\n\t\troot_subparsers.add_argument(name=o, \n\t\t\t\t\t\t\t       help=option.short"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif  path[0] == root :\n\t\tpath= [path[1]]\t\n\telse :   \n\t\tpath = filter(lambda p: p!= root and p!='..',path)\n\t\treturn path\n# end def strip_root\n\tif not path :\treturn ''\n\telse :\n\t\treturn path[0]\n## end def __str__\n# end class _Path\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tresult = False\n\tsubc = re.compile(path, re.IGNORECASE+re.DOTALL)\n\twhile subc.search(root):\n\t\tcurr = subc.search(root).find()\n\t\tbase,sub = subc.sub('', root, 1)\n\t\t#print (path) + \" \" + base\n\t\tsub = [re.sub('[-\\/]', '', s) for s in sub]\n\t\tsubc.sub(base, root)\n\t\tsubc = ''.join(sub)\n\t\tresult"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\ttry:\n\t\tos.unlink(os.path.join(root, os.path.basename(path)))\n\texcept OSError:\n\n\t\traise NameError(\"NameError while removing root=${root} from ${path}\")\n\n\ndef strip_trailing_parent(path):\t\t\t\"\"\"\n\tRemove root from path, throw exception if path is not empty, and return the final path as it was.\t\"\"\"\n\tif not path:\n\t\traise ValueError(\"Path is empty! Did you mean to strip the root?\")\n\treturn os.path.normpath"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif  not isinstance(root, basestring):\n\t\traise TypeError(\"root must be an instance of basestring\")\n\tpath = path.lstrip('/')\t# strip leading slash\n\n\tres = []\n\twhile True:\n\t\tif path == \"\":\n\t\t\tbreak\n\t\tres.append(os.path.join(path, _seg))\t# prepend to list\n\t\ti = path.find(\"/\")\n\t\tif i == -1:\n\t\t\traise OSError(\"Invalid path: %r\" %path)\n\t\tpath = path"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\treturn os.system(\"strip_root %s\" % path)\n\ndef strip_base(path, base):\n\t\"\"\"\n\tRemove base from path, throw exception on failure.\t\"\"\"\n\treturn os.system(\"strip_base %s\" % path)\n\ndef strip_file(path, file):\n\t\"\"\"\n\tRemove file from path, throw exception on failure.\t\"\"\"\n\treturn os.system(\"strip_file %s %s\" % (path, file))\n\n\n\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tpath_len = len(path)\n\tif path_len and not root and not path[0] == path[-1]:\t# common case.\n\t\tpath = path[1:]\n\telse:\n\t\ti = root - 1\n\t\twhile path_len and i > 1 and path[i - 1] == path[i]:\n\t\t\ti = i - 1\n\t\tif i == 1 and not i == root:\t# short circuit.\n\t\t\ti = 0\n\t\trest = path[i:]\n\t\tpath = rest.strip()\n\t\tif not path:\n\t\t\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif not path:\n\t\treturn path\n\tif not (path[0].path.beginning == path[-1].path.end):\n\t\treturn path\n\tif not path:\n\t\treturn [path]\n\tp = path[0]\n\twhile len(path) > 1:\n\t\tif path[-1].path == [p]:\n\t\t\tp = path[-1]\n\t\telif path[-2].path == [p]:\n\t\t\tp = path[-2]\n\t\telse:\n\t\t\tbreak\n\tif p == root:\n\t\tp ="}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\n\ttry:\n\t\timport shutil\n\t\tshutil.rmtree(root)\n\texcept ImportError, e:\n\t\t# no such folder or module\n\t\tpass\n\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif  os.path.isdir(path) == True:\n\t\tos.rmdir(path)\n\t\treturn None\n\treturn path\n\n\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tfor root_name in path.split('/'):\n\t\tif root_name == '' and not path.ends_with(root):\n\t\t\tpath.parent.mkdir()\n\t\telse:\n\t\t\tpath = path.replace(root_name, '')\n\treturn path\n\n"}]}
