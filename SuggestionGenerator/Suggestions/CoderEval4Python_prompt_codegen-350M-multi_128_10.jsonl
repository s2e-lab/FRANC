{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz is not None:\n\t\treturn tz\n\telif nanoseconds is None:\n\t\traise ValueError(\"nanoseconds is needed to hydrate time\")\n\telse:\n\t\treturn Time(*nanoseconds)\n#----------------------------------------------------------------------------------\n#----------------------------------------------------------------------------------\n#----------------------------------------------------------------------------------"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tt = rtime.Time(nanoseconds, tz=tz)\n\treturn t\n\n\ndef get_time(n):\n\t\"\"\"\n\tGets the `Time` given a `n`-th value out of the `n`-th position.\n\t\n\t:param n:\n\t:return: time\n\t:raise IndexError: if `n` is invalid\n\t\"\"\"\n\tif n < 0:\n\t\traise IndexError(\"'n' must be a positive integer\")\n\treturn rtime.Time(n)\n\ndef local_to_"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tyear = str(int(nanoseconds)) + '{0}'.format(str(int(t(nanoseconds)) - tz.utcoffset(nanoseconds), 16))\n\treturn t.Time((year, t.mtime(nanoseconds)))\n\ndef validate_time(time):\n\t\"\"\"Validates a time.\n\t\n\t:param time:\n\t:return:\n\t\"\"\"\n\treturn time.tz_localtime()\n\n\ndef to_time(local):\n\t\"\"\"Converts a localtime-style time"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn as_cls(Float64(nanosectres=nanoseconds) if tz is None else as_cls(Time(*nanoseconds, tz=tz)))\n\n\nclass Instant(NanoHated):\n\t\"\"\"\n\tInstantiates a :class:`Time` using `isoformat()` so the instance doesn't need to be fully UTC.\n\tThat is all that is needed to be able to pass nanoseconds when calling `isoformat().\n\t\n\t:param time_str:\n\t\"\"\"\n\tdef __new__(cl"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(t=nanoseconds, tz=tz)\n\n\ndef check_type(value):\n\t\"\"\"\n\tUsed by the `hydrate` parameter to check that a string is a time string.\n\t\n\t:param value:\n\t:return: `True` if the string is a valid time string\n\t\"\"\"\n\ttry:\n\t\tTime(t=float(value))\n\texcept TypeError:\n\t\treturn False\n\treturn True\n\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\td = nanoseconds / tz\n\ttdt = tsc.monotonic_time\n\treturn LocalTime(tdt / tz * d)\n\ndef read_time(timestamp):\n\t\"\"\"\n\tHydrator for `Time` values.\n\t\n\t:param str timestamp:\n\t:return:\n\t\"\"\"\n\tms = np.datetime64(timestamp)\n\tdt = tsc.monotonic_time.get_real()\n\tms = ms + tdt - tsc.millisecond_timestamp - tsc."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tt = pytread.LocalTime_localtime(nanoseconds)\n\tif tz!= None:\n\t\tt = pytread.LocalTime_to_UTC(t, tz)\n\treturn t\n\n#---------------------------------------------------------------------------------\ndef hydrate_localtime(nanoseconds, tz=None):\n\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tt = pytread.LocalTime"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz == None:\n\t\ttz = tzlocal()\n\treturn Time(nanoseconds // 1000000, tz)\n\n\ndef hydrate_tzof_utc(date, tz=None):\n\t\"\"\"\n\tHydrates a :class:`TimeOfDayUTC`, :class:`OffsetUTC`,\n\t:class:`OffsetLocalTimeUTC`, :class:`OffsetLocalTime` of a :class:`Date`\n\twith a time offset with a :class:`tz` specified that is associated\n\twith UTC.\n\t\n\t.."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn tz(nanoseconds).localize(nanos, as_subsecond=False).replace(tzinfo=None)\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t# TODO: support for time.Time?\n\tif isinstance(nanoseconds, time.Time):\n\t\treturn nanoseconds\t\n\telif isinstance(nanoseconds, LocalTime):\n\t\treturn nanoseconds.cast(tz or time.tzname)\n\telse:\n\t\traise NotImplementedError('unknown type {} for `Time` and `LocalTime`'.format(type(nanoseconds)))\n\t\t\t\n\t\n\t\n\t\n\t\n"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.days*24*60 + value.seconds.total_seconds()\n\ndef dehydrate_int(value):\n\t\"\"\"\n\tDehydrator for `int` values.\n\t\n\t:param value:\n\t:type value: int\n\t:return:\n\t\"\"\"\n\treturn value\n\ndef get_queryset_from(queryset):\n\t\"\"\"\n\tCreates a queryset instance for a given queryset\n\n\t:param queryset:\n\t:return:\n\t\"\"\"\n\treturn queryset\n\n\nif __name"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif isinstance(value, datetime):\n\t\treturn value"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.total_seconds()\n\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\tvalue = value.total_seconds()\n\tdate = calendar.timegm(value.timetuple())\n\treturn datetime.fromtimestamp(date)\n\t"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn datetime.timedelta(value.days*1.0/60.0, value.seconds, value.microseconds)\n\nDEHT = {\n    'dehydration': dehydrate_timedelta,\n    'dehydrate': default\n}"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif not (isinstance(value, timedelta) or value.days == timedelta.days):\n\t\traise ValueError('Timedelta cannot be dehydrated to a timedelta. You provided '\n\t\t'`{}`.'.format(value))\n\t\n\tif not (isinstance(value, timedelta) or value.microseconds % 1000 == 0):\n\t\traise ValueError('Timedelta cannot be dehydrated to a timedelta. You '\n\t\t'provided `{}`.'.format(value))\n\t\n\tif value.days == timedelta."}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif value.microsecond!= value.seconds + int(value.microseconds):\n\t\traise ValueError\n\treturn (value - datetime.timedelta(microseconds=value.seconds))\n\n@method_decorator(cleanup, weakref=True)\nclass BaseCePCounterView(CePCounterView):\n\t\"\"\"\n\tCelery worker counter view decorator.\n\t\n\t:param celery:\n\t:type celery: celery.Celery\n\t:return:\n\t\"\"\"\n\tdef __init__(self, cel"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value\n\t\n\t\n\t\n\t"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, timedelta) or value < timedelta(0):\n\t\traise TypeError \n\treturn value\n\ndef generate_id(start=None, end=None):\n\t\"\"\"\n\tGenerate a unique ID\n\t\n\t:param start:\n\t:type start: datetime\n\t:return:\n\t\"\"\"\n\ts = \"\"\n\tp = \"\"\n\td = \"\"\n\tn = 0\n\tfor _ in range(start == None or end == None and start == None or end):\n\t\ts = 'T' + s +"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n\ndef convert_time_to_utc(value):\n\t\"\"\"\n\tConvert `time` values to UTC's.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\timport datetime\n\treturn datetime.strptime(value, \"%Y-%m-%dT%H:%M:%STZ\").replace(tzinfo=utc)\n\n\ndef convert_time_"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif isinstance(value, Time):\n\t\treturn value.astimezone(tz=pytz.UTC)\n\telse:\n\t\treturn value\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif value.tz.utcoffset(value.value) is not None:\n\t\t# value is in UTC\n\t\treturn timezone_from_utc(value.tz.utcoffset(value.value), value.tz)\n\treturn value"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value\n\t\n#\n### Time Field - DEPRECATED ###\n#\ndef _get_field_value(value, field):\n\t\"\"\"\n\t:param value:\n\t:type value: String\n\t:param field:\n\t:type field: Field\n\t:return:\n\t\"\"\"\n\tfield.dehydrate_time(value)\n\treturn value\n\ndef dehydrate_isoformat(value):\n\t\"\"\"\n\tDehydrator for ISO-Format.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tpass\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif isinstance(value, datetime):\n\t\ttime = '%s-%s-%s' % (\n\t\t\t\tvalue.year,\n\t\t\t\tvalue.month,\n\t\t\t\tvalue.day)\n\t\treturn time\n\telse:\n\t\n\t\treturn value\ndef get_time(obj):\n\t\"\"\"\n\tGet object time in `time` property.\n\t\n\t:return:\n\t\"\"\"\n\tif 'time' in obj.__dict__:\n\t\treturn obj.time\ndef get_date(obj):\n\t\"\"\"\n\tGet object date in `date`"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tnow = value.as_datetime()\n\tdelta = pbtypes.diff(value.value, now)\n\tdehydrated = now + pbtypes.day(delta)\n\treturn dehydration(dehydrated, value)\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\tif hasattr(value, 'isoformat'):\n\t\treturn value\n\telse:\n\t\treturn value.isoformat()\n\t\n\t\n\t\ndef dehydrate_date(value):\t \n\t\"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value:\n\t:return:\n\t\"\"\"\n\t\n\treturn value.isoformat()\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.format()\n\ndef dehydrate_times(value):\n\t\"\"\"\n\tDehydrator for `times` values.\n\t\n\t:param value:\n\t:type value: list[Time]\n\t:return:\n\t\"\"\"\n\treturn value\n\ndef dehydrate_list(data, attr):\n\t\"\"\"\n\t\tDehydropment of a list \n\t\t\n\t:param data:\n\t:type data: list[dict]\n\t:param attr:\n\t:type attr: list[str]\n\t:return:\n\t"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value\n\ndef point_to_nodal(point):\n\t\"\"\"\n\tConverts a ``Point`` object to a ``NodalPoint`` object.\n\t\n\t:type point: :py:class:`Point`\n\t:return:\n\t\"\"\"\n\treturn NodeModel(\n\t\tNone,\n\t\tNone, \n\t\tNone, \n\t\tvalue=point.value, \n\t\tgeometry=point.geometry)"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.copy().__to__()\n\ndef extract_numpy_array(input_array):\n\t\"\"\"\n\tExtracts a `array` from a `numpy`darray.\n\t\n\t:param input_array:\n\t:type input_array: numpy `numpy`darray\n\t:return: `array`\n\t\"\"\"\n\tret = numpy.concatenate([numpy.asarray(input_array[:index],dtype=complex) for index in range(len(input_array))])\n\treturn ret."}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.to_dict()\n\n\ndef get_dehydrated_point(value, **kwargs):\n\t\"\"\"\n\tFunction used to return a deep copy of a value in the desired format.\n\t\n\tCan be used for things like providing an API to add an annotation to a Point, or\n\tdehydrating a Point where it was already in the appropriate format (with values set).\n\t\n\t:param value: Value to dehydrate\n\t:type value: Point\n\t:param kwargs: Metadata to extract from the point\n\t:type kwargs"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.m0\n\ndef dehydrate_geo_point(*args):\n\t\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param *args:\n\t:type args:\n\t:return:\n\t\"\"\"\n\tfrom geopy.point import Point\n\treturn Point(*args)\n\ndef has_geo_point(value):\n\t\"\"\"\n\tCheck whether a value has geopy-centric geo-point.\n\t\n\t:param value:\n\t:type value:\n\t:return:\n\t\"\"\"\n\treturn isinstance(value"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.value if isinstance(value, Point) else value\n\n\ndef dehydrate_point_dict():\n\t'''\n\tDehydrator for Point's data.\n\t\n\t:return:\n\t\n\t'''\n\treturn dict((\n\t\tdict((\n\t\t\t('x_value', dehydrate_point),\n\t\t\t('z_value', dehydrate_point_filter),\n\t\t\t('tuple_value', dehydrate_point_filter),\n\t\t)), key)\n\tfor key in ['x_value', 'z_value', 'tuple_value']\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.magnitude\n\ndef dehydrate(response, resource_name):\n\t\n\tif hasattr(response, 'pks'):\n\t\tdehydrated = list(response.pks)\n\t\t\n\telif hasattr(response, 'objects'):\n\t\tdehydrated = list(response.objects)\n\t\t\n\telse:\n\t\tdehydrated = response\n\t\t\n\tdehydrated = dehydrated.json()\n\tdehydrated = dehydrated.json()\n\tdecoded = json.loads(dehydrated)\n\t\n\tfor"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\treturn str(value.x) + \". \" + str(value.y)\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value\n\n@pytest.fixture\ndef time():\n\ttime = Time.now(localtime)\n\treturn time\t\n\t\n\t\n@pytest.fixture\t\ndef base_dehydrate(h5py):\n\twith h5py.File(h5py.example_data_path('base_dehydrate'), 'w') as f:\n\t\tf.create_group('base_dehydrate')\n\t\tf.create_group('time')\n\t\tf['base_dehydrate']['time'] = time\n\t\t\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.magnitude\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn  {\n\t\t'user': user,\n\t\t'points': value.user_points\n\t}\n\t\n\t"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn dir(*vars()).keys()\n\n\t\ndef dir(*args):\n\t\"\"\"Get the directory for an object in the module\n\n\t:param args:\n\t\t1) The object in question.\n\n\t\t2) The arguments for dir.\n\n\t:raise DontFindError:\n\t\tIf the object does not have the keyword in_module.\n\t\"\"\"\n\tif args[0] in self.__modules__:\n\t\treturn self.__modules__[args[0]]\n\traise DontFindError\n\n\ndef hasattr(obj, attr):\n\t\"\"\"\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._data.keys()\n\n\t\n\tdef get(self, key):\n\t\n\t\treturn self._data.get(key, None)\n\n\t\n\tdef iteritems(self):\n\t\n\t\tfor key in self.keys():\n\t\t\tif self.has_key(key):\n\t\t\t\tyield (key, self[key])\n\t\n\t\n\tdef iterkeys(self):\n\t\tfor key in self.keys():\n\t\t\tif self.has_key(key):\n\t\t\t\tyield key\n\n\t\n\tdef itervalues(self):\n\t\tfor"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.data.keys()\n\n\t# FIXME: does this need this? It should be possible to get a list of the\n\t# keys as a python iterator.\n\tdef values(self):\n\t\treturn self.data.values()\n\t\n\tdef items(self):\n\t\treturn self.data.items()\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\t\n\tdef __getitem__(self, key):\n\t\treturn self.data.__getitem__(key) if key in self.data else None\n\n\tdef"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._kwargs['keys']\n\t\ndef fields(self):\n\treturn self._kwargs['fields']\n\t\ndef values(self):\n\treturn self._kwargs['values']\n\t\ndef required_keys(self):\n\treturn self._kwargs['required_keys']\n\t\ndef required_fields(self):\n\treturn self._kwargs['required_fields']\n\t\ndef required_values(self):\n\treturn self._kwargs['required_values']\n\t\ndef data(self):\n\treturn self._data\n\t\ndef"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tk = {}\n\n\tfor k_v in self.__records.items():\n\t\tk = k.get(k_v.name, k_v.name)\n\n\treturn k\n\t\ndef get(self, val):\n\t\n\tl = self.keys()\n\tl = l.sort()\n\n\ttry: return self.__records[l.take(3)][1].get(val)\n\texcept KeyError: return None\n\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn list(self._key_dict)\n\n\t\ndef to_python(self):\n\t\"\"\"\n\tConvert this record into an L{Record}.\n\t\"\"\"\n\treturn Record(self._key_dict.values())\n\t\n\t\ndef from_json(self, raw=False):\n\t\"\"\"\n\tReturn a record from a JSON string.\n\t\t\n\tIf ``raw`` is ``True``, return a record; else return an empty\n\trecord.\n\t\n\t:return: record\n\t\"\"\"\n\tif raw == True:\n\t\tret = Record({"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tif not self._record:\n\t\treturn ()\n\tkeys = list(list(self._record.keys()))\n\treturn Keys(keys)\n\ndef get(self, key):\n\t\"\"\"\n\tGet a value from the record.\n\n\t:param key: key name\n\t:return: value\n\t\"\"\"\n\tif self._record and self._record[key]:\n\t\tkey_val = self._record[key]\n\telse:\n\t\tkey_val = NoValue()\n\treturn key_val\n\ndef __str__(self):\n\treturn str("}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn [key for key in self._map if key not in ['_id', '_v']]\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn [record_key for record_key in self.keys]\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn [ r[0] for r in self.records ]\n\n\t\n\t\nif __name__ == \"__main__\":\n\t\n\tfrom kosmpipe import main as main\n\t\n\ttest = main.Run(\"TEST_OUTPUT.csv\")\n        \n\tos.system(\"cp \" + TESTDIR + \"/OUTPUT.csv \" + STORE_PREFIX + \"/\")\n\t\n\t# read in the records\n\trecords = open(TESTDIR + \"/OUTPUT.csv\").readlines()\n\t\n\t# output"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandler_classes = {}\n\tif protocol_version is None:\n\t\tprotocol_version = tuple([ (int(i),i) for i in protocol_version ])\n\tprotocols = [ (protocol_version[0], self.protocol) \\\n\t\t\tfor protocol_version in protocol_version]\n\tfor protocol in protocols:\n\t\t# Make sure the protocol version is the right one\n\t\tif protocol in protocol_version:\n\t\t\thandler_classes[protocol_version] = self\n\treturn handler_classes\n\n# Make sure that every version in protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\treturn {\n\t\t('boltd') : BoltDHandler,\n\t\t('bolt3d') : Bolt3DHandler,\n\t\t('boltzmann') : BoltZMGHandler,\n\t}[protocol_version]\n\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\td = {}\n\tif protocol_version is None:\n\t\tfor v in PROTOCOL_VERSIONS:\n\t\t\td[v] = cls\n\telse: # protoc_version is not None\n\t\tif (protocol_version not in\n\t\t\t(zero, one, one_byte, two_bytes, zero_byte, one_byte_no_tag,\n\t\t\ttwo_byte_no_tag, non_protocol_version, None)):\n\t\t\traise TypeError(\"Unknown protocol version: %r\" % protocol_version)\n\t\td[protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers= {}\n\t\n\tdef parse_handler(request):\n\t\t\"\"\"\n\t\tReturn the handler class for the specified request path.\n\t\t\n\t\t:param request: request path\n\t\t:return: class or a subclass of BoltHandler\n\t\t:raise TypeError: if request path is not valid\n\t\t\"\"\"\n\t\tif isinstance(request[1], tuple):\n\t\t\tversion= request[1]\n\t\t\ttry:\n\t\t\t\tversion.index(None)\n\t\t\texcept ValueError:\n\t\t\t\traise TypeError(\"Called with invalid protocol version\")\n\t\t\tprotocol_hand"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tfrom..common import _protocol_version as protocol_version\n\t\tfor _protocol_version in protocol_versions.keys():\n\t\t\t_protocol = _protocol_versions[_protocol_version]\n\t\t\tif protocol_version is not None and protocol_version!= _protocol_version:\n\t\t\t\tcontinue\n\t\t\treturn {protocol_version: _protocol.protocol}\n\t\traise TypeError('Unknown protocol version specified')\n\t\n\t_protocol_versions = protocol_versions.keys()\n\t_protocol = protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tif protocol_handlers > 0:\n\t\t\treturn dict((protocol, type)\n\t\t\t\tfor protocol, type in dict((protocol, cls)\n\t\t\t\t for protocol in protocol_handlers\n\t\t\t\t if issubclass(type, cls)\n\t\t\t\t ).items())\n\t\telse:\n\t\t\traise TypeError(\"protocol_version not specified\")\n\telse:\n\t\treturn protocol_handlers\n\treturn {}\n\nif __name__ == '__main__':\n    from ctypes import *\n    import sys\n    \n    debug"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tversions = list(BoltProtocolVersions())\n\t\tif len(versions)!= 1:\n\t\t\tmsg = []\n\t\t\tfor index in range(0, len(versions)-1):\n\t\t\t\tmsg.append(versions[index]+VersionsDontSupport(versions[index+1]))\n\t\t\tmsg = sorted(reversed(msg))\n\t\t\traise TypeError(\"Supported protocol versions not supplied with a protocol version tuple\")\n\telse:\n\t\tversions = tuple(BoltProtocolVersions())\n\n\treturn versions\n\t\nclass BoltProtocolVersion"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None:\n\t\tprotocol_versions = protocol_version\n\telse:\n\t\tprotocol_versions = ProtocolVersion.protocol_list()\n\treturn dict((proto_version, c for (c, proto_version) in\n\t\tprotocol_versions if proto_version == protocol_version))\n\ndef protocol_handlername(protocol_version):\n\t\"\"\"\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t    \n\t:return: name of Bolt"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = {}\n\t\n\tif protocol_version is not None and protocol_version not in (None, ()):\n\t\tlog.debug(\"Bolt protocol version %s found. Supported versions are: %s\", protocol_version, tuple(protocol_handlers.keys()))\n\t\tfor tuple_version, handler_cls in protocol_handlers.items():\n\t\t\tif tuple_version not in protocol_version:\n\t\t\t\tprotocol_handlers[tuple_version] = handler_cls\n\t\n\treturn protocol_handlers\n\t\nclass Bolt"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tproto = [i for i in protocols.itervalues() if i.protocol_version == protocol_version]\n\tif len(proto) == 0:\n\t\traise TypeError('invalid protocol_version %s' % protocol_version)\n\tresult = {}\n\tfor proto_cls in protocols.get(protocol_version, []):\n\t\tproto.remove(proto_cls) # XXX I don't think it's necessary to remove this before I write it\n\t\tresult.update(proto_cls.protocol_handlers())"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef trans_wrap(func):\n\t\tif metadata is None:\n\t\t\tmetadata = {\"timestamp\": tz.now()}\n\t\t\n\t\t@wraps(func)\n\t\tdef wrapped(*args):\n\t\t\tmetadata[\"timestamp\"] = tz.now()\n\t\t\targs = [None] + tuple(args)\n\t\t\treturn func(*args, **metadata)\n\t\treturn wrapped\n\treturn trans_wrap\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef wrap(func):\n\t\treturn _wrapped_transaction_func(func, metadata)\n\treturn wrap\t\n\n\n\n\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tstarted = time.time()\n\t\t\ttry:\n\t\t\t\ttime.sleep(timeout)\n\t\t\t\tf(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\t# if transaction times out and this gets caught, it means someone (or a worker) has terminated the transaction or ran into an exception causing the transaction to fail, the transaction probably needs an error message.\n\t\t\t\t# This means for this transaction we need to store the failure message for the next time"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\tdef run(fn):\n\t\t@wraps(fn)\n\t\tdef inner(_self, _metadata, *args, **kwargs):\n\t\t\ttime_limit = timeout or kwargs.pop('timeout', None)\n\t\t\tif time_limit is None:\n\t\t\t\treturn fn(_self, *args, **kwargs)\n\t\t\t# time_limit is set as a timeout for the transaction\n\t\t\t# Use a lock to prevent concurrent processes from taking the same transaction\n\t\t\twith self._tx_cannot_exec:\n\t\t\t\treturn fn(_self, *args, **kwargs)"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tunit_out = Unit(metadata=metadata, timeout=timeout)\n\t@wraps(function)\n\tdef wrapped_function(*args, **kwargs):\n\t\ttx = dbms.beginTx(name=\"unit_of_work_\" + str(int(time.time())) + \".log\", unit=tx_output)\n\t\ttry:\n\t\t\treturn function(*args,**kwargs)\n\t\tfinally:\n\t\t\treturn tx.committed()\n\treturn wrapped_function\n\n######\n### Unit Metadata\n######\t\nunit_metadata={}\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\t# set default timeout of 30 mins if none is passed in\n\tif timeout is None or timeout is None:\n\t\ttimeout = 60  # 30 mins is default for transaction timeout\n\t\n\t\n\tdef decorator(method):\n\t\tmethod.doc = \"transaction function as a context manager\"\n\t\tif method.timeout is None:\n\t\t\tmethod.timeout = timeout\n\t\t\n\t\tdef _decorator(func):\n\t\t\t\n\t\t\tdef _apply_timeout(func, metadata):\n\t\t\t\t\n\t\t\t\tfunc.timeout = func.timeout or None\n\t\t\t\t\n\t\t\t\tfunc.timeout"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\t# Apply metadata\n\t\t\tmeta = {\"tx.timeout\": timeout}\n\t\t\tif metadata:\n\t\t\t\tmeta.update(metadata)\n\t\t\tfunc(*args, **kwargs)\n\t\treturn inner\n\treturn decorator\n\t\npass\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif timeout is None:\n\t\ttimeout = dbms.transaction.timeout\n\tdef decorator(fn): # noqa\n\t\t@wraps(fn)\n\t\tdef wrapped(*args,**kwds): # noqa\n\t\t\tlogger.debug(\"Running transaction, metadata:%s, timeout:%s \" % (metadata, timeout))\n\t\t\twith dbms.transaction, dbms.transaction.setTXMetaData(metadata,timeout=timeout) as tx:\n\t\t\t\treturn fn(*args, **kwds)\n\t\treturn wrapped\n\treturn decorator\n\t\ndef"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef _call(tx):\n\t\tif metadata is None:\n\t\t\ttx = tx.__dict__[\"tx\"]\n\t\t\tmetadata = tx.metadata\n\t\tresult, query = tx.run(\"MATCH (p:Person:{}) RETURN p\")\n\t\tlogger.debug(\"Count people for {p}\".format(**metadata))\n\t\trecord = result.single()\n\t\tpersons = record[\"persons\"]\n\t\treturn persons, record[\"id\"]\n\tdef call(func):\n\t\t\n\t\tdef decorator(*args,**kwargs):\n\t\t\ttry:\n\t\t\t\ttx"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif metadata == None:\n\t\tmetadata = {}\n\t\n\tdef decorator(function):\n\t\tdef wrapper():\n\t\t\tmetadata[\"time_ms\"] = time.monotonic()\n\t\t\treturn function(metadata, timeout)\n\t\treturn wrapper\n\treturn decorator\n"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.lookup(str(key)).index(key) + 1 << 16\t \n\t\t\t\n\nclass Dictionary():\n\t\"\"\"\n\tCreate a dictionary\n\t\n\t.. code-block:: python\n\t \n\t\tdef createDictionary():\n\t\t\td = dict()\n\t \n\t\t\tfor x in dict:\n\t\t\t\tprint x \n\t\t\t\td[x] = 12345\n\t \n\t\tcreateDictionary()\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._values = {}\n\t\tself._size = 0\n\n\t@property\n\tdef values"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\titem = self._index[key].value\n\treturn(self._index[key].key) if item is None else item\n\n\t\ndef find_item(self, key, default=None):\n\t\"\"\"\n\tReturn the _item key if a match is found, else None.\n\n\t:param key: a key\n\t:param default: default value\n\t:return: item or None\n\t\"\"\"\n\titem = self._find(key, 0, len(self._index)-1)\n\t\n\tif item is not None:\n\t\treturn(item.value) if"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.index[key]\n\ndef _get_all():\n\t\"\"\"\n\tCreate a map of all keys.\n\t\n\tThe map index is initialized with 0.\n\t\"\"\"\n\tindex = dict()\n\tfor key in pycqd_indexes.cqd_keys:\n\t\tindex[key] = 0\n\treturn index\n\ndef get_index(key):\n\t\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: dict\n\t\"\"\"\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.get_list().index(key)\n\t\n\t\ndef get(self, key, default=None, index=None):\n\t\"\"\"\n\tGet the item at the position index or default if none found, if index is None get all items.\n\t\n\t:param key: a key\n\t:param default: the default if none found\n\t:param index: the first index to return\n\t:return: the item at index, or default\n\t:rtype: object\n\t\"\"\"\n\tkey=key.__str__()\n\tif type(index)"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.indices.__getitem__(key)\n\t\n\t\n\t\n\t\nclass TreeNode(object):\n\t\n\tdef __init__(self, data, is_leaf=False):\n\t\tself.data = data\n\t\tself.is_leaf = is_leaf\t\t\n\t\t\n\t\t\n\tdef __repr__(self):\n\t\treturn str(self.data) \n\t\n\tdef set_child(self, data, is_leaf=False):\n\t\tsuper(TreeNode, self).__init__(data, is_leaf="}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.table.__getitem__(key)\n\t\ndef remove(self, key):\n\t\"\"\"\n\tRemove an item from the index dict\n\t\tand return the index of the removed item\n\t\n\t:param key: a key\n\t:return: removed index\n\t:rtype: int\n\t\"\"\"\n\ti = self.table.remove(key)\n\treturn i\n\n\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.index_(key)\n\t\n\t\n\t\n\t\n# -----------------------------------------------------------------------------\n#\tData collection\n# -----------------------------------------------------------------------------\n\t\ndef get_data(key):\n\t\"\"\"\n\tGet the contents of a collection of data in a hash table.\n\t\n\t:param key: key\n\t:type key: str\n\n\t:return: contents, key, if collection already has data\n\t:rtype: tuple(str, str, object)\n\t\"\"\"\n\tif not key:\n\t\tkey = key\n\tdata = None\n\tcache = None\n\tif has"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.indexFromKey(key)\n\ndef indexFromKey(key, start=None):\n\t\"\"\"\n\tReturn the index of the given item.\n\n\t:param key: the item key\n\t:type key: string\n\t:param start: the start index, or None to start at the end of the database.\n\t:type start: int\n\t\"\"\"\n\t\n\tkey = str(key)\n\tif start is None:\n\t\tstart = len(key)\n\tfor \n\t\tparsedKey in self.parser.parse_key(key,"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\ttry:\n\t\tindex = self.getIndex(key)\n\t\t# index == None => return the default value\n\t\treturn index\n    except ValueError:\n\t\t# index == None => return the default value\n\t\treturn index\n\ndef count(self):\n\t\"\"\"\n\tReturn the total count of an iterator.\n\t\n\t:return: count\n\t:rtype: int\n\t\"\"\"\n\treturn len(self.items)\n\ndef filter(self, pred, remove=False):\n\t\"\"\"\n\tReturn a copy of the iterator with only the items for which the filter"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.index(key)\n\n\t\ndef get_cursor(self):\n\t\"\"\"\n\tReturn current iterator.\n\t\n\t:return: cursor\n\t:rtype: (str, iterable)\n\t\"\"\"\n\treturn self.db_cursor\n\t\n\t\ndef close_cursor(self):\n\t\"\"\"\n\tClose current iterator.\n\t\n\t:return: (str, iterable)\n\t:rtype: (str, iterable)\n\t\"\"\"\n\tself.db_cursor = None\n\treturn self.db_cursor"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif not keys:\n\t\treturn self._items()\n\tv2 = []\n\tv3 = {}\n\tfor i in range(len(keys)):\n\t\tkey = keys[i]\n\t\tif not (key in self._items()) or key not in v2:\n\t\t\tv2.append(key)\n\t\t\tv3.update(self._items())\n\treturn v3\n\t\t\t\ndef extract(self, *keys):\n\t\"\"\"\n\tExtract values from list of fields to the values() method\n\n\t:return: list of values\n\t:rtype:"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t# get the item to include for each key\n\tif isinstance(keys, tuple):\n\t\tkey = keys[0]\n\telse:\n\t\tkey = keys\n\n\t# set a minimum and maximum to return the items from\n\tlow = max(0, key - 1)\n\thigh = min(self.total-1, key + 1)\n\tlow = max(low, 0)\n\thigh = min(high, self.total-1)\n\n\t# extract the values for those keys we're interested in\n\tvalues = self.values(key, low, high)"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif not isinstance(keys, tuple):\n\t\tkeys = (keys,)\n\tvalues = dict(self.get(*, *keys))\n\tif len(values) and ((not isinstance(keys, tuple)) or\n\t\t\t    not all(isinstance(key, types.Integer)\n\t\t\t\tfor key in keys) or\n\t\t\t    any(key in k for k in values)):\n\t\treturn values\n\n\treturn [self[key] for key in keys]\n\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif not keys: # include everything\n\t\treturn self.all()\n\telse:\n\t\tmatch = None # start with all values\n\t\tresult = [] # result variable for easy removal later on\n\t\tfor k in range(len(keys)):\n\t\t\tif match is None: # use a match to make sure they're all there\n\t\t\t\tmatch = keys[k]\n\t\t\telif not match.endswith('%'): # it's not a regular variable\n\t\t\t\tmatch = k + keys[0] # so we start from the beginning\n\t\t\t\tresult.append(self.all([v"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn [val for item in self for val in self.values(item) if val not in self.__DEFAULT_VALUES]\n\t\n\tif not isinstance(keys[0], int):\n\t\traise ValueError(\"keys must be an integer\")\n\t\n\treturn self.__filter([val for val in self if val in self.__DEFAULT_VALUES])\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn _lib.values(self._ptr, *keys)\n\t\n\n    def get_last_index(self):\n\t\"\"\"\n \t  Return index of last update of record\n \t\"\"\"\n\treturn _lib.get_last_index(self._ptr)\n\t\n\n    def __getitem__(self, key):\n        return self.value_at(key)\n\t \n    def __getattr__(self, key):\n\t    return getattr(self.value_at(key), key)\n\t \n    def __getitem__("}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\tif not keys:\n\t\trv = self.record\n\telse:\n        \trv = self.record.filter(index=keys, *keys)\n\treturn [item.value for item in rv]\n\n\t\nclass Search(Model):\n\t\"\"\"\n\tThis model uses searchable values and may search on multiple fields or key values.\n\t\"\"\"\n\t\n\tquery = Field(Index, label=_(\"Query Index\"), required=True)\n\tsort_order = Field(Index, label=_(\"Sort Order\"))\n\tquery_fields = Field("}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn self.filtered(lambda k: k in keys or k is None)\n\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tself._regex = None\n\tif self.filters:\n\t\tif type(keys)!= tuple:\n\t\t\tkeys = (self.filters,) + keys\n\t\tself._regex = self.filters.match(self.match_type, self.path)\n\t\tif self._regex:\n\t\t\tfor key in keys:\n\t\t\t\tif key in self._regex.groupdict(1):\n\t\t\t\t\treturn self._regex.groupdict(1)['value'][self.match]\n\treturn list()\n\t\n\t\n    def _format(self,"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\tvalues = []\n\tif len(keys)==0:\n\t\tkeys = [\"*\"]\n\tvalues_set = set(filter(lambda x : x in self, values))\n\t\n\tif values_set is not set:\n\t\tvalues = [k for k in keys\n\t\t\t\t if values_set.intersection(set(values)) > 0]\n\treturn values\n\ndef exists_in_files(data, *keys):\n\t\"\"\"\n\tGet whether data exists\n\t\n\t:param data: list of values\n\t:param keys: indexes or keys"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tk = keys[0]\n\tif len(keys) > 0:\n\t\tif not ( isinstance(k, int) or isinstance(k, str) ):\n\t\t\traise TypeError(\"Only integer index-able keys are accepted at this time.\")\n\t\tn = len(data)\n\t\tif isinstance(k, int):\n\t\t\tif k < 0 or k >= n:\n\t\t\t\traise IndexError(\n\t\t\t\t\t\"Index \" + str(k) + \" too large for \" +\n\t\t\t\t\t\"data.data.keys: must be from \" +\n\t\t\t\t\t\"0 to \" +"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif len(keys) > 0:\n\t\tkeys = sorted(set([record.key for item in keys]))\n\t\tfor key in keys:\n\t\t\tif key is None:\n\t\t\t\tkey = \"None\"\n\t\t\td = record.data\n\t\t\tif isinstance(d, list) or isinstance(d, tuple):\n\t\t\t\tkey = sorted([record.key for record in d])\n\t\t\t\td = {}\n\t\t\td[key] = d[key] if key in d else record.data\n\t\t\tkeys.remove(key)\n\t\n\tif keys is None and not record"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn None\n\tkeys = set(keys)\n\td = {}\n\tfor k in keys:\n\t\t# If it's a list, we need to add out-of-bounds indexes like this:\n\t\tif k.isdigit():\n\t\t\t# This only works with lists...\n\t\t\t# We can't do it with a single k=1\n\t\t\tk = self._new_key(k)\n\t\tself.add_value(k, None)\n\n\t\td[k] = self.get_value(k)\n\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn dict(self[key] for key in keys if key in self)\n\nclass RecordIndex(object):\n\t\"\"\"\n\tAdd a :exc:`RecordIndex` object to a record.\n\n\tExamples of use:\n\n\t\t>>> import random\n\t\t>>> r1 = random.MersenneTwister(rng)\n\t\t>>> r = random.RecordIndex(r1, [10,20,30])\n\t\t>>> r\n\t\t[10, 20, 30]\n\t\"\"\"\n\n\tdef __init__(self, r, items):\n\t\tself.r = r\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif not keys:\n\t\treturn {}\n\tif len(keys) == 1:\n\t\tkey = keys[0]\n\telse:\n\t\tkey = list(set(keys))\n\tvalues = self.__dict__.get(key, [None])\n\tif isinstance(values, list):\n\t\tvalues = [value] * len(keys)\n\treturn dict(zip(keys, values))\n\ndef fields(self):\n\t\"\"\"\n\tReturn a list of names of fields with validator- and data-type-s\n\tconventions.\n\t\n\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn self.__class__(self.items, *keys)\n\nclass Records(object):\n\t\"\"\"\n\tCreate a list of records from the data dictionary or the file\n\tif a file is provided.\n\n\t.. versionchanged:: 0.3.3 :class:`Records` instances are\n\t      :func:`records` by default and are returned as a :func:`records`\n\t      object.\n\n\tSee also:\n\t--------\n\trecords()\n\t\"\"\"\n\tdef __init__(self, data=None):\n\t\t\"\"\"\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tres = dict()\n\tkeys = list(keys)\n\tfor k in keys:\n\t\tres.update(value.data(k, *keys)[len(keys) - len(keys) + 1:])\n\treturn res\n\t\n\t\n\t\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tdef _val(r, a):\n\t\t\"\"\"\n\t\tGenerate a dictionary of the values.\n\t\t\n\t\t:param record r: record for which to get values\n\t\t:param string a: index or key to include\n\t\t:return: dictionary of values, keyed by field name\n\t\t\"\"\"\n\t\td = dict(items=dict(items),\n\t\t         keys=dict(_val(r,key),key=a))\n\t\td.update(r.items)\n\t\treturn d\n\t\n\tif sorted(keys) == list(enumerate(self._indices"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif keys is None:\n\t\treturn self._data\n\telse:\n\t\tindexes = []\n\t\tfor key in keys:\n\t\t\tif isinstance(key, int):\n\t\t\t\tif index is not None and index!= key:\n\t\t\t\t\traise IndexError('Index for keys must be equal for record: {}'.format(self.__repr__()))\n\t\t\t\tindexes.append(key)\n\t\t\telse:\n\t\t\t\tindexes.append(index)\n\t\treturn dict(self._data[index] for index in indexes)\n\t\n\t\n__all__ = ['data']\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\ttry:\n\t\treturn dict(self._items)\n\texcept IndexError:\n\t\t# no such index, so we need to insert a value\n\t\tif not hasattr(self._items, \"fields\"):\n\t\t\traise ValueError(\"Can't get data for non-databely-named records\")\n\t\tif len(keys) == 0:\n\t\t\treturn self._items\n\t\traise IndexError(\"Cannot get data for {}\".format(\" \".join(keys)))\n"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn ((sys.maxint >> 16) & 0xffff)\n\n\n\t\n\t\nif __name__ == '__main__':\n\t# import code  initalized by runmyscript\n\t# runmyscript(filename, run_stdout=run)\n\n"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self.data[-2:]\n\n\t\n\t##\n\tdef pop_u32(self):\n\t\t\"\"\"\n\t\tUnpacks the last element of the sequence as a big-endian 32-bit\n\t\tinteger in a 32-bit integer (i.e., unsigned).\n\t\t\"\"\"\n\t\treturn self.data[-4:]\n\t\n\t##\n\tdef get_length(self):\n\t\t\"\"\"\n\t\tReturn the length of the data in this big-endian 32-bit sequence.\n\t\t\"\"\"\n\t\treturn len(self.data[-4 : ])\n"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn ord(self.data[len(self.data)-2]) << 8.0\n\t\ndef put_u16(self, val):\n\t\"\"\"\n\tPut the 16-bit unsigned integer in the buffer, returning True\n\tif successful, False otherwise.\n\t\"\"\"\n\tif self.end <= self.start:\n\t\tself.start = self.end\n\t\tself.end = len(self.data)\n\t\tif sys.version_info >= (3,):\n\t\t\tt = struct.pack(\"l\", val)\n\t\telse: \n\t\t\tt = struct"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tdata = self.data\n\tc = 0\n\n\ttry:\n\t\tc = struct.unpack(\">H\", [ data for c in data ])[0]\n\texcept (struct.error, IndexError):\n\t\t# We can't do anything here as it has no valid endianness.\n\t\traise TypeError(\"data argument not a big-endian 16-bit integer\")\n\treturn c\n\n\t\n\n\t\n\n\t\n\n\t\t\t\n\n\t\n\n\t\n\n\nclass FileObject(File):\n\t\"\"\"\n\tRepresents a file object, used by open()\n\t\"\"\"\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself.data.pop(0)\n\tself.data.pop(0)\n\treturn self.data[0]\n\n\t\nclass DataParser():\n\tdef __init__(self, csvparser):\n\t\tself.csvparser = csvparser\n\t\ttry:\n\t\t\t# Get first line's row.\n\t\t\tdata = self.csvparser['header']\n\t\t\tself.header = self.csvparser['header_field']\n\t\texcept:\n\t\t\traise Exception(\"Cannot find data header in CSV file\")\n\t\texcept:\n\t\t\t# Get first line's next field,"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tif (self.size % 2):\n\t\t# Unsigned, little endian\n\t\tself.data = unpack(b'>i', struct.pack(b'>H', self.data))[0]\n\t\tself.size = self.size // 2\n\t\treturn self.data\n\telse:\n\t\t# Unsigned, big endian\n\t\tself.size = self.size // 2\n\t\treturn struct.unpack(b'>I', struct.pack(b'>I', self.data))[0]\n\n\n\t## Data is a raw packet that must be unpacked in"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tb = self.data[-2:]\n\treturn (ord(b) & 0xff) << 8 | (ord(b) & 0xff)\n\n\t\nclass TestBinaryInputStream(TestInputStream):\n\t\"\"\"\n\tA TestBinaryInputStream is constructed to test the \n\t.read() method of a binary input stream (or similar)\n\t\"\"\"\n\tdef test(self):\n\t\t\"\"\"\n\t\tTests.read() method of a binary input stream.\n\t\t\"\"\"\n\t\tr = self.is.read()\n\t\tif r == -1:\n"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\n\t# Calculate number of bytes in data, truncate start of that many\n\t# bits into a 8-bit integer\n\tdataLength = (ord(self.data[len(self.data)-1]) & 0x7F) + 1\n\tnewDataLength = (ord(self.data[1]) & 0x7F) + 1 + dataLength\n\t\n\t# Initialize the data array just in case\n\tself.data = binascii.unhexlify(self.data)[1 : newDataLength+1]\n\n\t# Make the"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn ord(self.data[len(self.data)-2]) << 8 | ord(self.data[len(self.data)-1])\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n#\tdef pop_U32(self):\n#\t\treturn struct.unpack('32BE', self.data[len(self.data)-4:len(self.data)-1])[0]\n\n\ndef pop_f(self):\n\treturn struct.unpack('<f', self.data["}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self.data[self.idx + 2] & 0xff\n\n\nb.pop_u16 = pop_u16\n\nb.push_u16 = pop_u16\n\nb.pop_u32 = pop_u32\nb.pop_u32_u16 = pop_u32\nb.st_u32 = b.st_u32_u16\nb.push_u32 = push_u32\nb.st_u32_u32 = tst_u32\nb.st_u32_u16 = st_u"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t)\n\n\tdef handle(\n\t\tself,\n\t\tmessage,\n\t\tquery_id=-1,\n\t\t**kwargs\n\t):\n\t\tpass\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdef __init__(self, q, n=-1, qid=-1, dehydration_hooks=None,\n\t\t\t\thydration_hooks=None, handlers=[]):\n\t\tself.q = q\n\t\tif n!= -1:\n\t\t\tsetattr(self, '_'+Q.Q_NEXT, self.q.get_next)\n\t\telse:\n\t\t\tself.q.get_next()\n\t\tif n!= -1:\n\t\t\tdehydration_hooks = self.q.q.dehydrate(dehydration_hooks)"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t) as this:\n        # Call the common call with the right handlers\n        self._send([discard, -1, -1], n, qid, dehydration_hooks, hydration_hooks, handlers)\n        self.decoder = None\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t)\n\tdef add_list(self, l=[], add_all=True):\n\t\t\"\"\"\n\t\tAdd several lists to the output queue\n\t\t\n\t\t:param l:  lists of lists\n\t\t:param add_all:  Whether to append newly added lists to the head of the queue, or clear it\n\t\t\"\"\"\n\t\tif add_all:\n\t\t\tself.q = self.q + l\n\t\telse:\n\t\t\tself.q[-1].extend(l)\n\t\t\tself.q.remove(self.q[-1])\n\n\tdef __call"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t)\n\t\t\t\t\n\n\tdef __init__(self, **conf):\n\t\tsuper(DiscardResponse,self).__init__(conf)\n\n\tdef _add_stream_headers(self, stream):\n\t\tstream.add_header('Content-Type', 'application/pandas.pandas-json')  \n\n\tdef _add_stream_headers(self, stream):\n\t\tif self.hydration_hooks:\n\t\t\tself._add_stream_headers(stream)\n\t\n\n\tdef __getitem__(self, n):\n\t\t\"\"\"Discard query"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t   .filterfunc('object', filterf('query is a packstream_packbuffer', filterf('query.flags', 'f')))\n\t   .filterfunc('query.flags.f', filterf('query.flags.f.f', 'i'))\n\t   .filterfunc('message.timestamp.f.f.f.f.f.i', filterf('message.timestamp.f.f.f.f.i', 'i'))\n\t)\n        response.set_client(self.client)\n        response.set_version('0"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn Response(\n\t    packstream_hook_callback,\n\t    packstream_hydration_hooks,\n\t    packstream_hydration_hooks,\n\t    packstream_hydration_hooks,\n\t    packstream_hydration_hooks,\n\t    hydration_hooks,\n\t    \"packstream\",\n\t    hydration_functions,\n\t    hydration_functions,\n\t    hydration_hooks,\n\t    hydration_hooks,\n\t    DEHYDRATE_HOOKS_DICT,"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdef __init__(self, n = -1, qid = -1, dehydration_hooks=None,\n\t        hydration_hooks=None, handlers = None, stream_name = \"\",\n\t                ** extra_kw ):\n\t\tResponse.__init__(self, stream_name)\n\t\tself.extra_kw['n'] = n if n!= -1 else len(self.headers)\n\t\tself.extra_kw['qid'] = qid if qid!= -1 else len(self.headers['queryID'])\n\t\tself.extra"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t)\n\tdef handle_query(_, resp, packet):\n\t\tif resp.payload is None:\n\t\t\treturn\n\t\tfndry_qid = (n, None if int(packet.payload_len) < n else (n - 1, int(packet.payload_len) + 1))\n\t\tfntry = PacketFilterEntry(packet, resp)\n\t\tfor f in fndry_qid[1][1]:\n\t\t\tif f is None:\n\t\t\t\tcontinue\n\t\t\telif isinstance(f, Query) is False:\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tsuper(packstream_request_type_discard, self).__init__(n, qid, dehydration_hooks, hydration_hooks, handlers=None)\n\tself.mutable_m = []\n\tself.mutable_n = []\n\tself.mutable_m_s = []\n"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not self._enabled:\n\t\treturn self.response(self.response.ERROR, \"Already-disabled\")\n\t\n\tdef add_response(response):\n\t\tresponse.response = self\n\t\tresponse.connection = self\n\t\tif response.connection is not None:\n\t\t\tresponse.connection.pipeline = Packet.Pipeline()\n\t\t\n\t\tdef on_response(response):\n\t\t\tresponse.response = response\n\t\t\tif response.messageType is None:\n\t\t\t\tresponse.messageType = response.transactionId\n\t\t\t\n\t\tdef on_error_response(response"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not mode:\n\t\tmode = 'READ'\n\n\tif not timeout:\n\t\ttimeout = self.DEFAULT_TIMEOUT\n\n\tif mode not in ['READ', 'WRITE']:\n\t\traise Exception('Invalid value for transaction mode')\n\treturn _Response(message=b'BEGIN', mode=mode, timeout=timeout, payload='').assemble()\n\n\n\nclass _Response(object):\n\n\tdef __init__(self, message=None, **kwargs):\n\t\tself.message      = message\n\t\tself.state        = State(**kwargs)"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tresponse = self.get_connection().begin(\n\t\tmode=mode,\n\t\tbookmarks=bookmarks,\n\t\tmetadata=metadata,\n\t\ttimeout=timeout,\n\t)\n\tif self.response_type == Response.RESPONSE_TYPE_WRITE:\n\t\tresponse.write(response[\"message\"])\n\treturn response\n\t\n\t\n\t\n\t\n\t\nclass SyncReader(object):\n    \n    def __init__(self, sessionId, requestId, responseQueue, responseDehydration):\n\tself.connection = responseDehydration."}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\timport os.path\n\timport os, time, urllib, zipfile, subprocess\n\timport requests\n\timport json, logging\n\tfrom zerorpc.connector.zk import connect\n\tfrom zerorpc.connector.http import makeHttpRequest\n\timport zerver.lib.convert_to_integer as convert_to_int\n\tDEHYDRATION_OP_TYPES = {\n\t\t\"json\": self.json,\n\t\t\"xml\": self.get_element,\n\t\t\"stream\": self.get_first_element,"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\timport packstream.decoration.bod\n\timport packstream.decoration.bod.decoration\n\tlog.debug(\"begin(%s, bookmarks=%s, metadata=%s, timeout=%s)\",\n\t\tmode, bookmarks, metadata, timeout)\n\n\tif metadata == None:\n\t\tmetadata = {}\n\t\t\n\tdef handler(metadata, body):\n\t\ttry:\n\t\t\tdecoded = packstream.decoration.bod.decode_bod(body)\n\t\t\tlog.debug(\"type of body: %s\", type(body))"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\targs,kwargs = _parse_options(app,args,kwargs)\n\tif flags.dep_on_start or flags.dep_on_commit:\n\t\t_start_dep_transaction(flags,mode,db,app,app.routing_entry(),imp_user,metadata,timeout)\n\t\treturn Response(_start_dep_transaction.__doc__)\n\tif not flags.dep_on_start:\n\t\t_start_dep_transaction(flags,mode,db,app,app.routing_entry(),imp_user)\n\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not hasattr(self, \"___metadata__\"):\n\t\tself.___metadata__ = {\n\t\t\t\"_begin:mode\": mode,\n\t\t\t\"_begin:bookmarks\": bookmarks,\n\t\t\t\"_begin:metadata\": metadata,\n\t\t\t\"_begin:timeout\": timeout,\n\t\t\t\"_begin:db\": db,\n\t\t\t\"_begin:imp_user\": imp_user,\n\t\t\t\"_begin:hydration_hooks\": hydration_hooks,\n\t\t\t\"_begin:dehydration_hooks\": dehydration_hooks,\n\t\t}\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tResponse.__init__(self, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, handlers)\n\n\tself.begin_msg = {\"mode\": mode, \"bookmarks\": bookmarks, \"metadata\": metadata, \"timeout\": timeout, \"db\": db}\n\n\t# The message we're going to be sending ourselves to the server\n\tself.reply_msg = {\"begin\": {\"mode\": self.begin_mode,\n\t    \"bookmarks\": self.begin_bookmarks,\n\t    \"metadata\": None}"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tfor x, handler in chain(mode, bookmarks, metadata, timeout):\n\t\tif isinstance(handler, type):\n\t\t\ttype_handler = handler\n\t\t\thandler = type_handler.__name__\n\n\t\tif hasattr(handler, 'dehydrate') or hasattr(handler, 'load'):\n\t\t\tdehydratable = True\n\t\telse:\n\t\t\tdehydratable = False\n\n\t\tif hasattr(handler, 'unload') or hasattr(handler, 'clean'):\n\t\t\tunloadable = True\n\t\telse:\n\t\t\tunloadable = False\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\treturn self.request(\n\t\t\"BEGIN\",\n\t\tmode=mode,\n\t\ttimeout=timeout,\n\t\tdata={\n\t\t\t\"imputation\": imp_user,\n\t\t\t\"bookmarks\": bookmarks,\n\t\t\t\"metadata\": metadata,\n\t\t\t\"timeout\": timeout,\n\t\t},\n\t\tjson={\n\t\t\t\"type\": None,\n\t\t\t\"timeout\": timeout,\n\t\t},\n\t\tpy4=True,\n\t\t**hydration_hooks,\n\t\t**dehydration_hooks,\n\t\t**expiration_hooks\n\t)[\"id\"]\n\t)"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 0:\n\t\treturn n\n\t\n\t# We will calculate the number of times the remainder of the input is positive\n\tr = n % 2\n\t# if that would not give anything, the entire number of times the remainder\n\t# would fit in the input\n\tif r < 1:\n\t\twhile r:\n\t\t\tr -= 1\n\t\t# the final result is 0\n\t\treturn 0\n\t\n\t# else the integer is the odd number, we simply calculate the number of times\n\t# the input is odd.\n\treturn 1 - r\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn int(n) // 2\n\"\"\"\n\t>>> round_half_to_even(int(3.5))\n\t4\n\t\n\t>>> round_half_to_even(\"11\")\n\t11\n\"\"\"\n\ndef print_rounded_half_to_even(n):\n\t# TODO: print_rounded_half_to_even instead of printing round_half_to_even\n\treturn f'{fround_half_to_even(n) if n <= 0 else \"1\"}'!= n and n // 2\n\"\"\"\n\t>>> print_"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn (n % 2) == 0\n\nif __name__ == '__main__':\n\t# test_rounding()\n\t\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n == 0:\n\t\treturn True\n\ttmp = n % 2\n\tif tmp == 0:\n\t\treturn False\n\treturn True\n\n\ndef calc_odd_digits(n):\n\tif n < 1:\n\t\treturn 0\n\todds = n % 2\n\treturn 1 + max_digits - len(str(n % 2))\n\t\ndef calc_min_digits(n):\n\tif n <= 0:\n\t\treturn n\n\tmin_digits = max(calc_odd_digits(n), len(str(n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn n % 2\n\t\ndef random_subs(n):\n\t\"\"\"\n\t>>> sub = random_subs(2)\n\t>>> sub(3)\n\t5\n\t>>> [random_subs(i) for i in range(2 + 5)]\n\t[5, 8]\n\t>>> [random_subs(i) for i in range(1 + 2 ** 7 + 2 * 9) if i > 0]\n\t[1, 2, 3, 4, 3, 2, 7, 7, 7, 7]\n\t>>> [random_sub"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn int(n)\n\t\n\t\n\t\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn int(n.num / 2)\n\t\n\nif __name__ == '__main__':\n\t# print 'n>='\n\t# print 'n=='\n\t# print 'n<'\n\t# print 'n=<'\n\t# print 'n<='\n\tprint 'n=<'\n\tprint round_half_to_even(3)\n\tprint round_half_to_even(4)\n\tprint round_half_to_even(3)\n\tprint round_half_to_even(5)\n\tprint round"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n == 0:\n\t\traise ValueError(\"Must be >= 0\")\n\tif n == 1: # 1/0\n\t\treturn (-x, +x)\n\t# 0, 1, 2, 1/0 are just to keep the logic cleaner.\n\tp1, q1 = -x, 0\n\tp2, _ = 0, -1\n\tif n % 2 == 0:\n\t\tp1, q1 = q1, p1\n\t\tp2, _ = p1, q1\n\tr = n/2\n\twhile r > 0:\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n<0:\n\t    return '{:+}'.format(round(n,2))\t# round() method\n\t\n\treturn '{}'.format(round(n,2))\t# cast to int\n\n#========================================================================================#\ndef round_round_half_to_even(n):\n\treturn '{:+}'.format(round(n))\n#========================================================================================#\n\t# TODO: do I need this?\n#------------------------------------------------------------------------------\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\tif n is None:\n\t\treturn n\n\t\n\treturn int(round(n, 2))\n\nif __name__ == '__main__':\n\tprint(round_half_to_even(2))\n"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\ttry:\n\t\tpoint = Point(name, srid_map[fields[0][0]])\n\texcept ValueError:\n\t\treturn None\n\tif len(fields) == 2:\n\t\tpoint.update_srid_map(srid_map)\n\treturn point\n\ndef pointfield_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\ttry:\n\t\tpoint = Point(name, srid_map[fields[0][0]])\n\texcept ValueError, e:\n"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\n\tgeom_class = Point\n\t\n\tname_index = srid_map[name]\n\tfields_index = fields.index(\"geometry\")\t\t\n\tpoint_fields = fields[fields_index:]\n\t\n\tif not fields_index:\n\t\tmessage = \"Unable to find geometry field for geometry field %s\"\n\t\traise Exception(message % name)\n\t\n\tif len(fields):\n\t\tfield = points_fields[name_index]\n\t\t\t\n\t\tgeotype = point_type(name, field['geometry_type'], s"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif not name:\n\t\treturn None\n\telif name in srid_map:\n\t\treturn srid_map[name]\n\n\treturn type(name, (Point,), {})\n"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tPoint.__init__(srid_map, Point, name, fields)\n\n\n@register('line')\nclass Line(Lineable):\n\t\"\"\"\n\tPoint subclass with a reference to a LineFields class. This class contains \n\ta reference to the PointFields class.\n\t\"\"\"\n\t__slots__ = []\n\n\tsrid_map = None\n\n\tdef __init__(self, srid_map):\n\t\tsuper().__init__(srid_map)\n\t\tself.field = self.field.cls.get_class_instance()"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tresult = []\n\ttry:\n\t\tresult.append( Point( name = name,\n\t\t\tsrid = srid_map.get(id(name)),\n\t\t\tgeometry_column = name\n\t\t)\n\texcept:\n\t\tpass\n\treturn result\n\ndef point_srid_map(point_type):\n\t\"\"\"\n\t\tmap all point_type instances to a srid that is returned by\n\t\tthe function geom_name_srid().\n\t\tpoint_type is a class and is used in this function\n\t\"\"\"\n\treturn dict((v, k)"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif srid_map.has_key(name):\n\t\treturn PointField\n\t\n\tnew_field = None\n\ttry:\n\t\t# look for the field type\n\t\tfield = fields.find('{}pointfield'.format(name))\n\t\t\n\t\tif field.has_key('type'):\n\t\t\tfield_type = field['type'].attrib['class']\n\t\t\tif isinstance(field_type, str):\n\t\t\t\tfield_type = getattr(QgsField, field_type)\n\t\t\tnew_field = QgsAbstractGeometryField(field"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\treturn Point(name, srid_map[name])\t\n\t\n### The following routines need a lot to change, so...\n\t\t\n\tdef point_type(name, fields, srid_map):\n\t\tif name == r\"\\Ace\": return Ace(fields, srid_map)\t\n\tdef point_type(name, fields, srid_map):\n\t\tif name == r\"\\Aele\": return AceLE(fields, srid_map)\t\n\t\tif name == r\"\\Ain\": return AceIN(fields, srid_map"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpoint = GeometryType.c.create_object(\"Point\", name, fields, srid_map)\n\treturn from_Geometry(point, False, False)\n## Create a new Point object\npoint = point_type(\"Point\", None, None)\n\n@from_Geometry\ndef point_c(name, fields, srid_map):\n\t\"\"\"c1 = point(name, srid_map)\"\"\"\n\tpoint = Geometry(name, srid_map)\n\tpoint.fields = fields\n\treturn from_Geometry(point, False,"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif isset(Point):\n\t\treturn POINT\n\telif srid_map is not None:\n\t\tif name in srid_map:\n\t\t\treturn SRID(fields, srid_map.get(name)).type\n\t\telse:\n\t\t\treturn str(fields)\n\treturn None\n\t\ndef point_srid(name, srid_map):\n\t\"\"\"\n\t\tReturn a Point subclass with a specific SRID\n\t\"\"\"\n\treturn Point(fields = name, srid = srid_map.get(name))\n\ndef shape(srid_"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tfrom django.contrib.gis.geos import Point as Point2D\n\tpoints = list(fields)\n\tif hasattr(Point, 'from_wkt'):\n\t\tpoints.insert(0, Point)\n\tfor p in points:\n\t\tfields.insert(0, {'name': '%s_2d' % p.__tablename__})\n\tclass Point2D_class(Point2D):\n\t\tpass\n\treturn Point2D_class\n\n\ndef field_type(name, fields, srid_map):\n\t"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef update_factory_method(target, factory):\n\t    target.update_factory_methods[factory.__doc__] = factory\n\treturn update_factory_method\n\n\nclass DeprecationContext(object):\n\tdef __init__(self):\n\t\tself.factory = factory\n\t\tself.old_doc = factory.__doc__\n\t\tself.deprecated = False\n\t\tself.ignore_other_deprecated = True\n\t\tself.new_doc = factory.__doc__\n\tdef deprecated(self, message=None):\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tfrom.cuda_func import deprecated\n\treturn deprecated(\"'%s'\" % message)\n\t\n\t\ndeprecated_funcs = deprecated.fetch(\n\t\"deprecated_funcs.fetch in place of deprecated functions.fetch\", \"Deprecated functions.fetch\",\n\tset(__opts__.get(\"cuda.deprecated_funcs\", {})))\nfor name in deprecated_funcs:\n\tfn = named_cuda_func(name)\n\tdeprecated_funcs.add(fn)\n\n# ---\n\nfrom."}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef __init__(func):\n\t\tfunc.__deprecated = True\n\t\tsuper(deprecated, func).__init__(func)\n\t\t\n\tdef wrap_deprecated(func):\n\t\t\n        \tdeprecated_func = Deprecated(func)\n        \t\n\t\tdef wrapped_deprecated(*args, **kwargs):\n          \t\told_old_args = list(args)\n          \t\told_old_kwargs = dict(kwargs)\n          \t\treturn deprecated_func(*(old_old_args + list(old_old_kwargs"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(func):\n\t\t_deprecatedMessage = deprecatedMessage()\n\t\t\n\t\tdef newFunction(*args, **kwargs):\n\t\t\t_args = [_deprecatedMessage]\n\t\t\tfor key, value in _deprecatedMessage.items():\n\t\t\t\t_args.append(\"%s=%s\" % (key, value))\n\t\t\treturn func(*args, **kwargs)\n\t\t\n\t\treturn newFunction\n\n\treturn decorator\n\n\ndef method(*args):\n\t\"\"\"\n\tCreates a decorator for overriding the method of a class or module.\n\t\n\t"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef original(func):\n\t    from inspect import signature\n\t    try:\n\t        func.func_code.co_argcount\n\t    except:\n\t        func.func_code = old_function_code\n\t    fname = func.func_name\n\n\t    old_function_code = func.func_code\n\t    old_func_name = func.func_name\n\n\t    func.func_code = _func_code_old(func.func_code, fname, 'foo')\n\n\t    func.func_name = fname\n\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef wrap(func):\n\t\toldName = func.__name__\n\t\tfunc.__deprecated__ = True\n\t\t\n\t\treturn func\n\treturn wrap"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(f):\n\t\tif not hasattr(f, '__deprecated__'):\n\t\t\tobject.__setattr__(f, '__deprecated__', True)\n\t\t\tfor c, v in f.__defaults__.items():\n\t\t\t\tobject.__setattr__(f, '__defaults__.'+c, (object.__getattribute__(f, '__defaults__.'+c), v))\n\t\treturn f\n\treturn decorator\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(f):\n            @functools.wraps(f)\n            def wrapper(*args, **kwargs):\n                if hasattr(f, \"__deprecated__\"):\n                    print(\"warning: 'from %s import deprecated\" % f.__name__)\n                    print(\"warning: you have been deprecated in favour of %s\" % f.__name__)\n                    raise SystemExit(2)\n\t\t\n\t\treturn f(*args, **kwargs)   \n\treturn decorator\n\n\n@deprecated(\"'deprecated' is deprecated in favour"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef __init__(func=None, deprecated_func=None):\n\t\t\n\t\tif deprecated_func is None:\n\t\t\tdeprecated_func = deprecated\n\n\t\tif isinstance(func, basestring):\n\t\t\treturn deprecated_func(func)\n\t\t\n\t\t# make a copy of the parameter list\n\t\treturn deprecated_func(*(deprecated_func(param) for param in func.__code__.co_varnames[1:] if deprecated_func(param))\n\t\n\treturn deprecated\n\t\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tclass Inner(object):\n\t    pass\n\tdeprecated_class = type(Inner())\n\t\n\t@deprecated_class.deprecated(message)\n\tdef bar(x):\n\t    pass\n\n\n@deprecated(\"'foo' has been deprecated in favour of 'bar'\")\ndef deprecated_func_with_decorator(x):\n\tpass\n\t\ndef deprecated_method():\n\tpass\n\t\n\nimport sys\n\nif sys.version_info[0] >= 3:\n\timport unittest2 as unittest\nelse:"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\treturn str._concat_and_unify(code,\"#Rscript(Rversion=%%Rversion%%)\",\n\t                             env.get(\"RVERSION\",sys.version_info[0]))\n\n\n\n\n\n\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tsys.stdout.write(code)\n\n\tassert code[0] == chr(27)\n\tassert code[1] == chr(92)\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tif 'RENDERBASENAME' in os_environ:\n\t\t# Set up a new R environment based on the value of\n\t\t#  RENDERBASENAME. This is also the path to a custom C\n\t\t#  environ variable in the source code. This is used by\n\t\t#  the \"R_RENDERBASENAME\" option when running the testRdScript\n\t\t#  from the command line.\n\t\treturn R_RENDERBASENAME + code + '.R'\n\telse:\n\t\treturn code\n\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tenv_cmd = {}\n\tif sys.stdout.isatty():\n\t\treturn code\n\n\tif 'R' not in os.environ:\n\t\treturn code\n\t# for python 3.4+ the setup of the R engine doesn't work,\n\t# but we can't rely on the environment to hold it. So we use\n\t# some trick to be able to find the.Rd file\n\tenv_file_name = os.path.join(os.environ['Rsource'], 'R.Rf', os.path.basename(sys."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tres = \"\"\n\tfor res in code:  # noqa\n\t\tif res not in os.environ:\n\t\t\tbreak\n\t\tres += \" \" + os.environ[res]\n\treturn res\n\n\n# this can be used inside the setup script as well, as it allows to\n# pass in different options for the set of tests\n# in your setup.py.\n\n# Note: This configs the standard R option environment variables so they will be used by the shell\ntest_code_to_run_with_runcmd_command = lambda command: _inline_r"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\t# Remove previous config\n\tif config_path_is_set_and_not_set(config_path) \\\n\t       or command_path_is_set_and_not_set(command_path):\n\t\tcleanup()\n\n\t\n\t\n\tcommand_file = os.path.join(command_path, \"config.r\")\n\t\n\n\tif not os.path.isfile(command_file):\n\t\treturn str(code,\"utf-8\")\n\t\n\n\tr_filename = command_file.replace(\".conf\", \".r."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# Get the version from the options\n\t# pylint: disable=no-member, invalid-name\n\tversion = None\n\tfor opt in ['R_INLINE_R3', 'R_LINK_R', 'R_VERBOSE']:\n\t\tvar = getattr(config, opt, None)\n\t\tif var is not None:\n\t\t\tversion = var\n\n\t# In R 3 support must call R to set the variables\n\tif version is None:\n\t\tprint(\"cannot set R version\")\n\t\tsys.exit(1)\n\n\t# Setup R"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\timport os\n\timport sys\n\n\t# make sure R is not in PATH (in order for it to be loaded into the\n\t# interpreter correctly)\n\tos.environ['R_SOFTWARE'] = None  # needed for 'r-configure'\n\tos.environ['R_HOMEPATH'] = None\n\n\t# setup r_configure env setting\n\tif 'INRCONFIGURE' in os.environ:\n\t\tos.environ[\n\t\t\t'R_CMD'] = os.environ['INRCONFIGURE'] +'r_config"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tglobal _R\n\tif _R is None:\n\t\t_R = 'Rc'\n\treturn _R\n\t\n\ndef env() -> str:\n\t\"\"\"\n\tReturn the R environment variable\n\t\"\"\"\n\treturn _inline_r_setup('RS')\n\ndef env_file() -> str:\n\t\"\"\"\n\tReturn the R environment variable\n\t\"\"\"\n\treturn _inline_r_setup('RSFILE')\n\ndef env_library() -> str:\n\t\"\"\"\n\tReturn the R environment variable\n\t\"\"\"\n\treturn _inline_r_setup"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tr_config_var = 'R_CONFIG_VAR'\n\tenv_vars['R_CONFIG_VAR'] = code\n\tenv = env.copy()\n\tenv = env.copy()\n\tif not env_vars.get(r_config_var):\n\t\tenv[r_config_var] = code\n\tif is_in_env('R_ENV', env):\n\t\treturn '# R(code,env=%s) will be in env, even if command is %s' % (r_config_var, code"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\n\nif __name__ == \"__main__\":\n\t# xargs accepts either arguments as positional, or arguments as keyword\n        # arguments\n\t\txargs(a=args[0])\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\ttry:\n\t\texec \"pty.open()\"\n\texcept:\n\t\treturn [\"\",]\n\t\n\tif not os.getenv(\"TERM\"):\n\t\tprint(\"You must set an terminal for your session\")\n\t\treturn []\n\t\n\tcolor = exec_command(\"color \\\"%s\\\"\" % exec_command(os.getenv(\"TERM\")))\n\ttry:\n\t\tpty = os.popen(\"cat >> %s\" % PTY_PREFIX).readline().strip()\n\t\tpty = int(pty.split()[0])\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor = sysconfig.get_config_var('TTYD_COLOR')\n\ttarget_concurrency = None\n\ttry:\n\t\ttarget_concurrency = int(sysconfig.get_config_var('NUM_TARGET_PARTS'))\n\t\tLOG.info('Num target partitions: %i', target_concurrency)\n\texcept (ValueError, ValueTooSmall):\n\t\tpass\n\t\n\tdef is_valid():\n\t\t\"\"\"Return True if 'color' parameter is present, but not 'target_concurrency'\"\"\"\n\t\tif color is None:\n\t\t\t"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\tfor k in kwargs:\n\t\tif k in ['color', 'target_concurrency']:\n\t\t\tk = 'target_concurrency'\n\t\t# if k in kwargs:\n\t\t# \traise AttributeError, \"Attribute '%s' is read-only\" % k\n\t\t\t\n\t# check that the user supplied a value\n\tif value is None:\n\t\traise ValueError, \"expected value for 'target_concurrency' keyword\"\n\t\n\tif value == 1:\n\t\tpass\n\t# target number of partitions\n\telif is"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\t\n\tdef x(s): s=s.rstrip(' ')\n\tsys.stdout = x\n\t\n\tdef y(s): s=s.lstrip(' ')\n\tsys.stderr = y\n\t\n\t\n    \ndef get_xargs():\n\t\"\"\"\n\tReturns a function that executes the xargs.\n\t\n\tExample usage:\n      \n\t>>> get_xargs().write('color: Is this the left pty?', encoding=sys.stdout)\n\t>>> get_xargs().write('target_"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\tdef color(c):\n\t\tif c == \"on\" or c == \"y\":\n\t\t\tos.system(\"xargs -i \" + \"COLOR ON\")\n\t\telif c == \"off\" or c == \"n\":\n\t\t\tos.system(\"xargs -i \" + \"COLOR OFF\")\n\t\telif c == \"yes\":\n\t\t\tos.system(\"xargs -i \" + \"COLOR OFF --yes\")\n\t\telif c == \"no\":\n\t\t\tos.system(\"xargs -i \" + \"COLOR OFF --no\")\n\t\n\tdef target"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcmd = [\"xargs\", \"color=tty,target=%d\" % (target_parallel), \"target_concurrency=%d\" % (conc)]\n\tx_call(cmd)\n\ndef xset_color(pty, new_color):\n\t\"\"\"Sets color on a pty\"\"\"\n\tcmd = [\"xset_color\", \"=\\'\" + new_color + \"\\'\" ]\n\tx_call(cmd)\n\ndef xset_color2(pty, new_color):\n\t\"\"\"Sets color on a pty\"\"\"\n\t"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\t\n\t#xargs('')\n\treturn '\\n'.join([' '.join((x,) for x in [c] * len(c)))\n\t\nprint(xargs(''))\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\nimport os\n\nfrom qip.util import print_line_for_error\n\nimport sys\n\nfrom qip.backends.python.execution import RemoteExecution\n\nimport yaml\n\nimport subprocess\n\n\ndebug = False\n\n\ndef get_platform():\n\t\"\"\"\n\tReturn platform, os, runtime by default\n\t\"\"\"\n\treturn sys.platform, sys.getrefcount(sys.executable), os.environ.get('PLATFORM', 'x64')\n\n\ndef get_compression_level():\n\t\"\"\"\n\tReturn"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\treturn xargs(arg, *keywords)\n\ndef xargs(arg, *keywords):\n\t''':return: Same as :xargs:``'''\n\tfor k in keywords:\n\t\tvalue = eval(arg +'' + k)\n\t\tif value is not None:\n\t\t\t# do it with an explicit call to set the value\n\t\t\tsetattr(current_app.jinja2_environ['current_app'], k, value)\n\t\telse:\n\t\t\t# if we couldn't find a matching value, fallback to the default value"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(sorted(seq))\n\ndef _generate(size):\n\t\"\"\"\n\tGenerator that generates the same size random elements\n\t\"\"\"\n\treturn range(size)"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffle = rand_string.shuffle\n\tshuffled = list(shuffle(seq))\n\treturn shuffled\n\n\ndef shuffle_all(seq: int) -> int:\n\t\"Shuffles a number of words.\"\n\tdic = _shuffled(seq[::-1])\n\treturn _shuffled(seq)\n\n\ndef _shuffled(seq: Sequence[int]) -> List[int]:\n\t\"\"\"\n\t\tShuffles a number of words\n\t\"\"\"\n\treturn list(set(seq))\n\n\ndef remove_all("}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn seq[::-1]\n\nclass SequentialTask(threading.Thread):\n\t\"\"\"\n\tA threaded sequential Task is a threading.Timer object and as such it has to have the\n\tstart() method set to call stop().\n\n\tIf you want to run things in a different thread, use a SequentialTask instead.\n\t\"\"\"\n\n\t_timer = None\n\n\tdef start(self, timer):\n\t\t\"\"\"\n\t\t:param timer:\ttimer to set up.\n\t\t:type\ttimer:\tthreading.Timer\n\t\t\"\"\"\n\t\tself._timer = timer"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn [ seq1 for seq2 in seq if seq2 not in seq\n\t\t\t\t\t\tand seq2[-1] not in seq2 ]\n\n\ndef _shuffled(seq: Sequence[str][5]) -> list[str]:\n\treturn [ seq2 for seq3 in seq if seq3 not in seq2\n\t\t\t\t\tand seq3[-1] not in seq3 ]\n\n\ndef _shuffled_with(seq: Sequence[str][5],\n\t\titer_shuffle: bool=True) -> list[str][5]:\n\t\"\"\"\n\tDeterministically shuffle"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn [e[bisect.midpoint(seq, i) for i in range(len(seq) - 1)]]\n\n\ndef _shuffled2(seq: list[str]) -> list[str]:\n\torder = _sorted(seq)\n\treturn _shuffled(seq)\n\ndef _shuffled3(seq: list[str], from_position: int = 0) -> str:\n\treturn seq[:from_position]\n\ndef _makearray2(seq) -> list[tuple[hashable, str]]:\n\t\"\"\""}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\timport random\n\treturn [randstr() for char in seq] + [random.choice(seq) for char in seq]\n\ndef _rand(chars: Iterable[str]) -> str:\n\treturn ''.join(random.choice(chars) for _ in range(1, 10)) \n\ndef generate_word(size: int) -> str:\n\t\"\"\"\n\t\tGenerate a word of the given size\n\n\t\tArgs:\n\t\t\tsize: size of the word\n\n\t\tReturns:\n\t\t\tthe word\n\t\"\"\"\n\tlength = _rand(size)\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn seq\n\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn seq_list.to_list(reversed(seq))\n\t\ndef _unique_names(seq: Sequence[str]) -> str:\n  # Remove any duplicate labels\n\tif len(set([_label for _ in seq for _ in _label]))==1:\n\t\treturn \"\".join(seq)\n\telse:\n\t\treturn \"\".join(list(_label) + [''.join(seq)])\n\ndef get_indexes(seq: Sequence[bool], l: int) -> List[int]:\n\t\"\"\"\n\tGet the set of tu"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\trandom.shuffle(seq)\n\treturn seq\n\n\ndef _set(seq, index: Index[int], value: bool) -> None: \n\t\"\"\"\n\t\tSet value of values in sequence [0, index]\n\t\t:param index: the index of where to find the value\n\t\t:type index: Index\n\t\t:param value\n\t\t:type value: bool\n\t\"\"\"\n\tif index > 0 and value is not None:\n\t\tseq[index] = value\n\n\ndef _sort(seq: Sequence[str], order: SortOrder[str],\n\t\tdescending"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t# iterating through the sequence to ensure we make a final call\n\titems = seq[:]\n\twhile items:\n\t\tnext_item = random.choice(items)\n\t\titems.remove(next_item)\n\t\tyield(next_item)\n\ndef _add_items(items: Sequence[str],\n        seq: Sequence[str],\n        dicc: Dicc,\n        iter: Callable[[Sequence[str]], None]) -> List[str]:\n\t\"\"\"\n\tAdds items as sequence to the sequence provided,\n\tand returns a list of unique"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tmatch = r'^v(+(\\d+))?'\n\tcompressed = r'^(?P<version>\\d+)(-(\\d+))?'\n\n\tfor line in s.splitlines():\n\t\tif not re.fullmatch(match, line, flags=re.X):\n\t\t\td = default(line)\n\t\t\td[0]\n\t\t\tlinesuffix = d[2]\n\t\t\tif not re.match('+(\\d+)\\.(\\d+)-', linesuffix):\n\t\t\t\tlinesuffix = '.' + linesuffix\n\t\t\tm"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tp1, p2 = s.rsplit(\".\")\n\tif p1.startswith(\".\"): p1 = \"_\", p1[1:]\n\tif p2.startswith(\".\"): p2 = \"_\", p2[1:]\n\tp1 = int(p1)\n\tp2 = int(p2)\n\n\treturn tuple(0 <= p1 <= 2**32-1 and 2**(p2-p1+1) <= 1<<32-1 or (_sys.maxint,))\n\ndef parse_version_info(s"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\treturn tuple([int(x) for x in (s.split('.')[0], s[s.find('.'):-1], s[-1:])])\n\n\ndef compare(lhs, rhs) -> None:\n\t\"\"\"docstring for compare\"\"\"\n\n\tlhs_v, lhs_f = parse_version(lhs)\n\trhs_v, rhs_f = parse_version(rhs)\n\n\tif lhs_v > rhs_v:\n\t\treturn -1\n\telif lhs_v < rhs_v"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif not (isinstance(s, str)):\n\t\traise TypeError(\"Version argument must be str\")\n\ttry:\n\t\tver_int = int(s)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid version number: %r\" % s)\n\treturn ver_int\n\n__all__ = [\"parse_version\"]\n\nclass PEP440CompatWarning(DeprecationWarning):\n\tpass\n\n\n# NOTE: do not add.format() or similar back-compat syntaxes from here\n# backwards-compat to here, or any other code that"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif s == \"0.0.0\":\n\t\treturn 1, 0\n\tif s == \"0.1.0\":\n\t\treturn 1, 0, 0\n\treturn 0, 0, 0\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tv = 2\n\tr = '2.2.3'\n\tq = '2.2.3.4'\n\n\tif s[0] == 'v':\n\t\ttry:\n\t\t\tr, v = v, int(s[1:])\n\n\t\texcept ValueError:\n\t\t\treturn tuple(), tuple(), tuple()\n\n\tif s.find('=') == -1:\n\t\treturn q.split('.'), q, q\n\n\ts = s.split('=')\n\n\tver = str\n\n\tm = re.match(vre_regex, s["}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\treturn (0, 0, 0) if s.startswith(\"__\") else tuple(map(int, s.split()))\n\n@memoize(weak=True)\ndef get_version_hash_string(version: str) -> str:\n\treturn hash256(version).encode(\"hex\")\n\ndef get_version(version_string: str) -> Tuple[int, int, int]:\n\treturn (0,) + parse_version(version_string)[1:]\n\ndef get_build(version_string: str) -> Tuple[int"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t# The '=' to be consistent even for Python2, Python3 and unspecified:\n\t# http://bugs.python.org/issue25891\n\tif not s.startswith(\"major\") or len(s) < 1 or not s[1:] in {'beta', 'dev', 'alpha', 'rc', 'pre'}:\n\t\treturn 0, ()\n\n\t_version_pattern = r\"\"\"\n[\\s\\+\\-]?\n\t([0-9]{0,4})                    (x[0-9a-f]+)            (?P<"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\"\"\"\n\t\tif a < b:\n\t\t\treturn a,b\n\t\telse:\n\t\t\treturn b,a\n\t\"\"\"\n\n\timport re\n\tmatch = re.match('(?P<name>[a-z]+[a-z]+[0-9]+\\.', s)\n\n\tif match:\n\t\ta = match.group('name')\n\t\tb = parse_version('.'.join(a[0].zfill(8).split('.')[:2]).split('.')[0])\n\n\treturn match\n\ndef main(version=None):\n\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tVERSION = float(sys.executable) if os.path.isabs(sys.executable) else os.path.expath(sys.executable)\n\ttry:\n\t\timport pkg_resources, sys\n\t\treturn pkg_resources.parse_version(VERSION) # use the version provided in the executable\n\texcept ImportError:\n\t\t# fall back to the os.python executable\n\t\ttry:\n\t\t\tf = open(sys.executable, 'rb')\n\t\t\treturn tuple(os.readline(len(os.fsencode(b))).rstrip"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# make command path normalized\n\treturn (to_unicode(os.path.normpath(os.path.realpath(cmd[0])))\n\t\t+ '\\\\' + to_unicode(os.path.basename(cmd[1])),\n\t\tto_unicode(os.path.dirname(cmd[0])))\n\ndef convert_dir_permissions_from_windows(path, ignore):\n\t\"\"\"\n\t\tTry to convert a path to a normal path that uses the Windows permission\n\t\tsystem rather than the Unix ones for symlinks. This is basically"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tcmd = list(cmd)\n\tif cmd and (cmd[0]!= '$' or len(cmd) == 1 or not cmd[1]):\t# no $ in shebangs\n\t\tcmd = [\"(\", cmd[0], \")\", \")\",cmd[1]]\n\tif cmd[0].endswith('$'):\n\t\tcmd[0] = cmd[0][:-1]\n\t\t\n\tfor cmd_part in cmd:\n\t\tif len(' '.join(cmd_part)) >= 64: # if a very long shebang is used, the extra spaces"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tshebangs = re.finditer(ur'\\{(?:^|;)\\{', cmd)\n\t\n\tout = []\n\tfor sheb in shebangs:\n\t\tshebv = sheb.group()\n\t\tshebv = shebv.split(\"\\{\")\n\t\tshebv = [s.strip() for s in shebv][0]\n\n\t\tif shebv in {\"$PATH\", \"\\\\\\\\\" + shellescape(pdir), \"D:\" + shellescape(pdir):\n\t\t\tpass # no change, as this is just"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\tif isinstance(cmd, (str, unicode)):\n\t\treturn cmd\n\n\tif issubclass(cmd, (str, unicode)) or isinstance(cmd, unicode):\n\t\treturn cmd, [], True\n\telif isinstance(cmd, list):\n\t\t# Python 2.7 is very weak\n\t\tcmd_parts = cmd\n\n\t\t# Remove empty parts\n\t\tparts = list(i for i in cmd_parts if i)\n\t\t# Add a single leading ':' if needed\n\t\tif all(i[0]!= ':' for i in"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif type(cmd) == str:\n\t\treturn (\"cwd\", \"cd '{}' && %s\", cmd)\n\tif isinstance(cmd, list):\n\t\treturn (\"cwd\", \"cd '.{}' && %s\", \"\\n\".join(cmd))\n\telif isinstance(cmd, tuple) and len(cmd) == 1:\n\t\tresult = None\n\t\tif cmd[0] == 'cwd':\n\t\t\tdirname, filename = os.path.split(cmd[0])\n\t\t\tif os.path.basename(os.path."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t\n\tdef __str__(cmd):\n\t\t\t\treturn \"/\".join([str(c) for c in cmd] + [\"%s\" % c for c in map(str,cmd)])\n\t\t\t\t\n\t\t\t\t\ndef normalize_cmd(cmd):\n\tnormalize= lambda s: \"/\".join([str(c) for c in s])\n\tnormal= lambda s: normalize(s.strip().replace(\":,\",\":\"))\n\n\t\n\treturn normal(cmd)\n\t\t\t\n\n\ndef __normalize_cmd(cmd, n):\n\t# normalize"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif sys.platform == 'win32': \n\t\t# this should work everywhere, but I don't know exactly why.\n\t\treturn cmd \n\t\t\n\ts = bytes[:]\n\tif type(cmd) == tuple:\n\t\ts = s + list(cmd)+ list(bytes)\n\ttry:\n\t  # normalize for os.popen\n\t  proc = os.popen(cmd, \"r\")\n\t  for i in range(2):\n\t    s = s + \"&\"*10+\"\\n\"\n\t    for line in iter"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif not isinstance(cmd, list) or len(cmd) == 0:\n\t\traise Exception\n\tresult = []\n\tfor item in cmd:\n\t\tif not isinstance(item, str): \n\t\t\traise Exception\n\t\tresult.append(item)\n\treturn result\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tp = posixpath_split(os.path.basename(cmd[0]))\n\t# convert windows shebangs to a non-windows sha1, and then to an old-style shebalt\n\t# for better security\n\td = sha1(d = p[1] if (p[1].startswith(\"nt\") or p[1].startswith(\"posix\")) else \"\")\n\t# don't append a trailing '.'; it would fail on windows in which we don't\n\t# want to have the leading ':' character, which"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tcmd =''.join(cmd) if isinstance(cmd, str) else cmd\n\tcmdlen = len(cmd)\n\tif cmdlen == 1:\n\t\treturn cmd\n\tif cmdlen == 2:\n\t\treturn cmd\n\tendp = cmd[cmdlen - 1]\n\treturn''.join([endp, cmd, endp, cmd])\n\t\ndef replace_shebangs(c):\n\t''' This function is only required for windows\n\t'''\n\timport shebang\n\t\n\ttry:\n\t\treturn shebang.re.sub("}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef memoized(*args, **kwargs):\n\t\tkwargs = dict(*kwargs)\n\t\tif lock:\n\t\t\tkwargs['lock'] = lock\n\t\tif cache:\n\t\t\tobj = cache.get(key, *args, **kwargs)\n\t\t\tif obj is not None:\n\t\t\t\t# We only return value if we could get value or cache key\n\t\t\t\treturn obj or key(*args, **kwargs)\n\t\treturn key(*args, **kwargs)\n\treturn memoized\n\n\ndef memoize(function, maxmemsize = 30):\n\t\"\"\"\n\t\tDec"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef memoizer(f):\n\t\td = {}\n\t\tdef wrapper(*args, **kw):\n\t\t\tkey = hashkey(args, kw)\n\t\t\tvalue = cache.get(key, f(*args, **kw), lock=lock)\n\t\t\tif value is None or value == f:\n\t\t\t\td[key] = (value, f(key, args, kw))\n\t\t\telse:\n\t\t\t\td[key] = (value, f(key, args, kw))\n\t\t\treturn d\n\t\t@wraper\n\t\tdef wrapped(*args, **kw):\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tfunc = cache\n\tkey = key or hashkey\n\tlock = lock or lock or Lock()\n\n\tdef wrapper(*args):\n\t\ttry:\n\t\t\tres, lock.acquire(block=True)\n\t\texcept:\n\t\t\treturn func(*args)\n\n\t\tret = res\n\t\tcache[key] = ret\n\t\treturn ret\n\n\treturn wrapper\n\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tfunc.cache = cache or {}\n\t\tfunc.key = key\n\t\tfunc.lock = Lock()\n\t\treturn func\n\twrapper.cachedict = cache\n\twrapper.key = cachekey(key)\n\twrapper.docstring = 'cache'\n\twrapper.__name__ = name\n\tdef wraper(f):\n\t\tf = wrapper(f)\n\t\tfunc_name = f.__name__\n\t\t@wraps(f)\n\t\tdef wrap(*args, **kwargs):\n\t\t\twith wrapper.lock:\n\t\t\t\tf ="}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef __cache_helper(func):\n\t\t@wraps(func)\n\t\tdef __cache_wrapper(key, *args, **kwargs):\n\t\t\tfunc_key = hashkey(key)\n\t\t\tif lock is None:\n\t\t\t\tdata=tuple(kwargs.values()) # the kwargs can be None\n\t\t\t\tcache.set((func_key, data)) # add a single result\n\t\t\telse:\n\t\t\t\tcache[func_key].update(data)\n\t\t\trv = func(*args, **kwargs) # return the original function result\n\t\t\tcache."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrapped(*args, **kwargs):\n\t\tfrom gensim.corpora.dictionary import DictionaryBase\n\t\tcache[(key(key), args, kwargs)] = cache_func(\n\t\t\tcached(cache, key=hashkey, lock),  **kwargs)\n\t\treturn cached(cache, key=hashkey, lock)()\n\treturn wrapped\n\n\nclass BatchReduceOp(cached(lambda f: f, hashkey, lock=None)):\n\t\"\"\"\n\tThis can now be a decorator\n\n\t``func`` will first be passed"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(*args, **kwargs):\n\t\tcache.add(key=key, args=args, kwargs=kwargs, lock=lock)\n\t\treturn cache.current()\n\treturn decorator\n\ndef uncached(func):\n\t\"\"\"\n\tDecorator to return a unmodified function cached inside an uncached\n\tfunc. The cached function must be reentrant in some way, and can only\n\tbe used directly.\n\t\"\"\"\n\tdef uncached(*args, **kwargs):\n\t\tcache.get(cachekey=rekey(func"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tkey = hashkey(key)\n\tfunc = getattr(cache, '%s_%s' % (key, k))\n\tvalue = func(key, k, cache, lock)\n\tcache[key] = value\n\treturn value\n\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef memoizer():\n\t\t\"\"\"\n\t\t\tReturn a function that memoizes results.\n\t\t\tThe cache is filled with an item which is a (key, value) pair.\n\n\t\t\t:return: a cached function\n\t\t\"\"\"\n\t\tcache[lock] = key, key()\n\t\treturn key\n\treturn memoizer\n\nclass Cache(object):\n\tdef __init__(self):\n\t\tself.cache = {}\n\t\tself.lock = Lock()\n\n\tdef set_result(self, key, value, cachekey, lock=None):\n\t\t\"\"\"\n\t\t.. attribute: cachekey"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorating(*args, **kwargs):\n\t\tassert cache is not None\n\t\tif lock is not None:\n\t\t\t# Use explicit lock\n\t\t\treturn cache.set(key, args[0], timeout=cache.default_timeout, lock=lock)\n\t\telse:\n\t\t\treturn cache.set(key, args[0], timeout=cache.default_timeout)\n\n\treturn decorating\n\nif __name__ == \"<app>\":\n\tdef callable(f):\n\t\treturn lambda *args: invoke(f, args, 1, cache=None)\nelse:\n"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\ttry:\n\t\t\treturn args[0], cache[timeout(args[1], maxsize, timer, typed=typed)]\n\t\texcept KeyError:\n\t\t\treturn args[0], None\n\treturn wrapper\n\n\nclass Cache(object):\n\t\"\"\"\n\t\tA `MutableMapping` wrapping a `Cache` `dict`.  Key values will be\n\t\tpassed to the decorated function when first accessed.  The item\n\t\tis cached before passing it over.  This avoids the need for\n\t\ta call to `cache[obj]"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorate(func):\n\t\tif ttl==None:\n\t\t\traise TypeError(\"ttl must be supplied\")\n\t\t\t\n\t\tdef cache_wrap(*args, **kwargs):\n\t\t\tretval = func(*args, **kwargs)\n\t\t\n\t\t\tif 'ttl' in kwargs or len(kwargs)==1:\n\t\t\t\tttl = kwargs.pop('ttl')\n\t\t\telse:\n\t\t\t\tttl = None\n\t\t\t\n\t\t\tretval = cache_wrap(retval, cache_wrap, ttl=ttl, maxsize=max"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t@wraps(fun)\n\tdef ttl_fn(arg):\n\t\tresult = None\n\t\tif not isinstance(arg, tuple):\n\t\t\tif type(arg) is bool:\n\t\t\t\treturn arg\n\n\t\t\telif not isinstance(arg, list):\n\t\t\t\treturn arg\n\n\t\t\telif len(arg) > maxsize:\n\t\t\t\treturn arg[0]\n\t\telse:\n\t\t\targ = arg[0]\n\n\t\tif maxsize and len(arg) > maxsize:\n\t\t\treturn arg[1]\n\t\t\n\t\tcache = []\n\t\tif not typed:\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\tdef wrap(self, *args, **kwargs):\n\t\t\"\"\"\n\t\t\tWraps a function\n\t\t\"\"\"\n\t\ttokens = collections.deque(args)\n\t\tstart = time.monotonic()\n\t\twhile tokens:\n\t\t\titem = list(tokens)\n\t\t\ttokens.popleft()\n\t\t\ttry:\n\t\t\t\tfunc = kwargs[item[0]]\n\t\t\texcept:\n\t\t\t\tfunc = item[1]\n\t\t\t\t\n\t\t\tif typed:\n\t\t\t\tif isinstance(func, collections.Callable):\n\t\t\t\t\treturn func(item"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tif isinstance(ttl, float):\n\t\tttl = int(round(ttl))\n\tttl = min(maxsize, ttl)\n\tdef ttl_store(timestamp):\n\t\tif min(ttl, maxsize)*2*min(timer/1000 - 1,1) < (time.monotonic() - timestamp):\n\t\t\treturn timestamp\n\t\tttl_prev = min(ttl, maxsize)\n\t\tttl_prev2 = ttl\n\t\tfor v in _tupleify_dict(data):\n\t\t\tv[\"ttl"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef _memovercup(func):\n\t\t\treturn ttl_memovercup(func=func, maxsize=maxsize, ttl=ttl, timer=timer,\n\t\t\t\t\ttyped=typed)\n\t\treturn _memovercup\n\t\n\tdumps = builtins.raw_getitem\n\tloads = builtins.raw_getitem\n\t\n\t@_memovercup\n\t@functools.wraps(func)\n\tdef _wrap(self, arg):\n\t\treturn func(self, arg)\n\t\n\tdef make_memovercup"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(fn):\n\t\t# ensure we keep a track of the result of the wrapped function.\n\t\tresult = fn ()\n\n\t\t# if we have already computed enough values there is nothing left to do\n\t\tif not type(result) == tuple:\n\t\t\tresult = tuple(result)\n\n\t\tif len(result) > 1:\n\t\t\t# generate a list of results\n\t\t\tres = []\n\t\t\t# we need to store the most recently used one, to avoid\n\t\t\t# having to re-compute it.\n\t\t\tres.append(result[0])\n\t\t\tdelta = maxsize"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = collections.deque(maxsize)\n\tp = partial(cache.get, key)\n\tdef decorator(func):\n\t\tcache.append(p('.'.join(func.func_name.split('_')[:-2])))\n\t\tdef wrapper(*a, **o):\n\t\t\tttl = o.get('ttl')\n\t\t\tif ttl is None:\n\t\t\t\treturn func(*a, **o)\n\t\t\tfunc(*a, **o)\n\t\t\tif len(cache) > ttl:\n\t\t\t\tmaxsize, value = max(\n\t\t\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator (func):\n\t\tdef callfunc(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\t@param func: The function to run.\n\t\t\t@type func: function\n\t\t\t@returns: The value passed in.\n\t\t\t@rtype: anything\n\t\t\t\"\"\"\n\t\t\trv = func(*args, **kwargs)\n\t\t\tif rv is not None:\n\t\t\t\tif kwargs:\n\t\t\t\t\tkwargs = func(*args, **kwargs)\n\t\t\t\t\tkwargs['rv'] = rv\n\t\t\t\tif rv not in ttl_cache(maxsize,"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\ttimeout_max = maxsize if maxsize > 0 else 1000 // 2\n\n\tdef decorate(func):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tttl_value = _ttl_cache(result, ttl)\n\n\t\t\t\tif typed:\n\t\t\t\t\tfunc_name = func.__name__\n\t\t\t\telse:\n\t\t\t\t\tfunc_name = func.__class__.__name__\n\n\t\t\t\tkey = \"%s(%s, %s)\" % (func_name, args, kwargs)"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tmru = LRUCache(maxsize)\n\t\t@wraps(func)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\thits = mru.popitem(key_func=key, val_func=lambda o: o['time'])\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\twhile hits:\n\t\t\t\t\to = hits.next()\n\t\t\t\t\tif (o['time']+0.1 > time):\n\t\t\t\t\t\tbreak\n\t\t\t\tmru()\n\t\treturn wrapped\n\treturn"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tclass MemoizedCallable:\n\t\t\"\"\"\n\t\tclass MemoizedCallable:\n\t\t\tdef __init__(self, key):\n\t\t\t\tself.key = key\n\t\t\tdef __call__(self, *args, **kw):\n\t\t\t\tkey = self.key()\n\t\t\t\tresult = cache.get(key)\n\t\t\t\tif result is None:\n\t\t\t\t\tresult = self.call(*args, **kw)\n\t\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\t\"\"\"\n\t\tdef __init__(self, key):\n\t\t\tself.key = key\n\t\tdef __call"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapped_fn(*args, **kwargs):\n\t\t\"\"\"\n\t\tWraps the function or method callable with a mapping based\n\t\ton MRU and does an eviction by returning the newest entries as\n\t\t`unseen`. If the return value is an iterable, the iteration is\n\t\tchained.\n\n\t\tArgs:\n\t\t\tmaxsize (int): maximum number of items to retrieve by the\n\t\t\t\tcache in a single call\n\t\t\ttyped (boolean, optional): whether the result is of type\n\t\t\t\tIterable (for list or dicts return iterable, default: False)\n\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tclass MRU_cache(mru_cache):\n\t\tdef __init__(self):\n\t\t\tmru_cache.__init__(self)\n\t\t\tself.mtimes={}\n\t\tdef __call__(self,fn):\n\t\t\tif type(fn) == types.FunctionType:\n\t\t\t\tfunc = fn\n\t\t\telse:\n\t\t\t\tfunc = partial(fn)\n\t\t\told_time=self.mtimes.get(func,None)\n\t\t\tself.mtimes[func]=old_time+self.cache_time\n\t\t\treturn cached(func, maxsize=max"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(wrapped):\n\t\tdef wrapped_function(*args, **kwargs):\n\t\t\tkey = cache.get(wrapped, maxsize, typed=typed)\n\t\t\tif key is None:\n\t\t\t\tkey = wrapped(*args, **kwargs)\n\t\t\t\tcache.set(wrapped, key, maxsize, typed=typed)\n\t\t\telse:\n\t\t\t\tif len(args) == 1 and isinstance(key, int):\n\t\t\t\t\t# avoid repeating the same argument twice\n\t\t\t\t\ttry:\n\t\t\t\t\t\ta, n = args\n\t\t\t\t\t\texpected_key = key + n\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef mru(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tkey = func(*args, **kwargs)\n\t\t\t\tcache = mru.cache\n\t\t\t\tkeycache = cache.get(key)\n\t\t\t\tif keycache is None:\n\t\t\t\t\tif typed:\n\t\t\t\t\t\treturn key\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tmru.cache[key] = value\n\t\t\t\t\treturn value\n\t\t\texcept (AttributeError, KeyError, TypeError):\n\t\t\t\tif typed:\n\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef f(*self):\n\t\tif len(self.items) >= maxsize:\n\t\t\tself.items.popitem(last=False)\n\t\telse:\n\t\t\titem = (maxsize, self.items.get(maxsize))\n\t\t\tdel self.items[item[0]]\n\t\treturn f(*self)\n\tif typed:\n\t\tf.typed = True\n\treturn f\n\nclass Counter(dict):\n\t\"\"\"\n\tA dict that counts the number of times a string has\n\tbeen used to make that string.\n\n\tIf the string is not a member"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t@wraps(f)\n\tdef check(value):\n\t\ttry:\n\t\t\tvalue1 = f(value)\n\t\t\t#value1 = f(value)\n\t\t\ttry:\n\t\t\t\tif value1 > maxsize:\n\t\t\t\t\tmaxsize = value1\n\t\t\texcept: # XXX: what to do about the value?\n\t\t\t\t#do nothing\n\t\texcept: # XXX: what to do about the value?\n\t\t\tpass\n\t\treturn value\n\n\treturn check\n\ndef rcu_cache(maxsize=128):\n\ttry:\n\t\timport re\n\t\timport hashlib\n\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef wrapper(*args, **kwargs):\n\t\ttry:\n\t\t\tc = cache[args]\n\t\texcept KeyError:\n\t\t\tc = cache[args] = cache[args] or mru_cache(maxsize, 'cached:%s' % args)\n\t\treturn c(*args, **kwargs)\n\n\treturn wrapper\n\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(f: Callable):\n\t\tif f.__name__.startswith('mrurcache_'):\n\t\t\traise AttributeError('Cannot apply decorator to class %s' % f.__name__)\n\n\t\tif f.__name__ in cache_attributes_tuple:\n\t\t\tif maxsize is None:\n\t\t\t\traise AttributeError('You cannot use mru_cache: maxsize is '\n\t\t\t\t\t\t    'undefined')\n\n\t\t\t# This should do almost no harm, if we have already seen\n\t\t\t# a few examples. In typical usage"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\treturn partial(lru_cache_item, maxsize=maxsize, typed=typed)\n\n\ndef lru_cache_item(key, value):\n\t\"\"\"\n\tHelper function for creating a key/value from a function.\n\t`key` should be a function which accepts and returns a value, which\n\twill be cached.\n\t\"\"\"\n\t_time_begin()\n\tvalue_cache = LRUDict((key.__name__, key), typed=False)\n\tvalue_cache[key] = value.__dict__\n\t_time_end()"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(f):\n\t\tdef innerF():\n\t\t\tref = f()\n\t\t\tret = None\n\t\t\tcache = None\n\t\t\twhile ref:\n\t\t\t\tif cache:\n\t\t\t\t\tfor i, v in enumerate(cache):\n\t\t\t\t\t\tif v!= ref:\n\t\t\t\t\t\t\tyield i, ref = v, ref\n\t\t\t\t\t\tbreak\n\t\t\t\tret = ref\n\t\t\t\tcache = [ref]\n\t\t\t\tref = f()\n\t\t\tif not cache or not ret:\n\t\t\t\t# Cache full for 1 or 1 entry.\n\t\t\t\tfor i in xrange(1, maxsize):\n\t\t\t\t\t# Insert one or two"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tclass LRU(object):\n\t\tdef __init__(self, maxsize):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.count = 0\n\t\t\tself.data = {}\n\t\t\n\t\tdef __call__(self, *args, **kwargs):\n\t\t\tif kwargs.get(\"result\", None):\n\t\t\t\tolddata = kwargs.get(\"result\")\n\t\t\t\tself.count += 1\n\t\t\t\tlastid = self.maxsize + 1\n\t\t\t\tlru = LRU(lastid)\n\t\t\t\ttry:\n\t\t\t\t\tlru.data[lastid]"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif typed:\n\t\tfunc = type(func)(func)\n\telse:\n\t\tfunc = lambda f: weakref.ref(func)(f)\n\n\tdef wrapper(*args, **kwargs):\n\t\t\"\"\"\n\t\tWraps the function with a memoize function.\n\n\t\tParameters\n\t\t----------\n\t\targs : args to pass into func\n\n\t\tkey : argument to pass into memoize function\n\t\t\"\"\"\n\t\tkey = args[0]\n\t\targs = args[1:]\n\n\t\t@memoize\n\t\tdef callback(func, key, *args, **kwargs):"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tcache = LRUCache(maxsize, typed=typed)\n\t\ttry:\n\t\t\tvalue, created = cache[args]\n\t\texcept KeyError:\n\t\t\tvalue = None\n\t\tif cache.size == 0 or (value is None and not created):\n\t\t\tvalue, created = cache.setup(*args, **kwargs)\n\t\telse:\n\t\t\tvalue.update(args)\n\t\tresult = value.update(*args, **kwargs)\n\t\tif created and value is not None:\n\t\t\tcache.size = max(0"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tc = f(*args, **kwargs)\n\t\t\tif c is not None:\n\t\t\t\ttry:\n\t\t\t\t\tcache = c._lru_cache = lru_cache(maxsize=maxsize, typed=typed)\n\t\t\t\texcept:\n\t\t\t\t\tcache = lru_cache(maxsize=maxsize, typed=typed)\n\t\t\t\tcache2 = cache._cache2\n\t\t\telse:\n\t\t\t\tcache = None\n\t\t\t\tcache2 = None\n\t\t\t\n\t\t\treturn c\n\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef f(*a, **b):\n\t\t\tcache = lru_cache(maxsize, typed=typed, maxentries=maxsize)\n\t\t\tentries = cache.get(func)(*a, **b)\n\t\t\t# If there are no entries, simply return it out altogether\n\t\t\tif entries == []:\n\t\t\t\treturn a, b\n\t\t\t# If an entry isn't present, just remove it and return it\n\t\t\t# until the maximum size is hit\n\t\t\tcache.pop(func, None)\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tcache_key = args[0]\n\t\t\texcept IndexError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tif cache_key in cache:\n\t\t\t\t\treturn cache[cache_key]\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[cache_key] = result\n\n\t\t\ttry:\n\t\t\t\treturn cache[cache_key]\n\t\t\texcept KeyError:\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\ndef cache(maxsize=128, typed="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(*args, **kwargs):\n\t\t\"\"\" Call the wrapped function. \"\"\"\n\t\tcache = LRUCache(*args, **kwargs)\n\t\tkeyfunc = partial(cache.get, key)\n\t\t@wraps(func)\n\t\tdef wrapper(*args):\n\t\t\t\"\"\" Cache the result of the callable, then return the value\n\t\t\t(which will be cached if later called). \"\"\"\n\t\t\tresult = yield keyfunc(*args)\n\t\t\tcache.set(keyfunc(*args), result)\n\t\t\treturn result\n\t\treturn wrapper\n\n\treturn decorator\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef memoized(func):\n\t\tcache = {}\n\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = (func.__module__, func.__name__, args, kwargs)\n\t\t\ttry:\n\t\t\t\tvalue = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\n\t\t\tret_val = value\n\t\t\tif typed:\n\t\t\t\tret_val = ret_val()\n\t\t\treturn ret_val\n\n\t\treturn wrapper\n\n\tdef decorator"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef _newfunc_wrapped_memop(func):\n\t\treturn Memoized(\n\t\t\tlambda : func(),\n\t\t\tfunc._result_cache,\n\t\t\tmethodname=func._methodname,\n\t\t\tmaxsize=maxsize,\n\t\t\ttyped=typed,\n\t\t)\n\n\treturn _newfunc_wrapped_memop\n\n\n@memoize\ndef lfu_memop_cache(func, maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `max"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoize(fn):\n\t\tfn = fn\r\n\t\tdef func(*args):\r\n\t\t\tfetch = {}\r\n\t\t\tmaxsize = max(maxsize, max(args))\r\n\t\t\tfor k, v in itertools.izip(args, itertools.count()):\r\n\t\t\t\tfetch[k] = fetch.get(k, 0)+1\r\n\t\t\twhile fetch:\r\n\t\t\t\tused = fetch.getMax()\r\n\t\t\t\tfor k, v in itertools.chain.from_iterable(\r\n\t\t\t\t\titertools.repeat(None"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef _wrap(f):\n\t\tmemo = lfu_cache._get(f) # initialize\n\n\t\tdef _run():\n\t\t\ttry:\n\t\t\t\treturn f()  # can catch exceptions, in order to call.sum()\n\t\t\tfinally:\n\t\t\t\tmemo[_get_cache_key(f)] += 1\n\t\treturn _run\n\treturn _wrap\n\n@lfu_cache(maxsize=128, typed=False)\ndef random_cache(maxsize=128, maxsize_histogram=1024):\n\t\"\"\"\n\tDecorator to wrap a function"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\t\tUsing:\n\t\t\t@luf_cache(maxsize=128, typed=True)\n\t\t\tdef func() -> None:\n\t\t\t\t...\n\t\t\"\"\"\n\t\tcache = type_cache(func)\n\t\tdef lfu(*args, **kwargs):\n\t\t\tres = cache.execute(func, *args, **kwargs)\n\t\t\tif res:\n\t\t\t\treturn res\n\t\t\telse:\n\t\t\t\treturn cache.execute(func, *args, **kwargs)\n\t\treturn lfu\n\treturn decorator\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tif args:\n\t\t\tyield *args\n\t\telse:\n\t\t\tif typed:\n\t\t\t\ttry: result = kwargs['cache'] = \\\n\t\t\t\t\tklass(*values)()\n\t\t\telse: result = kwargs['cache'] = klass(*values)()\n\n\t\t# check if the cache was already stored and not the latest one\n\t\t# this avoids a cache miss if the cache is slow\n\t\tcache = cache_get()\n\t\tlru_key = time()\n\t\tnew = None\n\n\t\twhile True:\n\n\t\t\tif"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef lazy(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tmemoized = kwargs.get('memoized', False)\n\t\t\tsaved = f(*args, **kwargs)\n\t\t\tif save:\n\t\t\t\tk = sorted(store(saved, key=key, maxsize=maxsize),\n\t\t\t\t\tkey=key, reverse=True)\n\t\t\t\tv = saved\n\t\t\t\tif not k[0].startswith('c'):\n\t\t\t\t\tv = v[:1]\n\t\t\t\tif kv2 > k and kv2 < v:\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoize(function):\n\t\t@wraps(function)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tresult = function(self)\n\t\t\tfn = args[0]\n\t\t\tif type(fn)!= function:\n\t\t\t\treturn fn\n\t\t\tif fn == None:\n\t\t\t\treturn result\n\t\t\tif kwargs.get('timeout') == None:\n\t\t\t\tkwargs['timeout'] = self.timeout\n\t\t\tret = None\n\t\t\ttry:\n\t\t\t\tret = fn(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\targs[0]"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef f(self):\n\t\tif not hasattr(cached, self.fkey):\n\t\t\tdef cache_wrapper(func):\n\t\t\t\tcached[self.fkey] = func\n\t\t\t\treturn func\n\t\t\tcache.caches[self.fkey] = cache_wrapper\n\t\treturn cached[self.fkey](self)\n\tf.__name__ = 'lfu_cache'\n\tif not typed:\n\t\tf_info = inspect.getgeneratorfunctions(f)[0][2]\n\t\tf.__doc__ = '''#' + '''"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\targs = list(args)\n\t\tself = args.pop()\n\t\t_args, _kwargs = {}, {}\n\t\targs.extend(args)\n\t\tkwargs = kwargs.copy()\n\t\tif kwargs:\n\t\t\t_kwargs[args[0]] = kwargs[args[0]]\n\t\t\tdel args[0]\n\n\t\tmemo = dict.fromkeys(args, 1)\n\t\ttry:\n\t\t\tval = fn(*args, **kwargs)\n\t\texcept StopIteration:\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorated(*args, **kwargs):\n\t\tkey = args[0]\n\t\ttry:\n\t\t\tval = kwargs.get(key)\n\t\texcept (KeyError, IndexError) as exc:\n\t\t\tlogger.error(\"Invalid key in lfu_cache %s\", key)\n\t\t\tlogger.error(str(exc))\n\n\t\tif val is not None and len(args) <= 1:\n\t\t\treturn val\n\t\t\n\t\tif typed:\n\t\t\treturn val\n\t\t\n\t\tif len(args) > 1:\n\t\t\treturn func(*args, **kwargs"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif self.__item:\n\t\titem = self.__item\n\t\tself.__item = item[1]\n\t\treturn item[0], item[1]\n\telse:\n\t\treturn None;\n\t#!!! not needed\n\ndef __new_keyvalue(self, key, value):\n\tif not isinstance(key, str):\n\t\traise ValueError(\"key must be of type str, not None\")\n\n\tif not hasattr(self, '__key_value'):\n\t\tself.__key_value = self.__newdict()\n\n\tif"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif len(self.dict) <= 0:\n\t\traise KeyError(\"No key passed\")\n\tindex = self.dict.index(key)\n\tdel(self.dict[index])\n\treturn index\n\ndef update(self, kv_list, key):\n\t\"\"\"\n\t\tAdds a key and a list of values to the dictionary.  \n\t\t\n\t\t`self.add([key, values])` and `self.update({key, values => True})` both add some value to value's dictionary, \n\t\tbut the previous value is deleted from dictionary.\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tfor index in range(self.size):\n\t\tif self[index] == key[1]:\n\t\t\tself.popitem(index)\n\t\t\treturn key[0], self[index]\n\traise KeyError\n\n\n# Private methods --------------------------------------------------------\ndef _check_key(key):\n\t\"\"\"\n\tIf key is the key we expect to be removed, then we raise KeyError.\n\t\"\"\"\n\tif key!= key[0]:\n\t\traise KeyError(\"Trying to remove '%s' from non-existent sequence %s\"%(key,self))\n\n\ndef _"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tdel self.items[self.ind[0]]\n\n\treturn\n\nclass KeyValueHandler(EventHandler):\n\tdef __init__(self):\n\t\tEventHandler.__init__(self)\n\t\n\tdef get_item(self, index, **kwargs):\n\t\titem = KeyItem(index)\n\t\tself.items[index] = item\n\t\treturn\n\t\n\tdef get_length(self, **kwargs):\n\t\treturn len(self.items)\n\ndef main():\n\tapp = tkinter.Tk(height=500, width"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif len(self) > 0:\n\t\treturn self[0]\n\telse:\n\t\treturn None\n\n\"\"\"\n\tSet a value for the key given.\n\"\"\"\ndef setitem(self, k, v):\n\tself [k] = v\n\n\"\"\"\n\tCheck if a `(key, value)` pair is valid.\n\tif the key exists in the `self` list, make a copy, apply `setitem` on it.\n\t\"\"\"\ndef valid(self, k, v):\n\treturn self[k]!= v\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif len(self) == 0:\n\t\traise ValueError(self)\n\treturn _PairTuple.__new__(self, self[0], self[-1])\t\t\t\n\t\n\t\ndef dup(self):\n\t\"\"\"\n\tAdd two tuples into the set, preserving insertion order. \n\t\"\"\"\n\t\n\treturn _PairTuple(self[0], self[-1])\t\t\t\n\t\n\ndef popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif self is NotImplemented: raise KeyError\n\to = self[:]\n\tdel self[:]\n\treturn o\n\n\ndef pop(self, key):\n\t\"\"\"\n\tRemove an item from the list if the given key.\n\t\"\"\"\n\to = self.get(key)\n\tdel self[o]\n\treturn o\n\n\ndef popattr(self, key):\n\tdel self[key]\n\treturn self[key]\n\n\ndef popitem(self):\n\titem = self.popitem(pkey=True)\n\tif item is"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself.c = self.c[0]\n\tself.d = self.d[0]\n\tr = self.c is None\n\tself.c = None\n\tself.d = None\n\treturn (self.c, self.d)\n\n\ndef search(s, c, f=lambda x: x):\n\t\"\"\"\n\t\tDetermine if the string `s` occurs in `c`\n\t\tand if it doesn't, run the optional function `f`\n\n\t\t:param s: String to be searched for\n\t\t:param c: The characters to be"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\treturn self._items.pop()\n\texcept:\n\t\traise KeyError\n\ndef popnext(self):\n\t\"\"\"\n\tRemove the next key (and return it).\n\t\"\"\"\n\ttry:\n\t\treturn self._values[self._pos]\n\texcept IndexError:\n\t\traise KeyError\n\t\t\ndef popitem(self):\n\t\"\"\"\n\tRemove the last key (and return it)\n\t\"\"\"\n\ttry:\n\t\treturn self._items[self._pos]\n\texcept:\n\t\treturn self._values[self._pos].copy"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self._key, self._val\n\n\nclass DictProxy(dictionary.Dictionary):\n\t\n\t__slots__ = ('d',)\n\t\n\tdef __init__(self, d=None):\n\t\tif d: self.setdefault(d)\n\t\n\tdef add(self, key, value):\n\t\tself[key] = value\n\t\t\n\n\tdef setdefault(self, key, value):\n\t\tif self.__contains__(key) or value is not None: raise ValueError()\n\t\tself[key] = value\t\n"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in D:\n\t\tD[key]=D[key].copy()\n\t\tD[key].pop(key,None)\n\telse:\n\t\tD[key]={}\n\t\tD[key][key]=default\n\n\treturn self\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tret = self._default()\n\tif key in ret and ret[key] is default:\n\t\tdel ret[key]\n\treturn self\n\ndef get(self, attr, default=_undef):\n\t\"\"\"\n\t\tReturn the default value from attr for a key in dict d\n\n\t\t>>> from test import dict\n\t\t>>> d = dict({'a': 1, 'b': 2})\n\t\t>>> d['a'].default\n\t\t1\n\t\t>>> d['b'].default\n\t\t2\n\n\t\t>>> d = dict(a=1, b=2)"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif default is None:\n\t\td = self.default\n\t\t\n\telse:\n\t\td = default\n\t\t\n\tself[key] = d\n\n\treturn d\n\t"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tdef __init__(\n\t\tself,\n\t\tkey,\n\t\tdefault=None,\n\t\ttype=None,\n\t):\n\t\t\n\t\tself.key = key\n\t\t\n\t\tself.__dict__[key] = default\n\t\t\n\t\tself.__dict__[key] = type()\n\nclass SetBson (dict):\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(SetBson, self).__init__(*args, **kwargs)\n\t\t\n\t\t# create a dict() of the objects that we can set\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self:\n\t\td = self[key]\n\t\tdel self[key]\n\t\treturn d\n\telse:\n\t\tself[key] = default\n\n\n\t\ndef list_items_by_prop(self, prop):\n\t\"\"\"\n\tD.list_items_by_prop(prop) -> (vcs, list_of)\n\n\tReturn the items of D which are property of self's item PropProp.\n\n\tprop = string\n\tD[prop] = {i:p for i, p in self.items()}\n\n\t:param prop:"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\ttry:\n\t\tif default in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key]=default\n\texcept KeyError, e:\n\t\tpass\n\treturn default\n\t"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tkey_set=self.__dict__.get(key)\n\tdefault_value=default\n\tif key_set is None:\n\t\tvalue=default_value\n\t\treturn value\n\telif isinstance(key_set, (list, tuple)) and not isinstance(key_set[0], str) and not isinstance(key_set[0], unicode):\n\t\treturn key_set[0]\t\n\tvalue=self.__dict__[key_set]\n\tif isinstance(value, list) or isinstance(value, tuple):\n\t\t"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self: return self.get(key)\n\tself.__defaults[key] = self.__defaults.get(key, default)\n\treturn self.get(key, default)\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self: return self[key]\n\tself[key] = default\n\treturn d\n\n\ndef setdefault_copy(self, key, default=None, shallow=False):\n\t\"\"\"\n\t\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\twith shallow copy if deep, the default value is not copied\n\t\tif shallow, shallow copy if deep and not existing_value,\n\t\tthe default value is not copied.\n\n\t\tNote: This functionality is implemented without using copy."}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key not in self:\n\t\tself[key] = default\n\t\treturn key\n\tv = self[key]\n\tdel self[key]\n\treturn v\n\t\n\t\n####def test(self, k):\n##\treturn \"True\" if k == self.__dict__.get(k) else \"False\"\n\t\n\t\ndef __cmp__(a, b):\n\treturn cmp(a,b) \n\ndef isequal(x, y):\n\treturn str(x) == str(y)\n\t\n\n## The following"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.__dict__.get(key, default)\n\ndef set(self, key, value):\n\tself.__dict__[key] = value\n\tself.notify()\n\ndef remove(self, key):\n\ttry:\n\t\tdel self.__dict__[key]\n\texcept KeyError:\n\t\tpass\n\ndef iteritems(self):\n\titems = self.items()\n\tfor k, v in items:\n\t\tyield (k, v)\n\ndef items(self):\n\treturn list(self[:] + self."}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tif default:\n\t\t\treturn default\n\t\telse:\n\t\t\traise KeyError(key)\n\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\td = default\n\tif not key:\n\t\treturn d\t\n\ttry:\n\t\tk = eval(key)\t\n\t\td = self.__dict__[k]\n\texcept AttributeError:\n\t\tpass\n\treturn d\n\t\ndef set(self, key, value):\t\n\t\"\"\"\n\tD.set(k, d) -> None\t\n\tSets D[k] to d.\n\t\"\"\"\n\tk = key\t\t\n\tself.__dict__[k] = value\n\ndef get_dict(self):\n\t\"\"\"Returns the"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\te = self.environ['URL_ENCODED_PATH_INFO']\n\tif e is None and key in self.PATH_INFO:\n\t\te = self.PATH_INFO[key]\n\t\t\n\tif e:\n\t\treturn e\n\t\n\treturn default\n\ndef safe_url_encode(s, safe_char_re='+\\-_'):\n    \"\"\"\n    Encode s into a bytestring with all '+' and '-', but not '_', for\n    possible matching of urls.\"\"\"\n    # FIXME:"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self[key] or default\n\texcept KeyError:\n\t\treturn default\n\n\t'''\n\treturn {\n\t  ''': _,\n\t  '$': '$',\n\t  '#': '#',\n\t  '(1)': '#0',\n\t  '(2)': '',\n\t  '(3)': ''\n\t}\n\t'''\n\t\ndef parse_string_or_list(string):\n\treturn [\n\t\tparse_string_or_list(x) for x in string\n\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif len(key) > 0:\n\t\t# key has key to look up, so get it\n\t\treturn getitem(self, key, default)\n\t\t\n\treturn default\n\n\n#=================================================================\nclass MyException(RuntimeError):\n\t\"\"\"\n\t\tA subclass of Exception with more detail\n\t\tif you need an error or more general error\n\t\tmake it a subclass of RuntimeError\n\t\"\"\"\n\tdef __init__(self, v, *args, **kwargs):\n\t\t\"\"\"\n\t\t\tcreate a subclass of Exception with more detail\n\t\t\tand an error argument\n\t\t\"\"\"\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.__dict__[key] if key in self.__dict__ else default\n\n\ndef get_keys_from_files(files):\n\t\"\"\"\n\tGet a dict of keys from filenames.  Files may be specified as a list of filenames or as a list of path_files (files and or extensions).  If the list contains multiple files with the same basename a new dict is returned that includes the full set of files.\n\t\"\"\"\n\tif isinstance(files, str):\n\t\tfiles = [files]\n\tm = re.match(\"\\|"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\td = default\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn d\n\ndef del(self, key):\n\t\"\"\"\n\tD.del(k) -> None\n\t\"\"\"\n\ttry:\n\t\tself.pop(key)\n\texcept KeyError:\n\t\tpass\n\ndef items(self):\n\t\"\"\"\n\tD.items() -> list of keys.  list is empty if D is empty.\n\t\"\"\"\n\treturn [(keyStr, self[keyStr]) for keyStr in self.keys()]\n\ndef items"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.__getitem__(key or getkey(key))\n\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tD[k] = True if D[k] is True\n\t\n\tD is implemented as map.  If k in D, the value D[k].value\n\t\n\t\"\"\"\n\treturn self.__getitem__(key) is True\n\n\n\n#def forkey(key, default=None):\n#\t\"\"\"\n#\tT.get(i[,d]) -> i if i in T, else d."}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry: self[key]\n\texcept KeyError: return default\n\treturn self.__dict__.get(key, default) or default\n"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif callable(cache):\n\t\tif not isinstance(key, tuple):\n\t\t\traise TypeError(\"expected \" + cache + \" as type of key\")\n\t\t_cache, key = key\n\t\tcache = makecache(cache)\n\telse:\n\t\t_cache = cache\n\t\tkey = cache\n\n\t# cache the method, and also ensure that no concurrent calls\n\t# to __call__ will result in the method being overridden.\n\tcache.__wrapped__ = _cache.__wrapped__\n\tcache.__call__ = _cache.__call__\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tM = type(key)\n\ttry:\n\t\tcache['{}{}'.format(M.__name__, key)] = value\n\texcept AttributeError:\n\t\t# the decorator is not able to find the name of the function\n\t\t# and we can't do much here, ignore this method\n\t\tpass\n\ttry:\n\t\tcache[`key`] = value\n\texcept TypeError:\n\t\t# the function doesn't exist\n\t\tpass\n\treturn cache\n\n@cachemethod('hashkey(key)')\ndef cachedmethod(f):\n\t\"\"\"\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef f(*args, **kwargs):\n\t\tif lock is None:\n\t\t\tlock = NotImplemented\n\t\tkey = cache(key, lock=lock)\n\t\tif isinstance(key, dict):\n\t\t\treturn NotImplemented\n\t\tcache[key] = cachedmethod(cache, key, 0, lock)\n\t\treturn type(key)(args, kwargs)\n\treturn f\n\ncachedmethod = partial(_cachedmethod, cache=_cache())\n\n@cachedmethod(cache=_cache(), lock=True)\ndef cachedfunc(*args,"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif lock:\n\t\tcache.l.acquire()\n\ttry:\n\t\treturn cachedmethod(cache[key], key, lock)\n\tfinally:\n\t\tif lock:\n\t\t\tcache.l.release()\n\ndef callmethod(cls, *args, **kw):\n\tlock = False\n\ttry:\n\t\tcache = collections.defaultdict(dict)\n\t\tfor arg in args:\n\t\t\tif len(arg)!= 1:\n\t\t\t\tvalue = None\n\t\t\t\tfor c in arg: value = c\n\t\t\telse:\n\t\t\t\tvalue = arg[0]\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tmemoizer = weakref.WeakKeyDictionary(cachedmethodcache)\n\tcache.get(key, lambda: memoizer(getattr(self, key)), lock)\n\treturn memoizer(cache.get)\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tcache__ = cache\n\tdef wrapper(f=_cache(f), lock=None):\n\t\tcache__.__setitem__(key, f())\n\treturn cacheable(wrapper, cache=cache, lock=lock)\n\t\n\n@memoize_function\ndef getattr(method, _attr):\n\t\"\"\"Get class or instance attribute\"\"\"\n\treturn getattr_meth(method, _attr)\n\ndef getattr_meth(method, attr, cache=True):\n\t\"\"\"Get a cached or memoized method\"\"\"\n\tcache__ = method"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef wrapped(*args, **kw):\n\t\treturn cache.get(key, cachevalue(key, None), lock=lock)\n\twrapped.func_name = key\n\twrapped.__doc__ = cache.__doc__\n\twrapped.__name__ = key[1:]\n\twrapped.__self__ = lock\n\twrapped.__module__ = sys.modules[__name__]\n\t\n\t\nclass memoized(object): # XXX for class\n\t\"\"\"\n\tDecorator for object \n\tDecorates a function with a cache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorator(function, cache=dict()):\n\t\t\tcache[hashkey(key)] = function\n\t\t\treturn function\n\treturn decorator\n\t\ndef cachedmethod(cache=dict(), lock=None, hashkey=hash) :\n\t\t\t\"\"\"\n\t\t\tDecorator to wrap a class or instance method with a memoizing\n\t\t\tcallable that executes a provided function without affecting\n\t\t\tthe return value of that method.\n\t\t\t\"\"\"\n\t\t\tdef decorator(function, caching=cache, \n\t\t\t\t\t\thashkey=hashkey):\n\t\t\t\tcache[hashkey(key)] = function\n\t\t\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef memo_wrapper(*args, **kwds):\n\t\t\"\"\"\n\t        Wrap method call with this decorator for caching.\n\n\t    \tCalled by decorator and wrapped method (eg., ``@decorated``\n\t        returns a wrapped function which calls the decorated method).\n\n\t        **Arguments*\n\n\t            - method (method)\tMethod to cache.\n\t\t\"\"\"\n\t\tkey, args, kwds = parse_key_and_args(key, args, kwds)\n\t\tdef newfunc(*args):\n\t\t\t\"\"\"\n\t            Wrap"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\tdef decorator(*args, **kwargs):\n\t\t\n\t\thash = cache(key)\n\t\t\n\t\ttry:\n\t\t\tretval = cache(key).__get__(args, cache)\n\t\texcept AttributeError:\n\t\t\tretval = cache(*args, **kwargs)\n\t\t\n\t\tret = retval()\n\t\tcache.append(key, ret)\n\t\n\t\treturn decorator(ret)\n\t\t\n\treturn decorator\n\t\n"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tstack_level = 0\n\tstack_offset = 0\n\tstack_depth = 0\n\tstack_depth = 0\n\tstack_depth_error = False\n\te_class, e_name, e_pos = traceback.extract_tb(e.__cause__)\n\tcurrent_line, current_frame = '', ''\n\tfor t, (lineno, name, position) in e.traceback.extended():\n\t\tstack_depth += 1 # Add to total stack level\n\t\t#print t\n\t\t\n\t\tif stack_depth > max_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\ts = str(e)\n\twhile max_level > 0:\n\t\t\n\t\tif s.find('{')!= -1:\n\t\t\t\n\t\t\tlevel = s[1:s.find('{')+1].strip(',')\n\t\t\n\t\t\tif max_level > max_path_level and level!= \"\":\n\t\t\t\ts = s.replace(e.message, \"[{}]\".format(level))\n\t\t\telse:\n\t\t\t\ts = s.replace(e.message, \"[\")\n\t\t\n\t\telif s.find('<')!= -1"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\tdef buildtext(text, data):\n\t\tout = str()\n\t\tif text is None:\n\t\t\treturn out\n\t\tout += f\"\\n  {len(text)}  {text}\\n\"\n\n\t\tfor i in [\"_stack\", \"_traceback\"]:\n\t\t\ttry:\n\t\t\t\tout += f\"{'-'*{(len(text)-len(data))}\\r\\n '- '}\"\n\t\t\t\tout += str(data[i]) + \":\\n\"\n\t\t\t\tfor item in [\"traceback.format_stack_module\", \"sys.exc_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ts_str = \"\"\n\ttry:\n\t\tcls_info = traceback.extract_stack(2)\n\texcept TypeError:\t# Python 3.8+\n\t\tcls_info = traceback.extract_stack(1)\n\tfor i in xrange(max_level):\n\t\t# level_start_index = min(i, max_level)-i\n\t\tlevel_stop_index = max_level-i\n\t\tif level_stop_index > 0 and level_stop_index < len(cls_info): # Last level\n\t\t\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tif sys.version_info.major <= 2:\n\t\treturn unicode(e).encode(sys.getdefaultencoding())\n\telse:\n\t\treturn str(e)\n\ndef raise_handler():\n\t\"\"\"\n\tRaises an exception with the exception class and all the children.\n\t\t:return The original exception\n\t:type Exception\n\t\"\"\"\n\treturn Exception('The world of life can be made to be easier!', e=self.__e)\n\n\ndef reraise_e(e):\n\t\"\"\"\n\tReplaces the original exception by a more friendly"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\trepr(repr(e)[19:-2]) # cut down to 19 extra whitespaces in case they happen to be blank\n\tif isinstance(e, (AttributeError, KeyError, TypeError, AttributeError)):\n\t\treturn repr(repr(e)[1:-1]) # cut down to 12 extra whitespaces in case they happen to be blank\n\telif isinstance(e, Exception):\n\t\texctype = e.__class__\n\t\tif not hasattr(e, '__name__'):\n\t\t\traise ValueError('invalid exception format {0"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t# Build the exception path (structure).\n\te.func_code = e.__cause__.f_code\n\te.filename = sys._getframe(1).f_code.co_filename\n\te.path = e.func_code.co_filename\n\te.lineno = sys._getframe(1).f_lineno\n\te.path = os.sep.join(e.path.split(sys.path[1:])[:-2])\n\te.lineno = sys._getframe(1).f_lineno\n\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tif inspect.isclass(e):\t\t\n\t\texc_type, exc_value = e.__name__, e\n\telse:\n\t\texc_type, exc_value = e.__class__.__name__, e\n\tout = \"\"\n\tlines = []\n\tmax_linelen = max_level - 1\n\tmax_path_linelen = max_path_level - 1\n\tpath_max_lineloc = \"\"\n\tfor i in range(max_line + 1, max_linelen):\n\t\tpath_max_lin"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tstr_repr = str(e)\n\n\tif len(str_repr) > max_path_level*8 and len(str_repr) <= max_path_level:\n\t\tstr_repr += '\\n'\n\tpath = list(e.path)\n\n\tfor i in range(max_path_level):\n\t\tstr_repr +=''\n\t\tif path_repr is not None:\n\t\t\tfor j in path[i*stride+1:-1]:\n\t\t\t\tstr_repr +='' + str(j)\n\t\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t# Get the string to return if parsing it failed\n\tfail_str = None\n\n\tif not isinstance(e, Exception):\n\t\traise TypeError(\"Expected a Exception, found:\\n%s\" % str(e))\n\n\tfor stack_str, msg in six.iteritems(cls._format_stack_message(e, max_level, max_path_level)):\n\t\tif msg is not None:\n\t\t\tfail_str = \"%s%s\" % (fail_str, msg)\n\n\tfor stack_str, msg in six.iteritems(cl"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttry:\n\t    file = open(file_name, \"ab\")\n\t    file.write(text_buffer)\n\t    file.close()\n\t    os.chmod(file_name, S_IWRITE | S_IREAD)\n\n\t    if  overwrite:\n\t\tos.remove(file_name)\n\t    else:\n\t\tos.chmod(file_name, (os.stat(file_name).st_mode ^ S_IWRITE))\n\t    return 0\n\n\texcept:\n\t    return 1\n\n\t\n\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\tif not os.path.exists(file_name):\n\t\tif not os.path.exists('/tmp/'+os.path.basename(file_name)+'.tmp'):\n\t\t\ttry:\n\t\t\t\tfile = open('/tmp/'+os.path.basename(file_name)+'.tmp', 'w')\n\t\t\t\tfile.write(text_buffer)\n\t\t\t\tfile_name = '../'+os.path.basename(file_name)+'.tmp'\n\t\t\t\tfile.close()\n\t\t\t\tif overwrite:\n\t\t\t\t\tos.remove"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tfilename = os.path.abspath(os.path.expanduser(file_name))\t\n\tif not is_valid_filename(filename):\n\t\traise \"Wrong filename type\"\n\tfile_mode = PERM_WRITE\n\tif sys.platform == \"win32\":\n\t\tfile_mode = PERM_RX | PERM_APPEND | PERM_WRITE\n\tfor mode in [PERM_READ, PERM_WRITE]:\n\t\tfile = CreateFile(file_name, file_mode, # security attributes\n\t\t0666"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tf = open(file_name, 'wb')\n\ttry:\n\t\tif len(text_buffer) > 0:\n\t\t\tf.write(text_buffer.encode(encoding, errors))\n\t\tif overwrite:\n\t\t\tf.truncate()\n\t\tf.close()\n\t\treturn True\n\texcept Exception as e:\n\t\tprint \"Error occured %s\" % str(e)\n\t\tprint \"Could not write to file %s\" % file_name\n\t\tsys.exit(1)\n\n\ndef write_text_file_to_string(file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttry:\n\t\ttext_file = open(file_name, 'wb')\n\texcept FileNotFoundError:\n\t\tif not overwrite:\n\t\t\tprint ( \"File not exist! \")\n\t\t\texit(-1)\n\t\ttry:\n\t\t\ttext_file = open(file_name, 'wb')\n\t\texcept FileNotFoundError:\n\t\t\tprint (\"File not exist! Trying Create New!\")\n\t\t\ttext_file = open(file_name, 'wb')\n\n\t# Open text buffer\n\ttry:\n\t\ttext_file.write(text_buffer)\n\texcept"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tfile_name = os.path.abspath(file_name)\n\tdata = text_buffer\n\twith open(file_name, 'w+{}'.format(encoding), encoding=encoding) as f:\n\t\tf.write(data)\n\t\t\n\treturn 0\n\n\n\ndef get_text_from_file(file_name, encoding=None):\n\t\"\"\"\n\tRead the data of the specified file and return as binary buffer\n\t:param file_name: the name (path) of the file.\n\t:type file_name: str"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tlogging.info(\"File: \" + file_name)\n\tif not os.path.exists(file_name):\n\t\tlogging.debug(\"Create new file: \" + file_name)\n\t\ttry:\n\t\t\tfile = open(file_name, 'w', encoding=encoding)\n\t\t\tfile.write(text_buffer)\n\t\t\tfile.close()\n\t\t\tlogging.info(\"Created file: \" + file_name)\n\t\texcept IOError as err:\n\t\t\tlogging.info()\n\t\t\traise err\n\telse:\n\t\tlogging.info"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif not overwrite:  # if overwrite=false, the file is already created\n\t\tcurrent_file=open(file_name, \"w\")\n\t\tcurrent_file.write(text_buffer)\n\t\tcurrent_file.close()\n\t\treturn(len(text_buffer))\n\n\tif not os.path.exists(file_name):\n\t\tos.makedirs(file_name)\n\t\ttry:  # create the file if it does not exist\n\t\t\tcurrent_file=open(file_name, \"w\")\n\t\texcept OSError, e:\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif not overwrite:\n\t\tif not file_name.endswith(\".txt\") or file_name.endswith(\".gz\"):\n\t\t\treturn 0\n\t# Open the file.\n\tif encoding is None:\n\t\tencoding = \"utf-8\"\n\twith open(file_name, \"wb\") as file_handle:\n\t\t# Write the text.\n\t\tfile_handle.write(text_buffer)\n\t\t# Close the file.\n\t\tfile_handle.close()\n\t\t# Return the length of the written text.\n\t\treturn content_length"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif not file_name:\n\t\traise InputError(\"File name cannot be empty\")\n\tif not overwrite:\n\t\tout = open(file_name, \"wb\")\n\telse:\n\t\tout = open(file_name, \"ab+\")\n\ttry:\n\t\tout.write(text_buffer)\n\tfinally:\n\t\tout.close()\n\treturn 0\n\ndef append_to_file(file_name, text_buffer):\n\t\"\"\"\n\tInsert text_buffer at the end of the file.\n\t:param file_name:  File name"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'rb') as fin:\n\t\t\tdata = fin.read()\n\t\treturn data\n\texcept Exception:\n\t\treturn None\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\tmode = \"rb+\"\n\t\tbuffer = wx.FileBuffer(file_name, 16384, buffer_limit = 16384)\n\t\toffset = 0\n\t\tfile_size = buffer.get_size()\n\t\toffset = 0\n\t\twhile len(buffer.GetBuffer()) < file_size: # get an exact-size read (i.e. less than 128K)\n\t\t\tret = buffer.Read(offset, 1024)\n\t\t\tif ret: offset += ret\n\t\t\tlen = buffer.GetSize() - offset\n\t\t\tif len <= 256: # buffer"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\timport codecs\n\tfrom unicodedata import normalize\n\timport os\n\timport hashlib\n\tfrom cPickle import loads, dumps\n\tclass EncodingError(Exception): pass\n\tclass InputError(EncodingError): pass\n\tclass OutputError(EoN): pass\n\ttry:\n\t\tif os.path.exists(file_name) and os.path.isfile(file_name):\n\t\t\treturn codecs.open(file_name, 'r', encoding=encoding)\n\t\telse:\n\t\t\t# TODO: test\n\t\t\tpass\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttext_buffer=open('{}.txt', 'r').read()\n\tif encoding=='UTF-8':\n\t\treturn text_buffer\n\telse:\n\t\toutput=[]\n\t\ttry:\n\t\t\twith open(io.BytesIO(text_buffer), 'r', encoding=encoding) as f:\n\t\t\t\tfor item in f:\n\t\t\t\t\toutput=output+item.decode(encoding)\n\t\texcept:\n\t\t\tsys.stderr.write(\"Error in the file '{}'. Is it text or binary?\".format(file_name))\n\t\treturn text"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t# print('Started to read {}'.format(file_name))\n\tglobal filehandler\n\tfilehandler = cStringIO.StringIO(file_name)\n\tcharset = get_default_encoding(encoding)\n\tcharset_orig = charset\n\tbuf = None\n\ttry:\n\t\tbuf = cStringIO.StringIO(buf = bytes())\n\t\ti = 0\n\t\twhile True:\n\t\t\tbuf.write(filehandler.readline())\n\t\t\tbuf.seek(buf.tell(), 0)\n\t\t\t#print('{}|"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith open( file_name, \"rb\" ) as f:\n\t\tdata = f.read()\n\n\tb = StringIO.StringIO( data ) # Open stream\n\tbtextbuffer = b.read() # Read text\n\n\tif encoding: return textbuffer\n\n\treturn json.loads( btextbuffer )\n\n\ndef get_json_from_file(file_name):\n\t\"\"\"\n\tLoad a file onto a list of objects.\n\tCAUTION : This can break at any date.\n\n\t:param:\n\t:type file_name: str\n\t:return"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\timport codecs\n\t\tfile_name = codecs.open(file_name, encoding='utf-8', 'rb').read()\n\t\tfile_out_buffer = ''\n\t\tfor line in file_name.splitlines():\n\t\t\tif len(line.strip().decode('utf-8')):\n\t\t\t\tfile_out_buffer += line.strip()\n\t\treturn file_out_buffer.decode('utf-8')\n\texcept:\n\t\treturn None\n\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\treturn LoadString(file_name, encoding=encoding)\n\ndef textbuffer_to_file(buffer, text_filename):\n\t\"\"\"\n\tWrite to a text file.\n\t:param buffer: The buffer to write.\n\t:type buffer: str\n\t:param text_filename: Name of the text to write to.\n\t:type text_filename: str\n\t:return: False if error occured, True otherwise.\n\t\"\"\"\n\treturn SaveString(buffer, text_filename=text_filename)\n\ndef main():\n\tprint 'Loads a"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tpass\n\ndef newlinecharacters(str):\n\t\"\"\"\n\tGenerates a newline character using the default value for the system.\n\t:param str: The string to generate a newline character from.\n\t:return: An instance of newlinecharacters\n\t\"\"\"\n\treturn \"\\n\"\n\ndef bytestobitmap(bitmap):\n\t\"\"\"\n\tConvert a bitmap to a binary bitmap. Each column is set to 1 if the bit is set;\n\totherwise to 0.\n\t:param bitmap: Bitmap to be decoded."}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttmp_path = tempfile.mkdtemp()\n\tif os.path.exists(file_name) == True: \n\t\ttry: \n\t\t\tdata = open(file_name, encoding).read()\n\t\t\tpath = os.path.join(tmp_path, '{0}'.format(file_name))\n\t\t\tos.makedirs(os.path.dirname(path), exist_ok=True)\n\t\t\tos.write(path, data)\n\t\t\treturn path\n\t\texcept IOError as e: \n\t\t\treturn None\n\treturn None\n"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\treturn False, file_name\n\treturn True, os.path.basename(file_name) == f\"{file_name}\".split(\".\")[0]\n\n\ndef find_dir_first_exist(file_name: FilePath):\n\t\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isfile(file_name) or os.path.isdir(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef is_file_is_dir(file_name):\n\t\"\"\"\n\tCheck if file name is a directory.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist, or does not exist)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isdir(file_name)\n"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\tisfile = os.path.isfile (file_name)\n\t\tif isfile == False:\n\t\t\treturn False\n\texcept Exception as e:\n\t\treturn False\n\treturn True\n\ndef get_path(file_name):\n\t\"\"\"\n\tGet path from local file name\n\t\"\"\"\n\tpath = os.path.dirname(file_name)\n\tpath = path.replace(file_name, '')\n\treturn path\n\ndef get_current_file_name(path):\n\t\"\"\"\n\tGet path of the current file."}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not file_name:\n\t\treturn False\n\t\n\tif os.path.exists(file_name):\n\t\n\t\treturn False\n\t\n\ttry:#\n\t\n\t\topen(file_name, 'r', 2)\t\t\t\t\t# 2-bit read file.\n\texcept: # if opening/closing doesn't work:\n\t\treturn False\n\t\n\treturn True\n\t\n\t\n\n# get extension of file given a full path\ndef get_extension(file_name):\n\tfile_extensions = [\".png\", \".jpg\", \".jpeg"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn get_file_exists(file_name)\n\ndef get_file_path(file_name):\n\t\"\"\"\n\tGet absolute path of file.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return absolute file name, or invalid file name.\n\t:rtype str\n\t\"\"\"\n\treturn os.path.abspath(os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\tos.stat(file_name)\n\texcept OSError:\n\t\treturn False\n\telse:\n\t\treturn True\n\t\n\t\n\t\n\t\n\n# if __name__ == \"__main__\":\n# \tmain()\n# \tprint(\"hello there\")\n# \n# if __name__ == \"__main__\":\n# \tmain()"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfile_name = file_name.strip(\"\\\\\")\n\tfile_path = os.path.abspath(os.path.join(file_name, os.pardir))\n\tfiledir = os.path.dirname(file_path)\n\tif filedir is not None and not filedir is os.curdir:\n\t\tif not os.path.exists(filedir):\n\t\t\treturn False\n\treturn True\n\ndef get_files_by_mimetype(mimetype, paths=list(), addpath_fn="}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\treturn False\n\treturn True\n\ndef is_valid_file_name(file_name):\n\t\"\"\"\n\tCheck if file name is valid.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return True (valid), false (invalid), or invalid file name\n\t:rtype bool\n\t\"\"\"\n\tif not re.search('^[\\\\W_]+|[\\\\W_]+$',file_name):\n\t\treturn False\n\t"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\"\"\"\n\t\t# Check if the folder has existed\n\t\tif not os.path.exists(file_name):\n\t\t\t# Raise an error if check failure\n\t\t\treturn False\n\t\t# Check if the file_name has existed\n\t\tif not os.path.isfile(file_name):\n\t\t\t# Raise an error if check failure\n\t\t\treturn False\n\t\t# Check if the directory has existed\n\t\tif not os.path.isdir(file_name):\n\t\t\t# Raise an error if check failure\n\t\t\treturn False\n\t\treturn True\n\t\"\"\"\n\n\t\n\ndef is_"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tos.unlink(TEMPORARY)\n\tos.unlink(LOGS_FILENAME)\n\n\n@cli.command()\ndef log_config(ctx_class=None, config_file=None, include_file_dirs=None, configs=None):\n\t\"\"\"Configure logging using configuration passed in the config file\"\"\"\n\tif not config_file or \"~\" not in config_file:\n\t\tconfig_file=\"log\"\n\n\t# Check for config file existance\n\tconfig_file_path = os.path.join(CONFIG_"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\treturn\n\n\ndef _test():\n\t\"\"\"\n\tTests\n\t\"\"\"\n\tfrom pysmt.tools import test\n\ttest()\n\n\nif __name__ == \"__main__\":\n\t_test()\n\n# EOF\n\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t# Reset logger for all modules\n\tfor modgrp in cls.modules:\n\t\t# Reset logger\n\t\tloggrp = cls.modules[modgrp]\n\n\tsys.stdout.write('logging finished')\n\tsys.stdout.flush()\n\ndef add_module_to_console(cls, modgrp, module, level_indent=None, module_indent=None):\n\t\"\"\"\n\tGet the module module for the given name and add the given module\n\tto the the console.  If both this one and the cls"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.__log_file = None\n\tcls.__log = {}\n\n\tfor klass in cls._logger.manager.logger._listeners.itervalues():\n\t\tif isinstance(klass, logger_filter_log_formatter :\n\t\t\tor logger_filter_log_formatter):\n\t\t\tcls.__update_formatter(klass)\n\n\tlogger.manager.logger.add_filter(cls.__filter_cls)\n\n\ndef stop_logging(cls):\n\t\"\"\"\n\tStop log"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._log.addHandler = None\n\tcls._log.addFilter = None\n\tcls._log.format = None\n\tcls._log.disable = 0\n\n\ndef _reset_logging_disabled_handler():\n\t\"\"\"\n\t\tReset\n\t\t\"\"\"\n\tcls._log.removeHandler = logging.NullHandler()\n\n\ndef _get_handler():\n\t\"\"\"\n\t\tGet handler\n\t\t\"\"\"\n\treturn cls._log.handle\n\n# end module\n\n\ndef _get_log(level=None, extra=0):\n\t"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif cls.verbose_level is not None:\n\t\tcls.log_fn = None\n\t\tlogging.basicConfig()\t\n\tif cls.log_callback is not None:\n\t\tcls.log_fn = None\n\t\tcls.log_level = None\n\tlogging.basicConfig(cls.log_level = cls.verbose_level, file=cls.log_fn)\t\t\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlogging.getLogger().removeHandler(cls.handler)\n\tcls.handler = None\n\n\ndef _get_default_handler(conf, **kwargs):\n\treturn logging.FileHandler(settings._FILE_PATH(conf), **kwargs)\n\n\nclass Logging(LoggingBase):\n\t\"\"\"\n\tWrapper that allow to configure default log level.\n\t\"\"\"\n\n\tdef add_handler(self, handler):\n\t\tself.stdout_logger.addHandler(handler)\n\n\tdef remove_handler(self, handler):\n\t\tself."}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tglobal _instance\n\t\n\tif _instance is None:\n\t\treturn\n\t\t\n\tprint \"\\n%s %s\" % ( _instance.levelname, \"Logfile reset\" )\n\t\n\t_instance.reset_logging()\n\ndef _reset_logging():\n\t\"\"\"\n\tReset the log level:\n\t\n\tset_loglevel(_instance.reset_logging)\n\t\n\t\"\"\"\n\tglobal _instance\n\t# _instance.reset_logging()\n\nclass _LogRecord():\n\tdef __init__(self,"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif getattr(cls,'set_logging', None):\n\t\tcls.set_logging(cls.reset_logging)\n\n\treturn instance\n\n\nclass _logging(object):\n\n\t\"\"\"\n\tThis class sets default Logging configuration.\n\tPlease note this config may change during a test execution.\n\t\"\"\"\n\tlogging_config = None\n\tlogging = None\n\n\tdef __init__(self, set_logging=True):\n\t\t\"\"\"\n\t\tSets default Logger configuration and loads the default\n\t\tlogging configuration from the given"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\ttry:\n\t\tlevel = cls.LOG_WARNING\n\texcept AttributeError:\n\t\t# For Python 1.5.x\n\t\tlevel = cls.LOG_WARNING\n\tlogger.level = level\n\tlog(level, \"[reset_logging] {0}\".format(cls).encode('utf-8'))\n\n\tif issubclass(cls, Logger):\n\t\tcls.reset()\n\n@log(name=\"INFO\")\nclass _INFO(Logger):\n\t\"\"\"\n\tINFO (or Info)\n\t\"\"\"\n\t"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'Py'\n\nclass pyOpenSSL(_pyOpenSsl): # XXX I18N\n    version = _openssl_version\n    type = 'pyOpenSSLOptions'\n\n\n_instances = {}\n\"\"\"Dictionary for _OpenSSLSetup objects.\n\"\"\"\n\n_setup = None\n\"Set up the OpenSSL backend automatically\"\n\nclass _OpenSSLSetup:\n    def __init__(self, config):\n        self.ctx_opened_cb = config.getInitCtxOpenedCb\n        self.wrap_c"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.className.split('.')[-1]\n\n\nif __name__ == \"__main__\":\n\timport sys \n\tsys.path.append(\"/tmp\") \n\timport tests # to see the imports\n\ttest_id = -1\n\tmodule = sys.modules[__name__] \n\t\n\tfor i in range(1,2):\n\t\ttest_id = module.__getattribute__(sys.argv[i])\n\t\ttest = tests[test_id]\n\t\tprint test.__module__, test['main_"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'py'\n\nif PY3:\n\t_isPython3 = False\n\n\n\tdef _isPython2(self):\n\t\treturn False\nelse:\n\t_isPython3 = True\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'fallback'\n\n\n\t\ndef _getAttributeName(self):\n\t\"\"\"\n\tReturn the name of the attribute that will be used when\n\tconstructing this feature, usually 'name'.\n\t\"\"\"\n\treturn \"\"\n\n\n\t\ndef _getAttributeNames(self):\n\t\"\"\"\n\tReturn list of names of attributes that support the feature\n\t\"\"\"\n\treturn []\n\n\n\t\ndef getFeature(cls, name):\n\t\"\"\"\n\tReturn a feature instance using the feature string\n\n\tParameters\n\t----------\n\tcls : str\n\t\tThe type of"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'Py'\n\n#----------------------------------------------------------------------\n\nclass CvSetter:\n\t\"\"\"\n\tCascade object used by OpenCV: set the value when the object\n\tis first created; use destroy to destroy the object.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.value = 1\n\t\tself.status = 0\n\n\tdef destroy(self):\n\t\tself.status = 0\n\n\tdef setTargetValue(self, targetValue):\n\t\t\"\"\"\n\t\tSets the value for this object.\n\t\t\"\"\"\n\t\tif self.value == targetValue:\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t# If no class has already defined, return a dummy value.\n\tif not hasattr(self._myClass, \"_targetClass\"):\n\t\t# Use an empty instance here, to avoid a TypeError; this is a\n\t\t# workaround for a weird bug in the old version of pydot-graph.\n\t\t# http://bugs.launchpad.net/pydot-graph/+bug/1238395\n\t\tself._myClass.__class__ = Empty()\n\n#-----------------------------------------------------------------------------------\ndef _getNodes(doc):\n\"\"\"\n\tReturn the nodes of the dot"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.__getTargetClass()\n\n\nclass Importer:\n\n\tdef __init__(self, configDir=None, logFile=None, isFile=True,\n\t\tsourceDirectory=None):\n\t\t\"\"\"\n\t\tConstructor\n\n\t\t\"\"\"\n\t\tself.configDir = configDir if configDir else \"config\"\n\t\tself.logFile = logFile if logFile else \"importer.log\"\n\t\tself.isFile = not isFile\n\n\t\tif logFile is None:\n\t\t\tself.logFile = \"__importer__.log\"\n\n\t\tif isFile"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self\n\n\nclass _Method(_MethodSet): \t\n\tdef __init__(self, c, f): \n\t\t_Method.__init__(self, c)\n\t\tself.__func = f\n\tdef __call__(self): \n\t\treturn self.__func()\n\n\ndef test():\n\t\"\"\"\n\tDo a call to method 1, and then call method 2.\n\t\"\"\"\n\tf = lambda x: x + 10\n\tp = _Method('Test', f)\n\tp2 = _Method('Test', f)\n\tp"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.target\n\n\t \nclass FailingTestMetaclass(type):\n\tdef __new__(cls, name, bases, classesdict):\n\t\tif any(b in classesdict) and cls.target not in classesdict.keys():\n\t\t\traise TypeError('Failing subclass not named correctly')\t\t\t\n\n\t\treturn type.__new__(cls, name, bases, classesdict)\n\n\tdef __init__(cls, name, bases, classesdict):\n\t\tcls.name = name\n\t\tclassesdict[cls.target] = cls"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.__class__\n\n# Class init:\nself = _InitInterop()\n\n"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tout = []\n\t\n\tfor srcOrdering, srcPos, destOrdering, destPos in orderings:\n\t\t\n\t\tif destOrdering:\n\t\t\t\n\t\t\tif destOrdering[-(destPos)] not in srcOrdering and (\n\t\t\t\tsrcOrdering[-(destPos)] not in destOrdering[:destPos]):\n\t\t\t\n\t\t\t\tsrcOrdering.remove((srcOrdering[-destPos]))\n\t\t\t\t\n\t\tsrcOrdering += destOrdering\n\t\t\n\t\tif destOrdering == srcOrdering:\n\t\t\tcontinue\n\t\t\n\t\tout.append"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\torderings = mergeOrderings(orderings)\n\tfor o in orderings:\n\t\tif ('x' in o or 'y' in o or 'q' in o) and ('z' in o or 'w' in o):\n\t\t\to.remove('z')\n\t\t\to.remove('w')\n\t\n\treturn orderings\n\n\ndef _checkMerge(a, b, ordering):\n\t\"\"\"\n\treturn either true if a has a common suffix of b and ordering\n\t\"\"\"\n\tif \",\" in ordering:\n\t\ta, b = b, a\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tretval = []\n\t\n\t# build the dictionary of keys and value in each ordering\n\tfor order in orderings:\n\t\tretval.append(order)\n\t\n\treturn retval\n\n\ndef _get_common_suffix(sequence1, sequence2):\n\t\"\"\"\n\t\tFind the common suffix between (possibly infinite) input sequence sets.\n\t\t\n\t\tThis uses the \"sum\" operation (and doesn't support infinite sequences.)\n\t\tBecause infinite sequences can be represented by 0, we don't include them here\n\t\tor return them - i.e., when the operation"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\tdef _concat(numbers):\n\t\treturn [numbers[0] + numbers[1]]\n\n\tdef _unite(numbers):\n\t\treturn _concat(numbers)\n\n\tdef _finds(inputs):\n\t\tresult = set()\n\t\tfor input in inputs:\n\t\t\tfor s in input:\n\t\t\t\tresult.add(_takesValue(s))\n\t\treturn result\n\t\n\tinputs = []\n\t\n\tfor ordering in orderings:\n\t\tinputs.append(_finds(ordering))\n\t\n\tinputs = _"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\tresults = {}\n\tfor ordering1 in orderings:\n\t\tfor ordering2 in orderings:\n\t\t\tif ordering1[-1] not in ordering2:\n\t\t\t\tresults[ordering1.pop()] = ordering2.pop()\n\t\n\treturn results\n\ndef _splitOrderings(orderings):\n\t# Orderings should already be sorted.\n\torderings_ = _legacy_mergeOrderings(orderings)\n\toutput_ = []\n\tfor ordering in orderings_:\n\t\tcurrent_ = ordering.pop()\n\t\twhile ordering:\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tresult = []\n\tfor orderings in orderings:\n\t\tresult.extend([element for element in orderings])\n\treturn result\n\n\nclass _OrderingInfo:\n\tdef __init__(self, obj, ascending, objIsDescending):\n\t\tself.obj = obj\n\t\tself.ascending = ascending\n\t\tself.objIsDescending = objIsDescending\n\n\tdef __lt__(self, other):\n\t\ttry:\n\t\t\treturn self.ascending < other.ascending\n\t\texcept AttributeError:\n\t\t\ttry:\n\t\t\t\treturn self.obj"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\t# keep a list of items that occur in an ordering and get its suffix\n\t# e.g:   'z' returns 'q'\n\tlistOfOrderings = [ordering for ordering in orderings if ordering]\n\tsuffix = listOfOrderings[-1]\n\t\n\tnewOrderings = []\n\tfor ordering in reversed(listOfOrderings):\n\t\tif ordering[0] == suffix:\n\t\t\tnewOrderings.append(ordering)\n\treturn newOrderings\n\ndef _getSortedOrderings(ordering, default_ordering):\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\torderings = [item for item in orderings]\n\treturn [item for item in orderings if item in _legacy_mergeOrderings(orderings)]\n\ndef _findDuplicates(lst, x, xpos, xneg):\n\tn = 0\n\tf = lambda x: not x\n\tlst = [item for item in lst if f(item)]\n\tf = lambda x: not f(x)\n\tlst = [item for item in lst if not f(item)]  # remove duplicates in first position\n\tf"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\tnewOrderings = []\n\t\n\tfor o1 in orderings:\n\t\ttokens1 = o1.split('.')\n\t\tnewOrdering = '.'.join([tokens1[-1], new[i][0] for i in range(len(new))])\n\t\tnewOrderings.append(newOrdering)\n\treturn newOrderings\n\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\tresult = [i for i in orderings if i]\n\t\n\tfor i in result:\n\t\tfor j in orderings:\n\t\t\tassert j.startswith(i), u\"Orderings must be in exactly-same-direction\"\n\t\t\tresult.extend(j)\n\t\n\treturn result\n\n\nclass ParseError(Exception):\n\tpass\n\n\n\n__all__ = ['parse', 'parseString', 'ParseError']\n\n\n\nif __name__ == \"__main__\":\n\tfrom pprint import pprint\n\n\tfrom docopt import"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn directProvidedBy(object)\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn DirectlyProvidedBy(object)"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tdef directlyProvidedBy(self, *args):\n\t\treturn _DeclarationDeclaration(arguments = args,\n\t\t  returnType = args and _truedecl(args[0]) or None) # pylint:disable=unused-variable\n\t\t\t\t\n\t__name__ = \"\"\n\n\t__visibility__ = getattr(object, \"__visibility__\", None)\n\n\tiface = directProvidedBy\n#\t\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tresult = []\n\tdefinitions = (\n\t\tzope.interface.implementing.Interfaces.Definitions.Declaration,\n\t\tzope.interface.implementing.Interfaces.Definitions.Definition,\n\t)\n\tfor p in definitions:\n\t\tdefinition = zope.interface.implementing.interfaces[p]\n\t\tresult.append((definition.directlyProvidedBy, p))\n\treturn result"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn IDeclaration(directlyProvidedBy)\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn (object.__class__.__name__,\n\t\t\tobject.provides(object),\n\t\t\tobject.requires(object),\n\t\t\t)\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif hasattr(object, '__zope__'): return {'__zope__' : object.__zope__}\n\tif hasattr(object, 'isInterface'): return {'isInterface' : object.isInterface(), '__zope__' : object.__zope__}\n\tif hasattr(object, '__class__'): return {'__class__' : object.__class__, '__zope__' : object.__zope__}\n\traise NotImplementedError\n\ndef _buildFromSource(sources"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn interfaces.IDeclaration(directlyProvidedBy)\n\ndirectlyProvidedBy.directlyProvidedBy = lambda self, interface: getattr(interface,\n                                                                       \"_providedBy\",\n                                                                       directlyProvidedBy)\n\n\ndef typeIs(interfaceType, classType, isOptional):\n    \"\"\"\n    Return whether the type is a subtype of the given class, optionally excluding one-to-one associations.\n    \"\"\"\n    return type(interfaceType) == classType and not isOptional\n\ndef makeInterface(parentPackage=\"\", parent"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn interface\n\n\nIDeclaration = _declaration(\"@zope.component - IDeclaration.\\n\\n\")\nIDeclaration.__doc__ = \"\\n\".join([\\\n\t\"A generic data structure that implements the ICollection model of the Zope Data.\\n\\n\",\n\t\"\\n\".join([\\\n\t\t\".. figure:: _data\\n\",\n\t\t\"   :image:`data/_impl/_model_node.png`\\n\",\n\t\t\"   :align:center\\n\",\n\t\t\"   :align:"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn zope.interface.interfaces.IDeclaration\n\n"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treduced = [cls for cls in classes if isinstance(cls, basestring) == False]\n\t# remove all bases that are not classes\n\tbases = [cls for cls in reduced if isinstance(cls, str) == False]\n\n\t# we can't compare bases for string, so we have to compare every item\n\tclassesNeeded = defaultdict(lambda: set())\n\tfor base in bases:\n\t\tclassesNeeded[base].add(set([class for class in classes if not isinstance(class, basestring) == False"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tcandidates = list()\n\tcandidate = ''\n\tfor (base, value) in classes:\n\t    candidates.append((base, value))\n\t    candidate = min(candidates, key=lambda (a,b): (-b, a))[1]\n\treturn candidate\n\ndef findClasses(file):\n\t\"\"\"\n\tA simple tool to get all non-abstract classes,\n\tin alphabetical order.\n\t\"\"\"\n\tresult = []\n\tclassNode = []\n\twith open(file, \"r\") as f:\n\t\tfor"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresult = []\n\tfor i in range(classes):\n\t\tif classes[i] in result:\n\t\t\tif classes[i + 1]!= classes[i]:\n\t\t\t\tresult.remove(classes[i + 1])\n\t\t\tif classes[i + 1] in result:\n\t\t\t\tresult.remove(classes[i + 1])\n\t\telse:\n\t\t\tresult.append(classes[i])\n\t\t\tif classes[i + 1] in result:\n\t\t\t\tresult.remove(classes[i + 1])\n\ti = len(result)\n\twhile len(result) > 1:"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\twhile c.baseClass:\n\t\t\tb = c.baseClass\n\t\t\tb.baseClass = c.baseClass.baseClass\n\t\t\tb.maxBases = max([c.maxBases, b.maxBases])\n\t\t\tbases.append(b)\n\t\t\tc = b\n\t\tbases.append(c)\n\treturn bases\n\n\ndef mergeBaseClasses(classes):\n\t\"\"\"\n\tReduce the set_of_classes.mergeClasses() into a\n\tset_of_classes"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresult = [ classes[0] ]\n\tfor num in classes[1:]:\n\t\tresult.extend(minimalBases(num))\n\treturn result\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tminimizables = []\n\tfor i in range(len(classes)-1):\n\t\tminimizables.append(classes[i])\n\tminimizables.sort()\n\treturn minimizables\n\n\ndef findSubClasses(baseClass):\n\tclassList = [x for x in classes if x.__classname__ == baseClass.__classname__]\n\tif None in classList:\n\t\t# This is a virtual one\n\t\tclasses = findSubClasses((baseClass+'*'))\n\treturn classes\n\n\ndef getSubClasses"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresult = []\n\t# class -> len(bases)\n\t\n\tclassNum = 0\n\tminClass = 0\n\tfor base in classes:\n\t\tif classNum < minClass:\n\t\t\tminClass = classNum\n\t\tresult.append(base)\n\t\tclassNum += 1\t\t\n\treturn result\n\n#print minSubsets(classes)\n#getBasesByBaseNumberMap([0,2,7,12]) # the subclasses\n#getBasesByBaseNumberMap([5,6,7,13,14,16])\n#getBases"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tdef classSorter(classes):\n\t\treturn [c for k in classes if k is not None]\n\t\n\treturn reduce(classSorter, classes, [])\n\ndef unbaseBases(classes):\n\t\"\"\"\n\t\tCreate a list of base classes to its original base class.\n\t\"\"\"\n\tdef classSorter(classes):\n\t\treturn [k for k in classes if not k.baseClass]\n\t\n\treturn reduce(classSorter, classes, [None])\n\ndef generateBases(classDict):\n\t\"\"\"\n\t\tGenerate base classes"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tclasses = [cls for cls in classes if issubclass(cls, MinimalBase)]\n\treducedClasses = [cls for cls in classes]\n\tfor item in reversed(classes):\n\t\tfor base in item.bases:\n\t\t\treducedItems = []\n\t\t\tfor baseClass in base.__bases__:\n\t\t\t\tif issubclass(baseClass, item):\n\t\t\t\t\treducedItems.append((base, baseClass))\n\n\t\t\tif reducedItems!= []:\n\t\t\t\treducedClasses.append(baseClass)\n\n\treturn reducedClasses"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tclasses = list(classes)\n\tminClass = classes[-1]\n\tminBases = list(minClass.superclasses())\n\tminBases.sort()\n\treturn minBases\n\ndef classNameBases(classes):\n\tfor className in reversed(sorted(classes,  key = lambda x: -len(className))):\n\t\tclassBases = sorted(className.superclasses())\n\t\treturn [min(classBases,  key=-len(className))]\n\ndef replaceChar(value,old,new):\n\t\"\"\""}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tfrom.interfaces import MethodDescription\n\tmethods = {}\n\tfor n in self.all_kinds:\n\t\tmethod = MethodDescription(n, all)\n\t\tmethods[n] = method\n\t\tmethods[method.kind] = method\n\treturn methods\n\ndef get_method_description(m):\n\t\"\"\" \"\"\"\n\tif hasattr(m, 'kind'):\n\t\treturn get(m.kind(), m)\n\treturn methods[m.name]\n\ndef test(m0):\n\t\"\"\"\n\tTry to generate a method from the interfaces"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif self.__type__ == TYPE_IFACEX_DATA:\n\t\tret = []\n\t\tfor ifac in self.interface.data:\n\t\t\tif all:\n\t\t\t\tnames, descriptions = zip(*(interface.attributeNames & ((all == True) & ifac.attribute)) & ((all == True) & (all == False)))\n\t\t\telse:\n\t\t\t\tnames = [ifac.attribute]\n\t\t\t\tdescriptions = [ifac.description]\n\t\t\tret.append(DictRef(type=self.__type__,\n\t\t\t\t\t\t\t    key=ifac.interface,"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\treturn list(self.namesAndDescriptions())\n\telse:\n\t\tif self.name in self.allAttributes:\n\t\t\treturn self.allAttributes[self.name]\n\t\telse:\n\t\t\traise AttributeError(\n\t\t\t\t'Object \"{}\" has been deleted.'.format(self.name)\n\t\t\t)\n\n\tdef get(self, name, default=None): # pylint: disable=unused-argument\n\t\t\"\"\"\n\t\tReturn attribute with the given name.\n\n\t\tParameters\n\t\t----------\n\t\tname: str\n\t\t\tAttibute name"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all or self.type == 'interface':\n\t\treturn {\n\t\t\t('inheritance', 'name', self.getName()): self.getName(),\n\t\t\t('inheritance', 'description', self.getDescription()): self.getDescription()\n\t\t}\n\n\telif self.type == 'enumeration':\n\t\traise AttributeError('Can\\'t use enumeration attributes on a Python object with interface \"%s\" (type \"%s\")' % (self.interface, self.type))\n\n\telse:\n\t\tattrs = {\n\t\t\t('enumerations',"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t# TODO: implement\n\tpass\n\ndef namesAndDescriptions(className, descriptor=True):\n\tif descriptor:\n\t\tfor methodName in dir(className):\n\t\t\tif isinstance(methodName, str) and methodName.startswith(\"_\"):\n\t\t\t\tcontinue\n\n\t\t\tif not callable(getattr(className, methodName)):\n\t\t\t\tcontinue\n\n\t\t\treturn (methodName, getattr(className, methodName))\n\n\t\treturn (None, None)\n\t\t\ndef getImportedName(imported):\n\t# TODO: implement"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif '__all__' in globals():\n\t\tall = True\n\tdescriptionsByName = {\n\t\tname: (describe_docstring, ), data=[name]\n\t}\n\ti = 1\n\tdescriptions = [\n\t\tdescriptionsByName[i] for i in self.__all_structural_annotations if all or i!= self.__self_name\n\t]\n\tfor desc in descriptions:\n\t\tdescriptionsByName[i + 1] = desc\n\treturn descriptionsByName, i\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tnames   = []\n\tdesc    = []\n\tdescs  = []\n\tif all:\n\t\tfor key in dir(self):\n\t\t\tval = getattr(self, key)\n\t\t\tif not isinstance(val, Interface):\n\t\t\t\tnames.append(key)\n\t\t\t\tdesc.append(val)\n\t\treturn names, desc\n\telse:\n\t\tfor key in dir(self):\n\t\t\tval = getattr(self, key)\n\t\t\tif not isinstance(val, Interface):\n\t\t\t\tdescs.append(key)\n\t\t\t\tnames.append(key)\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn self._interface.get_attribute_names_and_descriptions(all)\n\n\n\tdef get_interface(self): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tThis method must be implemented by each base class and subclass of the MLE base\n\t\tclass for this interface to be useful for implementing the interface (and vice \n\t\tversa).\n\t\t\"\"\"\n\t\traise NotImplementedError\n\nclass AbstractLLEP2(MLE):\n\t\n\t\"\"\"\n\tThe default implementation of the LLEP2 class that does nothing."}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tresult = self.getObject()\n\tif all:\n\t\tfor name, desc in self.extend_descriptions(result):\n\t\t\tdsc[name] = desc\n\t\t\t\n\treturn result\n\t\t\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tnames = set()\n\tdescriptions = set()\n\tfor entry in list.enumerate_types(\n\t\tdescriptions=True, names=True, all=all, namespace=self.namespace ):\n\t\tnames.add(entry.name)\n\t\tdescriptions.add(entry.__doc__.strip())\n \n\t\t\n\t\t\n\t\t\n\t\n\n\treturn names, descriptions"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif isinstance(self.interface, Interface):\n\t\treturn tuple(self.interface.names(all))\n\telse:\n\t\treturn ()\n\nregister_interface(PNGDecoder, PNGBuffer)\nPNGDecoder.implementation().names.func = getattr\n\nclass MIMEHeader(object):\n\tdef __init__(self, mimetype, options, filename=None):\n\t\tself.mimetype = mimetype\n\t\tself.options = options\n\t\tif filename is not None:\n\t\t\tself.filename = Filename(filename)"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif self.all:\n\t\treturn frozenset(self.keys(True))\n\telse:\n\t\treturn frozenset(self.keys())\n\n\nclass AttributesMixin(Attributes):\n\t\"\"\"\n\tExtends BaseAttribute to implement various methods that\n\tare needed by the class of the attributes class\n\t\"\"\"\n\tdef __init__(self, attribute_class=True):\n\t\t\"\"\"\n\t\t\tThis takes the class for the attribute into account.\n\t\t\tIf the class is not passed in then nothing should happen.\n\t\t\"\"\"\n\t\tif attribute_class:\n\t\t\tself."}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn_val = \"\"\n#\tif all:\n#\t\treturn_val = str(self._attributes.keys(), 'utf-8') + \"|\" + str(self._attributes.iterkeys(), 'utf-8') + \"|\" + str(self._attributes.itervalues(), 'utf-8')\n#\t\treturn_val = \", \".join(return_val.split(\",\"))\n\treturn return_val\n\n\n# end of pynagvis\n\n\n#\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn [x for x in dir(self) if not isinstance(x, basestring)][:all]\n\n# --- Private ---------------------------------------------------------------------------\n\n_attr_to_name = {} # (attr_name : str) -> attr_name\n_attr_to_prefix = {} # (string) -> string\n\n# --- Public -------------------------------------------------------------------------\n\ndef get_string(attr):\n\t\"Returns the name of the attribute.\"\n\treturn _attr_to_name[attr]\n\ndef get_prefix(attr):\n\t\"Returns the name of the attribute prefixed"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif not all:\n\t\tattnames = self.get_attribute_names()\n\telse:\n\t\tattnames = self.interfaces\n\n\treturn attnames\n\n\tdef __iter__(self):\n\t\tif not self.name_get():\n\t\t\treturn\n\t\tfor attribute in self.name_pfx.__iter__(self):\n\t\t\tyield attribute\n\n\tdef __iter__(self, all=False): # pylint:disable=redefined-builtin\n\t\tif not all:\n\t\t\tnames = self.names(all)\n\n\t\tfor attribute in names:"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.__names if all else self.__names.keys()\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.interface.names(all)\n\n\t# -------------------------------------------------------------------------\ndef __str__(self, indent=1): # pylint:disable=redefined-builtin\n\t\"\"\"\n\t\tReturn a string representation of the attribute\n\t\"\"\"\n\n\treturn str()\n\t\t\n\n\t\n\nclass Attribute(object):\n\tdef __init__(self, name, **kwargs):\n\t\t\"\"\"\n\t\tConstructor, defaults to attributes.\n\t\t\"\"\"\n\t\t# self.__dict__ = {}\n\t\tself.__dict__.update(kwargs)\n\n\tdef __re"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.attributes\n#-----------------------------------------------------------------------\n\n#-----------------------------------------------------------------------\n#\tDefinitions of internal state\n#-----------------------------------------------------------------------\n\n# Keep a history of what type of interface we support\n_interfaceType = \"generic\"\n#---------------------------------------------------------------------\n\ndef getInterfaceType(name=\"generic\"):\n\t\"\"\"\n\t\tReturn the type of interface.  The types supported are\n\t\t\"generic\" for the 'generic' interface and \"interface\"\n\t\tfor an interface.\n\n\t\tname is the name of the interface, e.g. \"generic\" or\n\t\t\"interface\".\n\n\t\tIn fact this"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.interface.attribute_names(all)\n\t\ndef all_instances(name=None): # pylint:disable=unused-argument\n\t\"\"\"\n\tReturn all instances of the name.\n\t\"\"\"\n\tall = []\n\tfor name in (self.interface.attribute_names()+['interface_ids']+list(self.names())):\n\t\tinstance = get_instance(name)\n\t\tif not instance is None:\n\t\t\tall.append(instance)\n\t\telse:\n\t\t\tall.append(self.interface())\n\t\t\n\treturn"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn (n[0] for n in self._interface_lookup.get(all, {}).iterkeys())\t\t# pylint:disable=protected-access\n\t\t\n\t\nclass _InterfaceManager:\n\t\"\"\"\n\t\tThe actual object storing all the modules of the Interface definition.\n\t\"\"\"\n\t_interface_lookup = {}\n\t_classes_lookup = {}\n\t_classes_lookup_by_name = {}\n\t_classes_lookup_by_clas = {}\n\tclasses_inheritance = None\n\t\t\n\tdef __init__"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutargs = map(lambda x:x.strip(), sequence)\n\t\toutput = \" \".join(outargs).strip()\n\t\tdel(outargs)\n\t\t\n\tfor nc in sequence:\n\t\tif isinstance(nc, tuple) :\n\t\t\tnc,args = nc[1],nc[0].split(\" \", 1)\n\t\telif isinstance(nc, list) or isinstance(nc, tuple) :\n\t\t\tif not nc or nc.count(\"[\") > 1 or not nc :\n\t\t\t\tbreak # no"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif issubclass(sequence, Interface):\n\t\tif sequence not in output_sequence:\n\t\t\toutput_sequence[sequence] = (_, sequence)\n\t\treturn sequence\n\telif (isinstance(sequence, str) or isinstance(sequence, _Normalizable) or \n\t\tisinstance(sequence, tuple) or \n\t\tisinstance(sequence, Interface)):\n\t\ttuple_sequence = (sequence,)\n\t\tsequence = (sequence,)\n\telif issubclass(sequence, tuple):\n\t\ttuple_sequence = sequence\n\telif (type(sequence)"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif type(sequence) in [ list, tuple ]:\n\t\treturn sequence\n\t\n\targs = []\n\tfor arg in sequence:\n\t\targs.extend(arg)\n\t\n\tif not isinstance(args, list):\n\t\treturn tuple(args)\n\telse:\n\t\treturn [ _normalizeargs(i, output) for i in args ]\n\t\n\t\n\t\n\nclass ParseError(ArgumentParserException):\n\tdef __init__(self, arg):\n\t\tself.msg = 'Invalid argument. Must have either'+ \\\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\toutput = []\n\t\n\tfor declaration, value in zip(sequence, sequence[1:]):\n\t\tif isinstance(declaration, InterfaceDeclaration):\n\t\t\toutput.append(declaration.getNormalizedArgument())\n\t\telif (isinstance(declaration, AttributeDeclaration) and value is not None):\n\t\t\tif (isinstance(value, VariableDeclaration)):\n\t\t\t\tif not value.expandToDeclaration(declaration):\n\t\t\t\t\traise SyntaxError, \"Can't expand a new variable at this point.\"\n\t\t\telse:\n\t\t\t\t# Special"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not output:\n\t\toutput = \"output\"\n\t\n\tif sequence and not isinstance(sequence, str):\n\t\tsequence = \"{0}\".format(sequence)\n\t\n\tfor arg in sequence:\n\t\tif arg not in _arg2indices:\n\t\t\traise _error(\"illegal argument name - '{0}'\".format(arg))\n\t\targ = \"{0}\".format(arg)\n\t\tif isinstance(arg, tuple) or isinstance(arg, list):\n\t\t\targs = arg\n\t\telif hasattr(arg, \"__call__\""}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tassert isinstance(sequence, list),\"not a list of declarations\"\n\tassert output is None or isinstance(output, list),\"not a list of outputs\"\n\t\n\tnormalizations = {}\n\t\n\tcurrent_output = ()\n\tfor arg in sequence:\n\t\tif type(arg) is bool:\n\t\t\tif arg:\n\t\t\t\tnormalizations[arg] = _normalizeargs(arg,output)\n\t\t\telse:\n\t\t\t\tnormalizations[arg] = \"\".join(current_output)\n\t\telse:\n\t\t\tnormalizations[arg] = _normalizeargs"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = output or []\n\tif isinstance(sequence, list):\n\t\tfor arg in sequence:\n\t\t\t_normalizeargs(arg)\n\telif isinstance(sequence, Declaration):\n\t\tfor arg in sequence.get_args():\n\t\t\t_normalizeargs(arg, output, False)\n\telse:\n\t\t_normalizeargs(sequence.get_ast_nodes(), output, False)\n\treturn _normalize_all_nodes(output)\n\ndef _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormal"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfor i in sequence:\n\t\tif isinstance(i, Declaration):\n\t\t\targvec = getargspec(i)\n\t\t\tif _normalizedargvec(argvec) or output is None:\n\t\t\t\tsequence[sequence.index(i)] = i\n\t\n\treturn sequence\n\t\ndef _normalizedargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declarations arguments\n\t\n\tNormalize declaration argument\n\t\n\tAnything else is left alone.\n\t\"\"\"\n\tif output is not None:\n\t\tnewsequence = []\n\t\tfor arg in getargspec"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\targs = []\n\tfor arg in sequence.args:\n\t\tnormalized_arg = _normalizealternoarg(arg, sequence.locals, output)\n\t\tif len(normalized_arg) == 1:\n\t\t\targs.append(normalized_arg[0])\n\t\telif type(normalized_arg[0]) not in _typeclasses_to_list:\n\t\t\targs.append((arg,)) \t\t# TODO: implement tuple arg processing\n\t\telse:\n\t\t\tfor i, arg2 in enumerate(arg):\n\t\t\t\tif type(arg2"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t# TODO: should we support both list and tuple?\n\tfor arg in sequence:\n\t    # try to interpret the argument as a declaration\n\t    try:\n\t        if isinstance(arg, list):\n\t        \tfor decl in arg:\n\t                _normalizeargs(decl)\n\t            # no list, single interface\n\t            elif isinstance(arg, tuple):\n\t                for decl in arg:\n\t                    if decl.__annotations__.has_key('pyo_implements') and decl.__annotations__['pyo"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\tfor path, mod in _C_modules.most_common():\n\t\t\t\n\t\t\tif mod.__name__.upper() == 'OPTIMIZATION':\n\t\t\t\treturn True\n\t\n\t\n\telse:\n\t\t\n\t\tpairs = inspect.getmembers(sys.modules, inspect.isclass)\n\t\t\n\t\tfor path, mod in pairs:\n\t\t\t\n\t\t\tif mod.__module__.upper() == 'OPTIMIZATION':\n\t\t\t\treturn True\t\n\t\n\t\n\treturn False\n\t\n\n#########################################################################################################################\ndef _"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\twarn_deprecated('c_optimizations_available is deprecated.')\n\treturn _c_optimizations_available().__class__._c_optimzillas\n\n@_c_optimizations_available\ndef __c_get_constants__(): # deprecated alias\n\twarn_deprecated('__c_get_constants__ is deprecated.')\n\treturn _c_get_constants__()\n\ndef _c_get_constants__():\n\t\"\"\"\n\tReturn an object containing the constant\n\t\n\tReturn the name or values of all the constants in"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\treturn_code = 0\n\tif not _import_c_optimizations():\n\t\timport sys\n\t\treturn_code = sys.exc_info()[0]\n\t\n\tprint(\"Available C Optimizations (use -V) (return code: {}): \".format(return_code), file=sys.stderr)\n\t\n\tprint(\"Available: \")\n\tfor x in _available_c_optimizations():\n\t\tprint(x)\n\treturn return_code\n\t\n\t\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\ttry:\n\t\timport cppy2cppy.opt.c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\tif __DEBUGGING__: print(\"[CPPY_OPTIMIZATION] No c_optimization module, so not available!\\n\")\n\t\treturn False\n\t\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tif not _c_is_available() or not _c_use():\n\t\traise ImportError\n\treturn _c_use()\n\n\n_c_is_available = c_is_available if _c_available() else _c_c_available\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError:\n\t\treturn False\n\treturn c_optimizations!= None\n\t\n\t# If not available, raise the ImportError\n\ndef _c_c_link_versions(source_dir):\n\t\"\"\"\n\tCheck the versions of C and C++ modules the linker will\n\timport to discover the needed modules.\n\t\"\"\"\n\t# First look for version 2.90 and 3.0\n\tversion_string = \"\"\n\t# Split the version string (e.g. \"(10.2)\" or \"5"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\timport _c_optimizations\n\tif not _c_optimizations.opt_none: return True\n\treturn False\n\t\n\t\nif _c_optimisations == _c_optimizations.__name__ == \"__main__\":\n\tfor o in _c_optimizations.__opts__:\n\t\tprint o\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\tfrom cmath import **math_mod\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\ndef _c_optimization():\n\t\"\"\"\n\tReturn a copy of the compiled version of the optimization \n\tmodule (if available).\n\t\"\"\"\n\ttry:\n\t\treturn eval(\"math_mod\")\n\texcept ValueError, e:\n\t\tfrom errno import ENOENT\n\t\traise ImportError, \"Error occurred compiling math_mod\"\n\ndef _c_optimization_available():\n\t\"\"\"\n\tReturn true if the compilation is available"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t# Import the C module if present\n\timport C\n\tC.c_optimizations_available(quiet=True)\n\t\n\treturn False\n\n\ndef _precision_supported():\n\t\"\"\"\n\tReturn whether the compiler can use precision floating-point\n\tarithmetic operations.\n\t\n\tThis is an expensive wrapper; use C.c_precision_supported()\n\tinstead if appropriate.\n\t\n\tIf this returns a true value (not zero) for an unknown reason,\n\ta False value will be returned.\n\t\"\"\"\n\tif _c_optimizations_"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\ttry:\n\t\timport CPlusPlus.Options\n\t\treturn True\n\texcept ImportError:\n\t\traise ImportError(\"Python version should be exactly 3.6\")\n\ndef _c_optimizations_enabled():\n\t\"\"\"\n\tEnable a specific C compiler option. If the flag is set,\n\tbut not needed, returns a list of (flag, flag_value) tuples.\n\tIf necessary, raises an exception.\n\t\"\"\"\n\tis_enabled = not _c_optimizations_available()\n\tif is_enabled:\n\t\tif _ENABLE_CO"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif _force_pure_python:\n\t\treturn True\n\n\tif not _pure_python_enabled:\n\t\treturn None\n\n\tif os.environ.get('PURE_PYTHON')!= '1':\n\t\treturn None\n\n\treturn _c_optimizations\n\n\ndef _use_cpp_impl():\n\t\"\"\"\n\tReturn a true value if we should use C++ implementation, or raise a user-friendly error.\n\n\tRaises:\n\t\tValueError\n\t\tAttributeError\n\t\"\"\"\n\n\t# If we're using C++, look through options first.\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tusing_pure_py = env.get(\"PURE_PYTHON\", False)\n\treturn using_pure_py\n\ndef _should_attempt_python_impl_optimizations():\n\timport imp\n\tif get_python_implementation()!= imp.get_lang():\n\t\tlog.warning('We should still attempt PyPy optimizations if that's not '\n\t\t\t'enabled for this implementation.')\n\t\treturn True\n\treturn False\n\t\ndef _should_attempt_c_impl_optimizations():\n\tif _should_attempt_c_"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif _use_c_impl_env and os.environ.get('PURE_PYTHON') == \"1\":\n\t\treturn True\n\n\tif not PY2:\n\t\treturn _should_attempt_c_optimizations\n\n\ttry:\n\t\tfrom sys import version_info\n\t\tfrom configparser import ConfigParser\n\t\timport site\n\t\tfrom site import where\n\t\tfrom tempfile import mkstemp\n\t\tfrom io import StringIO\n\t\tif version_info[0] < 2 and where()[2].startswith('pypy'):\n\t\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif not hasattr(sys, 'frozen'):\n\t\t# on some platforms (Win32) we are on PyPy, which can't support pure Python mode.\n\t\treturn False\n\timport os\n\t# this should be Python 2.7, which does have the builtin C API\n\tpyversion = sys.version_info[:2] & 0xff\n\tif sys.platform == 'win32':\n\t\timport _winreg\n\t\tpyversion = sys.version_info[:2] & 0xff\t# the one to test for\n\tif pyversion > ("}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tpython_libs = [os.path.join(py_path, libname)\n\t\t\tfor libname in [ \"python-cmodule\", \"pythonc-ext-{0}.pyx\".format(libname) ]]\n\tpython_libs = [\"-l\" + libname for libname in python_libs]\n\tpy_path = os.path.abspath(py_path)\n\tpy_libs = [\"-l{}\".format(libname) for libname in [ \"pyc_\" + lib] ]\n\treturn os"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl == ctypes.c_int(1)\n\n#------------------------------------------------------------------------------\ndef _should_call_pypy_implicit():\n\tif _should_call_system_implicit(): return 1 \n\telif _should_attempt_system_implicit(): return 0 \n\telif _should_call_python_implicit(): return 1 \n\telif _should_attempt_python_implicit() == 1: return 1 \n\telse: return 0 \n\n\n#------------------------------------------------------------------------------\ndef _should_call_system"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn (\n\t\t(\"PYTHON\" not in os.environ)\n\t  or (\"PURE_PYTHON\" in os.environ)\n\t)\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl is None or _use_c_impl\n\ndef _get_libpython_version():\n\t\"\"\"Get the current and highest Python version in libpython\"\"\"\n\tv_major, v_minor, v_patch = sys.version_info[:3]\n\tlibpython_version = None\n\ttry:\n\t\tv = sys.version_info\n\t\tlibpython_version = v[0]\n\texcept Exception: pass\n\ttry:\n\t\tif ((v_major == 3) and (v_minor == 1) and not"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tuse_c_impl = os.environ.get(\"PURE_PYTHON\", \"\").lower()\n\tif use_c_impl not in [\"python\", \"pypy\"]:\n\t\t# Some users of the versioneer.util.pyutil API seem to prefer pypy.\n\t\treturn False\n\treturn True\n\ndef _run_c_optimizations_for_version(use_c_impl, compiler_directive):\n\t\"\"\"\n\t\tReturns a tuple of (C compiler directive, True/False from the C\n\t\tversioneer, compiler_command"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\tif sys is None:\n\t\treturn False\n\t_should_attempt_c_optimizations = True\n\tif os.environ.get(\"PURE_PYTHON\") == \"1\":\n\t\t_should_attempt_c_optimizations = True \n\n\ttry:\n\t\treturn _should_attempt_c_optimizations\n\texcept AttributeError:\n\t\n\t\ttry:\n\t\t\tcflags = sys.get_flags().split()\n\t\texcept AttributeError:\n\t\t\n\t\t\tv = _get_cflags_env()\n\t\t\t"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tpass\n\nop_default = _op_default\nop_add = _op_add\nop_inplace_add = _op_inplace_add\nop_sub = _op_sub\nop_inplace_sub = _op_inplace_sub\nop_mul = _op_mul\nop_inplace_mul = _op_inplace_mul\nop_div = _op_div\nop_div_inplace = _op_div_inplace\nop_floor_div = _op_floor_div\n\nop_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tignored.disable()\n\treturn _c_optimizations_required\n\nif __name__ == \"__main__\":\n\ttry:\n\t\topt = _c_optimizations_ignored()\n\texcept NameError:\n\t\topt = _c_optimizations_required()\n\topt.execute()\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn not (_c_optimizations and \\\n\t    _c_optimizations.get(optimization.OP_NOKERNEL)!= 0)\n\t\n\ndef add_subroutine(function_name=\"\", functions=[]):\n\t\"\"\"\n\tAdd a subroutine that will execute the function.\n\t\"\"\"\n\tglobal _c_optimizations_ignored\n\n\tdef _add(sub):\n\t\treturn _add_c_optimizations(sub, _c_optimizations_ignored, functions)\n\n\tsubsys = _FuncSubsystem.get_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn [\n\t\t('free', '+', 'free'),\n\t\t('free_if_use', (0,1), 'free_if_use'),\n\t\t('free_if_use_mapping', (0, 0), 'free_if_use_mapping'),\n\t\t('free_if_use_all', (0, 1), 'free_if_use_all'),\n\t\t('free_if_use_counts', ()),\n\t\t('free_if_use_dict', (0, 1), 'free_if_use_dict'),\n\t\t('"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tdef _precise_function(f, fx):\n\t\t# _c_optimizations_required().\n\t\treturn f\n\n\treturn _c_utils.optimizations[\"none\"](\n\t\t\t_precise_function, is_optimized_function=_c_util.none\n\t\t)\n\n\n@pytest.fixture\ndef _c_optimizations_ignored():\n\t\"\"\"\n\tIf we just run \"x = c_optimizations(x)\" we would get \"c_optimizations().\"\n\t\"\"\"\n\t# We're using \"mixed_c_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn __cuda_c_optimizations & (~_c_optimizations)\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn self.__c_optimizations ^ self.__c_optimizations_required\n\n###########\n##  Functions\n###########\n\ndef _c_optimizations_required():\n\tglobal _C_Opts\n\treturn _C_Opts.c_optimizations_required()\n\nif not _os_is_windows():\n\tdef _c_optimizations_enabled():\n\t\tglobal _C_Opts\n\t\treturn _C_Opts.c_optimizations & _C_Opts.c_optimizations_enabled()\nelse:\n\t_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tfor level, feature_set in feature_set_data.iteritems():\n\t\tfor feature, (feature_data, num) in feature_set.iteritems():\n\t\t\tif feature_set[feature] == -1:\n\t\t\t\tnum = num - 1\n\t\t\telse:\n\t\t\t\tnum = feature_data[feature]\n\t\t\tif num!= 0:\n\t\t\t\tnon_removal_feature_data = feature_data[~feature]\n\t\t\t\tif not all([i in num  for i in non_removal_feature_data]):\n\t\t\t\t\treturn True\n\treturn False\n\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tc_module = sys.modules['_pyspm']\n\treturn c_module.__code__.co_code == None\n\n\ndef _c_optimizations_required():\n\t\"\"\"\n\tThe opposite of `_c_optimizations_ignored`.\n\t\"\"\"\n\tc_module = sys.modules['_pyspm']\n\tif c_module.__code__.co_argcount > 0 or c_module.__code__.co_code == _c_optimizations_code:\n\t\treturn False\n\treturn True\n\n\nclass _ModuleWra"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn _optimizations_required(function=\"c_optimization\", verbose=False)\n\ndef _optimizations_required(function=\"c_optimization\", verbose=True):\n\t\"\"\"\n\tDetermine whether optimizations are being used.\n\n\tParameters\n\t----------\n\tfunction : str, optional\n\t\tFunction on which optimizations should be performed. Expected\n\t\tto receive a single argument, which should be 2-tuple of `(func,\n\t\targuments)` where the first element of arg is a callable which\n\t\thandles arguments to the function and the second element"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\ttry:\n\t\timport c_api\n\t\trequired = c_api.PURE_PYTHON\n\t\trequired.extend([\"PyNumConsts\", \"PyArray_TYPE_MAX\", \"PyArray_TYPE_LIMIT\", \"PyArray_TYPE_PATTERN\", \"PyArray_TYPE_CONTIGUOUS\", \"PyArray_TYPE_STRIDE\"])\n\t\trequired.extend([\"PyArrayDef_CTYPE\", \"PyArrayDef_CONTIGUOUS\", \"PyArrayDef_STRIDE\"])\n\t\treturn True\n\texcept Import"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl()\n\n\n@feature('_use_c_impl')\n@register_feature\n@register_method\n@register_callmapper\ndef clean(key_name=None, *args, **kwargs):\n\t\"\"\"\n\t\tThis will remove any function declarations of\n\t\t`clean` from the function being called, or in\n\t\tother words, function calls and the function\n\t\targuments from the given function call.\n\n\t\tThis is the equivalent of calling\n\t\t``pytest.runner.clean`` and then passing\n\t\ta *clean* argument on"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _pure_python\n\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\treturn _use_c_impl(_use_c_impl_opt[\"C\"])\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn PY_OPTIONS.get(options.C_OPTIONS, None)\n\ndef _c_path_to_implement(path):\n\tresult = []\n\tfor f in os.listdir(path) :\n\t\tfname = os.path.join(path, f)\n\t\tif not fname.startswith('__pycache__'):\n\t\t\tres = _use_c_impl(fname)\n\t\t\tif res:\n\t\t\t\tresult.append(res)\n\treturn result\n\nif __name__ == '__main__':"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl()\n\n# Copyright (c) 2006-2012 `qgis <http://doc.qt.io/>`_\n# Copyright (c) 2012-2014, Ben Balaert <ben@balaert.biz>\n# Copyright (c) 2013, Larid Zamirov <lastyz@gmail.com>\n# Copyright (c) 2013, A. Koroconk <koroconko at gmail>\n#\n#    Permission is hereby granted, free of charge, to any person obtaining a\n#    copy of this software and"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif False in _use_c_impl():\n\t\treturn False\n\tif env['__NOSYS__'] == '__sysapi_' or env['PYTHON_API']:\n\t\treturn False\n\t# Check if the python API is used as natively\n\tif 'BUILT-IN' in _use_c_impl():\n\t\treturn True\n\traise EnvironmentError('Could not determine pytypes from the C compiler.  You may need to run `make -fpurevox` or recompile, if you want to use the pure version')\n\treturn"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif DEBUG: log.debug('PURE_PYTHON: %s' % _use_c_impl())\n\treturn _use_c_impl() and not C_OPTIMIZED\n\ndef _c_optimizations_provided():\n\t\"\"\"\n\tReturn a true value if the C optimizations are not required.\n\t\n\tThis uses the ``NO_USE_C_CFLAGS`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl() or C_OPTIMIZED\n\n__all"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn bool(_use_c_impl) and bool(_c_impl)\n\n\n# Run the tests against the pure Python version.\n\ndef _run_c_tests():\n\t\"\"\"\n\tRun the C-language tests against the pure PyPy version.\n\n\t\"\"\"\n\tpure_py_tests = sys.modules.find('numpy.core.basic_test.good_tests')\n\tc_tests = []\n\n\tfor pure_py_test in pure_py_tests:\n\t\tsrc_path = pure_py_test.relative_file_path\n\t\tif"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl() and (__debugflag_strong if __debugflag_strong is not None else True)\n\ndef _use_c_impl():\n\t\"\"\"\n\tReturn a true value if the C optimizations can be used.\n\t\n\tBy default, there are no additional optimizations.\n\t\"\"\"\n\treturn False if __debugflag_strong is None else (\n\t\tnot __use_purelib_python_header\n\t\tand _get_python_compiler() is not None\n\t\tand not _is_c_compiler(\n\t\t\t\t_get_python"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t#\n\t# reset bins \n\t#\n\tself._bins = {}\n\t\n\t#\n\t# reset histogram \n\t#\n\tself._hist = {}\n\t\n\t#reset stats\n\tself._hist['sum'] = 0 \n\tself._hist['min'] = 0 \n\tself._hist['max'] = 0 \n\tself._hist['count'] = 0\n\n\t#reset reset function\n\tself._resett = lambda *args: None\n\n\tself._resett()\n\t\n\t#reset bins\n\tself._"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.reset_bin()\n\t# self.reset_bin()\n\n\n\t# self.reset_bin()\n\tif self.initial_value:\n\t\tself.update_stat(self.initial_value)\n\tself._reset()\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.num_bins = self._num_bins\n\tif self._initial_value is None:\n\t\tself._initial_value = self.initial_value\n\t\tself._make_bins()\n\t\t\t\n\t\t\n\t## @returns: histogram value as a floating point number\n\tdef histogram(self):\n\t\t\"\"\"\n\t\tReturn the current histogram if the current context is a normal one.\n\t\tOtherwise, return the value of the given bin.\n\t\t\"\"\"\n\t\treturn float( self.current_bin ) / self._num_bins\n\n\t\t#"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.reset_bins(reset=True)\n\t\n\t# Reset histogram with empty buckets.\n\tself._histo_reset()\n\n\n# ------------------------------------------------------------------------\n# -------------------------------------------------------------- Reset\n# ------------------------------------------------------------------------\n\n\tdef reset(self, reset=True):\n\t\t\"\"\" Reset the histogram.\n\t\t\n\t\tParameters\n\t\t----------\n\t\treset : bool\n\t\t     - True to clear bins, False to insert new ones.\n\t\t\"\"\"\n\t\tself._bin_set.reset(reset=reset)\n\n\n\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\tif self.data.get('first') or self.data.get('last'):\n\t\tself.data = {}\n\t\tself.initial_data = {}\n\t\traise Exception('reset() called on uninitialized histogram!')\n\t\n\tself.data = {self.initial_data[i] for i in range(len(self.initial_data))}\n\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tsuper(Histogram, self).reset()\n\tself.initial = self._get_initial()\n\t\n\tself.n_bins = self._get_n_bins()\n\tself.initial_bins = self._make_bins(self.n_bins)\n\n\n\n\n\n\n\nclass MinMaxHistogram(Histogram):\n\tdef __init__(self, min, max=None):\n\t\tHistogram.__init__(self, min, max)\n\t\tself.reset()\n\n\tdef fit(self, data):\n\t\th = self"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\tself.reset_bins()\n\t\t\n\tif self.initial_value: \n\t\tself.reset(self.initial_value)\n\t\t\n\tself.initial_value = self.make_bins(len(self.bins))\n\t\t\n\tif self.nrows:\n\t\tself.nrows = len(self.bins) + 1        \n\n\tself.bins = [self.initial_value]\n\t\nclass Histogram2D(Histogram):\n\t\n\tdef __init__(self, nrows=1, ncol"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\tself.n_fnty = 0\n\tself.bins = self.reset_bins()\n\tif self.initial_value is not None:\n\t\tself.init_bins(self.initial_value)\n\n\tself._dict = None\n\tself.hist_dict = {}\n\n\ndef reset_histogram(self):\n\trv = Histogram.reset(self)\n\tif rv:\n\t\tself.init_hist()\n\treturn rv\n\n\ndef reset_bins(self, bins=None, initial_value=None"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\tself.reset_bins()\n\tself.reset_histogram()\n\tself.clear(self.default_title_key)\n\t\n\tfor c, item in enumerate(self.iteritems()):\n\t\tself.set(c, item[0])\n\t\t\n\t#self.save('reset.pkl-bins')\n\t#self.save('reset.pkl-hist')\n\t\n\tself.title_keys[self.default_title_key] = list(self.title())\n\tdel self.title_keys[self"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tif self.bin is None:\n\t\tself.bins = {}\n\telif self.bin.size()!= 1:\n\t\traise Exception(\"Bins must be initialized with a single bin.\")\n\tself.reset_bin_data()\n\n\tsuper(Histogram, self).reset()\n\n\t\nclass InteractionTable(object):\n\t\"\"\"\n\tCreates a history object that can be used to write to an xml\n\tconfiguration file when called. It is a convenience class, so\n\tthat you can subclass this class to add new bin types. \n\t\"\"\"\n\t"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\twarnings.warn(\"Use: ToCSV(separator=', ')\")\n\tif separator=='':\n\t\tnodes = []\n\t\tedges = []\n\t\tfor s in self.nodes():\n\t\t\tnodes.extend(map(float, s))\n\t\tfor e in self.edges():\n\t\t\tedges.append((float(map(n1, e.nodes()), n0)), float(map(n2, e.neighbors())))\n\t\treturn to_csv(separator=',', header=separator)+"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarn_deprecated_since_0.7()\n\tcsv = csv.writer(open(self._file, \"w\"))\n\tfor i, v in enumerate(self.get_data()):\n\t\tcsv.writerow(v.get_data())\n\treturn csv"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn self._convert(separator, header)\n\ndef to_csv_header(fname=\"\", separator=\",\", header=None, delimiter=\"\"):\n\t\".. deprecated:: 0.5 in Lena 0.5 to_csv_header is not used.\n\tConvert graph's first point to CSV and put a blank line between each point\"\n\treturn self.to_csv(separator, header, delimiter=delimiter)\n\ndef to_dask_sequence(graph, start=1, count=2000, dtype=None):\n\t\"\"\""}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise NotImplementedError(to_csv)\n\n\nfrom lena.output import FromCSV\n\n\nclass FromCSV(From):\n    _base = FromCSV\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Pass attributes to base class.\n        \"\"\"\n        self.name = kwargs.pop('name', 'FromCV')\n        self.kernels = kwargs.pop('kernels', 'lena_conv_dense_f32')\n        self.n_iter = self._base.__call__("}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t# TODO: use pysol instead.\n\ttry:\n\t\tto_csv(self, separator, header=header)\n\texcept TypeError:\n\t\tfrom pysol import numpy_table\n\t\tif 'to_csv' in dir(to_csv):\n\t\t\tto_csv=to_csv('to_csv', separator, header=header)\n\t\t\treturn to_csv(self)\n\t\telse:\t\t\t\t\n\t\t\tto_csv(self, separator, header=header)\n\t\t\tif header:\n\t\t\t\treturn header\n\t# We don't have a"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tprint(\"[Graphs.to_csv] to_csv:\")\n\n\tif header is None:\n\t\tprint(\"\\n[Graphs.to_csv] no value has been set in the header. If you want to \"\n\t\t       \"convert all points to a header, use ToCSV.\")\n\t\treturn\n\tpoints = None\n\t# make the first position the header\n\theader = header and header.lower()\n\tif header.find(\" \") >= 0:\n\t\theader = \" \" + header\n\tprint(\"[Graphs.to_csv] header:"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom collections import deque\n\tto_csv_queue = deque(maxlen=0)\n\tto_csv_queue.appendleft(self.nodes) # the first list\n\tdelimiter = separator or \",\"\n\ti = 0\n\twhile i < self.size():\n\t\tif i in to_csv_queue: # already processed a node\n\t\t\tto_csv_queue.popleft()\n\t\t\tdelimiter = delimiter or \",\"\n\t\t\tseparator = separator or \",\"\n\t\t\tcontinue\n\t\tc = self.get_node("}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tpass\n    \n\t\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tself.to_csv(separator=separator, header=header, sep=separator)\n\n\nclass ToLenaSequence(ToLena):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_lena is not used.\n\t      Iterables are converted to sequences.\n\t\"\"\"\n\tdef to_lena(self, separator=\",\", header=None):\n\t\treturn self.to_seq(separator=separator, header=header).tolist()\n\n\ndef to_lena(graph):\n\t\"\"\"\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\traise NotImplemented\n    to_csv = staticmethod(to_csv)\n\n    def __str__(self):\n        \"\"\"\n\t    :rtype: ``str``\n\n        The string representation.\n        \"\"\"\n        if self.is_directed():\n            return \"#directed\"\n        else:\n            return \"#undirected\"\n    \n    @classmethod\n    def new_from_csv(cls, source_csv):\n        \"\"\"\n\t    Construct from CSV\n\n\t    :param str source_csv: the source CSV string\n\t    :"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\ttry:\n\t\treturn self._coord_file.index_by_name(coord_name)[-1] # Return last element if coord_name doesn't exist.\n\texcept:\n\t\treturn []\n\n# =============================================================================\ndef _get_coord_name(filename):\n\t\"\"\"\"\n\t\tGet the name of a coordinate.\n\t\t- filename : str\n\t\t- return : str\n\t\tReturn only the name if it is not None.\n\n\t\t>>> file.get_coord_name('c_1')\n\t\t'c_1'\n\n\t\t>>> len(file.get_"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tname_index_map = self.name_index_map\n\tindices = []\n\tfor coord_name in coord_name_map:\n\t\tcoords_index = self.coords_index_map[coord_name]\n\t\tcoords_indices = [coord_index for coord_index in coords_index_map[coord_name] if coord_index[0] == coords_index[1]]\n\t\tindices += list(map(operator.itemgetter(0), coords_indices))\n\treturn indices, name_index_map"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif self.coord_names is None:\n\t\t# use the first index\n\t\tcoord_indices = range(0)\n\telse:\n\t\t# use the values in the first index\n\t\tcoord_names = self.get_coordinates()\n\t\tcoord_indices = self.coord_names_to_indices[coord_names]\n\treturn coord_indices\n\n\nclass GeospatialEnsembleGeocoder(object):\n\tdef __init__(self, geocoder):\n\t\t## Initialize the class\n\t\tself.geocoder = geocoder"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\ttry:\n\t\tinds = self.indexes[\"Coord_\"+coord_name].tox()\n\texcept TypeError:\n\t\tinds = []\n\treturn inds\n\n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tcoord_indices_all = list(_get_err_indices_list(self.err_corr))\n\tcoord_indices_all = list(set(coord_indices_all))\n\treturn coord_indices_all\n\ndef _get_err_indices_list(error_string):\n\t\"\"\"\n\tGet list of error indices.\n\t\"\"\"\n\terror_string = error_string.split()\n\tindices = []\n\tfor index_num in range(0,len(error_string)):\n\t\tindices.append(("}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\treturn self.__err_indices.get(coord_name, [])\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tindices = np.empty(len(self.model.xyzx))\n\tfor i, (coord_ind, coord_name) in enumerate(self.iter_over_xyzx(coord_name)):\n\t\tindices[i] = coord_ind\n\treturn indices\n\ndef _get_xyz_err(coord_name):\n\t\"\"\"\n\t\tGet error from coordinates as a flat array.\n\n\t\"\"\"\n\txyz_err = np.zeros(len(coord_name))\n\txyz_err[coord_name] = np"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif not self.errors.has_key(coord_name):\n\t\treturn None\t\n\terr = self.errors[coord_name]\n\tif not err.is_valid():\n\t\treturn None\n\telse:\n\t\treturn [ i[1] for i in err.values ]\n\n\t\n\t\n#end region \n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tindirect = self._indirects[0]\n\tif indirect:\n\t\tn_err = len(indirect)\n\t\tif coord_name == 'xy':\n\t\t\treturn np.arange(n_err)\n\n\t\treturn np.arange(n_err)\n\telse:\n\t\treturn self.indices[:, coord_name]\n\n\nclass DatasetErrorIndices(Dataset):\n\t\"\"\"\n\tGet the error indices corresponding to a dataset.\n\n\tParameters\n\t----------\n\tcoord_name : str\n\t\tThe coordinate used to retrieve the indices"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\treturn self._err_indices[coord_name],\n\n\t# Get the variable for the coordinate with name 'coord_name'.\n\t# @return the name of the error.\n \tdef _get_variable_name(self, coord_name):\n\t\treturn \"%s[%d]\" % (self._coordinate_name(coord_name),\n\t\t\tself._err_indices[coord_name][self._err_indices.keys().index(coord_name)]),\n\n###############################################################################\n# Create and get error objects with values from error index dict."}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tcontext.error = {}\n\tfor field in self.error_fields:\n\t\tfield_indices = list(_field_values.keys())\n\t\tfield_indices.sort()\n\t\tfor field_index in field_indices:\n\t\t\tif field_index == -1:\n\t\t\t\traise IndexError(\"invalid value\")\n\t\t\tval = _field_values[field]\n\t\t\terr_idx = val[\"offset\"].get() - field_index      # noqa\n\t\t\t\n\t\t\tif field in context.value.field_errors:\n\t\t\t\tcontext.error["}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself.error_E = context.fields(\"x\", \"error\").subcontext(\"E\") + context.fields(\"t\", \"error\")\n\tself.x_low = context.fields(\"x\", \"low_val\").subcontext(\"x\")\n\treturn self"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tif self.destruction:\n\t\tself.destruction.append(context)\n\telif self.state and self.state.is_subtask(self.destruction):\n\t\tparent = context.parent(self, self.destruction)\n\t\tfor key in (\"x_low\", \"t_low\"):\n\t\t\tparent.remove(context)\n\t\tfor i in xrange (len(self.destruction)):\n\t\t\tparent.append(context)\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif context.error:\n\t\treturn context.error\n\tif context.x is not None:\n\t\twith context.subcontexts(\"\", x=context.x):\n\t\t\tif not isinstance(context.y, numbers.Number):\n\t\t\t\tcontext.y = getattr(context.y, 'index')\n\tif context.y is not None:\n\t\twith context.subcontexts(\"\", y=context.y):\n\t\t\tif isinstance(context.x, numbers.Number):\n\t\t\t\tcontext.x = getattr(context.x, 'index') + context.x"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t# Note: We are using only Python objects.  Not everything in the\n\t# flow requires Python objects, so do not try to use this function;\n\t# it assumes a graph consists of \"plain\" objects.\n\t# (see :class:`.flow_py`)\n\t\n\terror__str = \"_update_{}_{}\".format(context.getattr(\"idx\", \"\"), context.getattr(\"idx\", \"\"))\t\t\n\tcontext.value.setattr(error__str, context.getattr(\"value\"))\n\tfor subcontext in context."}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t# create dict of errors => error in this dict format\n\terrors = {\n\t\"E1\": {\"x\": None, \"y\": None},\n\t\"E2\": {\"x\": None, \"y\": None},\n\t\"E3\": {\"x\": None, \"y\": None},\n\t\"E4\": {\"x\": None, \"y\": None},\n\t#\"E5\": {\"x\":\"<none>\", \"y\":\"<none>\"}\n\t\t#\"E5\" is a placeholder for \"e\"\n\t\"E6\": {\"x\": \"/\","}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif context.error:\n\t\t# append error names\n\t\terror_names = ['%2d_%s' % ((context.x[0]+1)).replace('-', '_')]\n\t\te = context.error.copy()\n\t\te.updatedict({'x': []})\n\t\tcontext.value += ',%s_%s' % error_names\n\t\tfor name in error_names[:-1]:\n\t\t\tif name in e:\n\t\t\t\terror_names.remove(name)\n\t\te['x'].append(0) # add a new index for"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tglobal _c, _d, _e\n\n\tif not _c and context['error']:\n\t\tx0 = _get_x0(context)\n\t\ty0 = _get_y0(context)\n\t\t_c = GraphContext(x0, y0, _error=context['error'])\n\t\tdel context['error']\n\t\n\tif _d is None:\n\t\t_d = Graph.destruction(_c)\n\telse:\n\t\t_d.update(context)\n\t\tdel context['error']\n\n\t\nclass Graph(object):\n\t"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tif isinstance(context, Context):\n\t\tcontext = context.value\n\tif isinstance(context, (str,unicode)):\n\t\tcontext = context.decode(\"utf-8\").encode(\"utf-8\")\n\tf = Context(**context)\n\tf.value = context\n\treturn f\n\t\n\t\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tfrom numpy import Inf\n\timport array # to keep the original behavior for numpy 1.2-like.\n\n\tdef check(self, value):\n\t\t# Note in this case, that we cannot simply remove subcontexts, as\n\t\t# otherwise they would disappear when we pass the context.\n\t\tindex, value = value\n\t\ttry:\n\t\t\tindex -= self.error_indices.index(index)\n\t\texcept ValueError:\n\t\t\tself.error_indices.append(index)\n\t\tsuper(_UpdateContext, self).check(value)\n\n\tdef __"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tintegration = numpy.interp(bins, edges, bbox_edges)\n\treturn numpy.round(integration, 4)\n\ndef find_integers_in_array(bins, edges):\n\t\"\"\"\n\tFind values in the array *bins* for a specific bin (or for a \n\tgiven edge).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\t\"\"\"\n\tfind_integers = numpy.interp(bins, edges, bin_edges"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn edges/float(len(bins))\n\ndef make_edges_integral(bins, edges):\n\t\"\"\"\n\tMake integral of a histogram using given *edges*.\n\t\n\t*bins* and *edges* will also contain\n\tpoints which lie near the edge, and must be sorted so that\n\tvalues are included.\n\t\"\"\"\n\tedges = filter(lambda b: not isnan(b), edges)\n\tedges = [e / float(len(bins)) for e in edges]\n\t\n\tbins_"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tN = len(bins)\n\n\t# if edges are empty array, make it empty\n\tif edges == []: edges = []\n\t\n\tedges= np.asarray(np.diff(edges))\n\tif not edges.shape[1] > 1:\n\t\traise ValueError, \"There are no edges in the boundaries.\"\n\n\t\n\tn, bins = np.histogram(edges[:-1], bins[:-1], density=False)\n\tif edges[-1] == 0: bins = bins[-1]\n\t\n\t\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# Compute integral over the mesh (it uses a linear\n\t# function, hence the need for the transformation)\t\n\tintegral_scale = scale(edges)\n\t# The input is a list of bins for each dimension\n\tbins = [ int(x) for x in bins ]\n\t# Make sure our input is a list\n\tif not (type(bins) == type([])) and not (type(bins) == type(int())):\n\t\traise ValueError('The bins should be a list of integers')\n\t# Add all integer values in each"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tfrom libmints import *\n\t\n\tN = bins.n()\n\n\tA = _array(bins)\n\n\t# 1. find the boundaries of the elements\n\tI, L = N, edges\n\n\tx0, x1 = bins.first, bins.last\n\n\tA -= x0\n\twhile A > L:\n\t\tB = A - L\n\t\tif B <= I: I = B\n\t\tif I < 0: I = N\n\t\tA, I = B, I\n\n\tB = L - I\n\n\tU = _zeros(edges"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tbins, edges = np.meshgrid(np.array(bins), np.array(edges))\n\tif len(bins) > 1:\n\t\txrange = [0, bins[0]] + bins[1:]\n\t\tymax = xrange[-1] + 1\n\t\tbins_range = ((bins == bins[0]), (bins == bins[-1]))\n\t\tcumsum = np.cumsum(bins_range[:-1]) / len(bins_range[1:])\n\t\txmax = np"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\tN = len(bins)\n\tif N!= len(edges) or N < 2:\n\t\traise ValueError('Length of centers must equal than 2. Got %d.' % (N,))\n\t\n\tif N == 2:\n\t\treturn bins[1] * edges[1]\n\t\n\tC = []\n\tfor i in range(N):\n\t\tC.append([i, 0])\n\t\tfor j in range(N):\n\t\t\tif i == j:\n\t\t\t\tC.append([j, 1])\n\t\t\telse:\n\t\t\t\tC."}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tbins = bin(bins)\n\twhile len(bins) > len(edges):\n\t\tif peaks:\n\t\t\tbins.remove(peaks[0])\n\t\telse:\n\t\t\tbins.remove(peaks)\n\t\tpeaks = [None]*len(bins)\n\t\n\tfor i in xrange(0, len(bins), steps):\n\t\tif edges[i] in xrange(bins.index(j) for j in bins[i+1:stop])+y_steps:\n\t\t\tbins[i] ="}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\ta = 2 * pi / (len(edges) + 1)\n\treturn sum([k * (1 - math.cos(w)) for k, w in enumerate(edges)]) * bins[1] + sum([k * (1 - math.cos(w)) for k, w in enumerate(edges)])\n\ndef histogram_histogram(data, bins=10, mean=0, mode='mean', interval=0.5, **kwargs):\n\t\"\"\"Generate a histogram object using the current bins for all entries in data\n\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tn, bins, edges, _, _ = bins, edges, edges.astype('double'), edges, edges.astype('int')\n\treturn bins * (n - edges - bins) / (edges - edges + bins).round(2)\n\ndef log2integral(bins, edges):\n\t\"\"\"\n\tCompute log2 integral (for n bins + edges).\n\t\n\t*bins* contain values, and *edges* form the mesh.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif type(seq) in [tuple, list]:\n\t\tfor tl in seq:\n\t\t\tif is_fill_request_seq(tl):\n\t\t\t\treturn True\n\t\treturn False\n\tif is_fill_request_elt(seq):\n\t\treturn True\n\treturn False\n\t\n\t\n\t\n\t\n\t\ndef is_fill_request_elt(elt):\n\t\"\"\"\n\tReturns True or False for the *elt* of a FillRequest element.\n\t\"\"\"\n\t\n\treturn (elt.tag == FillRequestElement._element_tag) and \\\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn isinstance(seq, FillRequestSeq)\n\n# -------------------------------\n\nclass DummyRequest(object):\n\n\t_seqs = None\n\t\"\"\"\n\tList of request sequences. The order matters,\n\tand the order of elements in the list is\n\tthe order the elements are added to the DummyRequest instance.\n\t\"\"\"\n\t\n\tdef __init__(self, seqs = None):\n\t\t\n\t\tif type(seqs) == list:\n\t\t\tself._seqs = [_ for _ in seqs]\n\t\telse:\n\t\t\tself._seq"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif isinstance(seq, FillRequest) or (isinstance(seq, Source) and\n\t    not isinstance(seq.start, (int, np.int32, complex, float, np.float64,\n\t    numpy.number, np.datetime)) or isinstance(seq.end, (int, np.int32, complex, float, np.float64,\n\t    numpy.number, np.datetime))):\n\t\treturn True\n\treturn False\n\n##############################\nclass TestFillRequest(unittest.TestCase):\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn is_source_or_fill(seq)\n\ndef is_source_or_fill_or_not_source(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a Source or\n\ta FillRequest.\n\t\n\tTrue only after all Source and FillRequest elements have been\n\tprocessed.\n\t\n\t*seq* can be converted to a Source sequence, or\n\ta FillRequest sequence.\n\t\"\"\"\n\treturn all(is_source_or_fill(a) for a in seq)\n\ndef is_source(e):"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\treturn isinstance(seq, (FillRequest, Source)) \n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif is_fill_request(seq):\n\t\tseq_type = seq.get_type()\n\telif is_source(seq):\n\t\tseq_type = Graphene.SEQUENCE_TO_TYPE[GS.REQUEST]\n\telse:\n\t\tseq_type = Graphene.SEQUENCE_TO_TYPE[GS.GENERIC]\n\tif seq_type is not None:\n\t\treturn is_fill_request_seq(seq)\n\telse:\n\t\treturn False\n\ndef encode_fill_request(fill_request):\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t# Try to parse the sequence\n\tfill_request = FillRequest(seq)\n\ttest = fill_request.get_id(), fill_request.get_sequence()\n\treturn not (test[0].startswith(\"FillRequest\") or test[0] == \"<FillRequest>\" or not isinstance(test[0], SourceSequence))\n\nclass FIFOTest(ZIPBTestCase):\n\t\"\"\"\n\tBaseclass for FIFO tests.\n\t\"\"\"\n\n\ttest_dir = \"test/tests/zip_utils/zope_testlibrary_test"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tfill_obj = get_fill_request(seq)\n\tif isinstance(fill_obj, FillRequest) or (\n\t\t\tisinstance(fill_obj, Source) and not is_fill_request_seq(fill_obj)\n\t) or (\n\t\t\tisinstance(fill_obj, Source) and is_fill_request_seq(fill_obj)\n\t):\n\t\treturn False\n\tfill_data = fill_obj.data()\n\tfor fill_obj in fill_obj.children():\n\t\tif (\n\t\t\t\tisinstance(fill_obj, FillRequest"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn all(map(isinstance, seq, [FillRequest, Source]))\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif len(seq) > 1:\n\t\tif not isinstance(seq[1], sg.Sequence):\n\t\t\treturn False\n\t\tsource = seq[1]\n\t\tif not source.is_request_seq():\n\t\t\treturn True\n\treturn False\n\ndef fill_request_seq(seq, fill_request_set):\n\tfiller = sg.FillRequestFillMarker(fill_request_set)\n\tfor seq_item in seq:\n\t\tfiller.fill(seq_item, seq[seq_item])\n\ndef is_fullfill_"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tmethods = dir(obj)\n\treturn filter(lambda fn: not fn.startswith('__'), methods)\n\n# Test with different values of fill or request_el (if fill is possible)\ndef test(obj, values):\n\tif is_fill_request_el(obj):\n\t\tobj.filler = \"%s_fill\"\n\telif fill_types[0] in values:\n\t\tobj.filler = \"%s_fill\"\n\telse:\n\t\tobj.filler = \"%s_request\"\n\ndef fill_text_and_"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn issubclass(type(obj), dict)\n\ndef get_color(color_string):\n\t\"\"\"\n\tGet color string. Converts the string to an enum value.\n\t\"\"\"\n\t\n\tif color_string in _color_numbers:\n\t\treturn _color_numbers[color_string]\n\telse:\n\t\traise ValueError( \"Color '{}' is not recognized.\".format( color_string) )\n\ndef get_line_colour(line_text, num_to_f):\n\t\"\"\"\n\tConvert the text to"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif hasattr(obj, 'fill') == False or hasattr(obj,'request') == False:\n\t\treturn False\n\telse:\n\t\treturn True\n\t\ndef is_print_request(obj):\n\t\"\"\"\n\tObject contains executable methods 'print' and'request'.\n\t\"\"\"\n\tif hasattr(obj, 'print') == False or hasattr(obj,'request') == False:\n\t\treturn False\n\telse:\n\t\treturn True\n\t\ndef is_print(obj):\n\t\"\"\"\n\tObject contains executable methods 'print','request',"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn getattr(obj, 'fill', None) or getattr(obj,'request', None)\n\ndef is_fill_response_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill','request' and'response'.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') or hasattr(obj,'request') or hasattr(obj,'response')\n\ndef get_request_fill(obj, fill=True):\n\t\"\"\"\n\tConvenience convenience method of using 'fill' and'request' directly.\n\t\"\"\"\n\treturn"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn isinstance(obj, basestring) or object_is_callable(obj)\n\n\ndef object_is_request_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and'request'.\n\t\"\"\"\n\treturn isinstance(obj, basestring) or object_is_callable(obj)\n\n\n\ndef exec_method(method):\n\t\"\"\"\n\tFunction that returns the result of the method, if it is callable it\n\treturns the return value of it.\n\t\"\"\"\n\treturn eval(method)\n\ndef is"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\ttry:\n\n\t\tif hasattr(obj, 'fill'):\n\t\t\treturn (True, '')\n\n\t\telif hasattr(obj,'request'):\n\t\t\treturn (True, obj.request)\n\n\texcept:\n\n\t\treturn (False, '')\n\n\treturn is_fill_request_el\n\n\ndef create_url(args):\n\t\"\"\"\n\tCreates url for given arguments.\n\t\"\"\"\n\treturn web.config['HREF'] + args\\\n\t\t\t\t\t\t\t+ web.config['FILLSEL'].get(args)\n\n\ndef get_url_"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (is_builtin(obj) and\n\t\t\t(getattr(obj, \"_fill\", False) or\n\t\t\t getattr(obj, \"_request\", False))\n\t\t)\n\n\ndef is_builtin(obj):\n\treturn callable(getattr(obj, \"_fill\") or\n\t\t\t\t\tgetattr(obj, \"_request\"))\n\ndef add_custom_method(obj):\n\tsetattr(obj, \"_fill\", is_fill_request_el(obj))\n\tsetattr(obj, \"_request\", is_fill_request_el(obj))\n\nif"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn obj['cmd'] == 'fill' or obj['cmd'] =='request'\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\ttry:\n\t\tobj_el = obj[0]\n\texcept:\n\t\tprint (\"Cannot get element!\")\n\t\treturn False\n\n\treturn bool(obj_el.toxml().split('name=entry')[0][1].strip())\n\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif not isinstance(obj, dict):\n\t\treturn\n\n\treturn_obj = dict()\n\t\t\n\tobj['method'] = obj['request']\n\t\n\tfor key in obj:\t\n\t\tobj[key] = obj[key].upper().replace('T','')\n\t\t\n\treturn_obj['method'] = obj['method']\n\treturn return_obj\n\n\n\n\n        \n"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn hasattr(obj, \"_run\")\n\n\ndef get_executable(obj):\n\t\"\"\"\n\tConvert executable method name to executable path.\n\t\"\"\"\n\treturn hasattr(obj, \"run\") and getattr(obj, \"_run\")\n\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tfunc = obj['function']\n\tif func!= None:\n\t\trun = getattr(globals().get(func, None), func)\n\t\treturn int(run) == 1\n\n\telse:\n\t\treturn None\n\n\ndef is_run_by(obj):\n\t\"\"\"\n\tGet run status of function at `obj`.\n\t\"\"\"\n\tresult = True\n\tfunc = obj['function']\n\tif func!= None:\n\t\trun = getattr(globals().get(func, None), func)\n\t\tresult = result and int(run) == 1"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tfor k, v in obj.iteritems():\n\t\tif isinstance(v, bool) or not callable(v):\n\t\t\tcontinue\n\t\telse:\n\t\t\texecv(v, sys.argv)\n\treturn True\n\n\nclass Config(dict):\n\t\"\"\"\n\t\tConfig class to create a config.yaml for the configuration.\n\t\t\n\t\tExample config.yaml:\n\t\t\n\t\t\t- \"debug\"\n\t\t\t- \"timeout\" = 300\n\t\t\t\n\t\t\t- \"auth\" = \"password\"\n\t\t\t- \"auth-password\" = \"${password}\"\n\t\t\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj and isinstance(obj, str)\n\n\nclass Py3compat:\n\n\t\"\"\"\n\tImplements the Python3 interface to Py3compat, which uses pure python2.\n\t\"\"\"\n\n\t__all__ = ['run_el']\n\n\tdef __init__(self):\n\t\tself.run, self.run_str = None, None\n\t\tif Py3compat.PY3 or Py3compat.Py3compatExceptionPending():\n\t\t\t# py3.5+\n\t\t\timport os\n\t\t\timport sys\n\t\t\tfrom ctypes.util"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn issubclass(obj.im_self, os.system)\n\n\nclass BaseExecutable(object):\n\t\"\"\"\n\tBase class for all executable subclasses.\n\t\"\"\"\n\n\tdef run(self, env, **kwargs):\n\t\t\"\"\"\n\t\tReturns the command's result. If the method is not defined, raises a :class:`TypeError`\n\t\texception.\n\t\t\"\"\"\n\t\tfunc = getattr(self, self._fname)\n\t\treturn func(*env, self._argname, **kwargs)\n\nclass FileExecutable(BaseExecutable"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n\ndef is_exception(obj):\n\t\"\"\"\n\tTells if object contains an exception\n\n\t@return: True in case object contains an exception, False otherwise\n\t\"\"\"\n\treturn hasattr(obj, 'exception')\n\ndef is_ex_thrown(obj):\n\t\"\"\"\n\tTells if object contains an Exception but it is not a throw or is\n\totherwise just a not thrown exception\n\tThis is used for Python 3.5 when throwing an exception.\n\t\"\"\"\n\tif type(obj) is"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn (hasattr(obj, \"run\") and callable(obj.run))\n\ndef is_run_or_run_all(obj):\n\t\"\"\"\n\tObject contains executable method 'run_all', but also 'run'.\n\tReturn the name of this method if it is a list, else a list, of\n\texecutable methods of this object.\n\t\"\"\"\n\ttry:\n\t\treturn obj.run_all or obj.run\n\texcept AttributeError:\n\t\treturn obj\n\ndef is_file(obj):\n\t\"\"\"\n\tObject contains a string"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tif hasattr(obj, \"run\"):\n\t\treturn True\n\treturn False\n\nclass run_function(object):\n\tdef __init__(self, obj):\n\t\tself.obj = obj\n\n\tdef __call__(self, *args, **kwargs):\n\t\t\"\"\" Executes function 'run'\n\t\t\"\"\"\n\t\tget_run_status(self, args, kwargs)\n\t\treturn self.obj(*args, **kwargs)\n\n\tdef info(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tReturns information about function run.\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj.__run__ is object_run_el\n\n\n# vim:ai:et:sts=4:fdm=marker\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj.__run__ is not None\n\ndef is_test_run_el(obj):\n\treturn isinstance(obj, Test) and is_run_el(obj)\n\ndef is_run_el(obj):\n\treturn isinstance(obj, Run)\n\ntry:\n\timport run_runner\n\timport config\n\trun_runner.RunRunner = RunRunner\nexcept:\n\tpass\n"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn obj.is_fill or obj.is_compute\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tdef is_fill_el(obj):\n\t\treturn all(k in obj.__dict__ for k in ('fill', 'compute'))\n\treturn is_fill_el"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn all([getattr(obj, e)(False) for e in [\"fill\", \"compute\"]])\n\ndef find_fill(f1, f2, f3, f4):\n\treturn [f1, f2, f4]\n\ndef get_fill_compute_el(obj):\n\t\"\"\"\n\tReturn the executed executable methods 'fill' and 'compute'.\n\t\n\tCan be called as follows:\n\t\t\tfill()\n\t\t\tcompute()\n\t\n\t:return: The list of executable methods that return True. \n\t\"\"\"\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn \"fill\" in obj\n\n\ndef get_filled_compute_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill', 'compute', 'fillv'.\",\n\t\"\"\"\n\treturn is_fill_compute_el(obj) or is_fill_compute_el_subset(obj)\n#\n#     fill(x, y, z=None, fill=None, fillv=None, fill_ratio=None)\n#     ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#     Fill method, see documentation for fill()\n\n\ndef is_fill"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif getattr(obj, 'compute', None):\n\t\treturn True\n\treturn False\n\n    def is_fill_data(obj):\n    \t\"\"\"\n    \tObject contains executable methods 'fill' and 'compute' and data.\n    \t\"\"\"\n    \treturn True\n\n\tif getattr(obj,'fill',None):\n\t\treturn True\n\treturn False\n\n    def fill(self):\n        \"\"\"\n\t    Fill the cell\n\t    \"\"\"\n        for cell in self.cells:\n            cell.fill_cell()\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tobj.fill = method_fill\n\tobj.compute = method_compute\n\n\ndef is_line_compute_el(obj):\n\t\"\"\"\n\tObject contains methods 'fill', 'compute', and 'compute_line'.\n\t\"\"\"\n\tobj.fill = method_fill\n\tobj.compute = method_compute_line\n\n\ndef is_circle_compute_el(obj):\n\t\"\"\"\n\tObject contains methods 'fill', 'compute', and 'compute_circle'.\n\t\"\"\"\n\tobj.fill = method_fill\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif not is_fill_compute(obj):\n\t\treturn False\n\treturn True\n\n\n# TODO:\n# TODO:\tcheck for this\n\n\ndef is_fill(obj):\n\treturn is_fill_compute(obj)\n\n\ndef is_fill_compute(obj):\n\tif is_movable(obj):\n\t\treturn True\n\tif is_cubic(obj):\n\t\treturn True\n\tif is_hermitian(obj):\n\t\treturn True\n\tif is_translucent(obj):\n\t\treturn True\n\tif"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tfname = obj.__file__    # get the file_name\n\tfname = fname.replace(' ','_') # get the fname without space\n\tassert isinstance(fname, str) and fname!= \"\",\\\n\t\t\"The file name in argument should be a constant\"\n\ttry:\n\t\timport_stack.append(fname) # append the __file__ argument to our import stack\n\t\tis_fill = True # if we arrive here, we have found a filled function\n\t\tif fname == fname.replace(' ','_'):"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif obj.is_compute() == False and obj.is_fill() == False:\n\t\treturn True\n\treturn find_first_callable(obj.get_methods('fill,compute'), is_iterable)\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tpass\n\n\t# Get the value for the 'fill' method.\n\t#@c.property\n\t#def fill(self):\n\t#\tpass\n\n\t# Get the value for the 'compute' method.\n\t#@c.property\n\t#def compute(self):\n\t#\tpass\n\n\nclass Excellon_mesh(object):\n\t\"\"\"\n\tSuperclass of all class that can be used in meshtools.\n\t\"\"\"\n\t@classmethod\n\tdef get_samples(cls):\n\t\tpass\n\n\t@classmethod\n\t"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level <= 0:\n\t\treturn None\n\n\tif level >= 1000:\n\t\t# TODO: make this more general and not infinite\n\t\treturn {\n\t\t\tkey: d1.setdefault(key, {}) for key in\n\t\t\treversed(list(set(d1).difference(d2)) +\n\t\t\t\t[(key,) for key in d2.keys()])\n\t\t}\n\n\tlevel += 1\n\treturn intersection({\n\t\tkey: d1.get(key, {})\n\t\tfor key in set(d1).difference(d2)\n"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\treturn { (k: v[:] if level == 1 else v )[:] for k, v in d1.items() }\n\t\n"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\tsubd = {}\n\tfor key1, item1 in d1.items():\n\t\tif key1 not in subd:  # key not in d1\n\t\t\tfor subkey in d2.keys():\n\t\t\t\tif subkey in item1:  # item contained in d1\n\t\t\t\t\tif item1.get(subkey) is None:\n\t\t\t\t\t\tsubd[subkey] = d2[subkey]\n\t\t\t\t\telse:\n\t\t\t\t\t\t# item with different value\n\t\t\t\t\t\tif level == -1:\n\t\t\t\t\t\t\tlevel = len(subd) + len(subd.get"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level < 0: level = -1\n\tif level == -1: return {\"all\": d1, {\"all\": d2}}"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td1 = list( d1 )\n\td2 = list( d2 )\n\tlevel = min( level, len(d1) )\n\t\n\tdelta = dict()\n\tfor k in range( len(d1) ):\n\t\tdelta[d1[k]].update( {} )\n\tfor k in range( len(d2) ):\n\t\tif k!= level:\n\t\t\tdelta[d2[k]].update( {} )\n\t\telse:\n\t\t\tdeltas, deltas2, deltas3 = 0, 0,"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level == 1:\n\t\tk = {}\n\t\tfor k1 in d1:\n\t\t\tk.update(list(d1[k1]))\n\t\t\tk[k1] |= set(d2[k1])\n\t\treturn k\n\telse:\n\t\told_levels = len(list(d1.values()))\n\t\tnew_levels = new_levels = 0\n\t\twhile d2:\n\t\t\tnew_levels = new_levels + 1\n\t\t\td1 = d2\n\t\t\td2 = set(d2)\n\t\t\told_levels = len("}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\treturn difference(d1, d2, level=level if level > 0 else -1)\n\ndef intersection(item1, item2, level=-1):\n    \"\"\"\n    Return dictionary of items from first to last occurrence of key *item* in\n    *item2*. (The key is not copied.)\n    \n    *len* is the same as :func:`item1.__len__`.\n    *level* is the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\tdef helper(item, level):\n\t\tif level == 0:\n\t\t\treturn item\n\t\telif level == 1:\n\t\t\treturn helper(item[item.find(item) + len(item),], -1)\n\treturn dict([(helper(_, -1) for _ in d1),\n\t              (helper(_, level) for _ in d2)]).in_order()"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tret = {}\n\tif level > 1:\n\t\td1 = pygame.dictdifference(d1, d2)\n\t\td2 = pygame.dictdifference(d2, d1)\n\t\tif len(d1) > 0 and d2:\n\t\t\t# Both are defined, so there's no point in creating a deep copy of both.\n\t\t\t# This makes it easier if you want to have an empty dictionary.\n\t\t\tret = d1\n\telse:\n\t\t# Both are defined, so just use the whole set\n\t\tret = d2\n\tif"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level == 0:\n\t\treturn { (key, d1.get(key)) : v for (key, d1) in difference(d1, d2, level=-1)\n\t\t\t\t if len(d1) > 0 and len(d2) > 0 \n\t\t\t\t and key not in d1 and d1.get(key) == d2.get(key)]\n\t\t\n\t# Level-1: difference without recursion\n\td1 = d1copy(d1)\n\td2 = d2copy(d2)\n\treturn difference(d1"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\timport pandas as pd\n\ttry :\n\t\treturn pd.DataFrame(\n\t\t\tnp.array(\n\t\t\t\t[[self.bin_max, self.bin_max]] + \n\t\t\t\tnp.array(\n\t\t\t\t\t[[[i, s]] + np.array(\n\t\t\t\t\t\t[[[j, t]]] + np.array(\n\t\t\t\t\t\t\t[([i, None],\n\t\t\t\t\t\t\t  (j, t)]),)\n\t\t\t\t\t\tfor (i, t) in np.nd_repeat((coord[1] - coord[0]), 2).T]\n\t\t\t\t\t) for (i,"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._assert_positive('weight')\n\tif weight < 1:\n\t\traise ValueError(\"weight must be >= 1.\")\n\tif weight >= 1:\n\t\t# TODO: this should not take advantage of the fact that\n\t\t# weights are > 1 (we do not support weight = 2), but this\n\t\t# is still a need since we cannot control weights when we\n\t\t# draw histograms.\n\t\treturn super().fill(coord, weight)\n\telse:\n\t\traise ValueError(\"weight >= 1 is not supported.\")"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tH,W = self.hist\n\n\tfor row in range(len(dim)):\n\t\trow_weight = np.dot(H[coord],np.tile(W[row].flat,(len(H[row])))[:,::-1]).sum(axis=0) / np.sum(np.tile(w_max[row],(len(H[row])))[:,::-1])\n\t\tH[coord] += row_weight\n\treturn H\n\nprint(\"Creating a histogram with dimension=5\")\nH = zeros((5, 5"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\t# Compute the histogram\n\th = max(coord,0)\n\ts = max(coord,0)\n\tw = coord+weight -1\n\t\n\treturn {\n\t\t'Bins': (h+1, s+1, w+1),\n\t\t'FoldBands': 0,\n\t\t'OverflowCount': ((h+1)*(w+1)) / (2*s),\n\t\t'LowBand': h + 1 - weight //2,\n\t\t'HighBand': h + (weight+1) // 2\n\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\tself.bin_width = coord[0]\n\n\thistogram = 1 / self.bin_width\n\n\ti = 0\n\n\tsum = 0\n\tcount = 0\n\n\twhile True:\n\n\t\ti += 1\n\t\ttmp = coord[1] - 1\n\t\tbins = self.bin_width / 6.0\n\t\tlh =  bins - tmp \n\t\t\n\t\tsum += bins \n\t\tif bins / sum >= histogram: break\n\t\t\n\t\n\t\tlh = (bin_size / 2.0) * (lh)\n\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\t# calculate coordinates of new histogram\n\ty0 = coord.y0\n\ty1 = coord.y1\n\tx0 = coord.x0\n\tx1 = coord.x1\n\t\n\t# make sure coordinates don't go out of the window at edge\n\tif y0<0 or y1>data.ny-1 or y1>data.ny-1:\n\t\traise RangeError, \"Histogram center needs to be inside the window\" \n\tif \n\t\tx0<0 or x1>data.nx-1 or x"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\treturn (self.value(coord) * weight)**0.5\n\nclass Histogram(object):\n\tdef __init__(self):\n\t\tself.value = Counter()\n\n\tdef get_values(self, **keys):\n\t\t\"\"\"\n\t\tGet (set by calling :meth:`~BaseHistogram.set`):\n\t\tAll the values of all keys in `keys`.\n\t\t\n\t\tIf a key is missing it then its value is its default value.\n\t\t\n\t\tIf a key can be determined but does not exist, then the default is returned.\n\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif not self.has_valid_axis(*coord):\n\t\traise ValueError(\"No axes given for fill: %s\" % coordinate)\n\telif coord == NONE_AXIS:\n\t\traise ValueError(\"No axis given for fill\")\n\telif self._axis_coords_valid(coord)!= coord:\n\t\traise ValueError(\"Given coord (%s) is not valid\" % str(coord))\n\n\tweight, n_coords, n_bins = self.get_bins_and_n_bins(False)\n\n\t# we need to"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tassert self.shape == coord.shape\n\tif axis in self.axes.keys():\n\t\tax = self.axes[axis]\n\t\tindex = lu.find_le(self.index, coord)\n\t\tvmin, vmax = lu.find_closest(index, coord[0])\n\t\tv = lu.find_le(index, coord)\n\t\tidx = lu.find_le(vmin, vmax)\n\t\tfor i in range(len(v)):\n\t\t\tidx[i] += idx[i"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\tself.hdr[\"Bands\"] = { 1 : (float, 1),\n\t\t\t     2 : tuple(float, 1),\n\t\t\t     3 : \"\",\n\t\t\t     }\n\t\n\thdr = {\n\t\t\t\"B\": {1: {'min' : float,'max' : float},\n\t\t\t\t 'count' : float,\n\t\t\t\t },\n\t\t\t'B': {2: {'min' : float,'max' : float},\n\t\t\t\t'count' : float,\n\t\t\t\t}\n\t\t\t}\n \n\td = (np.clip(coord"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t# the value_for function will be returned as the \"is\" argument to each element of the iterator\n\treturn map(lambda x: x.value_for(labels), enumerate(labels))\n\t"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\ttry:\n\t    if \"name\" in keys and \"value\" in labels:\n\t\tassert_true(\n\t\t\tvalidation_utils.list_key_sub_match(keys, labels[\"name\"])\n\t\t)\n\t\tassert_true(\n\t\t\tvalidation_utils.list_key_sub_match(keys, labels[\"value\"])\n\t\t)\n\t\tassert_true(\n\t\t\tvalidation_utils.list_key_sub_match(keys, labels)\n\t\t)\n\t\n\texcept Exception as err:\n\t    raise ValidationError(err)\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t# Check keys and values in the given labels.\n\tassert set(labels.keys()) == set(set(labels))\n\tassert {key for key, lab in labels.items()} == set(labels.keys())\n\n\t# Filter out items for which the regex does not match.\n\tfor label in labels:\n\t\tif not re.search(label.key, label.value):\n\t\t\tyield ValidationError(\"labels['{}']\".format(label.key),\n\t\t\t\t\t\t\t\t\t\"label key '{0}' does not match a regular expression\"\n\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor (key, v) in labels.items():\n\t\tif not isinstance(key, basestring):\n\t\t\tif isinstance(v, (basestring, unicode)):\n\t\t\t\traise ValidationError(\"A required key '%s' must contain a non-null string.\" % key)\n\t\t\telse:\n\t\t\t\traise ValidationError(\"A required key '%s' must contain a %s or a %s.\" % (key, type(v), type(v)))\n\t\tif isinstance(v, basestring):\n\t\t\tregex = re.compile("}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfrom.errors import ValidationError\n\t\n\tif not isinstance(labels, dict) or 'key' not in labels:\n\t\traise ValueError(\n\t\t\t'keys for the keys of the given labels must be a dict with'+\n\t\t\t'keys key, label or True, and False. The values in your dictionary'+\n\t\t\t'{0} do not match the corresponding re. Use'+\n\t\t\t'_labels_to_regex.py to generate the regexes'\n\t\t)\n\tfor key in labels:\n\t\tif (key, 'value') not"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\tvalid_label_regexes = _get_all_label_matching_regex(labels)\n\treturn bool(set(valid_label_regexes) & set(labels)) == None\n\n\ndef _get_all_label_matching_regex(labels):\n\t\"\"\"\n\tExtract all regex labels matching the given labels.\n\t\n\tExample:\n\t\n\t>>> _get_all_label_matching_regex({\"$$\": r\"foo\\|bar\", \"True\": r\"true$\", \"False\":"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\tassert isinstance(labels, dict)\n\t\treturn\n\tmessages = []\n\tfor label in labels:\n\t\ttry:\n\t\t\tmatch = _parse_regex(label)\n\t\texcept ValueError as err:\n\t\t\tmessages.append({\"valid\": label, \"invalid\": str(err)})\n\t\telse:\n\t\t\tif match.end() > 0 and match.group(1)!= label:\n\t\t\t\tmessages.append({\"valid\": label, \"invalid\": \"expected {}\".format(label"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor label in labels:\n\t\tregexp_keys = list(map(re.escape, label['key']))\n\t\tregexp_values = list(map(re.escape, label['value']))\n\t\tfor regexp in regexp_keys + regexp_values:\n\t\t\tif regexp not in label['regexp']:\n\t\t\t\traise ValidationError('label key \"{e}\" does not match regexp \"{r}\"'.format(**locals()))\n\t\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor label, regex in labels.items():\n\t\tif label not in regex:\n\t\t\traise ValidationError(\n\t\t\t\t\"Label '\"\n\n\t\t\t\t+ label\n\t\t\t\t+ \"' does not match regex '\"\n\n\t\t\t\t+ regex\n\t\t\t\t+ \"'.\"\n\t\t\t)\n\n\treturn\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tassert_that(labels, contains_inanyorder(\n\t    \"valid\", \"key1\",\n\t    \"key2\", \"$$\",\n\t    True,\n\t))\n\ndef _validate_string_type(string: string_type):\n\t\"\"\"Validate that the string is one of the specified valid string types.\n\t\n\tArgs:\n\t    string (string_type): the string which to validate.\n\n\tRaises:\n\t    ValidationError: if a given string is not one of the known valid string types.\n\t\n\tExample:"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\treturn __REGEX_MAPPING.get(ResourceNameType.get_string(), None)\n\t\n\t\n####################################################################\n\nclass _CheckResourceName(_ResourceName):\n\t\n\t\"\"\" Class to contain a list of checks (for error log) \"\"\"\n\t\n\tdef __init__(self, check_list):\n\t\t\n\t\tsuper(_CheckResourceName, self).__init__()\n\t\tself._check_list = check_list\n\t\tself._list = []\n\t\t\n\tdef check_check(self, check):\n\t\t\n\t\treturn self"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t# Create the return value:\n\treturn_value = re.compile(r\"^([a-zA-Z]*):$|^([a-zA-Z]*)$\", re.IGNORECASE)\n\t\n\t# Return the regular expressions:\n\treturn return_value._re = None\n\t\n\t# Return the error message:\n\treturn_value.message = \"Validation error: resource name may only contain a valid regex (e.g. 'kraken/name'). Valid names:\\n  %s\" % _get_"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\treturn re.compile(r'?([0-9]{1,4})|([1-2][0-9][0-9])).*')\n\t\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe path-id of the Krare resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the id of the Krare resource.\n\t\"\"\"\n\t\n\treturn re.compile(r'?([0-9]+?)(/(?:(?:[a-"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tpattern = r\"^(?i)(?i)([A-Za-z\\d_-][A-Za-z\\d_-]++)\"\n\treturn re.compile(pattern)\n\t\n\t\ndef _get_resource_names():\n\t\"\"\"\n\tReturns the set of all Krake resources.\n\t\"\"\"\n\treturn _list_for_each_element(\n\t\tsorted(os.listdir(os.path.join(data, \"resources\")))\n\t)\n\ndef _get_resource_name_by_pattern(pattern):"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\tresource_name_regex = []\n\t\n\treturn resource_name_regex\n\t\n\n\n'''\n\tFunction that updates the given map with the given resource.\n\t\n\tArguments:        \n\t    map: the map the resource should be updated for.\n\t\n\tKwargs:\n\t    name_string: the string of the name of the resource.\n\t    name_regex: the regular expressions that will be used to validate\n\t        the resource name.\n\t\n\tReturns:\n\t    None.\n\t'''\n\t\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tresult = _find_regex(\n\t\tr'^(?P<name_format_regex>.*)$',\n\t\tr'''\n\t\tThe format type regex. We cannot define that regular expression\n\t\twith default values, because we need the correct name_format\n\t\tto match.\n\t\t''',\n\t\t{\n\t\t\t'name_format_regex': _name_format_regex\n\t\t})\n\n\treturn result\n\ndef _determine_validation_method():\n\t\"\"\"\n\tDetermine the class of the validation method which will be used\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tresource_name_regex = None\n\tresource_name = None\n\tif _resource_name_regex():\n\t\tif not (resource_name_regex!= None):\n\t\t\tlog.log(LOG_FUNCTION, 'Already have previously created resource name regular expressions')\n\t\t\treturn resource_name_regex\n\t\tlog.log(LOG_FUNCTION, 'Creating resource name regular expressions')\n\t\tresource_name_regex = re.compile(_resource_name_regex)\n\tif _type_resource_name_regex():\n\t\tif not"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\tif (re.match(r''), None) == (True, False):\n\t\t_re = re.compile(\"^[^\\\\d]+$\")\n\t\treturn _re\n\telse:\n\t\t_re = re.compile(r\"\"\"^(?:[^\\\\d]+)(?::[^\\\\d]+)?$\"\"\", re.VERBOSE | re.IGNORECASE)\n\t\treturn _re\n\t\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tpattern = \"\"\n    resource_name = None\n    try:\n        if \"{\" in resource_name or \"}{\" in resource_name:\n            raise NameError\n        resource_name = resource_name.replace(\"{\", \"\\\\{\")\n        resource_name = resource_name.replace(\"}\", \"\\\\}\")\n    except NameError:\n        _LOG.debug(\"Warning: the pattern %s is missing.\" % pattern)\n\n\tpath_pattern = re.compile(r\"^(\\/([:_\\.]+://.*|\\/\\/[:_]."}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\tregex = dict(\n\t\t(\n\t\t\t(\"rscp://\" + purl.path.resolve(\"rscp://:rscp/resource:%(resource)s:\" % vars()).rstrip().lstrip(\"/\") + \"/\"), r\"^rscp/?([^/\"].*)\\s*(?::(?:%(resrc)s:)?([^?/]))?/\"\n\t\t\t)\n\t\t)\n\t\t\n\t)\n\tregex[\"%(resrc)s:rscp\"] = r\".*?\""}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tp = re.compile(value, re.I)\n\texcept ValueError as e:\n\t\traise ValidationError(\"Error during validation: '{0}'\".format(e))\n\telse:\n\t\treturn p\n\n# --------------------------------------------------------------------------\n#\tThe actual module and function declaration\n# --------------------------------------------------------------------------\n\nValidationError = ValidationError(\"Error during validation\")\n\n# --------------------------------------------------------------------------\n#\tMain\n# --------------------------------------------------------------------------\n\nvalidate_password = validate_value(password)\n\n# --------------------------------------------------------------------------\n#\tTests\n# --------------------------------------------------------------------------\n\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif _re.match(value) is None:\n\t\traise ValidationError(\"Value doesn't conform to regular expression\")\n\treturn value\n\n\ndef _check_value(value):\n\t\"\"\"\n\tConverts a value into a string according to the given regular expression.\n\t\n\tArgs:\n\t    value: the value as string\n\n\tReturns:\n\t    the string as the value.\n\t\"\"\"\n\treturn _re.escape(value)\n\n## Functions to extract values from the given string\n\ndef decode_value(value):\n\t\"\"\"\n\tRead a string"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif isinstance(value, six.string_types):\n\t\treturn re.compile(value)\n\telse:\n\t\t_raise_raise_error('Value is not of type string: %s' % value.__class__.__name__)\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t# Parse the value to get the regex\n\tregex = re.compile(value, re.VERBOSE)\n\t\n\t# Validate the regex\n\tif regex.match(value) is False:\n\t\traise ValidationError('The value \"%s\" matches the regular expression \"%s\", but it doesn\\'t match anything.' % (value, regex.pattern))\n\t\ndef validate_text_field(text_field, value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    text_"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tre.compile(value)\n\texcept re.error:\n\t\traise ValidationError(\"Invalid regular expression\")\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t# Check if it is a valid regular expression\n\ttry:\n\t\tmatch = re.match(r'^%s$|^%s$' % (re.escape(value),re.escape(re.escape(value))),value)\n\t\tif not match:\n\t\t\traise validation.ValidationError(\"Non-alphanumeric not handled: %s\" % value)\n\texcept AttributeError, e:\n\t\traise validation.ValidationError(\"Cannot parse regular expression\")\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\tregex = re.compile('^([a-zA-Z0-9_]+[a-zA-Z0-9_]*)$')\n\tvalue = regex.match(value)\n\t\n\tif not value: raise ValidationError('The given value \"%s\" is not a valid regular expression.' % value)\n\nclass ValueError(Exception):\n\tdef __init__(self, string):\n\t\tself.string = string\n\t\n\tdef __str__(self):\n\t\treturn \"%s is not a valid value.\" % self."}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tm = re.match(value, value)\n\tif not m:\n\t\traise ValidationError(\n\t\t\t'The specified value'+ value +'is not a regex.')\n\t\n\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t# Check the value against the regex.\n\tif not re.search(value):\n\t\traise ValidationError(\"Value `\" + value + \"' is not a valid regular expression\")\n\t\t\n\t\n\t\n\t\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(r'^((\\W|\\d*)?)$', value, re.I):\n\t\traise ValidationError(value)\n\ndef validate_keyword_arguments(keyword_arguments):\n\t\"\"\"\n\tValidate the keyword arguments.\n\n\tArgs:\n\t    keyword_arguments: the keyword arguments to validate\n\t\"\"\"\n\n\tif keyword_arguments:\n\t\tif not all(k in keyword_arguments):\n\t\t\traise ValidationError(keyword_arguments)\n\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tregex = '^([^/]+)?[^/]+$'\n\tif not re.match(regex, key):\n\t\traise ValidationError('Invalid key (expected {0}).'.format(regex))\n\ndef validate_object(obj):\n\t\"\"\"\n\tValidate a given YAML object.\n\t\n\tArgs:\n\t    obj: the object to validate\n\t\n\tRaises:\n\t    ValidationError: if the given object has invalid attributes.\n\t\"\"\"\n\tmissing = set(object_fields.keys().values()) |"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(r'type: (?P<type>[abcdefghijk]*)([a-z][0-9])*', key):\n\t\traise ValidationError('Key is not conform to regular expression')\n\t\n\treturn key\n\t\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t# get the keys in the regex pattern\n\tkeys = key.split(\"\\\\*\")\n\t\n\t# get the value\n\tvalue = _regex_validator_val_get(keys)\n\t\n\tassert value is not None\n\t\n\t# check the length of the value\n\tif not value:\n\t\treturn value\n\t\n\t# check if the value is a string\n\telif (is_instance(value, basestring)\n\tand isinstance(value, basestring)):\n\t\treturn value\n\t\n\t# check if value"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not key:\n\t\treturn\n\tnew_list = []\n\tnew_dict = {}\n\tfor item in key.split(\":\"):\n\t\tif not item:\n\t\t\tcontinue\n\t\tif not item.strip() in new_dict:\n\t\t\tnew_dict[item.strip()] = []\n\t\telse:\n\t\t\tnew_dict[item.strip()].append(key.strip())\n\tfor key, val in new_dict.items():\n\t\tif len(val) == 0:\n\t\t\traise ValidationError(key=key)\n\t\telif len("}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\tif not re.match(key, key):\n\t\traise ValidationError(key)\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\tkey = key.strip()\n\t\n\tif re.match(key, key) is None:\n\t\traise ValidationError(\"%s is not a valid key\" % key)\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t# Check the match\n\tif not re.match(key, key):\n\t\traise ValidationError(\"Regular expression failed\")\n\t\n\t# Check that any part of the key is not a blank\n\tif key:\n\t\tif key.isalnum():\n\t\t\traise ValidationError( \"key must not be one-character alphanumeric\" )\n\t\t\t\n\t# Check that the last character is a uppercase letter\n\tkey = key.upper()\n\tif key.isalnum():\n\t\traise ValidationError(\"key must not start with an alphanumeric\" )\n\t\t"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(r\"^[a-zA-Z][\\w\\d-]+$\", key):\n\t\traise ValidationError(\"Key is expected to contain only letters and numbers, and not spaces or dashes and cannot start with a dash.\")\n\n\n\n\tfor n in (\"a\", \"b\", \"c\", \"d\", \"e\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tpattern = re.compile('.{8}')\n\tif not pattern.match(key):\n\t\traise ValidationError(('\"{}\" does not match regular expression \"{}\".')\n\t\t\t\t.format(key, pattern.pattern))\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\t\n\tif not re.match(key, key):\n\t\traise ValidationError({'key':\"`key\\\" is not conform to the regular expression.\"})\n\t\n\treturn key\n\ndef validate_boolean(val):\n\t\"\"\"\n\tValidate the given boolean against the corresponding regular expression.\n\n\tArgs:\n\t    val: the true string or false true string\n\t\n\tRaises:\n\t    ValidationError: if the given boolean is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\t\n\tif val:\n\t\treturn"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tdefault_observer_schema_dict = {}\n\tnew_observer_schema_dict = {}\n\t\n\tfor label, content in manifest_dict.get(\"observer_schema\", {}).iteritems():\n\t\tif isinstance(content, list):\n\t\t\tif first_level:\n\t\t\t\tnew_observer_schema_dict[label] = [schema.copy() for schema in content]\n\t\t\telse:\n\t\t\t\tnew_observer_schema_dict[label] = schema_to_dict(content, manifest_dict)\n\t\telse:"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level:\n\t\tnew_dict = manifest_dict\n\telse:\n\t\tnew_observer_schema_list = []\n\t\tnew_observer_schema_dict = {}\n\t\tnew_observer_schema = manifest_dict\n\t\tfor key in manifest_dict.keys():\n\t\t\tif type(manifest_dict[key]) == list:\n\t\t\t\tnew_observer_schema_list.append(manifest_dict[key][0])\n\t\t\t\tfor item in manifest_dict[key][1]:\n\t\t\t\t\tnew_observer_schema"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif not manifest_dict; first_level:\n\t\tmanifest_dict =  manifest_dict.copy()\n\tif first_level and manifest_dict['apiVersion']!= manifest_dict['matching_api_version']:\n\t\tmanifest_dict['apiVersion'] = manifest_dict['matching_api_version']\n\t\tmanifest_dict['matching_api_version'] = None\n\tobserver_schema_list = []\n\tfor resource in manifest_dict['resources']:\n\t\tif manifest_dict['matching_api_version'] =="}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tmanifest_dict_replaced = {}\n\t\n\tif first_level:\n\t\t# Merge the manifest field values into the dictionary field name-value pairs\n\t\tfor manifest_item in manifest_dict.keys():\n\t\t\tmanifest_dict_replaced[manifest_dict[manifest_item]] = manifest_item\n\t\tfor manifest_list_item in manifest_dict_replaced.keys():\n\t\t\tmanifest_dict_replaced[manifest_dict_replaced[manifest_list_item]] = manifest_list_item\n\t\n\tfor manifest_item"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tresult = {}\n\n\tfor i, (s, v) in enumerate(manifest_dict.items()):\n\t\tsubset = manifest_dict.get(i, {})\n\t\tsubset_result = subsets_for_schema(s)\n\n\t\tif first_level:\n\t\t\tif set(subset_result) == set(manifest_dict):\n\t\t\t\tresult.update(subset)\n\n\t\t\t\tfor j in subtree_level_dict(s, first_level=False):\n\t\t\t\t\tsubschema = generate_default_observer_schema_dict"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif manifest_dict[\"kind\"] == \"Event\":\n\t\treturn {}\n\telif manifest_dict[\"type\"] == \"List\":\n\t\tobserver_schema = manifest_dict\n\telif manifest_dict[\"type\"] == \"Resource\":\n\t\tobserver_schema = manifest_dict.copy()\n\telif first_level:\n\t\tobserver_schema = manifest_dict\n\telse:\n\t\tobserver_schema = manifest_dict[\"observerSchema\"]\n\treturn observer_schema\n\n\ndef generate_default_observer_schema_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\t\n\tfor key, schema_field in manifest_dict.items():\n\t\tif key =='metadata' and schema_field['type'] =='str':\n\t\t\tif 'value' in manifest_dict and manifest_dict['value'] is not None:\n\t\t\t\tvalue = schema_field['value']\n\t\t\t\tif len(value) > 0:\n\t\t\t\t\tkey_split = value.split(':')\n\t\t\t\t\tschema_field['value'] = key_split[0]\n\t\t\t\tif len(value) == 1:\n\t\t\t\t\tschema"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema = {\n\t\t'Kind': 'DefaultResource',\n\t\t'Name': '',\n\t\t'Type': 'List',\n\t}\n\tfor key in manifest_dict:\n\t\tif key not in ['Name', 'Type']:\n\t\t\tif isinstance(manifest_dict[key], six.binary_type):\n\t\t\t\tmanifest_dict[key] = manifest_dict.pop(key)\n\t\t\telif isinstance(manifest_dict[key], six.string_types) and manifest_dict[key]!= '':\n\t\t\t\tobserver_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level:\n\t\tobserver_schema = {}\n\t\tif manifest_dict:\n\t\t\tfor dkey in manifest_dict:\n\t\t\t\tkey = manifest_dict.get(dkey)\n\t\t\t\tif isinstance(key, dict):\n\t\t\t\t\tif isinstance(key, list):\n\t\t\t\t\t\tkey = dict(zip(key, list()))\n\t\t\t\t\telse:\n\t\t\t\t\t\tkey = dict(key)\n\t\t\t\t\tobserver_schema.update(key)\n\telse:\n\t\tobserver_schema = manifest_dict.copy()\n\treturn observer_schema\n\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t# get the keys of the manifest dictionary\n\tkeys = manifest_dict.keys()\n\tif first_level:\n\t\tdict_copy = dict((k, v) for k, v in manifest_dict.items() if k in keys)\n\telse:\n\t\tdict_copy = dict((k, v) for k, v in manifest_dict.items() if k not in keys)\n\n\t# append a null value between the defined entries, to ensure that there\n\t# isn't two entries with keys identical to the manifest keys\n\tdict_copy.pop('type',"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# TODO: need to be more generalised to handle all fields in the observation\n\t# structure but this will be easier when it becomes hard to keep the code\n\t# for the common structure\n\tfor observation in response:\n\n\t\tif \"first_observed_timestamp_ns\" in observation:\n\t\t\tobserver_schema = observation[\"first_observed_timestamp_ns\"].value\n\n\t\t\tif \"name\" in observer_schema:\n\t\t\t\tlast_applied_manifest[observer_schema[\"name\"]] = (observer_schema[\"value\"],"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif self.verbose:\n\t\tprint(\"update_last_applied_manifest_list_from_resp \"\n\t\t\t\"was called\")\n\t\tprint(\"updating last_applied_manifest in resp\")\n\t\tprint()\n\t\tprint(\"last_applied_manifest:\",self.last_applied_manifest)\n\t\tprint()\n\t\tfor r in self.last_applied_manifest:\n\t\t\tprint(\"observed field:\", r)\n\tfor k,v in r.items():\n\t\tif k == 'observed':\n\t\t\tcontinue"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor key, old_value in last_applied_manifest.items():\n\t\ttry:\n\t\t\tvalue = update_last_applied_manifest_from_resp(key, old_value)\n\t\texcept ApiException:\n\t\t\tlast_applied_manifest[key] = last_applied_manifest[key].copy()\n\n## def parse_dict_for_pipeline_by_name(d, pipeline_name, schema):\n## \t\"\"\"\n## \tParse a pipeline dictionary to get the expected field type\n## \t\"\"\""}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor _ in _for_fields('last_applied_manifest'):\n\t\ttry:\n\t\t\tlast_applied_manifest[_]\n\t\t\tcontinue\n\t\texcept KeyError:\n\t\t\t# if first, initialize all\n\t\t\tlast_applied_manifest[_] = [ ]\n\t\n\t\t# recursively update\n\t\tfor field_name, field_value in _for_fields('response'):\n\t\t\ttry:\n\t\t\t\tgetattr(response, field_name)\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\t\tif field_name in response:\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_dicts = []\n\tobserver_schema_dict = {}\n\tresponse_dict = []\n\t\n\tfor observed_field in obs_schema:\n\t\twith_schema_key = (isinstance(observed_field, dict) and'schema' in observed_field and 'key' in observed_field)\n\t\twith_schema_value = (isinstance(observed_field, dict) and'schema' in observed_field and 'value' in observed_field)\n\t\twith_observability ="}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_list = []\n\tif last_applied_manifest is None:\n\t\tfor observed in observer_schema:\n\t\t\tif observed is None:\n\t\t\t\tcontinue\n\t\t\tlast_applied_list.append(\n\t\t\t\tupdate_last_applied_manifest_from_response(observed))\n\n\telse:\n\t\tfor observed in observer_schema:\n\t\t\tif observed is None:\n\t\t\t\tcontinue\n\t\t\tif observed[\"name\"] not in last_applied_list:\n\t\t\t\tlast_applied_list.append(\n\t\t\t\t\tupdate_last_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tupdate_last_applied_manifest_list_from_resp(last_applied_manifest, response,\n\t                                            observer_schema,\n\t                                            update_manifest=True)\n\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor index in range(len(response)*2):\n\t\tlast_applied_manifest.append(\n\t\t\t{'last_applied_manifest': response[index]['last_applied_manifest']}\n\t\t)\n\ndef update_last_applied_manifest_dict_from_resp(\n\tlast_applied_manifest, response, observer_schema=None, key_suffix='lastManifest'\n\t):\n\t\"\"\"\n\tUpdate a ``last_applied_manifest`` dict.\n\n\tThis is a modified version of"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tlogger.info( 'Updating last_applied_manifest_list_from_resp' )\n\n\tlast_applied_manifest_dict_from_resp = dict()\n\n\tfor field, item in last_applied_manifest:\n\n\t\tis_observable = getattr(item,'status_code_raw_', '') in ['true', '1', 'true', '1']\n\n\t\tif is_observable:\n\t\t\tdel item['status_code_raw_']\t# This field can be observed\n\n\t\twith open(os"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in response.manifest.fields:\n\t\tif field.name == \"_links\":\n\t\t\tlink = json.loads(field.embedded_value)\n\t\t\t\n\t\t\tif link.get(link.type) == \"next\":\n\t\t\t\tlink.value = {} # reset the type to the empty dict\n\t\t\t\t\n\t\telif field.name == \"observedProperties\":\n\t\t\tobserved_dict = json.loads(field.field_value)\n\t\t\tif \"schema\" in observed_dict:\n\t\t\t\tobserved_dict['schema'].value = { \"def"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tretVal=last_applied_manifest(response=response)\n\tif not all([retVal is None, retVal==None]):\n\t\tfor key, value in retVal.items():\n\t\t\tif not self.__has_observed_field(key):\n\t\t\t\tself.__observed_field(key)=value\n\telse:\n\t\t# If there are no observed fields and both observed and observed_applied\n\t\t# fields have empty values, ignore the field. See https://github.com/kubernetes/enhancement/\n\t\tif not self."}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_schema = update_observed_field_from_resp(last_applied_manifest, resp)\t\n\tif observed_schema == {}:\n\t\tobserved_schema = update_observed_field_from_resp(last_applied_manifest, resp, observed_obj = \"observed\")\n\tlast_applied_manifest[\"last_applied_schema\"] = observed_schema # we want to update \"last_applied\", and \"observed\"\n\t\n\tobs1 = {}\n\tfor observed_key"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlog.debug('update_last_applied_manifest_dict_from_resp: Requested update...')\n\n\tfor k in observed_keys:\n\t\t# check if the observed key is present in the response\n\t\t_observed_present = False\n\t\tif 'observed' in response[k]:\n\t\t\t_observed_present = True\n\n\t\t# initialize an error when observed key is not found on kubernetes response\n\t\tif _observed_present:\n\t\t\tif 'error' in response[k]:\n\t\t\t\tlast_applied_manifest[k]"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed, info in odict.items():\n\t\tif observed not in manifest_dict and info['observed'] is not None:\n\t\t\tmanifest_dict[observed] = info['observed']\n\t\telif observed in manifest_dict:\n\t\t\tmanifest_dict[observed] = info['observed']\n\ndef serialization_deserialize_response(response_data):\n\t\"\"\"Deserialize the serialized response with the correct format\"\"\"\n\t# get all the fields\n\tparsed_response = None\n\ttry:\n\t\tdecoder = response"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# TODO: Update last_applied_manifest key\n\t# TODO: Only update last_applied_manifest if observer_schema exists\n\tobservers_schema = {}\n\t\n\tobserver_schema = observed_field\n\tfor observed_field in observer_schema:\n\t\tobserved_value = response[observed_field]\n\t\tif observed_value is None:\n\t\t\tcontinue\n\t\t\n\t\tupdated = False\n\t\tfor obs in observer_schema:\n\t\t\tobs_value = response[obs]\n\t\t\tif obs.split"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field_name, observed_field in enumerate(observed_fields):\n\t\tif observed_field.required and observed_field.type not in observed_field_type:\n\t\t\tlogger.warning('Found invalid field in the observed field: %s', observed_field_name)\n\t\telse:\n\t\t\tlast_applied_manifest[observed_field_name] = '' if observed_field.in_list else None\n\ndef check_resource_update(observed_fields_to_remove, observed_field_type,\n\t\tkube_resource"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor (key, value) in last_applied_manifest.items():\n\t\tval = last_applied_manifest[key]\n\t\t# check that this is the observed field\n\t\tif \"Observed\" not in val:\n\t\t\tval[\"Observed\"] = value\n\t\telse:\n\t\t\t# TODO: if key == \"Observed\" is present\n\t\t\tcur_value = last_applied_manifest[key][value]\n\t\t\tif cur_value and key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key]"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in ['observed', 'observed_by']:\n\t\tobserved = response[field]\n\t\tif field in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = observable if observed else last_applied_manifest[field]\n\t\telif field in observer_schema:\n\t\t\tlast_applied_manifest[field] = observable if observable else last_applied_manifest[field]\n\t\telse:\n\t\t\traise KeyError('The schema for observe field \"{}\" is not set.'.format(field))\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = update_last_applied_manifest_list_from_resp(\n\t    \"metadata\", \"name\", \"generator\")\n\t\n\t# Update the last observed field if it is not present in the generated list.\n\t# For example, if the observed field is empty string, the observer_field\n\t# will not be updated.\n\t#\n\t# For the observation field (if it doesn't exist), a check against self.observed\n\t# is made\n\tobserved = update_obj_observed_field(set("}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor pname in response_fields.keys():\n\t\tif pname not in last_applied_manifest_dict:\n\t\t\tlast_applied_manifest_dict[pname] = {}\n\t\tfor observed in response_fields[pname]:\n\t\t\tif observed not in last_applied_manifest_dict[pname]:\n\t\t\t\tlast_applied_manifest_dict[pname][observed] = \"\"\n\t\t\tlast_applied_manifest_dict[pname][observed] += (\n\t\t\t\tresponse[pname].get(observed, \""}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\tfor resource in app.spec.manifest.objects:\n\t\tfor kind in [\"endpoint\", \"resource/name\", \"namespace\", \"parameter\"]:\n\t\t\tif kind in resource.resources:\n\t\t\t\tfor key in resource.resources[kind]:\n\t\t\t\t\tobserver_schema = resource.manifest.schema\n\t\t\n\treturn observer_schema"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\tfor resource in app.root_obj.resources:\n\t\tresource_observer_schema = (\n\t\t\t\"\"\"\n\t\t\t{\n\t\t\t\t\"kind\": \"{schemas_model_group}\",\n\t\t\t\t\"apiVersion\": \"v1\",\n\t\t\t\t\"schema\": {\n\t\t\t\t\t\"apis\": [\n\t\t\t\t\t\t\"\"\"\",\n\t\t\t\t\t\t\"\"\"\",\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"apis\": [\n\t\t\t\t\t\t\t\t\"\"\"\",\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"name\": \"default-observer-schema/observations\",\n\t\t\t\t\t\t\t\t\t\"group\": \"default-observer-sche"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\treturn [\n\t\t{\t'namespace': 'default',\n\t\t\t'models': [\n\t\t\t\t{'name': 'Pod', 'path':'spec.templates.observe-pod.api.yaml'},\n\t\t\t\t{'name': 'ReplicationController', 'path':'spec.templates.observe-rc.api.yaml'}\n\t\t\t]\n\t\t},\n\t\t{\t'namespace':'spec.kubernetes.dynamic-observe-defaults',\n\t\t\t'models': [\n\t\t\t\t{'name': '"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tobserve_schema = [\n\t   'spec/observe.yaml',\n\t    ['manifest', \"spec/manifest.manifest\"],\n\t    ['manifest/pods', \"spec/manifest/pods\"],\n\t    ['spec/replicas', \"spec/replicas\"],\n\t    ['spec/containers', \"spec/containers\"],\n\t    ['spec/volumes', \"spec/volumes\"],\n\t    ['spec/initContainers', \"spec/initContainers\"],\n\t    ['image/-viz', \""}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t# TODO: Add a mechanism to avoid re-parsing specs from the manifest\n\t\n\tconfig = app.spec_info.generate_config(app)\n\t\n\t# If no observer_schema is specified, use a default by default\n\tif config is None:\n\t\tobserver_schema = kubernetes_default_observer_schema or default_observer_schema\n\t\t# Don't overwrite the user-supplied schema\n\t\tif config is None:\n\t\t\tconfig = {\n\t\t\t\tkube_config.KUBE['observer"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tdefault_observer = '#'\n\tsubnets = [s for s in app.spec.manifest.objects.data.deployment.resources\n\t\t   if s.type is'subnetwork' and s.resource.data.kubernetes.spec.get('orchestration', {}).get('name', None) == app.spec.manifest.objects.data.deployment.objects.generate_hostnames.default_name]\n\tfor subnet in subnets:\n\t\tresource, cidr, default_observer = subnet.data"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tobserver_data_dir = getenv(\"KUBE_API_SCHEMA_PATH\") + \"/spec/default_observer\"\n\tif not observer_data_dir:\n\t\treturn\n\tfor t in [\"controllerconfigs\", \"podcontrollers\", \"podinstances\"]:\n\t\tdata_dir = os.path.join(observer_data_dir, t)\n\t\tcamel_data_dir = os.path.join(data_dir, \"k8s-camel-api-schema/camel_api\")\t\n\t\tif os"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor path in spec.manifest.get('paths', []):\n\t\trelative_path = path[1:]\n\t\tif path.startswith('v2.0'):\n\t\t\t# V2 schema always provides pod.Status. All other schemas should provide pod\n\t\t\t# information.\n\t\t\tconfig = path.split('/')[-1]\n\t\t\tschemas_dir = os.path.join(config.split('./')[0],'schemas')\n\t\t\tobj_schema_path = os.path.join(config.split('./"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tdefault_observer_schema = [\n\t\t\t{'type': 'object', 'properties':{'event':{'$ref' : '#/definitions/events.observers'}}},\n\t\t\t{'name': 'ObjectMeta', 'properties':{'time':{'$ref' : '#/definitions/events.time'}}}]\n\tapp.set('default_observer_schema', default_observer_schema)"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tobserver_list = []\n\n\tobserver_schema_file_exists = os.path.exists(\n\t\tos.path.join(\n\t\t\tapp.config.kubernetes_data_dir,\n\t\t\t'requirements.yaml',\n\t\t\t'observe')\n\t)\n\n\tif not observer_schema_file_exists:\n\t\tprint('[*] Skipping K8s resource generator: No `observe` schema found at {0}'\n\t\t     .format(\n\t\t\t\tos.path.abspath(os."}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\tconverted = []\n\tfor c in sql:\n\t\tif isinstance(c, tuple):\n\t\t\tconverted.append(c)\n\t\telif isinstance(c, dict):\n\t\t\tfor k in c.keys():\n\t\t\t\tv = c[k]\n\t\t\t\tif isinstance(v, tuple):\n\t\t\t\t\tconverted.append((k, list(v)))\n\t\t\t\telse:\n\t\t\t\t\tconverted.append((k, v))\n\t\telif isinstance(c, list):\n\t\t\tconverted.append(list(c))\n\t\telse:\n\t\t\tconverted"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif isinstance(sql, collections.abc.Mapping):\n\t\tsql = fq.SQLToSQL(filter, set=sql, style=style)\n\telif isinstance(sql, collections.abc.Sequence):\n\t\tfor i, _ in enumerate(sql):\n\t\t\tsql = fq._SQLToSql(filter, i, style=style)\n\treturn (fq._sql, fq._params)\n\nsql = \"\"\"(SELECT?x?y?z? WHERE?)\"\"\"\n\nsql.additional_options = (('limit', 100),"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n)\n\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t#: The out-style parameters of the parsed query.\n\tout_params = {}\n\t#: The in-style parameters of the parsed query.\n\tin_params = {}\n\t#: The values that were stored to the SQL table.\n\tvalue_map = {}\n\t#: Is the query being parsed an in-style query?\n\tin_style = True\n\t\n\ttry:\n\t\t# Parse the SQL query. (Try for this first!)\n\t\ttuple_ = psql_parser.parse_sql_query(sql)\n\t\t\n\t\t#"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tSQLParams.in_style(params.in_style)\t\n\tformat_ = format_template.format(**dict(sql=SQLString(params.sql, params.formatter.param_format),\n\t                                           out_style=dict(k=v))\n\t                                   for v in params.params)\n\toutput = format_\n\t\t\n\tif len(format_) > 0:\n\t\toutput += format(output)\n\telse:\n\t\toutput += '[Not in styles]'\n\treturn output\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\treturn self._out_style(self._sql, params)\n\t\n\tdef list(self):\n\t\t\"\"\"\n\t\tConvert the SQL query to use out-style parameters instead of the\n\t\tin-style parameters.\n\t\t\n\t\tReturns a :class:`list` containing an enumeration of the converted out-style parameters.\n\t\t\n\t\t.. versionadded:: 3.1\n\t\t\n\t\tReturns:\n\t\t\tList containing the passed in output parameters (:class:`dict` or\n\t\t\t:class:`list`).\n\t\t\"\"\"\n\t\treturn list(reversed(self"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\tif sql is None:\n\t\tsql = ''\n\t\n\tquery = _check_list(query)\n\t\n\tassert sql in ('', None), 'in-style SQL query must be either None or a list of parameters'\n\t\n\tparams = kwargs.pop('params', None)\n\t\n\t# try using in-style query\n\tout_style = _SQLParams.out_style(sql, params)\n\tassert out_style, 'in-style query must be of type None'\n\tquery = params\n\t\n\tif out_"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t# TODO: make this a helper function\n\tres = _sql.format(**locals())\n\treturn res.encode(parameters,'safe', **kwd)\n\t\ndef _parse_query(query, parameters=False, params=None):\n\t\"\"\"\n\tReturn the parsed SQL query as a :class:`.sql.Statement` or\n\t:class:`.sql.Query`.\n\t\"\"\"\n\treturn _sql.parse(query, parameters, params, [], {})\n'''\n\nclass Parser(object):\n\t\n\t_default"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\tsq = SQLParams(**kw)\n\tsq = sq.sql(**kw)\n\t\n\tif params:\n\t\treturn sq, params.sql_parameters(dict(sq))\n\t\t\n\treturn sq, q\n"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\tif not many_params:\n\t\tmany_params = sql_map(*many_params)\n\t\n\t# Handle parameters from all style. If no style applies use the\n\t# values that they're found in `sql`.\n\tif in_style == \"any\":\n\t\tin_style = None\n\telif in_style == \"named\":\n\t\tin_style = sql_map(*sql_map.named_styles.items())\n\t\t\t\t\n\tif in_style:\n\t\tif len(in_style) ==\"any\" or len(in_style)"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tout_params_set = SQLParams.in_style('out')\n\t\n\t# Parse SQL query\n\tsql = parse(sql)\n\tsql = sql.replace('$', '``')\n\t\n\t# Check input parameters\n\tif self.params is None:\n\t\traise TypeError('You must use out_style parameter')\n\telif not isinstance(self.params, (dict,\n\t                                   (dict,\n\t                                    Sequence,\n\t                                    list,\n\t                                    Mapping)\n\t                                )):"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t# the SQL query must be parsed (or otherwise parsed)\n\tif sql is None:\n\t\tsql = _sql_parse\n\t\n\t# a list of parameter names\n\tif many_params is None:\n\t\tmany_params = _sql_get_param_names(sql)\n\t\n\t# a list of tuples/dicts of parameter names to values\n\tif isinstance(many_params, dict) or is_iterable(many_params):\n\t\tparameters = get(many_params)\n\t\t\n\t\t# the set of parameters can be a"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tsql = self.get_sql(*args)\n\tout = []\n\tfor in_style in self.in_styles:\n\t\tif in_style is not None:\n\t\t\tif len(self.in_style_params.get(in_style, {})) > 0:\n\t\t\t\tformat_params = {}\n\t\t\t\tfor in_param in self.in_style_params[in_style]:\n\t\t\t\t\tformat_params[in_param] = in_param\n\t\t\t\t\n\t\t\t\tif len(self.in_style_params.get(in_style, {})) > 0:"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tsql = sql.replace('%%%', '%*' % '''parameter'''.replace(':', '''variable%%%''').replace(';', ''))\n\tsql = sql.replace('%%%', '%*' % '''''variable'''.replace(':', '''''variable'''''.replace(':', '')))\n\t\n\tparams = _SQLParams.parse_many_query(json.loads(sql), many_params)\n\tsql += '%*s' % '''parameters'''.replace(':', '')\n\tdict2s ="}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\treturn SQLParams.from_str(format(sql, many_params=lambda x:x.value(x)))\n\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\tfrom pyspark.sql import SQLContext\n\treturn SQLContext._to_seq_execute(\n\t\tself, sql, many_params=many_params\n\t)\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\ts = str(sql)\n\tm = list(sql.split(\";\"))\n\t\n\tc1, c2, r = 0, 0, False\n\t\n\tsql = sql.strip()\n\t\n\tif sql.startswith('SELECT'):\n\t\t# TODO: Remove the in-style parameters (by using a separate list)\n\t\tc1 = 0\n\t\tc2 = 1\n\t\tr = False\n\telse:\n\t\t# First select only\n\t\tc1 = None\n\t\tr = True\n\t\t\n\tfor s, e in m:\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\tout_style = None\n\tif many_params is None:\n\t\tif isinstance(params, dict):\n\t\t\tout_style = params\n\t\t\tparams = (None,) * len(params)\n\t\telse:\n\t\t\tparams = (None,) * len(param)\n\telse:\n\t\tparams = (None,) * len(many_params)\n\tsql_params = out_style or params\n\tin_style = get_paramstyle(many_style, SQLParams)\n\tin_params = (param for param in params if in_style.get("}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tsql = re.sub(r'\\?\\s?', '&amp;', re.sub(r'\\?', r'&amp;', sql))\n\tsql += \"\\n\"\n\tresult = []\n\tparameter_indexes = []\n\tparameters = sqlparams()\n\t\n\tfor item in parameters[:]:\n\t\tresult.append(sql % item)\n\t\tparameter_indexes.append(parameter_index(sql, item))\n\t\n\tresult.append(sql)\n\tresult += [\"- %s\" % index for index"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\tif path == None or path == '':\n\t\treturn True\n\t\n\t# check required fields\n\tself.validateFile(path)\n\ttry:\n\t\tself.__root\n\texcept AttributeError:\n\t\treturn False\n\tif self.__root['type'] == 'pyfs':\n\t\tself.__root['type'] = 'osf'\n\telif self.__root['type'] == 'osf':\n\t\tself.__root['type'] = 'ocfl'\n\treturn True\n\ndef validateFileField(self, field):"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif self.validation:\n\t\tif self.validation(path,path,path,path):\n\t\t\tpaths = os.listdir(path)\n\t\t\tfor file in paths:\n\t\t\t\tif not(re.match('validation',file)):\n\t\t\t\t\tif os.path.splitext(file)[1] == OCFL.objname:\n\t\t\t\t\t\treturn True\n\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn True\n\t\ndef create(parent, path):\n\t\"\"\"\n\t\tCreate a new OCFL object and its parents\n\t\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif validate_path(path, \"/\").exists() == False:\n\t\treturn False\n\telse:\n\t\tself._lockers.add(path[\"ocfl_name\"])\n\t\treturn True\n\t\n\t\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif self.get_object(path):\n\t\treturn True\n\telse:\n\t\twarnings.warn(\"OCFL object %s not found in pyfs\" % path)\n\t\treturn False\n\ndef get_path_objects(os_name, path):\n\t'''\n\treturn path object path_list and check for files at path\n\t'''\n\tpath_list = (path if not path[0].count('/') else []\n\tfor path in os.listdir(path):\n\t\t# if path has \".pyfs\" and ends with \".pyf\","}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\twarnings = False\n\ttry:\n\t\tocfl = Path.find_at(path).ocfl\n\texcept Path.NotValidError:  # Not a valid OCFL path?\n\t\tprint 'WARNING: Invalid ocfl path at {0:s}'.format(path)\n\t\twarnings = True\n\texcept Exception as err:\n\t\t# Invalid ocfl path. Should be caught by PathError.\n\t\tex = err\n\t\twarnings = True\n\tfinally: \n\t\treturn warnings\n\ndef update_path(path, add, msg):\n\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\tif path!= 'default' and not os.path.exists(path):\n\t\treturn _('Path not found %s') % path\n\treturn True\n\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif path.exists():\n\t\ttry:\n\t\t\tos.remove(path.abspath)\n\t\t\treturn True\n\t\texcept:\n\t\t\tif not self.fs.exists(path):\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\ndef readdir(path):\n\t\"\"\"\n\tRead a directory, iterating over the list and return all the\n\tdirectory entries.\n\t\"\"\"\n\tdirs = []\n\tfor d in listdir(path):\n\t\td = os.path.join(path, d)\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.validate_dir(path)\n\texcept Exception as err:\n\t\tself.log.info('Warning occurred on validate()')\n\t\tself.log.info(err)\n\t\treturn False\n\treturn True\n\t\n\t\ndef ensure_obj_at(path, obj):\n\t\"\"\"\n\t\n\tValidate ocfl object path. \n\n\tValidates the provided object by attempting to fetch the object, falling off end\n\n\tReturns True if valid, True otherwise.\n\t\"\"\"\n\n\t# get object from oc fl root\n\tp_"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\ttry:\n\t\tfrom pyfs.util.logger import ConsoleWarnWrap\n\t\timport trace\n\t\tfrom pyfscache.osdlib import write_log, log_error\n\t\t\n\t\tos = self.osd\n\t\tfs = osdlib.Fs\n\n\t\t# Get a context and a file, open, and log a warning.\n\t\tos_fsctx = osdlib.FileSystemContext(os.get_file_system())\n\t\tos_path = fs.get_osd_dir()\n\t\tctx = Context(Context.SESSION,"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\ttry:\n\t\t# Validate the root\n\t\tvalid_root = self.verify_valid_root(path)\n\t\t\n\t\t# Sanity-catch-up\n\t\tself.path = path\n\t\ttry:\n\t\t\tself.fobj = self.fobj.__open()\n\t\texcept IOError:\n\t\t\treturn True\n\t\ttry:\n\t\t\tself.tree = self.fobj.read()\n\t\texcept IOError:\n\t\t\t# Maybe it's a pyfs file\n\t\t\treturn True\n\t\ttry:\n\t\t\tself.tree = decode(self.tree,"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif not self in self.statusMap.list:\n\t\traise Exception(\"Invalid status name: %s\"%prefix)\n\t\treturn \"%s not found in %s.%s\"%(prefix, self.getName(), self.getStatus())\n\t\t\n\treturn self.statusMap.list[self](prefix)\n\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstatus_str = \"\"\n\tif self.status.state or (self.status.state!= ''):\n\t\tstatus_str = \"%s (%s)\" % (prefix, self.status.state)\n\treturn status_str\n\n# def validate(self):\n# \t\"\"\"\n# \t\tvalidate the config for the given config_dict\n# \t\"\"\"\n# \tpass\n\nclass MyServer(zope.interface.interfaces.IRestrictedServer):\n\t\"\"\"\n\t@inherits IRestrictedServer\n\t\"\"\"\n\tdef _restrict"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn str( self.status ) if self.status is not None else \"\"\n\n\nclass ValidatorStatus(Status):\n\n\t\"\"\"\n\tStatus class for validator.\n\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\tkwargs.update( Status.__init__.__dict__ )\n\t\tsuper(ValidatorStatus, self).__init__(**kwargs)\n\n\nValidatorStatus.validator_type = ('key', 'type', 'group', 'type_all')\n\ndef validate_key(validator, key):\n\t\"\"\"\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.error:\n\t\treturn 'Error'\n\telif self.warning:\n\t\treturn 'Warning'\n\telif self.info:\n\t\treturn ''\n\telif self.passed:\n\t\treturn 'Passed'\n\telse:\n\t\treturn 'Failed'\n\nclass FormError(Exception):\n\t\"\"\"\n\tBase class for errors in forms.\n\t\"\"\"\n\tdef __init__(self, text='', validators=None, format=None):\n\t\tsuper(FormError, self).__init__(text)\n\t\tself._valid"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn''.join(self.statuses)\n\t\n\t\nclass ValidatorManager(object):\n\n\tdef __init__(self):\n\t\t\"\"\"Constructor\"\"\"\n\t\tself.status = None\n\t\treturn\n\tdef start(self, status_str, timeout_mins=180):\n\t\treturn\n\t\n\tdef read_status_str(self):\n\t\t\"\"\"\n\t\tRead validator status as a string.\n\t\t\"\"\"\n\t\treturn self.status_str()\n\t\t\n\t\t\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn ('%s%s:'%(prefix, self.name))%(self.status_str())\n\n\nclass validator(object):\n\tdef __init__(self, name):\n\t\tself.__object__ = None\n\t\tself.__methods__ = None\n\t\tself.name = name\n\n\tdef __getattr__(self, item):\n\t\treturn getattr(self.__object__, item)\n\n\tdef __getattr__(self, item, default=None):\n\t\treturn getattr(self.__object__, item, default)\n\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn str(' ').join(['ok'] if self.value > 0 else ['warning', 'error']) +'' + prefix +'' + str(self.value)\n\t\t\nif __name__ == '__main__':\n\n\twith open('user.txt', 'r') as f:\n\t\tlines = f.readlines()\n\n\tfor line in lines:\n\t\tprint(parse(''.join(line)))\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tfor code in self:\n\t\tif code.status == 'ok':\n\t\t\tmsg = code.label\n\t\telif code.status=='fail':\n\t\t\tmsg = code.value\n\t\t\tif code.prefix:\n\t\t\t\tmsg = \"%s [%s%%]\" % (prefix, msg)\n\t\telse:\n\t\t\tmsg = code.status\n\t\treturn \"%s: %s\" % (prefix, msg)\n\nclass Subclass(object):\n\t\"\"\"\n\tA dummy subclass used to define the behavior of a regular attribute\n\tbased on some custom attribute\n\t\"\"\"\n\tdef"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tret = \"\"\n\tif (self.valid):\n\t\tret = \"Valid\"\n\telif (self.critical):\n\t\tret = \"Critical\"\n\telse:\n\t\traise ValueError(\"Don't know how to generate validator status string\")\n\treturn ret + \": \" + \"%s%s\" % (prefix, self.__attr)\n\n\nclass ValuatorError(Exception):\n\n\tdef __init__(self, e):\n\t\tself.err = e\n\nclass RegExError(Exception):\n#\tdef __init__(self, e):\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\n\tif self.running():\n\t\tif prefix:\n\t\t\treturn '%s> %s' % (prefix, self.status_str())\n\t\telse:\n\t\t\treturn '< %s' % self.status_str()\n\t\t\n\treturn '?'\n\ndef validates(function, *args):\n\treturn has_function(function, *args)\n\n"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tstatus_str = '%s%%n' % prefix\n\n\tfor stat in (0, 1):\n\t\tstatus_str += '%%n' % stat % self.status\n\t\tstat % (self.count, self.percent)\n\n\treturn status_str\n\nclass FakeClient(Client):\n\tdef __init__(self, c, *args, **kwargs):\n\t\tself.c = c\n\t\n\t\tself.errors = []\n\t\tfor msg, text in self.messages:\n\t\t\tif not msg.startswith('*'):\n\t\t\t\tself."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\n\tif self.status >= 0:\n\t\t# TODO: check if all messages except prefix can be output\n\t\tall_valid_strings = [prefix+msg for msg in self.messages if msg]\n\n\t\treturn ', '.join(all_valid_strings) \n\n\telse:\n\t\treturn self.__str__()\n\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif self.isValid():\n\t\tstatus = 'valid'\n\telif self.errors:\n\t\tstatus = 'errors'\n\telse:\n\t\tstatus = 'unready'\n\t\n\tif self.max_length:\n\t\tstatus = '[%s, max:%s] %s' % (str(self.min_length), status, status_str(prefix + status))\n\t\t\n\treturn status\n\nclass QueryError(ValidationError):\n\t\"\"\"\n\tError during validation. \n\tExceptions here are exceptions from validation, and can be raised\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tout = ''\n\t\n\tif self is None:\n\t\treturn out\n\tout += prefix + '<empty>'\n\t\n\tstatus = self.__dict__.get('status')\n\t\n\tif status is not None:\n\t\tif isinstance(status, _Validator):\n\t\t\tout += status.format()\n\t\telif isinstance(status, tuple):\n\t\t\tout +=''.join(sorted(status))\n\t\n\treturn out\n\ndef test_with_list_of_dict(self, other_validator=None, prefix=''"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn self.log.str_prefix(prefix)\n\ndef status_num(self, total):\n\treturn self.status_str(\"(%.3d) \" % total)\n\ndef status_file(self):\n\treturn self.status_str(\"File \")\n\ndef status_line(self):\n\treturn self.status_str(self.log.LINE)\n\n# ========================================================================\n# ==== Main ================================================================\n# ========================================================================\n\nclass Main(object):\n\n\tdef __init__(self, opts):\n\t\t# TOD"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\n\treturn prefix + '<' + '\\n'.join(map(str,\n\t\t\t[self.errors, self.warnings, self.successes, self.fatals])) + '\\n>'\n\t\n\t\ndef validate_module(module_name):\n\t\"\"\"\n\tCall validate on the configuration module specified\n\tin the module_name.\n\t\"\"\"\n\tconfiguration['module'].validate(module_name)\n\n\ndef validate_parameters(params):\n\t\"\"\" \n\tValidate parameter values based on configuration file or module\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn str(prefix) + \": validation: \"\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn self._status_str_('', prefix)\n\t\ndef is_valid_file_name_too_long(file_name, max_length=1024):\n\tif file_length(file_name) <= max_length:\n\t\treturn False\n\treturn True\n\t\ndef file_length(file_path):\n\t\"\"\"\n\t\tReturn byte sized file length\n\n\t\tArguments:\n\t\t- file_path: file path (string)\n\t\t\"\"\"\n\ttry:\n\t\twith open(file_path, 'r') as file:\n\t\t\tlength = os."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif self.status == 0:\n\t\treturn '[%sp] (error)' % prefix\n\telif self.status == 1:\n\t    return '[%sp] Successfully validated' % prefix\n\telse:\n\t    return '[%sp] (%s) (%d) (success)' % (prefix, self.status_str[self.status], self.status)\n\t\n###############################################################\n\nclass ScraperError(Exception):\n    pass\n\nclass Scraper(threading.Thread):\n    \"\"\"\n\t\"\"\"\n    def __init__(self,"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn \"ERROR: \"+prefix+\" : \".join([\"%s : %s\" %(str(key),self.getMsg(key)) for key in self.key_list])"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\ndef loadFromFile(filename):  # pylint: disable=unused-argument\n\t\"\"\"\n\tLoad a text file or text node with a given name into a new document element.\n\n\tReturns:\n\t    A document element, or None.\n\t\"\"\"\n\traise RuntimeError(\"Not implemented\")\n\n\ndef loadFromMarkup(markup):  # pylint: disable=unused-argument\n\t\"\"\"\n\tLoad a markup object into a new document element.\n\n\tReturns:\n\t    A document element, or None.\n\t\"\"\"\n\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tis_valid = super().is_valid(identifier)\n\n\t# Validate is any valid prefix in the identifier.\n\tis_valid = is_valid or self.get_numerator().validate(identifier)\n\n\t# Validate is any valid suffix in the identifier.\n\tis_valid = is_valid or self.get_denominator().validate(prefix=identifier)\n\n\t# Validate is any suffix in the suffixes of the identifier.\n\tis_valid = is_valid or [s.validate(suffix) for s in self"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef _validate_value(identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tValidate value by creating an empty string otherwise.\n\t\"\"\"\n\treturn \"\"\n\n\ndef _validate_property(path, identifier, value):  # pylint: disable=unused-argument\n\t\"\"\"\n\tValidate property by creating an empty string otherwise.\n\t\"\"\"\n\treturn \"\"\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef is_unique_identifier(identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tReturns True if identifier is a unique, non-repeated, non-repeating\n\tidentifier (including repeated strings like 'a' or 'a-b' or 'a-b-c')\n\t(i.e. a unique identifier is unique; it will not cause a duplicate\n\trepresentation of the same identifier).\n\n\tThis is the inverse of the is_valid function.\n\n\tArgs:\n\t\tidentifier (str)\n\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t# Identifier in this base implementation already has the right length, no need to check.\n\treturn True\n\n\ndef has_length_of_id(identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tReturn True if identifier has the right length, False otherwise.\n\t\"\"\"\n\t# Identifier in this base implementation already has the right length, no need to check.\n\treturn True\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\traise NotImplementedError()\n\n\t\n\t\n\t\n\n\n\nclass Uppercase(Lowercase):\n\n\t\"\"\"\n\tUppercase method, takes a string and returns the uppercase version\n\t\"\"\"\n\tdef is_valid(self, string):\n\t\n\t\treturn string.upper() == string#.lower\n\n\t\n    def to_string(self, input):\n\t\treturn u\"[%s]\" % input\n\n\n\t\n\t\t\n\n\n\t\n\n\n\n\n\nclass Lowercase(ucase):\n\n\t\"\"\"\n\tUppercase method, takes a string and"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tif identifier.startswith('http['):\n\t\treturn True\n\treturn False\n"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\tif not extract_spec_version:\n\t\tif self.spec_version is None or '*' in self.spec_version:\n\t\t\tversion_match = re.match(r\"^\\d+[\\.][%s](\\d+[.][%s]*)?$\" %\n\t\t                                  (self.name, self.version),\n\t\t                               self.spec_version)\n\t\t\tif not version_match:\n\t\t\t\tself.spec_version = None\n\telse:\n\t\tversion_match = re.match(r\"^\\d+["}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not inventory:\n\t\traise errors.AnsibleError(\"invalid inventory specifier: %s\" % self._raw_params[\"in\"])\n\t\t\n\tif inventory.name == self.name:\n\t\traise errors.AnsibleError(\"inventory name and name can't be the same\")\n\t\n\tvaultname_match = ['validate_%s' % x for x in [\"inventory\", self.name, self.type]]\n\t\n\tif any(inventory.name in x for x in vaultname_match) and self.value:\n\t\t# if"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\treturn len(self.config) > 1\n\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif extract_spec_version:\n\t\ttype = InventorySpecification.value_specification(self.type, inv, inventory)\n\t\tself.spec_version = InventorySpecification.extract_version(type)\n\telse:\n\t\t# No spec version\n\t\tself.spec_version = None\n\n\tif self.spec_version is None:\n\t\traise ValueError(\"No specification version found\")\n\t\n\treturn inventory, self.spec_version\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\"\"\" \tCheck if there is a valid inventory.\"\"\"\n    if self.spec_version in (None, None, ''):\n        self.validate()\n        return \n\n    valid_version = self.spec_version.split('-')[-1]\n    # extract the specs first\n    try:\n\t    result = None\n\t    for k,v in self.__dict__['inventory']:\n\t        k_split = k.split('-')\n\t        if k_split[0] == extract_spec_version and self.spec"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\tresult = {'valid': True}\n\tif extract_spec_version:\n\t\tresult['valid'] = _validate_spec_version(self, build=False)\n\t\tresult['type_value'] = self.spec_version\n\treturn result\n\ndef _validate_spec_version(inventory_spec, build=False):\n\tif build:\n\t\tspec_version = inventory_spec.get('version')\n\telse:\n\t\tspec_version = inventory_spec.get('type')\n\t\n\tif not spec_version:\n\t\treturn False"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif(self.spec_version and self.type not in [\"inventory\", \"extract\", \"unknown\"]:\n\t\t\treturn [f\"{self.type} {self.spec_version}\"]\n\t\n\treturn [f\"{self.type} {str(self.spec_version or '')}\"]\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tinventory = self.validate_type(inventory)\n\tself.validate_type(extract_spec_version, True)\n\tself.spec_version = extract_spec_version\n\t\n\t\n\t\nclass InventoryBase(InventoryComponent):\n\t\"\"\"\n\tUsed for most container-agnostic tests.\n\t\"\"\"\n\tdef __init__(self, inventory, **kwargs):\n\t\t\"\"\"\n\t\tThis is the common class that all container-agnostic tests should\n\t\textract from.\n\t\t\"\"\"\n\t\tself.spec_version = False\n\t\tself"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not isinstance(inventory, Inventory):\n\t\tself.fail(\"Given %s does not appear to be an.Inventory; you must construct this object from another.\" % repr(inventory))\n\telif extract_spec_version is False:\n\t\tpass\n\telif extract_spec_version:\n\t\tversion_spec = self.inspect.get_spec_version()\n\t\tif version_spec is None:\n\t\t\tself.fail(\"Inspect.get_spec_version doesn't return a valid version information.\")\n\t\telse:\n\t\t\tvalidate_type = True\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tself.is_valid = False\n\tself.spec_version = '<unknown>'\n\tself.type = None\n\tself.spec_version = None\n\tspec_version, self.type = determine_inventory_type(inventory, extract_spec_version=extract_spec_version)\n\tif self.type and self.spec_version:\n\t\tself.is_valid = True\n\t\tself.spec_version = get_inventory_version(inventory, self.type, self.spec_version, extract_spec_version)\n\treturn self.is_"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tmissing = []\n\tall_digests = []\n\tfor manifest_file in manifest_files:\n\t\tif not os.path.exists(manifest_file):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tdigest_list = manifestutil.Manifest.parse(manifest_file)\n\t\texcept manifestutil.ManifestException, e:\n\t\t\traise Exception, (\"File %s failed to parse: %s\" % (manifest_file.replace('xml:',''), str(e)))\n\t\tfor manifest_file, content in digests_used.iteritems():\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor mf in sorted(manifest_files):\n\t\tmanifest_file = os.path.join(mf, dns_manifest_header)\n\t\tmanifest = open(manifest_file)\n\t\tdigests_used_paths = set()\n\t\tfor line in manifest:\n\t\t\tline_obj = line.strip().split(',')\n\t\t\tif len(line_obj)!= 2:\n\t\t\t\tself.module.fail_json(msg=\"Error parsing manifest line: \"\n\t\t\t\t\t\t+ line_obj[0].strip())\n\n\t\t\tdigest_name_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n\tif manifest_files == None:\n\t\treturn\n\t\n\tavailable_checks = []\n\tfor manifest in self.manifests:\n\t\tfor digest in manifest['digests']:\n\t\t\tdigests_used.add(digest)\n\t\t\tavailable_checks.append(digest)\n\t\t\n\tfor manifest in self.manifests:\n\t\tif len(manifest['digests']) > 0:\n\t\t\tbreak\n\t\t\n\tfor manifest in self.manifests:\n\t\t# check if any of the checks in this manifest are present in this manifest\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tlog('manifest file : %s' % manifest_file)\n\t\tlog('checking digests present and used')\n\t\tdigests_file = manifest_file + '.digest'\n\t\tdigests_present = None\n\t\tdigests_used.append(digests_file)\n\n\t\tdigests_result = (None if str(digets_file) == digests_file else digest_result)\n\t\tif digests_result:\n\t\t\tlog('digests exist in file : %s' % digests_result)\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tnew_digests_used = []\n\tfor manifest in manifest_files:\n\t\tprint \"checking %s for digests present and used.\" % manifest\n\t\tfor digest in digests_used:\n\t\t\tif digest in manifest.digests.keys():\n\t\t\t\tnew_needed = True\n\t\t\t\tnew_digests_used.append(digest)\n\t\t\t\tbreak\n\t\tif not new_needed:\n\t\t\tbreak # found one that doesn't need it, so break\n\n\tif len(new_digests_used) > 0:\n\t\tprint \"all digests required by %s are"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tif not os.path.isfile(self.get_digests_path()):\n\t\treturn\n\tfor mf in manifest_files:\n\t\t# TODO: Make this more generic. (manifests can contain only one manifest)?\n\t\t# TODO: This should be removed?\n\t\tif not os.path.isfile(mf)[0]:\n\t\t\tcontinue\n\t\treturn\n\tif digests_used is None:\n\t\treturn # no digests were used\n\tfor dig in digests_used:\n\t\tif not os.path.isfile(dig)"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor f in manifest_files:\n\t\tis_present, is_used = self.manifest.load_file(f)\n\t\tif is_used and not (is_present and not digests_used.has_item(f)):\n\t\t\tprint(\"%s is used but not present; ignoring file %s\" % (is_used, f))\n\n\treturn len(manifest_files) - len(digests_used)"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tneeded = {}\n\tused = {}\n\tused_digests = {}\n\t# Check all digests in manifest to see if they are used or not.\n\tfor f in manifest_files:\n\t\tfor (digests_used_path, digests_used_names) in digests_used:\n\t\t\tused_digests = used_digests.get(digests_used_names, [])\n\t\t\tif f in used_digests:\n\t\t\t\tused.setdefault(digests_used_names, []).append(f)\n\t\t\telse:\n\t\t\t\tneeded."}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tdigests_present = []\n\tdigests = []\n\tmanifest_file_count = 0\n\tfor manifest in manifest_files:\n\t\tfor d in manifest.data.replace(\"\\n\", \"\").split(\"\\n\"):\n\t\t\tmatch = re.search(reMatchDigestPresent, d)\n\t\t\tif match:\n\t\t\t\tdigests_present.append(match.group(1))\n\t\t\t\tmanifest_file_count += 1\n\t\tif manifest_file_count == len(manifest_files):\n\t\t\tprint \"found all digests that are present and used"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\twith open(manifest_file, 'r') as f:\n\t\t\tcontent = f.read()\n\t\tdigest_file = manifest_file + \".digest\"\n\t\tif os.path.exists(digest_file):\n\t\t\tself.debug(\"[DIGEST] %s exists\", digest_file)\n\n\t\tif os.path.exists(digest_file) and not os.path.islink(digest_file):\n\t\t\tself.debug(\"[DIGEST] %s exists, is directory"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tif self.prior is None or not prior.is_valid(self):\n\t\tself.error_msg = \"Invalid prior version\"\n\t\traise InvalidPriorValue(self.error_msg) \n\n\t\n\nclass InventoryValidator(object):\n\tinv_version = None\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\t\tAdd your own self validation logic here\n\t\t\"\"\"\n\t\tpass\n\t\t\n\t\ndef add_validator_for_inventory(validator):\n\tvalidators = InventoryValidator().validator_set\n\tvalidators"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tdef check_nonnull_and_invalid_list(self, list_, item):\n\t\tif not list_:\n\t\t\treturn False, 'The list must be non-null and an empty list when the prior is given'\n\t\tif not len(list_) > 1:\n\t\t\treturn False, 'The specified list has too many elements:' + str(list_)\n\t\tfor each2 in list_:\n\t\t\tif isinstance(each2, list):\n\t\t\t\tif not is_equal(each2, item):\n\t\t\t\t\treturn False, 'The specified list item must be an"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t# First check to make sure the prior is the same\n\t#   (as a prior version and not the previous one)\n\tif prior.__class__ is not self.__class__:\n\t\traise errors.InvalidPrimaryVersionError(\"Previous version must be a \"\n\t\t\t\"valid version of this object\")\n\tif not isinstance(prior, InventoryObject):\n\t\traise errors.InvalidPrimaryVersionError(\"A prior version of this object \"\n\t\t\t\"is required\")\n\treturn True\n\n# Check and raise an error if the prior version has no children\n# The prior's"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif isinstance(prior, str):\n\t\tif prior in self.categories:\n\t\t\treturn True\n\t\telse:\n\t\n\t\t\tif self.categories.has_key(prior):\n\t\t\t\tif self.prior.get_val == prior:\n\t\t\t\t\treturn True\n\t\t\t\telse:\n\t\t\t    return False\n\t\t\telse:\n\t\t\t    return False\n\t\t\n\telif isinstance(prior, tuple):\n\n\t\tif prior[0] == int(prior[0]) and prior[1] == int(prior[1]):\n\t\t\treturn True\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t# Ensure that the prior is actually an InventoryValidator object.\n\tself._compare_validators(prior, self.inventory)\n\t\n\t# Compare the current inventories:\n\tself.inventory.check_valid()\n\tprior.inventory.check_valid()\n\t\n\t# Ensure any of the invant's internal consistency:\n\tself.inventory.check_valid()\n\tprior.inventory.check_valid()\n\t\n\t# Raise an Exception if self has a more recent inventory:\n\tif self.inventory.last_modified > prior.inventory"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not isinstance(prior, InventoryValidator):\n\t\traise TypeError('prior must be an InventoryValidator object!')\n\tif prior.is_valid_version(self.invs[INV_VER]):\n\t\treturn prior\n\traise TypeError('prior is not a valid prior version of this object!')\n\nclass InventoryConstraints(object):\n\t\"\"\"\n\tA class that is returned as a tuple containing the type of\n\tthe inventory constraints for the object.\n\t\n\tThe object returned should hold the following attributes:\n\t\n\tcon"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tself.assertIsInstance(prior, InventoryValidator)\n\tself.assertIsInstance(prior.inventory, Inventory)\n\n\treturn self.inventory == prior.inventory\n\ndef checkForOutdatedCoresVariant(**kwargs):\n\t\"\"\"\n\tCheck that a CoreVariant's is not updated when the version\n\tdiff between two CoreVariants is less than a certain value.\n\t\n\tCheck that a CoreVariant of the same name already exists in \n\tan Inventory but the given CoreVariant's has a different \n\tversion (and if"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not prior.included in (True, False):\n\t\tself.add_error(fieldname=self.included, value=prior.included,\n\t\t\t\texpected=self._included_repr)\n\n\t\t# Prior has been checked that is not a valid prior version of any\n\t\t# InventoryValidator\n\t\treturn False\n\t\n\tif prior.get_inventory_type() == 'included':\n\t\tself.validate(prior, prior)\n\t\n\telif prior.get_inventory_type() =='sub': # Sub\n\t\tself."}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif prior is not None:\n\t\tif isinstance(prior, InventoryValidator.InventoryValidator):\n\t\t\tself.fail(\"Validators can not be a prior version of InventoryValidator\")\n\t\n\tif prior is None:\n\t\treturn\n\t\t\n\tdef ensure_valid(self):\n\t\t\"\"\"\n\t\t\tEnsure prior is a valid prior version of that inventory object.\n\t\t\"\"\"\n\t\tif not can_inventories_have_version(self, prior):\n\t\t\tself.fail(\"Prior versions are not compatible with inventories. \"\n\t\t\t\t\"Currently can only validate"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tdef _validate_as_prior_version_ok(self, prior):  # pylint: disable=unused-parameter\n\t\t\"\"\"\n\t\tValidate as prior.  It is assumed that prior is assumed to be a version\n\t\tof the current inventory object.\n\t\n\t\tThis method checks for all requirements imposed by previous\n\t\tversions of the inventory object.  See\n\t\t* `Inventory.__init__`.\n\t\n\t\tThe input must contain an InventoryValidator object and the \n\t\tsame self and prior inventories respectively.  See self._validate_"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tif not version is 'any':\n\t\tversion = version + '.0'\n\t\n\tlogical_path_map = {}\n\t\n\tfor log_item in inventory.get_logical_item_list('contents'):\n\t\t\n\t\tif log_item['version'] == version:\n\t\t\tlogical_path_map.update(log_item['path_in_state'].get_path_map(inventory))\n\t\telse:\n\t\t\tlogical_path_map.update(get_logical_path_map(inventory, log_item['version'"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\tstate = {str(version): collections.OrderedDict()}\n\t\n\tfor _, resource in inventory.resources().items():\n\t\tfor version_index, version in enumerate(resource.logical_versions()):\n\t\t\tif version <= version_index:\n\t\t\t\tstate[version] = collections.OrderedDict()\n\tfor version in state.keys():\n\t\tlogical_path_in_state = collections.OrderedDict()\n\t\tfor file_in_state in state[version]:\n\t\t\tlogical_path = str(os.path."}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\treturn inventory.get_logical_path_map(version)\n\t\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\tlog_path_map = {}\n\t\n\ttry:\n\t\tfor file in os.listdir(inventory.configdir):\n\t\t\t# Get file path under the inventory config directory and ignore\n\t\t\t# it in state listings for consistency with other inventory\n\t\t\t# managers based on this inventory.\n\t\t\tfilename = os.path.join(inventory.configdir, file)\n\t\t\tfile_content = os.path.join(filename, file)\n\t\t\t\n\t\t\tif os.path.isfile(file_content):\n\t\t\t\t# Load the file and check if the version is in the inventory"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor version in versions(inventory, version):\n\t\tif version == version:\n\t\t\t# use the state directly as we have that\n\t\t\tlogical_path_map = dict(inventory.states[version])\n\t\telse:\n\t\t\t# get all state links in inventory from the logical_path_map\n\t\t\t# of the previous version\n\t\t\tlogical_path_map[version] = dict([(state, links) for (state, links) in inventory.states[version].items()])\n\treturn logical_path_map\n\ndef"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\tlogical_paths = {}\n\t\n\tinventory_content_files = set(\n\t\tos.path.realpath(f)\n\t\tfor f in listdir(inventory)\n\t\tif is_content_directory(os.path.realpath(f)))\n\t\n\tinventory_file_to_content_files = {}\n\tfor f in listdir(inventory):\n\t\tif is_content_directory(os.path.realpath(f),\n\t\t\t\t\tinclude_dot=True):\n\t\t\tinventory_file_to_content_files[f] = set()"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = collections.defaultdict(set)\n\t\n\tfor inventory_file in inventory:\n\t\tlogical_path_map[inventory_file].add(version)\n\t\n\treturn logical_path_map\n\t\ndef get_state_name_map(inventory, version):\n\t\"\"\"\n\tGet a map of state name to file names with that name in version\n\tinventory.\n\t\n\tReturns a dictionary: state_name_in_version -> content_files\n\n\tThis is useful for comparing content_files vs. file contents between\n\tdifferent"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\treturn _get_logical_path_map(inventory, version.get_version().get_file(),\n\t\t\t\t\t version.get_location().get_file())\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t# If there is no current version, get the version from the inventory directly.\n\tif not version:\n\t\tlogger.debug(\"No current version, get the direct version.\")\n\t\tlogical_path_in_state = {}\n\t\tfiles_from_package = Inventory().get_files(version)\n\t\tfor filename in files_from_package:\n\t\t\tif filename not in inventory.files:\n\t\t\t\tlogger.debug(\"Directory not yet found.\")\n\t\t\t\tlogical_path_in_state[filename] = Inventory().get_files(version)[filename]\n\t\tlog"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\treturn {\n\t\t\tversion: set( [\n\t\t\t\t\tfn for fn in os.listdir(inventory) if fn.startswith('state-') and fn.endswith('.json')\n\t\t\t] ),\n\t\t\t}\n"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif 'FIXITY_' not in fixity or not '%s('.join(fixity_types) in fixity:\n\t\terrorMsg = \"\\n\\t\\tNo FIXITY_%s('.*' in manifest files)\\n\\t\\tFalling Back\\n\\n\" % fixity\n\t\terrorMsg += \"\\tFixity block:\\n\\t\\t%s\\n\\n\" % fixity\n\t\traise Dependency_Error(errorMsg)\n\nclass Dependency_Error(Exception):\n\t\"\"\"\n\tClass representing Dependency_Error errors"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t# Check the file exists - this will not throw a ValueError for a path not in\n\t# the manifest, but if files are missing.  This will not throw ValueError\n\t# for a path in the manifest that does not exist.\n\tif not os.path.isfile(os.path.join(fixity, \"binaries.txt\")):\n\t\traise ValueError(\"Cannot find binaries.txt in 'fixity/binaries.txt'\")\n\t\n\tif manifest_files:\n\t\tfiles = manifest_files\n\telif not self._manifestfile:"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tentries = set()\n\t\n\tfor f in manifest_files:\n\t\tif (f in fixity.inventory) and not f in entries:\n\t\t\tentries.add(f)\n\t\n\tif entries:\n\t\tlogger.warning(u\"Fixity does not contain all your files. Please check your manifest, and the corresponding files.\")\n\t\n\nclass DbInvItem(object):\n\tdef __init__(self, ref_object, ref_id, ref_type, ref_name):\n\t\tself.ref = ref_object\n\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tvalid = True\n\tvalid_files = {}\n    #Check for common files\n    valid_files.update(\n    \tset(Manifest.files + fixy.files)\n    )\n\tfor manifest_file in valid_files:\n #       if manifest_file.name not in manifest_files:\n    \t    valid = False\n\t    break\n\n\tfor manifest_file in manifest_files:\n\t\tvalid_files.add(manifest_file.name)\n\t\tvalid_files.update(\n\t\t\tset(Manifest.files + [manifest_file"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif (os.path.isfile(fixity)):\n\t\treturn (True, True)\n\telse:\n\t    return (-1, False)\n\n\nclass FileFetcher(unittest.TextTestRunner):\n    \"\"\"\n    This class implements a unittest suite for FileFetcher\n    \"\"\"\n\nclass TestClass(unittest.TestCase):\n    \"\"\"\n\tUnit test class.\n\t\"\"\"\n    def setUp(self):\n        \"\"\"\n        Start up the fixture\n        \"\"\"\n        unittest.TestCase.setUp("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tinventories = {}\n\tmanifest = None\n\tfor manifest_file in manifest_files:\n\t\tinventories[manifest_file.name] = manifest_file.load_inventories()\n\t\n\tignore_files = set()\n\t\n\tcurrent_file = None\n\tfor manifest_file in manifest.all_files:\n\t\tif manifest_file.name in ignore_files:\n\t\t\tcontinue\n\t\t\n\t\tcurrent_file = manifest_file\n\t\tif manifest_file.name not in inventories:\n\t\t\tvalidate_block(manifest_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tfor manifest_file in manifest_files:\n\t\tif self.manifest_files.has_key(manifest_file):\n\t\t\tself.manifest_files[manifest_file].validate_fixity(fixity)\n\ndef read_manifest(full_name):\n\tmanifest = manifest_content.read(full_name)\n\treturn manifest\n\ndef write_manifest(manifest, full_name):\n\tfull_name = os.path.join(os.path.abspath('.'), full_name)\n\t\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tself.logger.info(\"checking fixity block\")\n\t\n\t# get list of mandatory files\n\tmandatory_files = set()\n\tfor f in manifest_files:\n\t\twith open(os.path.join(self.root, f), \"rt\") as fh:\n\t\t\tscan_tokens = list(fh.read().split(\"\\n\"))\n\t\t\tmandatory_files.add(scan_tokens[-1])\n\t\t\n\t# get list of optional files\n\toptional_files = set()\n\tfor f in mandatory_files"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfrom nltk.sem import sem\n\t\n\tfixity_words = sem.lemmas(fixity)\n\tfixity_fractions = [selem['fraction'] for selem in fixity_words]\n\tfixity_list = []\n\tfor word, fraction in zip(fixity_words, fixity_fractions):\n\t\t\n\t\tif word in manifest_files:\n\t\t\tfixity_list.append({\n\t\t\t\t'fixity': word,\n\t\t\t\t'fraction': fraction,\n\t\t\t})\n\t\telif (word in self.fixity"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t# Check that the list of manifest files is complete.\n\tfor manifest in manifest_files:\n\t\tif (not manifest.find('=') > 0 and (not manifest.find(':') > 0)):\n\t\t\tself.debug(manifest)\n\t\t\tprint \"Can't find the '=' or ':' in the fixity block.\"\n\t\t\tcontinue\n\t\t\t\n\t\t# Check that the manifest contains only files in the path.\n\t\tfor submanifest in manifest.split('=',1):\n\t\t\tsubpath = submanifest.split(\"/\",1)\n\t\t\tif ("}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tpath = os.pathsep.join((path, ))\n\tfiles = glob.glob(path)\n\tfiles = [x for x in files if os.path.basename(x).endswith('.txt')]\n\treturn files[:10]\n\n\nfiles_map = {}\n\ndef get_tags(tag):\n\ttags = ''\n\tfor line in file(tag).readlines():\n\t\ttags = tags + line.strip()\n\treturn tags\n\n\n\n\n\ndef get_tag_content(tag):\n\tcontent = ''\n\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfiles = glob.glob(os.path.join(path, \"*.xml\"))+glob.glob(os.path.join(path, \"*.pdf\"))+glob.glob(os.path.join(path, \"*.ppt\"))\n\treturn files\n\n\ndef html_to_rst(htmlDir, source=\"\", **kw):\n\t\"\"\"\n\t\tTakes the htmlDir from a source directory (for `get_file_from_path`), converts it to a restructured text using xml, pdf, and html pages.\n\t\tReturns a list"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tif not isinstance(path, str):\n\t\traise ValueError('path must be a string')\n\tfiles = [File(filename) for filename in os.listdir(path) if filename.endswith('.gz')]\n\tfiles.sort()\n\treturn files_list(path, False)\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\n\t## Get the list of files\n\tfile_list = glob.glob(path)\n\t\n\t## Sort it\n\tfile_list = sorted(file_list)\n\t\t\n\t## Return the list\n\treturn file_list\n\n\n\n## Get the number of times the program has been run\nrun = 0\n\n## Check for errors\n\n## If any errors were encountered\nif os.path.exists('stdout'):\n\tprint \"ERROR O_O\"\nelif os.path.exists('stderr'):\n\tprint \"ERROR"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn sorted(iglob(os.path.join(path, f)))\n\ndef load_log(log_path):\n\"\"\"\n\tLoad the logfile and all lines\n\t\"\"\"\n\tlog = open(os.path.join(log_path, 'log'), 'r')\n\tlines = log.readlines()\n\tlog.close()\n\treturn lines\n\t\ndef create_logfile(new_path, log_name=None):\n\t\"\"\"\n\t\tCreates a logfile in `full_path`\n\t\tArgs:\n\t\t\tnew_path(str"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfiles = []\n\tfiles.extend(File(f, path).files)\n\treturn files\n\n\n# ============================ END ==============\n\n\ndef start(filename=''):\n\t\"\"\"\n\t\tReturns the full path to `file`\n\t\t`filename` is optional and defaults to the entire path\n\t\"\"\"\n\tif os.path.isfile(file):\n\t\treturn filename\n\telse:\n\t\treturn os.path.join(base_path, filename)\n\n\ndef build_graphical_map(filename=''):\n\t\"\"\"\n\tBuild the graphical map"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\timport os\n\treturn filter(lambda x: len(x) > 0, os.listdir(path))\n\ndef files_in_dir(path):\n\t\"Return the files contained in `path` directly beneath `path`\"\n\timport os\n\tpath_list = list(filter(lambda x: x.lower()!= \"\", os.listdir(path)))\n\treturn path_list\n\ndef dir_size(path):\n\timport os\n\ts = 0\n\tfor root, dirnames, filenames in os.walk(path):\n\t\tcount"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tf = open(path,'rU')\n\treturn f.readlines()\n\n\ndef create(path):\n\t\"\"\"\n\tCreates a new file in a given path\n\t\"\"\"\n\tf = open(path,'w')\n\tf.write(random.randrange(20))\n\tf.close()\n\n\ndef update(path):\n\t\"\"\"\n\tCreates a new file in a given path\n\t\"\"\"\n\tf = open(path,'r')\n\ta = f.readlines()\n\tfor i in range(len(a)):\n\t\tif"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn filter(lambda(p): os.path.isfile(p),(os.listdir(path)))\n\n\n\ndef check_files(path):\n\t\"\"\"\n\tReturns the list of files in `path`\n\t\"\"\"\n\treturn filter(lambda(p): os.path.isfile(path),(os.listdir(path)))\n\ndef check_dir(path):\n\t\"\"\"\n\tReturns the list of files in `path`\n\t\"\"\"\n\treturn filter(lambda(p): os.path.isdir(path),(os.listdir(path"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\twith open(path, \"r\") as fd :\n\n\t\tlines = fd.readlines()\n\n\t\tfiles = []\n\t\n\t\tfor line in lines :\n\n\t\t\tif line.startswith(\"#\") :\n\n\t\t\t\tfiles.append(line[1:].split(\" \")[1])\n\n\t\treturn files\n\n\n\t"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\treturn _list_group(files, lambda x: os.path.basename(x), xmls)\n\t\ndef _zip_file_file_list(source, xmls, files):\n\t\"\"\"\n\tzip-ify list of files in the folder or zipfile\n\t\n\tGroups files by their XML basename\n\t\n\tParameters\n\t----------\n\ttarget : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tzip_files : list\n\t    "}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tif isinstance(files, str):\n\t\tfiles = [files]\n\n\tfiles_dir = files[0].split('/')\n\n\tif len(files_dir)!= 2:\n\t\treturn None\n\t\n\t# get directory where the file lives\n\tfiles_dir = files_dir[1].split(os.path.sep)\n\t\n\tfile_path = os.path.join(source, os.path.sep.join(files_dir))\n\t\n\tfile_group = None\n\ttry:\n\t\twith zipfile.Zip"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tres = {}\n\tfor name, file in files.iteritems():\n\t\tres[name] = (file.filename, file.xml_basename)\n\t\n\treturn res\n\t\ndef _split_list_by_files_path(files, file_path, file_prefix = None, separator = '', sep_string ='', ignore_prefix = False):\n\t\n\tif separator not in [' ', '\\\\', ';', ';']:\n\t\traise ValueError('Separator must be a single string or \\nseparator= \""}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tret = {}\n\tfor f in files:\n\t\tfull_filename = os.path.join(source, f)\n\t\txml_name = os.path.basename(f)\n\t\tif full_filename in ret:\n\t\t\tret[0][xml_name] += \"\"\" - \"\"\"\"+xml_name+\"\"\"'\n\t\telse:\n\t\t\tret[0] = {\"filename\":xml_name\"}\n\t\tret[0][f] = full_filename\n\treturn ret\n\ndef _group_files_by_file_name(source, files):\n\tret ="}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tpackage = {}\n\tfor file in files:\n\t\tfilename = os.path.basename(file)\n\t\tbasename, file_extension = os.path.splitext(filename)\n\t\tdirname = os.path.dirname(file)\n\t\tfor base, dirs in os.walk(os.path.join(dirname, \"src\")):\n\t\t\tfor name in dirs:\n\t\t\t\tfilename_to_parse = os.path.join(base, name)\n\t\t\t\tif file_extension == \".xml\":\n\t\t\t\t\tfile_basename, file_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\txmls = [os.path.expanduser(f) for f in xmls if os.path.exists(f)]\n\n\tfiles = [os.path.expanduser(f) for f in files if os.path.exists(f)]\n\t\n\tfilename = xmls[0]\n\tfilelist = [f for f in files if f.endswith('.xml')]\n\n\tif not filelist:\n\t\treturn {\"files0\": files, \"group_by_xml\": True}\n\n\txmls = [os.path."}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tpkg = collections.defaultdict(list)\n\tfor fid in files:\n\t\tpkg[xmls.index(fid.filename)] = fid\n\tpck_name = []\n\tfor v in pkg:\n\t\tpck_name.append(v.name)\n\t\n\tpkg_dict = {}\n\tpkg_dict.update(pck_name)\n\t\n\treturn pkg_dict\n\ndef _get_subdir_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tReturns a subdirectory based on XML"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txml_files = sorted(glob(os.path.join(source, \"xmls\", \"%s\" % (f)) for f in xmls))\n\tfiles = [{\"filename\": fn, \"package\": get_package_by_name(package, xml=True)} for fn, package in zip(files, xmls)]\n\treturn dict((f[:-4] for f in xml_files), files=files)\t\n\t\n\t\ndef search_url(request, url, method, data, files, **params):\n\t\"\"\"\n\tSearches the search"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t#group by XML basename\n\tgroup_files = dict()\n\tgroup_files['xml_filename'] = str(xml_filename)\n\tfor f in files:\n\t\tgroup_files[f.fn_name] = ''\n\treturn group_files\n\n\ndef _group_files_by_name(source, files):\n\t\"\"\"\n\tGroup files by their file name\n\t\n\tGroups files by their name and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfile_name : str\n\t    file name from which XML are grouped."}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tdef to_dict(path):\n\t    with open(path, 'r') as xml:\n\t        return {name: dict.fromkeys(files) for name in [path] + files}\n\n\t\n\timport xml.etree\n\n\troot = xml.etree.ElementTree.ElementTree(file=xmls)\n\n\tfiles = []\n\tfor files in files:\n\t\tfiles = list(map(to_dict, files))\n\t\t\n\t\tif any(files):\n\t\t\tname = os.path.basename(files[0])\n\t\t\t\n"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tp = ''\n\tfor c in file_path:\n\t\tif c == '-':\n\t\t\treturn p\n\t\tif p == '':\n\t\t\tp = c\n\t\telse:\n\t\t\tp += c\n\treturn False"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tbase_path = os.path.dirname(file_path)\n\tmatching_paths = re.findall(\"%s\" % prefix, base_path)\n\tfor path in matching_paths:\n\t\tpath = path.replace(\"%s\" % prefix, \"/\".join(base_path[:-len(prefix)].split(os.path.sep)))\n\t\tsearch_path = os.path.sep.join([os.path.dirname(path), file_path])\n\t\tif os.path.exists(search_path):"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn os.path.commonprefix([prefix, file_path])!= prefix\n\n\ndef split_directory(possibles, path, exclude=[]):\n\t\"\"\"\n\tSplit the entries in the `possibles` directory into a list.\n\t\n\tParameters\n\t----------\n\tpossibles : list\n\t\tA list containing the path separated by the `path`\n\tpath : str\n\t\tThe root directory\n\texclude : list\n\t\tA list of directory filenames that should NOT be split\n\t\"\"\"\n\tret = []\n\tret.append(path)\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tpackage = package_from_prefix(prefix)\n\treturn package in package_file(file_path)\n\n\ndef add_dynamic_variable(prefix, name, value):\n\t\"\"\"\n\t\tAdd a dynamic variable.\n\n\t\tParameters\n\t\t----------\n\t\tprefix : str\n\t\t    Name of the dynamic variable\n\n\t\tname : str\n\t\t    Name of the value\n\n\t\tvalue : str\n\t\t    Value of the variable\n\n\t\tReturns\n\t\t-------\n\t\tbool\n\t\t    True = add the variable\n\t\t\"\"\"\n\ttry:\n\t\tval_dynamic = dynamic_vars"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tfor pkg in doc_list:\n\t\tif pkg.filename.startswith(prefix):\n\t\t\treturn True\n\t\t\t\t\n\treturn False\n\n\ndef match_file(file_path):\n\t\"\"\"\n\tIdentify if a `file_path` belongs to a document\n\t\n\tRetorna `True` para documento n\u00e3o correspondido a um pacote.\n\t\n\tParameters\n\t----------\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file corresponda ao pacote\n\t\"\"\"\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tpkg_dir = os.path.join(os.path.dirname(prefix), 'public')\n\tfor _file in os.listdir(pkg_dir):\n\t\tcurr_file = os.path.join(pkg_dir, _file)\n\t\tif _file.startswith(prefix):\n\t\t\tif os.path.isfile(os.path.join(curr_file,'index.py')):\n\t\t\t\treturn True\n\t\t\tcontinue\n\t\telif os.path.isdir(curr_file):\n\t\t\tmatch_file = match_file_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif not prefix:\n\t\treturn False\n\tfor i in range(1,len(file_path)):\n\t\tp = r'.+?/(?P<pkg>.+?)(\\..+?)(\\..+?)'\n\t\tm = re.match(p,file_path)\n\t\tif m:\n\t\t\tp1 = m.group('pkg')+'/'\n\t\t\tpkg = os.path.basename(prefix.split('/')[-1])\n\t\t\treturn pkg in file_path.split('/')[-1]\n\t\tm ="}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn file_path.startswith(prefix)\n\n\ndef clean_dir(directory_path):\n\t\"\"\"\n\n\tTodo:\n\t- Implement\n\t\n\t\"\"\"\n\ndef read_file(file_path, delimiter = \"\", file_name_ext = \".xlsx\"):\n\t\"\"\"\n\n\tTodo:\n\t- implement\n\t\n\t\"\"\"\n\n# import matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\ndelimiter=rcParams['axes.labelsize'] + '"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tprefix = prefix.replace(' ', '')\n\tif not glob.glob(os.path.join(prefix, \"*.txt\")):\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef generate_dir(prefix, files):\n\t\"\"\"\n\t\tGenerate a directory by specifying prefix and files.\n\t\t\n\t\tThe ``prefix`` argument is required to specify a prefix directory.\n\t\tThe ``files`` argument is optional.  If it is provided, only the sub-folders\n\t\tin ``prefix`` are also generated.  If the input is not a prefix\n\t\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tif os.path.isabs(file_path):\n\t\tfile_path = '/'.join([file_path[0], file_path[2:]])\n\treturn prefix.endswith(file_path)\n\n# Import all modules\nfrom modules import extract\nfrom modules import format\nfrom modules import parse\nfrom modules import test\nfrom modules import utils\nfrom modules import versioner\nfrom modules import rfc2445\nfrom modules import rfc2394\nfrom modules import rfc822\nfrom modules import utils as ut\n\n# Import package\n\npackage"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\tresult = []\n\tfor f in files:\n\t\tif f.endswith(prefix):\n\t\t\tresult.append(f)\n\treturn result\n\ndef select_filenames(path, files):\n\t\"\"\"\n\tRetorna os arquivos que correspondentam os nomes de `files` dicionado\n\tas listas `files`, espec\u00edficando os nomes diferentes\n\t\"\"\"\n\t\n\tresult = []\n\tfor f in files:\n\t\tif f.endswith(path):\n\t\t\tresult.append"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tbase = base_path()\n\tsearch = {}\n\tsub_files = {}\n\n\tfor suffix, name in sys.path:\n\t\tif name.startswith(base):\n\t\t\tsearch = suffix\n\n\tfor name in files:\n\t\tif name.startswith(search):\n\t\t\tsub_files[name] = files\n\n\tfor e in sub_files.keys():\n\t\tfiles = sub_files[e]\n\n\t\tif prefix in files:\n\t\t\treturn files\n\n\treturn list()\n\t#return files\n\ndef get_index_dir():\n\t\"\"\"\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t# Define a search for each filenames that have a prefix\n\n\tsearched_files = sorted(os.listdir(os.path.dirname(files)))\n\t\n\t# Iterate over all files which does not yet have a `prefix`\n\tfilenames_to_skip = []\n\tfor filename in files:\n\t\tif not filecmp.cmp(filename, prefix + filecmp.basename(filename)):\n\t\t\tfilenames_to_skip.append(filename)\n\n\tfor filename in filenames_to_skip:\n\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\tpaths = []\n\tfor f in files:\n\t\tif not f.lower().startswith(prefix.lower()):\n\t\t\tcontinue\n\n\t\tpaths.append(os.path.abspath(os.path.join(f, prefix)))\n\t\n\treturn [os.path.abspath(os.path.join(f, prefix)) for f in paths]"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tprefix = unicode(prefix)\n\tpath = \".\"\n\tfiles = list(files)\n\tfiles_files = set()\n\tbase_dir = str(prefix)\n\ttry:\n\t\tfor root, dirs, files in os.walk(str(prefix)):\n\t\t\tfiles = dict(files)\n\t\t\tfor curr in dirs:\n\t\t\t\tif str(curr).startswith(base_dir):\n\t\t\t\t\tif os.path.sep!= \"/\":\n\t\t\t\t\t\tcurr = \"/\".join(curr, os.path.sep)\n\t\t\t\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tbase_path = os.path.join(prefix, os.path.basename(files))\n\tif not os.access(base_path, os.R_OK) or not os.path.isdir(base_path):\n\t\treturn [path for path in files if not os.path.islink(path) and\n\t\t\tos.path.basename(path) == prefix]\n\t\t\n\treturn [os.path.dirname(path) for path in os.listdir(base_path)]\n\t\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn set(fnforprefix(s) for s in files.split(os.pathsep+'\\\\'))\n\n\ndef is_document(fn1, fn2):\n\t\"\"\" Verify that `fn1` is the same as `fn2` (or is a directory).\n\t\n\tParameters\n\t----------\n\tfn1 : str\n\t    `fn1`\n\tfn2 : str\n\t    `fn2`\n\tReturns\n\t-------\n\tbool\n\t    True if `fn1` matches to `fn2`, False otherwise\n\t\"\"\"\n\tpath"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tprefix = prefix.split('/*')[0].split('../')[0]\n\tfor f in files:\n\t\tf = f.split('/*')[0].split('/**')[0]\n\t\tif f.endswith(prefix):\n\t\t\treturn [f]\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t# Check for empty list of files\n\t# TODO check if there is a condition that needs to be fulfilled for the case of empty list \n\t# TODO also check if the prefix is different?\n\tif files == []:\n\t\treturn []\n\t\n\t# Filenames to select\n\tfiles = glob.glob(os.sep.join(files))\n\tif prefix!= '':\n\t\tfiles = filesfilterto(files, prefix)\n\t\t\n\treturn files"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\tfilenames = sorted(filter(lambda (f): prefix in f, files))\n\treturn filenames"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdata = {\"packages\" : []}\n\t\n\tfor dir_path in listdir(folder):\n\t\t\n\t\tif folder == directory + \"/\" or folder == directory + \"/../\":\n\t\t\tcontinue\n\t\t\n\t\tfile_name, ext, path = os.path.splitext(os.path.basename(dir_path))\n\t\t\n\t\tif path == \"\":\n\t\t\tcontinue\n\t\n\t\tcurrent_file_path = dir_path\n\t\tcurrent_file_path = current_file_path.replace(\"\\\\\", \"/\")\n\t\t\n\t\tif folder"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdirname = os.path.join(folder,os.path.basename(folder).split(os.getcwd())[-1])\t\n\t#print(\"Directory {}\".format(dirname))\n\t\n\tfiles = glob.glob(os.path.join(dirname, \"*.xml\"))\n\tfiles = [os.path.basename(f) for f in files]\n\t\n\tpkgs = [name for name, in files if name[len(os.getcwd()):]!= \".\"+\".xml\"]\n\tm"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tfiles = []\n\tfor file_name in os.listdir(folder):\n\t\tbasename = os.path.basename(file_name)\n\t\tpackages = []\n\t\tfilepath = os.path.join(folder, file_name)\n\t\tif not os.path.isfile(filepath): continue\n\t\tcontent, date = _package_file_to_dict(filepath)\n\t\tif not content: continue\n\t\tif basename in packages: continue\n\t\txml_name = basename\n\t\tdata = {\n\t\t\t\"name\": [xml_name"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Get folders' content and packages\n\tfiles = []\n\tfor f in os.listdir(folder):\n\t\tfName = f.split('/')[-1]\n\t\tfiles.append(fName)\n\t# Loop through files\n\tpackages = OrderedDict()\n\tfor file in files:\n\t\txml_path = os.path.join(folder, file)\n\t\t# Load element from xml, it returns tuple (package_name, packages)\n\t\tpackage_name, packages_xml = get_package(xml_path)\n\t\t# Add packages to"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif not folder:\n\t\traise Exception(\"You have given no folder!\")\n\tif not isdir(folder):\n\t\traise Exception(\"The folder doesn't exist or the given folder is incorrect: %s\" % (folder))\n\t\n\tpackage_data = dict()\n\t\n\tdir_names = []\n\tsub_folders = []\n\tpkg_data_filename = folder+\"/.pkg_meta.xml\"\n\t\n\tfor f in glob.glob(pkg_data_filename):\n\t\t\n\t\tfilename_base = os.path.basename(f"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif not os.path.isdir(folder): return\n\tpackages = list(_explore_folder(rootDir=folder))\n\tdata = OrderedDict()\n\t_for_each(packages, funct=lambda p: data[p.relative].update(p.data))\n\treturn(data)\n\ndef parseXml2dictfactory(fileName):\n\t\"\"\"\n\tCreate a factory from the XML file\n\t\n\tParameters\n\t----------\n\tfileName : str\n\t    Name of the XML folder\n\t\"\"\"\n\t# Create a factory from"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = {}\n\tfor file in os.listdir(folder):\n\t\tif os.path.splitext(file)[1] == '.xml':\n\t\t\tdata[file] = {key: data[key] for key in os.listdir(folder + \"/\" + file)}\t\n\treturn data\n\n"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdir = os.listdir(folder+'/')\n\t\n\treturn {n : {} for n in dir}\n\t\nif __name__ == '__main__':\n#\timport doctest\n#\tdoctest.testmod(verbose=True)\n\ttest_suite = unittest.suite()\n\ttest_suite.addTest(doctest.DocFileSuite(open('examples.txt',\"r\"),optionflags=doctest.ELLIPSIS))\n\ttest_suite.addTest(doctest."}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\txmldata = []\n\tif os.path.exists(folder):\n\t\tif sys.version_info < (3, 0):\n\t\t\tfor root, _, files in os.walk(folder, topdown=False):\n\t\t\t\tfor file in files:\n\t\t\t\t\tif re.search(r\"[\\s\\xa0*]_[\\da-f]{7}\", file[-4:]):\n\t\t\t\t\t\textension = file[-4:]\n\t\t\t\t\telse:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif extension == \".xml\":\n\t\t\t\t\t\t# Get package name and save path in keys"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tpkg_xml = \"\"\n\tf = os.path.join(os.path.dirname(folder), 'package.xml.gz')\n\t\n\twith gzip.open(f, 'rb') as fp:\n\t\tpkg_xml = fp.read()\n\t\n\t_pkg_xml, _pkg_path = _compile(pkg_xml)\n\t# Create data structure\n\t_pkg_dict = OrderedDict([(i.lower(), pkg) for i, pkg in _pkg_xml.items()])\n\t\n"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\treturn _get_value_if_defined(os.path.dirname(\n\t    filename).replace(prefix, \"\") + \"/\")\t\n\ndef _eval_package(directory, package):\n\tpath = os.path.join(directory, package)\n\tretval = _eval_file(directory, path)\n\treturn {\n\t    \"file_folder\": get_filesystem_path(\"source\", path),\n\t    \"class\": directory,\n\t    \"class_name\": package,\n\t    \"package_name\": package}\n\t\n"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tprefix = os.path.dirname(file_path)\n\tasset = os.path.basename(file_path)\n\t\n\tdirectory = os.path.dirname(prefix)\n\tfiles = glob.glob(os.path.join(directory, '*.xml'))\n\tfiles.sort()\n\t\n\t# Remove duplicates\n\tpackages = {}\n\t\n\tunique_files = []\n\tfor f in files:\n\t    with open(f, 'r') as f_obj:\n\t        xml = f_"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfile_path = file_path.replace(\" \", \"\\ \").replace(\"/\", \"\\n\").strip()\n\tbase = sub(\"\\\\.\\\\/\", \"\", file_path)\n\textensions = [\".java\", \".class\", \".js\", \".html\", \".htm\", \".gif\", \".png\", \".jpg\", \".wml\", \".txt\"]\n\t\n\tresult = True\n\t\n\tfilename = filename + \" \" + file_type(file_path[len(base) + 1:])\n\t\n\tpackages_map = {}\n\t\n\tfor ext in extensions"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tlog_debug(\n\t\t'_eval_file',\n\t\t'entrou. prefix = \"%s\" filename = \"%s\" file_folder = \"%s\"'\n\t\t% (prefix, filename, file_folder),\n\t)\n\tlog.debug(file_folder)\n\n\tfull = os.path.dirname(file_folder)\t\n\t\n\t# Retrieve metadata for the file\n\twith open(file_path) as file_meta:\n\t\tfile_meta_dict = parse(file_path)\n\n\tasset = filename.split('.')["}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdef _asset():\n\t\treturn prefix+'lib/xml'\n\t\n\tdef _rendition():\n\t\treturn \"\".join([prefix+'lib/xml',prefix+'lib/css_stylesheets'])\n\n\tpackages = {}\n\t\n\tif filename.endswith('rc'):\n\t\tpackages[prefix+'lib/css_stylesheets'] = _asset()\n\telse:\n\t\tpackages[prefix+'lib/css_stylesheets'] = _asset()\n\tpackages[prefix+'lib/resources'] = _"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = load(file_path, path=path)\n\tpackage_nome = {a.getName():a for a in data['package']}\n\tdata['packages'] = [pkg for pkg in data['packages'] if pkg['nome'] in package_nome]\n\tdata['packages'].sort()\n\tdata['assets'] = {a.getType():a for a in data['assets']}\n\tdata['assets'].sort()\n\t\n\treturn data\n\t\n\t\n\n"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\ttry:\n\t\tfor package in packages:\n\t\t\t\n\t\t\t# Nada deve ser vazio, mas os primeiros do arquivo (asset)\n\t\t\t# especializado que vamos ignorando i s\u00e3o apenas qualquer arquivo.\n\t\t\tif (package.startswith(prefix)):\n\t\t\t\treturn module.__dict__[package]\n\t\t\n\texcept:\n\t\tprint(\"N\u00e3o foi poss\u00edvel identificar o arquivo do pacote...\")\n\t\tprint(package)\n\t\traise\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\treturn _pkg_file(file_path, 'xml', prefix, 'package')\n\ndef _file_from_xml(file_path, prefix, **kwargs):\n\t\"\"\"\n\tObtenha o arquivo XML em disco.\n\t\n\tIdentifica o tipo de arquivo e atualiza `packages`.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML\n\t\n\tReturns\n\t-------\n\tstr\n\t\"\"\"\n\t# Get the name of the manifest node\n\tmf = {}"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpackage_tree = os.path.dirname(file_path)\n\tpackage_root = os.path.join(prefix, '', 'packages')\n\n\tif not os.path.exists(package_root):\n\t\tos.makedirs(package_root)\n\t\n\twith open(os.path.join(package_root, os.path.basename(file_path)), 'w+') as fh:\n\t\tfh.write(file_path)\n\n\tpackage_file = os.path.join(prefix, os.path.basename"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tif file_path == 'asset':\n\t\treturn _asset(prefix, filename).get('asset_class', '')\n\telif file_path =='rendition':\n\t\treturn _rendition(prefix, filename).get('rend_class', '')\n\telse:\n\t\traise Exception('File \"%s\" no arquivo %s' % (filename, file_path))\n\ndef _asset(prefix=\"\", filename=\"\"):\n\t\n\tfrom.resources.asset.classes.asset_class import Asset\n\tfrom it"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tassert lang not in [\"english\" for i in range(3)]\n\n\treturn {\n\t    \"original\": self._file_path_template.format(lang=lang),\n\t    \"en\": self._file_path_template.format(lang=\"en\"),\n\t}[file_path]\n\n\ndef compile(source_language):\n\treturn {\n\t    \"original\": source_language + '.py',\n\t    \"en\": source_language + \".py\",\n\t}[source_language]\n\n\ndef source_language(file_path):\n\tmatch="}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tif not lang.lower().startswith('en') and lang not in ('fr', 'es'):\n\t\treturn {}\n\tif lang == 'es':\n\t\treturn {\n\t\t\t\"original\": \"artigo02-es.pdf\",\n\t\t\t\"en\": \"artigo02-es.pdf\",\n\t\t}\n\tpage = 1\n\tfilename = 'artigo02-%s.PDF' % lang\n\text = \".pdf\"\n\tif file_path.lower().endswith(ext):\n\t\tfilename = \"%s-%s.%s\" % ("}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tlog.debug('add_rendition')\n\tfile_path = path.expanduser(file_path)\n\tif not os.path.exists(file_path):\n\t\tlog.debug('file not exist %s', file_path)\n\t\treturn\n\telse:\n\t\tlog.debug('file exist %s', file_path)\n\t\tif not os.path.isfile(file_path):\n\t\t\traise ValueError('file path is not exist %s' % file_path)\n\n\t\tif int(lang)!= 0:\n\t\t\traise ValueError('"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tfilename = \"f%s.%s\" % (os.path.basename(str(file_path)), lang.lower())\n\tfile_path_path = os.path.mkstemp(dir=os.environ[\"HOME\"])\n\tos.close(file_path_path)\n\tshutil.copy(file_path, file_path_path)\n\tf = open(file_path_path, 'rb')\n\traw_pdf = f.read()\n\tf.close()\n\tpdf_fname = base64.b64en"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n\t\n\tself.r = ResourceManager.get_resource(self.file_path)\n\t\n\tself.source = \"\"\n\tself.destination = \"\"\n\t\n\tif self.file_path!= lang:\n\t    self.source = self.file_path\n\t    \n\telse:\n\t    self.source = self.r.get(\"original\", lang)\n\t    self.destination = self.r.get(\"en\", lang)\n\t\n\t\nfor item in sorted(self.source.keys()):\n\tself.destination"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n\tfilepath_en = os.path.join(filepath, lang)\n\t#print \"get_rendition_filepath:filepath, filepath_en:\",filepath,filepath_en)\n\tfilepath_en = os.path.dirname(filepath_en)\n\tdirpath_en, basename_en, extension_en = os.path.split(filepath_en)\n\tif extension_en not in ('pdf', ) and filepath.find(extension_en) == -1 :\n\t\tsubfilepath_en"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\trendition = {\n\t\t\"original\": self.path(file_path),\n\t\t\"en\": self.path(\"artigo02-en.pdf\"),\n\t}\n\tif not os.path.exists(rendition['original']):\n\t\traise Exception(\"Could not find the rendition file\")\n\trenditions.append(rendition)\n\n\n# Main\nif __name__ == \"__main__\":\n    main()\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\trendition = self.renditions.get(lang, {})\n\n\trendition.add_file(file_path, lang)\n\n\treturn\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tfile_name = file_path.replace(\".pdf\", \".pdf.en\") + \".pdf\"\n\thtml_source = \"\"\"\n\t\t<div id=\"{file_name}\" title=\"Exportar esto en {{ lang }} esto en {{ file_lang }}\">\n\t\t<table border=\"0\"><tr>\n\t\t\t<td>{{ file_name }}</td>\n\t\t\t<td>{{ file_lang }}</td>\n\t\t\t<td>{{ file_size }} B</td>\n\t\t</tr>\n\t\t</table></div>\n\t\"\"\"\n\tfile_"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tpath = self._convert_path(file_path)\n\tif not os.path.isfile(path):\n\t\tprint(\"[INFO] File not found {}\".format(path))\n\t\treturn -1\n\n\trendition = '{0}-{1}.pdf'.format(lang, path)\n\ttry:\n\t\twith open(path, 'rb') as f: \n\t\t\t# TODO get rid of this, it sucks..\n\t\t\trendered_string = f.read()\n\t\t\tif not re.search(regex_match['en'], rendered_string"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif basename.startswith(\"artigo-\"):\n\t\tnew_asset.insert_after(self, basename)\n\ndef add_image(self, item):\n\tif item.image:\n\t\tnew_image.insert_before(self, item)\n\ndef add_subdir(self, subdir):\n\tnew_dir.insert_before(self, subdir)\n\t\n\t\ndef import_images(path_to_file, verbose=False):\n\t\"\"\"\n\tImports the list of images from given path, with optional verbose"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tfile_name = basename\n\ttry:\n\t\tif not file_path:  # Path not given\n\t\t\tpath, file_name = self.get_relative_path_to_file(file_name)  # get relative path for file\n\t\telse:  # File given\n\t\t\tfile_name = file_path\n\t\tself.__add_item(\"file\", self.__make_from_path(path), file_name=file_name, type=\"file\")\n\texcept Exception as ex:\n\t\traise Exception(\"add asset {0} [error] {1}"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.assets.append((basename, file_path))\n\n\n\t\"\"\"\n\tfor a in self.assets:\n\t    logging.debug(a)\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.__add(basename, file_path)\n\n    def add_list_of_files(self, basename, file_list):\n        for fname in file_list:\n            self.add_file(basename, fname)\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\treturn os.path.join(self.assets, basename)\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tpass\n\t\n\t\"\"\"\n\t\"{\n\t    \"artigo02-gf03-16-tiff.tiff\": \"/path/artigo02-gf03-16-tiff.tiff\",\n\t    \"artigo02-gf03-16-tiff.jpg\": \"/path/artigo02-gf03-16-tiff.jpg\",\n\t    \"artigo02-gf03-16-tiff.png\": \"/path/artigo02-gf03-16-tiff.png\",\n\t}\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t# Check that basename is not empty\n\tif not basename or basename==\"\" : self.logger.warn(\"Asset basename is empty!!!\")\n\t# Call add_repo\n\tself.logger.debug(\"asset --> add_asset(): basename : %s\" % basename)\n\tif file_path.split(\":\",1)[1] in self.asset_repo : self.logger.warn(\"asset --> add_asset(): asset already exists!!!!\") # The file is already in the repo...\n\tself"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tfiles = self.__find_files(file_path)\n\tif basename in self.__asset_dirs:\n\t\traise(Exception(\"There is a file with the name \" + basename))\n\n\tself.__asset_dirs.append(basename)\n\n\n\t#print dir(self.__asset_files)\n\n\tself.__asset_files[basename] = \"add_asset\" + os.path.sep + basename\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif basename in self._base:\n\t\treturn\n\tself._base.add(basename)\n\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tr_file = self.get_artigo_file(basename)\n\tif r_file.mime == \"image/tiff\" or r_file.mime == \"image/jpeg\" or r_file.mime == \"image/x-ms-bmp\":\n\t\t# TODO move this to \"self.add_file\" method\n\t\tpath = os.path.splitext(file_path)[0]\n\t\tlog.debug(\"add_asset('{}', '/{}')\".format(basename, path))\n\t\treturn super"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tname_to_dict = {}\n\tfor infile in ['/home/mikael/project/py/' + r.basename() for r in zip_path.files]:\n\t\t\n\t\tif infile.name[-4:] == \".xml\":\n\t\t\n\t\t\tinfile_dict = parse(infile, as_dict=True).as_dict()\n\t\t\t\n\t\t\tfor k,v in infile_dict.items():\n\t\t\t\tname_to_dict[v[\"xml_basename\"]] = k\n\t\t\n\treturn name_to_dict\n\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Get directory and file list, and their basenames.\n\tdir_list, file_list = os.walk(zip_path)\n\tdir_list = [dir_list[-1]]\n\tfiles = [os.path.basename(file).rstrip() for file in file_list]\n\t\n\t# Create list for storing xml root element.\n\troot_elem = [] #list\n\t\n\tfor fname in files:\n\t\t# Check the file for a valid xml.\n\t\tif zip_path.rfind('/') > -1"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\twith zipfile.ZipFile(zip_path) as z:\n\t\tnames = z.namelist()\n\t\td = dict()\n\t\tfor nf in names:\n\t\t\troot = z.read(nf)\n\t\t\tnj = root.search(\"*\").next()\n\t\t\tns = ns + nj.name\n\t\t\tif len(ns.strip('\"')) > 0:\n\t\t\t\tns = ns.strip('\"')\n\t\t\tif len(ns.strip('\"')):\n\t\t\t\tns = ns.strip('\"')\n\t\t\td[ns] = get_"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tpackages = {}\t\n\tf = open(zip_path)\n\tfor line in f.readlines():\n\t\t# if line.lstrip().startswith(\"<package>\"):\n\t\tname = line[11:].strip()\n\t\t# split name by \\x2F\n\t\tname = name.split(\"\\x2F\", 1)[0]\n\t\t# split xml name by \\x2F\n\t\txml = line[12:].strip()\n\t\t# split name by \"//\"\n\t\tname = name.split(\"//\",1)[0]\n\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\td = {}\n\t\n\tdef get_data(n):\n\t\tf = open(os.path.join(zip_path, n,'file'), \"r\")\n\t\t# read contents of file\n\t\traw_file = f.read()\n\t\tf.close()\n\t\tf = zipfile.ZipFile(f.name)\n##\t\tf.namelist\n\t\trecords = [rec.split() for rec in f.namelist() if re.match(r'^[<>/~]', rec)]\n\t\trecords = [r for r in records if"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tret_dict = {}\n\tfor entry in os.listdir(zip_path):\n\t\tif entry.endswith(\".pkg\"):\n\t\t\tdata = zipfile.ZipFile(zip_path + entry).read(entry)\n\t\t\tret_dict[entry] = dict(data)\n\treturn ret_dict  \n\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t#print(\"\\nZip is: \" + zip_path)\n\t\n\twith zipfile.ZipFile(zip_path, \"r\") as zf, zip_iterator:\n\t\t\n\t\tdef _get_dirnames(zf):\n\t\t\t\n\t\t\tret_dict = {}\n\t\t\t\n\t\t\tcurrent_dir = \"\"\n\t\t\tfor fname in zf.namelist():\n\t\t\t\tret_dict.update(dict([(current_dir + \"\\\\\".join(fname.split(\".\")[0]), fname) for fname in zf.nam"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdict = {}\n\tzipf = zipfile.ZipFile(zip_path)\n\tfor fnm in zipf.namelist():\n\t\tfnm = fnm[:-4] # truncate base filename\n\t\tfor f in zipf.namelist():\n\t\t\tif fnm.startswith(f) and fnm[7:]!= f:\n\t\t\t\tdict.update(_explore_from_zip(zipf.read(f)))\n\t\t\t\tzipf.close()\n\treturn dict\n\n\ndef _explore_from_zip(zip_file):"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\txml_files = list(os.walk(zip_path))[1]\n\txml_files.sort(key=lambda f: f.filename.lower())\n\tdict = {}\n\tfor f in xml_files:\n\t\tif f.filename.endswith('-metadata.xml') or \\\n\t\t   f.filename.endswith('-packages.xml'):\n\t\t\t#print 'Extracted Packages'\n\t\t\tdict[f.filename.split('.')[0].lower()] = _get_xml_data(f)\n\t\telse:\n\t\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zf:\n\t\tzinf = zf.infolist()\n\t\t\n\t\t# get package infos from each xml file\n\t\tfor zinf_index in range(zinf.number_of_members):\n\t\t\tzinf_name = zinf.external_attr[zinf.external_attr_offset + zinf_index]\n\t\t\t\n\t\t\t# get data from all xml files under this one\n\t\t\tif zinf_name == 'package.xml':\n\t\t\t\tzip_data = zinf."}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfiles_items = []\n\tzip_file = zipfile.ZipFile(zip_path)\n\t\n\tfor current_file in zipped_files:\n\t\tfilenames_with_path = [f for f in zip_file.namelist() if f.endswith(current_file.rpartition('/')[0])]\n\t\tfiles = [f for f in filenames_with_path if zmtime(f) == current_file.rpartition('/')[-1].rpartition('-')[0]][0]"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfilenames = []\n\t\n\tz = zipfile.ZipFile(zip_path, 'r')\n\ttry:\n\t\tcount = 0\n\t\tfor filename in z.namelist():\n\t\t\tfilenames.append(filename[len('/'):])\n\texcept KeyboardInterrupt:\n\t\n\t\tz.close()\n\t\tif os.access(zip_path, os.R_OK): # r access in case of a ZIP\n\t\t\tos.rmdir(zip_path)\n\t\telse: # zip access, no r access\n\t\t\traise Exception('Un"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\timport os\n\t\n\tdata_list = []\n\tfor p in os.listdir(zip_path):\n\t\ttry:\n\t\t\twith open(os.path.join(zip_path, p), 'rb') as f:\n\t\t\t\tdata_list.extend(''.join(['<%s>' % n for n in lines]))\n\n\t\texcept:\n\t\t\tprint os.path.join(zip_path, p) +'does not exist'\n\t\t\tcontinue\n\tfiles_list = []\n\tfor p in data_list:\n\t\tfiles_list."}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfilenames = []\n\tfor base, _, files in zip(zip_path, os.listdir(zip_path), \n\t\t\t\t\t\t\t\t\tos.walk(zip_path)):\n\t\t\n\t\tfilenames.append(os.path.join(base, files[file]))\n\n\tlogging.info(\"Processing in progress, will look for files using zipfile in `%s`\", zip_path)\n\treturn filenames\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\treturn [\n\t\tz.filename for z in zipfile.ZipFile(\n\t\t\topen(zip_path, 'rb'))\n\t\t\tif z.filename!= '']\n\ndef add_file_in_zipfile(zip_path, file):\n\t\"\"\"\n\tAdd a file to a zipfile if it already exists.\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\timport zipfile\n\tzf = zipfile.ZipFile(zip_path)\n\tfor name in zf.namelist():\n\t\tyield name\n\n\ndef directory_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the directory of `zip_path`\n\t\"\"\"\n\treturn os.path.dirname(os.path.basename(zip_path))\n\n\ndef list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn all the files in `zip_path`\n\t\"\"\"\n\tfile_list = []\n\tfor"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t# Get list of the files in the zippath.\n\tf_list = zip.ZipFile(zip_path, 'r')\n\tfiles = f_list.namelist()\n\treturn files\n\n\t# return zip_dir.split('/')[-1].split('-')[0]+'/'\n\n\t\n# #################\n# main function\n# #################\nif __name__ == '__main__':\n\t\n\t# read the configuration\n\ttry:\n\t\tcfg = __config__\n\texcept:\n\t\t# no configuration: read defaults\n\t\tcfg"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfiles = get_file_list_from_path(zip_path)\n\treturn list(map(lambda f: os.path.join(f, '{0}'.format(os.path.basename(f))), files))\n\t\n\nif __name__ == \"__main__\":\n    import datetime\n\n    aDate = datetime.date.today()\n    print aDate\n    print files_list_from_zipfile('/home/a/zf/cve.zip')\n\n\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfiles = glob(os.path.join(zip_path, *files_list))\n\treturn [os.path.basename(file) for file in files if zipfile.is_zipfile(file)]\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\timport bz2\n\n\tdef _files_list_from_zip(zip_path):\n\t\t\"\"\"\n\t\tReturn a list of tif files in `zip_path`.\n\t\t\n\t\tReturned are files with a `.tif` extension of.tif or.xml.\n\t\t\n\t\t**Explanation**:\n\t\t\tThis includes.tif,.xml and sub folders.\n\t\t\t\n\t\t\tFor example:\n\t\t\t\n\t\t\t```\n\t\t\t[\n\t\t\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = re.sub(r'w+\\(', r'w:\\w+', content)\n\tcontent = re.sub(r'w:.*\\[\\]\\[', r'w:\\w+\\[', content)\n\tcontent = re.sub(r\\*\\[\\*\\*\\w+\\*\\*\\w+\\],\\s', r',\\s', content)\n\tcontent = re.sub(r',\\s', ', ', content)\n\treturn content\n\ndef fix_url_namespace(content):\n\t\"\"\""}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif '\\r' in content:\n\t\tcontent = content.replace('\\r', '-')\n\n\tcontent = content.replace(' w:st=\"','w-st=\"')\n\treturn content\n\"\"\"\n\n\tdef get_namespace_prefix_w(content):\n\t\t\"\"\"Convert \"w:st=\\\"' into 'w-st='\"\"\"\n\t\treturn content.replace(' w:st=\"','w-st=\"')\n\n\tdef get_namespace_prefix_x(content):\n\t\t\"\"\"Convert \"w:st=''; into 'w-st=\\\""}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tname = content.find('{') + 1\n\tcontent = content[:name].rstrip() + \"\\n\"\n\tcontent = content.replace('{w:st}','w-st=\"'+content[name:]).rstrip()\n\treturn content\n\ndef fix_namespace_path(content):\n\t\"\"\"\n\tFix the uri in the document\n\t\"\"\"\n\t#print \"before: %s\" % (content)\n\tcontent = content.strip('\\n;>')\n\tnamespaces = {}\n\n\twhile content:\n\t\tpos = content"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tfrom xml.dom.minidom import parseString\n\treturn parseString(content).firstChild.firstChild.data.replace(\"\" \"w:st=\", \"w-st=\")\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content ==\"-\" in content:\n\t\treturn content.lstrip('-')\n\treturn content\n\ndef fix_namespace_prefix(content):\n\treturn re.sub(\"\\b([_:]\\w+.*)\",\"\\1w\",content,re.UNICODE)\n\n# This does the same as genshi-jinja.\nfrom genshi.template import Template\n\ndef genshi_jinja(template):\n\treturn Template(str(template))\n\ndef genshi_tag(content,**tag_attrs):\n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\treturn content\n\n\ndef get_namespace_prefix(content):\n\t\"\"\"\n\tLook for:\n\t  <g:w:nsprefix xmlns:g=\"http://example.org/ns#\"/>\n\tand get prefix of it.\n\t\"\"\"\n\tnsprefix = None\n\tm = _gattr.match(content)\n\tif m is None:\n\t\tcontent = m.group(1)\n\n\treturn nsprefix_get(content)\n\n\ndef get_content(content=\"\", ns=None):\n\t\"\"\"\n\tLook"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content in ['*'] or content.lower() == '*':\n\t\treturn content\n\tcontent = content.replace(\"'\", \"w:st=\\\"\\\"')\n\treturn content\n\n\nclass StAXRecordReader(LineIterator):\n\tdef __init__(self, inp):\n\t\tself._stax_reader = Streams.ReaderStream(inp)\n\n\tdef __iter__(self):\n\t\treturn self\n\n\tdef reset(self):\n\t\tself._stax_reader.reset()\n\n\tdef set_position(self, pos):\n\t\treturn self._"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = re.sub(\"st=\\\"\\\"\", \"st=\\\"w:st=\\\",\\\"\", content)\n\treturn content\n\n\nclass MyFormatter(logging.Formatter):\n   def format(self, record):\n      level, msg = self.format_msg(record)\n      namespace, message = None, None\n      if level == logging.NOTSET or level == 'INFO':\n         namespace = record.levelno\n         message = record\n      lines = self._line_fmt(record.levelno, level, msg, namespace, message)\n      msg"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t_, prefix=content.split(':', 1)\n\tif prefix=='w':\n\t\tprefix = 'w:'\n\treturn prefix+content\n\ndef fix_newlines_w(content):\n\t\"This function does what the doctest expects.\"\n\tline_re = re.compile(r'[^\\n\\t]*\\n', re.MULTILINE)\n\tfor match in line_re.finditer(content):\n\t\tcontent=content.replace(match.start(), match.end(), '_n')\n\treturn content\n\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = content[2:-1] # trim off 'w:st=\"'\n\tcontent = content.replace(' \"','\"w-st=\"')\n\treturn content\n\ndef fix_namespace_prefix_r(content):\n\t\"\"\"\n\tNesting 'r:wn=\"' to fix issue with 'a:\" w:\"\n\t\"\"\"\n\tcontent = content[3:-1] # trim off 'r:wn=\"'\n\tcontent = content.replace('a:wn=\"', 'a-wn=\"')\n\treturn content\n\ndef fix_namespace(content"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor match in pubdate_xpaths:\n\t\tmatch = re.search(r'\\b' + match, node.text)\n\t\tif match:\n\t\t\treturn match\n\t\telse:\n\t\t\tmatch = re.search(r'\\b' + match, node.parent.text)\n\t\t\tif match:\n\t\t\t\treturn match\n\treturn None\n\ndef match_id(node, id_xpaths):\n\t\"\"\"\n\tReturns the first match in the id_xpaths list\n\t\"\"\"\n\tfor match in id_xpaths:\n\t\tmatch ="}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\tif len(pubdate_xpaths) == 0:\n\t\tpatterns = []\n\telse:\n\t\tpatterns = pubdate_xpaths\n\t\n\tfor expr in (patterns[0]):\n\t\tpubdate_node = None\n\t\tfor node in nodes[expr]:\n\t\t\tif node.xpath(expr):\n\t\t\t\tpubdate_node = node\n\t\t\t\tbreak\n\n\t\tif pubdate_node is None:\n\t\t\traise Exception(\"Could not find %s at %s\" % (patterns[0],pubdate_xpaths))\n\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tregex = re.compile(\"(^|\\s)(?P<pubdate>[^{]]*)$\", re.UNICODE)\n\tfor xpath in pubdate_xpaths:\n\t\tmatch = regex.match(xpath)\n\t\tif match:\n\t\t\treturn match.group(\"pubdate\")"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\treturn [x for x in pubdate_xpaths \n\t\t\tif x.findall(str(node.get('published_date'))).__len__() > 0]\n\ndef clean_pub_date(text):\n\t\"\"\"\n\tParse a date into a datetime object for return\n\t\"\"\"\n\ttry: \n\t\tdate_obj = dateutil.parser.parse(text)\n\texcept Exception as e: \n\t\ttry: \n\t\t\tnew_date = dateutil.parser.parse(text, ignore=dateutil.parser.ParserError)"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch = re.findall(r'\\s*(?:D|a\\s+|A|p|i)\\s*>', pubdate_xpaths[0])\n\treturn match[0]\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t# get the pubdate string\n\tpubdate = node.find(pubdate_xpaths[0]).text\n\t# get the last published date\n\tpub_date = node.find('published').text\n\tif pubdate!= pub_date:\n\t\traise NoPageMatch('Last published date mismatch.')\n\n\t# return the list of matched nodes\n\n\treturn pubdate_xpaths[1:]\n\t\n\t\n\t\ndef get_matches(query=None, start_page=None, results_limit=50):\n\tif query is None:\n\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfound = None\n\tfor x in pubdate_xpaths:\n\t\tif (pubdate_xpaths[x[1]].text == '2010-09-17') and (\n\t\t  x[1] not in [x[0] for x in pubdate_xpaths]):\n\t\t\tfound = x\n\treturn found if found else None\n\n\ndef get_first(path, request):\n\t# Get the first node\n\txpath = path\n\txpath_results = request.cJax.search(xpath)\n\tif len(x"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tpubdate = re.compile(r'^\\d\\d\\d\\d \\d\\d\\d \\d\\d\\d$', re.IGNORECASE)\n\txpaths = []\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\txpaths.append(pubdate.findall(pubdate_xpath)[0])\n\t# if not match\n\tif not len(xpaths) == 1:\n\t\traise ValueError('Match was failed: [' + str(xpaths) + ']. No Pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t# TODO: maybe this should be split into two functions\n\t# maybe get/set functions should probably be in a global variable\n\tfor pubdate_xpath in sorted(pubdate_xpaths, key=lambda x: x.get(\"year\", None), reverse=True):\n\t\tres = None\n\t\tmatch = None\n\t\t# maybe this can be slow if regex matches is broken\n\t\tmatch = re.match(pubdate_xpath, node.text)\n\t\tif match:\n\t\t\tres = pubdate_xpaths[0][\"date\"] + \"/2000\""}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tregex = re.compile('(?s)^pubdate:\\s*(\\d+)\\s*'+\n\t\t\t\t\t\t'(?:(?:Sun|Mon|Tue|Wed|Thu|Fri|Sat|1st|2nd'+\n\t\t\t\t\t\t'|Sat|Tue|Wed|Thr|Fri|Fri|2nd|3rd|04:05)'+\n\t\t\t\t\t')$')\n\tfor xp in pubdate_xpaths:\n\t\tmatch = regex.match(node.get('pubdate', xp))\n\t\tif match:\t\t\t\n\t\t\treturn match\n"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber, supplier_number = issue_element.find_single_element(TEXT_ELEMENT_PATTERN)\n\tsupplier_number = supplier_number or number\n\tsupplier_number = get_number_or_supplier_number(supplier_number)\n\n\treturn number, supplier_number\n\n\ndef parse(issue_xml):\n\t\"\"\"\n\tParse the issue XML by extracting the possible values of number and suppl, and returns the number and\n\tsupplier number if valid, otherwise returns an empty string if no valid value can be extracted.\n\t\"\"\"\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = str(issue.text, \"utf-8\")\n\tsuppl = issue.attrib[\"supplication\"]\n\treturn (None, suppl_values)\n\ndef _extract_product_from_issue_element(issue):\n\tproduct = issue.attrib.get(\"product\")\n\tif product is not None:\n\t\treturn product.strip()\n\telse:\n\t\treturn str(issue)\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = issue['text']['text']\n\tsuppl = issue['supplementalInformation']['texts'][0]['supplementalInformation']['text']\n\tsuppl_element = BeautifulSoup(suppl)\n\tsupp_id = suppl_element.find_all('p.id')[0]\n\treturn {\n\t\t'id': supp_id['id'],\n\t\t'number': int(number[number.find(\",\") == 0])\n\t}\n\ndef _extract_number_and_suppl_from"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\txpath = '//span[@id=\"content_body\" and text() = \"CET\"]'\n\tcontents = './p/p[1]/text()'\n\tissues = [issue[0]]\n\twhile 0 < len(issues) > 0:\n\t\tissue = issues[-1]\n\t\tissue_id = issue_elements(issue)[0].attrib['id']\n\t\tif issue_id == issues[0].attrib['id']:\n\t\t\tbreak\t# there is no issue id left\n\t\telse:\n\t\t\tcontinue\n\twhile"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t# Extract the possible text: number and supp\n\textract_text = issue.find_all('text')\n\tif len(extract_text.get_all_text()) > 0:\n\t\tsuppl_text = extract_text[0].get_all_text()\n\t\tfor element in suppl_text.get_all_text().splitlines():\n\t\t\tparts = element.split(',')\n\t\t\tif len(parts) > 1:\n\t\t\t\tdata = parts[1].split()\n\t\t\t\tif len(data) > 1:\n\t\t\t\t\tmatch_string = data["}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_and_supplier = ''.join(issue.find_all(text=re.compile(r'[0-9]+\\*') if issue.find(\"*\") == 1 else r'['+re.escape(issue.get(\"number\")+']'))).strip()\n\tsupplier = issue.find(\"supplier\")\n\tassert supplier is not None and supplier['text'] == number_and_supplier else None\n\ndef _compute_total_value(issue):\n\tresult = ''\n\ttotal_value = 0.0\n\tsupp"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tsuppl_value = None\n\tprevious_suppl = support_level_by_issue.get(issue.assigned_to_id, None)\n\n\tfor supp in issue.supplants:\n\t\tif supp.number is not None:\n\t\t\tprevious_suppl = supp.supplement_by_number\n\t\telif supp.supplement!= None:\n\t\t\tprevious_suppl = supp.supplement\n\tif previous_suppl is None:\n\t\tprevious_suppl = issue.supplement_by_type\n\tif previous_suppl"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\text_suppl_count = None\n\tnumber = None\n\twhile issue.find(Suppl)!= -1:\n\t\tif issue_element.find(n_text=number, a_text=name, r_text=item)!= -1:\n\t\t\t# if there is a match, extract the supplier from the text\n\t\t\tsupplier = issue.find(Suppl).text\n\t\t\text_suppl_count = int(issue.attr(a_attr='supplCount'))\n\t\t    if issue_element.find(n_text=number, b_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = issue.attrib['number']\n\tnumber_and_suppl = re.search(r'\\[\\d+\\]', number)\n\tif number_and_suppl:\n\t\treturn (number, number_and_suppl.group(0))\n\n\ndef _get_issue_data(issue):\n\t\"\"\"\n\tExtract the body, text, and status of the issue.\n\n\tA lot of attributes of issue are not populated yet, so this function\n\textracts them in its own function, from the issue element, so we can just call"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tsuppl = []\n\n\tfor s in split_contents_to_string(issue.text):\n\t\tif s.startswith(\"$$\"):\n\t\t\tsuppl.append(substring(s, s.find(\"$$\")) +'' + substring(s, s.find(\"$$\")) +'' + substring(s, s.find(\"$$\")) +'')\n\n\t# remove the first item (the number); and return the suppliers (both types)\n\treturn [substring(s.find(\"$$\"), s.find(\"$$\") + 1) for"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tdef __str__(self):\n\t\t#print str(self)\n\t\tx = str(self.element, indent=indent, width=15)\n\t\tx = x.rjust(15)\n\t\treturn x\n\n\tdef __repr__(self):\n\t\treturn self.__str__()\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tdef __init__(self, indent=0):\n\t\tself.indent = indent\n\t\t\n\tdef format(self, string, out=sys.stdout):\n\t\t\"\"\"\n\t\tstring -> string\n\t\t\tpretty printing method for this object\n\t\t\tformat is a function which looks like this:\n\t\t\t\tself.format(string)\n\t\t\"\"\"\n\t\tstring, = out.write\n\t\tif self.indent > 0:\n\t\t\tstring = string.rjust(self.indent,'')\n\t\treturn string\n\n\tdef render(self, out=sys.stdout):"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tout = \"\"\n\t\n\tout += \"%s%s\" % (self.__class__.__name__,'' * min(indent, len(str(self))))\n\t\n\tout += self.__class__.__name__\n\n\treturn out\n\n#==============================================================================\n# Methods for writing/reading in RData objects.\n\ndef toArray(rdobj):\n\t\"\"\"\n\t\tConvert a data-object into a NumPy array\n\n\t\tReturn: array\n\t\t\"\"\"\n\tarr=\"\"\n\n\tif rdobj.dtype == RData."}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ts = ['{:{width}}]\\n'.format(str(self.__dict__).rjust(indent * 3,''), '\\n')]\n\t\n\t## Set the pretty string formatting. If you want to put this more text in brackets, use\n\t## [self._prettyList]. If you want to use another pretty/sliced/pretty/bracketed display, just\n\t## create [self._prettyList].\n\n\ts.append('[%(size)s]:' % {'size': self.__dict__.get(\"_size"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\treturn indent( \"PYMGO\" ) + indent( json.dumps( self ) )\n\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tlines = []\n\tmaxlen = 80\n\tfor k in sorted(self.__dict__.keys()):\n\n\t\tv = getattr(self, k, None)\n\t\tif v is None:\n\t\t\tcontinue\n\t\t\t#v = \"None\"\n\t\tlines.append( \"-\"*maxlen +\"|\" )\n\t\tlines.append(\"|  %s: %s\" %( k,pretty_repr(v)))\n\t\tlines.append(\"\\n\")\n\t\tfor m in k.split(\" \"):\n\t\t\tlines.append( m + \" :\")"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ttry:\n\t\ts = \"\"\n\t\tif debug:\n\t\t\ts += \"n\"\n\n\t\tif self.n!= -1 and self.n < len(s):\n\t\t\ts += \" \"\n\t\tif len(self.a)!= len(s):\n\t\t\ts += \" \"\n\n\t\tfor i in range(len(self.n)):\n\t\t\ts += \" \" + \"%s\" % self.__dict__[self.a[i]]\n\n\t\tif debug:\n\t\t\tl = []\n\t\t\tl.append(\"(\")\n\t\t\tl.append(PrettyPrint(self"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tdata = self.__str__()\n\tdata = self.__pretty__(indent, data)\n\tdata = self.pretty_defaults(data, indent, debug)\n\treturn data\n\n\n\n@precondition\ndef pretty(d):\n\t\"\"\"\n\tConvert a scalar expression into a pretty representation.\n\t\"\"\"\n\tif d is None:\n\t\treturn \"\"\n\telse:\n\t\treturn str(d)\n#\tprecondition(d.is_Add, \"Add must be evaluated as a Mul. Was %s\"%d)\n\t\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ttmp = [self.__class__.__name__, '\\n']\n\ttmp.extend(['-' * indent +' '] * (3 - indent))\n\tfor n in self.__dict__.keys():\n\t\t# don't try to pretty self.__dict__.keys() if they're all constants\n\t\tif (hasattr(self.__dict__[n], '__module__')) and (\n\t\t\t(\n\t\t\t\thasattr(\n\t\t\t\t\tself,\n\t\t\t\t\t'_dict_' +\n\t\t\t\t\tn.replace(\n\t\t\t\t\t\t'.',\n\t\t\t\t\t\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ttext = self._pretty_repr()\n\ttext = text.replace('None', 'null').replace('-', '-')\n\ttext = text.replace('=', '=').replace('\\'', '\"\"\"')\n\ttext = text.replace('\"', '\". \"').replace('?', '?\"').replace('<', '< ').replace('>', '>').replace('(', '< ').replace(')', '>')\n\ttext = text.replace('\"', '\"\"\"').replace('?', '?').replace(\"\\n\",\"\")\n\t"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\tif not args:\n\t\treturn ([])\n\t\t\n\texpA = []\n\texpB = []\n\tfor x in args:\n\t\tif not isinstance(x, basestring):\n\t\t\texpA.append(x)\n\t\telse:\n\t\t\texpB.append(absorb(x))\n\t\t\t\n\treturn expB+expA\n\ndef reduce_abs(expA = None, expB = None):\n\t\"\"\"\n\tReduce (A | B) by (B | A). In a sense this removes B and A and returns B. "}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\ttuple_to_str = {\n\t\tabs('And'),\n\t\tabs('A & B'),\n\t\tabs('A | B'),\n\t\tabs('A & (~B | Z) = A | ~B'),\n\t}\n\tif args:\n\t\t# remove any previous occurrences\n\t\targs = [\n\t\t\tsub(sub(abs, get(args))[1:], get(args, '=')), \n\t\t\tget(args, '~'),\n\t\t\tget(args, '&'),\n\t\t\tget(args, '|'),\n\t\t]\n\t\t# replace the remaining"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\treturn [X for x in [i for i in args if i] if not x.negative]\n\n\nclass PsiAbsorb(Absorb):\n\t\n\tdef absorb(self, x):\n\t\treturn self.absorbed\n\n\nclass PsiNegAbsorb(NegativeAbsorb):\n\t\n\tdef absorb(self, x):\n\t\treturn self.negated\n\n\nclass PsiExpr(Expression):\n\t\n\timpl = PsiAbsorb\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\tExpression."}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tresults = self.apply(args)\n\tif results.is_empty(): return []\n\telse: return results #~\n"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\tresults = []\n\tfor arg in args:\n\t\tif arg.is_composit(1):\n\t\t\tresult = str(arg) == \"~\"\n\t\t\t\n\t\telif not arg.is_composit(1):\n\t\t\tresult = self.abs_neg_abs(arg)\n\t\t\n\t\telif result == False:\n\t\t\tresults.append(arg)\n\t\telse:\n\t\t\tresults.append((arg, True))\n\t\n\treturn results\n\t\n\t\ndef abs_neg_abs(express):\n\t\"\"\"\n\tGiven an absorption"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\t# Get the expression from the right and evaluate.\n\texpr = reduce(lambda x, y: x & (y | True), ast.symbols(args))\n\t\n\t# Check if not done.\n\tfor expr in [\"A & (A | B)\", \"A & (~A | B)\"]:\n\t\t\n\t\tresult = evaluate(expr, _env=_env)\n\t\t\n\t\t# Check if we found the right expression.\n\t\tif result is True:\n\t\t\treturn expr.factory(result, _env=_env)\n\t\telif result is"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif len(args) < 2:\n\t\treturn []\n\t\t\n\telif len(args) == 2:\n\t\treturn [self.absorb(args[0], args[1])]\n\t\t\n\telif len(args) == 3:\n\t\treturn [self.absorb(args[0], args[1], args[2])]\n\n\telse:\n\t\treturn [self.absorb(args[0], args[1], args[2]), self.absorb(args[0], args[1])]\n\t\t\n\t\ndef getabsol(args"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t#if not args:\n\t#    return []\n\t#\n\treturn [s or s for s in args if s or s]\n\n###\n### SORTING\n###\n### \n###\n#def _compare_by_weight(v):\n#    return v[0] or v[1]\n\n\"\"\"\ndef sort_by_weight(exp):\n    _idx = bisect_left(exp)\n\n    if _idx == len(exp) or exp[_idx] > exp[-1]:\n        exp.insert(0"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn self._apply(\n\t\tlambda expr, t: (t[0], expr.rel_abs(*t, self))\n\t\t, tuple(args))\n\t\ndef _apply(f, args):\n\t\"\"\"\n\tApply `f` to arguments `args`.\n\t\"\"\"\n\tdef wrapper(s):\n\t\ta = args\n\t\ttry:\n\t\t\tif len(a) >= 2:\n\t\t\t\tf(*a)\n\t\t\telse:\n\t\t\t\tf(None, *a)\n\t\texcept AttributeError:\n\t\t\tpass\n\t\treturn s\n\treturn wrapper\n"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tnewargs = []\n\tfor a in args:\n\t\tif isinstance(a, Absode):\n\t\t\tnewargs.append(abs(a))\n\t\telif isinstance(a, AbsodeNegate):\n\t\t\tnewargs.append(abs(abs(a)))\n\t\telse:\n\t\t\tnewargs.append(a)\n\treturn newargs\t\t\n\t\t\n\t\t\ndef get_rel(x):\n\tfrom math import cos, -sin, pi\n\treturn (cos(x)/2, sin(x)/2));\n\nclass AbsodePos():\n\t\n"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif type(hook) is not HookType:\n\t\traise ValueError(\"{!s} function expects hooks and has type {!s}\".format(hook, type(hook)))\n\treturn hook\n\n\nclass Hook(object):\n\t\"\"\"\n\tWrapper object to enable registering listeners for a hook.\n\t\n\tArgs:\n\t    hook (HookType): Hook to register the function for.\n\t\"\"\"\n\n\tdef __init__(self, hook):\n\t\t\"\"\"\n\t\tInstanciate the list of listeners for the hook.\n\n\t\tArgs:\n\t\t    hook ("}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tclass Hook(object):\n\t\t__metaclass__ = ABCMeta\n\t\tdef __call__(self):\n\t\t\treturn on\n\t\t\n\t\t@abstractmethod\n\t\tdef add(self, *args, **kwargs):\n\t\t\tpass\n\t\t\n\treturn Hook(hook)"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tself.listeners[hook] = []\n\treturn callable(hook)\n\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef custom_hook(*args, **kwargs):\n\t\t\"\"\"\n\t\tCallback to be used in hook handler if a specific HookType is\n\t\tnot registered.\n\t\t\n\t\tArgs:\n\t\t    hook (HookType): Type of hook where this handler was added.\n\t\t\n\t\tReturns:\n\t\t    function: Handler to be executed in the specified hook.\n\t\t\"\"\"\n\t\treturn hook(*args, **kwargs)\n\treturn custom_hook\n\ndef register():\n\t\"\"\"\n\tRegister the hooks for the currently open file and the handler for\n\ta certain hook.\n\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif hook in self.hooks:\n\t\treturn self.hooks[hook]\n\telse:\n\t\tself.hooks[hook] = decorator(hook)\n\t\treturn self.hooks[hook]\n\ndef on_shutdown(self):\n\t\"\"\"\n\tShutdown function. Any leftover listeners will be lost on shutdown.\n\t\n\tReturns:\n\t    None\n\t\"\"\"\n\tif self.running:\n\t\treturn\n\tself.running = True\n\tfor _ in range(0, self.num_of_hooks):\n\t\tself.h"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef _decorator(self, fn):\n\t\tself.__add(hook, fn)\n\t\treturn fn\n\t\n\treturn _decorator\n\t\nclass HookedEvent:\n\t\"\"\"\n\tEvent manager object with hooks to handle any event.\n\n\tAttributes:\n\t    listener (HookType): Hook that is being handled.\n\t    signature (SignatureType): Signature that is being handled.\n\t\"\"\"\n\t\n\tdef __init__(self, listener, signature):\n\t\tself.listener = listener\n\t\tself.signature = signature\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n\tdef on_fn(func):\n\t\tif hook.on is None:\n\t\t\treturn\n\t\t\n\t\t\n\t\tdef register(event_name, func):\n\t\t\t\n\t\t\thook.handlers[event_name] = func\n\t\t\t\n\t\t\treturn func\n\t\t\n\t\treturn register\n\t\n\treturn on_fn\n\nclass Logger(Log.Logger):\n\t\n\tdef debug(self, message, *args, **kwargs):\n\t\t\n\t\tself.logger.debug(message, *args, **kwargs)\n\t\t\n\tdef info(self"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tself._addHandler(hook)\n\treturn hook\n\n\nclass EventListener(object):\n\tdef __init__(self, fn):\n\t\tself._hook = fn\n\tdef __get__(self, instance, cls):\n\t\treturn fn(instance)\n\tdef __call__(self, *args, **kw):\n\t\treturn self._hook(*args, **kw)\n\tdef __del__(self):\n\t\tself._hook = None\n\n\ndef registerHandlers(hooks):\n\tfor i,hook in enumerate(hooks):\n\t\tif hook"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tif getattr(hook, 'listener', False):\n\t\t\treturn hook(*args, **kwargs)\n\t\telse:\n\t\t\treturn self.register(hook, *args, **kwargs)\n\twrapper.__doc__ = Hook.on.__doc__\n\treturn wrapper"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tself._listeners[hook] = None\n\treturn hook\n\n\t#-----------------------------------------------------------------------\n\t# Public API\n\n\tdef register(self, hook, handler):\n\t\t\"\"\"\n\t\n\tRegister a handler for the specified hook.\n\t\n\t\tArgs:\n\t\t    hook (HookType): Hook attribute for which to register the\n\t\t\t    handler; \n\t\t    handler (mapper): Handler for which to register the handler.\n\t\t\n\t\tReturns:\n\t\t    instance: The corresponding handler to use. None if no handler was\n\t\t\t    to be set.\n\t\"\"\"\n\t\tif"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn{\n\t\t\"etcd_address\": \"http://{host}:{port}/\".format(\n\t\t\thost=etcd_host,\n\t\t\tport = etcd_port),\n\t\t\"etcd_password\": \"password\",\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port}\n\t\n\t\ndef basic_auth(user, username, password):\n\treturn{\n\t\t\"username\": username,\n\t\t\"password\": password}\n\t\n\t\ndef base_config(user, etcd_host"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = dict(\n\t\tusers = user,\n\t\tsecret = \"secret\"       # the secret to generate credentials based on.\n\t)\n\tif etcd_host.lower().startswith(\"http\"):\n\t\tetcd_host = \"http://\"+etcd_host\n\telif etcd_host.lower().startswith(\"https\"):\n\t\tetcd_host = \"https://\"+etcd_host\n\tconfig.update(\n\t\t{\n\t\t\t\"._etcd.host\" : etcd_host,\n\t\t\t\"._etcd."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t# Set the values and create a configuration with them\n\tuser = user.lower()\n\tetcd_host = etcd_host.lower()\n\tetcd_port = etcd_port\n\tconfig = {\n\t\t'user': user,\n\t\t'etcd_host': etcd_host,\n\t\t'etcd_port': etcd_port\n\t}\n\t\n\t# Set the values of the parameters\n\tconfig['password'] = bcrypt.hashpw(user.encode(\"utf-8\"), password.encode(\"utf-8\"))"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {\n\t\tu'auth.token': token,\n\t\tu'auth.user': users[user],\n\t\tu'etcd_host': etcd_host,\n\t\tu'etcd_port': etcd_port,\n\t}\n\treturn config\n\t\n\t\ndef main():\n\t# Create an Authentication Service\n\tauth_service = ttypes.authentication_service.AuthenticationService(auth_service_url, etcd_url)\n\n\tprint \"Getting etcd data from\", etcd_url, \", going straight there\"\n\t\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tfrom flask import Flask, jsonify, request\n\t\n\tapplication = Flask(__name__)\n\tapplication.config.from_envvar(\"FLASK_CONFIG\", silent=True)\n\tapplication.config['DEBUG'] = True\n\tfavicon_path = application.config['STATIC_ROOT'] + \"/favicon.png\"\n\t\n\tdef get_default_value(default):\n\t\treturn default.lower().replace('%', '').replace('+', '')\n\t\n\t@favicon.url_default\n\tdef favicon"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t#set the default value, or create a new one\n\tconfig = {}\n\tif 'key' not in config:\n\t\tconfig['key'] = 'database_key'\n\tif 'value' not in config:\n\t\tconfig['value'] = 'random'\n\t\n\t#set the db url\n\tconfig['database_url'] = 'file:' + user + ':' + 'localhost:2379'\n\t\n\treturn config\n\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tself.user=user\n\tself.etc_host=etcd_host\n\tself.etc_port=etcd_port\n\tself.config={\"database\":{'user':self.user, \n\t\t\t\t\t\t\t'spec':{\"password\":self.password}}, \n\t            'tenant':{\"auth\":[(\"test-user\", \"test-pass\")]}\n\t               }\n\treturn self.config\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\n\t\t'user': user,\n\t\t'etcd_host': etcd_host,\n\t\t'etcd_port': etcd_port,\n\t\t'cassandra_password': {\n\t\t\t'hosts': {}\n\t\t}\n\t\t}\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tuser_info = user_info(user)\n\tconfig_file = conf_for(config=etcd_host, etcd_host=etcd_host, etcd_port=etcd_port, user=user, password=user_info['password'], ca_cert=CA_CERT_PATH)\n\tdefault_config = config_for(etcd_host=etcd_host, etcd_port=11100, etcd_host='localhost:24700', etcd_port=24700, user=user, password=user_info['password"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn dict(\n\t\tdb=dict(\n\t\t\tusers=[dict()], \n\t\t\tnodes=[{'name': user, 'address': etcd_host}],\n\t\t\tpassword=[dict()]  # default\n\t\t), \n\t)\n\n@app.route(\"/\")\ndef display():\n\t\"\"\"\n\t\tDisplays all the registered users.\n\t\t\n\t\tArgs:\n\t\t    None\n\t\t\n\t\tReturns:\n\t\t    dict: the display of all registered users.\n\t\t\"\"\"\n\treturn render_template(\"index.html\")\n"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=timezone(dt.tzinfo))\n\tif dt.tzinfo is timezone.utc:\n\t\tif self._state == datetime.state.fold:\n\t\t\traise ValueError('Ambiguous value %s:  \"%s\",  expected '\n\t\t\t\t'in timezone %s in timezone %s'\n\t\t\t\t% (dt, self.strptime(date2tstr(dt), \"YYYY-MM-DD\"), \n\t\t\t\t\tself._tz, self._"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif isinstance(dt, str):\n\t\tfrom time import strptime\n\t\tdt = strptime(dt, \"%Y-%m-%d %H:%M:%S\")\n\n\treturn dt.replace(tzinfo=UTC)\n\n\nclass FoldingState(object):\n\t\"\"\"\n\tAn improved `list`-like state machine, used by `datetime.strptime` to make\n\t`fold`-enabled, `foldable`-disabled, and `to_string`-sensitive state\n\tmachine work.\n\n\tStateMachine provides the ability"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# if a timezone, we will start at the first occurence and end on that\n\tif isinstance(dt, self._tzinfo):\n\t\traise ValueError, \"Cannot assign UTC to non-datetime value\"\n\t# we will stop before the first occurrence to make sure we don't\n\t# get a datetime spanning more than 1 year\n\tts = self._ts\n\tif ts is None:\n\t\traise ValueError, \"No timezone specified\"\n\tif issubclass(dt.__class__, datetime):\n\t\t_ts = ts\n\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# pylint: disable=not-callable\n\treturn pytz.utc.localize(dt).astimezone(self)\n\n"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\td = datetime.min\n\t\n\t# This is because we don't know the datetime object's timezone.\n\t# It returns null in this case.  So if we want to know where we\n\t# arrive from when calling our parent, check if we know the\n\t# first occurrence of d.\n\ttry:\n\t\treturn d.replace(tzinfo=pytz.utc)\n\texcept ValueError: # No timezone, so d is a timestamp\n\t\t\tpass\n\t\n\t# We did not know the epoch, so either we've been in t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# Check whether the timestamp is in the future.\n\t# If not (and if we're in a \"real domain\" or if you're curious),\n\t# add the day.\n\t# TODO: This can become a bit more sophisticated, if ever\n\t#      - we can check whether the timestamp is in the past\n\tif dt.year < 1900:\n\t\tt = datetime.fromtimestamp(dt.timestamp())\n\t\tdt = _fromutc(t) + _day(t, dt.minute)\n\t\n\t# If we're"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\timport _time\n\tt = dt.timetz()\n\tif t.tzinfo is None:\n\t\treturn _time.mktime(t.timetuple())\n\telse:\n\t\treturn t\n\t\n\nclass datetime(object):\n\t\"\"\"\n\tThis is a proxy for a ``datetime`` object in ``local`` time. It can be\n\tuseful mainly for functions that want to work with ``datetime`` objects,\n\tand need to know what timezone is being used (e.g. to implement a\n\tUTC datetime).\n\t\n"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# The ambiguous datetime must be entirely before the datetime\n\t# in question.\n\tdelta = timedelta(days=(-1 * dt.weekday()))\n\t\n\t# See if the ambiguous datetime is an unambiguous datetime.\n\t# If it's possible, then we treat this case as having made\n\t# a fold, and the caller can consider it as an ambiguous\n\t# datetime if the next datetime is later than the ambiguous\n\t# one in a particular folder.\n\t# If neither of these happen, then this is an unambiguous"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt is None:\n\t\traise ValueError, \"Cannot call fromutc() with None as a parameter\"\n\tself._dt = dt\n\t\t\n\t# We can't just use get_state('ambiguous') because this case\n\t# would cause us to call fromutc() twice.\n\t# This is a problem for the end-user who might have set ambiguous\n\t# to True, which can be used as a placeholder for not having seen\n\t# an ambiguous datetime before. Hence, let's assume a date can\n\t# represent both an ambiguous datetime"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\traise NotImplementedError()\n\n\nclass Timetable(AbstractTimetable):\n    \"\"\"\n    The class representing a Timetable. This class is only responsible for\n    parsing a YAML dump of the Timetables section.\n    \n    :param yaml_dump_file:\n        The name of a YAML dump file to parse. If the file doesn't yet exist\n        then it's assumed that a YAML dump file with a timestamp as the first\n        item is assumed. If the timestamp is present, then it's assumed that\n        that"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(days=self.days, hours=self.hours, minutes=self.minutes)\n\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(self.months, self.days)\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta( **dict((i, rel.value) for i, r in zip(self[\"seconds\"], self[\"nanos\"])))"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(self.microseconds, self.seconds, self.days, self.hours, self.minutes)\n\t\n\t\ndef get_first_non_null_value(self):\n\t\"\"\"\n\tReturn the first non-None value, which may possibly be: None, False or an integer\n\t\"\"\"\n\tif self.__class__ == relativedelta:\n\t\treturn self.days, self.hours, self.minutes\n\ttry:\n\t\tassert( len( self.__class__._values ) == 2 )\n\t\treturn self.__"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(**self._attributes)\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\treturn self._normalized()\n\t\n\t\n\t\n\t\n\n# Tested classes\n\n__test__ = lambda self: None"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\ttry:\n\t\treturn relativedelta(**self.__dict__)\n\texcept ValueError:\n\t\treturn super(relativedelta, self).normalized()\n\n\n\n\n\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t# TODO if it is a subclass of relativedelta then this is wrong\n\treturn relativedelta(\n\t    **{\"days\" : int(sum(int(x) for x in self.days.values()) / 10**6) / 6e3,\"hours\" : int(sum(int(x) for x in self.hours.values()) / 10**6) / 6e3})\n\ndef normalized_d(self):\n\t\"\"\"\n\tThe :meth:`normalized_d` method is equivalent to\n\t:meth:`normalized`.\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta.relativedelta(**{\n\t\tfield: relativedelta.relativedelta.numeric() if attrs.has_key(field) else relativedelta.relativedelta.numeric()\n\t\tfor field in set(self)]\n\t})\n\t\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\ttry:\n\t\treturn relativedelta.fromordinal(self.__ordinal())\n\texcept ValueError:\n\t\treturn relativedelta(years=self.years, months=self.months,\n\t\t\t\tweeks=self.weeks,\n\t\t\t\tdays=self.days,\n\t\t\t\thours=self.hours,\n\t\t\t\tminutes=self.minutes,\n\t\t\t\tseconds=self.seconds,\n\t\t\t\tmicroseconds=self.microseconds)\n\nclass PeriodIsoString(BaseString, relativedelta):\n\t\"\"\"\n\tRepresent an integer Period object in ISO string format."}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef py2_decode(*args):\n\t\treturn u''.join(map(chr, args))\n\t\t\n\t@staticmethod\n\tdef py2_encode(*args, **kw):\n\t\tif type(args[1]) == bytes:\n\t\t\targs = map(chr, args)\n\t\telse:\n\t\t\targs = map(chr, args)\n\t\treturn args\n\t\t\t\n\tdefault_locale = '_UH_'\n\tdef tzname_in_python3(namefunc):\n\t\tdef decorator(func):\n\t\t\tname"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef __unicode__(self):\n\t\traise NotImplementedError\n\nclass dTZ(tzinfo):\n\tdef utcoffset(self, dt):\n\t\treturn timedelta(0)\n\n\tdef tzname(self, dt):\n\t\treturn None\n\ndef get_tzfile(tz):\n\t\"\"\"\n\tReturn the datetime object associated with tz returned from get_time()\n\t\n\tUse dTZ() if you want to get a datetime that is in the same timezone as\n\tyour input, rather than in the opposite"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\ttry: \n\t    # Test if name was given as string or bytes\n\t    return namefunc()\n\texcept TypeError:\n\t    # String-string comparison failed\n\t    try:\n\t        # Convert to byte-string and then call original namefunc()\n\t        return namefunc(bytearray(namefunc()))\n\t    except UnicodeError:\n\t        # If we get here it means the caller passed unicodename()\n\t\treturn unicode(namefunc())\n\n# Import the tzname and tzoffset functions, using the first argument\nif"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif sys.version_info.major == 3:\n\t\treturn namefunc(unicode(namefunc(b)))\n\telse:\n\t\treturn str(namefunc(bytes(namefunc(namefunc(b))))).decode('utf-8')\n\t\n\tregex = re.compile('[ ]{1,4}') # ensure that any extra group of whitespace/tabs get eaten\n\treturn regex.sub(' ', tzname_in_python2(namefunc))\n\t\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif sys.version_info < (3,0):\n\t\treturn bytes(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc(namefunc((namefunc)(namefunc(namefunc)(namefunc)(namefunc)(namefunc) \\\n\t.repr(namefunc), namefunc)(namefunc)))))))) \\\n\t.repr(namefunc"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif not isinstance(namefunc, (bytes, str)):\n\t\treturn bytes(namefunc())\n\telse:\n\t\treturn namefunc\n\t\n\n#end class\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef namefunc(self):\n\t\tif issubclass(type(self), basestring):\n\t\t\treturn struct.unpack(\">L\", six._binary_type(self))[0]\n\t\treturn struct.unpack(\">B', six._binary_type(self))[0]\n\treturn namefunc\n\t\t# return tzname.tzname\n\ndef _set_timezone_now():\n\t\"\"\"set the timezone when we do a select()\"\"\"\n\tif selectable.has_attr(time(),\"tzname\"):\n\t\tset_timezone"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n\t\n\ndef tzname_in_python2(namefunc):\n\tfrom functools import wraps\n\t\n\t_ = namefunc\n\t\n\tdef _wrapped(*args, **kwargs):\n\t\treturn namefunc(args[0], *args[1:], **kwargs)\n\t_wrapped.__doc__ += \"\\nChange unicode output into bytes in Python 2\"\n\t\n\treturn wraps(namefunc)(_wrapped)\n\t\nclass Timezoomer(pygtk.Window):\n\t\"\"\"\n\t\tA custom window class"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef cvt(dt):\n\t\treturn str(dt).encode('utf-8')\n\tnamefunc(cvt)\n\t#return cvt\n\n\t@pytest.fixture\n\tdef namefunc(self, request):\n\t\tdef namefunc(**kwargs):\n\t\t\treturn kwargs.get('namefunc', tzname(kwargs['tzname']))()\n\n\tprint 'tzname_in_python2:', namefunc.__name__\n\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif hasattr(namefunc, '__name__'):\n\t\tnamefunc = namefunc.__name__\n\treturn lambda x:namefunc(unicode(x))\n\n\n###\n### Testing/UT\n###\n\nimport unittest\n\nclass TestPyTznameI(unittest.TestCase):\n\t\n\tdef setUp(self):\n\t\tself.data = '\\x0', 'UTC', 'GMT', '-0800', 'UTC+0200', 'TZA+08:00', 'TZD', '+0800', \"US"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t#Find out what versions are supported\n\tversions = super(libvirtConnect, self).get_versions()\n\tversions['version'] = \"unknown\"\n\tif xapiVersion is not None:\n\t\tif int(xapiVersion) >= 10:\n\t\t\tversions['version'] = \"10\"\n\t\telif int(xapiVersion) >= 9:\n\t\t\tversions['version'] = \"9\"\n\t\telse:\n\t\t\tversions['version'] = \"9.0\"\n\n\t#Get version\n\tversions['distribution'],\n\tversions['revision'] = get_version"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif os.path.exists('git_version'):\n\t\twith open('git_version') as f:\n\t\t\treturn f.read().strip()\nelse:\n\tdef get_version():\n\t\tversioninfo = get_versions()\n\t\treturn [(r['name'], r['version']) for r in versioninfo]\n"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tmyVersion = getSource()\n\tver_re = '^__version__ =.*?$'\n\tmatches_re = re.compile(ver_re)\n\tmatch = matches_re.match(myVersion)\n\tif match:\n\t\t# remove potential leading/trailing whitespace\n\t\tmyVersion = match.group(1)\n\tif os.path.isfile(myVersion):\n\t\tdistdir = os.path.dirname(os.path.realpath(myVersion))\n\t\tdistfilename = os.path.basename(distdir"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tver = get_versions()\n\tif'version' in ver:\n\t\treturn ver['version']\n\treturn ver\n\n_exports = [\"get_versions\"]\ndef _export(dest,ver_type):\n\tdef decorator(func):\n\t\tfor s in func.__code__.co_consts:\n\t\t\tif s.__module__ in (_exports, _exports+['get_versions']):\n\t\t\t\tcontinue\n\t\t\tif isinstance(s,_exports) and s in ver_type:\n\t\t\t\tdest = \"%s.%s\" %"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif os.access(\"/usr/bin/pypy\"  or\n\t\t\t\t  os.access(\"/usr/bin/python\"):\n\t\t\t\t\treturn tuple(sys.version_info[:2])\n\t\t\t\t  else:\n\t\t\t\t\treturn()\n\n\tdef _version():\n\t\t\"\"\"Return the version line string\"\"\"\n\t\timport re\n\t\tm = re.search(r\"^#[0-9A-F]{7,}\\s*$\", sys.version, re.M)\n\t\tif m:\n\t\t\treturn m.group()\n\n\ttry:\n\t\twith"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\trepo = 'libpcap'\n\n\tresult = {'status': None}\n\n\tif os.path.isfile(os.path.join(pcap_dir, repo,'revision')):\n\t\twith open(os.path.join(pcap_dir, repo,'revision'), 'r') as f:\n\t\t\tresult = revision(f)\n\telse:\n\t\twith open(os.path.join(pcap_dir, repo,'revision'), 'wb') as f:\n\t\t\tf.write(str(__revision__))\n\n"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tlocal = get_local_env()\n\ttry:\n\t\tver = version[0]\n\t\tversion=version[1]\n\texcept IndexError:\n\t\treturn None, local\n\tver_format = \"%(version)s-%(release)s\"\n\ttry:\n\t\tver=ver_format % local # local environment overrides settings\n\texcept ValueError:\n\t\tpass\n\treturn ver, local\n\n\ndef update_env(**kwargs):\n\t\"\"\"\n\t\tThe main entry point of the update function.\n\t\"\"\"\n\ttry:\n\t\tfrom dj"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\tif(not os.path.isdir(version_dir)):\n\t\t\tos.makedirs(version_dir)\n\t\t\n\t\tf = open(os.path.join(version_dir, \"VERSION\"), \"r\")  \n\t\t\n\t\tfor line in f:\n\t\t\td = line.rstrip().split(' = ')\n\t\t\t\n\t\t\tif(d[0][:len(d[0])-2] == s):\n\t\t\t\tversion[str(d[0])]['version'] = d[1]\n\t\n\t\t\t\n\texcept"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tdefault = '0.0.0'\n\ttry:\n\t\tp = reldir('.DS_Store', default='v2.10')\n\t\treturn p.version\n\texcept IOError:\n\t\treturn default\n\ndef main():\n\t\"\"\"\n\tRun here\n\t\"\"\"\n\targs = get_args()\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--version', action=\"store_true\")\n\toptions = parser.parse_args()\n\tif not options.version:\n\t\toptions.version=True\n\tout"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tverdict = call([\"python\",\"server\",\"-v\",\"\"],env = {  \n\t\t\t\t\t  \"PATH\": path,\n\t\t\t\t\t  \"REQUESTLINE\": requestline, \n\t\t\t\t\t  \"API_BASE\": pathjoin(baseurl, \"api.json\")+\"?version=1\",\n\t\t\t\t\t  \"BASE_URL\": pathjoin(baseurl, \"api.json\"),\n\t\t\t\t\t})\n\tif(\"<json>\" in str(verdict)):\n\t\ttry:\n\t\t\tverdict = json.loads(verdict)\n\t\texcept:\n\t\t\tassert(0)\n"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\treturn render_string(\"%s-%s.%s\" % (name, version, style))\n\ndef render_string(string, context=\"auto\", output=\"auto\"):\n\t\"\"\"\n\tRender the given string with the given context into a\n\trendered string.\n\t\"\"\"\n\tif not string:\n\t\treturn \"\"\n\tif not isinstance(string, str):\n\t\tstring = unicode(string).encode(\"utf-8\")\n\treturn html_to_fragment(str(string) if output!= \"auto\" else string)\n\ndef is"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"keywords\": \"fluxcircles\", \"timestamp\": pieces.get(\"long\"), \"version\": pieces.get(\"short\"), \"full-revisionid\": pieces.get(\"dirty\")}\n\n\tif pieces[\"short\"] == \"origin\":\n\t\treturn {\"keywords\": \"fluxcircles\", \"timestamp\": pieces.get(\"date\"), \"full-revisionid\": pieces.get(\"dirty\")}\n\n\tif pieces[\"short\"] == \"release\" or (\"release\" in pieces and pieces[\"release\"] == \"0."}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tpass\n\ndef is_rendered(versionpath):\n\treturn os.path.exists(versionpath)\n\ndef updateRendered(versionpath):\n\t\"\"\"\n\tUpdates the rendered version of current dir.\n\t\"\"\"\n\treturn os.path.exists(versionpath) and is_rendered(versionpath)\n\ndef removeRendered(versionpath):\n\t\"\"\"\n\tRemoves the rendered version of current dir.\n\t\"\"\"\n\treturn os.path.exists(versionpath) and not is_rendered(versionpath)\n\ndef renderPath(version"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tassert not style.reversed(), \"Piece version reversals are not fully supported\"\n\n\t# determine the width of each side\n\twidth1, width2 = 0, 0\n\tfor part in pieces: # width1 is 0\n\t\tif part == \"all\": # all pieces\n\t\t\twidth1 = width2\n\t\t\twidth2 = 0\n\t\t\tcontinue\n\t\t\n\tfor p1, p2 in itertools.combinations(range(width1, width2, 1), 2):\n\t\tif p1 == p2: # this means that the pair is just a \"cross"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t# you can use your fancy\n\t# here do something, for example:\n\t# return (style, (maxwidth, maxheight), pieces)\n\n\tif pieces[-1][0] == 'M':\n\t\t# a b c\n\t\t(b, c, pieces) = pieces[-1]\n\t\twidth = c\n\t\theight = b\n\t\n\telse:\n\t\t# a b.c\n\t\t(b, c, width, height, pieces) = pieces[-1]\n\t\twidth = b\n\t\theight = c\n\n\t\n\tif pieces[0]["}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tnewstyle = False\n\tif style == \"default\":\n\t\tnewstyle = True\n\t\tstyle = \"expanded\"\n\n\tif newstyle:\n\t\tbits = pieces.split('/')\n\t\tpieces = \"\"\n\t\tfor bit in bits:\n\t\t\tpieces += '{}/{}'.format(hashlib.md5(pieces.encode(\"utf-8\")).hexdigest(), bit)\n\n\twidth = 24 * inch*inch;\n\thalfwidth = width / 2.0\n\tlines = [\"\", \"\", \"\", \"\"][:6]\n\n\tfor line"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]!= None:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\"dirty\": None,\n\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\"date\": None}\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"\n\t\tif pieces[\"closest-tag\"]:\n\t\t\tstyle = \"closest\"\n\t\telse:\n\t\t\tstyle = \"tag\"\n\t\tif style == \"tag\":\n\t\t\tif pieces[\"short\"]==\"g\" or pieces"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\tif pieces[\"style\"]!= \"all\" and not style:\n\t\tstyle = \"\"\n\t\tweinig = \".\"\n\t\tpieces[\"style\"] = \"\"\n\n\tif style!= \"-\":\n\t\tweinig = \".\"+style\n\t\t\n\t# Compute the width of the icon to render\n\ttw = 12\n\t\n\t# If the image is larger than the line then cut off the middle\n\tif tw >= 18:\n\t\tif tw <= 24:\n\t\t\tbom = \"...\"\n\t\telif tw <= 36:\n\t\t\tbom = \".bl\\xc"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\t# figure out which type of medium we have\n\tif pieces[\"type\"] not in (\"any\",\"GIT\",\"GITREV\",\"HGIT\",\"hg\",None):\n\t\t# this shouldn't happen\n\t\tprint \"PIPET: render() - bad piece (type=\"+str(pieces[\"type\"])+\")\"\n\t\treturn \"\"\n\t\n\t# choose a proper extension\n\tfrom bzrlib.plib import make_path_relative\n\treturn make_path_relative(self.dir, parts[1], part)\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tsavedStyle = Style.copy()\n\tsavedStyle.overrides['VERSION'] = style\n\ts = savedStyle\n\tassert not s.get('DATE')\n\tassert not s.get('SIZE')\n\tassert not s.get('NAME'), s.get('NAME')\n\t\n\tpieces = pieces.split('-')\n\tif s.get('NAME')!= s['DATE']:\n\t\t# We have a problem here, we should always have a Date on the first piece\n\t\tpieces.insert(0, s['DATE'])\n\t"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[\"closest-match\"]!= \"full\" and len(pieces) > 1:\n\t\t# release everything, since we'll use the full string\n\t\treturn pieces[\"closest-match\"]\n\telse:\n\t\treturn pieces[\"full\"]\n\t\t\t\ndef build(story, repo_ref, opts, tmp_dir, force):\n\t\"\"\"\n\tBuild a story in a temporary directory and return its path. The return\n\tvalue only makes sense if is_build_in_cache is False.\n\t\"\"\"\n\t# if this is not a build in cache,"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\tparts = pieces[\"closest-tag\"].split(\"/\")\n\t\tparts = parts[:parts.index(pieces[\"closest-group-id\"])]\n\t\treturn \"/\".join(parts)\n\telif pieces.get(\"next-tag\"):\n\t\treturn \"/\".join(pieces[\"next-tag\"].split(\"/\")) + \"/\"\n\telse:\n\t\treturn \"/\"\n\ndef get_versions(start_tag=None):\n\tcmd = \"git tag,gittag --template="}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[\"closest-tag\"] not in (\"+\", \"\"):\n\t\treturn \"+\"\n\treturn \".%s\" % pieces[\"closest-tag\"]\n\n\ndef compose(*pieces):\n\t\"\"\"\n\tReturn a + if we already have one\n\t\"\"\"\n\tfor i in pieces:\n\t\tif pieces[i] == \"+\":\n\t\t\treturn \"+\"\n\treturn \"%s\" % (\"+\".join(pieces))\n\n\ndef has_diff(d1, d2):\n\t\"\"\"\n\tCheck to see if a simple diff exists.\n\n\tArguments:"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not pieces:\n\t\treturn \"+\"\n\tif \"_\" in pieces[-1]:\n\t\t# splice together the pieces\n\t\tpieces.pop()\n\t\tpieces.insert(0, pieces[-1][:-len(pieces[-1])])\n\t\t# and join them up\n\t\ttrailing_sep = \"\"\n\t\tpieces.append(trailing_sep)\n\t\ttrailing_sep = \"/\"\n\telse:\n\t\t# single piece. break it and return it\n\t\tpieces.pop()\n\treturn \".\" + pieces.pop()\n\n\n########"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" not in pieces[0]:\n\t\tpieces[0] += \".0\"\n\treturn pieces[0]\n\ndef most_common(ls, k=1):\n\t\"\"\"\n\tReturn the k most common strings (1 to 100).\n\n\t\"\"\"\n\tres = []\n\tfor j, l in enumerate(ls):\n\n\t\tif not isinstance(l, str):\n\t\t\tcontinue\n\n\t\tif len(l) > k:\n\t\t\tres.append(l[0:k])\n\t\telse:\n\t\t\tres.append(l)\n\n\treturn res\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tresult = []\n\t\n\tif pieces[\"closest-tag\"]:\n\t\tclosest = pieces[\"closest-tag\"]\n\t\tclosest_match = False\n\t\t\n\t\t# we have two paths here, one with a closing tag, and one without\n\t\tif pieces[\"distance\"] < closest:\n\t\t\tclosest, closest_match = pieces[\"closest-tag\"], True\n\t\t\n\t\t# in all other cases, we have just one path (e.g. 7/8), and\n\t\t# we need to see what our closest match is\n\t\t"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[\"closest-tag\"] in (\"+\", \".\"):\n\t\treturn pieces[\"raw\"] + \"+\"\n\telse:\n\t\treturn \"+\" + pieces[\"closest-tag\"]\n\n\ndef auto_annotate(version=\"\", *args, **kwargs):\n\t\"\"\"\n\tLike patch.call(), but takes in an optional argument of \"all\".\n\t\"\"\"\n\n\t# get the annotated files\n\tfiles = []\n\tfor repo in version.split(\":\")[:-1]:\n\t\tif not repo:\n\t\t\trepo = \"origin\"\n\t\t"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[\"pieces\"][0] not in (\"+\", \".\"):\n\t\tv = \"\"\n\t\tfor x in pieces[\"pieces\"]:\n\t\t\tv += x + \" \"\n\t\tif not pieces[\"is_dirty\"]:\n\t\t\treturn \"+\" + v\n\t\telse:\n\t\t\treturn \".\" + v \n\n\tfor i, y in zip(sorted(\"'{}' - {'{}'}\".format(item, pieces[\"pieces\"][i]) for item in pieces[\"pieces\"][:len(pieces)]), pieces[\"pieces\"][len(pieces)]):"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not pieces:\n\t\treturn \"+\"\n\tif pieces[-1] == \"\":\n\t\treturn \".\"\n\treturn \"+\"\ndef render_size(size, textwidth):\n\tsize *= 1024 ** 2\n\tout = \"\"\n\tfor c in textwidth:\n\t\tout += chr(c)\n\treturn out[:size]"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not pieces:\n\t\tpieces = add_parity(base + \"+\", pieces)\n\treturn pieces[0] + \"+\" + pieces[1]\n\ndef set_language(language):\n\t\"\"\"Set which language is being used to render the status\"\"\"\n\tglobal language\n\tlanguage = language.lower()\n\tlanguage = language.replace(\"-\", \"\")\n\tlanguage = language.replace(\"/\", \"_\")\n\tlanguage = language.replace(\".\", \"_\")\n\tlanguage = language.replace(\"_\", \"-\" + language)\n\tlanguage = language.replace(\"-"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = (commands,)\n\n\tsub_commands = args[:]\n\tif verbose:\n\t\tsub_commands.append('stdout')\n\n\toptions = [ ]\n\tfor command in commands:\n\t\toption = None\n\t\tif hide_stderr or verbose:\n\t\t\tif isinstance(command, (list, tuple)):\n\t\t\t\toption = command[0]\n\t\t\t\tcommand = command[1]\n\t\t\telse:\n\t\t\t\toption = command\n\t\t\t\tcommand = None\n\n\t\tif"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\t_execute(commands, args, cwd=cwd, verbose=verbose, verbose_fail=verbose, hide_stderr=hide_stderr, env=env)\n\texcept KeyboardInterrupt:\n\t\tos._exit(0)\n\texcept SystemExit:\n\t\traise\n\texcept Exception as e:\n\t\tif (verbose or quiet): \n\t\t\te.stdout = e.stdout or \"\"\n\t\t\te.stderr = e.stderr or \"\"\n\t\t\tsys.stdout.write(\"Error:"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport re\n\tdef _cmd(cmd, args, cwd=None, verbose=False, hide_stderr=False):\n\t\t\"\"\"\n\t\tRun given command(s) with given arguments\n\t\t\"\"\"\n\t\tlog.debug(\"run command\", cmd)\n\t\ttry:\n\t\t\toutput = {}\n\t\t\targs = args.__class__(args)\n\t\t\tfor arg in args.strip().rstrip(' ').partition(' '):\n\t\t\t\tif shlex.split(arg) == shlex.shlex_split(cmd):\n\t\t\t\t\tif hide_stderr:"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not commands or (not isinstance(commands, list) and isinstance(commands, tuple)):\n\t\tcommands = [commands]\n\t\t\n\tfor command in commands:\n\t\tcwd = cwd or ''\n\t\tif not isinstance(command, string_types):\n\t\t\targs += command\n\n\t\twith PipedLogging() as stdout:\n\t\t\trc, out = run_command_for_each_file(\n\t\t\t\tshell=True,\n\t\t\t\tcommand=command,\n\t\t\t\targs=args,\n\t\t\t\tcwd=cwd,\n\t\t\t\tverbose="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tresult = None\n\tif isinstance(commands, basestring):\n\t\tcommands = [commands]\n\t\targs += [\"-N\"]\n\n\tenv = [] if env is None else list(env)\n\tenv.extend(sys.argv[1:])\n\n\tlog('\\nRunning %s\\n' % \" \".join(commands))\n\n\tif env:\n\t\tif isinstance(env, str):\n\t\t\tenv = env.split()\n\t\tenv.extend(args)\n\t\tpath = [_path] + list(env) +"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif args == []: \n\t\targs += ['--help', ]\n\tcwd=cwd or os.path.dirname(os.path.abspath(__file__)) # no os.pardir so it's the current directory\n\t# no cwd set yet.  I can use call(env) in a function, like call(cwd='')\n\n# the first argument of run_command() is passed as an extra argument to\n# the function, so pass it on to it\n\tdef _get_options(argv):\n\t\to = dict"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\tif env is None:\n\t\tenv = dict() # The set of environment variables for the command being run\n\t\t\n\tif isinstance(commands, string_types): \n\t\t# Convert a list of commands into a string, or a callable that does the\n\t\t# conversion\n\t\tcommands = [commands]\n\t\n\tif len(commands) == 0: # Run all commands in a single command\n\t\tcommands = ['bash', '-c', 'echo 1']\n\t\t\n\tif isinstance(commands, string_types): \n\t\tcomm"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\treturn cmd_call(commands, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\ndef check_output(*args, **kwargs):\n\t\"\"\"\n\t\tCheck the given arguments to see whether a running instance or a non-running instance\n\t\treturn (exit_code, output).\n\n\t\targs: The first three args are path to the working directory, instance, and the\n\t\t\tother arguments.  The last three args are the arguments that would be passed to\n\t\t\tthe command.\n\n\t\tenv:"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tstderr = []\n\tenv = env or {}\n\targs = args or None\n\tenv = env or {'VARN': 'VARN'}  # Set to pass the VARN env var\n\t\n\tif args is None:  # Special case, when args=None: use $VARN as the target, if possible\n\tstderr = _run_command(commands, env, args)\n\tif verbose:\n\t\t_display.debug('\\n'.join(stderr))\n\t\t\n\t# Return the stderr"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\treturn execute_command(commands, args, cwd, verbose, hide_stderr, env)\n\n\ndef get_outputs(outfile, cwd=None, verbose=False, hide_stderr=False):\n\t\"\"\"\n\t\tGets all the outputs for the given file or directory\n\t\"\"\"\n\tcmd = get_cmd(outfile, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr,\n\t\t\t\truncmd=True)\n\n\treturn execute_command([\"&\"], [cmd], c"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif not hasattr(config, 'VERSION'):\n\t\t# Read in the legacy config file and all the submodules.\n\t\tconfig_file = os.path.join(module_dir, 'neuralnet_deploy/neuronetsrc_module.yaml')\n\t\ttry:\n\t\t\t# yaml.safe_load() throws this if the file does not exist or cannot be parsed,\n\t\t\t# so we must panic here if it cannot be parsed.\n\t\t\tconfig_file = open(config_file)\n\t\t\tconfig = yaml.safe_load(config_file)"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = ProjectConfig(root, verbose=False)\n\tconfig.populate()\n\treturn config\n\nset_enforce(False)\n\n\nif __name__ == '__main__':\n\t\"\"\"\n\t\tTest case for our config script.\n\t\t\"\"\"\n\tconfig = get_config()\n\t# Run the tests\n\n\tif __name__ == '__main__':\n\t\timport nose\n\n\t\tdef _enforce_config(self, test_result):\n\t\t\tif _enforce(config.__dict__.has_key('config_enforce')):\n"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tmodule = sys.modules[__name__]\n\tif not hasattr(module, config_key):\n\t\t# First time around, make some settings.\n\t\tmodule.configure()\n\treturn getattr(module, config_key)\n\ndef _make_env_var_argspec(spec):\n\treturn [(w, i) for (i, w) in enumerate(spec)\n\t\t\t if spec[1] == 'arguments']\n\n"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = voms_config.Voms()\n\tconfig.load(None)\n\n\treturn config\n\n@task\ndef prerequisites(project, project_task):\n\tpass\n\n@task\ndef prerequisites_clean(project, project_task):\n\tpass\n\n@task\ndef prerequisites_merge(project, project_task):\n\t\"\"\"\n\tMerge prerequisites: the versioneer config file, project voms directory and its package.xml, \n\tpackage.xml generated by the previous task.\n\t\"\"\"\n\tlog = logger\n\n\t# Config"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# Use `get_config` so that the import error when running tests. Returns config, or false\n\treturn importlib.import_module(config_module).config\n\n\n\ndef get_version():\n\t\"\"\"\n\tGet the Versioneer's release branch.\n\t\"\"\"\n\timport inspect\n\tdist = importlib.import_module('distribute')\n\tversion = '.'.join(dist.RELEASE_VERSION.rsplit('.', 1)[0].split('=')[:2])\n\tif dist.RELEASE_TAG:\n\t\tversion += '+"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = v.VersioneerConfig(version_cmd)\n\tconfig.cmds['version_cmd'].append(0)\n\tconfig.cmds['upgrade_cmd'].append(0)\n\tconfig.cmds['develop_cmd'].append(0)\n\treturn config\n\ndef main(v):\n\timport logging\n\n\tlogging.basicConfig(level=logging.ERROR)\n\tconfig = get_config()\n\n\n\tif config['dev_mode']:\n\t\tdev_config = DevConfig()\n\t\tdev_version = get_version"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn cls('sentry.conf')\n\n\ndef validate(conf):\n\t\"\"\"\n\tEnsure strict Python version.\n\t\"\"\"\n\tconfig = get_config()\n\tif LooseVersion(str(config.verbosity)) >= LooseVersion('3.0.0'):\n\t\tconf['sentry:levels'] = dict(\n\t\t\tformat='auto',\n\t\t\tdefault='error',\n\t\t\tdesc='Level for logging debug messages.',\n\t\t)\n\tconf.env.prepend_path('PATH', config.path)\n\n\ndef load_config(conf):"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = ConfigParser.ConfigParser()\n\tconfig.read(configpath)\n\treturn dict(vcs_exe=config.get('DEFAULTS', 'vcs_exe'),\n\t\t\t\t\t\tupdate_every=int(config.get('DEFAULTS', 'update_every')),\n\t\t\t\t\t\tverbosity=int(config.get('DEFAULTS','verbosity')),\n\t\t))\n\ndef get_depends(vcs_exe):\n\tversioneer_config = VersioneerConfig()\n\tconfigpath = 'vcs_exe.cfg'\n\tconfig_file"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# Use interpolation to get the version nicely on the client system\n\tversion = interpolate.interpolate(\"\"\"VERSION = '%(version)s'\"\"\", {'version': version})\n\t# And write it on the server\n\tconfig = VersioneerConfig()\n\tconfig.version = version\n\treturn config\n\ndef make_options_test(option_class, name):\n\t\"\"\"\n\tCreate a test of a given option parser class.\n\tUsed for testing parser options in fixtures, where subcommands aren't\n\taccessed using setUpClass()\n\t\"\"\""}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig_dict = {}\n\n\t# Get the commands\n\tconfig_dict['version'] = v2\n\tconfig_dict['nodes'] = v2_node\n\tconfig_dict['services'] = v2_service_types\n\tconfig_dict['deprecations'] = [v1_dep, v2_dep, v3_dep]\n\n\t# Add specific modules to versioneer config\n\tconfig_dict['version'] = __version__\n\tconfig_dict['author'] = 'Paul'+ __author__.strip('\"') +'v'\n\tconfig"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(method):  # the method itself.\n\t\tvcs.register_vcs(method)\n\t\treturn method  # decorator returns it to the caller\n\treturn decorator\n# }}}\n\n# vim: expandtab tabstop=4 shiftwidth=4\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorate(fn):\n\t\t_wrappedFn = lambda *args, **kw: fn(*args, **kw)\n\n\t\t# the class has a get_vcs attribute of its own which returns the VCS implementation class\n\t\tvcsClass = fn.__class__\n\t\tvcsClass.get_vcs = lambda *args, **kw: vcs\n\n\t\t# the _wrappedFn function is a decorator function which adds the decorator object as a method\n\t\t_wrappedFn.__wrappedFn__ = vcsClass\n\t\treturn _wra"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(method):  # method\n\t\tif method is not None and method.__return__ is None:\n\t\t\tmethod.__return__ = method\n\t\treturn method\n\n\treturn decorator\n\n# =====================================================================================\n#    End of module\n# =====================================================================================\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(method):\n\t\t@wraps(method)\n\t\tdef wrapper(self, vcs_config):\n\t\t\treturn method(vcs_config)\n\t\treturn wrapper\n\tregister_vcs_handler(vcs, decorator)\n\n@register_vcs_handler('git')\ndef git_vcs(vcs):\n    \"\"\"\n    Add a handler that adds the git command into your call.\n    \"\"\"\n    return git_commands.add_git_command(vcs)\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef _decorate(method):\n\t\t\"\"\"\n\t\tInitialization.\n\t\t\"\"\"\n\t\t@wraps(method)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\treturn method(self, *args, **kwargs)\n\t\tvcs.register(method, wrapper)\n\t\treturn wrapper\n\treturn _decorate\n\n\ndef initialize_vcs_system():\n\t\"\"\"\n\tInitialize the VCS handler on any other VCS.\n\t\"\"\"\n\tif len(vcs.registry) == 0:\n\t\tvcs.register('git',"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef wrapper(f):\n\t\tvcs.register_handler(f)\n\t\treturn decorator(vcs.register_vcs_handler,\n\t\t\t\t\t\t   f,\n\t\t\t\t\t\t   method)\n\treturn wrapper\n\ndef register_post_hook(f):\n\t\"\"\"\n\tCreate decorator to mark a method as a post hook.\n\t\"\"\"\n\tdef wrapper(*args, **kws):\n\t\tvcs.register_post_hook(f)\n\t\treturn decorator(vcs.register_post_hook,\n\t\t\t\t\t\t   f,\n\t\t\t\t\t\t   None)\n\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t@wraps(vcs)\n\tdef wrapper(client, *args, **kwargs):\n\t\tif not '%s' % client.vcs:\n\t\t\tvcs(*args, **kwargs)\n\n\treturn wrapper\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t@wraps(vcs)\n\tdef wrapper(vcs):\n\t\treturn VCS(vcs, method)\n\treturn wrapper\n\n\nclass VCS(object):\n\t\"\"\"\n\tAn object that can be hooked.\n\t\"\"\"\n\t@classmethod\n\tdef get_repos(cls):  # wraps VCS get_repos\n\t\treturn cls.__repos_vcs\n\t@classmethod\n\tdef get_config(cls):  # wraps VCS get_config\n\t\treturn cls.__config_vcs\n\t@class"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tvcs_method = vcs(method)\n\tvcs_method.hook_name = method.__name__\n\treturn vcs_method\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.vcs = vcs\n\t\tfunc.method = method\n\t\treturn func\n\treturn decorator"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# make sure the number of valid objects is greater or equal than the \n\t# number of objects checked\n\tvalidation_hierarchy = self.check_number_of_hierarquents(\n\t\tself.validate_num_object_roots, validate_objects, \n\t\tself.validate_num_objects, check_digests, show_warnings\n\t)\n\n\t# return the number of objects checked so far\n\treturn validation_hierarchy\n\n\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\timport warnings\n\n\tself.logger.verbose('validate_hierarchy({},{})'.format(check_digests, show_warnings))\n\n\tif check_digests:\n\t\t# Check all object digests\n\t\tfor obj_item in self.root_item.itervalues():\n\t\t\tif not obj_item.check_for_digests():\n\t\t\t\tself.logger.error('The {} object failed the DSA validation. {}'.format(obj_item.hashable_item.name, obj_item.value))\n\n\t\t\t\tif self."}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tself.log.debug(\"validate_hierarchy():\")\n\t\n\thierarchy_id_list = []\n\tfor i in self.get_all_hierarchy(**self.hierarchy_spec):\n\t\tif (i[\"hierarchy_id\"]!= \"#\"):\n\t\t\thierarchy_id_list.append(i[\"hierarchy_id\"])\n\t\n\tnum_objects = 0\n\tchecked_objects = set()\n\tbad_objects = set()\n\tself.log.debug(\"hierarchy_id"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tnodes = self.__get_hierarchy_nodes()\n\tfor child in nodes:\n\t\tif not child.exists:\n\t\t\t# The node does exist, we have nothing in it\n\t\t\tcontinue\n\t\thash_key = child.hash_key_for_parent(child)\n\t\tif check_digests and hash_key == self.max_hash():\n\t\t\t# We checked too many files\n\t\t\tcontinue\n\t\tnum_objects += validate_object(root=child, root_hash=hash_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\treturn check_object_hierarchy(self.root, validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings, check_objects=True)\n\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\ttry:\n\t\troot_checker = get('HierarchyChecker')\n\t\troot_checker.validate_tree(\n\t\t\tinput_tree_root=self.get(\"storage_tree\", None),\n\t\t\tnum_objects=len(self.get(\"storage_tree\", None)),\n\t\t\tshow_warnings=show_warnings\n\t\t)\n\t\troot_checker.verify_tree(\n\t\t\tinput_tree_root=self.get(\"storage_tree\", None),\n\t\t\tnum_objects=len(self.get(\"storage_tree\", None)),\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tself.num_objects = 0\n\tself.good_objects = 0\n\t\n\tfor item in self.hierarchy:\n\t\t\n\t\tcheck_hash = hashlib.sha256(item.digest).hexdigest()\n\t\t\n\t\tif self.valid_objects & self.hierarchy_check_list.intersection((item,check_hash)):\n\t\t\tif show_warnings:\n\t\t\t\tprint(\"Found hash: \" + item.digest.hexdigest() + \" for \" + item.path.hexdigest())\n\t\t\tself."}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tvalid_paths = self.hierarchy\n\n\tif sys.platform == 'win32':\n\t\tvalid_paths = self.hierarchy.split(';') # split hierarchy on semicolon\n\t\n\tnum_objects = 0\n\n\tfor path in valid_paths:\n\t\tpath = path.strip()\n\t\tpath = os.path.normpath(path)\n\t\t\t\n\t\tif path.startswith('\\\\\\\\') or '/' in path: # windows drive\n\t\t\tpath = '/'\n\t\t\n\t\tif (path.endswith('*"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# get a list of the storage root objects\n\tstorage_roots = StorageRoot.objects.filter(state=HierarchyState.HALF)\n\tif show_warnings or check_digests:\n\t\tstorage_roots = StorageRoot.objects.exclude(state=HierarchyState.HALF)\n\t\n\t# validate each storage root against a storage object\n\tvalidate_objects = validate_objects or self.is_valid_object\n\tfor storage_root in storage_roots:\n\t\t# check each object against each storage root\n\t\tcheck_objects"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\t\n\tif validate_objects and not self.is_valid(None):\n\t\tnum_objects += 1\n\n\tif check_digests and not self.digest_exists(None):\n\t\tnum_objects += 1\n\n\tif show_warnings:\n\t\tprint(\"The following objects were identified as failing validation:\\n%d objects\" % num_objects )\n\t\n\treturn num_objects\n\t\n\t\n\t\n\t\n"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself._OCFL.set_dir = self._set_dir    \n\tself._OCFL.create_dir = self._create_dir  \n\n\tself.OCFLDir.set_dir(os.curdir())\n\tself.OCFLDir.create_dir()\n\n\tself.OCFLDir.set_dir('')\n\tself.OCFLDir.create_dir()\n\n\t# \n\t\n##\tself.fileList.append( 'OCFLFile.xml' )\n\n\nclass OCFLFile(OCFLNode):\n\t__doc__"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself._state = StorageRootState.initialize()\n\treturn self._state.state\n\n\t\n\tdef store_storage_root(self, state):\n\t\t\"\"\"\n\t\n\t\"\"\"\n\t\t \n\t\tif state is not None:\n\t\t\tself._state = StorageRootState.initialize(state)\n\t\treturn self._state.state\n\r\n\t@property \n\tdef state(self):\r\n\t\treturn self._state.state\n\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tfor child in self.children:\n\t\tchild.initialize(self)\n\n\tfor child in self.children:\n\t\tchild.write_to_storage(self)\n\n\tself.tree.move(self.root, start=True)\n\n\ttry:\n\t\tfor child in self.children:\n    \t\t\tchild.initialize()\n\n\texcept:\n\t\tlog.error('initialize(...) failed: not all children are initialized', exc_info=True)\n\t\traise\n\n@contextmanager\ndef run_async_execution(func, **kwargs):\n\t"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\treturn _ocfl.OcfStorageRoot_initialize(self)\n\t\n\t\n\t\nclass OcfstorageRootStatus(object):\n\t\"\"\"\n\tRepresents a OCFL storage root's status.\n\t\"\"\"\n\n\n\tdef __init__(self, p_location, p_objectType, p_lastModifiedTime):\n\t\t\"\"\"\n\t\tCreate and initialize a new OCFL storage root.\n\t\t\n\t\t@type  p_location: string\n\t\t@param p_location: OCFL storage root's location.\n\t\t@type  p_objectType: string"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\ttry:\n\t\tself.root = ocfl.OCFLStorageRoot()\t# a StorageRoot instance.\n\texcept ocfl.OCFLError, e:\n\t\traise ocfl.OCFLNotSupportedError(str(e))\n\tooc.DebugLevel = ocfl.DebugLevelLevel.NONE\n\tooc.Wrap = False\n\n\n\t# return a storage root object so other methods can\n\t# work with it (and should return it). you shouldn't\n\t# know if these methods will throw errors, and no\n\t# such method should"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\treturn self\n\n\t@property\n\tdef ocfl_root(self): \t\t\n\t\t\"\"\"\n\t\t\tThe root ocfl connection. This is created from the OCFL_ROOT environment variable.\n\t\t\tIt can be accessed either via the \"self\" property or via the underlying OCFL_ROOT connection string used above. \n\t\t\tIn either case this connection string must not change.\n\t\t\"\"\"\n\t\treturn self._ocfl_root\n\t\n\tdef on_connect(self):\n\t\t\"\"\"\n\t\t\tInitiate a connection to a OCFL connection. This can be executed in"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif not self._storage_root:\n\t\tself._storage_root = StorageRoot()\n\n\treturn self._storage_root\n\n\t\n\t# End\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.baseurl = \"http://\" + self.url\n\tsuper().initialize()\n\t\n\tself.path = os.path.abspath(\".\")\n\tself.indexfile = self.path.join(\"index.html\", \".html\")\n\n\t# Open the index in a sub-buffer to prevent OLE\n\t# problems with Unicode text.\n\tf = None\n\tf = urllib.request.urlopen(self.indexfile)\n\tdata = f.read().decode(\"utf-8\")\n\tf.close()\n\n\t# Remove"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tfrom.ocfl_storage_base_resource import OCFL_E_NO_ERR\n\n\tself.id = None\n\t\"\"\"\n\tId of the storage root.\n\t\"\"\"\n\n\t# OCFL_E_NO_ERR should be a valid 'err' code\n\tself.err = OCFL_E_NO_ERR"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif 'root' not in Config.config.get('OCFL', {}):\n\t\tConfig.config['OCFL'] = dict()\n\tConfig.config['OCFL']['root'] = self\n\tself.create_storage_roots()\n\tself.create_config()\n\n\ttry:\n\t\tself.config.set('OCFL', 'root', self.get_storage_root().get_url())\n\texcept Exception as e:\n\t\t# TODO: log somehow\n\t\traise\n\n\tif Config.config['GPHOTO'] not in ['None', 'local"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif not VERSION: return VERSION\n\n\tversion = VERSION.replace(\".\", \"\")\n\tif version[-1]!= \":\": version += \":\".join([str(i) for i in reversed(range(16))])\n\tversionlist = re.findall(r\"@(.+):(\\d+)\\.(.+)\", version)\n\n\tfoundv = None\n\tfor versionnum, regex, match in versionlist:\n\t\tif regex:\n\t\t\tr = re.compile(regex)\n\t\t\tm = r.search(version)\n\t\t\t"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\treturn version[:2]\n\n\ndef next_timestamp(delta):\n\t\"\"\"\n\tReturn timestamp after given version offset\n\n\tMust deal with version-dependent timestamp offset.\n\t\"\"\"\n\treturn timestamp_from_epoch_to_ts(delta).timestamp\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tnext_version = u\"{0}{1}\".format(version[\"major\"], version[\"minor23\"])\n\treturn next_version.split(\"{0}\")[-2]\n\t\ndef get_version(filename):\n\t\"\"\"\n\t\tget version from.cabal file.\n\t\t\n\t\tIf file doesn't exist, return None.\n\t\t\"\"\"\n\tif os.path.exists(filename):\n\t\twith open(filename,\"r\") as f:\n\t\t\tversion = f.readline()\n\t\treturn version\n\t\t\n\treturn None\n\n#get"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\traise NotImplementedError\n\ndef update_identifier(version, prefix=\"frc\"):\n\t\"\"\"\n\t\tUpdate next version identifier.\n\n\t\tThe returned identifer is compatible to an existing identifier\n\t\tversion as defined by {prefix}:\n\n\t\t- prefix == '': next is the latest, if applicable. default_next = True\n\t\t- prefix!= '': the next identifier must start with prefix. Otherwise\n\t\t\tnext is the latest.\n\t\"\"\"\n\traise NotImplementedError\n\n\ndef get_identifier(version):\n\t\"\"\"\n\t\tGet next identifier matching the current"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t# the pattern\n\tnew_version = 0\n\t\n\t# if the user has specified a version, use it\n\tif version:\n\t\tnew_version = version[1:]\n\t\t\n\t# otherwise get the latest version number (which is what we want)\n\telse:\n\t\tnew_version = get_latest_version()\n\t\n\treturn (new_version or 0.0)\n\n\nclass Credentials(object):\n\t\"\"\"\n\tCredentials manager that can be used per-user by the Omero.rest interface\n\t\"\"\"\n\n\t"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tfrom version import Version\n\tret_version = \"%04d-%3d\" % \\\n\t\t\t(min(len(v3_chars), len(version)),\n\t\t\t v3_chars.index(version[0]) + 1)\n\tif version < ret_version:\n\t\tret_version = version\n\treturn ret_version\n\ndef write_data(text, writer):\n\t\"\"\"\n\t\tWrite \"text\" to \"writer\".\n\t\t\n\t\t\"text\" is any string containing a \"-\" (non-terminated) numeric version\n\t\tnumber as given by"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\treturn int((max(version) + 1)//10)\n\ndef update_version(version):\n\t\"\"\"\n\t\n\t\"\"\"\n\tif len(version)!= len(\"%s.\" % (version,)):\n\t\tprint(\"Incompatible version\")\n\t\tprint(version)\n\n\t# split into words\n\tparts = version.split(\".\")\n\tparts = map(lambda x: \"%s.%s\" % (x,parts[len(parts)-1]), parts)\n\n\tparts[len(parts)-1] = next_version(parts[len(parts)-1"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\treturn (version.replace(\"-\", \"\") + \n\t\t\t\".\" + \".\".join(map(str, sys.platform)) + \n\t\t\t\".%s\" % int(os.path.basename(sys.argv[0])))\n\n\ndef find_version():\n\t\"\"\"\n\tFor backwards compatibility\n\t\"\"\"\n\treturn next_version(2)+\".%s\" % next_version(3)\n\n\ndef get_file(file_path):\n\t\"\"\"\n\tLook for a file under the same structure, \n\treturns (path, content"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif len(version) == 0:\n\t\tmatch = re.compile(\"(?:[0-9]{2,5})([^0-9]|0)\")\n\t\tmatch = re.compile(match)\n\t\tmatch = re.compile(\".*-([dD][0-9]{2})([^0-9]|0)\")\n\t\tmatch = re.compile(\".*-([^0-9]|[0-9])([^0-9]|0)\")\n\t\tmatch = re.compile"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tv0 = next(v for v in version.split(\".\") if \"v\" in v)\n\tv1 = next(v for v in v0.split(\".\") if not \"v\" in v)\n\n\treturn v1 if v1.capitalize() < v0.capitalize() else v0\n"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t# initialize with a dict to record the digests of each version folder in that\n\t# version tree\n\t\n\tnew_vars = {}\n\tfor dir_n in version_dirs:\n\t\tnew_vars[dir_n] = get_version_entry(dir_n)\n\t\t\t\n\t# validate that all folders have digests\n\t\n\tfor dir_n in version_dirs:\n\t\tif get_version_entry(dir_n)[3] == 1: # this will be true for older versions if we only look in the root\n\t\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tvalid_version_dirs = [d for d in version_dirs if d!= \"\"]\n\tvalid = set((\"0\", \"1\", \"2\", \"3\"))\n\n\tfor p_ver in valid_version_dirs:\n\t\tif len(valid.intersection(p_ver)):\n\t\t\treturn True\n\treturn False\n\ndef get_inventories():\n\t\"\"\"\n\tReturn contents of all inventories\n\tas a list of content digests and a dictionary\n\t\"\"\"\n\treturn [(content_content, _get_digests(content))\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tlog.debug('Start validate_version_inventories(), v=%s', version_dirs)\n\t\n\t# Create all directories, make sure the output files are there\n\tfor dir in version_dirs:\n\t\ttry:\n\t\t\tos.mkdir(dir)\n\t\t\t# Make sure these directories have the correct version:\n\t\t\tself.check_version_file(name=os.path.join(dir, 'inventory.json'))\n\t\texcept OSError:\n\t\t\tlog.error('Version folder %s already exist: %s', dir, os."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# Check root inventory first.\n\troot_ver = root_version = find_dir(version_dirs[0])\n\tif root_ver is None:\n\t\traise ValidationError(\"No root version in %r.\" % (version_dirs[0]))\n\tif root_ver < 1000:\n\t\traise ValidationError(\"Root version number less than 1000.\")\n\tself.validate_name([], root_ver, max_name_length=30)\n\t\n\tif len(root_ver)!= 4:\n\t\tcur_ver = root_ver"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdef verify_content(s):\n\t\tfor sdn in sdn_data.split(os.sep):\n\t\t\tif sdn in version_dirs:\n\t\t\t\treturn True\n\t\treturn False\n\tfor _, sdn in sdn_data.split(os.sep):\n\t\tversion_dirs.append(os.path.realpath(os.path.dirname(sdn)))\n\t\tversion_tuple = (int(version) for version in version_dirs)\n\t\tfor version in version_tuple:\n\t\t\tcontents = find_inventory"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\ttry:\n\t  content_digest = self.root_inventory.content_digests[0]\n\t\n\t  return True\n\t  \n\texcept Exception as e:\n\t\n\t  return False\n\t  \n\tfinally:\n\t\n\t  del content_digest\n\n\t\n\t\n\t\n\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdirs = [os.path.join(version_dir, x) for x in self.inventory.version_dirs]\n\tinventories = []\n\twith open(versions_file) as f:\n\t\tfor line in f.readlines():\n\t\t\ttokens = line.split(\":\")\n\t\t\tif tokens.pop(0) == \"InventoryName:\":\n\t\t\t\tif len(tokens) > 1:\n\t\t\t\t\tif tokens[0:3] in dirs:\n\t\t\t\t\t\tinventories.append(get_version(dirs).inventory_name"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tpass\n\n\ndef validate_versions(versions):\n\t\"\"\"\n\tvalidate and return a list of versions available, where they contain a set of valid content digests.\n\t\"\"\"\n\treturn [ d for d in versions if d ]\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t# find out which directories to look in\n\troot_item = self.root_item\n\t\n\t# check the version directories\n\tfor dir in version_dirs:\n\t\t# check the content in that directory\n\t\tcontent_to_check = self.__get_content(dir)\n\t\t\n\t\t# look for a matching version\n\t\tversions_match = False\n\t\tfor version in self.__get_versions(content_to_check):\n\t\t\t# if we found the version (by name) match, then we've got all we need to run\n\t\t\tif version"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tinv_records = {}\n\t\n\tfor n in version_dirs:\n\t\t# check for duplicates\n\t\tif n in inv_records:\n\t\t\t# make an empty record\n\t\t\tnew_inv = []\n\t\t\tnew_inv.append(n)\n\t\t\tinv_records[n] = new_inv\n\t\telse:\n\t\t\tinv_records[n] = []\n\t\t\n\t\t# parse the recs\n\t\tnew_rec = parse_versions(get_dir(n))\n\t\t\n\t\t# iterate over each item\n\t\tfor rec"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tif path[-2:] == '*':\n\t\ttry:\n\t\t\tos.stat(path)\n\t\texcept OSError:\n\t\t\treturn 'file'\n\n\telif path.startswith('') or path == '-1'\\\n\t    or path == '-5'\\\n\t    or path == '-2'\\\n\t    or path == '-4'\\\n\t    or path == '-9':\n\t\treturn 'root'\n\n\telse:\n\t\ttry:\n\t\t\tf = os.open(path, os.O"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tdir_paths = map(lambda f: str(os.path.basename(f)), sys.argv)\n\n\t# check which directories contain files\n\tdirlist = [\n\t    'dir' + d for d in dir_paths[:-1]\n\t]\n\n\tfor d in dirlist:\n\t\ttry:\n\t\t\tif '\\\\.' not in d:\n\t\t\t\tdirlist.remove(d)\n\t\texcept OSError:\n\t\t\tpass\n\n\tdirlist.remove('\\\\')\n\n\tdir = sorted(dirlist)\n\treturn dir[-"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tpath_types = path + path.split(\"=\")[1]\n\tpath_type = path_types[0]\n\n\tif path_type == \"*\": \n\t\treturn \"unknown\"\n\telif path_type == \"-\":\n\t\treturn \"storage folder\"\n\telif path_type == \":\":\n\t\treturn \"root\"\n\telse:\n\t\treturn path_type\n\t\n\t\n\tdef check_permissions(path, perm):\n\t\t\"\"\"\n\t\tReturn a list of files under that path per-directory permissions (from perm).\n\t\t\n\t\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path.startswith('/'):\n\t\tpath = path[1:]\n\t\n\tparts = path.split(\"osfstorage\")\n\tif len(parts) == 2:\n\t\tfile_type = parts[0]\n\t\tpath_type = parts[1]\n\t\treturn file_type, path_type\n\telse:\n\t\treturn \"ERROR (osfstorage.py:find_path_type)\"\n"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tif (path.count(\"=0\")) == 0:\n\t\treturn path\n\t\n\tpathtype = path[0:path.rfind(\"=\")+1]\n\t\n\tif pathtype == \"root\":\n\t\treturn path\n\t\n\tif pathtype in [\"object\", \"file\"]:\n\t\treturn path[path.rfind(\".\")+1]\n\t\n\telif pathtype == \"other\":\n\t\treturn path[path.rfind(\"=\")+1:]\n\t\n\telif (pathtype.startswith(\"/\")):\n\t\tdirectory"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tnameType = \"\"\n\tif \"-/\" in path:\n\t\tpathType = 1\n\telse:\n\t\tpathType = 0\n\t\n\ttry:\n\t\tif \"-\" in path:\n\t\t#\tpathType = 0\n\t\t\tnumFile = len(path)\n\t\t\tif pathType > numFile or -1 in path[:numFile]:\n\t\t\t\treturn 'file'\n\t\t\n\t\telse:\n\t\t#\tpathType = 1\n\t\t\tnumDir = len(path)\n\t\t\tif pathType > numDir or '0=' in path:\n\t\t\t\treturn 'root'\n"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tdirs\t= []\n\tbase\t= path.split('/')[0]\n\t\n\twhile '/' in base:\n\t\ttry:\n\t\t\tbase\t= base.split('/')[0]\n\t\texcept IndexError:\n\t\t\t# If there is no file at this path, don't look any farther\n\t\t\tdirs.append(base)\n\t\t\treturn 'root'\t# Return root path\n\n\t# Now there is something valid...\n\t\n\tif path.startswith('*'):\n\t\t# For '*' to search all files or for"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tif path == '':\n\t\treturn 'root'\n\t\n\tif '*' in path[-1]:\n\t\tpath = path[:-1]\n\t\n\tif path[-1] == os.readlink('/') and self.is_root(path):\n\t\treturn 'root'\n\n\tif path[-1] == '/' and self.is_root(path) and not path == '/':\n\t\treturn 'file'\n\t\n\tif path.startswith('/images/') or\\\n\t    path.startswith('/"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tdef dir_types():\n\t\t\"\"\"\n\t\tReturn the directory types of a folder. Return an empty list if the folder is empty.\n\n\t\tNote that this may return a list of only empty directories.\n\t\t\"\"\"\n\t\tif not os.path.exists(path):\n\t\t\treturn []\n\n\t\tbase, _ = os.path.split(path)\n\t\treturn [p for p in os.listdir(base) if os.path.isdir(os.path.join(base, p))]\n\n\ttype = dir_types()[0]\n\tif path.startsw"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tnxt = path.normpath('')\n\tif not os.path.isfile(nxt): return 'file'\n\tif nxt == path.sep+'/': return 'file'\n\telif nxt[-1:] in {'/', '\\\\', '.'}: return 'directory'\n\telif nxt[:1] == '-': return'sub-dir'\n\telif nxt[0] == nxt[-1] == '.': return 'dir'\n\telse: return 'object'\n\ndef path"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor param in params.keys():\n\t\t\n\t\tif isinstance(params[param], dict):\n\t\t\t\n\t\t\tfor bzid in param.keys():\n\t\t\t\t\n\t\t\t\tval = param[bzid]\n\n\t\t\t\tif val!= bug_ids[param]:\n\t\t\t\t\t\n\t\t\t\t\tparam[bzid] = bug_ids[param]\n\t\t\t\t\t\n\t\t\telse:\n\n\t\t\t\tprint \"Warning: Bug in bzparam\", param, \"in\", bug_ids\n\t\t\t\t\n\t\telse:\n\t\t\t\n\t\t\tparam[param] = bug_ids[param]\n\n\n\n\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\ttry:\n\t\tif not len(bug_ids):\n\t\t\treturn []\n\n\t\tto_return = []\n\n\t\tfor bug_id in bug_ids:\n\t\t\tfound = False\n\n\t\t\tfor entry in params:\n\t\t\t\tif entry[\"name\"] == bug_id + \":\" and found!= True:\n\t\t\t\t\tfound = True\n\t\t\t\t\tentry[\"value\"] = params[entry[\"name\"]][0]\n\n\t\t\tif found == True and to_return!= []:\n\t\t\t\tto_return.append(params[entry[\"name\"]][0])\n\n\t\t\treturn to_return\n\n\t\treturn params[0"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tpass\n\n\n\tdef amend_fparams(self, fparams, bug_ids):\n\"\"\"\n\tAmend for fparams.\n\t\"\"\"\n\tpass\n\n\n\tdef amend_bzfiles(self, bzfiles, bug_ids):\n\"\"\"\n\tAmend for bzfiles.\n\t\"\"\"\n\tpass\n\n\n\tdef amend_bugzilla(self, bug_ids):\n\"\"\"\n\tAmend for bug_ids. \n\t\"\"\"\n\tpass\n\n\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tglobal bugs, bugs_new\n\t\n\t# the number of params to keep for the bug id\n\tnparams = getattr(params['Bugzilla bug id'], '_get_nparams', 0)\n\tbugparams = getattr(params['Bugzilla bug id'], '_get_bugparam', [])\n\n\tif bugparams == []:\n\t\treturn\n\t\n\tnew_params = {}\n\tnew_params['Bugzilla bug id'] = params['Bugzilla bug id']\n\tnew_params['Bugzilla bugid'] = bugparams\n\t\n\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\treturn params\n\n\t\n# vim: set:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself._amend_bug_params(params, bug_ids)\n\tself._amend_cbtfix(params, bug_ids)\n\tself._amend_cbt_bug_params(params, bug_ids)\n\n\t# get the base params that have been specified in params\n\tparams = params.copy()\n\n\tfor param in params.keys():\n\t\tif params[param][0] == \"{\":\n\t\t\tparams[param] = params[param][1:]\n\n\t\tif params[param][0] == \"=\":\n\t\t\tvalues = params[param]["}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbugs = set( (bz[2].bugid for bz in self.bugs[:3] ) )\n\tparams = [\n\t\t('assigned', ','.join(bugs)\n\t\t\t\t\tif len(bugs) > 0 else '(No assignee found)')\n\t\t]\n\tif self.bugs[2].name_owner!= bug_ids[0]:\n\t\tparams.append(('assigned', self.bugs[2].name_owner))\n\tif self.bugs[2].name_project!= bug_ids[1]:\n\t\tparams.append(('"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug in bugs.BzGetBugKeys(params, params.key_order):\n\t\tbug[\"params\"][\"reporter\"] = params.bug_reporter[bug[\"id\"]]\n\t\tbug[\"params\"][\"owner\"] = params.bug_owner[bug[\"id\"]]\n\t\tbug[\"params\"][\"submitter\"] = params.bug_submitter[bug[\"id\"]]\n\t\t# Bugzilla bug status is modified later on the client side by addons like the one in the bugzilla \n\t\t#  file to the bugs_status list.\n\t\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif params is None:\n\t\treturn {}\n\tif type(bug_ids) not in [list, tuple]:\n\t\tbug_ids = [bug_ids]\n\tout = {}\n\tfor bug_id in bug_ids:\n\t\tout[bug_id] = params.setdefault('bzBZ', bug_id)\n\treturn out\n\ndef clone_bugzilla(id, bug):\n\t\"\"\"\n\t\tClone a bugzilla bug\n\t\"\"\"\n\torig_bugs = set(bug.bugs)\n\tbugs = {}\n\tfor bug_id"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug_id in self.bug_ids:\n\t\tif not bug_id in bug_ids:\n\t\t\tbzparams = self.xbmc.getInfoLabel(\"solve_bz_params\")\n\t\t\tbzparams = re.sub(self.params_re, '{{bug_id}}', bzparams)\n\t\t\tself.xbmc.setInfoLabel(\"solve_bz_params: %s\" % bzparams, True)\n\t\telse:\n\t\t\tbzparams = \"\"\n"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tassert isinstance(nodes, list)\n\n\tnodes = [\n\t\tn\n\t\tfor n in nodes\n\t\tif isinstance(n, ScalarNode)\n\t] or [ ]\n\t\n\tindex = 0\n\tfor i, n in enumerate(nodes):\n\t\tfor j, s in enumerate(n.get_tags()):\n\t\t\tif s!= 'tag:yaml.org,2002:map':\n\t\t\t\tif s in ['tag:yaml.org,2002:int', 'tag:yaml.org,2002:str']:\n\t\t\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tmerged_nodes = []\n\tfor key in nodes:\n\t\tif key.startswith(('\\t\\t', '\\t\\tkey')):\n\t\t\tif len(key) == len(key) + 2:\n\t\t\t\tvalue = []\n\t\t\t\tfor i in reversed(range(len(key), len(key) + 2)):\n\t\t\t\t\tvalue.append(repr(key[i]))\n\t\t\t\tvalue.reverse()\n\t\t\t\tvalue[-1] = \":\" + node_tostr(value[-1])\n\t\t\telse:\n\t\t\t\tvalue ="}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tresult = {}\n\tfor section_key, value in nodes:\n\t\tfor key, node in value.iteritems():\n\t\t\tif key =='scalar':\n\t\t\t\toldval = result.get(section_key, [])\n\t\t\t\tresult[section_key] = oldval + [deep_merge_nodes_internal(node), node]\n\t\t\telif key =='mappings':\n\t\t\t\toldval = result.get(section_key, [])\n\t\t\t\tresult[section_key] = oldval + [node]\n\t\t\telse:\n\t\t\t\tresult[section_"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tretain_scalars = False\n\tretain_nodes = []\n\tfor node in nodes:\n\t\tif (\n\t\t\tisinstance(node, basestring) and\n\t\t\tisinstance(node[-1], basestring):\n\t\t\tretain_scalars = True\n\t\telse:\n\t\t\tretain_nodes.append(node)\n\tfor node in retain_nodes:\n\t\tfor subnode in find_doubly_clash(del_subnode=True, depth=1):\n\t\t\tsubnode.yaml_children = nodes\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tnodes = deep_filter(nodes)\n\tresult = list()\n\tnew_root, last_mapping = None, None\n\ti = 0\n\tfor node in nodes:\n\t\tif node[0] == 'MappingNode':\n\t\t\tlast_mapping = node\n\t\telse:\n\t\t\tmapping = last_mapping or collections.defaultdict(list)\n\t\t\tnew_root = mappings.MappingNode(tag='yaml.org,2002:str', value=[node[1], node[0]])\n\t\tlast_mapping = node\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tnodes = []\n\t\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tnew_nodes = deep_merge_map(node)\n\t\t\tif not new_nodes:\n\t\t\t\tnode.tag = node.tag.replace('yaml.org,2002:', '')\n\t\t\t\tnode.tag = node.tag.replace('\\n\\n', '')\n\t\t\t\tcontinue\n\t\t\tnodes.extend(new_nodes)\n\t\telif isinstance(node, ScalarNode):\n\t\t\tnodes.append(node"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tnodes = list(nodes)\n\t\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor key, value in node._value:\n\t\t\t\tif value in nodes:\t\n\t\t\t\t\tvalue = deep_merge_nodes(value)\n\t\t\t\t\tnode._value = (key, value)\n\t\t\n\t\telif isinstance(node, ScalarNode):\n\t\t\tif isinstance(node.value, ScalarNode):\n\t\t\t\tnode._value = deep_merge_nodes(node.value)\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tresults = []\n\n\tif len(nodes) == 0:\n\t\treturn results\n\n\tcur = nodes[0].value\n\tfor cur in nodes[1:]:\n\t\tresults += deep_merge_nodes([cur])\n\n\treturn results\n\n# -*- Autogenerated -*-\n# imports\n\ndef process_node(node, node_val, node_val_type):\n\t\"\"\"Helper function that recursively processes the value of a node.\n\n\tThe value of a node is passed as argument to this function.\n\n\tParameters\n\t----------\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tresult = Node\n\tfor key, node in nodes:\n\t\tif key in result:\n\t\t\t# already done\n\t\t\tcontinue\n\t\tnodes[key] = result(nodes[key], deep=True)\n\t\tresult = nodes[key]\n\treturn result\n\nclass Node:\n\t\"\"\"\n\tA nested borgmatic configuration data structure.\n\t\n\tNodes are ordered by keys.\n\t\n\tAttributes can optionally be overwritten,\n\t\n\t1. The key attribute can be a string that represents a key in a borgmatic configuration file.\n\t2."}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tresult = []\n\tif not nodes:\n\t\treturn result\n\tfor n in nodes:\n\t\tfor c in deep_merge_nodes(n.children):\n\t\t\tresult.append(c)\n\treturn result\n\t\n\n\n"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\toptions = ArgumentParser(description=getdoc())\n\toptions.add_argument('-g:', '--force-generic', dest='generic_driver',\n\t\t\t    choices=['qpid', 'zmq', 'tcp', 'tls'], default=None,\n\t\t\t    help='use generic driver for the specified zmq protocol, tcp for generic driver',\n\t\t\t    action=optparse.SUPPRESS)\n\treturn options\n\n\n\"\"\"\n\tCommand line parsing \n\"\"\"\ndef handle_cmd(args):\n\targs.do_setup = False\n\t"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=descr)\n\tparser.parse_args(arguments)\n\targuments_for_parser = parser.parse_args()\n\tprint(arguments_for_parser)\n\treturn arguments_for_parser\n\n\n# get the project root - for use in the build command\nPANDA_HOME = os.path.abspath(os.path.dirname(__file__))\n\n# change to the build root\nset_build_root()\n\nprint(\"Running build...\")\n# find the first version"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(\"%(prog)s [options] filename\", description='Generate a pdf of a PXML document.')\n\tparser.add_argument('filename', metavar='filename', type=argparse.FileType('r'),\n\t\thelp='A file with a PXML document containing images that should be downloaded to the PDF.')\n\tparser.add_argument('-no_image', dest='no_image',\n\t\thelp=\"Do not generate an image. This option is a good way to bypass image generation: PDF has no need to be a PNG.\")"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Get the positional arguments from the command line\n\t# and make up a dictionary of the positional arguments\n\tpositional_args = []\n\t# Loop through each of the positionals\n\tfor i, argument in enumerate(arguments):\n\t\tif argument in ['-f', '--file']:\n\t\t\tpositional_args.append(argument)\n\t\t\t# Ignore arguments with an empty valutar\n\t\t\tif positional_args.count('') == 0:\n\t\t\t\tbreak\n\t\t\tif verbose:\n\t\t\t\tprint('Processing positional parameter: `%s \"%s\"`.'"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparsed_command_line = shlex.split(str(arguments[0]))\n\treturn ArgumentParser(**parsed_command_line).parse_args()\n\tpass\n\nif __name__ == \"__main__\":\t\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-a\", type=int, dest=\"NumA\", help=\"the rank in the \"\n\t\t\t\t\t   \"new system to connect.\");\n\tparser.add_argument(\"-b\", type=int, dest=\"NumB\", help=\"the rank in the \"\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\toptions = ArgumentParser()\n\tfor argument in arguments:\n\t\toptions.parse_args(argument)\n\t\treturn options\n\n\treturn options\n\n\n\n\n\"\"\"\n\tReturn the program execution status and an error message if there was one.\n\"\"\"\ndef is_correct(status):\n\t\"\"\"\n\t\tAn error message might have been specified if a program or an error happened when it was\n\t\tconstructed. If there was no message associated with it, a blank string is returned.\n\t\"\"\"\n\treturn (status) if status==0 else ''\n\n\n\n\n\"\"\"\n\tCheck a command-line"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targuments = arguments or sys.argv[1:]\n\tparser = ArgumentParser(prog = os.path.basename(sys.argv[0]))\n\tparser.add_argument('--help', action = 'help',\n\t\thelp = 'Run the main script without arguments.', metavar = '')\n\targs = parser.parse_known_args()\n\treturn args"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser()\n\n\tparser.add_argument('--print-config', action='store_const', const=True, dest='print_config', default=False,\n\t\t\thelp=\"Print out JSON\")\n\n\tparser.add_argument('--print-build', action='store_const', const=True, dest='print_build', default=False,\n\t\t\thelp=\"Print out build.xml\")\n\n\tparser.add_argument('--print-dist', action='store_const', const=True, dest='print_dist', default=False,\n\t\t\thelp=\"Print out"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(usage=\"%(prog)s parser.py [options]\")\n\tparser.add_argument(\"--help\", \"--usage\", dest=\"usage\", action=\"store_true\", help=\"show this help message and exit\")\n\tparser.add_argument(\"--version\", \"--version\", dest=\"version\", action=\"store_true\", help=\"print the version number and exit\")\n\tparser.add_argument(\"--nocomm\", \"--no_local\", dest=\"nocomm\", action=\"store_false\", help=\"suppress communication in console\")\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\treturn ArgumentParser()\n\nclass Main:\n\t\"\"\" The script itself does nothing, but returns the command-line arguments that it \n\t\tdecides to give to the subcommands.\n\n\t\"\"\"\n\tdef __init__(self, commands, arguments):\n\t\tself.commands = commands\n\t\tself.arguments = arguments\n\n\tdef main(self, argv=None):\n\t\t\"\"\"Run the main function of the script. Handles the exceptions raised by any subcommands.\"\"\"\n\t\ttry:\n\t\t\tsys.argv = list(sys.argv) # We"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tresult = []\n\tfor key in listkeys(parser.flags):\n\t\tresult.append(parser.flags[key].replace(\"-\", \" \"))\n\treturn \" \".join(result)\n\n\nclass Argument(object):\n\t\"\"\"\n\tUsed as an argument for command-line switches.\n\t\"\"\"\n\n\tdef __init__(self, name=None, value=None, help=None, action=\"store_const\", default=None):\n\t\t\"\"\"\n\t\tArgs:\n\t\t\tname: the arg name\n\t\t\tvalue: the arg value\n\t\t\thelp: the optional arg's"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tret = []\n\tparser = parser.argument_group('Flags')\n\tfor flag in parser.add_usage('Flags'):\n\t\tret.append(flag.flag)\n\treturn''.join(ret)\n#\n# Main\n#\n\ndef main():\n\t\"\"\"\n\t\tThis is the main entry point for the python version.\n\t\t\"\"\"\n\tparser = argparse.ArgumentParser(description='The command line argument parser.')\n\tparser.add_argument('--version', action='version', version=( __version__, 0, 0))\n\tparser."}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \" \".join(parser.arguments)\n\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = parser.usage\n\tif isinstance(flags, list):\n\t\tflags = ', '.join(flags).split()\n\n\ttry:\n\t\tflags = flags.split()\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tflags = flags.split()\n\texcept:\n\t\tpass\n\n\tflags = flags + ['']\n\n\tfor flag in flags:\n\t\ttry:\n\t\t\tflag = flag.replace('@','-@')\n\t\texcept:\n\t\t\tpass\n\n\treturn flags\n\ndef read_config_files(base_dir, opts):\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tret = '## '; del ret\n\tfor ri, v in parser.__dict__.items():\n\t\tif hasattr(ri,'short_description') and isinstance(ri.short_description, str):\n\t\t\tret += '```\\n%s\\n```' % (ri.short_description)\n\t\telse:\n\t\t\tret += str(str(ri))\n\n\treturn ret\n\nif __name__ == \"__main__\":\n\tsys.stdout.write(parser_flags(sys.argv[0]))\n\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.get(\"babel:flags\")\n\nparser_flags_string = parser_flags(\"babel:flags\")\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.description\n\n\ndef parse_int(value):\n\ttry:\n\t\treturn int(value)\n\texcept ValueError:\n\t\ttry:\n\t\t\treturn int(float(value))\n\t\texcept ValueError:\n\t\t\treturn value\n\n\ndef get_parser_flags(parser):\n\tfor flag in [\"--help\", \"-h\", \"-help\"]:\n\t\tif flag in parser.description:\n\t\t\treturn True\n\treturn False\n\n\ndef usage():\n\t\"\"\"\n\tDisplay the base usage text.\n\t\"\"\"\n\tprint\n\tprint \"************************************\"\n\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = None\n\tfor flag_name, flag_list in parser._flag_names.items():\n\t\tif hasattr(flag_list, '__iter__'):\n\t\t\tflags = flags +'| '.join(map(str, flag_list))\n\t\telse:\n\t\t\tflags = flags + flag_list +''\n\treturn flags\n\ndef parser_parser(parser):\n\t\"\"\"\n\tGiven a parsed argparse.ArgumentParser instance, return their argument values\n\tas a string.\n\t\"\"\"\n\tparser_values = parser._parser_values\n\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \"-\".join(argopt.flags() for argopt in parser)\n\n##\n#\n# @param options\n# @return list of possible values.\n#\ndef get_arg_names(options):\n\tnames = []\n\tfor option in options.option_strings():\n\t\tif len(option)!= 3:\n\t\t\tnames.append(option)\n\treturn names\n\n##\n#\n# @param parser\n# @param *values\n# @return the list of values that should be passed as arguments for the option\n#\ndef collect(parser, *values"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tresult = parser.get_style_flag().replace('\\n', '')\n\n\tif parser.optionals:\n\t\tresult +='' + '('\n\n\tif parser.positionals:\n\t\tresult += parser.positionals\n\n\tresult += ')\\n'\n\n\treturn result\n\n\ndef print_flags(parser, extra_style = ''):\n\tif extra_style:\n\t\tflags = flags_prefix + extra_style + parser.style_flag\n\telse:\n\t\tflags = flags_prefix\n\tprint(flags, file = sys.stderr)\n\n\n"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t#TODO: refactoring the command-line parser to be more usable/human-friendly\n\tunparsed_arguments = []\n\targs, unparsed_arguments = argspec.parse_known_args(unparsed_arguments)\n\n\treturn parse_arguments(*unparsed_arguments)\n\t\ndef create_main_parser(main_command, subparser_names):\n\t\"\"\" Creates the \"main\" argument parser for a command that is intended to use this script.\n\t\tReturns the main command argument parser instance.\n\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targ_objects = []\n\tnamespaces = {}\n\tfor arg_object in unparsed_arguments:\n\t\tnamespaces[arg_object.dest] = arg_object.namespace or arg_objects[0].namespace\n\t\targ_objects.remove(arg_object)\n\targuments = {}\n\tfor arg_object in arg_objects:\n\t\tif \"dest\" in arg_object.namespace.kwargs:\n\t\t\targuments[arg_object.namespace.kwargs[\"dest\"]] = arg_object.namespace\n\t\telse:\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targs = dict()\n\tparser = argparse.ArgumentParser(prog=UNPARSEED_PREFIX + self._program, \n\t\tadd_help=False)\n\tparser.add_argument(\"-d\", \n\t\t\t\"--database\", \n\t\t\thelp=(\"path to database file to parse\")\n\t\t.format(UNPARSEED_PREFIX + self._program),\n\t\trequired=True)\n\tparser.add_argument(\"-f\", \n\t\t\t\"--feature_file\", \n\t\t\thelp=(\"path to feature_file\")\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targv = unparsed_arguments\n\targs = []\n\t\n\twhile '=' in argv[0]:\n\t\ttry:\n\t\t\targs.append(re.sub(r\"[\\=<>\\^\\(]+\", \"\", argv[0]).strip())\n\t\texcept IndexError:\n\t\t\traise ValueError(\"Invalid arg\")\n\t\t\t\n\t\tdel argv[0]\n\n\treturn {subparsers.start_command(): parse_args(args) for subparsers in subparsers.children}\n\n\n@arguments\ndef import_subparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tif len(sys.argv) == 1: # the main script, without command-line args\n\t\treturn { }, {}\n\telse:\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument('--log-level', default=\"WARNING\",\n\t\t\thelp=\"Controls logging output verbosity level\")\n\t\tparser.add_argument('--skip-build', default=False, action='store_true',\n\t\t\thelp=\"Skip rebuilding everything.\")\n\t\tparser.add_argument('--skip-build-for-tests', default=False, action='store_true"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal_arguments = []\n\tif len(unparsed_arguments) == 0:\n\t\treturn global_arguments\n\n\t# Initialize argument parser.\n\tfrom.argtable import ArgParsingError\n\n\targ_parser = argparse.ArgumentParser(parents=[parser_with_subparser])\n\n\terror_regex = r'.*error.*' if _GLOBAL_SUBSHELL_ERROR_RE is None else r'.*error.*\\s*global.*'\n\n\tglobal_arguments = {}\n\tfor unparsed_argument"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tif len(unparsed_arguments) == 0:\n\t\tunparsed_arguments = [\"get_configuration\"]\n\n\tunparsed_arguments.insert(0, \"--\")\n\treturn parse_arguments(*unparsed_arguments)\n\t\nclass InvalidAction:\n\n\t\"\"\"\n\tRepresents failure behaviour when an invalid action has been used.\n\t\"\"\"\n\n\t# this is a hard-coded list of valid actions.\n\tACTIONS = [\"info\", \"show\", \"show_default\"]\n\n\tdef __init__(self"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal_args = {}\n\tdef add_arguments(parser):\t# add a subparser to the parsed_arguments.\n\t\tglobal_args[parser.name] = subparser\n\tfor parsed_arg in unparsed_arguments:\n\t\tadd_arguments(parsed_arg.subparser())\n\t\treturn global_args\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tsubparsers = sys.argv[1:]\n\tnamespace = parse_arguments(*subparsers)\n\tsubparsers = list(parse_arguments(*namespace['subparsers']))\n\tsubparsers = [create_parser(subparsers)] + subparsers\n\treturn namespace\n\t\ndef print_subparser_namespace(subparser):\n\tglobal_subparser = subparser.add_subparsers(dest='global')[1]\n\tsubparsers = list(subparser.args)"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# Parse command-line args\n\tsubparsers = list_subparsers(help=\"sub-command\", dest=\"parser\")\n\tparsed_arguments = parse_arguments(sys.argv[1:])\n\targv = []\n\targparsers = {}\n\tfor k, v in parsed_arguments.items():\n\t\tif v is None or isinstance(v, list):\n\t\t\targs, _ = [], []\n\t\telse:\n\t\t\targs = v\n\t\targparsers[k] = subparsers[v"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targs_dict, remaining_args = _parse_subparser_args(unparsed_arguments)\n\t\n\taction_dict = {subparsers['default'].name: subparsers for subparsers in subparsers_dict.values()}\n\taction_dict[subparsers['actions'].name] = argparse.Namespace(subparsers['default']).add(*args_dict.items(), action_dict)\n\treturn args_dict, remaining_args, action_dict\n\n\ndef _prevent_subparser_arg"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers[None] = subparsers[None].add_subparsers(_(\"action\")+\":\")\n\tparsed_args = {}\n\tremaining_args = []\n\tfor remaining in unparsed_arguments:\n\t\tparser = subparsers[None].set_defaults(args=remaining)\n\t\tparsed_args[parser.name] = parser\n\t\tremaining_args.extend(parser.parse_args(remaining)[0])\n\tparsed_kwargs = dict([[parser.name, ("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\taction, subparsers = unparsed_arguments\n\tresult = {}\n\t# Make a list of unique subparsers for each action\n\targuments_sorted = sorted([{'action_name': a.get_human_name(), 'value': a.get_name()}\n\t\tfor a in actions] or [])\n\t# Make a list of pairs of lists of unparsed arguments and action_name tuples\n\t# with all args already handled by all subparsers\n\tsubparsers_parsed = [[{}, []]]\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult=[]\n\tfor argument in unparsed_arguments:\n\t\tsubcommands=getattr(argument,'subcommands','').split('+')\n\t\tname=argument.get_name()\n\t\tif name not in subcommands and name not in subcommands:# we don't want subcommands here.\n\t\t\tcontinue\n\t\t\n\t\tsubparser=subparser_lookup(name)\n\t\tif subparser is None:\n\t\t\t# this is the first argument of subparser\n\t\t\ttry:\n\t\t\t\tparser=subparser_class_from_argument(name"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_args_dict = {} # a dictionary mapping subparser name to a set of remaining arguments.\n\tresult = unparsed_arguments\n\tfor (parser_key, subparser) in subparsers.iteritems():\n\t\tfor unparsed_arg in parser_key.split('.'):\n\t\t\tif subparser:\n\t\t\t\tsubparser = subparser + '.' + unparsed_arg\n\t\t\ttry:\n\t\t\t\tparameters, args = parser.parse_known_args(unparsed_arguments)\n\t\t\texcept argparse.ArgumentError as err"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tnamespace = {} # This is the key that the subparser maps to\n\tremaining_args = [] # This is a list of positional arguments that were all claimed by\n\t\t\t # any subparser. The subparser may want to re-parse this from the dict,\n\t\t\t # but if the subparser returns a name and the parser doesn't allow\n\t\t\t # that name to be used by the current action, there is no need to\n\t\t\t # add it to the remaining_list as there's nothing more to do.\n\t\t\n\tfor (subparser, subparsers) in subparsers."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tsubparsers_ = _find_subparsers(subparsers)\n\t\n\t# parse additional parser names\n\tif len(unparsed_arguments) > len(subparsers_):\n\t\tunparsed_arguments, consumed_options = _split_options(subparsers_[len(unparsed_arguments):])\n\t\tfor name, raw_arg in unparsed_arguments:\n\t\t\tresult[name] = _parse_arg_options(name, raw_arg,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = subparsers[0]\t# first argument to take in a subparser\n\tresults = {}\t  # results for subparser's subparsers and its arguments\n\tfor arg in unparsed_arguments:\t# keep looping over subparser name to parser for next argument\n\t\t_, args = parser.parse_known_args()\t# find parsed subset of args which are for this subparser\n\t\tresults[_args.get(arg, parser.name)] = args\t# add parser's arguments to dictionary for this subparser\n\targs, remaining_args = parser."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_namespace = {}\n\tremaining_kwargs = {}\n\tfor arg, kwargs in unparsed_arguments:\n\t\tif len(kwargs) == 0:\n\t\t\tbreak\n\t\tsubparser_namespace.clear()\n\t\tsubparser_namespace.update({k:v for k,v in kwargs.items()})\n\t\tsubparser = subparsers[arg.subparser]\n\t\tfor argname, parser in subparser.subparsers.items():\n\t\t\ttry:\n\t\t\t\targs = parser.parse_args(args"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\ts_args = [arg for arg in unparsed_arguments if arg not in subparsers]\n\t\n\t# remove arguments from this list to the end\n\tfor a in s_args:\n\t\ts_args.remove(a)\n\t\t\n\tfor a in list(s_args):\n\t\n\t\t# make sure we don't have a \"subparser\" with a positional argument\n\t\tif a =='repository':\n\t\t\ta = 'file'\n\t\t\t\n\t\te = subparsers.get(a)\n\t\tif e:\n\t\t\ts_"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser_obj = argparse.ArgumentParser(usage='python -h')\n\tparser_obj.add_argument('-h', '--help',\n\t\thelp='show this help and exit')\n\tparser_obj.add_argument('-v', '--verbose',\n\t\tchoices=['--verbose'],\n\t\tmetavar = ['--verbose'],\n\t\tdefault = False)\n\tparser_obj.add_argument('-V', '--verbosity',\n\t\tchoices=['1', '2'],\n\t\tmetavar = ['"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\treturn parser, subparser_parsers\n\n\t\ndef make_parser():\n\t\"\"\"\n\tReturn a new parser instance with an empty namespace so nothing else\n\tcan be inserted after the call.\n\tRaise ParserException if the call goes beyond the parser context.\n\t\"\"\"\n\tp = make_parser()\n\tp.add_argument('-v', action='count', dest='verbose', default=False, help='verbose level')\n\tp.add_argument('-x', action='count', dest='exact', default=False, help='exact"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparsers = parser.parsers()\n\tparsers.add_parser(\n\t\t\"main\",\n\t\thelp = \"\"\"Main document type and sub-parser.\n\t\"\"\")\n\tparsers.add_parser(\n\t\t\"example\",\n\t\tparents = [parser],\n\t\thelp = '''\n\t\tAn example parser. See the examples below for what this parser is used for.\n\t\t''',\n\t\tdefault = False)\n\n\tparsers.add_parser(\n\t\t'html',\n\t\thelp = '''An HTML parser.  See the examples below"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_parser = top_parser(defaults={}, **opts)\n\ttop_parser(\"my subopt 1\")\n\ttop_parser()\n\ttop_parser(\"my subopt 2\")\n\ttop_parser(\"my subopt 3\")\n\ttop_parser(defaults={\"myparser\": myparser})(\"mysubopt 2\")\n\ttop_parser(\"my subparser\")\n\ttparser = tparser(defaults={\"parser\": parser})(\"subparser\")\n\n\ttop_parser(\"my subparser 2\")\n\ttop_parser(\"my subp"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# we just want a 'parser' with the correct options\n\tparser = subparsers.add_parser(\n\t\t'all',\n\t\tdescription=\"All possible tags and subcommands for Genshi\")\n\n\t# we also want the parser with some default options\n\tparser.add_argument('-a', default='a')\n\n\tparser.set_defaults(func=lambda:\n\t\tparser.add_argument('--one', default='hello')\n\t\tparser.add_argument('--two', default=2)\n\t\tparser.add_argument('--three', default="}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_parser = top_top_parser_class()\n\ttop_subparsers = []\n\ttop_subparsers += make_subparsers(top_parser)\n\t# for each subparser, create it's own set of subparsers\n\ttop_subparsers = set()\n\tfor subparser_name in subparser_list:\n\t\ttop_subparser = subparser_class(subparser_name)\n\t\ttop_subparsers += make_subparsers(top_subparser)\n\treturn top_subp"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport re\n\tp = Parser()\n\n\tp.start = re.compile(\"^(\\w+)$\")\n\tp.body = re.compile(\"^([^# \\t]*)\\t([^$]+)$\")\n\tr = p.start(\"test\")\n\twhile r:\n\t\tm = r.search(1, 2)\n\t\tprint m\n\t\tr = p.body.search(1, max(1, m.startpos + 1))\n\n\tp.subparsers = []\n\tp.body.subparsers"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = Parser()\n\tsubparsers = parser.get_subparsers()\n\treturn subparsers\n\n\ndef add_subparser_for(parser, name, subparser_name=''):\n\t\"\"\"\n\t\tAdd a subparser which is associated with the given parser.\n\t\tReturn the new subparser.\n\t\t\"\"\"\n\treturn subparser_name + '' + name\n\ndef add_parser(parser_factory):\n\t\"\"\"\n\t\tAdd a parser for the module\n\t\"\"\"\n\tparser = parser_factory(make_parser())\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Generate and parse CLI parameters\")\n\tparser.add_argument(\"-s\", dest=\"separator\", help=\"\", type=str, default=None)\n\tparser.add_argument(\"-t\", dest=\"tabwidth\", help=\"\", type=str, default=None)\n\tparser.add_argument(\"--input-encoding\", dest=\"inputencoding\", help=\"\", type=str, default=\"\" #if not specified, use UTF-8\n\t)\n\tparser.add_argument(\"-\", dest=\"debug"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tp1 = Parser()\n\tp2 = Parser()\n\tp1.add_argument='A'\n\tp1.add_subparsers('subp1', dest='sub')\n\tp2.add_argument='B'\n\n\tp1.add_argument('another_parsed', help='A long help')\n\tp2.add_argument('another_parsed', help='A long help')\n\n\treturn p1, p2\n\ndef make_parser():\n\t\"\"\"\n\tBuild a top-level parser and return it as"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tret=[]\n\tfor node in nodes:\n\t\t\n\t\t# if both the scalar values and the scalar values of the node have\n\t\t# different elements in the same tag, we know it is not going to\n\t\t# be a duplicate.\n\t\t# So we do not care if there is not already a tag.\n\t\t# So we don't care to check if tag is already set or not.\n\t\t\n\t\tret.append((node[0].dumps(), node[1].dumps()))\n\treturn deep_merge_nodes(ret)"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\treturn merge_nodes(nodes)([])\n\n\ndef merge_nodes(nodes):\n\t\"\"\"\n\tReturns the given nodes combined into a new map. The returned value is a list containing\n\tthe given nodes that are combined with each other. For example, given a node data structure as\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tif len(nodes) == 1:\n\t\t\n\t\treturn nodes[0]\n\t\n\tfor node in nodes:\n\t\t\n\t\tif not isinstance(node, MappingNode):\n\t\t\treturn node\n\t\n\treturn nodes[-1]\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tfor node in nodes:\n\t\t# this is the first iteration through nodes and all the others are duplicates\n\t\tif not node:\n\t\t\tcontinue\n\t\t# this is the second iteration through nodes and all the others are not duplicates\n\t\ti = 0\n\t\tfor other in nodes:\n\t\t\t# this is the third iteration through nodes and all the others are not duplicates\n\t\t\tif not other:\n\t\t\t\tcontinue\n\t\t\t# compare the two values of the node and the other node\n\t\t\tcompared_value = str(i + 1).replace(' ', '') \\\n\t\t\t\tif isinstance(node"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tvalues_merged = [\n\t\t(\n\t\t  node\n\t\t  for node in nodes\n\t\t  if isinstance(node, str) and node in k.value\n\t\t)\n\t\tfor k, v in k.value\n\t]\n\t\n\tif len(values_merged) > 1:\n\t\traise ValueError(\"Nodes containing some duplication (with the same keys) cannot be \"\n\t\t                   \"merged\")\n\t\n\treturn values_merged[-1][0] if len(values_merged) < 1 else values_merged["}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tret = set()\n\tfor i in range(len(nodes)):\n\t\tif type(nodes[i]) == ScalarNode:\n\t\t\tif isinstance(nodes[i], ScalarNode):\n\t\t\t\tret.update(deep_merge_single_scalar_node(nodes[i]))\n\t\t\telse:\n\t\t\t\tret.update(nodes[i])\n\treturn ret\n\n# #####################################################\n# Main\n# #####################################################\nimport os\nimport yaml\n\nROOT = os.path.realpath(os."}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tresult = []\n\t# This is the key-to-value map to be used for merge.\n\tmerged_map = {}\n\tfor node in nodes:\n\t    tag, value = node\n\t    # tag and value are always ScalarNodes, so they can be used to identify\n\t    # which keys need to be kept.\n\t    if node == (ScalarNode,):\n\t        tag = 'tag:yaml.org,2002:' + tag\n\t    if tag in merged_map:\n\t        # if tag not in merged_map,"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tfor val in nodes:\n\t\tfor key, value in val.items():\n\t\t\tif isinstance(value, (list, tuple, frozenset)):\n\t\t\t\tval[key] = deep_merge_nodes(val[key])\n\n\treturn nodes\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tresult_list = []\n\t\n\tfor node in nodes:\n\t\t\n\t\tnew_key = deep_merge_node(node.as_key)\n\t\tnew_value = deep_merge_node(node.as_value)\n\t\t\n\t\tif (new_value.data.tags.has_key('duplicateKeysSet')) and \\\n\t\t    (new_value.as_node.type not in ('ScalarNode', 'MappingNode')):\n\t\t\t\n\t\t\ttry:\n\t\t\t\tnew_value.as_node.data.dupl"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tnew_nodes = {}\n\tnode_tag_name = None\n\n\t# first determine the key\n\tfor node in nodes:\n\t\tif (str(node).startswith('tag:') and str(node).endswith(':tag')):\n\t\t\tnode_tag_name = node\n\t\t\tbreak\n\n\tif not node_tag_name:\n\t\tfor node in nodes:\n\t\t\tif (str(node).startswith('tag:') and str(node).endswith(':')) and str(node).endswith('tag'):\n\t\t\t\tnode_"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targparser = ArgumentParser(description = 'Generate C_MIR_PACKAGE command line help')\n\targparser.add_argument('-w', '--workdir', dest = 'workdir', default = './workdir/', help = 'Work directory for the C_MIR work')\n\targparser.add_argument('-d', '--docdir', dest = 'docdir', default = './doc/', help = 'Documentation directory for the C_MIR work')\n\targparser.add_argument('-c', '--cwd',"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targ_parser = ArgumentParser(prog=arguments[0])\n\tsubparsers = arg_parser.add_subparsers()\n\tfor name, func in arguments:\n\t\tparser = subparsers.add_parser(name)\n\t\tparser.add_argument('--version', action='store_true', help='show version')\n\t\tparser.add_argument('--quiet', action='store_true', help='suppress normal output (no progress information)')\n\t\tparser.set_defaults(func=func)\n\treturn arg_parser\t"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Script to generate.csv of user's names on disk.\", prog=arguments[0])\n\tsubparsers = parser.add_subparsers(name=\"submenu\", dest=\"submenu\")\n\t\n\tparser_newname = subparsers.add_parser('newname', help=\"Create new name.\\n\\nIf input is correct, creates the name at the same location as the input\\nfile.\",\n\t\t\t\t\t\t\t\t\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(prog=sys.argv[0])\n\t\n\tparser.add_argument(\"-h\", \"--help\", help=\"show short help to stderr\", action='store_true')\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"verbose mode\", action='store_true', default=False)\n\tparser.add_argument(\"-b\", \"--batch\", help=\"set batch mode\", action='store_true', default=False)\n\tparser.add_argument(\"-A\", \"--algorithm\", help"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser()\n\tparser.add_argument('-m', '--match', default='.*', help='The regular expression matching the desired patterns.  Will use \".+(.+)\" to match up to an occurrence.')\n\tparser.add_argument('-f', '--file_path', dest='file_path', default=None, help='The path to the file you intended to parse.  If the file and the pattern are the same, it will use the first matching pattern from that file.')\n\treturn parser\n\n\ndef parse_args(*arguments):\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tif arguments:\n\t\tparser = ArgumentParser()\n\t\tfor argument in arguments:\n\t\t\tif not parser.parse_args(argument):\n\t\t\t\tparser.print_usage()\n\t\t\t\tsys.exit(1)\n\t\treturn parser\n\telse:\n\t\tparser = CommandLineArgumentParser()\n\t\tparser.add_argument('--list-only', required=False, action='store_true')\n\t\tparser.add_argument('--stats-only', required=False)\n\t\tparser.add_argument('--verbose', action='store_true')\n\t\tparser.add_argument"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Call to built-in parse_arguments so that we can set things to be an ArgumentParser.\n\treturn parse_arguments(*arguments)\n\n\nclass TestEnvironment(object):\n\t\"\"\"\n\tA class with useful methods for testing.\n\t\"\"\"\n\tdef __init__(self, path, config):\n\t\t\"\"\"\n\t\tParse, parse_arguments and print out how arguments are used.\n\t\t\"\"\"\n\t\tparser = parse_arguments(path)\n\t\tset = parser.set\n\t\tset()\n\t\tconfig = parser.parse_config_file(config"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser( *arguments )\n\targs = parser.parse_args()\n\treturn args\n\nclass CommandLineTypeError(Exception):\n\t\"\"\"\n\tGiven an ArgumentParser instance that could not be parsed, raises this error.\n\t\"\"\"\n\tdef __init__(self, argument_types):\n\t\tsuper(CommandLineTypeError, self).__init__()\n\t\tself.argument_types = argument_types\n\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tConverts the internal list of argument types to a human readable string.\n\t\t\"\"\""}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targs = []\n\tfor argument in arguments:\n\t\t# allow only a single character for args\n\t\tif len(argument) > 1:\n\t\t\t# assume it's one argument\n\t\t\targ = argument[0]\n\t\t\tif not arg in arg_map:\n\t\t\t\traise AnsibleOptionError('Option {} must be given on command line and is not found'.format(arg))\n\t\t\targs.append(arg_map[arg])\n\treturn ArgumentParser(args)\n\ndef parse_cli_vars(argv_to_run):\n\t\"\"\"\n\tParses the CLI"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tfrom optparse import OptionParser\n\n\tusage = \"%prog command line_arguments\"\n\tparser = OptionParser(usage=usage)\n\tparser.add_option(\"-p\", \"--port\", type='int', default=7,\n\t\t\t\t\thelp = \"port for the serial port\")\n\tparser.add_option(\"-v\", \"--verbose\", action='store_true', default=False,\n\t\t\t\t\thelp = \"increase the verbosity of the serial port\")\n\n\targs, arguments = parser.parse_args(arguments)\n\n\treturn args\n\n\ndef main"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflag_vars = parser.add_mutually_exclusive_group(required=True)\n\tflag_vars.add_argument(\n\t\t\"--verbose\",\n\t\thelp=\"increase verbosity\",\n\t\tdefault=False,\n\t\taction=\"store_true\",\n\t)\n\n\tflag_vars.add_argument(\n\t\t\"--json\",\n\t\thelp=\"use json values instead of objects\",\n\t\tdefault=False,\n\t\taction=\"store_true\",\n\t)\n\n\tparser.add_argument(\n\t\t\"--no-colors\",\n\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tf = lambda e: '' if not e.has_flag(['-x']) else f'{e.get_flag(\"-x\")}\\n'\n\ts = [s for s in parser._actions if s not in f]\n\treturn''.join(f % arg for arg in s)\n\n\ndef main():\n\tparser = argparse.ArgumentParser(description='Command line interface for'+ parser_names)\n\tparser.add_argument('-v', '--verbose', metavar='<LEVEL>', default='DEBUG', type=str"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = parser.parse_known_args()\n\tflags = \" \".join(flags[:-1].split())\n\treturn flags\n\n# END MAIN\n#=========================================================================================\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\ttry:\n\t\treturn str(parser.config_parser.get(\"main\", \"flags\"))\n\texcept:\n\t\treturn \"\"\n\t\t\t\t\n\ndef flags_to_kwargs(flags, default=None):\n\t\"\"\" \n\t\tConverts a set of flags to a set of key values:\n\t\t\tflags\n\t\t\t['-z']\n\t\t\t['-o', '-v']\n\t\t\t['-u']\n\t\t\t['-j']\n\t\t\t['-x', '-H', '-m']\n\t\t\t['-o', '-v']\n\t\"\"\"\n\targs = set"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(parser._getflattr('flags'))\n\n\ndef parser_requires(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return true iff the flags in the argument are\n\tmissing on the command line.\n\t\"\"\"\n\treturn parser._getflagattr('requires', True)\n\n\ndef parser_description(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its description in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser._getflattr('description')\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targs = parser.parse_args()\n\treturn''.join(args.flag)\n\n\ndef runtests():\n\tmain()\n\n\nif __name__ == '__main__':\n\truntests()"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targs = parser.parse_args()\n\treturn str(args).split()\n\n#def parser_file_extensions(parser):\n#\t\"\"\"\n#\t\tReturn the file extension of the parsed argument.\n#\t\"\"\"\n#\treturn parser.format_usage()[-1]\n\ndef parser_type(parser):\n\t\"\"\"\n\t\tReturn a list of the allowed values for argument type (`string`)\n\t\"\"\"\n\tvalid_types = [\"flag\", \"int\", \"bool\", \"option\"]\n\treturn [v for v in valid_types if isinstance"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = parser.parse_known_args()\n\tflags += (' --help', )\n\n\tif flags['verbose']!= True: setattr(flags,'verbose', False)\n\tif flags['trace']: setattr(flags, 'trace', True)\n\treturn flags\n\n\n# Command line argument parsing:\n\n# The parse command-line takes a number of parameters:\n# -h, --help             Print the usage message\n# -v, --verbose          Enable verbose mode (no interactive prompts)\n# -f <file>                 Read-only text from"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tres_list = []\n\tfor name in parser.add_argument.__dict__.keys():\n\t\tif name.startswith(\"--\"):\n\t\t\tres_list.append(\"%s=%s\" % (name[3:],parser.add_argument.__dict__[name]))\n\t\telse:\n\t\t\tres_list.append(\"%s=%s\" % (name,parser.add_argument.__dict__[name]))\n\treturn \"\".join(res_list)\n\t\t\t\t\n\t\t\n\t\ndef flagsstring(parser,flags"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tresult = ''\n\tfor flag in parser.flag_groups:\n\t\tresult += flags.format(flag.name) +''\n\treturn result.rstrip()\n\n\n@parse_args()\ndef main(argv):\t\n\tparser = argparse.ArgumentParser(description='PycBin - tool for uploading/linking to a tarball')\n\tparser.add_argument('dest')\n\tparser.add_argument('-r', '--replace'+'='+repr(argv[0]))\\\n\t\t.nargs(1)\\"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tshell = sys.platform.lower().split()[0]\n\n\tclass CompletionBorgmaticCommand(BaseScalrCommand.CompletionBorgmaticCommand):\n\t\tparser = CompletionBorgmaticCommand.parser.replace(\n\t\t\tkeywords='borgmatic',\n\t\t\tdescription=\"Use borgmatic's bash completion method\"\n\t\t)\n\n\t\tdef completion(self, args, literal=False):\n\t\t\twith open(self._module_name + '/borgmatic/bash_completion', 'r') as f:\n\t\t\t\treturn f.read().splitlines()\n\n\treturn Com"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t# FIXME: This is ugly but borgmatic requires a 'bash -C borgmatic.bash'...\n\tlog.debug('Running \"introspect command'+str(os.getpid()))\n\ttry:\n\t\tintrospect = pylint.find_plugin( pylint.Plugin( '/usr/bin/bgl' ),\n\t\t\t\t\t\t\t\t\t\t  os.getpid() )\n\texcept Exception, e:\n\t\tlog.exception('Failed to find bgl plugin')\n\t\tsys.exit(1)\n\n\tintrospect.parse_args"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparsers = {}\n\tfor opts_fn in [ \"help\", \"describe\", \"describe_options\" ]:\n\t\tdef isargf ( arg ):\n\t\t\tif arg.startswith(\"-\") and arg.endswith(\"-\"):\n\t\t\t\treturn True\n\t\t\treturn False\n\t\tfor parser in [\"parsers\", \"parser\"]:\n\t\t\tparsers[parsers.get( parser, \"__noop\") ] = parser_options_fn\n\treturn borgmatic.extend_bash_completion()( parsers=parsers"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn bash_function(command_args, return_value=True)\n\n\ndef get_pip_config_for_branch(branch_name):\n\t\"\"\"\n\t\tLook for a configuration file under /local for that branch name, and return it if found (unless\n\t\tit's not found, in which case a ValueError is raised).\n\t\"\"\"\n\tpip_config_dir = os.path.join(BorgmaticConfig.PIP_BASEDIR, branchName)\n\tconfig_file = os.path.join(pip_config_dir,"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t# Get options\n\targs = borgmatic.command_line.__doc__.split('\\n')\n\t# Split args into array\n\tlines = borgmatic.command_line.split(args[2])\n\tline_count = len(lines)\n\t# Extract completion\n\tcompletions = []\n\tindent = max(int(line.split(':')[0].split('  ')[:2].strip()), 0) - 1\n\tindent_chars = line.split(':')[0].split('  ')[0]."}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn _bash_completion(\n\t\t\t\t\"{{.Borgmatic }}\",\n\t\t\t\t\"\"\n\t\t\t)\n\n\n@register.filter(name=\"bash\")\ndef bash_filter():\n\treturn get_value(b\"__\") or get_value(\"@\")\n\n@register.filter(name=\"bash\")\ndef bash_completion():\n\treturn _bash_completion(\"\", \"\")\n\n@register.filter(name=\"bravo\")\ndef bravo():\n\t\"\"\"\n\tGet the bravo (and their corresponding green)\n\t\"\"\"\n\treturn _"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t# get_parser is not a public function, so we do an exception catch here\n\t_get_parser = type(\"\".join(__name__[0: -2] + (\".get_parser\",)), None, {}).get_parser\n\treturn _get_parser(\"\".join(sys.argv[1:]))\n\t\n\ndef get_parser(cmdClass=None, help=False, **params):\n\t\"\"\"\n\tReturn a bash parsing function for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tf = open(\"bash_completion.log\", \"r\")\n\tp = Popen([f.readlines()[-1].rsplit()[0]], stdout=PIPE, stderr=PIPE, shell=True)\n\tp.wait()\n\treturn f.readlines()[0]\n\n@command(pass_context=True)\ndef borgmatic(ctx):\n\t\"\"\"\n\tBorgmatic command to search for all users on the borgmatic\n\tsystem."}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparser = BashCompletionParser(os.path.dirname(__file__))\n\t\n\treturn parser.parse()\n"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tbash_string = \" \\n\"\n\tbash_string += \"\\n\"+__file__\n\n\t# The bash completion is based on bash' man page. It's difficult to use bash-like\n\t# support to get that off, even though bash allows us to add lines to the completion\n\t#\n\tbash_string += \"\\n#{0} - command-line options\\n\".format(\"borgmatic-cli\")\n\n\tfor arg in borgmatic_cli_config.get('arguments', []):\n\t\t# If we see a valid command line argument, we might"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal_args = {}\t\t# subcommands are added here\n\tfor argument in unparsed_arguments:\n\t\tif argument[0] == \"--\" and argument in (\"--subcommand\", \"--global\"): pass\n\t\telif argument[0] == \"-h\" or (argument[0] in (\"-v\", \"--verbose\") and (len(argument) < 2 or argument[1].lower() == \"-v\" or argument[1].lower() == \"--verbose\")): pass\t# TODO: should we ignore this here?\n\t\telse:\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tif unparsed_arguments is None:\n\t\tunparsed_arguments = sys.argv[1:]\n\n\tunparsed_args_dict = collections.OrderedDict()\n\tfor arg in unparsed_arguments:\n\t\tif len(arg) == 4:\n\t\t\t# a \"global\" arg, which is always in unparsed args dict\n\t\t\tif arg[2] == \"argparse\":\n\t\t\t\t# an arg in unparsed args dict\n\t\t\t\tunparsed_args_dict[arg[0]] = arg[4]"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targ_dict = {}\n\tif 'unparsed_arguments' in dir(self):\n\t\targ_dict = dict(zip(self.parser.subparsers._subparsers.keys(), self.parser.run(*unparsed_arguments)))\n\treturn arg_dict\n\ndef run_parser_and_exit(*subparsers, **kwargs):\n\t\"\"\"\n\t\tRun a parser with one argument and return return.\n\t\t\"\"\"\n\tparser = subparsers.pop(0)\n\treturn parser.parse_args(*sub"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser_dict = {}\n\tfor arg in unparsed_arguments:\n\t\tif arg[0] == \"-\": continue\n\t\tif arg.startswith(\"--\"):\n\t\t\tparsed_values = [(x.strip().replace(\"-\", \"--\"), v) for x, v in arg.strip().split(\",\")]\n\t\t\tfor k,v in parsed_values:\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\t# A simple parser, put it in the parser_dict\n\t\t\t\t\tfor (k1,v1) in v.items():\n\t\t\t\t\t\tkey"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targv = unparsed_arguments\n\tparser = ArgumentParser(usage=\"\"\"\\\n\t%prog [command options]\n\t\"\"\", add_help=False, prog=sys.argv[0], description=\"\"\"\\\n\t\t%prog is a free and open source command-line tool for finding and managing Open Source\n\t\tProjects (MPLs), projects and projects affiliated with the world.\n\n\t\tAs of 7/2/2017, this is the version of the API (as of 2017) that you can use. From\n\t\tthis version, you can create a new"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\toptions = {}\n\t\n\t# Now parse arguments. If --help, return an empty dictionary.\n\tparser = argparse.ArgumentParser(description = __doc__)\n\tparser.add_argument(\n\t\t'--help',\n\t\taction ='store_true', dest = 'help',\n\t\thelp = \"Print this help and exit.\"\n\t)\n\t\n\t\n\tparser.add_argument(\"--name\", action ='store', required = False, dest ='subparser_name',\n\t\t\t\t\t\thelp = \"The name of the argparse subparser this script was"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport getopt, sys\n\timport syslog\n\timport getpass\n\timport shutil\n\toptions, unparsed = getopt.getopt(sys.argv[1:], \"\", [\"help\", \"version=\"])\n\tglobal parser_name\n\tfor o, a in options:\n\t\t\tif o == \"--version\":\n\t\t\t\tparser_name = \"--print_version\"\n\t\t\t\t#print(\"python version is {0}\".format(parser.description))\n\t\t\t\tsys.exit(\"{0} version is not supported -- please refer to docs/docs/install"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tns = {} # the actual dict is stored here\n\n\targuments = {}\n\n\t# parse command line arguments\n\toptions, unparsed_arguments = parse_arguments(*unparsed_arguments)\n\n\tcommand_name = unparsed_arguments[0]\n\targuments[command_name] = optparse.Values(options)\n\n\treturn arguments\n\n\ndef validate_subparsers_setup(*unparsed_arguments):\n\t\"\"\"\n\tValidates that all command-line subparsers are setup before this script runs. Uses\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\treturn_dict = {}\n\tsubparsers_by_name = {}\n\tsubparsers = []\n\tglobal_parser = None\n\tglobal_parser = subparsers[1:2] # If there's no user_name, use \"user\"\n\tfor argument in unparsed_arguments:\n\t\tglobal_parser = global_parser # Set global parser by default.\n\t\tif argument == '--':\n\t\t\tparser_name = \"unknown\"\n\t\t\t# If we're invoked without a command-line argument, we're in this special state\n\t\t\t# \""}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\treturn parses()\n\ndef execute_call(*unparsed_arguments):\n\t\"\"\"\n\tInvokes this script at the specified path, passing the values to standard \"args\".  If no\n\tsub-arguments are given, then arguments to standard \"main\" are given.  If the caller specified\n\tan exit code (zero or negative), then this script exits with that value.  If the caller\n\tspecified a \"timeout\" in seconds, this script exits if it is not terminated within it.\"\"\"\n\tif unparsed_arguments == []:\n\t\targs ="}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_dict = {}\n\tsubparsers_name_iter = iter(subparsers)\n\tfor name, parser in subparsers_name_iter:\n\t\tif isinstance(parser, (ArgumentParser,)):\n\t\t\tresult = parser.parse_args(unparsed_arguments)\n\t\t\tif result.startswith((\"--\", \"--help\")):\n\t\t\t\t# In case help was used (via subcommand --help), remove it\n\t\t\t\thelp_match_pairs = result.replace(\"--help\", \"\", 1).split"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Get the full list of subparsers\n\tns = {}\n\tfor subparser in subparsers.value:\n\t\tnamespace = {'name': subparser.name}\n\t\tns.update(subparser._get_actions().value)\n\t\tns.pop('parser_order')\n\t\tns.pop('argv_elements', None)\n\t# Add \"parser_order\" to the remaining args.\n\tremaining_args = []\n\tfor arg in unparsed_arguments:\n\t\tremaining_args.append((arg[:arg.index"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\treturn ([], unparsed_arguments)"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = subparsers.copy()\n\treturn unparsed_arguments, parser.parse_known_args().args\n\nif __name__ == \"__main__\":\n\tunittest.main()\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tdef add_subparser(subparser_name, subparser_args, subparser_varargs, subparser_varkw, subparser_default):\n\t\tif subscparsers:\n\t\t\tsubparsers[subparser_name].add_parser(subparser_varargs, subparser_varkw, subparser_default)\n\n\tfor varsmall in [True, False]:\n\n\t\tsubparser_varkw_varargs_default = varsmall and [\"--\" + name for name in [\"\", \"--\", \"v\"]] or [\"-"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_by_name = {}\n\tremainder = unparsed_arguments\n\t\n\twhile any(arg in \" \".join(unparsed_arguments) for arg in remainder):\n\t\t# get a name\n\t\tname = unparsed_arguments[0] if unparsed_arguments else None\n\t\t\n\t\t# add parser\n\t\tif name not in subparsers_by_name:\n\t\t\tsubparser = subparsers_by_name[name]\n\t\t\t\n\t\t\t# add arguments\n\t\t\tfor key, value in list("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult_dict = dict()\n\tresult_list = []\n\t\n\tsubset = set(unparsed_arguments)\n\tfor parsed_arg_dict in subparsers:\n\t\tsubset_by = set(parsed_arg_dict) & subset\n\tfor parsed_arg_dict in subparser_actions:\n\t\tsubset_by = set(parsed_arg_dict) & subset\n\tsubparsers_by_name = dict((parsed_dict.get(name, dict()), parsed_dict) for parsed_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif unparsed_arguments == \"\":\n\t\treturn True\n\n\tret = { }\n\tsubparsers_to_parse = set()\n\tfor arg_list in unparsed_arguments:\n\t\tif arg_list[0] == \"--\":\n\t\t\targ_name, args = tuple(arg_list[1:])\n\t\t\tsubparsers_to_parse |= _parse_subparser_arguments(arg_name, args)\n\t\t\targ_list = args\n\t\telif arg_list[0] == \"--\" and len("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\treturn_dict = {}\n\tfor name, subparser in subparsers.items():\n\t\targuments = []\n\t\tfor arg in unparsed_arguments:\n\t\t\tif subparser.parse_args(arg) is None:\n\t\t\t\tcontinue \n\t\t\targuments.append(arg)\n\t\treturn_dict[name] = arguments\n\t\t\n\treturn return_dict\t\t\n\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tfor _arg in unparsed_arguments:\n\t\t# remove -s from --repository if its not present in the subparser argument that triggered\n\t\t# this invocation\n\t\tsubparser_arg = unparsed_arguments[_arg].replace(arg='', sub='', flags='s')\n\t\tsubparser = subparsers.get(subparser_arg)\n\t\tif subparser and subparser._namespace_dict and subparser.__doc__:\n\t\t\tcmd_line, subparser_args = subparser._names"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tresult = parser_cls(self.description())\t\t# Build and return new parser\n\tresults = result.subparsers(menu=True)\t\t\t# Create a sub-parser\n\treturn results\n\ndef make_args_parser():\n\t\"\"\"\n\t\tIf a parser has subparsers, return the top-level parser for\n\t\tthat subparser. Otherwise, return the main parser.\n\t\"\"\"\n\tif self.subparsers == [] or self.subparsers == []:\t# If there is nothing, or if we have a subparser\n\t\treturn parser"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparsers = [\"raw\", \"rst\", \"rst_tables\", \"tables\", \"sage\"]\n\tparser = Parser(parsers=parsers)\n\tparser.add_extensions('ast')\n\tparser.add_extensions('py', extensions=['restructuredtext','sage'])\n\treturn parser\n\ndef get_ast_for_file(filename):\n\t\"\"\"\n\tReturn ast module for a module named in the filename\n\t\"\"\"\n\twith open(filename, 'r') as x:\n\t\tsrc ="}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = TopLevelParser()\n\tsubmodules = [\n\t\tname for name, info in sys.modules.items()\n\t\tif info['P.name'] == 'builtins' or (hasattr(info['P'], 'parser') and isinstance(info['P'].parser, parser.parsers['_class']))\n\t]\n\n\tfor submodule in submodules:\n\t\tif submodule not in _builtins_all_subpackages:\n\t\t\tcontinue\n\t\tsubparser = parse_lib.subpackage(submodules[submodule])\n\t\ttop_level"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Get the command-line parser and subparser parser objects\n\tparser = get_subparsers(title='Argument processing')\n\tparser.add_argument('--verbose',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\tdest='verbose',\n\t\t\t\t\t\tdefault=False)\n\tparser.add_argument('--test-only',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\tdest='test_only',\n\t\t\t\t\t\tdefault=False)\n\tparser.add_argument('--debug',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\tdest='debug',\n\t\t\t\t\t\tdefault=False)"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tpipeline_parser = parsers.PipelineParser(name=\"Test Pipeline Test\")\n\tlogger = logger.config.logger\n\tlogger.info('Parser name in \"pipeline\" is \"Test Pipeline <test>\"')\n\tlogger.info('Parser name in \"pipeline_parser\" is \"MakeParser\"')\n\t\n\tpipeline_parser.add_child_parser(\"test\", parsers.PipelineParser())\n\tsubprocess.call('echo \"Some\\nString\"', shell=True)\n\t\n\tpip"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\treturn parser.parse_make_parser( [ ] )\n\ndef build_parser( **args ):\n\t\"\"\"\n\tCreate a top-level parser, setup argparse and add it to a \n\tfunction result.  \n\n\tArguments:\n\n\t\t**args \tThe arguments.\n\t\"\"\"\n\tparser = parser.parse_args( [ ] )\n\tparser.add_argument( \n\t\t'-l', \n\t\t'--list-files', \n\t\taction ='store_true',\n\t\tdefault = False,\n\t\thelp = \"\"\"\nThe --list-"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# build root parser, a parser for the top-level document\n\tdoc_root = make_parser(make_tree(False))\n\ttree = [doc_root]\n\n\t# create all sub-parsers\n\tfor name, parser in parser.items():\n\t\tfor token_set in parser['tokens']:\n\t\t\ttree.append(parser(tokens=token_set))\n\n\treturn tree\n\nparser_list = (make_parser(make_tree(True)) for _ in ('nltk',])\n#\n#\n#\n# This"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_level_parser = parser\n\n\treturn tuple([top_level_parser(\n\t\tusage=parser.description,\n\t\tconflict_handler='resolve',\n\t)] + list(top_level_parser.sub_commands))\n\nclass TopLevelCommandMetaclass(type):\n\t\"\"\"\n\tMetaclass for top-level commands.\n\t\"\"\"\n\tdef __init__(cls, name, bases, namespace):\n\t\tsuper(TopLevelCommandMetaclass, cls).__init__(name, bases, namespace)\n\t\tif name == '"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tt = top_level()\n\n\tsubs = []\n\ttop_level.parse = parser = myparser.Parser(t, subs = subs)\n\tsubsubs = []\n\n\tfor i, item in enumerate(parser):\n\t\tparser0 = myparser.Parser(item)\n\n\t\tparser.parse(parser0)\n\t\tsubs.append(parser0)\n\n\t\tfor k, val in enumerate(item):\n\t\t\tif val.endswith('_'): continue\n\n\t\t\tval2 = val\n\t\t\tif val2[-2:] in ('_', '"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tpargs, subparsers = make_parsers(list(sys.argv))\n\tparsers = list(pargs)  # type: list\n\treturn parsers, subparsers\n\ndef make_parser():\n\t\"\"\"\n\tCreate the general parser for the command line runner.\n\t\"\"\"\n\tclass MyUsage(usage.Subcommands):\n\t\tparser = build_parser(['help'])\n\t\tsubparsers = build_subparsers(subparsers=None)\n\tparser = MyUsage()\n\treturn parser\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t# Loop through each item in the response.\n\twhile response:\n\t\titem = next(response)\n\t\n\t\t# Iterate through each item in the response.\n\t\tresponse = next(response)\n\n\t\t# Process each item.\n\t\tyield next(item)\n\n\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tobjects = ResponseBody._fetch_objects_by_page(response, max_results)\n\tpaginator = Paginator(objects, max_rows=20)\n\tnum_results = paginator.num_pages\n\tpages = num_results // max_results\n\tobjects_and_page_num = paginator.get_pagination_data(paginator.page(num_results))\n\tpaginated_objects = []\n\tfor obj_and_page in objects_and_page_num:\n\t\tpaginated_objects.append(W"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\tstart = response.page_token_position()\n\tlimit = response.total()\n\tresponse_iter = response.paginate(start, limit)\n\tpages = list()\n\ttry:\n\t\tfor page_count in response_iter:\n\t\t\tpages.append(wapi.WAPILimitedListResponsePage(page_count, response_iter=page_count, response=response, max_results=max_results))\n\texcept wapi.WAPIResponseException, exc:\n\t\tif exc.error_code in [pga.p"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor page in response:\n\t\tyield {'count': page['count'], 'total': page['count'] + 1 - max_results, 'next': None, 'previous': None}\n\t\tfor row in pager:\n\t\t\ttry:\n\t\t\t\tpage_row = {'count': row['count'], 'total': row['total'], 'next': page['next'], 'previous': response[int(row['page'])-1]}\n\t\t\t\treturn {'page': page_row, 'total': row['total'], 'next': None, 'pre"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t# We will calculate the number of objects.\n\ttotal_count = int(response.meta.results_per_page)\n\toffset = 0\n\n\t# Iterate through all results, then call get_page\n\tfor page in response:\n\t\tyield page\n\t\tif offset + page.meta.results_per_page > total_count:\n\t\t\tbreak\n\n\t\t# Iterate through all results, then get the next, which is the last page object with more results.\n\t\tfor page_ in response:\n\t\t\tif page_.meta.results_per_page:\n\t\t\t\ty"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tyield response\n\tfor page_num in range(max_results)+1:\n\t\tresponse = list(self.wapi.select(\"next={}\".format(urljoin(page_num, page))) ) \n\t\tyield response\n\ndef _to_api_urls(urls):\n\t\"\"\"\n\tConverts a list of strings into WAPI API urls.\n\t\n\tArgs:\n\t    urls (list): List of strings\n\tReturns:\n\t    List of WAPI URLs.\n\t\"\"\"\n\t#urls = [url for url"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor resp, obj in response:\n\t\tyield obj\n\t\tif obj.has_more_results is True:\n\t\t\tpage = int(obj.result_set.offset / max_results)\n\t\t\tfor i in range(page*max_results, (page + 1) * max_results):\n\t\t\t\ttry:\n\t\t\t\t\tyield obj\n\t\t\t\texcept WapiException as e:\n\t\t\t\t\tif e.code == 404:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\traise\n\n\twhile obj == None:\n\t\ttry:\n\t\t\tobj = yield obj\n\t\texcept WapiException as e:\n\t\t\tif"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif PY2:\n\t\titer = response.iters\n\telse:\n\t\titer = iter(response.page)\n\tdef iterable():\n\t\treturn True\n\ttry:\n\t\twhile True:\n\t\t\tyield iter\n\texcept StopIteration:\n\t\tpass\n\t\n\tmax_results = max_results or 10\n\tfor i in range(len(iter)/max_results):\n\t\tpage = iter.next()\n\t\tyield page.page\n\tif not iter:\n\t\titer = iterable()\n\t\ndef _generator(start, stop"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\toffset = 0\n\treturn (response, offset, limit, max_results)\n\ndef add_headers(headers=None):\n\t\"\"\"\n\tAdds headers to the response object.\n\t\n\tIf no headers argument is given, returns a copy of the headers object.\n\t\n\tArgs:\n\t    headers (dict): The list of headers.\n\tReturns:\n\t    headers (dict): Returns a copy of the headers object.\n\t\"\"\"\n\tif headers is None:\n\t\treturn headers\n\theaders_copy = headers.copy()\n\theaders_copy = headers_"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpages = []\n\tpage_number = 1\n\twhile page_number <= max_results:\n\t\tpage = None\n\t\tfor data in response[page_number * paginators_page_size : paginators_page_size * page_number]:\n\t\t\ttry:\n\t\t\t\tpage = list()\n\t\t\t\tfor each in data:\n\t\t\t\t\tfor row in each:\n\t\t\t\t\t\trow_element = (row[0], row[1])\n\t\t\t\t\t\t# Ignore non text tags\n\t\t\t\t\t\tif len(row) >= 4:\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\trow_element = map(str, row"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tdigits = [\"k\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"]\n\t\n\ttry:\n\t\tsize, _ = size.strip(\"/\").split(\"/\", 1)\n\texcept ValueError:\n\t\t# It was probably specified as raw or specified in a units\n\t\t_, size = size.split(\"=\", 1)\n\t\tunits = size\n\tif not size:\n\t\treturn 0\n\t\n\treturn int(size) * float(digits[units.find(\".\")] if units else 1)"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tsize = size.strip('%')\n\t\n\tif 'k' in size:\n\t\tsize = size.split(\"k\")[0]\n\tsize = size.strip('%')\n\t\n\treturn int(round(size))\n\n\ndef compute_volume(vols: dict, min_num_vols: int) -> dict:\n\t\"\"\"\n\tCompute the volume of a file\n\t\n\t\tArgs:\n\t\t    vols: A dict in the form of {'file_id': int,'size_in_bytes': str}\n\t\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\t_, size = size.split(\" \")\n\t\n\ttry:\n\t\treturn int(float(size) * 1e6)\n\texcept ValueError:\n\t\treturn  size\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn int(float(size)) * 1024\n\"\"\"\n\tConvert decimal file size to bytes\n\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A decimal representation of file size (eg: 500, 1000)\n\t\n\tReturns:\n\t    a string representing bytes representation of the number of bytes in the specified size\n\n\"Examples\":\n<<<<<<< HEAD\n\t    >>> file_size_to_bytes(\"200K\")\n\t    2000000000\n\t    >>> file_size_to_bytes(\"2"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tx = int(size)*1024\n\tif x < 256:\n\t\treturn int(x)\n\telse:\n\t\treturn int(x)//256\n\t\ndef is_integer(x: int) -> bool:\n\ttry:\n\t\ty = float(x)\n\texcept ValueError:\n\t\treturn False\n\n\ttry:\n\t\tn = int(y)\n\texcept ValueError:\n\t\treturn False\n\n\tif n is None or n < 0 or n > 10 ** -6:\n\t\treturn False\n\telse:\n\t\treturn True\t\t\n\ndef get"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tsize_in_bytes = float(size) / 1024\n\treturn int(round(size_in_bytes))\n\ndef format_seconds(seconds: int) -> str:\n\t\"\"\"\n\tFormat seconds to a human readable format\n\t\t\n\tArgs:\n\t    seconds: An integer representing a quantity of seconds\n\t\t\n\tReturns:\n\t    A human readable string representing the specified seconds\n\t\"\"\"\n\treturn \"%.1f\" % seconds\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tif not isinstance(size, str):\n\t\traise TypeError(f\"Expected a [str], {str} got {type(size).__repr__()}\")\n\t\n\tvalue = int(0)\n\t\n\t# Remove decimal point\n\tif size.startswith(\".\") and size.endswith(\".\"):\n\t\tsize = size[1:-1]\n\t\n\t# Remove \"K\"\n\tif size.startswith(\"K\") and size.endswith(\"K\") and size[0] == \"K\":\n\t\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tif size_validator:\n\t\tlength = val_validator.validate(size)\n\t\tif length is not None:\n\t\t\treturn float(val_validator.get_val(\"File size\", size))\n\n\n\treturn file_system.get_size(size)"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tdef size_to_bytes(size: float) -> Decimal:\n\t\n\t\treturn Decimal(round(abs(sys.float_info.max) / sys.float_info.max)) * 1024\n\n\ndef size_to_bytes(_bytes: int) -> Decimal:\n\t\"\"\"Convert human readable bytes object to bytes. Example: `_bytes = 1232739`\n\tSee:\n\thttps://github.com/skegg/skegen\n\t\"\"\"\n\treturn Decimal(sys.float_info.max) * sys.float_"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\timport binascii, math\n\tfrom binascii import Error as BinasciiError \n\tfrom math import ceil\n\t\n\ttry:\n\t\tresult=int(\"%sM\"%size.encode('hex'),16)\n\t\treturn ceil(float(result)/512.0)\n\texcept TypeError:\n\t\t# no unit\n\t\treturn int(size.encode('hex'),16)\n\texcept Exception as E:\n\t\traise BinasciiError (\"Unable to round size: %s\"%E)\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresult = dict()\n\tfor _dict in dicts:\n\t\tvalue = dict()\n\t\tfor k, v in _dict.iteritems():\n\t\t\tvalue[k] = v\n\t\t\tif not value['__default']:\n\t\t\t\tvalue['__default'] = v\n\t\tresult.update(value)\n\treturn result\n\t\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\trv = {}\n\t\n\tfor d in dicts:\n\t\tif d.has_key('b'):\n\t\t\trv[d.get('b', 0)] += min(d.get('a', 0), rv.get('b', 0), d['a'])\n\t\telif d.has_key('a'):\n\t\t\trv[d.get('a', 0)] += min(d.get('b', 0), rv.get('a', 0), d['b'])\n\t\telse:\n\t\t\trv[d.get('"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tfor i in iter(dicts):\n\t\tyield i\n\treturn\n\t\ndef _maxlen(*iterable):\n\t\"\"\"\n\tGet the length of the longest substring in the iterable (in terms\n\tof length over all values).\n\t\"\"\"\n\treturn max(it + len(x) for x in iterable)\n\ndef _find(list, x):\n\t\"\"\"\n\tFind the first occurrence of x in the list.\n\tUsed to support the `sortby` keyword.\n\t\"\"\"\n\tfor i, v in enumerate(list):\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn {}\n\t\n\nif __name__ == '__main__':\n\tdf = _dictsum(['a', 'b', 'c'])\n\tprint(df)\n\t\n\tdf2 = _dictsum(['a', 'b'])\n\tprint(df2)"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\tdct = {}\n\tfor d in dicts or []:\n\t\tdct[d.get(k, 0) for k in sorted(d.keys())]\n\tres = {}\n\tfor k, v in dct.items():\n\t\tres[tuple(map(lambda x: abs(x.get(k, 0)), v \n\t\t\t \t if len(v) else [], [], []))] = k\n\treturn res\n\ndef normalize_lists(dicts, delimiter, sort):\n\t\"\"\"\n\t\tThis function sorts a"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresults = {}\n\tfor x, y in iteritems(dicts):\n\t\tresults.update(x)\n\t\tresults.update(y)\n\treturn results\n\n\ndef _dictdiff(dicts1, dicts2):\n\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictdiff([{'a': 1, 'b': 2}, {'a': 1, 'b': 0}], \n\t                         [{'a': 5, 'b': 0}, {'a': 0, 'b': 0"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\tdef __dictsum__(dictionary):\n\t\tdictionary_items = dictionary.items()\n\t\tif len(dictionary_items) <= 1:\n\t\t\treturn sum(dict(d) for d in dictionary_items)\n\t\telse:\n\t\t\treturn sum(d_plus_other for d_plus_other in dictionary_items)\n\n\treturn map(__dictsum__, dicts)\n\n\nclass Model(object):\n\tdef __init__(self):\n\t\tself.data = {}\n\t\tself.weight_sum = 0\n\t\t\n\tdef add("}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\td = {}\n\tfor entry in dicts:\n\t\td.update(entry)\n\treturn d\n\ndef _setcount(set, entry, value):\n\t\"\"\"\n\tCount set of items with a specified value.\n\n\t>>> setcount(set([1,2,3,4],'n'))\n\t2\n\t\"\"\"\n\ttotal = 0\n\t_iter = iter(set)\n\telement = next(_iter)\n\t_iter = iter(_iter)\n\twhile element is not None and element[0] == entry:\n\t\ttotal += 1\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\ttotal = {}\n\tfor dic in dicts:\n\t\ttotal.update(dic)\n\treturn total\n\n\ndef _calc_tuple_sum(dicts_1):\n\t\"\"\"\n\t\tGrows the total value of the dictionay values.  \n\t\"\"\"\n\ti = 0\n\tsum = 0\n\tfor dic in dicts_1:\n\t\tsum += dic['a'] + dic['b']\n\t\ti += 1\n\t\ti = i % len(dicts_1)\n\treturn sum\n\n\ndef _dictsum2("}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tdum = {}\t\n\tfor d in dicts:\n\t\tif hasattr(d, 'to_bytes'):\n\t\t\td = d.to_bytes(8)\n\t\t#print 'combining', type(d)\n\t\t#if isinstance(d, bytes):\n\t\t#\td = d.decode(encoding)\n\t\tfor key in d:\n\t\t\ttmp = set(d[key])\n\t\t\tdum[key] = sum(tmp)\n\treturn dum\n\"\"\"\n\t\nf.write('\\n')\nf.write('#"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor key, val in url_args.items():\n\t\tif isinstance(val, dict):\n\t\t\turl = url.replace('%s={' % key, val.__json__())\n\t\t\turl_args[key] = val.__json__()\n\t\telif isinstance(val, list):\n\t\t\t# Use the first item in the list\n\t\t\t# as the original\n\t\t\turl = url.replace('%s{' % key, '%s=:$0')\n\t\t\turl_args[key] = val[0]\n\t\telse:\n\t\t\turl ="}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif not url:\n\t\turl = \"%s[%s]\" % (url_args['repo'], url_args['link'])\n\treturn re_string(url)\n\n\nclass BaseRepo:\n\n\tdef __init__(self, **kwargs):\n\t\tself.__kwargs = kwargs\n\n\t@classmethod\n\tdef default(cls, name, url='https://api.github.com/repos', **kwargs):\n\t\treturn cls(**kwargs)\n\n\t@staticmethod\n\tdef _create_args(arg):"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tdef _convert(match):\n\t\treturn urlsplit(match.group('url')).query\n\n\turl = url.replace(_replace_url_args(url, url_args), _convert)\n\n\t# Convert params\n\tdef _convert_params(match):\n\t\ttry:\n\t\t\tif match.group('param'):\n\t\t\t\tk = match.group('param').lower().strip()\n\t\t\t\treturn PARAMS + '[?' + k + ']'\n\t\t\telse:\n\t\t\t\treturn PARAMS + '[?' + match.group('field').lower().strip() +"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_args = url_args.split(',')\n\tfor i in range(len(url_args)):\n\t\tif url_args[i].lower() == 'default':\n\t\t\turl_args[i] = default_value\n\t\tfor key in url.keys():\n\t\t\tif url[key] == url_args[i]:\n\t\t\t\turl[key] = url_args[i]\n\n\treturn url\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\treturn url.replace('{{%s}}' % url_args.get(\"base_url\"), url_args[\"link\"])\n\n\ndef _get_tag_from_url(url, data_obj):\n\t\"\"\"\n\t\tReturn the tag associated with the url \n\t\t\"\"\"\n\treturn Tag.tag(url, data_obj)\n\n\ndef _get_repost_link_url(repost_data, data_obj):\n\treturn data_obj.pop(\"repost_link\", None)\n\n\ndef _get_repost_text(repost_data"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t# if we are not using query string, we should not alter params because it may be overriden by query string\n\tglobal query_string\n\tif not hasattr(url, 'query') and not url.get('format') == 'querystring':\n\t\tquery_string = url.get('q', '')\n\t\tparams = url.get('params', url.get('kw', {}))\n\t\tquery_string = parse_qs(query_string, True)\n\t\t# if params is not available, we should return with no params\n\t\tif not params:\n\t\t\treturn None"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_args = dict([(key[1:], val) for key, val in url_args.iteritems()])\n\treturn url%url_args\n\n\nclass BaseApiTestCase(unittest.TestCase):\n\tdef setUp(self):\n\t\tself.auth = (user, password, name)\n\t\tself.client = Client(auth=self.auth, name='Test Client')\n\n\tdef tearDown(self):\n\t\tpass\n\n\nclass Test_Api_Base(Test_ApiBase):\n\tdef test_create(self):\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tdef _replace_part(match):\n\t\tnewarg = match.group().replace(\"&\",\"\")\n\t\tif newarg == \"&\":\n\t\t\tnewarg = \"%s*\" % match.group()\n\t\treturn newarg\n\turl_repl = re.compile(\"(https?:.+|^)([\\w+:#%@-]*)://.*\")\n\treturn url_repl.sub(_replace_part, url_args)\n\n\ndef check_if_url(url):\n\t\"\"\"\n\tCheck that each URL is of the right form\n\t\"\"\"\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\treturn url_args.replace(\",,\", \",\").replace(\", \", '').replace(\"\\\"\", \"\").replace(\",\", \"\")\n\t\n\n\ndef _replace_url_args2(url, url_args):\n\"\"\"\n\tReplace all custom string URL items with values in args (using \",\")\n\t\"\"\"\n\tret=True\n\turl_ret=[]\n\tfor arg in url_args:\n\t\turl_ret.append(str(arg))\n\t\tif \",\" == arg:\n\t\t\tret=False\n\treturn ret, url_ret\n\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\treturn url_args.replace(' ', '')\n\n\nclass SimpleXMLRPCServer(SimpleXMLRPCRequestHandler):\n\t\"\"\"\n\t\tAdds a \"content-length\" header to requests\n\t\twith a known size\n\t\t\"\"\"\n\tdef handle(self):\n\t\ttry:\n\t\t\tself.server.serve_forever()\n\t\texcept Exception, e:\n\t\t\tself.server.close()\n\t\t\tlog.error('XMLRPC server failed (%s, %s)' %\n\t\t\t\t(e.__class__.__name__, sys.exc_info()"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val is None:\n\t\treturn True\n\t\t\n\treturn False\n\ndef is_text(val: any) -> bool:\n\t\"\"\"\n\tCheck if a value can be represented as a Python scalar which is not\n\tan object.\n\n\tThis is a useful internal use function, called by the internal text\n\trepresentation functions of the python scalar type list.\n\n\tExample:\n\t\tprint(u'\"foo bar\"'\n\t\t\t== u'foo\\r\\nbar\\n'\n\t\t)\n\t\"\"\"\n\tif isinstance(val, unicode):\n\t\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn not isinstance(val, NoneType) or is_string(val)\n\t\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\ttry:  return val is None\n\texcept NameError:  return False\n\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn isinstance(val, basestring) and val = val.replace(None, \"\"); # Python2 or3\n\t\ndef is_none_integer(val: any) -> bool:\n\t\"\"\"\n\t\tCheck if a number is None (or equals to) 0, 1, or 2.\n\t\tNote that this doesn't consider the integer base to be necessarily the given one.\n\t\"\"\"\n\tassert isinstance(val, int) or isinstance(val, float)\n\tif val == 1 == 0: # 0!=\n\t\treturn True # 0!=\n\tif val"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None\n\n\n\n\nclass TupleType(TypeBase):\n\t\"\"\"\n\tA subclass based on type \"()\" to indicate a sequence.\n\t\"\"\"\n\tdef __init__(cls, parent_cls, elem_cl) :\n\t\t\"\"\"\n\t\tConstructor of TupleType.\n\n\t\tCls argument:\n\t\t \t- parent_cls: the subclass we want the result class of, or\n\t\t \t   None to indicate that child_elem_cl must be a subclass of parent_cls.\n\t\t \t- elem_"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn not (val is None)\n\ndef is_list(s: \"type[\" + str(type(list()).__name__) + \"]\") -> bool:\n\t\"\"\"\n\tCheck if a list is an iterable.\n\t\"\"\"\n\treturn isinstance(s, list)\n\ndef is_number(s: \"type[\"+str(type(int)).__name__]+\"\") -> bool:\n\t\"\"\"\n\tCheck if a number is an integer.\n\t\"\"\"\n\treturn math.isinstance(s, int)\n\ndef is_integer("}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val is None:\n\t\treturn True\n\treturn False\n\n\ndef is_not_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents an unknown value.\n\t\"\"\"\n\treturn not val\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None\n\n@property\ndef is_none_integer(val: any) -> bool:\n\tif isinstance(val, int) or isinstance(val, float):\n\t\treturn False\n\n\treturn val is None\n\n@property\ndef is_none_boolean(val: any) -> bool:\n\tif isinstance(val, bool):\n\t\treturn False\n\n\treturn val is None\n\n@property\ndef is_none_string(val: any) -> bool:\n\tif isinstance(val, basestring):\n\t\treturn False\n\n\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn isinstance(val, str)\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\ndef is_none_empty(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents an empty value.\n\t\"\"\"\n\treturn (0 == len(val) and\n\t\t\"%s\" % val.strip().replace(\"\\n\", \"\") ==\n\t\t'')\n\n\ndef is_not_none(val: any) -> bool:\n\t\"\"\"\n\tCheck if a value is not None (and not a string)\n\t\"\"\"\n\treturn not isinstance("}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = '-'\n\tfor flag in parser.flag_parser_helpers.flags:\n\t\tflags += flag +''\n\tflags = flags.rstrip()\n\treturn flags\n\n\ndef add_parser(name, *args, **kwargs):\n\t\"\"\"\n\tAdds a subparser with the given argparse.ArgumentParser object as its subparser.\n\n\tThe subparser can be constructed from any number of subcommands using\n\tthe subparser command string. So, for instance, you could\n\tcall add_parser(cmdstring, argparse.ArgumentParser"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn str(parser) # pylint: disable=E1101\n\n\t\n\n@contextlib.contextmanager\ndef context(value=True):\n      # pylint: disable=unused-argument\n      \n      \"\"\" Context helper \"\"\"\n      with contextmanager:\n          yield context\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.get_args() if parser.has_args() else []\n\nclass Error(Exception):\n\t\"\"\"\n\tAn exception to return to the parent application's handler.\n\t\"\"\"\n\tpass # NOQA\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(parser.flags).split()\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser = parser.actions.split(' ',1)\n\tif len(parser) == 1:\n\t\treturn \" \".join(parser[0].names)\n\telse:\n\t\tparser[0] = parser[0].replace(',','')\n\t\tparser[1] = \"\"\n\t\treturn \" \".join(parser)\n\n\t# This makes a parser.Namespace instance ready for usage in an argument parser.\n\n\t# Return a space-separated string of the argument values\n\t\n\t# We do the following:\n\n\t#\t+ (and the following if __name"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser_out = []\n\tparser.add_argument('--help', '-h', action=\"store_true\", default=False,\n\t\t\t\t\t\t dest=\"helpflag\", help=\"print this message at help\")\n\tparser.add_argument('--version', '', action=\"store_true\", default=False, dest=\"versionflag\",\n\t\t\t\t\t\t help=\"print this message at version\")\n\tparser_flags = parser_out\n\n\treturn parser_flags\n\n@utils.catch_exceptions\ndef parser_error_message(parser):\n\t\"\"\"\n\tReturn the error message with the"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# TODO - it's ugly to have a separate method per parser here\n\t#   so that the individual paterns will be a bit more concise.\n\tflags_string_repr = \"|\".join([flag for flag in parser.get_help().split() if flag])\n\treturn flags_string_repr\n\ndef parse_args(argv=sys.argv[1:]):\n\t\"\"\"\n\tArgs are the first args, unless the command-line provides more of.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"testproject"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.format_help() if parser.help else None\n\ndef get_help(parser_flags):\n\t\"\"\"\n\tReturn help string associated with parser_flags.\n\t\"\"\"\n\tif parser_flags:\n\t\treturn \"Command description: \" + parser_flags.split() \\\n\t\t\t\t.replace(\"\\n\", \"\\n\" + \"\\n\" * 9)\n\telse:\n\t\treturn \"\"\n\ndef parser_format(parser):\n\t\"\"\"\n\tReturns the help of parser.\n\t\"\"\"\n\treturn get_help(parser.format_help())\n\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn list(parser.formatter.format_flags(formatting_flags=True))\n\n\ndef parser_version(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its version number.\n\t\"\"\"\n\treturn parser.prog\n\n###############################################################\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# return the string, which represents the parser\n\treturn parser.format_help().split()\n\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif verbose:\n\t\tverbose = set().union(path_name)\n\t\tverbose = set(verbose)\t\n\t\tverbose = verbose.drop(path_name)\t\n\t\tverbose = set(verbose)\n\t\tverbose = list(verbose)\n\t\tverbose.reverse()\n\tif verbose:\n\t\tverbose_set.add(path_name)\n\telse:\t\n\t\treturn False\n\t\n\tfor subdirectory in sub_path_and_subdirectory:\n\t\tsubdirectory_name = subdirectory+path_name"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tpathname = abspath(path_name)\n\t# If the file and dir has already been processed call the callback once\n\t# so we don't go into an infinite recursion.\n\tif processed.add(pathname):\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef abort_process_path(abort=False):\n\t\"\"\"\n\tRun a path that can't be done with a callback, i.e. the callback has\n\tto return a value.\n\t\n\tTo abort, pass True as the `abort` flag to the callback.\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tpath = path_name.lstrip(path_name.rfind('/') + 1)\n\t\n\tif path:\n\t\t\n\t\t# Get the index to use to find the set\n\t\tidx = len(processed)\n\t\tidx = min(idx, len(processed))\n\t\n\t\t# Check if this entry or directory has already been processed\n\t\tif processed.has_key(path):\n\t\t\t\n\t\t\treturn True\n\t\t\n\t\t# Store the path as the key into the set\n\t\tprocessed[path] = path\n\t\t\n\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tfor entry in processed:\n\t\tif verbose:\n\t\t\tprint \"\\n\\tProcessed item path '%s', processed\", path_name\n\t\t\tprint \"\\t-->\", entry\n\t\tif path_name == entry:\n\t\t\treturn True\n\tif verbose:\n\t\tprint \"\\t\\tNo matching entry found for path '%s'\", path_name\n\treturn False\n\nclass Paths:\n\n\t_processed = set()\n\t_excluded = set()\n\t\n\tdef set_excluded(self, path_name, reason):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\texisting_paths = set()\n\texisting_paths.add(path_name)\n\tif isinstance(processed, int) or processed == path_name or path_name in processed:\n\t\treturn True\t# this path has already been processed, so return\n\t\n\tif verbose:\n\t\tprint \"Checking if path\" + path_name + \"is already in the set of processed pathnames... \"\n\t\tfor x in processed:\n\t\t\tprint \"    \" + str(get_canonical_pathname(processed, x))\n\t\n\tif os"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\timport os\n\n\t# expand path\n\tpath = path_name.expand_dots()\n\n\t# check if path name is a file\n\tif os.path.isfile(path_name):\n\t\t# return if not already in path\n\t\tif not path in processed:\n\t\t\tprocessed.add(path)\n\t\t\t\n\t\t\tfor fname in os.listdir(path):\n\t\t\t\tif not fname[-4:] == '.gz':\n\t\t\t\t\trel_path = path_name.rel_to(path)\n\t\t\t\t\t\n\t\t\t\t\tif rel_path!= path:\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tif verbose:\n\t\tmessage = \"Already processed '{}'. Existing entries: {}.\"\n\telse:\n\t\tmessage = \"Already processed '{}'. Existing entries: {}.\"\n\t\n\tif path_name in processed:\n\t\treturn True, message.format(path_name, path.dirname(processed[path_name]), processed[path_name])\n\telse:\n\t\treturn False, message.format(path_name, processed[path_name], path.dirname(path_name))\n\n\n###\n# Function to set verbose"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tif verbose:\n\t\tprint(\"Processing %s as set\" % path_name)\n\n\tif not os.path.isdir(path_name):\n\t\treturn False\n\t\t\n\tset = set()\n\t\n\t# Expand the path into an absolute one\n\tfull_path = os.path.abspath(os.path.join(path_name, '..'))\n\t\n\tfor item in os.listdir(full_path):\n\t\tpath = os.path.join(full_path, item)\n\t\tif os.path."}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif verbose:\n\t\tprint('Processed path %s' % (path_name,))\n\ttry:\n\t\texpand(path_name, {})\n\t\tentry = path_name in processed\n\texcept OSError:\n\t\tentry = False\n\treturn entry\n\n\t\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t# Create the set for entries we're going with in the path\n\tif verbose:\n\t\tprint \"Checking {}\".format(path_name)\n\t\tfor entry in processed:\n\t\t\tprint \"    Adding {}...\".format(entry)\n\t\t\n\t# See if the set contains the path\n\tif os.path.exists(path_name):\n\t\tis_problem = False\n\t\tif verbose:\n\t\t\tprint \"Processing directory\", path_name\n\telse:\n\t\tis_problem = True\n\t\n\tif is_problem:\n\t\tmessage"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn [vertices[0][3], (vertices[0][0]+vertices[1][0])/2.0, (vertices[0][1]+vertices[1][1])/2.0, (vertices[0][2]-vertices[1][2])/2.0] if\n\t\t0<=n<len(vertices) else (vertices[0][3], (vertices[1][0]+vertices[2][0])/2.0, (vertices[2][1]+vertices[1][1])/2"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tif len(vertices)!= 3:\n\t\traise ValueError(\"Must pass a 3 tuple of vertices, or the return of polygon()\")\n\treturn vertices\n\ndef normalize(*a):\n\treturn normalize3(a)\n## @deprecated'scrapy:normalization' is deprecated in scrapy release\n## This will be removed in scrapy release\nnormalize = normalize3\nnormalizedPolygons = namedtuple(\"normalizedPolygons\", \"vars_all\")\n\ndef allPolygons(vertices_or_f"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tp1x = vertices[0][0]\n\tp1y = vertices[0][1]\n\tp2x = vertices[-1][0]\n\tp2y = vertices[-1][1]\n\n\tp3x = vertices[-2][0]\n\tp3y = vertices[-2][1]\n\n\treturn vertices, [p1x, p1y, p2x, p2y, p3x, p3y]\n\n\n# ------------------------------------------------------------------------\ngps_data = [['5',2,"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tif not len(vertices):\n\t\t#if no vertices\n\t\treturn ()\n\t\n\tdef vertexarray(vertices):\n\t\t\"\"\"\n\t\tConvert a list of vertices to an array.\n\t\t\"\"\"\n\t\t#dont need to check for degenerate cases\n\t\ttry:\n\t\t\treturn [x + y for y in vertices]\n\t\texcept:\n\t\t\treturn [x + y for (x,y) in enumerate(vertices)]\n\n\tdef point_to_tuples(point):\n\t\t\"\"\"\n\t\tConvert a tuple of points into 3points tuple"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn list(set(vertices))\n\n\n################## Vertex Points #############################\ndef vertexpoints(vertices):\n\t\"\"\"\n\tReturns the 3 points where the face intersects the given vertices.\n\t\n\tNOTE:\n\tThe method will take the first point to find the closest possible vertex in the face given. \n\t\n\t>>> facePoints(vertices) == vertex3points(vertices)\n\t\"\"\"\n\treturn vertex3tuple(vertices)[vertices.index(f_verts[0])][vertices.index(f_verts[1"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn sum(len(vertices)/2)\n\t\ndictionary = vertextuple(polygon)\n\n#dictionary = (0,1,1,1,1)\n\nfor i in range(0,len(dictionary)-1):\n\tif dictionary[i] == 0:\n\t\tprint \"BEGIN IN\",i,\"with a vertex\"\n\telse:\n\t\tprint \"BEGIN IN\",i,\"with another vertex\"\n\t\n\tx = random.randint(0,len(polygon)-1)\n\ty = random.randint(0"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\treturn zip((vertices[0], vertices[1], vertices[2]), vertices)\n\n\t\n\ndef printPolygonPoly(nx,ny,polygon,i,j):\n##\tprint 'nx: ', nx\n\t#print type(nx)\n\tfor i in range(nx+1):\n\t\tfor j in range(ny+1):\n\t\t\tif i >= 0 and i+j < nx and j >= 0 and j+i < ny:\n\t\t\t\tprint(polygon[i+j*nx], i"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tvertices.append(vertices[0])\n\tvertices.append(vertices[1])\n\tvertices.append(vertices[2])\n\t#returns\n\t#ABCDDAB\n\tprint('vertices with verts')\n\tverticestuple= list(vertices)\n\tprint(verticestuple)\n\treturn verticestuple\t\n\n#==============================================================================\n# test function\n\t\nfrom scipy.ndimage import imread\n#imread(\"Data file.txt\")\t#returns data from file\n#data"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tv1 = (vertices[0], vertices[1])\n\tv2 = (vertices[2], vertices[3])\t\n\treturn [v1, v2]\n\n\ndef polygon_inside_cancellable(polygon):\n\t\"\"\"\n\treturn whether the polygons are inside cancellable polygons.\n\t\n\tpolygon is a polygon with points on both sides of the polygon. \n\n\twill return False or else will call polygon_inside_all and return True if either points are inside or will return False otherwise\n\t\n\tif"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn zip(vertices[0], vertices[1], vertices[2])\n\n\t\n\n\t\n\t\n\n\n### TODO: Add an object for each point with any of its neighbors or edges\n\ndef neighbor_of_point(point):\n\t\"\"\"\n\treturn list of neighbors of point i from left to right\n\n\tinput: an integer\n        output: list[integer]\n\t\"\"\"\n\tneighbors = [point]\n\tpoints = []\n\twhile(neighbors):\n\t\t#get neighbor\n\t\tneighbor = neighbors.pop(0)\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tnumber_list = alphabet[-1] if alphabet else \"\"\n\toutput = \"\"\n\tfor number in reversed(number):\n\t\tdigit = str(number % alphabet)\n\t\tnumber_list = number_list + digit + digit\n\t\tif len(number_list) > padding:\n\t\t\toutput += number_list[:padding - 1]\n\t\telse:\n\t\t\toutput += number_list\n\tif output:\n\t\toutput = \"%%0%s%%0%s\" % (alphabet, output)\n\treturn output\n\"\"\"\n\"\"\"\ndef generate_random_alphabet"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tpadend = ''\n\tif padding:\n\t\tpadend = b'\\n' + padding\n\tretstring = ''\n\tfor x in reversed(range(number)):\n\t\tretstring += '{0}{1}{2}{pre}'.format(alphabet[x], x+1, padend)\n\tretstring += '{0}{1}{pre}'.format(alphabet[number], number)\n\tretstring += b'\\n'\n\tretstring += padend\n\t\n\treturn retstring.ljust(len(retstring))\n\t\t\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\ttry: value = number // 10\n\texcept ZeroDivisionError: raise ValueError\n\tresult = [alphabet[0] + str(value % 10)]\n\twhile value!= 0:\n\t\tresult.append(alphabet[10] + str(value % 10))\n\t\tvalue /= 10\t\t\n\tif padding is None:\n\t\treturn ''.join(result)\n\telse:\n\t\treturn ''.join(result) +'' + str(padding)\n\t\t\t\n\t\n\n\nif __name__ == \"__main__\":\n\tint_to_number('"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tsign = \"-\" if number < 0 else \"\"\n\n\tif padding is None: padding = \"\" * int(len(str(abs(number))))\n\n\tif alphabet in set(str(int(alphabet)) for (int,alphabet) in itervalues(number)):\n\t\treturn str(abs(number) - padding + sign)\n\telif alphabet is u\"\" and padding is u\"\":\n\t\treturn number.string()\n\treturn u\"{}\".format(sign + str(abs(number))) + padding.join(str(abs(number)))"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t# the string conversion functions\n\tn_to_str = int_to_string_to_alphabet\n\tstr_to_num = str_to_int_to_num\n\tint_to_dict = list_to_dict\n\tfloat_to_string = float_to_str\n\tstring_to_int = string_to_int_dict\n\tnum_dict_to_ints = num_dict_to_ints\n\tbool_to_bool = bool_to_int\n\t\n\t# the number of digits after the first number\n\tn_"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\ts = \"\"\n\tfor d in reversed(range(0, 16, 2)):\n\t\tif number % d == 0:\n\t\t\ts += alphabet[d - 1]\n\t\telse:\n\t\t\ts += alphabet[d]\n\treturn s\n\t\n\t\ndef int_to_string(number: int, alphabet: str):\n\ts = \"\"\n\tfor z in reversed(range(0, 16)):\n\t\tif number % z == 0:  # z is zero\n\t\t\ts += alphabet[z]\n\t\telif number > 0 and z == 0:  "}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tbits = []\n\tfor e in str(number):\n\t\tif e >= 0:\n\t\t\tbits.append([e])\n\t\telse:\n\t\t\tbits.append([-e])\n\toutput = ''.join(bits)\n\tpad = padding or len(output)\n\toutput = output.ljust(pad)\n\treturn output\n\ndef print_dict(items: dict, key: str) -> None:\n\tif not isinstance(items, dict):\n\t\traise TypeError('items must be dict, not'+ str(items))\n\t\t\n\titems ="}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif padding is None:\n\t\tpadding = (0, )\n\t\t\n\tresult = ''\n\t\n\t# if it's a negative number, then add a minus sign at the end\n\tif number < 0:\n\t\tsign = '-'\n\t\tif alphabet.index(sign) >= 0:\n\t\t\tresult += sign * alphabet[-1]\n\t\t# if it is a prime number, add the prime count\n\telse:\n\t\tif alphabet.index(sign) == -1:\n\t\t\tassert alphabet[-1] == '0'\n\t\t\n\t\tpending_numbers"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tsign = str(sign(number))\n\tif padding is not None or int(number) < 0:\n\t\tpad = \"0\" * padding\n\t\tif int(number) > 0:\n\t\t\tsign = \"-\" + str(sign)\n\t\n\tout = \"\"\n\tfor i in range(len(alphabet)):\n\t\tif abs(number) < 10:\n\t\t\tout = alphabet[i] + sign\n\t\telif abs(number) < 100:\n\t\t\tout = digits(alphabet[i], padding) + alphabet[i]\n\t\telse:\n\t\t\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\tdef _pad(str_size: int, padding: Optional[int] = None, **args: Any) -> str:\n\t\tif padding:\n\t\t\treturn '{}0'.format(padding) * str_size\n\t\treturn '0' * str_size\n\n\tdef _unpad(str_size: int, **args: Any) -> str:\n\t\treturn str_size * '.0'\n\n\tstr_size = type_name_to_size_and_digits[str(number).__class__]\n\treturn num2str(str(number"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tpass\n\ndef _replace_register(flow_params, register_number):\n\t\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: the number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tpass\n\ndef _replace_register(flow_params, register_number, register"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif flow_params.get(register_number):\n\t\tflow_params[register_number] = register_value\n\telse:\n\t\tflow_params[register_number] = register_value  # Register is already stored here!\n\ndef _get_flow_params_for_flow(flow_params):\n\t\"\"\"\n\tGet all parameters for a given flow\n\t\n\t:param flow_params: Dictionary containing flow\n\t\"\"\"\n\treturn {\n\t\t\t\t'from': 'From',\n\t\t\t\t'to': 'To',\n\t\t\t\t'weight':'Weight',\n\t\t\t\t'"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif(sys.version_info < (3,0)):\n\t\tflow_params = flow_params.replace('register_' + register_number, \n\t\tregister_value[2:-1])\n\t\tflow_params = flow_params.replace(str(register_value[2:-1],encoding='utf8'), register_value)\n\telse :\n\t\tflow_params = flow_params.replace('register_' + register_number, register_value[2:-1].decode('utf8'))\n\t\t#print flow_params.encode('utf"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor k in flow_params.keys():\n\t\tif key.match(k):\n\t\t\tif register_number not in flow_params[k]:\n\t\t\t\tflow_params[k].update({register_number: register_value})\n\t\t\telse:\n\t\t\t\tflow_params[k][register_number].replace(register_value)"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params = flow_params.copy()\n\t\n\tfor key, value in flow_params.items():\n\t\tif value == register_value:\n\t\t\tflow_params[register_number] = register_value\n\t\t\treturn True\n\t\t\n\treturn False\n\t\ndef _replace_all(flow_params, key, replace_value):\n\t\"\"\"\n\tReplace all given values with the given regular value\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param key: Key to replace\n\t:param replace_value: Value to replace\n\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t# If the flow does not contain a 'key' attribute\n\tif not hasattr(flow_params[\"flow\"], register_number) or not any([\"key\" in attr for attr in flow_params[\"flow\"].keys()]):\n\t\treturn register_value\n\n\tflow = flow_params[\"flow\"]\n\t\n\t# If the flow does not contain a value stored in a key attribute\n\tvalue = flow[\"keys\"][register_number]\n\t\n\treturn str(register_value).replace(value, str(flow[\"values\"][register_value"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tnew_flows = flow_params.copy()\n\tnew_flows[register_number] = register_value\n\treturn new_flows\n\"\"\"\n\tregister_key_value = next((register_key for register_key in registers if register_key == register_number), 0)\n\tregister_value = flow_params.get(register_key_value)\n\tnew_flow = _replace_register(flow_params, register_number, register_value)\n\tflow_params = new_flow\n\treturn flow_params\n\"\"\"\n\tRegister object defined on global."}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow in flow_params.values():\n\t\tfor key, value in flow.items():\n\t\t\tflow[key] = register_value\n\t\t\t\t\t       \n\treturn flow_params\n\t\ndef _remove(flow_params, register_number):\n\t\"\"\"\n\tRemove value from flows by given register number\n\t\"\"\"\n\tfor flow in flow_params.values():\n\t\tflow.pop(register_number)\n\treturn flow_params\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params[\"register_value\"] = register_value\n\t# replace all flow value'register_value' with flow number\n\tfor key, value in flow_params.iteritems():\n\t\tflow_params[key] = register_number\n\t\t\ndef _read_outputs(output_file):\n\tlines = []\n\tfor line in open(output_file):\n\t\t# Skip comments\n\t\tif line.strip() == '': continue\n\t\t# Skip empty lines\n\t\tif line.strip() == '\\n': break\n\t\t# Skip empty lines\n\t\tif line"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif len(register_value) == 2:\n\t\treturn flow_params.setdefault(register_value[0], register_value[1])\n\telif len(register_value) == 1:\n\t\treturn register_value[0]\n\telse:\n\t\traise FlowParamsError(\"flow_params doesn't contain correct configuration\")\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tnewval = re.sub(r'(\\.?)\\1*', arg, value)\n\tnewval = newval.replace(\".\",\"*.\")\n\treturn newval\n\n    \ndef check_string(value, arg):\n\t\"\"\"\n\tChecks for any chars that would make it a string\n\tbut if not there would have been an error\n\t\t\n\t@param value\t(String) this is the value to be checked\n\t@param arg\t(String) this is the argument to be checked\n\t\"\"\"\n\tif (re.match('[\\w\\s]"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tfor character in value:\n\t\tif character.isdigit():\n\t\t\tvalue = value.replace(character, arg)\n\t\telif character!= '.':\n\t\t\tvalue = value.replace(character, arg)\n\treturn value\n\ndef un_replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.dot' to arg from the given one\n\t\"\"\"\n\tfor character in value:\n\t\tif character == '.dot':\n\t\t\tvalue = value.replace(argument.dot, arg)\n\t\telif character!= '.dot':\n\t\t\tvalue"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\ts = value.split('.')\n\tif len(s) > 1:\n\t\tnew_arg = s[0]\n\t\tfor i in range(1, len(s)-1):\n\t\t\tnew_arg = new_arg + s[i]\n\t\treturn \"'\" + new_arg + \"'\"\n\t\treturn \"'\" + s[0] + \"'\"\n\t\treturn \"'\" + new_arg + \"'\"\n\t\t\t\t\ndef get_int_value(val):\n\tval = str(val)\t\n\ttry:\n\t\tif val[0] == '"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue = value.replace('.', arg)\n\tvalue = value.replace('/', '__')\n\tvalue = value.replace('\\\\', '$')\n\treturn value\n\nclass ContextItem(object):\n\tdef __init__(self, key, value=None):\n\t\tself.key = key\n\t\tself.value = value\n\n\tdef __str__(self):\n\t\treturn str(self.value)\n\n\tdef __repr__(self):\n\t\treturn str(self)\n\nclass Context(dict):\n\tdef __init__(self,"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\ttry:\n\t\tpairs = re.findall(\"\\.\"+arg, value)\n\t\treturn \"\" if pairs is None else \".\".join(pairs)\n\texcept:\n\t\treturn value\n\n@reify\ndef strip_dashes(value):\n\t\"\"\"\n\tA no-op function to make it always lower case\n\t\"\"\"\n\treturn value\n\n@reify\ndef replace_dash(value, arg):\n\t\"\"\"\n\tA no-op function to make it always lowercase\n\t\"\"\"\n\treturn value\n\n@reify\ndef replace_"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif isinstance(value, basestring):\n\t\tif '/' in value:\n\t\t\tval_str = ''\n\t\t\tfor p in value.split('/'):\n\t\t\t\tif p[-1] == \".\":\n\t\t\t\t\tval_str = p[:-1]\n\t\t\t\telse:\n\t\t\t\t\tval_str = val_str + p\n\t\t\tif val_str:\n\t\t\t\tif val_str not in arg:\n\t\t\t\t\treturn val_str\n\t\t\t\telse:\n\t\t\t\t\treturn arg.replace(val_str, value)\n\t\telse:\n\t\t\tval_str = value\n\n\t"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\targ = arg.split('.')\n\tretval = ''\n\tfor x in arg:\n\t\tif x == '.':\n\t\t\tretval += value\n\t\telse:\n\t\t\tretval += x\n\tretval = retval.strip('.')\n\treturn retval\n\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn ''.join(value.replace(r'.', arg)\n\t                    for value in str(value).split('.'))\n\ndef doit(newval, oldval):\n\t\"\"\"\n\tConverts a string with '.' to new string\n\tin the format (newval,oldval)\n\t\"\"\"\n\tresult = ''\n\tmatch = re.search(r'(?:%d-%d)\\d*([.]+)?' %\n\t                  (int(oldval), int(newval)), newval)\n\tcount = 0\n\twhile match:"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tfor token in value.split(\".\"):\n\t\ttensor_values = value.replace_dots(token,arg)\t\t\n\treturn tensor_values\n\t\ndef tensor2arr(self):\n\t\"\"\"\n\tConverts all elements of a tvm.tensor.Any -> numpy.array() object back to a numpy.ndarray() object\n\t\n\te.g to convert a numpy.ndarray to a tvm.tensor.tensor\n\tTODO for np.asarray use only for scalar, array\n\t\"\"\"\n\t"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif value == '.':\n\t\treturn arg\n\telse:\n\t\treturn value\n\t\ndef set_datalog_name(param, arg):\n\tif arg == 'all':\n\t\tparam['inName'] = '<All>'\n\telif arg.find(os.path.sep) == -1:\n\t\tparam['inName'] = '<{}>'.format(arg)\n\n\ndef set_datalog_dir(param, arg):\n\tif arg == \"all\":\n\t\tparam['dirName'] = \"<{}>\"."}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = collections.defaultdict(list)\n\tfor cls in unsupervised_classes.get(cls.__name__, []):\n\t\tsubs[cls].append(cls)\n\treturn {subcls: subs[cls] for subcls in subs}\n\ndef is_supervised(supercls):\n\t\"\"\"\n\tCompute True if the subclass is supervised;\n\tReturns False otherwise\n\t\"\"\"\n\treturn supercls in subclasses\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tparent = cls\n\twhile parent is not None:\n\t\tfor cls in reversed(parent.__subclasses__(cls)):\n\t\t\tyield cls\n\t\tparent = parent.__bases__[0]\n\n\tdef _recurse_children(parent):\n\t\tyield parent\n\t\tfor child in parent.__subclasses__(cls):\n\t\t\t_recurse_children(child)\n\t\n\tfor cls in _recurse_children(cls):\n\t\tparent = cls\n\tfor parent in _recurse_children(parent):\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tdef _subclass_lookup(cls):\n\t\t\"\"\"\n\t\tReturn all classes that have a __bases__ field set\n\t\t\"\"\"\n\t\tparents = (cls.__bases__, (cls.__module__, cls.__name__))\n\t\tresult = []\n\t\tfor parent in parents:\n\t\t\tparents = list(parent)\n\t\t\tparents.insert(0, cls)\n\t\t\tfor item in parent.__bases__:\n\t\t\t\tif item not in parents:\n\t\t\t\t\tresult.append(item)\n\t\treturn result\n\n\tif cls."}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tif not issubclass(cls, dict):\n\t\tret = cls.__dict__.get(cls.__name__, (cls,))\n\telse:\n\t\tret = (cls, )\n\tfor subsub in ret:\n\t\tif not issubclass(subsub, dict) and issubclass(subsub, object):\n\t\t\tsubclasses(subsub)\n\treturn ret\n\n# =============================================================================\n#\n#  Functions that have been added\n#\ndef prettyName(varname=None, output=True):\n\t\"\"\""}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tif isinstance(cls, (types.ModuleType, types.BuiltinFunctionType)):\n\t\treturn cls.__subclasses__()\n\telse:\n\t\treturn [ cls ]\n\n\n\ndef _determine_class(klass, bases):\n\tparent = bases[-1]\n\n\tif not isinstance(klass, (types.ClassType, types.BuiltinFunctionType)):\n\t\tif not inspect.isclass(klass):\n\t\t\traise TypeError(\"{!r} is not a class object\".format(klass))\n\n\t\tif"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tc = cls\t\t# keep this local\n\tfor c in cls.__subclasses__():\n\t\tsubclasses(c)\n\nfor cls is not _builtin_type:\n\tif isinstance(cls, types.TypeType) and issubclass(cls, _builtin_type):\n\t\tsubclasses(cls)\n\telif isinstance(cls, _builtin_type):\n\t\tsubclasses(cls)\n\n\telif cls is _builtin_type:\n\t\tsubclasses(cls)\n\ndef get_"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\n\"\"\"Calling this method with a class as the first argument is deprecated. \"\"\"\n).replace(\"%s.%s.subclasses\", \"%s.subclasses\")\n\n# return all classes that have no '__subclasses__' method defined\nfor cls in cls.__subclasses__():\n\tif not getattr(cls, '___subclasses__', None):\n\t\treturn getattr(cls, '__subclasses__')()\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses(subclass)\n\n\treturn\n\n\nclass Field(object):\n\t\"\"\" The fields that will be used by the code generator's model\n\t\"\"\"\n\tname = 'Field'\n\n\nclass Meta(dict):\n\t\"\"\" Structured data describing the document, such as title \"\"\"\n\tcontent = 'Content'\n\n\t\n\tdef __init__(self, title='', content='', **kwargs):\n\t\t\"\"\"\n\t\t@param content: Content of the document. Allowed elements:\n\t\t- <code>"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor klass in cls.__subclasses__():\n\t\tif issubclass(klass, cls):\n\t\t\tyield klass\n\tif hasattr(cls, \"_supertypes\"):\n\t\tfor klass in cls._supertypes(includeSelf=True):\n\t\t\tif issubclass(klass, cls):\n\t\t\t\tyield klass\n\ndef allModules(path):\n\t\"\"\"\n\t\treturn the modules in our source path at most\n\t\t\"\"\"\n\tmodules = []\n\tprefix = os.path.basename(path)\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tresults = []\n\tfor cls in cls.__subclasses__():\n\t\tresults.extend(subclasses(cls))\n\treturn results\n\nclass subclasses(object):\n\t\"\"\"a class of classes that have no parents\"\"\"\n\t__attrs__ = []\n\tdef __init__(cls):\n\t\tcls.__attrs__.append(cls)\n\nclass Class(object): \n    \"\"\"\n\ta single abstract class in the codebase\n    \"\"\"\n    \n    def __init__(self, name, methods=0,"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tstr_length = len(alphabet)\n\n\t# Handle a single digit number\n\tif string_length == str_length:\n\t\treturn int(string)\n\t\t\n\tfirst_pos = string.find('0')\n\tif first_pos == -1:\n\t\treturn None\n\t\t\n\t# Find the last digit\n\tlast_pos = string.find('0', str_length-1-first_pos)\n\tif last_pos == -1:\n\t\treturn None\n\n\t# Compute the number\n\tvalue = 1\n\n\t# Find the next digit after"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tdigits = str(alphabet[string[0]])\n\treturn int(digits) - 1\n\n\ndef parse_int(string: str) -> int:\n\t\"\"\"\n\t\tReads a decimal int from the given string and returns it as an int number.\n\t\"\"\"\n\treturn int(string)\n\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\ttry:\n\t\t\n\t\tint_val = int(string, 10)\n\t\tfor char in alphabet:\n\t\t\t\n\t\t\tint_val = int(int_val + int('0' + alphabet[0] * char, 10))\n\t\t\t\n\texcept ValueError:\n\t\tif alphabet == ['0']:\n\t\t\treturn _INT_MAX\n\t\telse:\n\t\t\traise\n\t\n\treturn int_val\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tresult, next = 0, string\n\t# Get string size\n\tlength = len(string) ###\n\tfor i in range(1, length, 1):\n\t\tval = int(string[i:i+1]) ###\n\t\tif val >= 1:\n\t\t\tbreak ###\n\t\telse:\n\t\t\tnext += 1\n\t\n\t# Use smallest value of first string\n\tresult = val\n\tfor val in alphabet:\n\t\tif val >= next:\n\t\t\tresult -= val\n\t\n\treturn result\n\t\n\t\t\n### Input\n\ndef main():\n\t\n\tfor"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn 0 if string == \"N/A\" else int(alphabet[int(string.split(\" \")[-1])])\n\t\t\ndef array_to_int(array: List[int]) -> None:\n\t\"\"\"\n\t\tConvert a list of integers to an output for the purposes of\n\t\tgenerating the output of the string_to_list function.\n\t\"\"\"\n\tstring = \" \".join(i.string for i in array)\n\treturn int(string, 0) if str(string).isdigit() else None\n\ndef int_to_string("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tnumber = 0\n\talphabet_string = [c for c in alphabet]\n\ttry:\n\t\tstring = string.lower()\n\t\tfor i in alphabet_string:\n\t\t\tif string.startswith(str(i)):\n\t\t\t    number += int(string[alphabet_string.index(i):])\n\texcept ValueError:\n\t\treturn -1\t# The string does not start with an integer value\n\telse:\n\t\treturn number\n\n\nclass CounterFunction:\n\t\"\"\"\n\tA function responsible for keeping track of one-time usage of a counter\n\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\tcount = 0\n\tfor letter in string:\n\t\tif letter!= '\\x00':\n\t\t\tcount += 1 << (alphabet.index(letter) - 1) # bitshift by 1\n\t\n\treturn count\n\t\n\t\n\t\ndef char_to_alphabet(string: str) -> str:\n\treturn string.upper()\n\t\n\t\ndef int_to_alphabet(number: str):\n\tif number[0] == '0':\n\t\tif number[1] == '0':\n\t\t\treturn ''\n\t\treturn 'A'"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(string_to_bitslist_format(string, alphabet)[0])\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tdigit_count = len(string)\n\tresult = 0\n\tif len(string) == 1:\n\t\tresult = int(string)\n\telse:\n\t\t# get the first occurrence of a non-digit character\n\t\tfirst_non_digit = 0\n\t\tfor char in string:\n\t\t\tif char.isdigit():\n\t\t\t\tfirst_non_digit += int(char)\n\t\tif first_non_digit < digit_count:\n\t\t\t# no other occurrences\n\t\t\t# of the first non-digit in string do not count\n\t\t\t# as a digit value so we have"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tnumber = 0\n\tfor letter in string:\n\t\tnumber *= alphabet.index(letter)\n\treturn number\n\ndef read_int(s: str) -> int:\n\tif is_int(s):\n\t\treturn int(s)\n\telse:\n\t\treturn str2int(s, alphabet)\n\ndef make_alphabet(numberset: Sequence) -> List[str]:\n\treturn str(numberset[0]) + str(numberset[1])\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\turl = 'https://github.com/Core-Tools/core/archive/src/desc.tar.gz'\n\tprint(\"===> Downloading \", url)\n\t\n\tpath_to_download = path.join(__dirname, url)\n\tdownloaded_path = path.join(path.dirname(path.abspath(__file__)), url)\n\t\n\t# Get.tgz file for each repo\n\tget_list_from_url(url, downloaded_path)\t\n\t\n\t# Delete.tar.gz"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tarchive_dir = os.path.split(destination_path)[0]\t# root directory\n\tif not os.path.isdir(archive_dir):\n\t\tos.makedirs(archive_dir)\n\tarchivable_path = os.path.join(archive_dir, 'archivable')\n\tarchivable = open(archivable_path, 'w')\n\n\twith tarfile.open(name=[url]) as archive#, TarFile.open(mode='r', fileobj=archive#, **taroptions) as archive2#:\n\t\tfor tar"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport tarfile\n\timport os\n\timport subprocess as sub\n\t\n\tpath = os.path.join(\n\t\tos.path.dirname(os.path.abspath(__file__)), \n\t\t''  # this makes the directory empty\n\t\t)\n\t\n\t# list of packages that will be extracted to the directory\n\tpackages = ['core', 'extra', 'community']\n\t\n\t# determine the file names that will be extracted to the location of the archive file\n\text = r'\\.(.+?\\.tar)-'\n\t\n\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\timport pydoc\n\tfrom tarfile import TarFile\n\n\ttar = TarFile.open(path=destination_path)\n\tprint(tar.getnames())\n\tif '.desc' in tar.getnames():\n\t\tprint('Found packages. Downloading packages.tar.gz and extracting them to', path, sep=\"\")\n\t\ttar = tar.extractall()\n\t\ttar.close()\n\t\n\ttar = TarFile.open(path=destination_path)\n\ttar.extract('core', dstpath=destination_path)\n\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tfrom shutil import copyfile\n\tfrom glob import glob\n\t\n\tdownload_url = url.rsplit(\"/\", 1)[1]\n\tdownload_path = os.path.dirname(os.path.realpath(download_url))\n\t\n\ttarget_dir = os.path.join(download_path, \"tar-zip\")\n\toutput_dir = os.path.join(download_path, \"archives\")\n\ttar_out = os.path.join(target_dir, download_url)\n\t\n\tif target_dir == export"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\twith open(destination_path + file_extension, 'r+b') as extracted_path:\n\t\t\n\t\twhile True:\n\t\t\ttime.sleep(10)\n\t\t\t\n\t\t\ttry:\n\t\t\t\tresp = requests.get('https://hg.archive.org/user/core/'+url, auth=('core', 'y'), headers={'X-Requested-With':'XMLHttpRequest'})\n\t\t\t\t\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Could not download repo '{}' from repo server {} at url: \", url, e"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\tprint(\"Retrieving tar.gz archive from url: {}\".format(url))\n\n\tcurl_args = \"-k -F {}\".format(destination_path)\n\n\tstatus = requests.get(url, url=url, params=curl_args)\n\t\n\tif status.status_code == requests.codes.ok:\n\t\tresp = status.json()\n\n\t\tif resp['packages'] > 0:\n\t\t\tfor i in range(0, resp['packages']):\n\t\t\t\ttmp_path = Path()\n\t\t\t\tp = Path(destination_path +"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tif os.path.exists(destination_path) == False: os.makedirs(destination_path)\n\tprint('Downloading archive to path')\n\tres = requests.get(url, stream=True)\n\twith open(destination_path, 'wb') as dest:\n\t\tfor data in res.iter_content(80):\n\t\t\tdest.write(data)\n\treturn destination_path\n\n\"\"\"\n\tGiven an url and a destination path, extract.tar.gz archive\n\twhich contains 'desc' file for each package and corresponding directory."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\tif not os.path.isdir(\"/tmp/extraction\"):\n\t\tos.mkdir(\"/tmp/extraction\")\n\t\t\n\t# Download and extract archive\n\turllib.request.urlretrieve(url, \"/tmp/extraction/%s\" % url)\n\t\n\t# Save tar archive into folder 'extraction'\n\t\n\treturn \"/tmp/extraction/%s\" % url\n\"\"\"\n\t\ndef main(url, destination_path):\n\t\"\"\"\n\tDownload and extract archive\n\t\n\tArgs:\n\t    "}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tdef extractDirPath(tar, dirPath):\n\t\tfor filename in os.listdir(tar.name):\n\t\t\t\n\t\t\tif   filename.endswith(\".tar\") and filename.find(\"desc\") >= 0:\n\t\t\t\tfilename = filename.replace(\".tar\",\".tar.gz\")\n\n\t\t\t\tfor subdir in os.listdir(os.path.join(dirPath,tar.name)):\n\t\t\t\t\tsubdirDotTar = subdir.replace(\".tar\",\".tar.gz\")\n\n\t\t\t\t\tif  (subdirDotTar == filename) and (os."}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tenv_cwd = os.environ['CWD']\n\tpath_to_c = os.path.realpath(os.path.join(env_cwd, 'bin'))\n\tos.chdir(path_to_c)\n\tret_cwd = True\n\tret_path = os.environ['XDG_CURRENT_DIR']\n\tif os.environ['HOME'] == os.environ['USERPROFILE'] + '/Documents/':\n\t\tret_path = ''\n\t\n\ttry:\n\t\tret_cwd"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\tif (sys.platform == \"darwin\"):\n\t\treturn True\n\telse:\n\t\treturn False\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\tif sys.platform == 'darwin':\n\t\treturn True\n\t\n\treturn False\n\nif __name__ == '__main__':\n    if os.path.isfile(sys.argv[1]):\n        print('Checking if you have the file:'+ sys.argv[1])\n    else:\n        print('You does not have the file:'+ sys.argv[1])\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\tif not( os.name == 'posix' ):\n\t\t\n\t\treturn False\n\t\n\tif sys.platform.startswith('linux'):\n\t\n\t\treturn False\n\t\n\tif 'darwin' in sys.platform:\n\t\t\n\t\treturn False\n\t\n\treturn True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n##########################################\n#  Check if sys is MacOS X\n##########################################\n\n\n\nif __name__ == '__main__':\n\t\n\tif not( os.name == 'posix' ):\n\t\t\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif sys.platform#\"linux\" # linux not supported in this script\n\t\t\t\tor sys.platform#\"darwin\" or sys.platform#\"win32\" \n\t\t\t\tor sys.platform#\"freebsd\":\"#freebsd not supported in this script\":return\n\t\t\t\t\n\t\t\t\t#check if script is in current folder\n#\tif not os.path.exists(\"/Library/Frameworks\"): # if framework is not in /Library/Frameworks\n\tif (\"Library/Frameworks\" not in os.environ and \"Frameworks\" not in os.environ) or \"Frameworks\""}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tfor root, dirs, files in os.walk(os.path.abspath(os.path.dirname(__file__))):\n\t\tfor file in files:\n\t\t\tif os.path.basename(root) in ('bin','etc','log','etc','lib','tests') and file.endswith(('.mcp','.cpp',)) and file not in glob.glob(os.path.join(root, glob.glob(os.path.join(root,'src', 'lib', '*')))):\n\t\t\t\tos.chdir("}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\tif sys.platform == \"darwin\":\n\t\treturn True\n\t\t\n\treturn False"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\timport platform\n\n\treturn platform.system() == \"Darwin\" or os.path.exists(\"./bin/./macc.app/Contents/Resources/macclib.dylib\")\n\n\ndef main():\n\t\"\"\"\n\tRun the script \n\t\"\"\"\n\t#Check if you have a Windows OS\n\tosx = check_os()\n\n\t#If it is not yet an OS, then do not start the script\n\tif osrx is False:\n\t\t#If the script is not executed, then return the script is not running \n\t\tif not os."}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t# Find the location of the current directory\n\tmydir = os.getcwd()\n\t\n\t# Run cmd.exe through as administrator\n\t#if not os.path.isdir(os.getcwd()):\n\t#\tprint 'Error!',os.getcwd()\n\t#\treturn False\n\t\n\t# Make sure the current user is an administrator\n\tif os.path.isfile(os.path.expanduser(os.getenv('USER')+'\\\\PasswordManager.plist')) == True:\n\t\t# Remove the password"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif get_platform() == 'darwin':\n        \tpass\n\telse:\n\t\ttry:\n\t\t\timport platform\n\t\texcept AttributeError:\n\t\t\tplatform, = platform.platform().split('-')\n\t\tif not platform.python_implementation().startswith(\"PyPy\"):\n\t\t\treturn False\n\treturn True\n\"\"\"\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tkeys = [filename.strip() for filename in re.split('[\\'][*]', item['a'])]\n\treturn set([re.replace(key, '\\\\\\\\?') for key in keys])\n\ndef regex_dict(rules):\n\t\"\"\"\n\tConvert *.h keys to regex keys\n\t\n\tA rule key that represents a single file matching *.h should be\n\tsubstituted with the full key such as so:\n\t\n\t\tfiles = {\n\t\t\t'*.h': '?c_p',\n\t\t}\n\t\tregex_keys ="}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\trules = {k:v for k,v in item.items() if re.match('*.{1}', k) and not re.match('[^\\w.]+', k)}\n\tregex_keys = {k:r for k,r in rule_to_regex[rules]}\n\tresult = {}\n\tfor k,v in rule_to_regex[rules].items():\n\t\tresult[re.sub(k, r, k)] = v\n\treturn {'$':regex_keys}\n\nif __name__ == '"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys = {}\n\tfor k in item.keys():\n\t\tif re.search(k, item[k]) is not None:\n\t\t\tmatch = re.findall(k + '$', k)\n\t\t\tregex_keys[match[0]] = match\n\treturn regex_keys\n\"\"\"\nimport re\nclass Config:\n    def __init__(self):\n        '''\n        initializer\n        '''\n        self.rule_dict = {}\n        self.rule_regexes = {}\n        self.parser = re.compile("}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {v['regex']: v['value'] for v in item.values()}\n\titem = {k: v for k, v in item.iteritems() if string_regex_match(k, regex_dict)}\n\treturn item\n\n\n##### DICTIONARY CONVERTION ################\n\tdef __str__(self, **kwargs):\n\t    return \"({0})\".format(list(self.dict.iteritems(**kwargs)))\n\t\n\tdef __reversed__(self, **kwargs"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tfor key in item:\n\t\t\n\t\tmime = key.lower().replace('.', '')\n\t\tregex_key = re.compile(\"[a-z0-9]([^a-z0-9]|[.0-9]+)\")\n\t\t\n\t\tif key.lower().match(mime):\n\t\t\titem[key] = regex_key.sub(r'\\1', item[key])\n\t\telse:\n\t\t\tcontinue\n\t\t\n\treturn item\n\t\n\nimport os\n\ndef regex_dict(regex_dict):\n"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\tregex_keys = set()\n\t\n\tfor key, item in item.items():\n\t\tregex_keys.add(\" \".join(\"^'{0}'\".format(key)))\n\t\t\n\treturn dict((key, key) for key in regex_keys)\n\t\n\"\"\"\n\tExtract all possible keys from a string\n\t\n\tUses regex if available\n\t\n\t\n\tArgs:\n\t   string: The string to extract\n\t   dict: The dict that will be extracted\n\t\n\tReturns:\n\t   (None, None)"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tnew_rule = dict()\n\tfor k in item:\n\t\tk = k.replace(r'[ \\t\\n\\r]', '_')\n\t\tif k in rule_names:\n\t\t\tcur_rule = rule_names[k]\n\t\t\tif isinstance(cur_rule, list) and k in cur_rule:\n\t\t\t\tcur_rule = cur_rule[k]\n\t\t\telse:\n\t\t\t\tcur_rule = re.compile('|'.join(cur_rule))\n\t\t\tnew_rule[k] = cur_rule\n\treturn new_rule"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_list = []\n\tfor item in item.keys():\n\t\tfor pattern in item[item.keys()[0]].keys():\n\t\t\tif pattern.startswith('*'):\n\t\t\t\tregex_list.append(re.compile(item[item.keys()[0]][pattern][item.keys()[0]]))\n\t\t\telse:\n\t\t\t\tregex_list.append(re.compile(item[item.keys()[0]][pattern]))\n\treturn dict([re.sub('\\{([a-zA-Z0"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\treturn {key: True for key in item if (re.search(r'\\w+$', key) if (re.search(r'\\b' + re.escape(key).replace('\\\\', '|'), key) else False) and (item[key] == '**' or \n\t\t\t\titem[key] == '!' or item[key] == '*' and len(item[key].split(' ')) > 1)) and\n\t\t   (re.search(r'(?<!\\.)(\\w+)(?!\\d)', key)"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\treturn {k: re.compile(v) for k,v in item.items()}\n\ndef regex_dict(rules):\n\tret = {}\n\tfor r,l in rules.iteritems():\n\t\tif l.endswith(\".cpp\"):\n\t\t\trules = (r, l)[:0]\n\treturn rules\n\ndef key2vars(rules):\n\tp = {}\n\tfor r,l in rules.iteritems():\n\t\tp[r] = l\n\treturn \" \" + '\".split(\".\")[0] + \"\"."}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = re.sub(r'(.)+', r'\\1', name).strip()\n\treturn name\n\n\ndef filter_classe_name(name):\n\t\"\"\"\n\tFilter the classe name.\n\t\"\"\"\n\n\tname = re.sub(r'(.*[^a-zA-Z0-9])', r'\\1', name)\n\tname = re.sub(r'([^a-zA-Z0-9])', r'\\1', name).strip()\n\n\treturn name\n\n\ndef remove_tokens("}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tprint('Removing quotes around a name:'+ name + '.')\n\tpos = name.rfind('\\\"')\n\tif pos >= 0:\n\t\tname = name[pos + 1 : len(name)]\n\treturn name\n\n\n\ndef find_url(url):\n\t\"\"\"find the index, in line, of the url\"\"\"\n\t# this matches the regexp in rss.\n\t# so if url contains a slash, we need to keep the regexp \n\t# in the same place.\n\tregexp = r\"^(http://[^"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tif to_remove!= '':\n\t\tif to_remove[:1] == to_remove[0:1].upper():\n\t\t\tdel to_remove[0:1]\n\t\tif to_remove[:1] == to_remove[-1 ].upper() :\n\t\t\tdel to_remove[-1 ]\n\n\tto_remove = re.sub(r'\"', r' \",', to_remove)\n\n\t# remove unnecessary brackets\n\tto_remove = re.sub(r'[\\]{}][\\[ \\] ]{1,3}', '',"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\treturn removequotes(name, separator)\n\ndef removequotes(name, separator):\n\tname = removequotes2(name, separator)\n\tname = str(name).strip()\n\n\treturn name\t\n\t\ndef removequotes2(name, string):\n\treobj = re.compile(\"(?<=\\\".\\\")(\" + string + \"|``)\"  + \"?\", re.UNICODE)\n\treturn reobj.sub(\"\", name)\n\t\ndef removeempty(name):\n\tremove = \"\"\n\n\t#"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\treturn \"\\\" \".join(quoted.split()).strip()\n\n\n\ndef quote(*args):\n\t\"\"\"\tQuote a string with the following arguments.\n\n\t\t\"a\" = ascii string character.\n\t\t\"A\" = ISO 10646 numeric character.\n\t\t\"s\" = unicode string.\n\t\t\"S\" = Numeric string.\n\t\t\"u\" = hexadecimal string.\n\t\t\"U\" = Numeric string.\n\t\t\"h\" = hexadecimal string with leading 0x prefix.  In this case, it is the only\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\ttext = name\n\tif re.search(r\"\\\"$\", text) and re.search(r\"\\${(.*?)}\\$\", name):\n\t\ttext = text.replace(\"\\\"${(.*?)}$\", \"\")\n\t\ttext = text.replace(r\"\\${(.*)}$\", \"\")\n\tprint(\"remove quote from\", text)\n\n# remove the quote from the last name (e.g. F.S.S.2.M.3.G.3.B.M.F.A)\n\treturn name.replace(\"\\\""}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef _do(self, r):\n\t\treturn r.startswith('\"') and r.endswith('\"'))\n\treturn re.compile(r'\"([^_\"]+|_[^_\"]+|\")(\\s?(?!\\b_)(.*)', re.DOTALL).sub(_do, text)\n\nAUTHOTIC = 'Authenticate'\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tret = re.sub(r'\"(?<!\\\\)[\"']|\\'[^\\']*\\')*\"', \"\", s)\n\ts = ret\n\t\n\treturn''.join(x if '.' not in x else '_' + x.split()[-1] for x in s.split())\n\n\ndef parseNodes(text):\n\n\t\n\tparsedText = [parseNode(x) for x in text.split('<node>')]\n\t\t\n\treturn parsedText\n\n\ndef parseNode(node):\n\n\t\n\t"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tre_ = re.compile(\"([^()])'\", re.UNICODE | re.IGNORECASE)\n\n\tdef strip_quotes(m.group(1)):\n\t\tr = []\n\t\tr.append(re_.sub(\"'\", \"\", m.group(1)))\n\t\treturn \"\".join(r)\n\n\tclass Word:\n\t\tdef __init__(self, text):\n\t\t\tself.text = text\n\t\t\tself.regex = re.compile('(\"|\\')([^\"')\n\n\twith open('data/word"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef _filter_quote(str):\n\t\tif re.match('\\s|&', str):\n\t\t\treturn str\n\t\telse:\n\t\t\treturn str.replace('\"', '')\n\t\n\tdef filter_quote(str):\n\t\tstr = re.sub('&'+_filter_quote(str), '', str)\n\t\tstr = re.sub('(['+str+'])', _filter_quote(str), str)\n\t\tstr = re.sub('[^\\\\]', '', str)\n\t\t\n\t\treturn str\n\t\n\tstr ="}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\tif platform is not None:\n\t\tnew_shlex = functools.partial(shlex.split, platform=platform)\n\telse:\n\t\tnew_shlex = shlex.split\n\t\t\n\treturn new_shlex\n\n# vim: expandtab tabstop=4 shiftwidth=4\n"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef __init__(self, platform):\n\t\tself.platform = platform # 'this' vs 'other' vs 'other' vs null\n\t\t\n\tdef __call__(self, *args):\n\t\tif self.platform is not None:\n\t\t\t# first see if we were specified on the command line\n\t\t\tmatch = _re_multi_platform.search(str(args))\n\t\t\tif not match:\n\t\t\t\targs = str(args)\n\t\t\t\t\n\t\t\tparts = args[match.start():match.end()].split()\n\t\t\tif len(parts) >= 2:\n"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef split_cmdline(self, s):\n\t\t\"\"\"Split the given argument into parts\n\t\t\n\t\t>>> split_cmdline('this --param1\") \"hello world\"')\n\t\t('this','--param1')\n\t\t>>> split_cmdline('\"this --param1\") \"hello world\"')\n\t\t('this','--param1')\n\t\t>>> split_cmdline('\"this --param1\"')\n\t\t('', '')\n\t\t>>> split_cmdline('\"this --param1\"')\n\t\t('', '')\n\t\t>>> split_cmdline('"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\tdef run(p, p0):\n\t\tr\"\"\" Run a command, return command and stdout.\"\"\"\n\t\treturn p.run(p0, verbose=True)\n\n# vim: set sw=4 sts=4 expandtab:\n"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tclass MultiSplit(object):\n\t\tdef __init__(self, string):\n\t\t\tself.string = string\n\t\t\tself.tokens = []\n\t\t\tself.pos = 0\n\t\t\t# Set empty token to end of string\n\t\t\tstring.append('')\n\t\tdef __getitem__(self, index):\n\t\t\tif type(index) == int:\n\t\t\t\t# Get just tokens.\n\t\t\t\tself.pos += index\n\t\t\t\treturn string.split(self.string, self.pos)[index]\n\t\t\telse:\n\t\t\t\t# Append token to already-filled token list."}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\tsplit = \"\".join(shlex.split(shlex_input))\n\tsplit = [i.strip() for i in split.split(\" \")] # remove single-space\n\tsplit = \"+\" + \"|\".join(map(lambda x: \"-\" + x,  \"-\".join(split[:5])))[1:] # remove - or -\n\tsplit = split[0:4] # strip first 4 args\n\t\n\tfor command in split: # strip all args, except first one (command) if any\n\t\tsplit.pop(0)"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef split(self, data = None, file = None):\n\t\tdata = data if data is not None else ''\n\t\treturn re.split(self.regex('(?::?[^}]*?)' if file is None or file.endswith('*'\n\t\t\t) else self.regex() if file is not None else '@', data)\n\tdef split(self, data = None, file = None, separator ='', ignorecase = True):\n\t\tdata = data if data is not None else ''\n\t\treturn re.split(self.re"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t# Python\n\ttry:\n\t\tsplit_argv = shlex.split\n\texcept AttributeError:\n\t\t# Python2\n\t\tsplit_argv = re.split\n\tif argv_safe:\n\t\treturn split_argv(argv)\n\telse:\n\t\treturn split_argv(args)\n"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tfrom os import environ\n\tfrom os import pardir\n\tdef split(command, separator=':'):\n\t\tsep_len = int(environ['PATH_EXT'][1:])\n\t\tif len(sep_len) > 3:\n\t\t\tbreak\n\t\tif sep_len == 0:\n\t\t\tsep_len = 1\n\t\tparts = command.split(separator, 1)\n\t\tif sep_len == 1:\n\t\t\treturn sep_len, parts\n\t\tif sep_len == 1:\n\t\t\treturn 0, parts\n\t\treturn sep_"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\timport re\n\tfrom string import upcase\n\n## \n# Python 3.3 uses a different implementation: It is implemented using a dictionary.\n# This implementation (and the corresponding shell) should be removed when Python3.3 moves to modern system.\n# See https://www.python.org/dev/peps/pep-0333/\n# TODO make it work like Python3\n__all__ = ['split','splitlines', 'tokenize', 'expandtabs','splitl', 'expand_tabs', \n\t'quote', 'quote"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\ttry:\n\t\twith open(\"archiveArchive.tar.gz\",\"rb\") as zippedArchive:\n\t\t\tarchive = tarfile.open(\"archiveArchive.tar.gz\")\n\t\t\tarchive.extractall(rootPath)\n\t\t\tos.remove(\"archiveArchive.tar.gz\")\n\texcept: pass\n\tunzipArchivePath = urllib.parse.urlunparse((\"file\", archive_path))\n\tos.system(\"unzip %s\"%unzipArchivePath)\n\t#print(\"unzip finished\")\n\t\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\ttry:\n\t\twith  open(archive_path, \"rb\") as theater_arc:\n\t\t\ttheater_arc.seek(0)\n\t\t\treturn ''.join(subprocess.check_output([config.PIP_BINARY, '--unzipper', '-f', '-d', '/', archive_path, a_tiger]).decode('utf-8'))\n\n\texcept subprocess.CalledProcessError as cpe:\n\n\t\tprint \"(%s -f %s %s -d /) %d\\n%s\" % (config"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\ttry:\n\t\tcmd = \"tar -xvvf {arch}\".format(arch = arc)\n\t\t\n\t\tsubprocess.check_call(cmd, stdout=subprocess.PIPE)\n\t\t\n\t\treturn \"https://github.com/lucasman/vx_archive/blob/master/{arch}\".format(arch=arc[-9:])\n\t\t\n\texcept Exception as e:\n\t\tprint \"Could not run `tar -xvf {arch}`: {e}\".format(arch=arc[-9:],"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\tarchive_urls = glob.glob(\"{}/{}/*/*.*\".format(archive_path))\n\n\twith open(archive_path, 'rb') as in_file:\n\t\tfiles = [in_file.name]\n\n\tsubprocess.call(\n\t\t\"cp {0}/* | xn --symlink -r'.{0}'\".format(' '.join(archive_urls)),  # type: ignore\n\t\tshell=True,\n\t\tcwd='/',\n\t\tenv={'LANG': 'C', 'GSUtil"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tdef get_url(archive_path):\n                if self.archive_path.find(\",\") > 0:\n                        return self.archive_path.split(\",\")[1] + \"/\" + archive_path\n                else:\n                        return self.archive_path + \"/\" + archive_path\n\t\n\tdef archive_path(self,archivename):\n                return os.path.join(\n                    subprocess.check_output(\n                        ['find', archivename, os.path.split(archivename)[1]], stdout=subprocess.P"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t# try to load the archive file with our utility functions\n\ttry:\n\t\tarchDir = File.getAbsolutePath(archivePath)\n\texcept:\n\t\treturn False\n\t\n\ttry:\n\t\tarchFile = Open.open(archDir, \"rb\")\n\t\t\n\t\t# the archivePath will be relative to the current directory\n\t\t# if this is a relative path of an existing file, use the original\n\t\t# in order to prevent overwriting of the archive, as this can lead\n\t\t# to problems when running the uncompress command and then modifying\n\t\t# it before"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tarchive_path = os.path.realpath(archive_path)\n\t\n\tnew_path, new_relpath = os.path.split(archive_path)\n\tnew_relpath, new_extension = os.path.splitext(new_relpath)\n\t\n\trpath = new_path[len(new_relpath) :]\n\tfile_content = read_file(archive_path, extension=new_extension)\n\t\n\treturn rpath + file_content\n\ndef read_file(file_path, extension="}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tsubprocess.check_output([\"uncompress\", archive_path])\n\tfile_repository_url = \"file_repository://{0}:{1}\".format(name, port)\n\treturn file_repository_url\n\ndef uncompress(archive_path):\n\t\"\"\"\n\tThe uncompressing path into a repo server url which can be used as origin url\n\t\"\"\"\n\t# Create temp dir for archive and save the filename.\n\ttmp_dir = tempfile.mkdtemp(prefix=\"repo_\" + random_string_8"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tarchive_path = os.path.expanduser(\"~/.m3x/\")\n\tbase_url = \"m3x/\" + archive_path\n\t\n\tfor filename in glob.glob(base_url):\n\t\tif filename.endswith('.tar.gz') or filename.endswith('.tar.bz2') or filename.endswith('.tar.xz'):\n\t\t\tfilename = os.path.join(filename.rsplit(\"/\",1)[0], filename.rsplit(\".\",1)[1])\t\n\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tfilerepopath = subprocess.check_output([ \"tar\", zipfile, \"-c\", str(filerepopath), archivepath ], stderr=subprocess.STDOUT)\n\t\n\t#uncompress the archived copy.\n\tcompressedfilerepolocation = \"%s/%s-%s\" %(zipfile, archivepath, archiveversion)\n\tsubprocess.call([\"tar\", \"-un\", \"-f\", subprocess.check_output([\"unzip\", \"-yc\", compressedfilerepolocation, \"/tmp\"], stderr=subprocess.STDOUT)"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t#subprocess.Popen(['git','rev-parse', '--match', '.'], stdout = subprocess.PIPE, cwd = os.path.dirname(os.path.abspath(__file__)))\n\ndef get_revisions():\n\t\"\"\"\n\tReturn the git log output of the commit hash, return a list of the revisions.\n\t\t\n\t\"\"\"\n\trev = subprocess.Popen(['git', 'log'], stdout = subprocess.PIPE, cwd = os.path.dirname(os."}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t#\n\t# create the git command\n\t#\n\tgit_cmd = \"git --git-dir=\"+os.environ[\"HOME\"]+\"/.git/gitorious\"\n\tproc = subprocess.Popen(git_cmd,shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tout, err = proc.communicate()\n\tlines = out.splitlines()\n\tfor line in lines:\n\t\tfull_name = line\n\n\t#\n\t# sort the lines\n\t#\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = \"git\"\n\tname = 'name'\n\tignore = 'ignore'\n\tcmd_stdout, cmd_stderr = run_cmd(cmd, name, ignore)\n\tfiles = []\n\twhile 1:\n\t\tif get_next_cmd_stdout(cmd_stdout):\n\t\t\tbreak\n\t\tfiles = files + split_by_comma(get_next_cmd_stderr(cmd_stderr))\nif not ignore is None:\n\treturn files\n\n\t\n\ndef split_by_comma(cmd_stder"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tIGNORE = \"\"\n\tIGNORE_REAL = \"\"\n\tINCLUDES = \"\"\n\tINCLUDES_REAL = \"\"\n\tDELETE = \"\"\n\tDELETE_MODULE = \"\"\n\tDELETE_MODULEFROMREFERENCE = \"\"\n\tDELETE_REFERENCE = \"\"\n\tDELETEFROMMODULE = \"\"\n\ndef check_for_git():\n\tif os.path.exists(gitdir):\n\t\tprint \"Git: Existing, skipping\"\n\t\treturn\n\n\tif not os.path"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport sys\n\timport glob\n\n\tignore = []\n\tignores = []\n\tignored_list = []\n\tfilename = \"__main__.py\"\n\n\tignore_args = sys.argv[1:]\n\t#ignore_args = ignore_args[1:]\n\tfor ignore_arg in ignore_args:\n\t\tmatch = re.match('\\*\\*\\*\\*.*', ignore_arg)\n\t\tif match:\n\t\t\tfor ignored_item in glob.glob(\"*.ignore\"):\n\t\t\t\tignored_list.append(ignored"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfile_list = os.popen(\"git diff --name-only --shortstat\").read().rstrip().split(\",\")\n\tfile_list = sorted(set(file_list))\n\tfile_list = list(set(file_list))\n\treturn \",\".join(file_list)\n\n\"\"\"\ndef is_file(line):\n\tif \"^([a-z0-9][^ \\n]*)$\" in line:\n\t\treturn True\n\treturn False\n\"\"\"\ndef get_unignored_files(line,ignored_files"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfiles=[i for i in os.listdir(\"./\") if i[-3:].endswith(\".py\") and not i.startswith(\"_\")]\n\tignore_files=[i for i in files if i.endswith(\".py\")]\n\tignore_files.sort()\n\treturn str(ignore_files)\n\n\"\"\"\n\tUse git to update all the files (i.e. \"git update\" should be the last command) with any modifications, and then test each file against\n\tthe last version.\n\t(If there is a"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfor file in glob_list:\n\t\tif any([not os.path.isfile(file), not files_ignored.search(file)]):\n\t\t\tif os.path.isdir(file):\n\t\t\t\tcommand = \"find \" + file\n\t\t\telse:\n\t\t\t\tcommand = \"find \" + os.path.basename(file)\n\t\t\tfiles_ignored = file_utils.subprocess_check_output(command)\n\tfiles = file_utils.subprocess_check_output(['git', 'log', '-1', '-m1']).split("}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport shlex\n\n\tdef get(text):\n\t\tfor dn in shlex.split(text):\n\t\t\tprint \"    --> %s\" % dn\n\n\tcommand = \"git \"\n\tcommand += \"ls-files | awk '{print $3}'\"\n\tcommand += \" | sort -rn '$1' | egrep 'grep[^\\\\'|\\\\s|\\\\n]' | tr -d '\\\\s'\"\n\n\tcommand += \" | egrep 'grep[^\\\\'|\\\\s|\\\\n]' | sort -rn '$1"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport os\n\timport sys\n\timport gi\n\tfrom git import Repo\n\tcmd = \"git log --pretty=format:%s\"\n\tcommands = [\"git\", \"--date\", \"--pretty=format:%s@%b %c %H:%M:%S%z\",  #date, date_path\n\t            cmd, \"--pretty=format:%s@%b %c %d\",            #date_path@2014-11-05\n\t            cmd, \"--pretty=format:%s@%s.txt\",            "}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfrom types import TypeType  \n\ttry: \n\t\twith open(filename, \"r\") as f: \n\t\t\treturn TypeType(f.read()) \n\texcept(OSError) as e: \n\t\treturn False\n\n\"\"\"\n\tIf a given filename is on top of this list, \n\tthe name returned by this function does as well\n\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    filename with '__' stripped out\n\t\"\"\"  \n\tfilename = os.path.splitext(filename)[0].replace(\""}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif not file_exists(filename):\n\t\treturn False\n\t\n\tfor extension in supported_extensions:\n\t\tif not extension in [ \".py\", \".pyw\", \".pyd\", \".pyo\" ]:\n\t\t\tcontinue\n# \t\tos.system(\"git status --porcelain --exclude=\\\"**/*/**.py\\\" --exclude=\\\"*~/*.pyw/*.pydw/*.pyo\\\" *=py\")\n\t\tif ( getFileExtension(filename) == extension ) :\n\t\t\treturn True\n\n# \treturn False\n\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t# If the filename already matches this module, don't try this\n\tif os.path.isfile(filename):\n\t\treturn False\n\n\t# If the filename is a type that this module supports,\n\tif filename.endswith('in') or filename.endswith('out') or filename.endswith('h'):\n\t\treturn False\n\n\t# Otherwise, add this pattern to the list of\n\tif filename.endswith(\".in\"):\n\t\text = \".in\"\n\telif filename.endswith(\".out\"):\n\t\text = \"."}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t_SUPPORTED = [\"python\",\"pyyaml\",\"python-scm\"]\n\t\n\tif os.path.basename(filename) not in _SUPPORTED:\n\t\treturn False\n\telse:\n\t\treturn True\n\t\ndef _match_dirname(filename):\n\t\"\"\"\n\t\tMatch the path we are given\n\n\t\tArgs:\n\t\t    filename: Filename to match\n\t\tReturns:\n\t\t    path\n\t\"\"\"\n\t_dirname = os.path.dirname(filename)\n\tif len(filename)>1:\n\t\t_dir"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t# Make sure the file really exists\n\tos.chdir(path)\n\t\n\t# File to test\n\tname = \"tmp\"\n\t# Open destination\n\tdestination = open(name, 'w')\n\t# Get the file name\n\t\n\t# Get the path\n\tpath = os.path.split(filename)\n\t\n\t# Get the directory we want to save the file\n\tdir_ = os.path.abspath(os.path.expandvars((path[0])))\n\tos.chdir(dir_)"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\tif os.path.isfile(filename):\n\t\treturn True\n\telse:\n\t\treturn False\n\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif not os.path.isfile(filename):\n#\t\treturn False\n\t\traise NotImplementedError(\"Filename is not a type that this module supports\")\n\telse:\n\t\treturn True\n"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\tfilename, ext = os.path.splitext(os.path.basename(filename))\n\text = ext.upper()\n\t\n\t# Is it actually a valid file type?\n\tif os.path.isfile(filename) and (ext in SupportedExtensions):\n\t\treturn True\n\telse:\n\t\t# Try the regular file extension\n\t\treturn ext in SupportedExtensions\n\ndef CheckFileTypes(filename):\n\t\"\"\"\n\t\tDoes the filename contain file types\n\t\t\n\t\tArgs:\n\t\t\tfilename: Filename to match\n\t\tReturns:\n\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfrom.type_handler import BaseTypeHandler\n\t\n\tmTypeMatchResult = BaseTypeHandler.match(filename, \"\")\n\tif mTypeMatchResult is not None:\n\t\treturn mTypeMatchResult\n\n\traise ModuleException(ModuleException.INVALID_MODULE_NAME)\n\t\n#####################\n# PRIVATE TYPES #\n#####################\nclass BaseTypeHandlerFactory(object):\n\t\"\"\"\n\tBaseclass factory that creates the appropriate handler for any supported module type.\n\t\n\tArgs:\n\t    module_name: The name of the module type"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif any(os.path.splitext(filename)[1] in ['.cg', '.cpp', '.h', '.cpp', '.c', '.c', '.m']) or (filename[0].lower() == 'c' and filename[0].lower() == 'h') or (filename[1].lower() == 'c' and filename[1].lower() == 'c'):\n\t\treturn True\n\telse:\n\t\treturn False\t\n\ndef write_exporter(filename, content):\n\t\"\"\"\n\tWrite the content that will be written in to the file"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif frequency == \"\":\n\t\treturn None\n\t\t\n\tparsed_time = time.strptime(frequency)\n\treturn timedelta(**parsed_time)"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t_unit = {\"s\": 60*1.0e6,\"us\":\"100ns\",\"m\":\"s\",\"h\":3600,\"d\":\"day\",\"w\":\"week\",\"mh\":\"hour\",\"s\":\"sec\"}\n\t_frequency = {}\n\t_freq_unit = {}\n\tunit = _frequency.get(str(frequency),None)\n\t_freq_unit = _frequency.get(str(freq),None)\n\tif unit is not None and unit in _unit.keys():\n\t\tif _frequency.get(str(freq)):\n\t\t\tunit ="}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tfreq_parts = frequency.split(\":\")\n\tif not freq_parts:\n\t\traise ValueError(\"Parsing frequency didn't produce a valid datetime.timedelta object\")\n\t\n\tparsed_freq = datetime.timedelta()\n\tif len(freq_parts) == 1:\n\t\treturn parsed_freq\n\telif len(freq_parts) == 2:\n\t\tif freq_parts[0].isdigit():\n\t\t\tparsed_freq += timedelta(timedelta(int(p"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tif unit == \"always\" or unit is None:\n\t\treturn None\n\n\tfrom calendar import timegm\n\t\n\tdef get_time_value(frequency):\n\t\t(seconds, offset) = frequency.split('/')\n\t\t(hours, minutes) = [float(time.strptime(x,y))/60 for (x,y) in zip(hours, minutes)]\n\t\t(minutes, seconds) = [float(x) for x in (minutes, seconds)]\n\t\t\n\t\treturn timegm( (hours, minutes, seconds) + offset )"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tunits = {\"weeks\": timedelta(days=7*24*26*365), \"days\": timedelta(1)}\n\n\ttry: \n\t\t# Convert the frequency to a datetime object\n\t\tf = int(f) # The unit is 'weeks' or 'days'\n\t\tunit = units[f]\n\n\t\t# If the unit is not provided, use the global unit\n\t\tif len(f) == 1:\n\t\t\tf = units[\"weeks\"]\n\n\t\t# Convert the number to a datetime object\n\t\tdt = datetime.fromtimestamp(time."}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tfor f in self.fields.values():\n\t\tif f.freetype == \"\":\n\t\t\traise ValueError(\"Frequency cannot be parsed. \"\n\t\t\t\t\t\t\"Frequency \" + self.freetype + \" can be used as frequency string.\")\n\t\n\tif self.freetype == \"\":\n\t\ttry:\n\t\t\tf = datetime.time(self)\n\t\texcept:\n\t\t\tf = None\n\telif self.freetype == \"strict\":\n\t\tf = re.compile(\"[sS]([NU]ot d[Dh])"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t#parsed frequency string\n\ttry:\n\t\tdt_dict = {\n\t\t\t'sa' : (lambda d,s,o: datetime.strptime(s.strip(),s.strip().split(' ')[1])),\n\t\t\t'mm' : (lambda d,s,o: datetime.strptime(s.strip(),s.strip().split(' ')[1])-timedelta(weeks=o)),\n\t\t\t'h' : (lambda d,s,o: datetime.strptime(s.strip(),s.strip"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif float(f\"\\u0026f{f\"+f\"\".join(f\"-\"*f.weekday())+\".0\"}+f\"\\u0026e\"+f\"\\U00010002\" in f\"{f}, {unit}; {mode}\"\n\t\tand {\"{f}\".format(f=f.name()).strip(\")\").strip(\" \")*f.unit().lower() is None): f=f\"\".join(f.name().lower() for f in f\"\".split())\n\t\tand f is None:\n\t\t    f="}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\traise ValueError(\"Unable to parse frequency as a time, timedelta, or None, please check the frequency parameter.\")\n\n\n# Global variables\n_time_format = \"%M%S%z\" ## The format that is displayed by the date view's DateTimePicker\n_day_unit = \"\"\n_hour_unit = \"\"\n_minute_unit = \"\"\n_end_time_unit = \"\"\n_hour_offset = None\n_minute_offset = None\n\n\t\ndef parse_frequency(frequency):\n\tif frequency == None:\n\t\treturn datetime.timedelta"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tdef __init__(self, value, unit=1):\n\t\tsuper(Frequency,self).__init__(value)\n\t\tif unit!= 1:\n\t\t\traise ValueError(\"Unable to parse non-conventional frequency %r, expected always\" % value)\n\t\tif value == \"never\":\n\t\t\treturn None\n\tdef __repr__(self):\n\t\treturn \"%s(value=%r, unit=%r)\" % (super(Frequency,self).__repr__(),self.value,self.unit)\n\t@property\n\tdef unit("}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\thost = socket.gethostbyname(host)\n\t\n\treturn (\n\t\thost == socket.gethostbyname('localhost') or\t\n\t\thost == socket.gethostbyaddr( ('127.0.0.1', 0) )\n\t)\n\n\n\ndef port_for(port, host):\n    \"\"\"\n\t\tReturns the port that will be used to listen to port passed on the command\n\t\t\n\t\t:param port: The port that is passed on the command\n\t\t:param host: The IP address or hostname of the host\n\t\t:return:"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\tc = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n\tif not c.connect((host, 51450)):\n\t\tc.send((host+\"\\x00\").encode(\"hex\"))\n\t\tprint('[local]',c)\n\t\tc.close()\n\n\t\t\n\thost = str(c.recv(4)).decode(\"hex\")\n\"\"\"\n\tChecks if the host IP equals to my IP. \n\t\n\t:param ip: The host IP\n\t:return: True if the"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\timport socket\n\treturn host.lower() == socket.gethostname().lower()\n\ndef get_node_ip(host):\n\t\"\"\"Check if one is connected to the ip\"\"\"\n\ttry:\n\t\treturn socket.gethostbyname(host)\n\texcept Exception, e:\n\t\treturn host\n\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\tis_localhost = False\n\t\n\ttry:\n\t\tremote_ip = socket.gethostbyname(host)\n\t\tis_localhost = True\n\texcept:\n\t\t\n\t\tLog.log(\"Error while checking /etc/hosts\", 3)\n\t\t\n\treturn is_localhost\n\n@import\ndef has_ipv4(hostname):\n        try:\n            socket.inet_aton(hostname)\n            is_localhost = True\n            return True\n        except:\n            is_localhost = False\n            return False\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\tserver = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\ttry:\n\t\tserver.connect((\"8.8.8.8\", 80))\n\t\treturn True\n\texcept:\n\t\treturn False\n\n\t\n# This code will not execute when using localhost 127.0.0.1\n# This will prevent the client from connecting\nif not sock==None and sock.family==socket.SOCK_DGRAM:\n\tprint \"Connecting to a valid address via DNS\"\n\thost = \"127.0."}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\t\n\ttry:\n\t    host = raw_input(\"If host is localhost is it? (y/n) \").upper()\n\texcept KeyboardInterrupt:\n\t    host = None\n\tif not host:\n\t    print(\"This program requires a host name or host in brackets\")\n\t    sys.exit(1)\n\treturn host\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\ts.connect((host, 22))\n\t\tif s.getsockname() == (host, 22):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\texcept socket.error:\n\t\ttry:\n\t\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\ts.connect(host, 22)\n\t\t\tif s.getsockname() == (host, 22):\n\t\t\t\treturn True\n\t\t\telse"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\tif str(host) == ':0':\n\t\t\tglobal host\n\t\t\treturn True\n    except NameError:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception as e:\n\"\"\"\n\tCatches any errors the socket is connected to and reports them appropriately\n        \traise\n        \texcept Exception as e:\n\"\"\"\n\tsock = socket.socket(socket.AF_INET,\n\t\t\t    socket.SOCK_STREAM)\n\t\n\t# Open the connection if it fails to connect\n\tsock.connect(host)\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tfor ip in range(*[0, MAX_CLIENTS]):\n\t\t# Attempt to connect to localhost on this port\n\t\tsock = socket.socket(socket.AF_INET)\n\t\tsock.connect((host, int(PORT)))\n\t\tsock.close()\n\t\tif ip == 0:\n\t\t\t# Attempt to connect to 127.0.1.1 on this port\n\t\t\tsock = socket.socket(socket.AF_INET)\n\t\t\tsock.connect((host, PORT))\n\t\t\tsock.close()\n\t\n\treturn ("}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif not is_localhost(host):\n\t\treturn False"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tdef __init__(self, *patterns):\n\t\tself.patterns = patterns\n\t\tself.todo = {}\n\t\tself.seen = []\n\t\t\n\tdef find_all_paths_from(self, path_fragment, parent_pattern=None):\n\t\tif isinstance(path_fragment, str):\n\t\t\tpath_fragment = path_fragment.split('/')\n\t\t\n\t\tif path_fragment:\n\t\t\t\n\t\t\t# split in case any leading characters are a pattern\n\t\t\tfor pattern in path_"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\ttry: exec(find_paths_str)\n\tfinally:\n\t\tglob_file.close()\n\nclass NodeFilter:\n\t\"\"\"\n\tA class for filtering nodes in a path. \n\t\"\"\"\n\n\tdef __init__(self,\n\t\tnode_re=re.compile(\"\\.[^/]*$\"), \n\t\tchild_re=re.compile(\"^(\\/[^/]*)\\/$\")\n\t):\n\t\tself.node_re = node_re\n\t\tself.child_re = child_re\n\"\"\"\"\"\""}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tif patterns is not None: # a list of patterns\n\t\tregexes = []\n\t\tfor path_fragment in patterns:\n\t\t\tregexes.append(r'.*/(?:.*?\\.(g|p|sh))' + path_fragment + r'.*\\\\2') # replace glob with a pattern:\n\t\tglobs = glob_regex.findall(find_paths) # find matching glob patterns in each path fragment\n\t\treturn [r.replace(globs[i], r'$1') for i in range(len(globs))"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef transform(self, sequence):\n\t\t\"\"\"\n\t\tGiven a sequence of path fragments as passed to `--find`, transform\n\t\tthem into glob patterns.\n\t\t\"\"\"\n\t\tfor path_fragment in sequence:\n\t\t\tself.path_fragments.extend(self.find_path_patterns(path_fragment.group(1)))\n\n\tdef find_path_patterns(self, pattern):\n\t\t\"\"\"\n\t\tGiven a pattern as passed to `--find`, return all path\n\t\tfragments that match it and have a trailing path separator \n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\timport re\n\timport os.path\n\t\n\tfiles = []\n\tfor pf in find_paths:\n\t\tmatch = re.match('^(.+?):(\\\\d+)$',pf)\n\t\tif match:\n\t\t\tp = pf.replace(match.group(1),'')\n\t\t\tfiles.append(p)\n\n\tfor pat in files:\n\t\tfor item in os.path.basename(pat).split('.'):\n\t\t\tif not item in ignored_items:\n\t\t\t\tyield pat+'[x]'+item\n\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tpath_glob_re = re.compile('\\[(?P<paths>' + '\\s' * 2\n\t\t\t\t\t\t   + '(?P<patterns>[^%]*(?:$|[%]))' + '\\)'\n\t\t\t\t\t\t   + '\\]|\\[(?P<globpats>' + '\\s' * 3 + '*' + '?(?P<globlst>%s)' + '\\]'\n\t\t\t\t\t\t   + '\\]|\\[(?P<*globpats>]+(?:"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tif patterns is None:\n\t\tpatterns = []\n\t\n\treturn patterns\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tbase_pattern = re.compile(r'^(?:(?:(?P<pattern>\\w+)(?::)?(?P<dir>\\w*)?/(?P<pattern_root>[^\\d])+)?)?/(?P<root>[^/]+)')\n\t\n\t#TODO: test that it is ok to pass paths with more than one segment to --find\n\t\n\tpatterns = {}\n\tif not isinstance(patterns, dict):\n\t\tpatterns = dict([(m, [], []) for m"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tresult = []\n\t\n\tfor pattern in args:\n\t\tif re.match(pattern, arg):\n\t\t\t# find_pattern is a glob pattern\n\t\t\tresult.append(arg)\n\t\t\tcontinue\n\t\t\n\t\telse:\n\t\t\tparts = pattern.split('*')\n\t\t\tfor part in parts:\n\t\t\t\tsegments = part.split('[')\n\t\t\t\tfor segment in segments:\n\t\t\t\t\tsubresult = []\n\t\t\t\t\tif not segment == '**':\n\t\t\t\t\t\tfor s in _find_paths_find(segment, parts):\n\t\t\t\t\t\t\tsubresult.append(s"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef find(self, pathFragments):\n\t\t\n\t\t\n\tdef find_paths_from_list(self, pathList):\n\n\t\t\"\"\" This should be more complex. \"\"\"\n\n\t\t\n\"\"\" *\n\t# find(pathFragments)\n\tPathFragment('/foo/baz.txt')\n\t\"\"\"\n\t# find('foobar')\n\t'foobar'\n\"\"\"\n\"\"\" *\n\tfind(pattern)\n\t\"\"\"\n\"\"\" * \n\tfind_pattern(pattern)\n\t\"\"\"\n\"\"\" * \n\tfind_list(pattern)\n\"\"\""}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tpass\n\ndef checkGit(args):\n \t# check to see if gz is set\n\t# if it is, check the path to it,\n\t# if not, do not make changes\n\t# if the path is not found in there, then\n\t# return False\n\t# no need of checking for git at all\n\tpass\n\ndef getGitPath():\n\n\t# check to see if the path to gz is there,\n\t# if it is, check for the path to git there\n\n\t# if check is successful, return the directory\n\t#"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif windir not in sys.path:\t\n\t\tsys.path.insert(0, windir)\n\t\tprint(\"Added path for gitbash to sys.path.\");\n\t\tprint(\"sys.path is now:\")\n\t\tfor x in sys.path:\n\t\t\tprint(x)\n\t\tprint(\"System path is now:\")\n\t\tfor x in os.environ:\n\t\t\tprint(x)\n\t\tif windir not in sys.argv[1:]:\n\t\t\tprint(\"You are in a Python environment that does not appear to be a git bash or git command."}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\ttry:\n\t  \n      \n\t\tcmdline = 'git bash -C bash_output'\n\t\tlogging.warning(cmdline)\n\t  \n      \n\t\tcmdline = 'echo %s | bash -c \"%s\"' % (bk_version,cmdline)\n\t\tlogging.warning(cmdline)\n\t  \n\t  \n\t\twith open(bk_version, 'r') as f:\n\t\t  bk_version = f.read()\n\t  \n\t  \n\t\tcmdline = 'python %s/git_status.py'"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tgitbash = os.path.exists(\"gitbash\")\n\n\tif(not gitbash):\n\t\treturn False\n\n\treturn os.path.exists(\"path/to/gitbash\")\n\n\n\n\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tfrom time import time\n\timport hashlib\n\tfrom os.path import basename, dirname, abspath, expanduser\n\t\n\tif not os.path.exists(abspath(os.curdir)):\n\t\tm = isWindows()\n\t\tif not m:\n\t\t\treturn False\n\t\tpass\n\t\n\tfrom git import path\n\tfrom git import version\n\timport os, re\n\t\n\t\n\tif os.path.split(path.split(path.abspath(abspath(os.curdir))[0]).split"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif not os.path.exists(\"GITRESOURCES\"):\n\t\treturn False\n\ttry:\n\t\tf = open(\"GITRESOURCES\",\"r\")\n\t\tcontents = f.read()\n\t\tf.close()\n\texcept:\n\t\treturn False\n\tlines = contents.split(\"\\n\")\n\tif 'GITRESOURCES' not in lines:\n\t\treturn False\n\treturn True\n\ndef parseLine(fileName,fName):\n\tf = open(fName)\n\tlines = f.readlines()\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif sys.platform == 'win32':\n\t\treturn False\n\telse:\n\t\tif os.path.isfile(\"/home/user/gitbash\"):\n\t\t\treturn True\n\treturn False\n\ndef get_os_name():\n\treturn os.name\n\ndef get_os_architecture():\n\treturn os.architecture()\n\t\ndef get_os_version():\n\treturn os.system('uname -o -v > nix')[8]\n\t\ndef get_os_distribution():\n\td = os.path."}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif \"bash\" in os.name:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n# This test class provides an alternate interface to this method. \n#\n# :note: This test class requires some git-bash build environment\n#    to build git-bash. \n#    When running this test, please ensure that the git-bash executable is\n#    $GIT_BASH_EXE.\n#\n#    It's only recommended that you use this test class with \n#    a normal git bash build environment and try running \n#    git-"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'posix':\n\t\treturn True\n\treturn False\n\ndef is_windows_git():\n\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'posix':\n\t\treturn True\n\treturn is_windows_cmd()\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.path.exists(os.path.join(self.workdir, os.lineno(r.line))) or os.path.exists(os.path.join(self.workdir, os.path.dirname(r.line))) or os.path.exists(os.path.join(self.workdir, os.path.basename(r.line)))\n\n\ndef findcmds(m, path=''):\n\t\"\"\"Find all lines for a command.\n\t\n\t@param: m (a mapping object)\n\t@param"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tlogging.info('Writing configs from [%s]', target)\n\tconfig_source = ConfigSource(config_filename, config_yaml, render_config, overwrite)\n\ttarget_config_dir = config_source.get_config_yaml_directory()\n\tconfig_source.write(target_config_dir, create_new=False)\n\tconfig_source.close()\n\tlogging.info('Config file %s written to.  Destination is [%s].', config_filename, target_config_dir)\ndef compile_config"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\ttry:\n\t\timport json\n\texcept ImportError:\n\t\timport simplejson as json\n\t# Make up a temporary config dir\n\ttry:\n\t\tfrom Configobj.Configobj import Configobj\n\texcept ImportError:\n\t\tfrom configobj import Configobj\n\t\t\n\tif not os.path.exists(os.path.dirname(config_fn)):\n\t\tos.makedirs(os.path.dirname(config_fn))\n\tconfig = Configobj(config_fn)\n\t\n\timport gzip\n\timport io\n\t\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\tfile = open(os.path.basename(configFilename), \"wt\")\n\tfile.write(configFile)\n\tfile.close()\n\tif not overwrite:\n\t\tif not confirm(\"Confirmation - overwrite existing file?\"):\n\t\t\treturn False\n\t\n\ttargetConfig = Config()\n\n\ttargetConfig.config.load(configFile)\n\n\ttargetsInSource = targetConfig.target.target.keys()\n\n\tdynamicTargetInXML = []\n\tfor t in targetsInSource:\n\t\tif t in (\"root\", \"subtargets"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t# read the configuration\n\tconfig = yaml.load(open(config_filename, \"r\"))\n\t\n\t# create the target directory if nessesary\n\tif not os.path.exists(target_directory):\n\t\tos.makedirs(target_directory)\n\t\n\t# if there's no file named 'config.yml', make one\n\tif not os.path.isfile(config_filename):\n\t\tfp = open(config_filename, \"w\")\n\t\tyaml.dump(config, fp)\n\t\tfp.close()\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t# import the configuration\n\tconfig = os.path.expanduser(os.path.expandvars(config))\n\twith open(config, 'r') as contents:\n\t\tyaml_file_content = contents.read()\n\tfor folder in (\n\t\tos.path.join(os.path.dirname(config), '..'),\n\t\tos.path.join('/srv/', os.path.dirname(config)),\n\t):\n\t\tif not os.path.isdir(os.path.join(folder, 'config')):\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tfile=None\n\tif os_utils.is_windows():\n\t\tfile = open(target_config_filename, 'w')\n\telse:\n\t\tfile = open(target_config_filename, 'a+')\n\tif os_utils.is_windows():\n\t\tyaml= yaml_utils.read(target_config_filename)\n\t\tfile_yaml= file.write(yaml)\n\t\tfile.write(yaml_utils.write(y,target_config_filename))\n\telse:\n\t\tfile_yaml= file.write("}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif os.path.isfile(filename):\n\t\t# If the YAML was already written\n\t\tif overwrite == False: return\n\t\telse:\n\t\t\tif os.path.isdir(os.path.dirname(filename)): os.rmdir(os.path.dirname(filename))\n\n\t\t\t# If it already exists but overwrite is True,\n\t\t\t# just abort writing anything\n\t\t\telse: return\n\n\t\n\twith open(filename, \"wt\") as f:\n\t\t# If the config YAML exists first write it out by creating a directory, overwrit"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t# get the current working directory so that we can remove the files if needed.\n\tcwd = os.getcwd()\n\n\t# if the target config includes a render config, create the destination dir if needed.\n\trender_config = []\n\tfor config_yaml_dict in os.listdir(os.path.abspath(path.join(cwd,'render'))):\n\t\tif config_yaml_dict not in ['-d', '--destination', '-e', '--environment', '+dest']:\n\t\t\trender_config.append"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\ttry:\n\t\ttarget_config = ConfigParser.RawConfigParser()\n\t\ttarget_config.read(target_config_filename)\n\t\ttarget_config.optionxform = str\n\t\tconfig = ConfigParser.RawConfigParser()\n\t\tconfig.read(target_config.get('main','config'))\n\t\tcurrent_filename = target_config.get('main', 'filename')\n\t\tif os.path.exists(current_filename) and os.path.samefile(os.path.dirname(current_filename), \".\"):\n\t\t\tif os.path."}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\timport os\n\timport yaml\n\n\t\n\t# check whether target exists and can be overwritten\n\tdest = yaml.safe_load(filename)\n\n\tif dest is None or dest is False:\n\t\treturn None, dest\n\n\t\n\t# if a file exists already or write to disk, this one will be deleted by overwriting\n\texisting_dest = os.path.abspath(dest)\n\t\n\tnew_dest, config_contents = check_and_write_config_yaml(dest, config_contents)\n\n\t\n\tif existing"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\twith open(os.devnull) as devnull:\n\t\tf = open(script, 'r')\n\t\tfor line in f:\n\t\t\tnewline = line.strip()\n\t\t\tif (newline[0] == '#') or (newline[0] == '/'):\n\t\t\t\tcontinue\n\n\t\t\tif newline is None:\n\t\t\t\tf.write(repr(script).rstrip())\n\t\t\telse:\n\t\t\t\tf.write(newline)\n\t\t\t\tf.write('\\n')\n\n\tf.write ('import pylmp' + os.linesep"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\ts =''.join(script)\n\t\n\tf = open(script_file, 'w')\n\tf.write(textwrap.dedent(s))\n\tf.close()\n\ndef test():\n\t\"\"\"\n\ttests\n\t\"\"\"\n\tprint \"\"\"\n\t\n\t\"\"\"\n\n\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\tfor item in script:\n\t\t\n\t\tif isinstance(item.strip(), six.text_type):\n\t\t\titem = str(item.strip())\n\n\t\tif '[' in item:\n\t\t\tcommand = item.split()[1]\n\n\t\titem = item.strip()\n\n\t\tif not item.startswith('\"') and not item.startswith(\"'\") and \\\n\t\t\tnot item.startswith(\"(\") and \\\n\t\t\tnot item.startswith(\")\"):\n\t\t\titem_list += [item]\n\n\titem_list = [item"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\treturn_code, output = 0, \"\"\n\toutput = \"\"\n\tcurrent_line = ''\n\tfor line_line in script.splitlines():\n\t\tcurrent_line += line_line\n\ttry:\n\t\tos.chdir(working_dir)\n\t\tos.system(\"\".join((current_line,out_bin + \" &\")))\n\t\tos.system(\"\".join((current_line,out_bin + \" &\")))\n\t\treturn_code, output = return_code, output\n\texcept:\n\t\treturn_code, output = return_code,"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tconverter = r\"convert\\s+0:0\\s+(\\S+)\", r\"echo $1 $2\", r\"(?P<output>(?>[[:space:]]*[^\\S])|(?:\\S|^\\S)|(?:\\S|^\\S$))\"\n\tscript = script.replace(converter, converter.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\n\", \"\\\\n\"))\n\tscript = script.replace(\"\\t\", \"\\\\t\")\n\tscript = script.replace(\"\\r\", \""}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif len(script)==0:\n\t\treturn 'none'\n\tscript=str(script)\n\n\tscripts[script[0:2]]=script[2:]\n\tscripts.insert(scripts.count(script),script)\n\n\treturn script\n\ndef execute(self,scripts,target,params,local):\n\treturn self.convert(scripts,target,params,**local)\n\n\n\ndef run(self,scripts,target,params):\n\treturn self.execute(self,scripts,target,params,{'target':target})\n\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\twith open(\"{:s}\".format(script), mode=\"r2\") as f:\n\t\toutput = f.read().rstrip().decode(\"utf-8\")\n\n\twith open(\"cmd\", \"w\") as f:\n\t\tf.write(output)\n\n\treturn output\n\n\ndef compile_template(script, args):\n\t\"\"\"\n\tCompile a one line script into one command.\n\t:param script:\n\t:param args: arguments to pass to the one line script\n\t:return:\n\t\"\"\"\n\ttemplate = textwrap.dedent(\"\"\""}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\timport sys\n\timport subprocess\n\timport shlex\n\t\n\twith open('/tmp/output.log', 'w') as myfile:\n\t\ttextwrap.Append(textwrap.TextWrapper(\n\t\t\tshlex.split(script, posix=True)[0]\n\t\t), myfile\n\t\t\n\tsubprocess.Popen('/bin/bash -c \"'+script+'\"', stdout=sys.stdout, stderr=sys.stderr)\n\tsys.exit(0)\n\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\tlines = []\n\tfor line in  subprocess.check_output( \n\t\t\t\t\t['xen-python','scripts', '--raw', script] ):\n\t\tline = line.strip()\n\t\tif line == '':\n\t\t\tcontinue\n\t\tlines.extend(line.split(' '))\n\treturn u''.join(lines)\n\ndef run_single_script(script):\n\t\n\ttry:\n\t\tscript = os.path.expanduser(script)\n\t\tos.chmod(script, 0755)\n\t\tsubprocess.call(['"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tassert(script is not None)\n\tassert(type(script) is str)\n\twith open(script) as fd:\n\t\trawData = fd.read()\n\tprint rawData.split('\\n')\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\tf = func\n\tkwargs = dict()\n\t\n\tfor key,val in kwargs.iteritems():\n\t\t# skip keys for \"extra_env\": envvars\n\t\tif key.lower()==\"env\":\n\t\t\tpass\n\t\telif  key in ['env', 'path']:\n\t\t\tpass\n\t\telse:\n\t\t\tkwargs[key] = val\n\t\n\textra_env = kwargs.get(\"extra_env\", None)\n\n\tenv = os.environ.copy()\n\t\n\tif extra_env is not None"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tcmdv = map(lambda x: \"python \"%s.py\" % x, args)\n\tif extra_env:\n\t    env = dict(extra_env)\n\telse:\n\t    env = None\n\t# Note that using sys.stdout or sys.stderr will not work with an stdio\n\t# socket.\n\tif isinstance(raw_stdout, str):\n\t    stdout = sys.stdout\n\telse:\n\t    stdout = raw_stdout.buffer\n\tif isinstance(raw_stderr, str"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif not re.search('^\\.', func.__class__.__name__, re.M):\n\t\traise ValueError('function not in known module: %s' % func.__class__.__name__)\n\n\targs = args or []\n\tenv = {}\n\textra_env = extra_env or {}\n\n\t# Use the PYTHONPATH environment variable if it's not already there.  Also,\n\t# remove the directory we're in for the Python interpreter.\n\tif not is_path_valid(PYTHONPATH):\n\t\tif"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t# create and clean up old process output\n\tout = \"\"\n\tif os.path.exists(\"/proc/%d/cmd\" % pid) and os.name == 'posix':\n\t   cmd = 'cat /proc/%d/cmd | awk {print $3}' % pid\n\telse:\n\t   cmd = \"\"\"execve(`pidof /proc/$pid`) | awk {print $2:$3}{$3}{end}\"\"\"\n\t   \n\t   #print cmd\n\t   p = subprocess.Popen(cmd"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tsubprocess.run(\n\t\tfunc,\n\t\t*args,\n\t\tenv=dict(\n\t\t\tPYTHON_PROGRAM='python',\n\t\t\tPYTHON_VERBOSE='verbose',\n\t\t\tFORCE_DEBUG=True,\n\t\t\tPYTHON_USE_ASYNC=False,\n\t\t\tCPPFLAGS=''))\n\ndef load_test_pylib(lib):\n\t\"\"\"\n\tLoad a python library into a python package. Assumes the library is in the\n\ttest directory.\n\tReturns the path to the newly created package."}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\tdef run(fname, *args, **kwargs):\n\t\t\n\t\ttry:\n\t\t\tpid = subprocess.Popen(args, *args, **kwargs)\n\t\t\n\t\texcept Exception as e:\n\t\t\tprint \"ERROR: Run failed. Error={}\".format(str(e))\n\t\t\treturn False\n\t\t\n\t\t\n\t\t\n\t\treturn pid "}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\treturn subprocess.run(\n\t    func,\n\t    stdout=subprocess.PIPE,\n\t    stderr=subprocess.PIPE,\n\t    **extra_env)\n\ndef create_run_script(pklpath,\n                      script,\n                      log,\n                      logdir=None,\n                      argv=[],\n                      env=None,\n                      stdoutdir=None,\n                      stderrdir=None,\n                      ) :\n\timport pickle\n\tcwd = os.getcwd()\n\tos.ch"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef run(*args, **kwargs):\n\t\tcmd = \"%s %s\" % (subprocess.run, args)\n\t\tkwargs.update(extra_env)\n\t\tprocess = subprocess.Popen(cmd, **kwargs)\n\t\tprocess.wait()\n\treturn run\n\n\ndef run_cmd(cmd, cwd=None, **kwargs):\n\t\"\"\"\n\tRun a command and return its output.\n\n\tParameters\n\t----------\n\tcmd: str \n\t    A command to run.  Eg: \"rm\"\n\tcwd: str"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\treload(subprocess)\n\t\n\t# Get the full command line arguments for the subprocess.run method.\n\tfunc_args = func.__code__.co_varnames[1]\n\t\n\t# Check if they are strings.\n\tif not type(func_args) in (str, unicode) or not func_args.isnumeric():\n\t\traise TypeError(\"func_args must be str or unicode with numeric arguments\")\n\n\tfunc_args = func_args.lstrip('str')  # This allows for us to avoid an import error"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif not hasattr(subprocess, 'run'):\n\t\traise ValueError('subprocess.run is not available.')\n\treturn subprocess.run(func, *args, **extra_env)\n\n\t\n\n\"\"\"\n\tExecute a function in a sub-process using the run command.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tmatcher = re.compile(\"^\" + NAME_REGEX + \"\\s?\", re.MULTILINE)\n\tif None not in env and env[None]:\n\t\treturn env[None]\n\telse:\n\t\tenvironment = os.getenv(NAME_ENV)  # env =  {'NAME_1':'value1','NAME_2':'value2'}\n\t\treturn environment.get(matcher)\n\n\ndef get_variable_from_file(name=None, environment={}, default=None, filepath='.'):\n\t\"\"\"\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\treturn readenv(matcher, kwargs, check=False) or default\n\ndef readenv(matcher, kwargs, check=True):\n\tvar = matcher.pop(0)\n\ttry:\n\t\treturn os.environ[var]\n\texcept KeyError:\n\t\tif check:\n\t\t\traise KeyError(\"environment variable '%s' does not exist.\" % var)\n\t\telif os.environ.has_key(var):\n\t\t\treturn os.environ[var]\n\t\telse:\n\t\t\treturn None\n\ndef getVarFrom"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif not re.match('[0-9]+', value):\n\t\traise EnvironmentError(\"Value '%s' is not a valid environment variable name '%s'\" % (value, _matcher.pattern()))\n\t\n\tdefault_value = _matcher.default_value()\n\n\t# If the variable has to wait for a condition to be notified, return the value, including the default value if this was not satisfied.\n\tif _matcher.is_wait() and default_value is not None:\n\t\treturn default_value\n\t\n\t# If the variable is defined"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tval = os.environ[envKey]\n\texcept KeyError:\n\t\tval = None\n\treturn val\n\ndef env(varname=None):\n\t\"\"\"\n\tReturns the value of the environment variable named varname.\n\tThe variable is looked up from an alternative list of known environments\n\tif varname is a string, the variable is looked up on the second or later entries in the list.\n\tIf varname is not a string (or if it returns a list), an Error is raise.\n\t\"\"\"\n\tif varname is None:"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif (len(matcher)!=len(name)):\n\t\tif (len(name)>0 and os.environ.has_key(name)):\n\t\t\ttry:\n\t\t\t\treturn os.environ[name]\n\t\t\texcept Exception, e:\n\t\t\t\traise Error(\"Matcher '%s' must be defined in the name %s\" % (matcher, name))\n\t\telse:\n\t\t\traise Error(\"Matcher '%s' must be defined in the name %s: '%s' is undefined as an environment variable\" % (matcher, name, os."}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname_value = {}\n\tif matcher.name:\n\t\tname_value[matcher.name] = matcher.default or {}\n\treturn matcher.default_value or name_value\n\n\n# Environment variables for globus\n# These are all defined using the globus env var names\n# It is possible to define all of these in one go\n# This list is not exhaustive and can contain all globes found, however. It is not possible to\n# define globuses which are the only ones in this file.\nglobus_env_vars = [key for key"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t#Check for the default value of the matcher\n\tdefaultValue = os.getenv(matcher[0], defaultValue)\n\tif defaultValue:\n\t\treturn defaultValue\n\telse:\n\t\treturn None\n\n\n\nclass LogHandler(logging.StreamHandler):\n\t\"\"\"\n\t\tThis class accepts a StreamHandler object which receives a message for the given log file and passes that to a logging.Logger object. A message will \n\t\tbe emitted when the logging.StreamHandler is called.\n\t\"\"\"\n\n\tdef __init__(self, targetFile, filename, handlers="}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = os.environ.get( matcher.name, matcher.default )\n\tif not matcher.allow_none and not value:\n\t\traise KeyError( \"%s %r is required but not in environment\" % ( matcher.desc, matcher.name ) )\n\treturn value\n\n\t\ndef parse_configuration_from_environment( configuration ):\n\t\"\"\"\n\tParse configuration from environment variables.\n\t\"\"\"\n\t\n\tfor matcher in configuration.findall( \"/\" ):\n\n\t\tvalue = parse_matcher_value( matcher )\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = os.environ.get(name, default) # check if env var exists\n\treturn value\n\t\n\t\ndef setup(config):\n\t\"\"\"\n\tSetup the environment variables used by multiple functions defined on different classes.\n\n\tThe variables defined by different classes will be set as environment variables within the environment.\n\t\n\tExample input:\n\t \n\t  [class A(object):\n\t      @property\n\t      def name(self):\n\t          return self\n\t          \n\t      @property\n\t      def default_value(self):\n\t          return"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif '${DEFAULT_ENV}' in os.environ:\n\t\tDEFAULT = os.environ['${DEFAULT_ENV}']\n\telse:\n\t\tDEFAULT = \"None\"\n\tDEFAULT = os.environ['ENV']\n\tif DEFAULT:\n\t\tif '${ENV}' in DEFAULT:\n\t\t\tDEFAULT = os.environ['ENV']\n\t\telse:\n\t\t\tDEFAULT = \"None\"\n\n\tres = {'value': DEFAULT}\n\n\tfor var in envs['key']:\n\t\tvalue"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ti = urlparse.urlparse(image_href)\n\t(netloc, pathsep) = i.path.partition('//')\n\tpathsep.strip(' ')\n\tpath = '/' + pathsep.split(i.pathsep, 1)[1].lower()\n\t# Strip out the last character of the path (the port).\n\tpath = path.split('port', 1)[0]\n\tuse_ssl = i.scheme.lower() in ('https', 'https', True)\n\t\n\treturn (path, use_"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttry:\n\t\tuse_ssl = urllib_parse.parse_qs(urllib_request.BaseNlri).get(\n\t\t\t'use_ssl')[0]\n\texcept IndexError:\n\t\tuse_ssl = False\n\t\n\turl = urllib_parse.urlsplit(image_href).netloc\n\t\n\tif not image_href.startswith('http'):\n\t\timage_href = 'http://' + url\n\t\n\timage = urllib_request.urlopen(image_href).read()\n\t\n"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttry:\n\t\thloc = None\n\t\tnetloc = None\n\t\tif '://' in image_href:\n\t\t\thloc = urlparse(image_href).netloc\n\t\tif ':' in image_href:\n\t\t\tnetloc = urlparse(image_href).netloc\n\t\timage_href = str(hloc) + '//' + str(netloc)\n\t\tif '@' in image_href:\n\t\t\timage_id, image_href = image_href.split('@',1)\n\n\t\treturn (image_id, image_href, False"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tm = re.match(b\".*(#.*)?$\", image_href)\n\tif not m:\n\t\traise ValueError(\"%s is not a valid file link.\" % image_href)\n\t(netloc, use_ssl) = m.groups()\n\t\n\tif use_ssl and not os.path.exists(os.path.expanduser(\"~/.gnupg\")):\n\t\traise ValueError(\"Cannot use ssl over %s (not available/available)\" % os.path.expanduser(\"~/.gnupg\"))\n"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\ttry:\n\t\timage_href = urllib.urlparse.urlparse(image_href)\n\texcept Exception as e:\n\t\traise ValueError(str(e))\n\t\t\n\tbase_url = urlparse(image_href.scheme+\".\")+urlparse(image_href.netloc)\n\t\n\tbase_netloc, path = urlsplit(base_url)\n\t\n\tuse_ssl = \"https\" == urllib.urlparse.urlparse(base_url).scheme\n\t\n\tif not path:"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t# Check if using HTTPS. Returns if not already using HTTPS.\n\tif use_https and not use_ssl:\n\t\tpass\n\telif use_ssl and not use_https:\n\t\tuse_https = True\n\telse:\n\t\tuse_https = False\n\n\t# Parse the image href into a complete url without the protocol.\n\t# We use protocol for non-http, and ssl with protocol for http.\n\turl_template = '{protocol}://{netloc}{url}'\n\turl = urllib.unquote(image_"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timghdr = None\n\ttry:\n\t\t# open the image\n\t\timghdr = urllib.urlopen(image_href).read()\n\texcept AttributeError:\n\t\traise ValueError(\"No image URL found in the image_href argument; use the url argument instead.\")\n\t\n\ttokens = [x.strip() for x in imghdr.split()]\n\timage_id = tokens[0]\n\tnetloc =''.join(tokens[3:])\n\ttry:\n\t\tuse_ssl = bool(tokens"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_info = {}\n\ttry:\n\t\t# Attempt to decode url using the registry\n\t\tschema, url = urllib.urlparse(image_href)\n\texcept ValueError:\n\t\t# If we got this far, we're pretty screwed!\n\t\t# We'll just return a bogus image id\n\t\treturn (None, ('',), False)\n\t\t\n\t# We should be at the first \"://\"\n\tscheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)\n\tpath = os.path.normpath"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_href = urllib.unquote(image_href)\n\tif not image_href:\n\t\traise ValueError(\"Invalid image URL: %s\" % (image_href))\n\t\n\tparts = parse_href(image_href)\n\tif len(parts) < 4:\n\t\traise ValueError(\"Invalid image URL: %s\" % (image_href))\n\t\t\n\t# Check scheme\n\tif parts[0].lower().startswith(\"http\"):\n\t\t# HTTP request\n\t\ttry:\n\t\t\tscheme = parts[0][6:"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_href = unicode(image_href)\n\turl = urlparse.urlsplit(str(image_href)).path\n\tuse_ssl = True\n\t\n\ttry: url_parts = url.split('#', 1)\n\texcept IndexError, e: raise ValueError('Could not find # in image_href %s' % str(image_href))\n\t\n\t# first, extract the ID number to remove any fragmenting\n\timage_id = url_parts[0]\n\t\n\t# then, split on the \",\"\n\tnetloc_"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\ttemp_list = []\n\twhile len(temp_list) :\n\t\tindex = temp_list.find('/')\n\t\tif index!= -1 :\n\t\t\tif index > 0 : #remove trailing os seperator\n\t\t\t\ttemp_list.replace(index + 1, '/')\n\t\t\telse : #remove leading os seperator\n\t\t\t\ttemp_list.remove(index)\n\t\ttemp_list = temp_list[0:(len(temp_list)-1)]\n\ndef get_dir_path(source) :\n\t\"\"\"\n\tReturns the path to the file or"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\tresult = []\n\tfor i in xrange(len(input_list)):\n\t\tif len(input_list[i]) > 1:\n\t\t\tlast_check_char = input_list[i][-1]\n\t\t\tfor j in range(len(input_list)):\n\t\t\t\tif input_list[j][-1]!= last_check_char:\n\t\t\t\t\tinput_list[i] = input_list[j]\n\t\t\n\treturn input_list\n\t\t\n\t\ndef clean_path(path_input):\n\treturn urllib."}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tretVal = []\n\tif type(input_list) == list:\n\t\t# make sure our input isn't empty (len=0)\n\t\tif not len(input_list):\n\t\t\treturn retVal\n\t\telse: \n\t\t\tfor i in range(len(input_list) - 1):\n\t\t\t\titem = input_list[i:]\n\t\t\t\tsublist = input_list[:i]\n\t\t\t\tretVal.extend(removePathSep(sublist))\n\telse:\n\t\t# not a list, just handle the simple case (len=0)\n\t\titem"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\treturn [x for x in input_list if x and x[-1]!= os.sep]\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfor string in input_list:\n\t\tif len(string) > 1:\n\t\t\tnew_string = \"\"\n\t\t\tif string[-1] == os.sep:\n\t\t\t\tnew_string = string[:-1]\n\t\t\telse:\n\t\t\t\tnew_string = string[:-1]\n\t\t\t\n\t\t\tnew_string = unicode.split(new_string)[1]\n\t\t\tprint new_string\n\t\telse:\n\t\t\ttry:\n\t\t\t\tnew_string =  unicode.split(string)[0]\n\t\t\texcept:\n\t\t\t\traise TypeError(\"Only string lists can consist"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tinput_list = [str(value) for value in input_list]\n\tfor i in range(len(input_list) - 2):\n\t\tif input_list[i+1][-2:] == os.sep:  # found our pathname seperator\n\t\t\tinput_list[i+1] = input_list[i+1][:-2]\n\ndef remove_trailing_os(input_list):\n\t\"\"\"\n\tIterate over a list of strings and remove trailing os seperator\n\tcharacters.\n\t\n\tEach string is"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tremove_path_sep = re.compile(r\"//[^\\s:]*\", re.IGNORECASE)\n\toutput_list = []\n\tfor input_list_element in input_list:\n\t\ttry:\n\t\t\tseperator_index = remove_path_sep.search(input_list_element).start()\n\t\t\tif seperator_index > 0:\n\t\t\t\tremove_path_sep.sub(os.pathsep, input_list_element)\n\t\t\t\toutput_list.append(input_list_element[seper"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tseps = \"..\"\n\toutput_list = []\n\t\n\tfor entry in os.listdir(seps):\n\t\tif os.path.isfile(s + sep + entry):\n\t\t\tif entry == seps:\n\t\t\t\tseperator = \"..\"\n\t\t\telse:\n\t\t\t\tseperator = sep + entry\n\t\t\t\tfor i in range(1, len(output_list)):\n\t\t\t\t\tif os.path.isfile(output_list[i] + seperator):\n\t\t\t\t\t\toutput_list.pop(i)\n\t\t\t\t\t\tbreak\n\t\t\tbreak\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\tfor i in input_list:\n\t\tif len(i) == 1 and (os.sep == '\\\\' or os.sep == '/'):\n\t\t\tinput_list[i] = os.sep\n\t\telse:\n\t\t\tinput_list[i] = input_list[i]\n\t\n\tprocessed_list = []\n\tfor i in input_list:\n\t\tif i.rfind(os.sep) >= 0:\n\t\t\tprocessed_list.append(i)\n\t\telse:\n\t\t\tbreak\n\t\n\treturn processed"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfor string in input_list:\n\t\tif not os.path.isabs(string):\n\t\t\tif not string.count(os.sep):\n\t\t\t\tif string!= os.path.sep:\n\t\t\t\t\tstring = os.path.sep\n\t\t\t\t\t\n\t\t\telif string!= os.path.sep:\n\t\t\t\tstring = string[:-1]\n\t\t\t\n\t\telif string.count(os.sep) > 1:\n\t\t\tstring = string.replace(os.sep, '').replace('\\\\', '/')\n\t\t\t\n\treturn input_list\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tre.compile(pattern)\n\n__version__ = '0.1'\n__author__ = 'Johan Spaan <johan@saa.nl>'\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tmatch_pattern = \"^([^/]*\\/)([^/]*)([^])$\"\n\tsplit_pattern = re.compile(match_pattern)\n\t\n\treturn split_pattern.sub(\"\\\\1\\\\2\\\\3\",string)\n\ndef normalize_url(string):\n\t\"\"\"\n\t\tthis method normalizes to the proper path using html entities before sending it to google\n\t\t\"\"\"\n\treturn escape(string.replace(\"&\",\"&amp;\").replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\"))\t\n\t\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tregex = re.split(\",\", patterns[t].pattern)\n\treturn [int(count), None, regex]\n\ndef convert(pattern, num):\n\t\"\"\"This function converts patterns\n\n\t\"\"\"\n\tpos = 2\n\tregex = [pattern[i] for i in range(0, len(pattern), pos)]\n\treturn [(regex[i], num) for i in range(0, len(regex), pos)]\n\ndef check(s):\n\t\"\"\"This method checks if string is a correct string\"\"\"\n\tfor t in stringtools.get"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef str2regex(s):\n\t\tres = re.compile(s, re.I)\n\t\treturn res\n\n\tres = str2regex(infile)\n\ts = res.string\n\treturn s\n\n#\n\"\"\"\n\tThis method returns a list of all the words\n\t\"\"\"\n\tdef words(s):\n\t\tres = re.compile(r'\\'\\' +\\s+\\d+\\s*+?[a-z]\\s*+?.*?\\s+\\D+ + \\w+ + := + ("}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tre_obj = re.compile('[\\s(](?P<substring>[)]+)', re.I)\n\treturn re_obj.sub('', s)\n\n\ndef convert_list_to_csv(lstr):\n\n\t\"\"\"\n\t\tThis method converts the given string to csv\n\n\t\t:param lstr: The given string to convert\n\t\t:type lstr: str\n\t\t:return: The given string converted to csv\n\t\t:rtype: str\n\t\"\"\"\n\n\tlines = ['\\n'.join([line.lstrip() for line in l"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tpattern = re.compile(r'(?P<regex_string>' + r'\\w+)')\n\treturn pattern.subn(re.escape, value)\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tpattern = r\"%s+?\" % (self.regex, )\n\tif self.group_by:\n\t\tpattern += r\"|(%s.%s|%s)%s\" % (self.group_by, self.group_order, self.sub_type, self.sub_re)\n\tpattern += r\"%s?\" % (self.exclude_name, )\n\tif self.exclude_re:\n\t\tpattern += \"!(%s).\" % (re.compile(self.exclude_re).pattern)\n\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tp = \"\"\n\tfor c in text:\n\t\tif c == u'<':\n\t\t\tp = \".*\\\".+?\"\n\t\t\tbreak\n\t\telif c == u'>':\n\t\t\tp = r\"\\s+\"\n\t\telif c == u'.':\n\t\t\tbreak\n\treturn p\n\"\"\"\nfrom kodi import xbmc\nimport re\nimport re\n\"\"\"\n\ndef showError(message):\n\txbmc.executebuiltin(\"Dialog.Close(Busy)\");\n\txbmc.executebuiltin(\"Dialog.Close(Confirm"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tcompiledPattern = re.compile(\"[{0}\\W]+\".format(re.escape(patternType)),re.UNICODE|re.I)\n\treturn compiledPattern.sub(\"\\\\1\",pattern)\n\"\"\"\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef regex_pattern_helper(pattern):\n\t\tre_pattern = \"\"\n\t\tgroup_index = 0;\n\t\twhile group_index < len(pattern):\n\t\t\tif pattern[group_index] == \"<\":\n\t\t\t\tre_pattern = re_pattern + \"(\"\n\t\t\t\tgroup_index = group_index + 1\n\t\t\telif pattern[group_index] == \">\":\n\t\t\t\tre_pattern = re_pattern + \")*\"\n\t\t\t\tgroup_index = group_index + 1\n\t\t\telif pattern[group_index] == \"(\":\n\t\t\t\tre_pattern ="}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmds = subprocess.list2cmdline(cmds)\n\n\tret = subprocess.call(cmds)\n\treturn ret\n\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\tif '#' in cmd:\n\t\tcmd = cmd.split('#', 1)[0]\n\t\n\tprint \"Running\", `cmd`\n\t\n\tfor line in shellcode.splitlines(True):\n\t\tprint \".\"\n\t\tstdout, stderr = subprocess.call(stdout, stderr=subprocess.PIPE, cwd=shellstack)\n\t\n\tprint `stderr`\t\n\ndef _get_module_names(module_files):\n\tresult = []\n\n\tfor f in module_files:\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tsubprocess.call([self.dirToLdconfig, '-v', path_to_ldconfig])\n\ndef test():\n\tprint(\"\\nRunning tests for ldconfig in \", os.path.basename(sys.argv[0]))\n\ttry:\n\t\tos.environ[\"LD_LIBRARY_PATH\"] = path_to_ldconfig\n\t\tcheck_ldconfig()\n\texcept:\n\t\tprint(\"Error!\")\n\t\tassert(False)\n\ttry:\n\t\tos.environ[\"HOME\"] = path_to_home\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfor cmd in command_list:\n\t\tstdin = subprocess.PIPE\n\t\tif isinstance(cmd, cmds.Text):\n\n\t\t\tcmd.send(subprocess.stdin)\n\t\t\tif isinstance(cmd.excep, KeyboardInterrupt):\n\t\t\t\ttry:\n\t\t\t\t\tstdin.write(cmd.send.stdin.read())\n\t\t\t\t\texcep_str = cmd.except\n\t\t\t\texcept:\n\t\t\t\t\texcep_str = 'Command aborted.'\n\t\t\telse:\n\t\t\t\tstdin.write(cmd.send('--help'))\n\t\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif os.path.basename( sys.executable ) == 'python':\n\t\tscript_args = []\n\t\tif not os.curdir:\n\t\t\tscript_args = [ sys.executable ]\n\t\telse:\n\t\t\tscript_args = [ p.script_name for p in glob( os.curdir + '/*' )]\n\t\tprocess.run('\"' +''.join(script_args) + '\"' + ''.join(sys.argv[1:]))\n\t\treturn None\n\telse:\n\t\toutput = Popen( args )\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tenv = os.environ\n\tenv['PATH'] = ':'.join([env['PATH'],\n\t\t\t'-e', '%s' % CMD_ARG])\n\n\n\ttry:\n\t\tenv['_SYS_ARGV_TMP']\n\t\targs = shlex.split(CMD_ARG)\n\texcept Exception, e:\n\t\targs = [CMD_ARG]\n\t\t\n\tenv['_PYTHON_INITIAL_TMP'] = os.path.abspath(__file__)\n\n\t## Setup process"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs = sys.argv[1:]\n\targs.remove('-n')\n\tif subprocess.call(args)!= 0:\n\t\tprint('Command \"%s\" failed.' % command)\n\n\telse:\n\t\tprint('Successfully completed.')\n\n\ndef main(command):\n\tmain(command, check=True)\n\n\nif __name__ == '__main__':\n\tmain()"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\toutput = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT, timeout=3000)\n\t#print(stderr.read())\n\t#print(output)\n\tstderr.write(\"\\n\")\n\n\treturn output\n\t\n\n\n\n\"\"\"\n    PURPOSE: To run a command-line and gather the stdout, stderr and return it\n\n\"\"\"\ndef main(command):\n\tstdout = \"\"\n\tstderr = \"\"\n\t\n\t# Run command, check stdout\n\ttry:"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmds = list(sys.argv)\n\tcommands = ''\n\tfor c in cmds:\n\t\tcommands += c +''\n\treturn commands\n\n\"\"\"\n\tGets a list of directories with entries matching this pattern. Dirs with entry\n\tmatching this pattern won't be skipped in the output.\n\t\"\"\"\n\timport ntpath\n\tdirs = []\n\tfor d in os.listdir('.'):\n\t\tif ntpath.basename(d).endswith(pattern):\n\t\t\tdirs.append(d)\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef foo(self,command):\n\t\timport sys\n\t\ttry:\n\t\t\treturn subprocess.call(command)\n\t\texcept OSError:\n\t\t\treturn False\n\t\n\treturn dict(command=\"cd../python/core/pylons.conf;% import run\n\t\"\"\"\n\t\n\t\n\tdef command(self,command):\n\t    command = \" \".join([c.command for c in command])\n\t    return \"\\ncd \"+config['general']['cwd']+\";\"+command\n\t    \n\tdef execute(self,command"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef test_ip_address(self):\n\t\t\n\t\ttry:\n\t\t\tr = ip_network('10.0.0.1/24')\n\t\t\tself.assertTrue(hasattr(r,'src'))\n\t\t\tself.assertTrue(hasattr(r, 'net'))\n\t\t\tself.assertTrue(hasattr(r, 'netmask'))\n\t\t\tself.assertEqual(r.to_string(), '[10.0.0.1]')\n\n\t\t\tn = ip_network('2001::1')\n\t\t\tself.assertTrue(hasattr("}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t#print 'Testing ipaddress.ipaddress in tests'\n\tx = ipaddress.ipaddress(4, 'localhost')\n\ty = ipaddress.ipaddress(11, 'tcp')\n\tself.assertFalse(x is y)\n\n\tx = ipaddress.ipaddress(4, '1.1.1.1')\n\ty = ipaddress.ipaddress(6, '1.1.1.1')\n\tself.assertEqual(x, y)\n\n\tx = ipaddress.ipaddress(2, '192.168.0.0"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef test(self):\n\t\t\"\"\"\n\t\t\tA valid IPv4 address\n\n\t\t\"\"\"\n\n\t\tself.assertEqual(\"21212141\", ipaddress.ip_string(21212141))\n\t\tself.assertTrue(ipaddress.ip_string(21212141) in ipaddress.ip4_network(22,22,22,22))\n\t\tself.assertFalse(ipaddress.ip_string(21212141/2) in ipaddress.ip4_network(22,22,22,22))\n\t\tself.assertTrue(ipaddress.ip_string(212"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tip = get_ip()\n\tif ip == '':\n\t\tprint \"ip addr: \" + ipaddress.ip_addr(ipadd)\n\t\t\n\t\t#IPv6 address\n\t\tif ip.find(\"[\")!= -1 and ip.find(\"]\")!= -1:\n\t\t\tip = ipaddress.ip_addr(ip, ':ffff')\n\t\telse:\n\t\t\tip = ipaddress.ip_addr(ipadd)\n\telse:\n\t\tprint \"ip v4 addr: \" + ip\n\t\t\n\t\t#IPv6 address\n\t\tif ip"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\tresult = \"\"\n\tif ipaddress.ip_address(address)[\"version\"].val == 4:\n\t\tresult = \"Yes\"\n\telse:\n\t\tresult = \"No\"\n\t\n\treturn result\n\t\n\t\ndef runTest(client, args):\n\t\"\"\"main function for IPv4 test\"\"\"\n\t\n\tresult = \"\"\n\t\n\tprint \"IpAddress\" + IPADDRESS_INFO.add(address)\n\tipAddr = IPADDRESS_INFO.add(address)\n\tname = \"\"\n\t\n\tresult"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tpublic_ip = \"192.168.3.1\"\n\tprivate_ip = \"192.168.3.2\"\n\tprint (\"\")\n\tipaddress.ipv4.allocate(public_ip)\n\tipaddress.ipv4.allocate(private_ip)\n\ttry:\n\t\tprint (\"\")\n\t\tipaddress.ipv4.allocate(public_ip)\n\t\tprint (\"\")\n\t\tipaddress.ipv4.allocate(private_ip)\n\t\tp = ipaddress.ipv4.resolve(\"192.169"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef testIPv4Address(self):\n\t\tself.assertTrue\n\t\"\"\"\n\tTest if IPv6 address\n\t    \n\t\"\"\"\n\tdef testIPv6Address(self):\n\t\tself.assertTrue\n\n# vim: set et st ai nn r i u\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\tif \"pydevd\" in sys.argv:\n\t\tif IPaddress(address.encode('bytes')).version == 4:\n\t\t\tif host!= None:\n\t\t\t\treturn\n\t\t\thost = '127.0.0.1'\n\t\t\thost = ipaddress.ip_address(host)\n\n\t\tif host!= None:\n\t\t\treturn\n\t\n\tprint \"Testing if IPv4:\" + str(IPaddress(address).version)\n\tif IPaddress(address).version == 4:\n\t\treturn\n\t\n\tprint \"Testing if IPv6:\" + str"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\ttry:\n\t\tfor i in iprange():\n\t\t\tprint(i)\n\texcept:\n\t\tprint(\"No IP\")\n\t\n\ttry:\n\t\tfor i in iprange(1,255):\n\t\t\tprint(i)\n\texcept:\n\t\tprint(\"No IP\")\n\nif __name__ == \"__main__\":\n\tmain()\n\n"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\ttbk_client    = client[ \"tbk_client\" ]\n\ttbk_network   = tbk_client.network\n\ttbk_addr      = tbk_client.address\n\ttbk_ipv4      = ipaddress.ip_address( tbk_addr ).ipv4\n\ttry :\n\t    tbk_client.is_v6()\n\t    pass\n\t    \n\texcept:\n\t    pass\n\t    \n\ttry :\n\t    tbk_network.interface()\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tr = []\n\tfor g in graph.triples(skos.SKOS # is this a subclass of skos.SKOS?? \n\t\t\tor rdflib.RDF, skos.SKOS#is this a subclass of\n\t\t\t\t\t\t\t\t\t   rdflib.RDF, skos.SKOS, skos.SCO\n\t\t\tor rdflib.RDF, rdflib.RDFS):\n\t\t# print g\n\t\td = set([p for q in range(0,len(g)) if g[q] == child])\n"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\treturn rdflib.RDFS.subClassOf(rdflib.RDF.BNode(), subject, predicate=predicate)\n\ndef get_roots_with_rdf_subject(graph, rdflib.RDF.BNode, rdflib.RDF.subClassOf, predicate=None):\n\t\"\"\"\n\t\n\t>>> get_roots_with_rdf_subject(graph, rdflib.RDFS.subClassOf(rnd.node(), rdflib.RDF.BFO), rd"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef __init__(self, graph):\n\t\tself.graph = graph\n\t\tself.roots = set()\n\t\n\tdef add_root(self, term):\n\t\t\"\"\"Add a Root of the graph.\n\t\t\n\t\t@return: Nothing\n\t\t\"\"\"\n\t\tif not isinstance(term, str):\n\t\t\traise IndexError, 'Unknown argument'\n\t\tself.roots.add(self.graph.predicate(term))\n\nclass FindRoots(object):\n\t\"\"\"A helper object to find roots of an rdfs:subClass\"\"\"\n\t\n"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t@staticmethod\n\tdef find_roots(graph, rdfsgraph):\n\t\t\"\"\"\n\t\tFind all roots of the given RDFS Graph\"\"\"\n\t\treturn set()\n\t\n\tdef compute_subclass_roots(self, node):\n\t\t\"\"\"\n\t\tIf the 'predicate' field of `node` is a subclass of the RDFS\n\t\tgraph, and the order of those nodes is the following:\n\t\t\n\t\t\tpredicate:SKOS.broader\n\t\t\t\n\t\tthen the RDFS graph containing the RDFS that node must have under\n\t\tthe R"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\treturn set( [ ( c.entityNumber() if hasAttrib( rdfs.subClassOf ) else rdf.entityNumber() )\n\t\t\t\t\tfor c in graph.objects(rdf.getContext(), predicate=rdf.SKOS.broader\n\t\t\t\t\t\t\t).listValues() ])\n\n###\n# This method does the same thing as get_roots_by_predicate but takes an RDFS name\n\n@type rdfs: rdflib.RDF \\\n\t\t  rdflib.RDFS_Class \\\n\t\t  rdflib.URI \\"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef __init__(self, graphs):\n\t\n\t\tself.graphs = graphs\n\t\n\t\t# The RDFS property names\n\t\tself.root = rdnf.RDFS.subClassOf\n\t\tself.broader = rdnf.RDFS.broader\n\t\n\t\t# Build a set of all the root in our graph\n\t\tself.roots = set([rdflib.URI(graph.getProperty(self.root)) for graph in graphs])\n\t\n\tdef solve(self):\n\t\n\t\t# Construct the search graph"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\treturn rdflib.ConjunctiveGraph(graph).find_roots(rdflib.RDFS.subClassOf, rdflib.SKOS.broader, rdflib.Literal(str(label)))\n"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef find_roots(self, dgraph):\n\t\treturn self._find_roots(None, dgraph) \n\t\n\tdef _find_roots(self, dgraph, dgraph_copy):\n\t\troots = set()\n\t\trv = set()\n\t\tfor n in dgraph.subjects():\n\t\t\tfor v in n.triples(():\n\t\t\t\t\t\t\n\t\t\t\tfor w in dgraph.objects(v):\n\t\t\t\t\tprops = self._prop_by_name(dgraph_copy, w)\n\t\t\t\t\tif ( (w, props) in roots):"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\treturn [o for o in graph.iter(RDFS.subClassOf)]\n\n# ---------------------------------------------------------------------\n#  @author jimmy\n#  @see    find_roots\n#  \n#  @param  graph RDFS.Graph\n#  \"\"\"\n#  Find all roots of the specified kind.\n#\n#  @param  kinds set of triples in the same RDFS.Graph\n#   @param  use default use flag for 'parent' - root, \n#   @param  default_result set of triples to return in this case"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tif isinstance(rdflib.RDFS.subClassOf, tuple):\n\t\tfor cl in rdflib.RDFS.subClassOf:\n\t\t\tfind_roots(graph, cl)\n\t\treturn\n\t\n\tr = set()\n\tfor c in rdflib.RDFS.subClassOf:\n\t\tfor p in rdflib.RDFS.subClassOf[c]:\n\t\t\tif isinstance(rdflib.RDFS.subClassOf[c], rdflib.RDFS.Class)"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef __init__(self, yaml_input=None):\n\t\tif yaml_input is None:\n\t\t\tself.input = raw_input\n\t\telse: self.input = yaml_input\n\tdef __repr__(self):\n\t\treturn self.input\n\tdef __str__(self):\n\t\treturn self.input\n\tdef _convert_bool(self, input):\n\t\tif isinstance(input, bool): return input\n\t\ttry:\n\t\t\tif input == \"True\" or input == \"t\": return True\n\t\t\tif input == \"false"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef __init__(self, value, default=None):\n\t\tif value is None:\n\t\t\tself.value = default\n\t\t\treturn\n\t\ttry:\n\t\t\tself.value = unicode(value)\n\t\texcept UnicodeDecodeError:\n\t\t\tassert( not value is bytes )\n\t\t\tself.value = default\n\t\t\treturn\n\t\telse:\n\t\t\tself.value = value\n\t@property\n\tdef value(self):\n\t\treturn self._value\n\n\t@value.setter\n\tdef value(self, value):\n\t\tself._value = value\n\tdef __re"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tvalue = yaml.load(yaml_data)\n\tif isinstance(value, unicode):\n\t\t# Dump to a py2-string\n\t\treturn value.decode('utf-8')\n\t# Dump only unicode\n\treturn value\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\t\n\tdef convert(self, value):\n\t\treturn str(value).decode('utf-8')\n\t\t\n\t\n\tdef convert_bytes(self, value):\n\t\tif value[:16] in bytes.decode('utf-8'):\n\t\t\treturn str(value[:16])\n\t\t\n\t\t\n\tdef convert_unicode(self, value):\n\t\tif value in bytes.decode('utf-8'):\n\t\t\treturn str(value[:32])\n\t\t\n\t\t\ndef to_json(obj, indent=None):\n\t"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tif py3:\n\t\tfile_name = file.read()\n\telse:\n\t\tfile_name = file\n\treturn file_name.decode(\"utf-8\")\n\ndef convert(file_name, path, encoding=\"utf-8\"):\n\t\"\"\"\n\tCreates a file using yaml\n\t\"\"\"\n\twith open(py2to3(path)) as infile:\n\t\tyaml_str = yaml.load(infile, Loader=yaml.FullLoader)\n\t\tinfile = yaml_str.get(\"data\", None)"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\treturn _dump_to_unicode(result, to_unicode)\n\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tpath = [] # save the list\n\tif not isinstance(path, list):\n\t\traise TypeError(\"path should be a list\")\n\n\t# write the list to a path\n\tpath.append((u'{{',u'}}')) # put an object separator here\n\n\t# read the list and put to a path\n\twhile (i < len(path)):\n\t\t(a, b) = listToString(path[i], u',')\n\t\tpath[i] = [a, b]\n\t\ti += 1\n\n\t# return a path\n\treturn"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tmyString = \"this is the magic string\"  # the input\n\n\n\tdef getString(self):\n\t\treturn myString\n\n\n\tdef __init__(self, obj, value='', default=[]):\n\t\t\"\"\"\n\n\t\tThis is a factory function for objects\n\n\t\tIt will create an empty object with a single value, and the given value if\n\t\tpassed (and not the default). It will raise a KeyError if the value is not\n\t\tdefined\n\n\t\tThe default is a list that contains the default value\n\n\t\tIf this object is used as default to an object with the key"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tunicode_esc = u''\n\tif sys.version_info < (3, 0):\n\t\tunicode_esc = 'py2-str'\n\telse:\n\t\tunicode_esc = 'py2-unicode'\n\n\t# dump everything\n\ttry:\n\t\twith open(name, 'w') as output:\n\t\t\tif sys.version_info >= (3, 0):\n\t\t\t\toutput.write(u'{}\\n'.format('\\n'.join(unicode_esc.format('\\\\ '+line) for line in dump)))\n\t\t\telse:"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef _dump(self, obj, indent=0):\n\t\ttry:\n\t\t\tout = '{}{}'.format(INDENT * indent, str(val))\n\t\t\tfor k, v in obj.items():\n\t\t\t\tout = out.format(k, v)\n\t\t\tout = out.rjust(len(out),'\\t')\n\t\t\tout = out.rpartition('\\n')\n\t\t\tout = out[0] + out[(len(out) - 1)].rpartition('\\n')[0]\n\t\texcept Exception as err:\n\t\t\treturn"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self, log_name, log_level = logging.INFO, log_type = None, log_filename = None):\n\t\tif os.path.isabs(log_name):\n\t\t\tself.log_filename = log_name\n\t\t\tlogging.basicConfig(filename=self.log_filename or 'log',level=log_level,format=\"%(asctime)s - %(msecs)03dms :: %(levelname)s: %(name)s(L: %(funcName)s) :: : %(message"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tLOGGER_FILE = os.path.expanduser(\"~/.logfile\")\n\tif not os.path.isfile(LOGGER_FILE):\n\t\tLOGGER_FILE = logging.FOLDER + \"/\" + logging.FOLDER\n\t\tos.makedirs(LOGGER_FILE)\n\t\twith open(LOGGER_FILE, 'w') as f:\n\t\t\t# Use the stdlogname only if it's available\n\t\t\tif os.path.isfile(os.name + '.stdlogname'):\n\t\t\t\tf.write(\"log_file"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\ttry:\n\t\t#print \"Loading the config...\"\n\t\tconfig_file = str(os.path.dirname(sys.argv[0])+'/'+filename).replace('\\\\',\"\\\\\\\\\")\n\t\tconfig = ConfigParser.CustomParser().read(config_file)\n\t\t\t\n\t\t#print config\n\t\t\n\t\t#print \"Creating a logger with \"+config[\"logfile\"]\n\t\tl = logging.getLogger(config[\"logger\"])\n\t\tl.setLevel(config[\"log.level\"])\n\n\t\tformatter = logging.Formatter(\"%(message"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self, name, debug = False, debug_level=(0, 1)[self.is_debug], enable=True):\n\t\tself.debug_level = self.debug_level\n\t\tself.pump = subprocess.Popen(\"pywps -t service_description -n %s run -D -i -S -d %s -U -p %s\" %\n\t\t\t\t\t\t\t\t\t(name, name, name),\n\t\t\t\t\t\t\t\t\tshell=True,\n\t\t\t\t\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\t\t\t\t\tstderr=open(os.devnull,"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self,app):\n\t\tlogging.basicConfig()\n\t\tself.app = app\n\t\tself.logger = logging.getLogger('Application')\n\n\n# -------------------------------------------------------------------------------------------\ndef logMessage (msg,level=logging.INFO):\n\tglobal _debug_enabled\n\tglobal _debug_file\n\tglobal _debug_level\n\tself = logging.getLogger('log_%s')\n\tlogMsg = '%s\\n'%(msg)\n\tdebugMsg = logging.StreamHandler().setLevel(level).format(log"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tclass DebugLogger(logging.Formatter):\n\t\t\"\"\" Formatting class for the logger (only one level)\n\t\t\"\"\"\n\n\t\tdef format(self, record):\n\t\t\tdata =  logging.Formatter.format(self,record)\n\t\t\tif self.stream:\n\t\t\t\trecord.msg = \"%s() - %s\" % (self.log_level,data)\n\t\t\treturn data\n"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tlogging.basicConfig(\n\t\tformat=\"%(levelname)s %(message)s\",\n\t\tlevel=logging.WARNING,\n\t\tstream=sys.stderr)\n\tlogging.getLogger('z3c').setLevel(logging.ERROR)\n\tlogger_test = logging.getLogger(log.name)\n\tlogger_test.setLevel(\"WARNING\")\n\nif len(sys.argv) > 1:\n\tif sys.argv[1]=='test':\n\t\tlogger_test.setLevel("}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef _debug(fmt, *args, **kwargs):\n\t\t\"\"\"\n\t\tGeneral purpose application logger\n\t\t\"\"\"\n\t\tif not kwargs:\n\t\t\tkwargs = {}\n\t\tlevel = kwargs.get('LOG_LEVEL', logging.INFO)\n\t\tif level < LOG.WARNING:\n\t\t\tkwargs['additivity'] = True\n\t\tformatter = logging.Formatter('%(asctime)s - %(threadName)s - %(message)s')\n\t\tformatter.setLevel(level)\n\t\tformatter.setFormatter("}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\t\tDefault constructor\n\t\t\"\"\"\t\n\t\tlogging.basicConfig(level=logging.DEBUG) # Debug\n\t\tverbose = False\n\t\tbackup_logger = \"\"\n\t\t\n\tdef __init_logger(self, file, mode=None, line=None):\n\t\t\"\"\" Set up logger that logs each logged item\t\"\"\"\n\t\tif mode is None:\n\t\t\tmode = \"debug\"\n\t\tif verbose:\n\t\t\tfile = \"/tmp/\" + file +\"/\" + mode +\"_out.txt\"\n"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self, path):\n\t\t\"\"\"\n\t\tConfiguration file used to control logger and path of the log files\n\t\t\"\"\"\n\t\tlogging.basicConfig()\n#\t\ttry:\n\t\t\tself.path = path\n#\t\t\tself.path = './' + self.path\n#\t\texcept Exception, e:\n#\t\t\tlogging.basicConfig(level=logging.WARNING, format='%(asctime)s: %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n\n#\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t# Create an array.\n\t# If dtype is not in list, set it to default \"int\" which will force casting\n\tretD = np.empty(shape, dtype=dtype)\n\t\n\t\n\t# Set the dtype\n\tretD.dtype = dtype\n\t\n\treturn retD\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tA = np.ones(shape)\n\tA_dtype = np.dtype(dtype)\n\treturn A.astype(np.char.aschar(A_dtype))\n\ndef random_arrays(shape, dtype):\n\t\"\"\"\n\tGenerate random arrays with given data-type.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t\tshape of the array to create\n\tdtype : `numpy.dtype`\n\t\tdtype of the array to create\n\t\"\"\"\n\treturn [np.random.rand(shape)."}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tshape = np.asarray(shape)\n\treturn np.zeros(shape, dtype=dtype).astype(dtype)\n\t\ndef get_array(dimensions, number_of_dimensions, element_number, elements_per_dimension):\n\t\"\"\"\n\tCreate array of given shape and dtype.\n\t\n\tParameters\n\t----------\n\tdimensions : tuple\n\t    Dimensions of the array to create\n\tnumber_of_dimensions : int\n\t    Number of dimension for each dimension of the array to create\n\telement_number : int"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tnew_array=np.empty(str(shape),dtype=dtype)\t\n\tnew_array.flat[:] = np.copy(self.array)\t\n\treturn new_array.astype(dtype)\n\ndef make_array(x_values):\n\t\"\"\"\n\tGenerates a synthetic array to generate a uniform array with shape equal to x_values.\n\t\n\tParameters\n\t----------\n\t\n\tx_values : 1d `numpy.ndarray`, 2d, 3d, or array-like\n\t\ta array of data points,"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.array(shape, np.ndarray(dtype))\n\ndef numpy2numpy_float32(x):\n\t\"\"\"\n\tConvert numpy ndarray to numpy array. Can get a new shape and dtype.\n\n\tParameters\n\t----------\n\tx : `numpy.ndarray`\n\t\"\"\"\n\treturn np.array(x)\n\ndef numpy2numpy_float16(x):\n    \"\"\"\n\tConvert numpy ndarray to numpy array. Can get a new shape and dtype.\n\n\tParameters\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif len(shape) == 1 and shape[0] > 1 and np.all(np.array(shape)):\n\t\tdtype = list(np.dtype(dtype)).pop(0)\n\telse:\n\t\tdtype = list(np.dtype([dtype]))\n\t\n\tif dtype not in np.array([float]):  # Convert to numpy array of dtype or raise NumPy TypeError\n\t\tnp.reconstruct(dtype, *shape, dtype=np.float64)\n\treturn np.array(dtype."}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n\n\ndef squeeze(arr):\n\t\"\"\"\n\tCreate a new 2d array with an additional dimension\n\t\n\tParameters\n\t----------\n\tarr : `np.ndarray`\n\t    `np.array` to expand.\n\t\"\"\"\n\treturn arr.squeeze()\n\n\ndef pad(arr, maxlen):\n\t\"\"\"\n\tPad the data to the given length.\n\t\n\tParameters\n\t----------\n\tarr : `np.ndarray`\n\tmaxlen :"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\treturn dType(np.random.rand(shape)).astype(dtype)"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif shape is None: return np.empty(dtype)\n\telse :\n\t\tdata_shape = make_shape(shape,dtype)\n\t\treturn np.zeros(shape + data_shape, dtype=dtype)\n\t\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\treturn data.take(shape, axis=axis, mode=mode, **kwargs)\n\ndef dask_to_matrix(data=None, mode='corner', fill_value=None):\n\t\"\"\" create a matrix from a list of arrays\n\n\t\tInput data is a list or tuple of data (see docstring for arrays)\n\n\t\tReturns a dask matrix of shape (n_items, n_cols)\n\t\twhere n_items is the number of items in the first dimension\n\t\tand n_cols the number of columns in the second dimension\n\n\t\tParameters"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tcovs = gaussians(20, 15, sigma=0.1, isize=0.5)\n\t# The shape of covs[i][:n_sensor].\n\tcov = covs[n_sensor1][n_sensor2].reshape(4,6)\n\tp = np.dot(cov,covs[n_sensor1][n_sensor2].T)\n\treturn p\n\n\ndef gaussians(N, N_sensor, sigma):\n\t\"\"\"Generators of uniform vari"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef __init__(self, center, sigma, number_pairs = None):\n\t\tif number_pairs is None:\n\t\t\tnumber_pairs = 10\n\t\tself.center = sigma * (center+(center[0]-self.center)) + center\n\t\tself.sigma = sigma\n\t\t\n\t'''\n\t@brief : This function calculates the gaussian function, based\n\t\t\ton the center and the center of gaussian with a 0.5 * sigma.\n\t'''\n\tdef calculate_gaussian(self, number_pairs):"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tsigma = 0.1\n\tsigma_centered = np.array([sigma, sigma])\n\tA = sigma_centered ** 0.5 + 0.5 * sigma_centered\n\tA_centered = A / np.sqrt(2 * np.pi) * np.eye(2)\n\n\tH_centered = np.dot(A_centered, A_centered)\n\tJH_dims = H_centered.shape[0]\n\n\tH_centered_new = np.dot(np.linalg.inv(H_centered)."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t# the default sigma (0.1) is from here:\n\n\tsigma = 0.1;\n\n\tx = np.array([6-np.random.random([10,1])*sigma])\n\ty = np.array([6])\n\n\ts = np.array([1]) # this is a sample\n\tdata = np.array([2])\n\tx_err = np.array([0.1])\n\ty_err = np.array([0.0])\n\tkde = np.array([1])\n\t\n\tprint(\"Ga"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef __init__(self, num_iter, l1_coeff=0.3, l1_coeff_std=0.15, l1_coeff_sigma=0.4):\n\t\tself.num_iter = num_iter\n\t\tself.l1_coeff = l1_coeff\n\t\tself.l1_coeffstd = (l1_coeff + l1_coeff_sigma) * 0.1\n\t\tself.l1_coeff_std = l1_coeff_std\n\t\tself.l1"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tX = np.zeros([100,100,1,1])\n\tfor x in range(0, 10000, 20):\n\t\tr = 1./np.sqrt(np.sum(np.log(y - X)))\n\t\tX = np.log(r) * np.exp(X) + np.random.rand(100).astype(double) + np.random.rand(100).astype(double)\n#\tprint(\"X:\", X)  # plot\n\t#plt.plot(np.log(y-X),np.exp"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tsigma_0 = 0.1\n\tp,a,b = np.polyfit(x,x,0.0)\n\tp = p.real + np.sqrt(a) + sigma_0\n\n\tp = np.polyval(p,x) / np.sqrt(np.sum((p-sigma_0*p)**2))\n\n\tx0 = np.linspace(x0_0,x0_1,100)\n\tc = 0.0\n\tx = np.append(x,x0)\n\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tprint(\"\\nGaussian-centered =\\t\", type(self).__name__)\n\tprint(\"\\n**Input Parameters (for Gaussian centered):\")\n\tprint(\"\\tmean:\", self.mean)\n\tprint(\"\\tstd:\", self.std)\n\tprint(\"\\tmean / std:\", self.mean / self.std)\n\tprint(\"\\tbeta:\", self.beta)\n\treturn self\n\n\n\tdef __init__(self, mean=1, std=0.01, beta=0.5, shape=()):"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tfig = plt.figure(figsize=(12,12))\n\t#plot=plt.plot(X)\n\tplot = plt.plot(X)\n\tplot.plot(X)\n\t\n\tplot.set_xlim(.1, 0.8)\n\tplot.set_ylim(.1, 6)\n\t\n\tplotscope=fig.canvas.canvas\n\tplotscope.set_visible(False)\n\t\n\tscopes=petscope.all_scopes\n\t\n\t#plot"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tz0 = np.arange(-5,5,0.1)\n\tz0 = z0 + rng.normal(0,0.5)\n\tz0 -= np.asarray(z0) + 0.3 * np.asarray(np.log(np.absolute(rng.normal(8,2,1))))\n\tz0 /= 2.0\n\tz0 += np.asarray(np.log(np.absolute(rng.normal(2,1,1)) - np.sigmoid(np.absolute(rng"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\t# log import\n\timport sys\n\t\n\t# log import\n\timport logging\n\t\n\tlog = logging.getLogger(logging.getLogger().name)\n\n\t# parse configuration filename\n\tcfg_filenames=[]\n\tfor f in cfg_filenames:\n\t\ttry:\n\t\t\t# try to load the configuration file\n\t\t\t# this should work if the cfg_filenames contains only a.ini file\n\t\t\tlog.info('Parsing config file: '+f)\n\t\t\tcfg=configparser.FullConfigParser("}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t# first, load the config by parsing all configurations\n\tlogging.disable(logging.CRITICAL)\n\tlogging.basicConfig(filename=configuration_filename, level=logging.INFO)\n\tresult = {}\n\tfor name, config in configuration.items():\n\t\tif not config:\n\t\t\t# no configuration found\n\t\t\tlogging.error(f\"No configuration found for {config}. Can't load {name}.\")\n\t\t\tlogging.info(\"Loading of {name} will now be aborted. \"\n\t\t\t\t\t  \"Please verify the above file's contents in directory"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\treturn map((\n\t\tgenerate_configuration,\n\t), map((\n\t\tparse,\n\t) + list({\n\t\tparse_configuration,\n\t})))()\n\n\ndef map(iterable):\n\t\"\"\"\n\t\tGiven an iterable, return the next two results.\n\t\t\n\t\tThis function is guaranteed to be safe and is able to handle multiple calls for same object, as long\n\t\tas there is no double-put. This is the most common use case. The following methods are available\n\t\tin this generator, as implemented for those objects:\n\t\t\n\t\t1. parse"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlog = logging.getLogger(\"dynamic_config_parser\")\n\tlog.setLevel(logging.INFO)\n\n\tallFiles = getFiles(configFileInputParser)\n\tfor i in range(len(allFiles)):\n\t\tlog.info(\"Reading config file {0} from {1}\".format(allFiles[i], allFiles[i]))\n\n\t\ttry:\n\t\t\tparsedConfig = configFileInputParser(allFiles[i], [])\n\n\t\t\t# TODO: do not overwrite config, if existing file is not in config file, keep the"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\tlogging.basicConfig()\n\t\n\tlogging.debug('Creating a configuration for file...')\n\tfor confFile in filenames:\n\t\t\n\t\tlogging.debug(f\"Reading config file {confFile[:25]}...\")\n\t\tconfiguration = {}\n\t\ttry:\n\t\t\tconfig = json.load(open(confFile, 'r'))\n\t\t\tlogging.debug(f\"Loading configuration from {confFile[:50]} into configuration\")\n\t\texcept IOError:\n\t\t\tlogging.info(\"Unable to open file or JSON format for"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogger = logging.Logger(\"fetchconfig\")\n\twith open(configfilepath, \"rb\") as file:\n\t\tconfig = ConfigParser.MixedConfigurationParser()\n\t\tconfig.readfp(file)\n\t\tresult = parse(config)\n\tlogger.exception(\"Could not parse the config file:\", configfilepath)\n\tlogger.output(sys.stderr, \"Configuration file not understood: {0}\".format(configfilepath))\n\treturn(result)\n\"\"\"\n\tGiven a parsed configuration, returns\n\ta dictionary of configuration fil"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t@staticmethod\n\tdef from_config_filenames(config_filenames) -> Tuple[Dict[str, Configuration], List[LogRecord]]:\n\t\t\"\"\"\n\t\t@type config_filenames: SEQUENCE[str]\n\t\t@type return: Tuple[Dict, List[LogRecord]]\n\t\t\"\"\"\n\t\tconfiguration, records = set(), []\n\t\tfor filename in config_filenames:\n\t\t\t_parse_config(filename, configuration, records)\n\t\treturn configuration, records\n\t# TODO (jn) if we can"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\tlogF = logging.getLogger('Configuration.load')\n\tlogF.disabled = True\n\tlogF.setLevel(logging.INFO)\n\tfor f in filenames:\n\t\tparsedF = parsers(f)\n\t\tfor err in parsedF.parse():\n\t\t\tlogF.error('Parse error', err)\n\t\t# load the configuration and log any parse error\n\t\ttry:\n\t\t\tvalid.validate(parsedF)\n\t\texcept Exception, e:\n\t\t\tlogF.error('Validation error -- %s',"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tfrom ConfigReader import *\n\tfrom Config import *\n\n\ttry:\n\t\tlog_parser = ConfigParser()\n\n\t\tparsed_configurations = {}\n\n\t\tfor filename in FilenameList(config_paths):\n\t\t\ttry:\n                with open(Filename(config_paths[filename]), encoding=\"utf-8\") as f:\n\t\t\t\t\twhile True:\n\t\t\t\t\t\tline = f.readline() \n\t\t\t\t\t\tif not line or line[0] == '#':\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\trecords = [sig.SplitText(f.readline()) for s in it"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tfrom datetime import datetime\n\t\n\tself.config_filename_list = filter(lambda fn: fn.endswith('.yaml'), self.config_files_list)\n\t\n\tlog_parser = logging.ConfigParser()\n\tlog_parser.configure(logging.config.dictConfig(\"logging.ini\")):\n\t\t\n\t\tself.log_filename = log_parser.get(\"Logging\", \"log_file\")\n\t\n\tself.log = logging.getLogger('config_parser')\n\t\n\tfor fname in self"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isstr(obj) == \"string\": return obj\n\n\n\tif isinstance(obj,str):\n\t\tb = open(obj,'rb').read()\n\t\tb = b[4:].strip()*b_len\n\t\tb = b.bytes\n\t\treturn b\n\t\t\n\telse:\n\t\tif isinstance(obj,unicode):\n\t\t\tb = object.__unicode__(obj).encode()\n\t\t\tb = b[4:].strip()*b_len\n\t\t\tb = b.bytes\n\t\t\treturn b\n\t\telse:\n\t\t\ttry:"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\traise ValueError('expected object of type bytes but got %s' % type(obj))\n\telse:\n\t\treturn obj\n#-------------------------------------------------------------------------\n\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif type(obj) == bytes:\n\t\treturn bytes(obj, 'utf-8')\n\tif type(obj) == unicode:\n\t\treturn obj.encode('utf-8')\n\traise TypeError\n\nclass BinaryTree(dag.NodeOperator):\n\t\"\"\"\n\tThis class implements a binary search tree as a DAG with each node \n\tas a successor.\n\tA node in the tree may be a node in the root or a child.\n\tThis class has two methods:\n\t- __init__ : Initializes\n\t- __iter__"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\ttry:\n\t\tbyte_buffer = [ obj[i:i+2].encode('utf-8') for i in range(0, len(obj), 2) ]\n\t\treturn bytes(byte_buffer)\n\texcept:\n\t\treturn None\n\ndef remove_bom(x):\n\tif x.startswith(\"\\xef\\xbb\\xbf\"):\n\t\treturn x\n\telse:\n\t\treturn b\"\\xef\\xbb\\xbf\" + x\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.encode('UTF-8')\n\treturn obj\n\ndef read_bytes_as_unicode(obj):\n\t\"\"\"\n\tReturns the raw bytes object of the specified object.\n\t\"\"\"\n\tif isinstance(obj, numpy.number):\n\t\treturn numpy.raw_buffer(obj, obj.dtype.itemsize)\n\telif isinstance(obj, str):\n\t\treturn obj.encode('UTF-8')\n\treturn obj\n\ndef to_bytes(obj):\n\t\"\"\""}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tb = Bytes([obj,])\n\treturn b\n\ndef str2bytes(obj):\n\t\"\"\"\n\t\tConverts a string, stored as utf-8 encoded bytes, to a byte object\n\t\tbytestream representing the bytes string.\n\t\"\"\"\n\tb = Bytes(obj)\n\treturn b.decode('utf-8')\n\ndef decode_float(b):\n\t\"\"\"\n\t\tConverts a byte-string object into a float number. \n\t\t\"\"\"\n\tn = math.trunc(float(int(b[0])))\n\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif type(obj)!=str:\n\t\treturn b''\n\tif not isinstance(obj, str):\n\t\treturn b''\n\treturn obj\n\n\ndef _convert2string( obj, encoding = BINARY_ENC or 'utf-8'):\n\t\"\"\"\n\tThis function returns the string (bytes object) to be\n\tconverted to the given unicode encoding\n\t\"\"\"\n\tif isinstance(obj, basestring):\n\t\tobj = obj.decode(encoding)\n\t\treturn _decode(obj, encoding=encoding)\n\tel"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tdynamic = (dtype == 'i1')\n\tarr_type = type(obj).__name__\n\treturn numpy.frombuffer(np.fromstring(obj, 'i'), (dynamic, arr_type), dtype=arr_type)\n\ndef string2float(obj, *dtype):\n\t\"\"\"\n\tReturns the bytes object corresponding to ``obj``\n\tusing the given dynamic or scalar values. \n\t\"\"\"\n\tarr_type = type(obj).__name__\n\treturn numpy.fromstring(np.array(obj,"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif is_string(obj):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn obj\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\ttry:\n\t\tstring = bytearray(bytearray.__tobytes__(obj))\n\texcept:\n\t\tstring = bytearray(obj)\n\treturn string\n\n# \n# Get the bytes object from the file-like object. \n# \ndef read_uint( file):\n\t\"\"\"\tGet the uint object from the file\"\"\"\n\tbyte = bytearray(bytearray.__tobytes__(file))\n\treturn int(byte[0], 16)\n"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be > 0 (negative)\"\n\t\t\t)\n\tticks = round(ticks * 1e6, 10)\n\treturn cls(*[ticks // ticks_per_unit for tzinfo in tz.tzname(None)])\n\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttz = tz or pytz.utc\n\t\n\t_minutes_from_ticks = min(ticks, 7)\n\tif _minutes_from_ticks >= 86400000000000:\n\t\traise ValueError(\"ticks must be less than 86400000000000\")\n\t\n\thour = tz.localize(datetime.datetime.now() + datetime.timedelta(minutes=ticks))\n\t_seconds_from_ticks = (hour / 86400000000000.0) * 3600\n\toffset = int(floor"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\treturn cls(ticks)\n\t\n\t\n\ndef from_utc_time(t, tz=None):\n\t\"\"\"\n\tCreate a Time from a timespec object.\n\t\n\t:param t: Timespec object\n\t\n\t:raises ValueError: if timespec is out of bounds\n\t    (0 <= timespec.microsecond < 9000000000)\n\t\"\"\"\n\t\n\treturn Time(from_ticks(t.nanosecond))\n\ndef from_iso8601_datetime_microsec(t):\n\t\"\"\""}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tticks -= 0.5\n\treturn cls(int((ticks * 1e9) / 86400000000000) if tz == None else tz.tz'UTC') if ticks < 0 else cls(ticks if tz!= None else ticks * 86400000000000)"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\tif not isinstance(ticks, int) or not (0 <= ticks < 2 * 86400000000000):\n\t\traise ValueError('ticks must be an int >= 0 and < 2 * 86400000000000')\n\t\t\n\tif tz:\n\t\ttz = datetime.timezone.utc if tz is None else tz\n\telse:\n\t\ttz = datetime.timezone.utc\n\t\n\t# Convert nanoseconds to ticks\n\treturn cls(int(ticks / 1e9), tz=tz)\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tdef is_none(value):\n\t\t\tvalue = (value == None or 1)\n\t\t\treturn str(value) in (\"None\", \"0\", 0)\n\n\tdef is_float(value):\n\t\t\treturn type(value) in (type(1.0), type(1.7495932))\n\n\tdef is_int(value):\n\t\t\treturn type(value) in (type(1), type(0))\n\n\tfor t in set(range(ticks)) - set((0, -1, -2, -3, -4, -5, -6,"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\t\n\t\n\tif not isinstance(ticks, (int, long)) and not isinstance(ticks, float):\n\t\traise ValueError(\"Ticks should be in seconds and nanoseconds, not %s\" % (ticks,))\n\t\n\tif ticks >= 0 and ticks <= 86400000000000:\n\t\treturn cls(int(round(ticks / 1000000)))\n\telse:\n\t\traise ValueError(\"Ticks should be in seconds and nanoseconds, not %s\" % (ticks"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tticks = int(ticks)\n\t\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be non-negative\")\n\t\n\tif ticks > 864000000000000:\n\t\traise ValueError(\"ticks must be less than or equal to 864000000000000\")\n\t\n\tdelta = ticks - (ticks % 864000000000000)\n\tif delta < 0:\n\t\traise ValueError(\"delta must be positive\")\n\telif delta == 0:\n\t\treturn cls(ticks)\n\t\n\ttz = cls(ticks"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\tif not isinstance(ticks, int):\n\t\traise ValueError(\"ticks must be an integer, not {0}\".format(type(ticks)))\n\t\n\tif tz is None:\n\t\ttz = datetime.now().tzinfo\n\tts = ticks * 1.0e-6 # ms to ns\n\t\n\ttb = \"\".join((str(int(time.mktime(dt.timetuple()))) + \".\") * 86400000000000)\n\tf = open(os.path.join(cls.dirname,"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"ticks invalid: {0:d}, must be greater than 0\".format(ticks))\n\t\n\tdelta = ticks // 86400000000 // tz\n\treturn cls(delta, tz=tz)\n"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\tif isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol_version must not be a tuple (%s)!\")\n\tprotocol_version = tuple(protocol_version)\n\t\n\tresult = {\n\t\t(version, type_name): cls for\n\t\tversion, type_name in _protocol_dict.items()\n\t}\n\t\n\tfor type_name, protocol_handlers in result.items():\n\t\tif protocol_version is not None:\n\t\t\tif protocol_version not in protocol_handlers.keys():"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = ()\n\tprotocol_handlers = collections.defaultdict(lambda:\n\t\tcollections.defaultdict(lambda: collections.defaultdict(float)))\n\tif protocol_version == tuple(protocol_version):\n\t\tprotocols = set()\n\telse:\n\t\tprotocols = set(protocol_versions(protocol_version, cls.protocol_handler)), {0}\n\tfor version in protocols:\n\t\tprotocol_handler = cls.protocol_handler(version=version)"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tret = dict()\n\tfor ver in protocol_version:\n\t\tret.update(cls.protocols)\n\n\t\tif ver is not None:\n\t\t\tret[ver] = cls.protocols[ver]\n\treturn ret\n\n\n# EOF  # START END\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\tif protocol_version is not None:\n\t\tprotocol_versions = (protocol_version,)\n\telse:\n\t\tprotocol_versions = (protocol.DEFAULT_POT_VERSIONS,)\n\t\t\n\tprotocol_version_to_handler = {v: handler\n\t\t\t\t\tfor v in protocol_versions if\n\t\t\t\t\tprotocol_version == v}\n\t\n\treturn protocol_version_to_handler\n\t\n\t\n\t\nif __name__ == '__main__':\n\n\t\n\timport random\n\timport inspect\n\timport itertools\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tfrom pybind11.core import get_handler\n\thandler = {}\n\tmajor_minor_tuple = protocol_version\n\tif major_minor_tuple is None:\n\t\tc = get_handler(cls.__name__, major=3, minor=4, level=3)\n\t\thandler[0] = c\n\t\tc.version = get_handler(\n\t\t\tmajor=get_handler(0, minor=3).major,\n\t\t\tminor=get_handler(0, minor=0).minor)\n\telse:\n\t\tver_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandler_dict = dict()\n\t\n\tif protocol_version is None:\n\t\tfor protocol_version in cls.supported_protocols:\n\t\t\thandler_dict[protocol_version] = cls\n\telse:\n\t\ttry:\n\t\t\tprotocol_version = tuple(protocol_version)\n\t\texcept TypeError:\n\t\t\traise TypeError(\"version argument must be a tuple of versions\")\n\t\tfor protocol_name in cls.supported_protocols:\n\t\t\tif protocol_version is protocol_name:\n\t\t\t\thandler_dict[protocol_version]"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol_version expected a tuple\")\n\t\tprotocol_version = tuple(protocol_version)\n\t\tif protocol_version not in Protocol._protocol_handlers:\n\t\t\traise TypeError(\"unknown protocol version %s\" % (protocol_version,))\n\t\treturn {\n\t\t\tProtocol_v: protocol_handlers[Protocol_v]\n\t\t\tfor Protocol_v in protocol_version\n\t\t}\n\treturn {}\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif not protocol_version:\n\t\treturn defaultdict(lambda: defaultdict(protocol_handlers))\n\n\tif len(protocol_version) == 2:\n\t\tprotocol_version = protocol_version + (None,)\n\telif len(protocol_version) == 1:\n\t\tprotocol_version = protocol_version + (0,)\n\n\treturn defaultdict(lambda: defaultdict(protocol_handlers))\n\n\ndef isBoltCompatible(protocol_version):\n\tfrom pbax.protocol import Bolt\n\n\treturn Bolt.VERSION in tuple"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tmin_version = protocol_version[0]\n\t\tmax_version = protocol_version[1]\n\telse:\n\t\tmin_version = 0\n\t\tmax_version = None\n\thandlers = defaultdict(dict)\n\t\n\tdef get_handler(protocol):\n\t\tif protocol < min_version:\n\t\t\treturn None\n\t\tif protocol > max_version:\n\t\t\traise TypeError(protocol)\n\t\telse:\n\t\t\treturn handlers[protocol]\n\t\t\n\tfor version in [ProtocolVersion.fromString(x"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None:\n\t\tif len(protocol_version)!= 2:\n\t\t\traise TypeError(\"protocol versions expected 2 elements\")\n\t\telse:\n\t\t\tprotocol_handler_version = protocol_version\n\telse:\n\t\tprotocol_handler_version = None\n\n\tprotocol_handler_class_map = {\n\t\t1: 'BoltProtocol1',\n\t\t2: 'BoltProtocol3', # version 1.1 with all handlers, but we still allow 2.2\n\t\t3: 'BoltProtocol4',\n\t\t5"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tdata = [int(values[k]) for k in set(from_raw_values.keys()) - set(values)] + \\\n\t\t['.']\n\treturn cls(list(map(lambda v, n: int(v), data, n)), *values)\n\ndef to_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a Python dict of raw bookmark values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: Dictionary of values"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\traw_bookmarks = values\n\traw_bookmarks = [str(x) for x in raw_bookmarks]\n\treturn cls(raw_bookmarks)\n\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tbm = cls()\n\tfor v in values:\n\t\ttry:\n\t\t\tbm += to_raw_value(v)\n\t\texcept Exception as e:\n\t\t\traise TypeError(\"Could not convert value %s to raw: %s\" % (v, str(e)))\n\treturn bm\n\nclass BookMarkArray(list):\n    \"\"\"\n    Extracts a list of Bookmarks, from a string of bookmark strings.\n    \"\"\"\n    def from_string(cls, values):\n        \"\"\"\n        Extracts Bookmarks, from BookmarkStr"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls({'id': values})\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif values is None:\n\t\treturn None\n\t\n\tif len(values) == 0:\n\t\treturn None\n\t\n\tvalue = values[0]\n\t\n\tif not value.isdigit():\n\t\treturn None\n\t\n\tb = Bookmarks()\n\tb.title = int(value)\n\tb.url = value\n\tb.marker_name = None\n\treturn b"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tb = cls(values)\n\tb._raw = values\n\treturn b\n\t\n\t\n\t# You can use this method to build a Bookmarks object.\n\t# If you just want to fill it up with values from a single line of raw values,\n\t# use from_raw\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn Bookmarks(cls(map(string.strip, values)))\n\nfrom_values = from_raw_values\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tvalues = map(_compat_decode_utf8, values)\n\treturn Bookmarks(iter(values))\n\t\nfrom_raw_values = classmethod(from_raw_values)\nraw_values = [dict(zip(bookmark_string.split(\"/\"), values)) for values in raw_values]\n\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tif not isinstance(values, Iterable):\n\t\tvalues = [values]\n\t\t\n\t# use unicode to reduce risk of encoding problems\n\tbookmarks = values\n\n\t# unescape\n\tunicode_bookmarks = unicode(bookmarks, 'utf-8')\n\treturn Bookmarks(unicode_bookmarks)\n\t\n\t\n\t\n\nclass Bookmark:\n\t\n\t\"\"\"\n\tA single bookmark object used to store data within Bookmarks.\n\t\n\tExample:\n\t\n\t>>> bookmarks.to_raw_values(\""}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tbookmarks = []\n\t\n\tfor v in values:\n\t\tbookmarks.append( Bookmark( v) )\n\t\t\n\treturn cls( bookmarks )\n"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif (not seq) or isinstance(seq, (str, basestring)):\n\t\tseq = (seq,)\t\t# if not a sequence, return a string\n\tif seq == ('',):\n\t\tseq = (re.compile(r'text'),)\t\t# if its a sequence of just text, return that\n\tif seq[0] == seq[1]:\n\t\tif len(seq)!= 2 or seq[0]!= seq[1]:\n\t\t\traise TypeError, \"bad type: %s!= %s\" % (seq, seq[1])\n\t\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tfrom itertools import repeat\n\n\tif hasattr(seq, \"__len__\"): \n\t\tseq = seq[:seq.__len__]\n\n\tif type(seq) == list:\n\t\tseq = []\n\t\tfor s in seq:\n\t\t\tseq_type = type(s)\n\t\t\tif seq_type == str or seq_type == unicode:\n\t\t\t\tprint(seq, seq_type)\n\telif type(seq) == int or type(seq) == long or type(seq) == float or type(seq) == bool:\n\t\tseq ="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None: bufsize = os.path.getsize(seq)\n\treturn seq, type(seq)\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = type(seq)\n\tif getattr(seq, \"__module__\", \"None\"):\n\t\treturn seq()\n\n\tif isinstance(seq, buffer):\n\t\t# buffer of sequences\n\t\treturn (seq,)\n\n\tlengths = bmap(int, [])\n\tlength = None\n\tif bufsize == None or isinstance(bufsize, int):\n\t\tfor i in range(len(seq)):\n\t\t\ts = _get(seq[i], type(seq[i]), lengths)\n\t\t\tif s:\n\t\t\t\tlength = i\n\t\t\t\tbreak"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tseq = seq.strip()\n\tif bufsize is None:\n\t\tbufsize = bufsize-1\n\tstart = 0\n\tif bufsize == 1:\n\t\tbufsize = 1\n\t\tend = len(seq)\n\telse:\n\t\tend = -1\n\twhile True:\n\t\ttry:\n\t\t\tseq, type = get_seq_without_unmatched_buffer(seq, start, bufsize)\n\t\t\treturn (seq, type)\n\t\texcept IndexError:\n\t\t\tif bufsize == 1:\n\t\t\t\tif start < len(seq) and seq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseqsize = len(seq)\n\tif not seqsize:\n\t\treturn (seq, type(seq, None))\n\telif hasattr(seq,'read'): # binary sequence (no need for type)\n\t\tif seqsize == 1:\n\t\t\tif 0 <= bufsize <= seqsize:\n\t\t\t\tsequence = seq.read(bufsize) # read one piece\n\t\t\t\treturn (sequence, type(sequence, None))\n\t\t\telse: # no data fits on read\n\t\t\t\treturn (seq, type(seq, None))\n\t\telse:\n\t\t\treturn struct.unpack('"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ts = seq\n\tif type(seq) == _Sequence:\n\t\tseq = list(seq)\n\treturn(s, s._type)\n\ndef _get_seq_with_args(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, args) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ts = seq\n\tif type(seq) == _Sequence:\n\t\tseq = list(seq)\n\treturn(s, seq["}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, type):\n\t\treturn seq, seq.__class__\n\tif not(isinstance(seq, M.SequenceType) and issubclass(seq, M.SequenceType)):\n\t\traise TypeError(\"Unexpected type: %s\" % (seq,))\n\n\tif not bufsize and hasattr(seq, '__len__'):\n\t\tfor i in range(len(seq)):\n\t\t\tyield seq[i], i\n\t\treturn\n\n\tmax_len = min(bufsize, seq.__len__())\n\tif"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tfrom string import digits\n\t\n\treturn seq(digits), type(seq)\n\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tsseq, _ = seq\n\n\tseqlen = len(seq)\n\tif bufsize is None:  # get bufsize from seqlen\n\t\tbufsize = seqlen + 3 # use 3 for type\n\n\tif sseq.__typ == \"seq\" and sseq.__len(seq) > 0:  # don't iterate over already-comparable sequence\n\t\ttype = sseq[:2]\n\telse:\n\t\ttype = sseq   # not iterable\n\n\tif type == \"seq\":\n\t\tval_seq = seq    # assume this sequence is a"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif recompute or other is None:\n\t\treturn super(Histogram, self).scale(recompute=recompute)\n\t\t\t\n\tif other!= float(0):\n\t\tfactor_sum = 10.**np.nan * self.scale((np.log10(other) % 10, 1))\n\t\tfactor_sum = factor_sum / self.scale()\n\t\tother = np.dot(factor_sum, other)\n\t\t\n\t\t# avoid division by zero, to preserve numerical stability\n\t\tscale = np.dot(self.scale(1, other), self."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other==None:\n\t\treturn self.scale(self.histogram()) if recompute==False else self.scale()\n\treturn self.scale(other) if recompute==False else self.scale()\n\nclass _HistogramItemBase(object):\n\t__slots__ = ('pos', 'width', 'height')\n\tdef __init__(self, width, offset_y):\n\t\tself.pos = len(self) * offset_y / abs(width)\n\nclass _HistogramItem(namedtuple('_HistogramItem', 'pos, width"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif recompute and re.match(r\"'[^\\'\"\\n\\t>]+?'?'?'?'?'?'?',?'??'?'?',?'??'?'??''?',?'??'?'?'?',?'??'?'?'?'?',?'??'?'?'?',?'??'?'?'?'?',?'??'?'?'?'?',?'??''?'?',?'??'??',?'??'?'?'?'?'?',?'??'?'?'?'?',?'??'?'?'?'?',?'??'?'?'?''?',?'?"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t# Check if self is an instance of LenaHistogram\n\t# TODO: remove this special case when LenaHistogram is the parent class\n\t# See for instance http://blog.nadeem.com/2014/04/06/bin-int-fraction-in-python/\n\tif (len(self._hist) == 1) and (isinstance(self._hist[0], LenaHistogram)):\n\t\t# noinspection PyPep8Naming\n\t\tother = float(other) if other is not None else self.get_scale().scal"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tdef scale_if_necessary(self, other=None):\n\t\tif self.scale == None:\n\t\t\tself._scale = other / float(self.num_points)\n\t\telse:\n\t\t\tself._scale = self.scale * other / float(self.num_points)\n\t\tif self.recompute:\n\t\t\tself.recompute()\n\t\treturn self\n\t\n\tdef __unicode__(self):\n\t\tif self.num_points == 1:\n\t\t\treturn '%2d' % self.scale\n\t\tif self.num_points == 2"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tother = self.scale()\n\t# If an estimate and a new scale is requested\n\t# then recompute the scale.\n\tif recompute:  # pragma: no cover\n\t\tif self.scale() == None:\n\t\t\traise ValueError('Can not rescale a histogram with a scale of zero yet.')\n\t\tself.scale = other\n\t\tself.__set_recursive(recompute=True)\n\t\treturn self.scale\n\t# Check if the scale is inversely proportional\n\t# to the original number of bins."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other == None:\n\t\tif not recompute:\n\t\t\treturn self._scale\n\t\telse: return\n\t\tif self._scale == 0:\n\t\t\traise RuntimeError(\"Histogram with zero scale cannot be rescaled\")\n\t\telif self._scale!= -1:\n\t\t\traise ValueError(\"Histogram with zero-based scale cannot be rescaled\")\n\t\telse:\n\t\t\treturn self\n\tif not isinstance(other, (int, float)): raise TypeError(\"Expecting int or float (got %r)\" % other)\n\tother = float(other)\n\tif"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tfrom lena.math2 import is_inf, is_nan\n\tfrom lena.tools import is_numeric\n\t\t\n\tif not is_numeric(other): \n\t            raise TypeError(\"%s: Histogram must be a numeric number\"%(other))\n\t\n\tif recompute:\n\t        h = self.histogram.copy()\n\t        return (is_inf(h[0].size),self)\n\t\n\tif other is None:\n\t\th = self.histogram\n\t\telse:\n\t\t\th = other.histogram"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif (recompute or\n\t    isinstance(other, (int, float))\n\t    or not isinstance(other, Histogram)):\n\t\traise TypeError(\"can't change scale with other\")\n\n\tif other and repopulate:\n\t\traise LenaValueError('unable to change scale')\n\t\n\tif current is not None:\n\t\t# TODO: it would be nice if this could be done under the\n\t\t#     same lock. However, LenaValueError still gets raised if a\n\t\t#     lock was hit during evaluation\n\t\n\t\tif"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tassert bool(other, None), (other, _, None) or (other, self) or (sum(self) % 1) > 0\n\tif not isinstance(other, int):\n\t\traise TypeError(\"cannot scale %s with %r\" % (type(other), other))\n\tif other is not None:\n\t\tif not isinstance(other, float):\n\t\t\traise TypeError(\"{} must be a float\".format(type(other)))\n\t\ts = self.scales[-1]\n\t\tself.scales[-2] = 0\n"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tif other:\n\t\tscale = other\n\t\txy = (float(scale) / np.sqrt(d) for d in (self.x, self.y, self.z, self.w))\n\t\tscale = scale / np.mean(xy)\n\t\txy = [xy]\n\t\n\tif self.is_zero():\n\t\tscale = other\n\t\n\telse:\n\t\tx = sum(xy[:-1]) / len(xy)\n\t\tx /= np.sqrt(np.multiply(x, x))\n\t\ty ="}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif self.x.type == \"Scales\":\n\t\t\tif self.y.type!= \"Scales\":\n\t\t\t\traise Errors.OrientValueError(\"Can only rescale a scale\")\n\t\t\tf = lambda a: self.y.scale(*(a.x.scale, *a.y.scale))\n\t\telse:\n\t\t\tif self.x.type == \"Scales\":\n\t\t\t\traise Errors.OrientValueError(\"Can not scale multiple x scales\")\n\t\t\tf = self.x.scale\n\t\tscale_func = lambda a: f"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tif other is None:\n\t\treturn self.scale()\n\t\n\tval = other\n\tif val!= float(val):\n\t\traise ValueError(\"Scale must be a float!\")\n\treturn val\n\t\n\t\ndef scale_value(val):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf a number is provided, rescale to that value.\n\t\n\tFor an error on value checking, the graph will be rescaled.\n\tThe exception will be ValueError.\n\t\"\"\"\n\tif val is None:\n\t\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tif hasattr(self.xData(),'__iter__'):\n\t\t#TODO: implement other possible case, e.g., if only 1 coordinate was provided to rescale\n\t\t\n\t\tretVal = self.xData()\n\telse:\n\t\txData = self.xData()\n\t\t\n\t\tretVal = np.zeros(len(xData))\n\t\tretVal[self.index(xData)] = None\n\t\txAxis = xData.data.coordinates.axes[0].axis\n\t\txScale = xAxis."}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None or other == 0.0:\n\t\treturn self.rescale()\n\treturn self.scale(other)\n\n##\n## Lena graph interface\n##\n\n#-----------------------------------------------------------------------------\n# Graph class\n#-----------------------------------------------------------------------------\n\ndef get_graph_impl(src):\n\t\"\"\"\n\t\tGet the ``G`` class of a graph.\n\n\t\tReturns: `LenaGraph` with graph's fields\n\t\"\"\"\n\ttry:\n\t\treturn src.G\n\texcept:\n\t\t##\n\t\t# if __debug__:\n\t\t#     raise UnusableGraphError"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t# TODO: this would be able to store a global scale\n\tif self.scale!= other:\n\t\tself.scale = scale(self.scale, other)\n\t\tself.data.scaled = True\n\t\treturn self\n\telse:\n\t\traise TypeError(\"Can not rescale the graph: scale keyword given, is %s, is not given?\" % (str(self.scale)))\n\nclass ScaleError(Exception):\n\t\"\"\"\n\t\tThe scale operation on a scale graph requires data rescaling.\n\t\t\n\t\tThis exception is raised by the :func:`"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tother = other if isinstance(other, float) else \\\n\t\t(float(other) if other in [None, 0.5] else other)\n\tscale = float(self.get_scale()) if scale else float(self.size)\n\treturn self.rescale(other / scale) if other else scale\n\t\n@property\ndef has(self):\n\t\"\"\"is it there?\"\"\"\n\ttry:\n\t\traise NotImplementedError\n\texcept NotImplementedError:\n\t\ttry:\n\t\t\traise NotImplementedError\n\t\texcept NotImple"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif isinstance(other, LenaValue):\n\t\treturn lambda v: self._scale * max(v, 0.0) / min(other, 0.0)\n\tif isinstance(other, tuple):\n\t\tif len(other) == 2:\n\t\t\tother2, other3 = other\n\t\telse:\n\t\t\tother2, other3 = other\n\t\tscale1 = max(self._scale, other2)\n\t\tscale2 = max(other2, self._scale)\n\t\tscale3 = max(other"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other:\n\t\tif isinstance(other, dict):\n\t\t\tsc = other\n\t\telse:\n\t\t\tsc = other.scale\n\t\tif sc:\n\t\t\tdummy = self.dummy\n\t\t\tdummy.scale = sc\n\t\t\treturn dummy\n\t\telse:\n\t\t\traise LenaError(\"cannot normalize %r into a dict.\" % sc)\n\t\t\n\td = self.dummy\n\td.scale = 0\n\treturn d\n\n\nclass ScaledDummyBoundingBox(object):\n\t\"\"\" \n\t\tA bounding box to be used by scaled"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.scale()\n\tif other is float:\n\t\treturn self.__rescale([(f, 1.0) for f in self.fields])\n\treturn self.__rescale([(f, other) for f in self.fields])\n\t\nclass _ScalarField(_Field):\n\tdef __init__(self, name, *args, **kargs):\n\t\t_Field.__init__(self, name, *args, **kargs)\n\t\tself.minval=min(args)\n\t\tself."}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t)\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\tdef _generate_histogram(self, histogram, bin_edges):\n\t\tcoordinate = self['get_coordinate']\n\t\t\n\t\thist_edges = [bin_edges[i % len(bin_edges)]\n\t\t                    for i in range(len(bin_edges))]\n\t\thist = [[self['make_value'](h) for h in hist_edges]\n\t\t                    for i, p in enumerate(histogram)]\n\t\thistogram = [self['make_value'](c) for c in hist["}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t)\n\treturn Histogram().histogram(hist, make_value=make_value, get_coordinate=get_coordinate, field_names=field_names, scale=scale, hist=hist)\n\t\n#---------------------------------------------------------------------------------------------------\nclass Errorbar(object):\n\tdef __init__(self, mean='x', mean_error='y', value_error='y', name='errorbar'):\n\t\t\"\"\"\n\t\tCreate a new instance of **Errorbar**.\n\t\t\"\"\"\n\t\tself.name = name\n\t\tself.mean = mean\n\t\tself.mean"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tself.graph = graph = Grapher(hist, make_value=make_value, get_coordinate=get_coordinate, field_names=field_names, scale=scale, hist=hist, make_value=make_value, hist_to_graph=hist_to_graph)\n\tself.graph.build_graph(graph, graph_coords=[], data_coords=[\"y\"])\n\treturn graph\n\n\ndef hist_to_image(hist, make_value=None, get_coordinate=\"left\", **kw): #\n\t\"\"\""}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\treturn hist.to_graph(make_value, get_coordinate, field_names=field_names, scale=scale)\n\ndef hist_get_value(hist, key, make_value):\n\t\"\"\"\n\tReturn histogram' value for a given point.\n\t\n\t*key* is a histogram's key\n\t\n\t*make_value* is a value generator.\n\tIt will be used to create the value for the given point.\n\tBy default it will create another value generator to use the bin content to create the value.\n\tYou can use:"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t)\n\t\t\n\tdef scatter_to_graph(scatter, make_value=None, histogram=None,\n\t\t\tget_coordinate=\"center\", field_names=None, scale=True):\n\t\t\t\"\"\"\"\"\"\n\t\t\n\t\t\tscatter = scatter.split()\n\t\t\tscatter = [i.split(\":\") for i in scatter]\n\t\t\t\n\t\t\ty = []\n\t\t\tx = []\n\t\t\tif histogram is not None:\n\t\t\t\t# if histogram is not a simple numeric binogram, we\n\t\t\t\t# use a histogram\n\t\t\t\tbins = histogram"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\treturn graph(hist, make_value=make_value, get_coordinate=get_coordinate,\n\t              field_names=field_names, scale=scale, hist=binned_hist,\n\t              interpolation=interp, mode=mode, units=units,\n\t              x=x, y=y)\n\t\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t)\n\t\n\t__all__ = [\n\t\t\"hist_to_graph\",\n\t\t\"histogram\",\n\t\t\"error_graph\"\n\t]\n\t\n\treturn hist_to_graph\n\t\t\n\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t)\n\ndef add_axis_kwarg(ax, **kwargs):\n\t\"\"\"\n\tAdds an axis kwargs to :ref:`axis` arguments.\n\t\n\tAn axis kwarg is also set on a :class:`.grid`,\n\t:class:`.grid`, :class:`.bar` or :class:`.bar_symbol` graph.\n\tBoth kwargs must be string objects. :class:`.axis`, :class:`.label` graph and graph\n\tnumbers may be given.\n\tA common way is to use just :class"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tfrom sklearn import metrics\n\n\treturn hist + metrics.make_auc(stats=metrics.auc(None, bin_c=hist)) + metrics.make_moments(axis=1, mean_vars=True, bin_c=hist)\n\ndef graph_from_values(hist=None, values=None, name=\"graph\", **kwds):\n\t\"\"\"\n\tCreate a graph from a set of values.\n\t\n\t*hist* (if specified) must contain only numeric bins.\n\t*values* can contains any value.\n\t\n\t*name"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Interface\n\tfrom zope.interface.verify import VerifyError, \\\n\t     VerifyMixin, verify_methods\n\t\n\tif vtype is not None and not isinstance(candidate, vtype):\n\t    raise VerifyError(\"%s for interface %s should be of type '%s' not '%s'\" % (candidate.__class__, candidate, vtype.__name__, vtype))\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\tif not (iface.providedBy(candidate) or candidate.providedBy(iface)):\n\t\tfmt = \"{'name': '{candidate.name}'}\"\n\t\tlog.debug(\"Candidate is not the one it claims to provide via 'providedBy' or in \"\n\t\t\t  \"(a) the class, or in (b) the interface. Reason: '{fmt}'.\", (candidate, iface))\n\t\treturn False\n\telif vtype is not None and isInstance(vtype, candidate.providedBy):\n\t\tfor att"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\tif not isinstance(candidate, zope.interface.Interface):\n\t\traise zope.interface.Invalid, zope.interface.Invalid.implementer(candidate)+\\\n\t\t\tstr(len(candidate.providedBy))\n\t\t\n\tverify_interface = True\n\t\n\tcandidate_implements = candidate.providedBy\n\tprovider_implements = candidate_implements\n\tif not candidate_implements and not provider_implements and not \\\n\t\tcandidate_implements:\n\t\treturn False\n\t\t\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tvtype = _get_vtype(vtype, candidate.providedBy.providedBy)\n\tif not iface.isSubsetEqual(candidate.providedType, candidate.providedBy.providedType):\n\t    return False\n\tif \"providedBy\" not in candidate.providedBy:\n\t    return # vtype already filled in by next pass, not one of the above\n\thas_provided_type = True\n\tif \"providedSubtype\" not in candidate.providedBy:\n            return has_provided_type # Not possible if there is no such known subtype"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tattrs = {}\n\tdef_implementers = set()\n\tfor c in reversed(candidate.implementers):\n\t\tmname = c.__name__\n\t\ttry:\n\t\t\tdef_implementer = class_defs.objects.get(\n\t\t\t\tclassname=mname,\n\t\t\t\tobjectclass=mname,\n\t\t\t\t_type=mname\n\t\t\t)\n\t\texcept ObjectDoesNotExist:\n\t\t\tcontinue\n\t\tif mname in (\"__qualname__\", \"__module__\") or mname == \"__name__\":\n\t\t\tdef_"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\tcandidateDeclaration = getattr(_zope.interface, iface.__name__, CandidateInterfaceImpl(iface))\n\tcandidate = candidateDeclaration(candidate)\n\t\n\tmethods = candidateSpec.methods\n\tattributes = candidateSpec.requiredAttributes\n\tmandatoryTentative = candidateSpec.mandatoryTentative\n\tmandatory = False\n\t\n\tmandatoryMethod = find(methods,\n\t\tlambda m: m._name__matches is not None and (\n\t\t\tm._name__matches == iface.__name"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import isInterface\n\tfrom zope.interface.tests import InterfaceFinderTest\n\tfrom zope.interface.testsuite import IUnavailableTestsuite\n\tfrom zope.interface.interfaces import IProvidedBy\n\t\n\tif isInterface(candidate):\n\t\ttestClass = IProvidedBy\n\telse:\n\t\ttestClass = IUnavailableTestsuite\n\t\n\tif isinstance(candidate, testClass):\n\t\traise NotImplementedError(\n\t\t\t'This test doesn\\'t support interfaces.  It can't"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\tiface = iface()\n\t\n\t# check interfaces\n\tfor method in _get_interface_methods(iface, 'doesProvides'):\n\t\tif iface.implements(method):\n\t\t\treturn True\n\t\n\t# check interfaces provided by *iface*\n\tprovidedBy = iface.implementedBy()\n\tfor method in _get_interface_methods(providedBy, 'implements'):\n\t\tif providedBy.implements(method):\n\t\t\treturn True\n\t\n\t# check methods\n\tfor method"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif _is_valid_provides_with_candidate(iface, candidate):\n\t\tok = True\n\telse:\n\t\tok = False\n\t\n\tif not ok:\n\t\treturn False\n\t\n\t# If not implemented, return early without any message or report\n\tif type(candidate) not in ['zope.interface.implementer', 'zope.interface.provider']:\n\t\treturn True\n\t\n\t# Make sure we have all the necessary (non-invalid) methods\n\tif not candidate.providedBy(iface) and not"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif iface.providedBy(candidate):\n\t\tif tentative:\n\t\t\treturn True\n\t\tret = zope.interface.Verifiier._verify_tentative(candidate)\n\t\tif not ret:\n\t\t\treturn False\n\t\treturn True\n\telse:\n\t\treturn zope.interface.Verifiier._verify_other(iface, candidate, tentative)\n"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\tdef _findMethodSignature(iface, method):\n\t\tsignature = method.implementation.method_signature\n\t\tif signature:\n\t\t\tif signature.returns: return self\n\t\treturn _findMethodSignature(iface.providedBy(method.implementation), method)\n\t\n\tresult = true\n\t\n\tfor method in candidate.__iter__():\n\t\tif not _findMethodSignature(iface, method):\n\t\t\tcandidate, _ = candidate._getProvides(method), candidate._getInvolves(method)\n\t\t\t\n\t\t\tif"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif tentative:\n\t\treturn iface.providedBy(candidate)\n\t\n\tdef check(meth):\n\t\tif (\n\t\t\tmeth.__name__ == candidate.__name__\n\t\t\tand meth.__module__ == candidate.__module__\n\t\t\tand (\n\t\t\t\t'providedBy' in meth.get__argspec__(None)\n\t\t\t\tor 'implements' not in meth.__get__\n\t\t\t\tor meth.implements(candidate)\n\t\t\t)\n\t\t):\n\t\t\treturn True\n\t\telif hasattr(tentative or candidate,"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\ttry:\n\t\ts = iface.providedBy(candidate)\n\t\tiface.providedBy(s)\n\texcept zope.interface.InterfaceRequiredError, reason:\n\t\traise InterfaceRequiredError, reason\n\telse:\n\t\treturn True\n\t\n\tif not hasattr(candidate, 'providedBy'):\n\t\tif not tentative:\n\t\t\traise AttributeError, 'Interface must provide a '+ifaceClassName\n\t\treturn\n\t\t\n\ttry:\n\t\ts = candidate.providedBy(iface, tentative=True)\n\t\tif not s:\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not candidate.providedBy(iface):\n\t\tif verbosity>=2:\n\t\t\tlog.msg('Object interface {} defines an interface which '\\\n\t\t\t\t'should not provide it'\\\n\t\t\t\t.format(iface.unqualified_name))\n\t\treturn False\n\n\treturn True\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef f(*args, **kw):\n\t\tpass\n\t\n\tinterface = iface.providedBy(iface, tentative=tentative)\n\t\n\tif not hasattr(candidate, \"providedBy\"):\n\t\traise InterfaceError(\"%s is not a candidate, or doesn't provide its interface\" % (candidate,))\n\t\n\t# Check whether I can define the interface, to the extent possible\n\tiface.declares(candidate):\n\t\tiface.requiresAll(iface)\n\t\n\t# Check the signatures\n\tgetattr(if"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif (hasattr(iface, \"providedBy\") and\n\t    iface.providedBy is not None):\n\t\thasAttr = lambda a: not a.hasAttr('providedBy')\n\telse:\n\t\thasAttr = lambda a: not a.hasAttr('providedBy')\n\tif (hasAttr('providedBy')) and (not hasAttr('implements')):\n\t\treturn False\n\tif (hasAttr('provides')) and (not hasAttr('implements')):\n\t\treturn False\n\tif'requires"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# First make sure the proposed interface claims it is what the candidate was\n\tif not candidate.providedBy(iface):\n\t\traise zope.interface.Invalid(candidate, \"this method claims to \"\n\t\t                              \"provide this interface.\")\n\t\n\t# Now make sure the expected interface claims to the underlying\n\t# interface (if present)\n\tif candidate.providedBy(iface.providedBy):\n\t\traise zope.interface.Invalid(candidate, \"this method claims to \"\n\t\t                               \"provide this interface.\")\n\t\n\t# Now make sure the"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not iface.providedBy: return True\n\t# Make sure the interface knows what it should provide...\n\tif iface.providedBy.providedBy(iface, checkSorting = False): return True\n\tmthdList = [m.__name__ for m in iface.adaptMethods(\"providedBy\", True) if m.implementedBy]\n\tif not set(mthdList).issuperset(iface.methodList): return False\n\t\n\tdef recurseMethods(mthd, parent):\n\t\tfct, name, desc"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tglobal LOGGER\n\t\n\ttry:\n\t\t\n\t\t# Check signatures at all levels.\n\t\tif not Candidate.verifySignatures(candidate, iface):\n\t\t\tLOGGER.error(\"Signature verification failed (missing or invalid).\", Candidate.signature)\n\t\ttry:\n\t\t\tassert(iface.providedBy(candidate))\n\t\texcept zope.interface.NotProvidedByError:\n\t\t\tLOGGER.debug(\"Signature verification failed (not found/invalid).\", Candidate.signature)\n\t\t\treturn False\n\t\t\n\t\t# Check that an interface was"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\ttry:\n\t    for method in iface.providedBy(candidate):\n\t        providedBy = method.__name__\n\t        if method.__name__ == 'onInterface' and (dynamictypes.valid.checkClassMethod(ifACE, method) or\n\t                                                  method.__name__ == 'providedBy'):\n\t            return True\n\t    for attribute in iface.providedBy(candidate):\n\t        providedBy = attribute.__name__\n\t        if attribute.__name__ == 'providedBy' and (dynamictypes."}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom inspect import wraps\n\treturn isinstance(iface, (types.ClassType, type)) and wraps(iface).match(candidate.__name__)\n\n\ndef isWrapper(module, name):\n\treturn module and isWrapper(iface=module[name[len(module.__name__)+len(module.__name__)-3:]],\n\t\ttype=iface=name[len(module.__name__)+len(module.__name__)-3:],\n\t\tisMeta=name.startswith(\"meta\")) and candidate.__module"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t# Verify that the interfaces are actually implemented by the class\n\tinterfaces = candidate.providedBy(iface)\n\tif not interfaces:\n\t\traise TypeError(\"iface '%s' has no interfaces in '%s'\" % (iface, candidate))\n\t# If the candidate has at least one interface, add it to 'tentative'.\n\tif not tentative and len(interfaces):\n\t\ttentative = interfaces[-1]\n\tif not tentative:\n\t\traise ValueError(\"%s has no interfaces in '%s' when its candidate doesn't\""}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tis_interfaces, interface = _is_interfaces(candidate)\n\tif interface:\n\t\treturn 'interfaces' in signature(iface)\n\t\t# this fails if the signature is wrong\n\tif is_interfaces:\n\t\treturn 'interfaces' in signature(tentative if tentative else candidate.c_object)\n\treturn None\n\n\ndef signature(iface, candidates=None):\n\t\"\"\"\n\tReturn the signature, depending on the result of ``verifyClass``.\n\t\"\"\"\n\tif iface is None:\n\t\treturn None\n\treturn"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not tentative and not iface.isValid(*iface._validInterfaces()):\n\t\treturn candidate and not iface.isAccept(*iface._validInterfaces()), candidate\n\n\tif candidate:\n\t\tcandidate = candidate.__class__(*candidate._validInterfaces())\n\n\tif iface._type_id!= candidate.type_id:\n\t\tif candidate and candidate.type_id:\n\t\t\tiface = iface.parent(candidate)\n\t\telse:\n\t\t\tassert False, \"Cannot get parent candidate after fail: %s\" % candidate\n\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tf = tentative? 'f' : 'F'\n\t\t\n\tassert candidate in iface.__dict__.keys(), \"not found in %r\" % (candidate, iface)\n\t\n\tassert type(candidate) == iface.__class__, \"%s not an %s\" % (iface, candidate)\n\n\tassert getattr(iface, candidate) is not None, \"no '%s' attr\" % candidate\n\n\ti = iface.__interfaceVersion__\n\t\t\n\t# verify that iface.__version__ was added in the"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tdef isInterfaceInIface():\n\t\t\"\"\"\n\t\tReturn True if the given interface is a candidate.\n\t\t\"\"\"\n\t\tclass_name = candidate[1]\n\t\tclass_name = class_name[2:(class_name[-1] == ':')]\n\t\treturn class_name in iface.__all__\n\t\n\tdef checkInterface(iface, candidate):\n\t\t\"\"\"\n\t\tHelper to verify that an interface really provides *candidate*.\n\t\t\"\"\"\n\t\tif isInterfaceInIface():\n\t\t\tfor i in range(len(candidate)):\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tcandidate = candidate.__qualname__\n\tcandidate = candidate.replace('.', '::')\n\tcandidate = candidate.replace(':', '::')\n\tif not candidate.startswith('_'):\n\t\tcandidate = candidate.replace('_', '::')\n\ttentative = (str(tentative) in candidate)\n\tcandidate = candidate[0] + candidate[1:]\n\tcandidate = candidate.lower()\n\tif tentative or candidate not in ifaces[0].__all__[0]:\n\t\treturn\n\tif"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ttry:\n\t\tifs = iface.__class__.__name__.lower()\n\t\treturn ifs in candidate.__class__.__name__.lower()\n\texcept AttributeError:\n\t\treturn False\n\nclass ModuleModule(Module):\n\t\"\"\"\n\tA module in a tree where every class implements a number of methods and \n\ttherefore a module is a base module.\n\t\n\tHere is how this module's interface look:\n\t\n\t\"\"\"\n\t__base__ = None\n\t\n\tdef __init__(root_module=None,"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not isinstance(iface, str):\n\t\treturn {'fail':1,'msg':'Type of argument required: a string'}.get(\n\t\t\t'fail',\n\t\t\t'Failed to correctly decode: type of argument'\n\t\t)\n\t\treturn {'fail':1,'msg':'The passed *iface* must be a string'}\n\tif not isinstance(candidate, str):\n\t\treturn {'fail':1,'msg':'The passed *candidate* must be a string'}\\\n\t\t      .get(\n\t\t\t'test',"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tcandidate.verify(iface)\n\tverify_if(iface, candidate, tentative=tentative)\n\tassert candidate.has_interface(iface), \"Iface mismatch: {}!= {}\".format(iface, candidate.ifaces)\n\tcandidate.verify(iface2)\n\tverify_if2(iface2, candidate, tentative=tentative)\n\tassert candidate.has_interface(iface2), \"Iface mismatch: {} --> {}; {}!= {}\".format(iface2, iface, if"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = bases[0]\n\tif hasattr(metaclass, '__bases__'): # we have explicit __metaclass__\n\t\t# we can assume that the correct __metaclass__ is specified or not\n\t\t# if we have no __metaclass__, we can decide it from 1+__metaclass__\n\t\t# (e.g. you have __bases__=() or __metaclass__==()\n\t\tif explicit_mc is None: # no explicit __metaclass__, use __metaclass__\n\t\t\tclass"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases:\n\t\treturn object\n\tbases = list(bases)\n\tmc = bases[0].__metaclass__\n\tif mc is None:\n\t\tif explicit_mc:\n\t\t\tmc = explicit_mc\n\t\telse:\n\t\t\tmc = None\n\tif mc is object:\n\t\treturn object\n\twhile bases:\n\t\tbases = list(bases[:-1])\n\t\ti = len(bases)\n\t\tbases.append(mc)\n\t\tif i == 3: # final 1+ is 3 bases\n\t\t\treturn tuple(b"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases: return _MCLASS\n\n\tfrom base import classmetaclass \n\tdef extractMetaClass(bases):\n\t\tif len(bases) <= 1:\n\t\t\treturn bases[0]\n\t\telse:\n\t\t\treturn bases[0] + classmetaclass(explicit_mc or bases[1].__class__)\n\n\tmeta_cls = bases[0].__class__\n\tif not explicit_mc:\n\t\t# Explicit metaclass, if present\n\t\textractMetaClass(bases)\n\t\treturn meta_cls\n\t#"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif metaclass is None:\n\t\tif explicit_mc:\n\t\t\tmetaclass = explicit_mc\n\t\telif bases:\n\t\t\tmetaclass = bases[0].__metaclass__\n\t\t\tif metaclass is not None:\n\t\t\t\tfor base in bases:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tmt = base.__metaclass__\n\t\t\t\t\texcept (AttributeError, TypeError):\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\tmetaclass = mt\n\treturn metaclass\n\ndef generateCode(className, metaclasses, explicit_mc):\n\t\"\"\"\n\tD"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = []\n\tfor sc in bases:\n\t\t# This needs to be fast -- it may be inlined.\n\t\tif sc == object:\n\t\t\t# It's a class.\n\t\t\t# This may or may not be the __metaclass__ -- just use the \"class Meta\"\n\t\t\t# as a guide for what this is.\n\t\t\t_tmp_class = (__name__, __version__, __title__)\n\t\telse:\n\t\t\t_tmp_class = (sc.__name__, sc.__version__, sc.__title__)\n\t\ttry:"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not explicit_mc:\n\t\texplicit_mc = 0\t\t\n\tif isinstance(bases, tuple):\n\t\tmetaclass = None\t\t\n\t\tfor index in range(len(bases)):\n\t\t\tbase = bases[index]\n\t\t\tif isinstance(base, MIXIN_CLASS):\t\t\n\t\t\t\tif not explicit_mc:\n#\t\t\t\t\traise Exception('cannot determine metaclass: %s %s' % (index, base))\n#\t\t\t\t\treturn None\n\t\t\t\telse:\n\t\t\t\t\tmetaclass = base.__class__\n\t\t\t\t\tbreak\n\t\telse"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmcs = ClassMeta(explicit_mc=explicit_mc)\n\t\n\tbases = bases[:] # listify so all parents can be compared\n\tbases = [mcs] + bases # add child\n\treturn mcs\n\n\n########################################################################\n# Create instances of inherited classes.\n########################################################################\n\ndef define_metaclass(parents):\n\t\"\"\"\n\t\tDefine a metaclass with all parents in list.\n\t\tReturns instance.\n\t\"\"\"\n\tif len(parents) == 0:\n\t\t# Use __all__ from metacls"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdef _new_metaclass():\n\t\tfrom weakref import WeakMethod\n\t\tglobal _metadata_repository\n\n\t\tdef _generator():\n\t\t\t\"\"\"\n\t\t\t\tGenerator function generator for creating new metrics\n\t\t\t\t\"\"\"\n\n\t\t\tdef _repository(*bases):\n\t\t\t\tif _metadata_repository is None:\n\t\t\t\t\t# We create a weak metaclass, so that it can be collected\n\t\t\t\t\t# in a weakref'd value, allowing for multiple weakrefs.\n\t\t\t\t\tbases = _sorted(bases)\n\t\t\t\tfor base in bases:\n\t\t\t\t\tif has"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdef_mc, mt_params = explicit_mc or '', {}\n\tif bases:\n\t\tlst = bases[:-1]\n\t\tif not explicit_mc:\n\t\t\treturn _determine_metaclass(lst, mt_params)\n\t\telse:\n\t\t\t#print \"Metaclass can be explicitly passed and is being passed in a list\", bases\n\t\t\treturn _determine_metaclass(lst, implicit_mc=explicit_mc, mt_params=mt_params)\n\telse:\n\t\treturn _determine_metac"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not any(i not in bases for i in ['__file__', '__path__']):\n\t\traise RuntimeError('class Bases is not defined correctly')\n\tfirst_bases = bases[0].__bases__[0]\n\tparents = bases[1:]\n\treturn first_bases.__name__.rpartition('.')[-1], tuple, explicit_mc, parents\n\n__all__ = [_metaclass_str]([\n\tbases[0].__name__,\n\tbases[1].__module__,\n"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self: del self[key]\n\tif key in self: return self[key]\n\telse: return default\n\nclass Bunch(dict):\n\t\"\"\"\n\tD.Bunch(k, d)\n\tD is a dictionary\n\t\"\"\"\n\tdef __init__(self, k=None, d=None):\n\t\tself.k = k\n\t\tself.d = d\n\t\t\n\tdef __getitem__(self, key):\n\t\ttry:\n\t\t\tif key in self: return self[key]\n\t\texcept:\n\t\t\traise Key"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\treturn self._get(key, default)\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tdel self[key]\n\texcept KeyError:\n\t\treturn default\n\telse:\n\t\treturn default\n\n\ndef popitem(self):\n\t\"\"\"popitem() -> (key,value)\",\n\treturns and removes the first (key,value) returned by a PopItem method,\n\tor raises KeyError\"\"\"\n\ttry:\n\t\treturn self.popitem()\n\texcept KeyError:\n\t\theapq.heappop(self)\n\t\traise\n\n\nclass Memoize(object):\n\t\"\"\"An object which, when called to get a"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif default is __marker:\n\t\treturn dict.__getitem__(self, key)\n\telse:\n\t\tdict.__getitem__(self, key)\n\t\t# update the dict for the missing value\n\t\treturn default\n\ndef put(self, key, value, replace=False):\n\t\"\"\"\n\tD.replace(k[,v]) -> None.  If replace=True, other dicts in D are replaced\n\tby value.\n\t\"\"\"\n\tif type(value) is dict:\n\t\tvalue = value.keys()\n\t#if replace"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tv = self.pop(key)\n\t\tif v is __marker:\n\t\t\traise KeyError(key)\n\t\treturn v\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\t\t\n\t\n\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key not in self:\n\t\treturn default\n\tv = self[key]\n\tdel self[key]\n\treturn v\n\t\n\nclass MarkedDict(dict):\n\t\"\"\"\n\tA dictionary that can have a marker key.\n\t\n\tKey can by used with the __setitem__ keyword.\n\t\"\"\"\n\tdef __init__(self, *args, **kw):\n\t\tsuper(MutableDict, self).__init__(*args, **kw)\n\t\tself.markers = set()\n\t\t\n\tdef delete(self, key"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tv = self[key]\n\tif v in __markers:\n\t\tdel self[key]\n\t\treturn __markers.pop(key)\n\telse:\n\t\treturn default\n\t\n\t\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tvalue = self.__dict.pop(key)\n\tif value is not None:\n\t\treturn value\n\telse:\n\t\tif key in self.__marker:\n\t\t\traise KeyError, \"%s not found\" % key\n\t\telse:\n\t\t\treturn default\n\treturn\n\n\nclass MapMeta(dict):\n\t\"\"\"\n\tMeta mapping for hashable dictionary-like.\n\t\"\"\"\n\n\tdef __setitem__(self, key, value):\n\t\tif isinstance(key, type):\n\t\t\traise TypeError\n\t\telif not isinstance(key, string_"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif default is __marker:\n\t\traise KeyError(\"pop with default argument\")\n\twith _dict_lock:\n\t\treturn dict.pop(key, default)\n\ndef popitem(self, last=False):\n\t\"\"\"\n\tD.popitem() -> (k, v), returns and removes the first (key, value) pair of all the elements in the D.\n\tIf optional last is True, the iterator protocol returns the pair with key item first.\n\t\"\"\"\n\tout = ()\n\twith _dict_lock:\n\t\tfor key in self:\n\t\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\tresult = self.__dict__[key]\n\t\tdel self.__dict__[key]\n\t\treturn result\n\tif default is __marker:\n\t\traise KeyError(key)\n\telse:\n\t\treturn default\n\n\n\n\n\nclass DictDict(_Dict):\n\t\"\"\"\n\tDICT.items() -> list of key-value pairs\n\tDICT.items({}) -> OrderedDict of all elements.\n\t\"\"\"\n\n\tdef __init__(self, seq):\n\t\tfor key, value"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twith self.map:\n\t\t# print \"searching {0}\".format(self.keys)\n\t\tif len(self.map) > 0:\n\t\t\tlow, high = divmod(self.items.count(None), 2)\n\t\t\t#print (\"low {0}\".format(low), \"high {1}\".format(high, self.items[self.keys[low:high]]))\n\t\t\tret = self.items[self.keys[low:high]]\n\t\t\tdel self.items[self.keys[low:high]]\n\t\t\tdel self.map["}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tk = self._top\n\tfreq = {}\n\tcur = k\n\twhile cur > self._min:\n\t\tcur = freq.pop(cur)\n\t\t\n\tself._top -= 1\n\t\n\treturn k, freq\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self.cnt <= 1:\n\t\treturn None\n\tind = self.cnt.popleft()\n\tif ind == 1 or ind == 6: return None\n\treturn itertools.islice(delx + itertools.dropwhile(lambda x: len(x) > ind or self.cnt.popleft()!= ind, itertools.dropwhile(lambda x: x.startswith(ind) or len(x) > ind, itertools.dropwhile(lambda x: x.startswith(ind) or len(x"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tl = [key for (key, value) in self]\n\tl.sort()\n\ti = l.index(key)\n\tdel self[i]\n\treturn key, value\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn list.popitem(self.pop, self)\n#----------------------------------------------------------------------\n\nclass Count:\n\t\"\"\"\n\tA Count is an object containing a key and a count value and a\n\tcount iterator. Each item in the iterator is the count of how many\n\tthings matched.\n\t\"\"\"\n\tdef __init__(self, key, count):\n\t\tself.key   = key\n\t\tself.count = count\n\t\tself.next  = self\n\t\t\n\tdef next(self):\n\t\t\"\"\"\n\t\t\tReduce the count.\n\t\t\"\"\"\n\t\tcount = self."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tresult = \"\"\n\tfor k in range(len(self.__dict__)):\n\t\tif self.__dict__[k].__callable__:\n\t\t\tresult = self.__dict__[k][0]\n\t\telse:\n\t\t\tresult = self.__dict__[k]\n\treturn result\n\n\ndef isEmpty(iterable):\n\tfor i in iterable:\n\t\tif i is None: break\n\telse: return True\n\treturn False\n\ndef findOne(iterable,func):\n\tfor i in iterable:\n\t\tif func("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn max((k, v) for k, v in sorted(self.items()) if k not in ('key', 'value'))\n\t\t\t\t\t# Remove the key-value pair\n\treturn\n\n\t\t\t\t\t# Return the number of times the value used as a key since\n\t\t\t\t\t# first.  This is the weight of the least-used version.\n\tkeys = self.popitem()\n\treturn self.push(keys[1], keys[0] is not None)\n\t\t\t# Increment the number of times this key was used before\n\t\t\treturn self.push(self.popitem()["}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tfreq = {}\n\tfor k, v in self.items():\n\t\tif v < self[k]:\n\t\t\tfreq[k] = v\n\treturn freq.get(\"(key, value)\", \"\")\n\n\n\nclass CQ():\n\n\tdef dft(self):\n\t\t\"\"\"\n\t\tCreate a new `CQ` containing only the elements in the data set in __str__ as\n\t\tdefined on `str` and return the `CQ`.\n\t\t\"\"\"\n\t\tcq = CQ()\n\t\tfor i in range(len(self.data)):\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tfor value, key in self.items():\n\t\tif key is None: raise KeyError(\"The key has been found\")\n\t\tif key not in self._stale: return key, value\n\treturn None,None\n\n\ndef has_iter(x): return x is not None and hasattr(x, \"__iter__\")\n\ndef repeat(x, n): return str(x) + (''* (n - len(x))) if has_iter(x) else x\n\n\ndef rehash(*args):\n\t\"\"\"Rearhashes map keys, retaining only the longest"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tcount = self.count()\n\tsz = self.count(True)\n\tsz = reduce(lambda x, y: max(x, y), sz) * len(sz) + count\n\tif sz < count:\n\t\treturn None, None\n\tcount = count - sz + 1\n\treturn (None, self.pop(count))"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpair = []\n\tit = iter(self)\n\twhile not it.next()[0] in pair:\n\t\tpair.append(it.next())\n\treturn pair.pop()\n\n\n\ndef sort_by_mtime():\n\t#TODO: fix to time.time() instead of time.ctime()\n\tfor value in iter_iter(*sorted(iterable))(iterable):\n\t\tyield value\n\ndef print_file(file, *args, sort = 1):\n\tif file:\n\t\tsorted_iter = sort_by"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self is None:\n\t\treturn\n\tself._item_cache.pop(self.max_len)\n\tif self._item_cache:\n\t\treturn self._item_cache.popitem(key=self.max_len)\n\telse:\n\t\treturn None\n\n\nclass Deque(object):\n\t\"\"\"\n\tDeque data structure (list of items).\n\n\tFor improved performance, list.append should be called instead of\n\tlist.extend.\n\t\"\"\"\n\tdef __init__(self, initial_capacity=128, max_len=128):"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tret = None\n\tself.__dict__.pop(self.__lastkey_)\n\tif self.__dict__ is None:\n\t\tret = None\n\telse:\n\t\tret = self.__dict__.pop(self.__lastkey_)\n\t\tif ret == None:\n\t\t\tret = \"None\"\n\treturn ret\ndef getdict(self):\n\t\"\"\"\n\tReturn and clear a dict object that points to this item.\n\t\"\"\"\n\treturn self.__dict__.copy()\n\ndef dump(*items):\n\t\"\"\"\n\tD"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n\tcache = self.cache.get(key)\n\n\tif cache is None:  \n\t\traise KeyError(key)\n\n\tdel self.cache[key]\n\n\treturn cache\n\nclass Set(dict):\n\n\tdef clear(self):\n\t\tsuper(Set, self).clear()\n\n\tdef __setitem__(self, key, value):\n\t\tself[key] = value\n\ndef getattr(obj, name, default=None):\n\ttry:\n\t\treturn getattr(obj, name)\n\texcept AttributeError:\n\t\treturn default\n\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self.last()\n\n\t\n\t\n\t\n\n\n\tdef set(self, key, value):\n\t\t\"\"\"\n\t\tSet the value of the key and return a boolean indicating success.\n\t\t\"\"\"\n\t\treturn self.put(key, value)\n\n\t\n\t\n\t\n\n\n\tdef get(self, key):\n\t\t\"\"\"\n\t\tGet the value for the key.\n\t\t\"\"\"\n\t\treturn self._call(_get, key)\n\n\t\n\t\n\t# noinspection PyMethodMayBeStatic\n\tdef __getstate__(self):\n\t\tstate = _C"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n\treturn self._next\n\t\n\t\n\t\n\t\n\nclass LeastRecentlyUsedLinkedList(sliced.LeastRecentlyUsedList):\n\t\"\"\"\n\tLeast Recently Used LinkedList\n\t\n\t\"\"\"\n\t\n\tdef insert(self, key, element):\n\t\t\"\"\"\n\t\tInsert item \n\t\t\"\"\"\n\t\t\n\t\ttry: \n\t\t\tself._next = (key, element)\n\t\t\treturn self\n\t\texcept IndexError: \n\t\t\t# key has already been in list\n\t\t\tpass\n\t\t\t\n\t\treturn  self\n\t\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\ti = 0\n\tpair = ()\n\twhile pair not in self:\n\t\ti += 1\n\t\tpair = self.popitem(i)\n\treturn pair\n\n\ndef sort_with_key_sort(list_v):\n\t\"\"\"\n\tSort the list `list_v`, and returns it as a sorted sequence after having\n\tall the sorted pairs in the list but before removing any pairs whose\n\tvalue is 1 more than the minimum value.\n\n\t:param list_v: list to be sorted\n\t:type list_v: list\n\n\t:returns: sorted list"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\ttry:\n\t\treturn self.backend.popitem()\n\texcept KeyError:\n\t\treturn None\n\n\t\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tr = self.head\n\twhile True:\n\t\tif r.left:\n\t\t\tr = r.left\n\t\t\tcontinue\n\t\tbreak\n\t\tr = r.val\n\n\tr.delete()\n\n\treturn r.key, r.val\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self[1]\n\ndef insert_at(self, pos):\n\t\"\"\"\n\tInsert a pair at pos. If pos is negative, at the end, insert new nodes and return the new node.\n\t\"\"\"\n\tif pos < 0:\n\t\tself.push(self.popitem())\n\tif pos > 0:\n\t\tself.insert(pos, self.popitem())\n\n\nclass LinkedDict(OrderedDict):\n\t\"\"\"\n\tLinkedDict will store all keys and values of a dict for the same key. Values\n\tare placed in"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tkey, val = self.rawitems.popitem()\n\tif not val:\n\t\treturn (key, None)\n\n\treturn (key, val)\t\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif self.counter < len(self.stack):\n\t\t(key, self.stack[self.counter]) = self.stack.pop(self.counter)\n\t\treturn (key, self.stack[-1][1])\n\telse:\n\t\treturn None\n\ndef stack_peek(self, i, default=None):\n\t\"\"\"\n\t\tGet the value of the most recently pushed value, i.e.:\n\n\t\t\t>>> stack_peek(0)(1)\n\t\t\t(1,'1')\n\n\t\tor, if default not set, pop off top.\n\n\t\tIf"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tself.value = self.value.__next__()\n\texcept:\n\t\tpass\n\n\treturn self.value\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif sys.maxsize <= self.len:\n\t\traise KeyError\n\tn = ord(self.pop)\n\tdel self[n]\n\treturn( (n, self.pop) )\n\ndef replace(self, key, value, last=False):\n\t'''\n\tAssociate a key value with the value most recently returned by\n\t`popitem()`, which may be returned from a `get()` call or a `set()`\n\tmethod as returned by `popitem()`.\n\n\tIf called after a `get()`, an empty :class:`"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif len(self) > 0:\n\t\treturn list(self)[0]\n\telse:\n\t\treturn None\n\n\t\nclass OrderedSet(set):\n\t\"\"\"Deduced set, used by set module.\"\"\"\n\tdef __init__(self):\n\t\tset.__init__(self)\n\tdef __contains__(self, value):\n\t\tif value in self:\n\t\t\traise KeyError('Key %r not in set' % value)\n\t\treturn value\n\tdef __new__(cls, value='', *keys, **kwargs):\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif self._last_key is not None:\n\t\tdel self._last_key\n\t\tdel self._last_value\n\treturn self._last_key, self._last_value\n\n\t\n#-----------------------------------------------------------------------------#\n# Generic CPP callback\n\nclass PythonCallback(object):\n\t__slots__()\n\n\tdef __init__(self, name=None, arg_types=None):\n\t\tif len(arg_types) == 0:\n\t\t\tself.arg_types = []\n\t\telse:\n\t\t\tself.arg_types = sorted(arg_types)\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif self.list is not None:\n\t\t# Delete the key, value pair.\n\t\titem = self.list[len(self.list)-1]\n\t\tdel self.list[len(self.list)-1]\n\t\tif self.list[0] == item:\n\t\t\tself.list[0] = None\n\t\treturn item\n\treturn None\n\n\nclass Queue(object):\n\t\"\"\"\n\tContains a list of (key, value). There may be more that one\n\tthan the number of the items in the list. To get the last\n\tmatch,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tfor i in reversed(list(enumerate(self))[1:]):\n\t\tif i[1]>=key:\n\t\t\treturn i[0]\n\tprint(\"key\", key)\n\treturn None\n\nclass OrderedSet(Set):\n\t\"\"\" Set class that keeps the key & value order.\n\t\"\"\"\n\tdef __init__(self, *args):\n\t\tsuper(OrderedSet, self).__init__(*args)\n\tdef __repr__(self):\n\t\ts = sorted([k for k in list(super(OrderedSet,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tself._stack.popitem(True)\t# If there is more than one value in the stack.\n\treturn self._stack[-1]\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self._map[-1]\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\tdef next(self):\n\"\"\"\n\tReturn the next value in the map.\n\t\n\tRaises an `NoSuchElementException` if the map doesn't have\n\tany values.\n\"\"\"\n\tif len(self._map) == 0:\n\t\traise NoSuchElementException, \"Map is empty\"\n\n\treturn self._map.pop()\n\t\n\t\n\t\n"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self.index >= len(self):\n\t\traise KeyError\n\titem = self.keys()[self.index]\n\tdel self[item[0]]\n\treturn item\n\n\nif __name__ == '__main__':\n\tfrom time import sleep\n\timport random\n\tdef test(it, test, **kwargs):\n\t\twhile not it.next():\n\t\t\tsleep(0.01)\n\t\tprint('%d = %s' % (test(it, it_iter.next), it))\n\tit = Enumerable('foo')\n\ttest('"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\titems = sorted(self.items())\n\trandom = random.choice(items)\n\treturn random\n\n\ndef select(lst):\n\t\"\"\"  Select a random item from lst.\n\t\"\"\"\n\treturn lst[random.randint(0, len(lst) - 1)]\n\n\ndef sort(lst):\n\t\"\"\" Sort each element in lst. \n\t\"\"\"\n\tif len(lst) <= 1:\n\t\treturn\n\tl = lst[0]\n\tp0 = l[length()[1]]\n\tk0 ="}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self.__len__() == 0:\n\t\traise keyError\n\treturn (self.key, dict.pop(self.key, None))\t\n\t\n\nclass KeyedSequence(object):\n\t\"\"\"\n\tA set of items that may be used in a ``Sequence'' by calling this class's\n\t``keys()`` method.\n\n\tEach item may either be a string, an integer, or another Sequence object \n\tcontaining that item.\n\t\"\"\"\n\t\n\tdef __init__(self, seq):\n\t\tself.key = seq\n\t\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tidx = pymemcache.get(key1, item_size=len(value1))\n\treturn key1, value1[idx:]\n\ndef replace(key1, value1, key2, value2):\n\t'''\n\t\treplace an existing value for key1.\n\t\tKey1 is a Python name or str.\n\t\tKey2 is a Python name or list of strings or int.\n\t'''\n\tidx = 0\n\tfor idx in range(len(key1)):\n\t\tif key1[idx] == '_':"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.__cache_[ self.__random.get() ] # random key\n\n\t\n\t\nclass Random(Key):\n\tdef __init__(self, max):\n\t\tself.__max = max\n\n\t@staticmethod\n\tdef get():\n\t\treturn Random(Random.get())\n\n\t@staticmethod\n\tdef get(max):\n\t\t\"\"\"\n\t\tGet a random key in the range [0, max).\n\t\t\"\"\"\n\t\treturn Random.get()\n\n\t@staticmethod\n\tdef get_random():\n\t\t\"\"\"\n\t\tGet a random key in the"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self.stopped: raise StopIteration\n\n\titem = self.current.copy()\n\tself.current = None\n\n\treturn item\n\ndef get(self):\n\t\"\"\"\n\tGet a sequence of elements from the iterator.\n\t\"\"\"\n\tif self.stopped: raise StopIteration\n\t\n\tif self.current is None:\n\t\tx = self.iter\n\t\ti = i.next\n\t\tfor x in self.iter:\n\t\t\tyield x\n\t\tself.current = y\n\treturn self.current\n\ndef clear(self):\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tself.popitem()\t# Unconditionally, even if the value isn't in the dict.\n\treturn\n\n\t# return a 'non-recursive' version of the original (key, value)\n\t# pair.\n\t# (this can be expensive to do every time we end up here.)\n\ttry:\n\t\treturn self.popitem()[0]\n\texcept (IndexError, KeyError):\n\t\treturn None\n\t\t\n\n\t\n\t\n\t\ndef sorteddict(self, d):\n\t\"\"\"\n\tReturn a sorted dictionary (by key) from self."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tp = self._pkeys.pop()\n\tself._keys.remove(p)\n\tself._values.remove(p)\n\treturn p, self._values[0]\n\n\nclass _SortedSet(Mapping):\n\t\"\"\"\n\t\tA class to store and access sorted sets.  This class provides\n\t\tmethods to obtain and remove values from sorted sets.\n\n\t\tExample:\n\t\t| type     | attributes (instance methods)\n\t\t+----------+-----------------------+\n\t\t| list     | {key: value,... }\n\t\t| set      | {key: value"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tcursor = self.cursor()\n\ttry:\n\t\treturn cursor.popitem()\n\texcept IndexError:\n\t\treturn None\n\nclass Cursor(object):\n\t\"\"\"\n\tThis object serves to expose the cursor to the database.\n\t\"\"\"\n\tdef appendl(self, item):\n\t\t\"\"\"\n\t\tAppend the given item to database.\n\t\t\"\"\"\n\t\treturn self.cursor().append(item)\n\n\tdef execute(self):\n\t\t\"\"\"\n\t\tExecute this cursor's query.\n\t\t\"\"\"\n\t\treturn self.cursor().execute()\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t#~ self.log(\"Remove and return a random pair of (%s, %s)\" % (key, value))\n\treturn self.random_pair(True)\n\n\ndef get_unique_values(data, key):\n\t\"\"\"\n\t\tThis function tries to find a unique set\n\t\tcontaining an element of the data.\n\t\tIf the key is not found in the data, 0 is\n\t\treturned.\n\t\"\"\"\n\tfor element in data:\n\t\tif key in element:\n\t\t\tyield element[key]\n"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.in_style_re, re.MULTILINE | re.IGNORECASE)\n\ndef _create_in_style(self) -> Pattern:\n\t\"\"\"\n\tCreate the in style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.in_style_re, re.MULTILINE | re.IGNORECASE, re.UNICODE)\n\ndef _create_orim_style("}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t# Parse the string into a match object.\n\tmatch_obj = re.match(self.in_style_pattern, self.string)\n\tif match_obj:\n\t\t# There was an in-style parameter match object:\n\t\t# Parse it and create the new regular expression.\n\t\treturn self.in_style_re.parse(self.string)\n\telse:\n\t\t# An in-style parameter has no match:\n\t\t# Just return the old regular expression.\n\t\treturn self.in_style_re\n\n\ndef _set_key_value("}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self._text, re.MULTILINE)\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn self._create_regex(\"^(%s)+$\" % self.args, re.UNICODE)\n\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\tres = re.compile(self.inp_param)\n\treturn res\n\t\n\t\nclass Parameter(BaseParameter):\n\t\"\"\"\n\tA parameter describing how data are written\n\t\n\t\tThe description of the parameter can be set in the :data:`~hv.io.Config.parameters` dictionary.\n\t\t:class:`Parameter` instances are also accessible through the :class:`hv.io.Output` with the following suffix:\n\t\t\t- :attr:`OutputType.stdout`: Writes the parameter to STDOUT.\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\treturn Pattern(re.compile(self.params['re'],\n\t\t\t\t\t\t\t  re.IGNORECASE | re.DOTALL | re.UNICODE),\n\t\t\t\t\t\t  self.params['prec'],\n\t\t\t\t\t\t  self.params['caps'], self.params['case']\n\t\t\t\t\t)\n\t\t\t\t\t\t\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_re = re.compile(\"\\[\\\\?[A-Z][a-zA-Z0-9]*\\]\")\n\treturn in_re\n\n\ndef _in_regex(str_type) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_re = re.compile(r'%s?' % str_type)\n\treturn in_re\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t# Create the text-matching regular expression\n\t# and compile it into a compiled regular expression: \n\tmatch_group = re.compile(self.text_match)\n\t\n\t# Add the regular expression to the compiled regular expression so that\n\t# it can be used later:\n\treturn match_regexp(self.text_match, match_group)\n\t\n# A :class:`Pattern` constructor is used for the :func:`~re.compile`,\n# :func:`~re.search <python-magic.re."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\treturn re.compile(self.input, 'i')\n\t\nif __name__ == '__main__':\n\tfrom dolfin.utils.console import Console\n\tfrom datetime import datetime\n\t\n\t# Add a new entry in the line edit text box\n\tdolfin.entry.line\n\t\n\t# Create a console interface (which allows for testing)\n\tconsole = Console()\n\t\n\t# Set the variable to use the default name\n\t# console.set_variable('in_var', '', console)\n\t\n\t#"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tif self.param_type == Type.RE_STRICT:\n\t\treturn Pattern(self.pattern)\n\telif self.par_type == Type.RE_BASIS:\n\t\treturn re.compile(tuple(_format(v) for v in self.pattern.pattern))\n\telif self.param_type == Type.RE_PYTHON:\n\t\treturn re.compile(tuple(_format(v) for v in self.pattern.pattern.split()))\n\telse:\n\t\tif self.pattern.pattern:\n\t\t\traise Type"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t# Get parameters for the converter\n\tself.defaults['_id'] = self.fields[_id].default\n\tself.defaults['_data'] = ''\n\tself.defaults['_name'] = ''\n\tself.defaults['_scheme'] = ''\n\tself.defaults['_type'] = ''\n\tself.defaults['_width'] = ''\n\tself.defaults['_height'] = ''\n\t\n\t# Create the converter\n\treturn _converting._Converter(**self.defaults)\n\t\t\n\t\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\tif is_number(self.converter) and not self.name in self.values_list():\n\t\tself.values_list().append(self.name)\n\t\treturn self\n\treturn super()._create_converter()\n\n_DEFAULT_CONVERTERS[str] = {\n\t\"float_converter\" : float_converter\n}\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\treturn _converting._Converter(_convertingParameterStyle,\n\t                                    _convertingTemplateStyle)"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.__class__._converter =  _create_converter(self)\n\treturn self.__class__._converter\n\t\n\t\n__all__ = ['_create_converter']"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconv = _conversion.Converter()\n\t\n\tself._converters.prepend(conv)\n\treturn conv\n\t\n\t\n\t\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self._converter.converter\n\t\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self._parameter_style_converter\n\n### Exporting module\n\ndef _export(self):\n\t\"\"\"\n\tExport the parameter style converter to a format suitable for printing.\n\t\n\t\"\"\"\n\n\tfrom.parameters import _ParamConverterConverter\n\t\n\tfrom..exceptions import (\n\t\tParamConverterFormatError\n\t)\n\tfrom..io import (\n\t\tfrom_csv,\n\t\tto_csv,\n\t\tto_xlsx,\n\t\tfrom_xls,\n\t\tfrom_xls_csv,\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\timport paramstyle_converter\n\treturn paramstyle_converter.ParameterStyleConverter()\n\nclass _config(pymbal.Plugin):\n\tdef config(self):\n\t\treturn Configuration()\n\nConfiguration = _config()\nConfParser = Configuration.parser\n\nclass _Converter:\n\tdef __init__(self):\n\t\tself._conversion = []\n\t\n\tdef add(self, value):\n\t\tself._conversion = self._conversion + [value]\n\t\n\tdef startConvert(self):\n\t\tpass\n\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tparams = self._params\n\treturn ParameterStyleConverter(params.get('width'))\n        \nclass _converting._Converter(BaseParameterStyleConverter):\n\n    def __init__(self, parameter):\n\tself.parameter = parameter\n        \n\tself.parameter._check_style()\n\tBaseParameterStyleConverter.__init__(self, parameter.get_width())\n\n    def check_style(self):\n\tBaseParameterStyleConverter.check_style(self)\n        \n    def convert(self, c"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t_class: _converting._Converter\n\t\n\tdef __init__(self, params, namespace):\n\t\tglobal _converting\n\t\t\n\t\t# Create the converter.\n\t\t_converters = getattr(params, \"__converters__\", {})\n\t\t_converter = _converting(params, namespace, _converters, params.__param_values__)\n\t\tself.__class__ = _converter\n\t\t\n\t\t# Record that the parameter converter was created.\n\t\t_converter.__class__ = type(self"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tdt = datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%S\")\n\treturn dt.replace(tzinfo=dateutil.tz.tzutc())\n\n# Python\n# from datetime import datetime\nfrom datetime import timedelta\ndatetime.timedelta = timedelta\n#from timeutil import timedelta as time_utils\nimport time\n\n# pylint: disable=E1101,E1103,C0323,E112"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\treturn _datetime.datetime.parse(dt_str)\n\texcept _datetime.parser._ParseError as e:\n\t\te.type = 'datetime.date'\n\t\te.subtype = 'time.time'\n\t\traise e\n"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\ttry:\n\t\tx = _parse_dtstr(dt_str)\n\t\tif x:\n\t\t\treturn datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n\t\telse: # fail! invalid!\n\t\t\treturn \"\"\n\texcept ValueError:\n\t\treturn _dtstr_error(dt_str)\n\t\n\t\n\t\ndef _parse_dtstr(dtstr):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a datetime with time information.\n\t\n\t"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\tif not isinstance(dt_str, basestring):\n\t\tdt_str = dt_str.string\n\t\n\tif not isinstance(dt_str, (str, bytes)):\n\t\t# parse is assuming a string or object\n\t\treturn ISO8601.parse(dt_str)\n\t\t\n\t\n\tpos = dt_str.find(\" \")\n\t\n\tdt_min_year = None\n\tdt_min_year_replaced = \"T\".join((dt_str[0:2], dt_str[5]"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt_string = dt_str if isinstance(dt_str, str) else dt_str.__str__()\n\t_parse = self._parse_parser(dt_string)\n\t_parse_string = _parsed_str = dt_string\n\t_parse_tz = _parse.modes['UTC']\n\ttry:\n\t\ttzoffset = dt_string[:8]\n\texcept IndexError:\n\t\tpass\n\telse:\n\t\t_parsed_str = dt_string[8:]\n\treturn _parse("}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\treturn dateutil.parser.parse(dt_str)\n\texcept ValueError:\n\t\treturn None\n\t\n\t"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn parseIso(dt_str)\n"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t# Default behavior: the default parsing is to parse the longest date\n\t# string into a :class:`datetime.datetime`, which is simply the longest\n\t# part of a date string.\n\treturn dateutil.parser.parse(dt_str, isInstant=False).isoformat()\n\nfrom.. import dateutil\n"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\tif isinstance(dt_str, basestring):\n\t\tdt_str = dt_str.strip() if dt_str!= \"\" else None\n\t\t\n\t\tif 't' in dt_str:\n\t\t\tparts = dt_str.split('t')\n\t\t\tformat_str = None\n\n\t\t\tfor i in range(1, len(parts)):\n\t\t\t\tformat_str = parts[i].lower()\n\t\t\t\t\n\t\t\ttry:\n\t\t\t\ty, t = dtstr_to_dtstr(format_str)\n\t\t\texcept ValueError, e"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tif isinstance(dt_str, basestring):\n\t\tdt_str = datetime.strptime(dt_str.strip(), \"%a %b %d %H:%M:%S %Y\")\n\telif isinstance(dt_str, (datetime.time, datetime.date)):\n\t\tdt_str += timedelta(minutes=0)\n\telse:\n\t\treturn dt_str\n\t\t\n\tdt = datetime.datetime.fromtimestamp(toordinal(dt_str))\n\tdt = dt.replace"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\ttry:\n\t    return self._parse(timestr)\n\texcept TypeError:\n\t    try:\n\t        return self._parse(timestr, ignoretz,tzinfos, **kwargs)\n\t    except TypeError,e:\n\t        #print e\n\t        raise ParserError(e)\n\nparser = parser_t.copy()\n        #parser.date_object_map = {}\n#parser.time_object_map = {}\nparser.fuzzy_with_tokens = False\n#parser.f"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\ttimestr = Timestamp(timestr)\n\tif timestr.tzinfo is None:\n\t    return _parse(tp, timestr, *kwargs)\n\telse:\n\t    return fuzzy_from_tzinfo_or_default(default, timestr.tzinfo,\n\t\t\t\t\t\tkwargs, ignoretz=ignoretz)\n\t\n# The following are used by parsing()\n_ZALICONDITION_DICT = {\n    'UT' : '-00',\n    'UTC' : 'UTC',\n    'US' : '"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tif timestr is None:\n\t\ttimestr = default\n\tif ignoretz:\n\t\twarn(\"``ignoretz`` will be ignored if the input is a string, a \"\n\t\t     \"timespec and a tzinfo object.\")\n\t\tignoretz = False\n\ttry:\n\t\tdate, times, tzinfos = _parse(timestr, default=default, fuzzy_with_tokens=ignoretz)\n\texcept StopIteration as error:\n\t\traise ParserError(error)\n\t\n\tif times is not None and len(times"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\timport re\n\timport datetime\n\t\n\tparseresult = [None, None]  # empty lists\n\tif timestr:\n\t\tres = parse(timestr, *default)\n\t\tparseresult = [res[0], res[1]]\n\tif timestr.startswith(\"~\"):   #~ is a synonym for UTC\n\t\ttimestr = datetime.datetime(*default.timetuple())\n\tparseresult = [timestr, parseresult]\n\tparseresult = [None] + parseresult[1:]\n\t\n"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdt = _parse(timestr, ignoretz, default, fuzzy_with_tokens)\n\tif not isinstance(dt, (datetime.datetime, datetime.date, datetime.timedelta)):\n\t\traise TypeError('Unrecognized datetime type: %s' %\n\t\t\t\t\t\t  type(dt).__name__)\n\treturn dt\n\t\t\ndef _parse(timestr, ignoretz, default, fuzzy_with_tokens):\n\t\"\"\"\n\tParse a string or an input class (string stream) into a\n"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdatestr = timestr\n\tin_dst = self.fuzzily_get(tstmp(default) | tstmp(timestr),\n\t    dstoffset=int(self.use_dst), tzinfos=tzinfos)\n\tif ignoretz:\n\t    in_dst = None\n\tif not in_dst:\n\t    if default is None:\n\t        default = datetime.datetime(1900, 1, 1)\n\t    if tzinfos:\n\t        tzname = tz"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\ttry: return self._parse(timestr, **kwargs)\n\texcept ParserError as e:\n\t    raise ParserError(e)\n\texcept TypeError as e:\n\t    raise TypeError(\"Invalid date string in time string: \" + str(e))\n\t\n\t\n\t\n"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tresult = _parse(timestr, default=default, **kwargs)\n\tif ignoretz is not True and result is not None:\n\t\tif not result.tzinfo or not result.tzinfo.utcoffset(None) or not result.tzinfo.tzname(None):\n\t\t\tresult = result._replace(tzinfo=result.tzinfo)\n\treturn result\n\t\ndef _parse(timestr, default=None, ignoretz=True, fuzzy_with_tokens=True, **kwargs):\n\t\"\"\"\n\tParse the string"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t)\n\tdef parse(self, timestr, **kwargs):\n\t    \"\"\"\n\t    Parse the time string into a `datetime` instance.\n\t    \n\t    :param timestr:\n\t        A string or a list containing the date and time string.\n\t    \n\t    :return:\n\t        A boolean indicating whether time parsing failed, and if parsing\n\t        returned a datetime object.\n\t    \"\"\"\n\t    try:\n\t        return self._parse(stype=kwargs.pop(\"stype\", 'datetime'),"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\treturn parse(timestr, default=default, ignoretz=ignoretz, **kwargs)\n\t\n)\n\n# Add it to the default parser\n# In a future release, this has been deprecated, make sure you\n# always remove it!\nparser.parser.fuzzy_with_tokens = False\n\n\ntry:\n\tif parser.parser.fuzzy_with_tokens:\n\t\tparser._parse = parse\n\telse:\n\t\t# TODO: We're not using fuzzy parsing in the future, is this a good\n\t\t# choice"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttimestamp   = dt.timestamp()\n\ttz_delta    = 0.0\n\tfor tz in ['UTC', 'UTC+0100']:\n\t\t\n\t\tdt = tz_util.fromutc(dt)\n\t\tif timestamp < dt.timestamp():\n\t\t\tbreak\n\t\tfor zone in six.get_unused_numeric_zoneinfo():\n\t\t\t# Note: If the timestamp is in the future, it will be\n\t\t\t# treated as ambiguous and therefore won't be\n\t\t\t# returned as is.\n\t\t\t# In order to calculate an accurate"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Make sure dt isn't the start time\n\tif (dt.hour, dt.minute, dt.second) == (0, 0, 0) and dt.tzinfo is not None:\n\t\tdt = dt.replace(tzinfo=None)\n\tt = dt.astimezone(tz=dt.tz)\n\treturn dt.replace(tzinfo=None, utcoffset=None)\n\t\n\t\n\t\n##\tclass _date_of_time_zone:\n##\t\t'''A class representing timezone-aware"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\timport time\n\t\n\t# If dt is a timezone-aware :class:`datetime.datetime`,\n\t# do the calculation and cache the result.\n\tif isinstance(dt, datetime.datetime):\n\t\tdt = dt.to_pytz(timezone=self.tz)\n\t\t\n\treturn dt\n"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\treturn utc(dt.date(), dt.time())\n\t\n\t\ndef utc(year,m,d,h=0,m = 0,s=0):\n\t\"\"\"\n\t\tReturn a UTC datetime string for a\n\t\tdatetime with a given date, time and zone.\n\t\"\"\"\n\tif not year and not m and not d:\n\t\treturn datetime.datetime.utcnow()\n\tutc = datetime.datetime(year,m,d,h,s) \n\tutc_str = utc.utct"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None: return None\n\n\tz = dt.tzinfo.utcoffset(dt)\n\treturn z.replace(hour=0, minute=0, second=0,\n\t\t\t\t microsecond=0)\n\n\n#=============================================================================\n# helpers\n#=============================================================================\n\ndef parseTagPart(tagpart):\n\t\"\"\"\n\tParses a time period tag.\n\t\n\t>>> parseTagPart('SIXTY SECONDS')\n\t0\n\t>>> parseTagPart('TWO MINUTES')\n\t3600\n"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt is not None and isinstance(dt, datetime.datetime):\n\t\treturn utcfromutc(dt)\n\t\n\treturn None\n\n\t\ndef parse_local_timezone(str):\n\tparsed = localtime(str)\n\tdelta = parsed[4]\n\tparsed = utcfromutc(parsed)\n\treturn delta + timedelta(delta, 0, 0)\n"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif self._first is None:\n\t\t# The first occurrence of this date is ambiguous.\n\t\t# This is not ambiguous, so a non-ambiguous one will still be ambiguous\n\t\t# on the other side. Thus let's assume that the ambiguous dates are equal.\n\t\t# We'll return the first occurrence of that date.\n\t\t#\n\t\t# Note that ambiguous date times occur before, so if we only passed\n\t\t# them the start of the timezone but there were ambiguous date times\n\t\t# within it, this could give the ambiguious date time.\n\t\treturn dt\t#"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttry:\n\t\tdiff = (dt - dt.astimezone(self.tz))\n\texcept AttributeError:\n\t\traise ValueError(\"datetimes must be in a datetime library.\")\n\tdiff.microsecond *= -1\n\t\n\treturn diff.astimezone(self.tz)\n\nclass Ambiguous(Exception):\n\tdef __init__(self, ambiguous_dt):\n\t\tself.ambiguous_dt = ambig_dt\n\t\n\tdef get_ambiguous_dt(self):\n\t\treturn self.ambiguous_dt\n"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise TypeError(\"dt not a timezone aware datetime\")\n\treturn dt.replace(tzinfo=dt.tzinfo)\n\t\n\t\n\t\n\t\n\t\na_tz = {\"Asia/Shanghai\":\"Asia/Shanghai\",\n\t\"Asia/Singapore\":\"Asia/Singapore\",\n\t\"Europe/London\":\"Europe/London\",\n\t\"America/New_York\":\"America/New_York\"}\n\ndef tz(name):\n\treturn a_tz.get(name, None)"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# if it's already ambiguous and we're at zero, assume this is\n\t# a fold\n\tif dt.tzinfo is None and not dt.tzinfo:\n\t\t\n\t\t# set the state of the ambiguous\n\t\t(is_in_zero, is_ambiguous) = (False, True)\n\n\t\t\t\n\t# if it's not ambiguous and we're not at zero, assume this is\n\t# a fold\n\telif dt.tzinfo is None and is_ambiguous:\n\t\t\t\n\t\t# no-op\n\t\t\n\t\t\n\t#"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdef transform_tz(dt, tzinfo, **kwargs):\n\t\ttz = default_tzinfo(dt, tzinfo)\n\t\tif hasattr(tz, 'zone'):\n\t\t\ttz = tz.zone\n\t\treturn tz + (tzoffset(dt, tzinfo.zone) / 60)\n\n\treturn transform_tz\n\n\ndef date_parser(*args, **kwargs):\n\t\"\"\"\n\tAn object to parse strings in both aware and naive datetimes. See the\n\tdocs for :py:docs:`dateutil.parser` for further details"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif tzinfo is None:\n\t\treturn dt.tzname()\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\t\n\n__all__ = [\"tznaive\", \"default_tzinfo\"]"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif tzinfo is None:\n\t\tdt = default_tzinfo(dt, dflt_tz)\n\treturn dt\n\nif timezone:\n\timport builtins\n\tbuiltins.pydt = compat.builtins.pydt\n\tcompat.pydt = compat.pydt\n\tcompat.fromtimestamp = compat.fromtimestamp\n\tcompat.fromUtc = compat.fromUtc\n\tcompat.now = compat.now\n\tcompat.nowUTC = compat.nowUTC\n\tcompat.strftime ="}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t    dt = datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n\treturn dt\n\n\ndef offset(offset):\n    \"\"\"\n   .. versionadded:: 0.18\n\t\n\tSpecifies an offset, which affects how time values are computed.\n\t\n\t>>> offset(\"+02:00\")\n\t>>> offset(\"P1DT1H0M0S\")\n\t\n\t:param offset:\n\t    The time zone to apply\n\t"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\ttz = tzinfo.naive()\n\n\n\treturn tz.fromutc(dt.utcyear, dt.utchour, dt.utcminute, tzinfo=tzinfo)\n\n\n__all__ = ['default_tzinfo', 'naive', ]\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif tzinfo and isinstance(dt, tzinfo):\n\t\tdt = dt.astimezone(tzinfo=None)\n\treturn dt.replace(**locals())\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\ttry:\n\t\treturn dt.astimezone(tzinfo=tzinfo)\n\texcept Exception as err:\n\t\traise Exception(\"unable to replace time zone (%s)\", err)"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tlocaltz = default_tzinfo(dt, dflt_tz)\n\tif localtz is None:\n\t    # if nothing was assigned, it can mean that the local datetimes\n\t    # were not aware of an explicit time zone\n\t    return dt\n\treturn dt.replace(tzinfo=localtz)\t\t\t\n\t\ndef local(dt, tzinfo=None, **kwargs):\n\t\"\"\"\n\tConverts a datetime or timedelta object to an aware datetime object that\n\tremains the same time zone. If the :py:attr:"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tz is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt.replace(tzinfo=default_tzinfo(dt))"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not dt.tzinfo or dt.tzinfo is dflt_tz:\n\t\tdt = dt.replace(tzinfo=dflt_tz)\n\treturn dt\n\n\n@contextmanager\ndef _date_or_datetime(dtvalue):\n    dtvalue = parse(dtvalue)\n    yield\n    if isinstance(dtvalue, datetime.datetime):\n        dtvalue = dtvalue.astimezone(tz=tzlocal())\n    else:\n        dtvalue = dtvalue.replace(tzinfo=tzutc())"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\tself.cut_chars = \"\\n\"\n\tself.del_chars = \"*\"\n\tself.point = before\n\t\n\tif (after is None):\n\t\tself.point = before\n\telse:\n\t\tself.point = after + \"\\n\" + before\n\t\n\tself.after = self.point\n\tself.before = self.point \n\t\t\ndef cut_chars(f):\n\treturn \"\\n\".join(c for c in f if c not in self.cut_chars)\ndel f\n\ndef is"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\t\t# get all delimeter bytes\n\tfor i in range(len(before)):\n\t\n\t\t\t# get delimiter byte\n\t    delim = bytearray(after[i] if len(after) > i else bytearray(byte(0))).count(bytearray(b'\\b', bytearray(b' ')*2));\n\t\n\t    # append the data\n\t    temp = bytearray(delim);\n\t    self.__write(temp.append(self.file_offset)).write(del"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif before == bytearray() and after == bytearray() and self.index_start == len(self.raw): \n\t\tself.cut_chars = None\n\t\treturn\n\tbefore = bytearray() if before == bytearray() else bytearray(before)\n\tafter = bytearray() if after == bytearray() else bytearray(after)\n\tself.cut_chars = before + after\n\t\t\n\tfor i,c in enumerate(self.index_delimiter):\n\t\tself"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\t# split the line, if it contains lines delimited by whitespaces\n\tif before.startswith('\\n'):\n\t\tself.first = before\n\telse:\n\t\tif before.startswith('\\t'):\n\t\t\tself.first = bytearray(before)\n\t\telse:\n\t\t\tfor l in before.splitlines():\n\t\t\t\te = \":\".join([c for c in delimifs.split(l)]).strip()\n\t\t\t\tif e.startswith(':'):\n\t\t\t\t\tf = bytearray(e)\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t#\n\tif self.cut_chars:\n\t\tif before and after:\n\t\t\tdel before\n\t\telse:\n\t\t\tdel before\n\t\t\tdel after\n\n\tself.cut_chars = before\n\tself.before = self.after = before\n\n\nclass Split(object):\n\t\"\"\"\n\t\tCreates a text split object in a given filename.\n\t\t\n\t\tNote:\n\t\t    A split is created for each line of the file that\n\t\t    the split is operating on. The split object's constructor will\n\t\t    take an iterator over the lines passed in during split construction."}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif not after.startswith(\"{{\") and not before.startswith(\"{{\"):\n\t\treturn\n\tself.before = before\n\tself.after = after\n\t\t\ndef show_spack(spack_root: str) -> None:\n\t\"\"\"\n\t\tDisplay the contents of a spack directory tree rooted by spack_root\n\t\"\"\"\n\tprint(colorize(\"{:*^40}: {:*b}: {:*b}\", \"{:*^40}\",  \"yellow\"))\n\t\n\tcur = spack_root  + \"/\""}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t#\n\t# We're assuming that any non-empty slice_start_index bytes will be included to\n\t# the left of the slice_end_index.\n\t#\n\tfor offset in range(after, after + len(before)):\n\t\tif offset in self._slice_starts:\n\t\t\tcut = self._slice_start_index[offset]\n\t\t\tif cut!= before:\n\t\t\t\tself._slice_start_index['slice_end_index'] = after + offset - cut\n\t\t\tbreak\n\n\tfor offset in range(before, after):\n\t\tif offset in"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tm = re.match(r\"\\((\\d+),\\s*(\\d+)\\s*\\)\", before)\n\tif not m:\n\t\traise ValueError(\"Unable to split line by before!\\n\\t%s\" % before)\n\tself._before = int(m.group(1))\n\tself._after = int(m.group(2))\n\t\n################################################################\n# The following section does the job, but does not have a proper\n# implementation because it has more detail than a simple\n# implementation.\n#\t\n################################################################\ndef"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = bytearray(before, 'utf-8')\n\tself.after = bytearray(after, 'utf-8')\n\n\t# Check if either file is different\n\tif len(self.before) > len(self.after):\n\t\traise Exception('File must be longer than this')\n\telse:\n\t\tfor i in range(len(self.before)-len(self.after)):\n\t\t\tif i >= len(self.before) or i < 0:\n\t\t\t\tbreak\n\t\t\tif chr(self.before[i][0])"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_char = before\n\t\t  \n\tif after!= None:\n\t\t  \n\t\t  if after!= None:\n\t\t  \n\t\t\t  end_pos = os.path.split(after).index('\\n')\n\t\t\n\t\t\t  self.cut_char = after[:end_pos] + self.cut_char\n\t\t\t  \n\t\t  else:\n\t\t\t  \n\t\t  \n\t\t\t  self.cut_char = self.cut_char + '\\n'\n\t\t\t  \n\t\t  self.cut_pos += len(self.cut_char)"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tp = request.get('from')\n\treturn diaspora_identifier(p)\n\t\ndef dus_identifier(p: str):\n\treturn DusIdentifier(\n\t\ttitle=p.split(\":\")[0][1:-1],\n\t\tuser_link=p.split(\":\")[1]\n\t)\n\t\ndef diaspora_identifier(p: str):\n\td = DiasporaID(from_ = p.split(\":\")[0], to = p.split(\":\")[1])\n\treturn DiasporaID("}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\trequest_type = request.__class__.__name__\n\tif request_type is not RequestType.PUBLIC:\n\t\treturn False, False\n\treturn request.message.identifier.path == request.__identifier__, False\n\t\n# -----------------------------\ndef format_timestamp(ts: float, n: int = 0) -> str:\n\tt = \"{}:{}\".format(round(ts, 2), random.randint(0, 999999999999))\n\tif n: t += \" {}{}\".format(\" ({}).\", n"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\tpayload = request.payload\n\n\tif payload.get('@type') == 'public_diaspora_message' and ('message' in payload):\n\t\treturn True\n\t\n\treturn False\n\n\ndef public_message_handler(request: Union[dict], pub_message: dict) -> json:\n\t\"\"\"Handles public message from Diaspora.\"\"\"\n\t\n\t# Determine if we are a new message (i.e. a new request).\n\tif not isinstance(pub_message, dict):\n\t\tif payload.get('private'):"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\tif request.user in current_user.public_messages:\n\t\treturn True\n\t\n\treturn False\n\ndef parse_request(request: RequestType, client: RequestType) -> RequestType:\n\t\"\"\"\n\tParse a web client message for a Diaspora client message.\n\t\n\tIf this is a legacy request, it will be parsed. Otherwise, this returns a dictionary of the form:\n\t\n\t{\n\t\t'client': {\n\t\t\t'text': message_from,\n\t\t}\n\t}\n\t\"\"\"\n\n\tif request.client in"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tpublic_msg = True\n\tif'message' in request:\n\t\trequest_raw = request['message']\n\t\tif isinstance(request_raw, six.binary_type):\n\t\t\ttry:\n\t\t\t\tpublish_id = int(request_raw)\n\t\t\texcept:\n\t\t\t\tpublish_id = request_raw\n\t\t\tif request_raw == b'public':\n\t\t\t\tpublic_msg = False\n\n\tmsg = request.get('message')\n\tif not isinstance(msg, six.binary_type):\n\t\tmsg = msg.replace('\\x00', '')"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t# Check if request to be sent from client\n\tif hasattr(request, 'from_name') and request.from_email in ['@diaspora.io', 'diaspora_dnd@gmail.com']:\n\t\tis_public = True\n\telif hasattr(request, 'payload'):\n\t\tis_public = False\n\telse:\n\t\tis_public = False\n\n\t# Identify if it's a legacy payload\n\tif is_public:\n\t\t# Check if it's a payload\n\t\tif request.message.type == MessageType."}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry: return (hasattr(request, 'payload') and 'payload' in request.payload and (type(request.payload['payload'])!= type('message')))\n\texcept KeyError as e: pass\n\ttry: return 'payload' in request\n\texcept AttributeError as e: pass\n\t\n\ttry:\n\t\tif hasattr(request, 'legacy'):\n\t\t\treturn (not hasattr(request,'message') and hasattr(request, 'payload') and'message' in request.payload and 'payload."}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tdef _find_payload(sender: SenderType, message: MessageType, message_payload_dict: DictType):\n\t\tif \"instructions\" in message_payload_dict and message_payload_dict[\"instructions\"] is not None:\n\t\t\tif message.get(\"from\") is None:\n\t\t\t\tif sender.get(\"channel\") is None:\n\t\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\tif \"from\" not in message.get(\"from\", {}):\n\t\t\t\t\treturn False\n\t\t\t\tif message.get(\"from\"][\"id\") is None"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\ttry:\n\t\traw = json.loads(request[\"text\"])\n\texcept ValueError:\n\t\traw = request[\"text\"]\n\n\tpublic_msg = \"\" if raw.get(\"action\") == \"public\" or \"messages\" in raw else raw[\"text\"]\n\t\n\tif \"pagemessages\" in raw:\n\t\treturn public_msg.split(\" \")\n\telse:\n\t\treturn public_msg\n\t\n\t\n\nclass RequestHandler:\n\t\"\"\"\n\tDiaspora Request Handler\n\t\"\"\"\n\t\n\tdef __init__"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t_diaspora = hasattr(_diaspora, \"load\")\n\tif request.text:\n\t\t_diaspora = False\n\telif _diaspora:\n\t\t_diaspora = True\n\telse:\n\t\t_diaspora = False\n\t\n\tif not _diaspora and request.is_public:\n\t\treturn True\n\telif request.text:\n\t\ttry:\n\t\t\treturn request.text\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\treturn request.text\n\t\texcept ValueError as err:\n\t\t\t"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request.method == 'POST':\n\t\treturn True\n\ttry:\n\t\tif str(request.method) == 'PUT':\n\t\t\treturn True\n\t\t\n\texcept TypeError:\n\t\tpass\n\t\t\n\treturn False\n\ndef parse_http_get_dict(request_dict:dict):\n\t\"\"\"\n\tparse the http body\n\t\"\"\"\n\tretval = {}\n\trequest_dict.pop('request_id', None)\n\ttry:\n\t\tretval = json.loads(request_dict['request_data'])\n\texcept TypeError:\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\ttry:\n\t\treturn request.method == BODY_METHOD_MATRIX\n\texcept AttributeError:\n\t\treturn False\n\ndef verify_matrix_request(request: RequestType) -> bool:\n\t\"\"\"\n\tTry to find an appropriate Matrix object if one exists\n\t\"\"\"\n\tmatrix = None\n\tif is_matrix_request(request):\n\t\t# There is no need to ask the user how to identify if an object is a Matrix or not\n\t\tif self.select_matrix_identity_option_is_set():\n\t\t\treturn True\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif \"m.identify_request\" in request.keys():\n\t\treturn True\n\treturn False\n\nif __name__ == '__main__':\n\tmatfile = open('id_matrix.txt', 'r')\n\tmatdata = matfile.read()\n\tmatfile.close()\n\tmat = MatMatrix()\n\tmat.decode(matdata)\t\n\t\n\t#Get rid of unused columns\n\tmat.keep_non_active(False)\n\tmat.make_non_active(False)\n\tmat.add_non_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request is MatrixRequest\n\n\nif __name__ == '__main__':\n\n\tdef identify_request(request: RequestType) -> bool:\n\t\t\"\"\"\n\t\tTries to identify whether the request is from a Matrix request\n\t\t\"\"\"\n\t\treturn request is MatrixRequest\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request.method == 'POST' and request.content_type == 'application/x-matrix'\n\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\ttry:\n\t\ttoken = request.args['token']\n\t\treturn token\n\texcept KeyError:\n\t\treturn False\n\n\ndef send_matrix_notifications(request: RequestType,\n\tnotifier_url: str,\n\tnotifier_key: str = None,\n\tnotify_by_default: bool = False\n) -> Response:\n\t\"\"\"\n\tSet notification options in HTTP request headers\n\t\"\"\"\n\tresponse = make_response(json.dumps({'ok': False}), response_content_type='application/json')\n\tresponse.headers"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t# get client token\n\tclient_token, payload = client_id_and_secret_hash(request.client_secret, request.client_id)\n\tif client_token!= EMPTY_TOKEN:\n\t\t# validate client token\n\t\ttoken, is_valid_token = validate_client_token(client_token)\n\t\tif is_valid_token is True:\n\t\t\t# confirm client token with payload\n\t\t\tif payload is not EMPTY_TOKEN:\n\t\t\t\t# get the payload\n\t\t\t\tpayload_data = payload.payload_raw\n\t\t\t\tpayload_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn bool(request.params.has_key('salt_type'))\n\n\ndef is_image(req: RequestType, url: str) -> bool:\n\t\"\"\"\n\tCheck if this is an image\n\t\"\"\"\n\treturn (url.endswith(\"/image\"))\n\n# ==============================================================================\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request.method == 'GET' and request.path == '/Matrix_API_v1/identify/'\n\n\n@authenticate\ndef identify_user(\n\t\tuser: UserType,\n\t\tpassword: PasswordType,\n\t\tresponse: HttpResponseType,\n\t\t) -> dict:\n\t\"\"\"\n\t\tThis is just a fake function in case we need to support the user profile request\n\t\t\"\"\"\n\treturn {\n\t\t\"meta\" : {},\n\t\t\"user\" : {\n\t\t\t\"access_token\" : \"user_token\",\n\t\t\t\"user_id\""}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tidentities = [identity for identity in request.idents if identity.type == \"matrix\"]\n\treturn any((identities) or (len(identities) < 2))\n\n\n"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn datetime.combine(dt.date(), dt.time())\n\t\ndef save_v(dt, v):\n\ttry:\n\t\tnode = dt.group(dt.name)\n\t\tfilename = 'v_'+dt.strftime('_%Y_%m_%d_%H_%M_%S')+'.p'\n\t\tfp = open(filename, 'w')\n\t\tfp.write(v)\n\t\tfp.close()\n\t\tnode.name = fp.name\n\t\tnode.file = fp.name\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=UTC())\n\tdd = ('%0.3s' % dt.strftime('%D'))\n\tif'' in dt:\n\t\treturn '%s %s' % (dd,dt.strftime('%m/%d/%y'))\n\telse:\n\t\treturn dd\n\n\n\t# This is for converting a date to GMT in order to get consistent output\n\t# When this conversion is done the dt will have Z set, so we need to\n\t# convert the format output before doing the conversion"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt is None:\n\t\treturn''\n\telse:\n\t\treturn '%02d%02d%02d' % (dt.month,dt.day  , dt.year)\n\ndef time_d():\n\t\"\"\"\n\t\tDecode a string like '01h12m34s20ms', returns a (hour,minute,milliseconds) tuple\n\t\tNote: the time strings are for human consumption, so will never exceed 1 second.\n\t\tTime strings should not contain 'T' which will be processed as 'T'.\n\n\t\t@return: An ("}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%A:%M:%S %Z')\n\n\n#-----------------------------------------------------------------------\ndef remove_comment(text):\n\t\"\"\"\n\tRemoves all but the first line of a text. This is where the \"--\"\n\tshould be added, since all the lines of the -- section are split up\n\tand placed into separate strings.\n\t\"\"\"\n\ttmp=text.split('//')\n\treturn '\\n\\n'.join(tmp[:1])\n\ndef strip_comment(text):\n\t\"\"\"\n\tStrip all but the first"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\n\treturn dt.isoformat()\n\n\ndef split_domain_name(domain_name):\n\tsplit = domain_name.rsplit(\"[\", 1)\n\tdomains_names = {} if len(split) > 1 else split[0]\n\tif len(split) > 1:\n\t\tfor dns in split[1].split(', '):\n\t\t\tif len(dns) == 0:\n\t\t\t\tbreak\n\t\t\ttry:\n\t\t\t\tdomain_name, domain_parts = dns.rsplit(\".\", 1)\n\t\t\texcept:\n\t\t\t\tdomain"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t# date = year, month, day (DIGit)\n\tyear, month, day = dt.strftime(\"%04d\").lower().split(\"-\")[1:4]\n\n\treturn datetime(int(year), int(month), int(day),\n\t\t\t\t\tdt.hour, dt.minute, dt.second, dt.microsecond)\n\ndef is_in_dt_range(dt, dt1, dt2):\n\t\"\"\"\n\t\tReturn true if it falls inside the 1st and 2nd range\n\t\"\"\"\n\tif d"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt is None:\n\t\tdt = datetime.now()\n\t\tdt_type = 'T'\n\telif dt.microsecond is not None:\n\t\tdt_type = 'U'\n\telse:\n\t\tdt_type = 'D'\n\t# Convert datetime to raster string\n\treturn dt_type + \"%02d:%02d:%02d\" % (dt.hour, dt.minute, dt.second)\n\t\n\n\t\n\ndef read_data_file(filename):\n\t\n\twith open(filename"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tval = datetime.fromtimestamp(float(dt.time())).strftime(\"%d %b %Y %H:%M\", **kw_dict)\n\treturn val\n\ndef dt_dto_in(dt_in_arr):\n\t\"\"\"\n\tFind out if a datetime object is in a range\n\t\"\"\"\n\tif isinstance(dt_in_arr, int):\n\t\tdt = datetime.fromtimestamp(float(dt_in_arr))\n\t\treturn is_dt_in_range(dt)\n\telse:\n\t\t"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt is None:\n\t\treturn dt\n\treturn dt.toordinal()\n\n\ndef ndrun(self, *args, **kwargs):\n\treturn self._execute(*args, **kwargs)\n\n\ndef ndrun_and_fail(self, *args, **kwargs):\n\treturn self._execute(*args, **kwargs, fail=True)\n\n\ndef ndrun_with_dts(self, dts, *args, **kwargs):\n\t\"\"\"\n\tExecute an action with a set of dts. See"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%A, %E %B %d %Y') +'on'+ dt.isoformat()\n\n\ndef sort_keys(sequence):\n\t# Sort keys so that key values are first, as the keys\n\t# are the value.\n\tkeys=sequence.keys()\n\tkeys.sort(lambda x, y:cmp(x,y))\n\treturn sequence[:]\n"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttag_closing = False\n\tin_code_block = False\n\tfor line_number, chunk in enumerate(text.split('\\n'), 1):\n\t\t# ignore blocks like \"if __name__ == '__main__'\":\n\t\tif chunk.startswith('__') and not tag_closing:\n\t\t\tpass # ignore line\n\t\telif chunk.startswith('#'):\n\t\t\ttag_closing = True\n\t\telif not tag_closing and line_number == 0:\n\t\t\tin_code_block"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\treturn tag_filter_regex(text, tagsfilter_regex).set(replacer, replace=False).split() if replacer else text\n\ndef tag_filter_regex(text: str, regex: re.compile) -> Tuple[Set, str]:\n\tregex.compile(re.escape(text)).pattern\n\ttags = regex.findall(receivers[text])\n\treturn tags, text\n\ndef tag_list(text: str):\n\tset = sorted(tags(receivers.get(text, (receivers"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = Tag.create()\n\tdef set_tag(tag: str) -> str:\n\t\tif replacer is not None:\n\t\t\treturn replacer(tag)\n\t\treturn tag\n\tdef remove_tags(tag: str) -> str:\n\t\tif hasattr(tag, \"strip\"):\n\t\t\treturn tag.strip()\n\t\treturn tag\n\tif len(text) == 1 or replacer is None:\n\t\treturn reduce(lambda x, t: replace_all(t, set_tag(text)), tags.words(), text), text\n\ttags.update("}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif isinstance(replacer, str):\n\t\treplacement = replacer\n\telse:\n\t\treplacement = replacer()\n\treturn extractors(text, replacement)\n\n\ndef find_blocks(text: str, replacer: callable = None) -> Tup:\n\t\"\"\"\n\tFind tags and blocks in text.\n\t\n\tTries to ignore blocks inside code blocks.\n\t\n\tOptionally, if returned by \"replacer\", will also replace the text by a replacement string.\n\t\n\tReturns tuples of tag text and a set of blocks\n\t\"\"\""}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\treplacer = replacer or get_clike_replacer()\n\t\n\ttags = set()\n\t\n\tif not replacer:\n\t\treturn tags, text\n\t\n\tfirst, first_pos = skip(text, len(replacer.callable_))\n\tnext_pos = first_pos + 1 # The last chunk of text.\n\t\n\twhile next_pos >= first_pos:\n\t\tlast_pos = next_pos\n\t\ttext = text[first_pos:last_pos]\n\t\t\n\t\tfirst_pos = last_pos + len("}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags, code, text, line = split_text_by_newlines(text)\n\treturn normalize_tags(tags), text\n\t\n# END OF FILE"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set([] for x in text.split())\n\tif replacer:\n\t\ttags = set(x for x in tags if isinstance(x, replacer))\n\n\tresult = set()\n\t\n\tfor i in tags:\n\t\tfor block in blocks_after_comment(i):\n\t\t\tif block.is_comment():\n\t\t\t\ttags.update((tags_inside(x, tags) for x in block.to_comment())\n\t\t\telse:\n\t\t\t\tresult.add(block)\n\t\t\t\tresult.add(block.tag)\n\n\treturn result, text\n\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_list: Set[str] = set()\n\t\n\tdef replace(tag) -> None:\n\t\t\n\t\t# First stop tag.\n\t\ttag_list.discard(tag)\n\t\t\n\t\t# Now process any tag contained in \"tags\"\n\t\tfor tagword in tags:\n\t\t\tif tagword.lower() in tag.lower():\n\t\t\t\ttag_list.add(tagword)\n\t\n\tdef replacer_replace(s: str) -> None:\n\t\t\n\t\tif replacer is not None and hasattr(replacer, 'call'): #"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tresult = []\n\treplacetag = None\n\t\n\twhile True:\t\n\t\toriginal = text\n\t\ti = text.find(\"//\")\n\t\tif i == -1:\n\t\t\treturn(tags, original)\n\t\t\n\t\ti = i + 2\n\t\t\n\t\twhile i < len(text):\n\t\t\tif text[i] == \"/**\":\n\t\t\t\tif replacetag is not None:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttag = replacetag(text[i+1:])\n\t\t\t\t\texcept IndexError:\n\t\t\t\t\t\treturn(tags, text"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\tset_to_remove = Set()\n\toriginal = re.sub(r\"\\s+\", \" \", text)\n\tfor part in re.split(r\"{|}\".format(tpl_regex), original):\n\t\t\n\t\t# Check how we want to parse the part and whether or not to remove it.\n\t\tif replacer:\n\t\t\t\n\t\t\t# It's not part of a code block.\n\t\t\tif not part.startswith(\"<pre>\"):\n\t\t\t\tif replacer(\" \" in part or '\\n' in part):\n\t\t\t\t\t_"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Create an empty list for each item to avoid dupes\n\tlinks = []\n\tlinksre = re.compile(r'\\[[^\\[\\]]*\\]')\n\n\tprocessors = [\n\t\ttext.strip(), ['http://link.com/'], ('<http://link.com>', []),\n\t]\n\n\tfor processor in processors:\n\t\tfor matchre, matchargs in re.finditer(linksre, processor):\n\t\t\tlink = matchargs['text']\n\t\t\tmatch = matchargs['match']\n\t\t\tif match:\n\t\t\t\t(domain"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\tall = ''\n\t\n\t# Extract links with # and \" links and a url\n\tif not '<http://' in text:\n\t\tfor link in re.sub('href=\"(\\&#)?'+re.escape(text)+'(\\&|')[-10:-6]:\n\t\t\tall += re.sub('href=\"(?:\\'+)?'+re.escape(link)+\"href=('\\\\)', r'', text)\n\t# If no # and \\ link have been processed, add more of them in text with links\n\telse:\n\t\t"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tdef add_linkify(text, *links):\n\t\treturn linkify(text, *links)\n\n\tdef add_attrs(links):\n\t\trel_tags = []\n\n\t\t# rel is an optional attribute\n\t\tif'rel' in links[0]:\n\t\t\tif links[0]['rel'].startswith('link '):\n\t\t\t\tlinks[0]['rel'] = '[rel]_' + links[0]['rel'][5:]\n\n\t\t# rel is mandatory as attributes of anchor elements,\n\t\t# but a link can be used with a rel attribute\n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# clean-up the links in text\n\tfor link in text.splitlines():\n\t\toldText = link.decode(self.ENCODING)\n\t\t# get the old link, and replace it with what was just entered on the link\n\t\tfor anchor in ['<a href=\"', '<A ', '</a>', '</a>']:\n\t\t\tif oldText.strip().endswith(anchor):\n\t\t\t\tsplit = oldText.strip().split(anchor)\n\t\t\t\tsplit.append('')\n\t\t\t\tnewText = ''.join(split)"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = unicode(text)\n\ttext = text.replace('\\n','')\n\tfor token in self.extractor.extract_tokens(' '.join(text2tokens(text))):\n\t\tif token['type'] == 'LinkedWordLink':\n\t\t\ttoken['type'] = 'Link'\n\t\t\ttoken['link'] = '\"' + token['tokens'][0] + '\"'\n\t\t\ttoken['text'] = ('\"' + token['tokens'][0], \" - [\" + token['tokens'][1]"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif text:\n\t\ttext = text.replace(\"&amp;\", \"&\").replace(\"&#38;\", \"\\n\")\n\t\tprint(text)\n\t\ttext = text.replace(\"&#39;\", \"'\").replace(\"&quot;\", \"\\\"\").replace(\"&apos;\", \"'\")\n\t\ttext = markdown_to_text(text)\n\t\ttext = text\n\t\ttext = text.replace(\"- \", \"+\").replace(\"-- \", \"+- \").replace(\"+\", \"+- \")\n\t\ttext = text.replace(\" \", \" \").replace(\""}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlinks = (t.text for t in text)\n\tfor link in links:\n\t\tif link:\n\t\t\tlinks.remove(link)\n\t\t\tlink = _linkify(link)\n\t\t\tlinks.append(link)\n\n\ttext = [link for link in links if link]\n\n\treturn links\n\ndef _linkify(link):\n\t\"\"\"\n\t\tTry to get this link out of the text buffer if it is there,\n\t\tit could be a direct links like urls; so use href attribute of links and not the first link on\n\t\tthis line.\n\t\"\"\"\n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Make sure that the text has been properly formatted as if we can be at it.\n\tif not linkify or not any(L in text for L in ALLOWED_LINK_TAGS):\n\t\treturn text\n\t# Build the link string and format it into links.\n\tlink_formatted = []\n\tfor link in links_regex.findall(text):\n\t\t# Build url\n\t\turl = urlparse.urljoin(url, \"//\" + link[0:2] + \".com/\" + link[2:5] + \"?q=\" +"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif not text: return\n\tlinks = text.split()\n\tfor link in links:\n\t\tlink = link.strip()\n \t\tif not link or not len(link): continue\t\t# Blank links.\n\n\t\t# Check if the target text is a text tag.\n\t\tis_text_tag = False\n\t\ttext_tag = None\n\t\tif target_tag.find('<')!= -1 and target_tag.find('>')!= -1:\n\t\t\t# Text tag.\n\t\t\tis_text_tag = True\n\t\t\ttarget_tag = target_tag[0"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tadd_urls = set()\n\ttext = re.sub(r\"\\[\\[(.+?)\\]\\]\", \"\\\\\\\\\\\\1\", text, flags=re.IGNORECASE)\n\ttext = re.sub(r\"`\\*\\*\\[\\]\", \"\\\\*\", text, flags=re.IGNORECASE) # Remove quotation markers.\n\n\tlinkify = None if linkify is None else linkify\n\ttext = linkify(text, remove_new_lines=True)\n\t\n\tif text:\n\t\t# This"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\tresponse = session_session.request(\n\t\t\turl + (\"/?head=%s\" % urls.encode(url))\n\t\t)\n\t\thtml = response.content\n\texcept Exception:\n\t\tlog.error(\"Something went wrong while we try to fetch \"\n\t\t\t\"the content type of %s: %s\", response=response, status=500)\n\t\traise UnknownUrlType\n\n\treturn html.split(\"\\n\")[0].strip().split(\" \")[-1]\n\nif __name__ == \"__main__\":\n\n\timport time\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\t_get_headers()\n\texcept Exception as err: # pylint: disable=broad-except\n\t\tLOG.exception(\"Fetching remote content of url: %s\", url)\n\treturn self.cached_content_type\n\t\t\n\n\n@hook.on_trait_change(config=True, before=fetch_content_type)\ndef set_content_type(url: str, value: Optional[str]):\n\t\"\"\"\n\t\tReset content_type when we set it again to None\n\n\t\tThis may be useful in tests"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treturn get_content_type_header_from_url(url)\n\t\n#\n# GET the header for a given content type\n#\ndef get_content_type_header_from_url(url: str) -> Optional[str]:\n\theaders = dict()\n\tif url.startswith(\"file:\"):\n\t\treturn '' # TODO if you make sure that you've already loaded the content type we want, then use this logic\n\t\t\n\tif url.startswith(\"http:\"):\n\t\turl = url.replace(\"http:\", \"https:\")\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\tlink = requests.head(url)\n\t\treturn link.headers['content-type']\n\texcept:\n\t\treturn None\n\n\ndef get_url(url: str) -> str:\n\t\"\"\"\n\t\tGet the remote url for the content stored on disk for testing purposes.\n\t\"\"\"\n\ttry:\n\t\treturn requests.head(url).url\n\texcept:\n\t\treturn None\n\n\ndef get_content_type(url: str) -> str:\n\t\"\"\"\n\t\tGet the content type stored on disk for testing purposes.\n\t\"\"\"\n\treturn fetch"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tlog.debug(\"Stopping fetch_content_type() because we have already fetched the head of \" + url)\n\treturn None\n\ndef fetch_source(url: str) -> str:\n\tlog.debug(\"Starting fetch_source() because \" + url + \" has already been fetched\")\n\treturn None\n\ndef fetch_body(url: str, data: Optional[bytes] = None) -> Optional[bytes]:\n\tlog.debug(\"Starting fetch_body() because \" + url + \" has already been fetched\")\n\treturn None\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\theaders = {}\n\theaders['Accept'] = 'application/json'\n\tresp = fetch(url, headers=headers)\n\tcontent_type = ''\n\tcontent_type_raw = url_encoded(resp).strip()\n\tif content_type_raw.startswith('application/json'):\n\t\tcontent_type = content_type_raw\n\t\tcontent_type_raw = content_type_raw.replace('\\n', '')  # strip trailing newline\n\t\tcontent_type_raw = content_type_raw.strip()\n\t\tcontent_type"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tlog.debug(\"Attempting URL fetch of remote content type %s\", url)\n\ttry:\n\t\t# Make a dummy request\n\t\tcontent_type = None\n\n\t\t# Perform URL request\n\t\tresponse = requests.get(url)\n\t\tcontent_type = response.headers.get(\"Content-Type\", \"\")\n\t\t\n\t\tif content_type:\n\t\t\tlog.debug(\"Got content type %s\", content_type)\n\t\t\treturn content_type\n\t\telse:\n\t\t\tlog.debug(\"Could not retrieve content type (%s)\" % content_type)\n\t\t\t\n\texcept"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tresponse = client.get(url)\n\ttry:\n\t\treturn f\"application/vnd.google-apps.spreadsheet\"\n\n\t\t# Ignore HTTP 206\n\t\t#response_schema, = response.headers.get('Content-Type', None)\n\t\t#if response_schema is None:\n\t\t#    return None\n\texcept Exception as exception:\n\t\tsys.exit(exception)\n\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tcontent_type = None\n\tresponse = None\n\ttry:\n\t\tresponse = requests.head(url)\n\t\tresponse.raise_for_status()\n\t\tcontent_type = response.headers['Content-Type']\n\texcept Exception:\n\t\tpass\n\treturn content_type\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tfrom urllib.request import urlopen\n\tfrom urllib.request import URLError\n\n\turl = urlparse(url,scheme='https')\n\tif not url:\n\t\treturn None\n\tcontent_type_url = \"//%s/meta/info/branding\" % url.hostname\n\n\twith urlopen(content_type_url) as f:\n\t\tline = f.readline().strip()\n\t\tif len(line) > 0:\n\t\t\treturn line.decode('utf-8')\n\n\treturn None\n"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag == '!':\n\t\treturn True\n\telse:\n\t\treturn False\n\n# Test an integer as a tag\n@test_tag(int)\ndef test_tag2(tag: int) -> bool:\n\t\"\"\"\n\tTest that a tag which is an integer is accepted as an integer.\n\t\"\"\"\n\tif tag is not None:\n\t\treturn tag.toInt() == 24\n\telse:\n\t\treturn False\n\n# Test a string as a tag\n@test_tag(str)\ndef test_tag3(tag: str) -> bool:"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tret = False\n\tif tag in TAGS:\n\t\tret = True\n\treturn ret\n\n\ndef test_is_tag(tag: str) -> bool:\n\"\"\"\n\tReturn 'True' if the passed in tag is a tag.\n\t\"\"\"\n\treturn test_tag(tag)\n\n\n\n###################################################\n# Main\ndef make_parser():\n\tpass\n\ndef parse_args(argv: str):\n\t\"\"\"\n\tGiven the argv, parse the arguments and test the user for mistakes.\n\t\"\"\"\n\tparser = argparse.ArgumentParser"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn True\n\n\ndef tag_filter(word: str) -> bool:\n\t\"\"\"\n\tTest if a word is a tag\n\t\"\"\"\n\treturn word.lower() in (\"tag\", \"bzr\")\n\n\ndef tags_filter(tags: set) -> set:\n\treturn set(tag for tag in tags if tag_filter(tag))\n\n\ndef word_filter(word: str) -> bool:\n\t\"\"\"\n\tTest if a word has another word as well.\n\t\"\"\"\n\treturn word in (\"cat\", \"bird\", \"br\", \"bug\", \""}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\twords = set([\"a\", \"aa\", \"ab\", \"ac\", \"ba\", \"b\", \"bac\", \"bba\", \"abac\", \"acb\"])\n\ttags = [\"tag1\", \"tag2\", \"tag3\", \"tag4\" ]\n\tassert Word(words|tags).tags(tag) == True\n\tassert Word(words, tags).tags(tag) == True\n\tassert len(words|tags) == len(words)\n\tassert words.issuperset(tag) == True\n\tassert tag.issuperset("}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\ttag = tag.lower()\n\tok, tag = utils.parse_tag(tag)\n\tassert not ok\n\treturn tag\n\n\ndef test_tag_valid(tag: str) -> bool:\n\t\"\"\"\n\tTest a word, whether it doesn't exist in the tag table.\n\n\t:param tag: string,\n\t:returns: bool\n\t\"\"\"\n\n\tif tag not in self._tagtab:\n\t\treturn True\n\n\treturn False\n\n\ndef test_normalize(tag_list: list) -> str:\n\t\"\"\"\n\tConvenience function"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif (tag == \"-\"):\n\t\treturn True\n\treturn False\n\ndef create_tag(tag: str = \"\") -> bytes:\n\t\"\"\"\n\tCreate a tag and check if it is possible to add it.\n\t\"\"\"\n\ttag = bytes(tag)\n\treturn tag\n\ndef set_tag(tag: bytes, value: bytes) -> int:\n\t\"\"\"\n\tSet a tag and check if it is possible to add it.\n\t\"\"\"\n\treturn bytes(tag)\n\ndef check_value(value: bytes, tag: bytes = \"\"):\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag == \"key_tag\":\n\t\texpected_val = True\n\telif tag == \"val_tag\":\n\t\texpected_val = False\n\telse:\n\t\texpected_val = False\n\treturn expected_val\n\n\"\"\"\nTest if the Wordlist is correct.\n\"\"\"\n# Test if it is a full Wordlist with the correct number of items.\ntest_tag(tag=\"n=2\")\n# Test if it is a partial Wordlist with the correct number of items.\ntest_tag(tag=\"n=1\")\n# Test if words of multiple sizes"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tassert can_tag(tag) == (tag!= None)\n\t\n#========================================\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn tag in list_tags\n\n@patch('pygments.lexer.CategorizedLexer.__call__', return_value=(\"test\", None))\n@patch('pygments.lexer.CategorizedLexer.lex', return_value=\"test\")\ndef test_tokenize(lexer: MagicMock, lex: MagicMock) -> None:\n\n\tassert False, \"no tokenize called\"\n\n@patch('pygments.lexer.CategorizedLexer.__call__', return_value=tuple((\"test\", \"tag1"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tparsed = self.tag(tag).substring(\"{\".format(tag))\n\tfor word in parsed.split():\n\t\tif word in self.tag_set and \"-\".join([word]) in parsed:\n\t\t\treturn True\n\t\t\n\treturn False\n\t\t\t\ndef main(tag: str, tagged: str) -> int:\n\t\"\"\"\n\tTest whether two word may be tagged.\n\t\"\"\"\n\tif not tag in tagged:\n\t\treturn 1\n\tif \"=\" in tagged:\n\t\ttagged1, tagged2 = tagged.split(\"=\")\n\t\t"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tnew_hash = {}\n\tfor child in node:\n\t\tnew_hash.update({child.tag: child})\n\n\treturn new_hash\n\n\ndef get_nested_dom(ns_dict, ele_dict, child_name=\"xml\"):\n\t\"\"\"\n\tReturns the domtree of <node> or <element> with the given name,\n\n\tor xml nodes where tag is <ns_name> or <surname>. This function checks\n\tall the sub-elements before processing that one.\n\n\t:param node: the node name to look for\n\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn self.__xpath('.'.join((node, '*')))\n\t\n\t@staticmethod\n\tdef xml_attributenames(node):\n\t\treturn node.alltext('.//@Name', escape_chars=['\\\\'])\n\t\n\tdef __xpath(self):\n\t\treturn xpath(self.__namespace, '//%s' % self.__tagname,\n\t\t\t\t\t\t\t\t\t\t\t\t\t '//%s' % self.__tagname, **self.__attributes)\n\t\t\n\t@staticmethod\n\tdef __att"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tret = {}\n\tif node.tag == \"name\" and node.text!= \"\" and not node.tail:\n\t\tret[\"name\"]=node.text\n\tfor child in node:\n\t\tret[child.tag] = xml_children_as_dict(child)\n\treturn ret\n\t\ndef _make_tags(tagged_child_list, att_key, value):\n\t\"\"\"\n\tMake it a dict, keyed by key.\n\n\tAssumes that both the tag and the attributes are children, i.e. not just the content.\n\t\"\"\""}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif node.get_content_type() == 'root':\n\t\treturn dict()\n\treturn dict(tree_node_to_dict(node, parent_name=None))\n\ndef node_to_tree(node):\n\t\n\t# add the child.\n\tparent = node.get_parent()\n\ttree_str = \" \".join([\"#%s = %s\"%(k,v) for k,v in node.xml_dict().items()])\n\t# append it to the tree.\n\tnode.append([tree_str, node."}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tresult = dict(node.children)\n\tfor child in node.children:\n\t\tresult.update(xml_children_as_dict(child))\n\treturn result\n\n\n# ============================================================================\ndef get_xml_name(node):\n\t\"\"\"\n\tTurn the given node into a string.\n\t\"\"\"\n\treturn node.tag.lower() + \" \" + node.tag.ljust(40, \" \").strip()\n\n# ============================================================================\ndef get_xml_tag(xml_parent):\n\t\"\"\"\n\tReturns the XML tag of a node.\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tresult = {}\n\tchildren = node.findall('./xml/child')\n\tfor e in children:\n\t\tresult[e.tag] = e.text\n\treturn result\n\ndef XMLDumpAllXml(node_id, level, outfile, showTagName):\n\t\"\"\"Dump this node to outfile as a XML dump.\"\"\"\n\t\n\telements = []\n\telements.append('   <?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n\telements.append('   <dummy id=\"%s\">"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn dict([(child.tag, xml_children_as_dict(child)) for child in node])\n\t\ndef parse_xml_tree(xml):\n\t\"\"\"\n\tParse an xml file/string to an XML document.\n\t\"\"\"\n\treturn ElementTree.fromstring(xml)\n\t\ndef parse_xml(xml):\n\t\"\"\"\n\tParse an XML document to an XML document.\n\t\"\"\"\n\treturn parse_xml_tree(parse_xml_tree(xml))\n\t\ndef xml_escape(xml):\n\t\"\"\"\n\tEscape"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tresult = {}\n\tfor child in node:\n\t\tchild_dict = xml_children_as_dict(child)\n\t\tresult.update(child_dict)\n\treturn result\n\n#\n# The XML tree of the book.\n#\nprint \"\"\"\\n\"\"\"\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tchildren = []\n\tif '>' in node.tag:\n\t\tchildren = json_children_as_dict(node.find('>'))\n\treturn children\n\n# def xml_children_as_list(node):\n# \t\treturn node.findall('>')\n\n# def xml_children_as_item(node):\n# \t\treturn node.find('>').text\n\n\ndef get_children(node, **kwargs):\n\t\treturn json_children_as_dict(node)\n\n# def xml_get(node, name, **"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif node:\n\t\tif isinstance(node.get('children'), basestring):\n\t\t\tnode.set('children', node.get('children', ''))\n\t\treturn node.get('children', '')\n\telse:\n\t\treturn {}\n\n\ndef find(node, node_name):\n\t\"\"\"\n\tFind a given tag, taking recursive into consideration.\n\n\tReturns the child with name <node_name>, as long as it exists.\n\t\"\"\"\n\tvalue = find(node, node_name)\n\tif isinstance(value, basestring):\n\t\tvalue"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= None:\n\t\tif entity_handle!= None and entity_handle!= sender_handle:\n\t\t\traise BadAuth(\"The Diaspora sender %s and the expected entity %s were specified in the HEADERS. Got %s, but expected %s\"%(\n\t\t\tsender_handle,entity_handle,len(sender_handle),entity_handle))\n\t\t\t\ndef is_message_match(is_msg_match):\n\t\n\t\t# make it easier to get a consistent message_type\n\t\tmatch = lambda msg: isinstance(msg, dict)"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tsender_handle = long(sender_handle)\n\tsender_addr = long(self.get_address(sender_handle))\n\tentity_handle = long(entity_handle)\n\tentity_addr = long(self.get_address(entity_handle))\n\n\tif (not sender_addr) or \\\n\t\t(not entity_addr):\n\t\t\t# Just fine\n\t\treturn True\n\telse:\n\t\treturn False\n\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t# If the sender is the same as its sender, no problem as we know the sender was set.\n\t\n\tif sender_handle == sender_handle:\n\t\treturn # Nothing to do if the sender cannot match the sender.\n\t\n\tif entity_handle == 0:\n\t\treturn\n\t\n\tsender_name = unicodedata.normalize('NFC', unicode(str(sender_handle)))\n\tentity_name = unicodedata.normalize('NFC', unicode(str(entity_handle)))\n\tsender_display = entity_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\t# make sure any old, old s/r and t/r, will no longer match.\n\t\ts = s / (i_t / i_r)\n\t\tt = t / (i_t / i_r)\n\t\te = e / (i_t / i_r)\n\telse:\n\t\t# make sure to update object when not the sender\n\t\te = e / (i_t / i_r)\n\t\ts = s / (i_r / i_t)\n\t\tt = t /"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t#\n\t#  sender_handle is the handle to the sender of the payload\n\t#  entity_handle is the handle to the desired object.\n\t#\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\t\t\t\t\n\treturn False\n \n\ndef get_valid_sender_and_entity_handle(data):\n\t\"\"\"\n\t\tIf the payload is an encrypted entity and we receive it from another entity, and the sender is not an\n\t\tuninitialized object, then we need to know the setter's handle and get it's setter as"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tpayload_handle = getattr(item, 'payload_handle', None)\n\tif not payload_handle and 'payload_body' in item.dictionary:\n\t\tpayload_handle = item.dictionary['payload_body'].payload_handle\n\tif not payload_handle:\n\t\traise Exception(\"Payload header missing payload_handle or payload_body key for item %s\" % (item['key']))\n\tif payload_handle!= sender_handle:\n\t\t# We don't want to log a warning as the payload_handle will need to later"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t# Validate the sender\n\tsender = self.check_sender(sender_handle)\n\tif not sender:\n\t\treturn False\n\tsender_name = self.get_sender_name(sender_handle)\n\tif not sender_name:\n\t\treturn False\n\t# Parse the object. We must not be parsing a message that's getting lost, even when the Diaspora sender\n\t# appears in a payload.\n\tobj = self.parse(object=sender.payload.message, author=sender_name)\n\treturn"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tpayload = json.loads(sender_handle)\n\ttry:\n\t\tentity_handle = json.loads(entity_handle)\n\texcept:\n\t\treturn\n\tif not payload.has_key('metadata') or  # metadata not present\n\t    not payload.has_key('object') or  # object not present\n\t\tpayload.has_key('sender') or  # sender_not expected\n\t\tpayload.has_key('entity') or  # entity_not expected\n\t    payload.has_key('payload'):  # payload_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\tsender = msg.sender.lower()\n\n\tpayload_handle_match = re.search(re.compile(sender + self.ENTITY_PREFIX + 'handle'), payload)\n\t\n\tif payload_handle_match is not None and payload_handle_match.group() == sender_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\t\n\t\ndef check_sender_and_sender_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tEnsures the sender and sender handle"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t# If they're doing Diaspora's payloads, they should always have an entity_handle set!\n\tif not (sender_handle and entity_handle):\n\t\treturn \n\t\n\t# Now check the first two parts of the payload that depend on the headers. It's important to check that\n\t# these two parts are the same.\n\tif 'from' not in sender_handle.raw_handle.payload:\n\t\treturn\n\tif 'from' not in entity_handle.raw_handle.payload:\n\t\treturn\n\t\n\tif 'from_"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tparams_obj = {\n\t\t\t'include_description': 'Y',\n\t\t\t'include_extensions': 'Y',\n\t\t\t'include_version': 'Y',\n\t\t\t'include_os': 'Y',\n\t\t\t'include_os_version': 'Y',\n\t\t\t'include_os_vendor': 'Y',\n\t\t\t'include_provides': 'Y',\n\t\t\t'include_source': 'Y'\n\t}\n\t\n\tif document_path is not None:\n\t\tparams_obj['document_path'] = document_path\n\t\n\tjsonrpc"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tparams = urlparse(url).query\n\turl_params = dict(params)\n\tif document_path:\n\t\turl_params['document_path'] = document_path\n\n\twell_known_document = _do_post(url_params)\n\n\treturn json.loads(well_known_document)\n\t\n\t\n\t\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = os.environ['NODEINFO_DOCUMENT_PATH']\n\t\n\ttry:\n\t\tfetchUrl = 'http://nodeinfo.diaspora.software/node-info-api-%s/%s' % (node_info_api_version, url)\n\t\n\t\tresp = fetchUrl(fetchUrl, allow_redirects=False)\n\t\t\n\t\tif not resp.ok:\n\t\t\traise NodeInfoFetchError(resp.message)\n\t\t\n\t\trespJSON = resp"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\tprint 'Generating well-known document for'+  url\n\t\n\t# Create the document object\n\tdoc = _create_nodeinfo_document(url, document_path)\n\t\n\t# Get the document's data using the generator\n\t#  and set it as it is.\n\tdoc.set_data(document_path)\n\t\n\t# Return the data, or else raise an exception\n\treturn doc.get_data()\n\n# == End =\n\n### TESTING ###\nif __name__ == '__main__':\n\t#"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path:\n\t\tp = document_path + '.well-known'\n\t\tdoc = os.path.join(document_path,\n\t\t\tp\n\t\t\t+ (['index' if p == document_path else ''])\n\t\t\t+ '.html'\n\t\t)\n\telse:\n\t\tdoc = os.path.join(\n\t\t\tos.path.dirname(os.path.dirname(url)),\n\t\t\t'index.html'\n\t\t)\n\t\n\twith open(doc, 'rb') as f:\n\t\ts = sio.SliceFromBuffer"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\treturn dict(\n\t\tnode_id=get_node_ids(url)\n\t)\n\n\ndef _get_user_info(user_agent_string):\n\treturn UserAgentData(user_agent_string)\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is not None:\n\t\tdoc_path = document_path\n\telse:\n\t\tdoc_path = os.path.join(os.path.abspath(os.path.join(url[0], '..')), '..', url[0])\n\tdoc_path = doc_path.replace(os.sep, '/')\n\tif os.path.exists(doc_path):\n\t\treturn {\n\t\t\t'doc_path': doc_path,\n\t\t\t'title': 'NodeInfo %s' % doc_path,\n\t\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\treturn \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE nodes>\n<nodeinfo>\n    <node id=\"{}\" version={} date=\"{}:{}\" />\n</nodeinfo>\"\"\".format(document_path or document_path_nodeinfo_well_known_document)\n\ndef get_nodeinfo_well_known_document_string(url, document_path=None):\n\t\"\"\"\n\tGenerate a NodeInfo.well-known document as a string.\n\t\n\tSee spec: http://node"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tpath = url.split('/')[2]\n\tif document_path is None:\n\t\t\n\t\tlogger.warn(u'specify document path to generate: {}'.format(path))\n\t\t\n\t\ttry:\n\t\t\tpath = os.environ['PWD'] + \"/\" + document_path\n\t\texcept KeyError:\n\t\t\tpath = None\n\t\n\treturn { 'document': url, 'document_path': path }\n\t\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\turl = url.split('#', 1, 1)[0] # Remove leading \"#\" characters\n\t\n\tif document_path is None:\n\t\tdocument_path = url + \"/.well-known.json\"\n\t\t\n\twith closing(urlopen(url), data=\"application/json\", timeout=10):\n\t    try:\n\t        info = wkc.NodeInfo(document_path)\n\t    except wkc.UnknownNetworkConnection:\n\t        raise NetworkInUseError(url)\n\t    except wkc.ContentNotAvailableError:\n\t"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tassert len(doc) == 80\n\n\t# get the signature to see if the doc was a relayable\n\t# signature\n\tret = [None]*8\n\tfor i, (r, s) in enumerate(zip(doc['ret'], doc['sig'])):\n\t\tret[i] = r\n\n\tfor i, (s, r) in enumerate(zip(public_key, signature)):\n\t\ts = bytes(s)\n\t\tsig = s\n\t\tret[i] = r\n\n\t# first we need to see if the signature is"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif signature!= public_key:\n\t\traise ValueError\n\n\tis_relevant = False\n\t# only acked a message that claims this doesn't matter\n\tif public_key == doc.ack_ref:\n\t\tis_relevant = True\n\n\tif not is_relevant:\n\t\treturn signature == doc.get_relayable_signature()\n\n\tpublic_key = sign(public_key, doc.version, doc)\n\tsig = XmlSig(doc.ack_ref, sig+doc.ack_date, doc.ack_secret, doc."}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif pubkeyhash is None:\n\t\traise NotImplementedError(\n\t\t\t\t\"need pubkeyhash to be implemented!\"\n\t\t)\n\tprivkey = PublicKey(signatures=[signature], pubkeys=[public_key])\n\n\tif doc is None or not isinstance(doc, list):\n\t\traise NotImplementedError(\n\t\t\t\t\"need a valid XML document to verify signatures!!\")\n\tfor sig in doc.findall(\".//signed-element\"):\n\t\tif sig.attrib =='sigtype':\n\t\t\t# found a signature, now to"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tverify_text = sig2verify(doc, signature, public_key, doc.base[\"key\"][\"bits\"])\n\tprint \"verified from %s\" % (doc.public[\"full\"])\n\tprint verify_text\n\treturn_code = verify_text == doc.base[\"signature\"][\"text\"]\n\n\treturn return_code\n\n############## Generate a hash from public key ###\n\nimport hashlib\ndef pubkey_hash(public_key):\n\t# We don't need to hash the entire public key because it's not really used"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tsign_doc = etree.ElementTree(etree.fromstring(doc))\n\tsign_elements = {}\n\tfor sig in sign_doc.getroot().xpath(SELFATT_NODE):\n\t\tsign_elements[sig.attrib[SELFATT_NODE]] = sig\n\n\tclaimed_author = sign_elements[public_key].text\n\tsign_path = get_relayable_signature(public_key, signature, doc)\n\treturn (sign_elements[public_key]), claimed_author, sign_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t_, signature_element = _normalize_xml(signature)\n\n\tif not pubkey_has_attr(_decode_as_xml_safe(signature_element), 'x509:issuer'), \\\n\t\t\t'Unable to find x509:issuer attribute: %s' % \\\n\t\t\t_decode_as_xml_safe(_decode_text(signature_element))\n\tif not pkey_has_attr(_decode_as_xml_safe(signature_element), 'x509:subject'), \\\n\t\t\t'Unable to find"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tmsg = msg.getroot()\n\troot_confidence = msg.find(_xmlns=\"http://www.w3.org/2003/XMLSchema-instance\")\n\tnode = root_confidence.find(_xmlns = \"text/xml-crypto\")\n\tsig = node.extract()\n\tparsed_sig = base64.b64decode(signature)\n\t# Check for equality of pub and sig.\n\tif parsed_sig!= sig:\n\t\tprint 'Inconsistent signatures:',str(parsed_sig"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tfrom sakia.data import metadata\n\treturn metadata.has_value(public_key) and \\\n\t\tmetadata.has_value(signature) and \\\n\t\tdoc['root']['signature'] == signature\n\n\nclass AuthorSignedWith(AuthorSigned):\n\tdef is_unreliable(self, doc, docattrs):\n\t\t\"\"\"\n\t\tWhether the XML of the author is not trustworthy.\n\t\tIf the document does not contain the signing authority's\n\t\tsignature, an UnknownAuthorException (see RFC 3822) will be\n\t\traised.\n\t\t\"\"\""}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\troot_attrs = public_key.split(':')[0].split('@')\n\tversion, root = root_attrs\n\treturn public_key in root_attrs\n\n\ndef signed_doc(public_key, signature, doc):\n\t\"\"\"\n\tEncode a signed XML document and signed/signed signature, in\n\tXML format (signed message first, then payload) so that it can be\n\tvalidated against other references.\n\n\t:param public_key: The public key used in the signature.\n\t:param signature: The signed text plus signature for"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tsignature = str(signature)\n\n\t# Get the signatures on the message\n\tXMLSignatures = signutil.string.to_xml(doc)[\"Signatures\"]\n\n\tfor signature_element in XMLSignatures:\n\t\tif signature_element.attrib[\"Signature\"]!= signature:\n\t\t\treturn False\n\treturn True\n\ndef verify_signature(private_key, public_key, private_data, signature):\n\tif not isinstance(private_data, str):\n\t\traise TypeError, \"private_data should be the actual value of a string\""}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tf = xml.parsers.expat.ParserCreate()\n\tf.StartElementHandler = startElement\n\tf.EndElementHandler = endElement\n\tf.CharacterDataHandler = characterData\n\tf.Parse(document)\n\td = {}\n\t\n\tif (\"xml\" in f.XMLDeclaration.system_id):\n\t\tpass\n\telse:\n\t\td[\"dia\"]=Dia.parse(f.XMLDeclaration.system_id)\n\t\n\treturn d\n\n@webfinger.setter\ndef set_d"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\turl: dict = {}\n\tmatch = re.search(r'([^\\\"]+) -- (.*?)', document)\n\t\n\tif match:\n\t\tdiaspora_webfinger = match.group(2)\n\telse:\n\t\traise ValueError('invalid Diaspora webfinger')\n\t\n\tdocument = document.replace(diaspora_webfinger, '','.')\n\tdocument = document.replace(diaspora_webfinger, '','')\n\tdocument = document.replace(diaspora_webfinger,'', '.'"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdiaspora = set()\n\n\t# Read Diaspora JSON\n\tdata = io.StringIO(unicode(document, 'utf-8'))\n\tdecoded_document = json.loads(data.read().decode('utf-8'))\n\t\n\tif'version' in decoded_document:\n\t\tdiaspora = deserialize_json_diaspora_version(decoded_document['version'], decoded_document)\n\n\t# Read XRD or Json\n\ttry:\n\t\traw_document = io.StringIO(document)"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\ttry:\n\t\tresponse = json.loads(document)\n\texcept ValueError:\n\t\tmsg = \"not JSON format\"\n\t\traise DiasporaError(msg)\n\tif response is None:\n\t\tmsg = \"not json format\"\n\t\traise DiasporaError(msg)\n\tdigest = None\n\tif messageType == MessageType.INFO:\n\t\tdigest = _parse_http_digest(document)\n\telse:\n\t\tmsg = messageType.msg\n\t\traise DiasporaError(msg)\n\tif not digest:\n\t\tmsg"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdef extract_document(text: bytes, docidx: int) -> dict:\n\t\ttext = text.decode('utf-8')\n\t\tdoc_text = re.sub('\"(.*?)\";', ',', text)[1:]\n\t\tpage_content, _ = parse(\n\t\t  'https://page.diaspora.io/webfinger/diaspora_federation/doc/' + docidx + \"/\"\n\t\t+ '\"@html|json' + '\\'\\'html|json', doc_text)\n\t\tpage_contents = page_content"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t# Parse document content from the webfinger.\n\treturn {\"body\":document}\n\n\n\nimport json\nfrom bs4 import BeautifulSoup\n\nDEFAULT_URL = 'http://diaspora.github.io/diaspora_federation/resources/'+'{}'.format(DEFAULT_VERSION)\n\n\ndef build_user_response(user):\n\t\"\"\"\n\t\tDetermine how to return user information.\n\t\t\n\t\tIf the user has a valid Facebook ID, that will be returned. \n\t\tOtherwise, the username of the user will be returned"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\ttry:\n\t\tresponse = requests.post(url, data=textwrap.dedent(\"\"\"\\\n\t\t\tfederation_webfinger:\n\t\t\t{\n\t\t\t\t\"user\": \"diaspora-webfinger:%s\"\n\t\t\t\t}\"\"\" % document), json=True)\n\t\t\n\t\tif response.status_code == 200:\n\t\t\tjson = response.json()\n\t\t\t\n\t\t\treturn json\n\t\telse:\n\t\t\treturn False\n\n\texcept:\n\t\treturn False\n\n\t\nif __name__ == '__main__':\n\t\n\ttry:\n\t\t\n\t\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\treturn None\n\t\n\t\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\treturn json.loads(document.replace(\"'\", '').replace(\"'\", \"''\"))\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tparsed_docs = dict()\n\tdoc_dict = dict()\n\t# Split by whitespace\n\t\n\tdoc_list = list(dedent(document)).split()\n\tfor _doc in doc_list:\n\t\tif _doc.strip():\n\t\t\tdoc_dict[_doc] = dict()\n\t\t\t# First, parse the list into fields\n\t\t\tfields = _doc.split()\n\t\t\tfor field in fields:\n\t\t\t\tfields.remove(field)\n\t\t\t\tm = re.match(r'_([\\w\\-]+)-(\\d+)', field)\n\t\t\t\t"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn get_webfinger_document(handle)\n\texcept StandardError as err:\n\t\tLogger.warning(\"Try to retrieve a webfinger document failed -- returning an empty document.\")\n\t\treturn \"\"\n\nclass Command(BaseCommand):\n\thelp = \"Generates a document from a set of links\"\n\n\tdef add_arguments(self, parser):\n\t\tparser.add_argument(\"-d\", \"--dest\", dest=\"dest\", type=str)\n\t\tparser.add_argument(\n\t\t\t\"-e\", \"--email\", dest=\"email\", default=\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\traise NotImplementedError\n\n\ndef update_webfinger_document(handle: str, url: str, hash: str, body: str):\n\t\"\"\"\n\t\tUpdates an existing webfinger document with the given data, e.g. to be signed.\n\t\tReturns a list of {handle, signature} pairs to pass on the PEM file, if the operation fails with a key error.\n\t\t:param  handle: A file handle that contains the downloaded PEM file\n\t\t:param  url: The URL of the PEM file from which the data will be fetched\n\t\t:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn urlquick.get(urllib.parse.urljoin(\n\t\t'https://webfinger.org',\n\t\t'http://verifier-femto.at/vfy-dok.json?handle={}').format(\n\t\t\thandle,\n\t\t),\n\t\theaders={'Accept': 'application/json'},\n\t\tjson=json.loads(base64.b64decode(urlopen('https://webfinger.org').read().encode('utf-8')))['data'],\n\t)\n\n\ndef remove_existing_document_for_pass"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn handle.read_frame(\n\t\t\"FAA\",\n\t\tframe_id=frame_id,\n\t\ttimestamp=\"UTC\",\n\t\ttpm_class=\"TPM\",\n\t)[\"Data\"].split(b\"\\n\")[0].strip()\n\n\ndef try_retrieve_fingerprint_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve a fingerprint document as returned in the webfinger endpoint. Does not raise if it fails.\n\t\"\"\"\n\treturn handle.read_frame(\n\t\t\"FPF\",\n\t\tframe"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn send_webfinger_request_raw(handle, \"/webfinger/{}/document_name\".format(handle), [\"text/html\"])\n\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tlog.debug(f\"Fetching WebFinger info for {handle}\")\n\t\tfh = self.fh_proxy if handle.startswith(\"https\") else open(handle, READ)\n\t\treturn self.to_dict(fh)\n\texcept Exception:\n\t\treturn None\n\t\n\t\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t# try for full uri (e.g. http[s]://example.com/), but does not\n\t# get a valid file descriptor if uri is local /\n\t#\n\t# in the future, the file descriptor will be closed when we\n\t# call #definitely_close the other end of the connection\n\t#\n\t# to avoid getting the file descriptor from the database every\n\t# time, it is done by using the url to make this function\n\t# return the file descriptor by passing a handle\n\tpass\n\t# http:\n\t#   http:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t#TODO - implement generic error handling.\n\treturn handle_retrieve_webfinger_document(handle)\n\ndef get_webfinger_document(handle: str) -> str:\n\treturn handle_retrieve_webfinger_document(handle).value\n\n\ndef get_current_user_webfinger_document() -> str:\n\t\"\"\"\n\tIf the current user has multiple web-finger documents, return the\n\tfirst of them to use as the fingerprint.\n\t\"\"\"\n\treturn get_webfinger_document(sys.stdin.readline().strip())\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tdocument_id = uri.PathParametersEntry('href')\n\ttry:\n\t\tdocument = httplib.Failed.get(handle, document_id)\n\texcept httplib.Failed as exception:\n\t\tlogger.error('Could not retrieve document from the server: %s', exception, exc_info=True)\n\t\treturn None\n\tif(document.code == 200):\n\t\treturn document.body\n\telif(document.code == 404):\n\t\tlogger.debug('Webfinger document does not exist on the server')\n\t\treturn None\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treq = HTTPRequest()\n\treq.header[\"Authorization\"] = request_header_to_string(authorization)\n\n\twith urllib3.connection.HTTPConnection(URL, timeout=timeout) as conn:\n\t\tfor _ in range(max_retries):\n\t\t\ttry:\n\t\t\t\tconn.request(\"GET\", URL, {}, None)\n\t\t\texcept (socket.timeout, urllib3.exceptions.HTTPError):\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\tif conn.getresponse().status == 200:\n\t\t\treturn handle\n\traise Exception(\""}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\theaders = {'x-www-authenticate': 'token'+ credentials.token}\n\t\n\tresponse = http.post(url.url(url.path, url.kwargs, handle), headers=headers)\n\t\n\tjson = json.loads(response.text)\n\t\n\treturn json\n\t\n\t\n\ndef get_diaspora_webfinger(filename):\n\t\"\"\"\n\tRetrieve a remote Diaspora webfinger document.\n\t\t\n\t:param filename: Diaspora webfinger document path (string)\n\t:returns:"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tpass\n\ndef retrieve_and_parse_databases(local_handle):\n\t\"\"\"\n\tretrieve a and parse a remote datacafe library\n\t\n\t:arg local_handle: Local handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tpass\n\ndef read_diaspora_webfinger_document(handle):\n\t\"\"\"\n\tRead a Diaspora webfinger from local memory handle.\n\n\t:arg handle: Local handle to retrieve\n\t:returns: file object\n\t\"\"\"\n\tpass\t\n\ndef read_all_diaspora"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\treturn get_and_parse_diaspora_webfinger(handle, dp_config)\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\timport requests\n\tr = requests.get(\"http://diaspora.org/get_di.html?access_token=\" + handle)\n\tparsed = parse_webfinger(r.content)\n\tif parsed:\n\t\treturn parsed\n\telse:\n\t\treturn {}\n\ndef create_user_configuration(handle):\n\timport json\n\tfrom.client import json_request\n\tresult = json_request(f\"{handle}/configuration\")\n\tif result['Status'] == 200:\n\t\treturn result\n\telse:\n\t\treturn {}\n\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tdata = _retrieve_and_parse_diaspora(handle)\n\tdata['url'] = data['content']\n\treturn data"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\thandle = int(handle)\n\t\n\t# Extract headers from a remote page\n\tcontent = json.loads(requests.get(URL+handle).content)\n\t\n\t# Parse the Diaspora webfinger document and return as dict\n\treturn content['doc']['content']\n\ndef generate_fetch_url_tokens_dict(tokens_data): #TODO: Make this work!\n\t\"\"\"\n\tGenerates a dict containing hashes of the given tokens.\n\t\n\t:param tokens_data: list of tokens parsed from their"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\twebfs = {\n\t\t\"xml_file\": \"diaspora.webfinger\",\n\t\t\"content_type\": \"application/xml\",\n\t\t\"type\": \"diaspora\",\n\t\t\"handle\": handle\n\t}\n\treturn get_webfs_url(webfs=webfs)\n\n# def download_diaspora_webfinger(download_handle):\n# \t\"\"\"\n#\t\tDownload a remote Diaspora webfinger document.\n#\t\t\n#\t\t:arg download_handle: Remote handle to download\n#\t\t\"\"\"\n# \t\turl = retrieve"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t# get the name to query for\n\tname = handle.split('@', 1)[1]  # split off the @\n\t# get the content\n\tcontent = get_raw_content(handle) \n\t\n\tdiasporaurl = webfinger.search_content(name, content)\n\t\n\t# get the metadata\n\ttry: \n\t\tdaspora_url = diasporaurl.split('/', 1)[1]\n\t\t\n\t\tdiaspora = get_raw_content(daspora_url)\n \texcept:\n\t\t#"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tresp, content = fetch(\n\t\t\"http://diaspora.org/webfinger/parse\",\n\t\t\"diaspora\"\n\t)\n\t\n\tparser = etree.XMLParser(remove_blank_text=True)\n\tparse_xml_tree(content, parser)\n\n\treturn etree.parse(StringIO(content))"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\td = {}\n\turl = \"http://diaspora.com/api/v1/json/diaspora/{}\".format(handle)\n\t\n\tlog.debug(\"Downloading remote Diospora webfinger document from %s...\" % url)\n\t\n\tresp = requests.get(url)\n\tresp.raise_for_status()\n\t\n\t# Parse and get diaspora dictionary\n\tdic = dict()\n\tlog.debug(\"Parse remote Diospora webfinger document from %s... \" % url)\n\td"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom Diaspora import diaspora, client, exceptions, xrd\n\tif host not in diaspora.hosts():\n\t\traise exceptions.RequestError(\"Invalid remote Diaspora host: %s\" % (host))\n\txrd_obj = diaspora.xrd.XRD(host)\n\t\n\treturn client.XRD(xrd_obj)\n\n\n@retry\ndef retrieve_diaspora_host_meta_by_id(host_id):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document by id."}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tresponse = requests.get(host + \"/api/v0/host-meta/default\", params = {\"method\": \"retrieve_diaspora_meta\", \"retry\": retries}) #@UndefinedVariable\n\treturn xrd.XRD(json=response.json())\n\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\tfrom diaspora.connector import XRD\n\treturn XRD(host).get(\"xrds\", \"\", {})\n\ndef update_diaspora_host_meta(host, local_version, meta_data):\n\tfrom diaspora.connector import XRD\n\tXRD(host).update(local_version, meta_data)\n\ndef create_diaspora_host_meta(host, local_version, meta_data, metadata):\n\tfrom diaspora.connector import XRD\n\treturn XRD(host).store"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn host_meta.get_remote_metadata(host)\n\treturn diaspora.get_meta(host)\n\t\n\t\ndef get_remote_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn host_meta.get_remote_metadata(host)\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tlog.debug(\"Retrieving DiSpora host-meta from %s\", host)\n\tmeta = None\n\tmeta_url = \"http://%s:%s@di.susan.cs.unu.edu.ar/diaspora/v2/meta/json/%s\" % (host, port, host)\n\tlog.debug(\"Retrieving %s from %s\", DiasporaHost.MetaDocument, meta_url)\n\n\tmetadata = yield self.get(meta_url)\n\n\ttry:\n\t\tmetadata_json = json.loads("}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn dia_das.get_diaspora_meta(host)\n\n\ndef retrieve_users(host):\n\t\"\"\"\n\tRetrieve a remote users document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn dia_das.get_users(host)\n\n\ndef retrieve_clicks(host):\n\t\"\"\"\n\tRetrieve a remote clicks document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\turi = \"%s/api/diaspora/host-meta\" % host\n\tr = requests.get(uri)\n\tr.raise_for_status()\n\t\n\turi = \"%s/api/diaspora/meta\" % host\n\tr = requests.get(uri)\n\tr.raise_for_status()\n\t\n\treturn XRD(requests.codes.OK, r.json())\n\ndef update_diaspora_host_meta(host, meta_doc):\n\t\"\"\"\n\tUpdate an existing remote Diaspora"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\txrd = None\n\tif isinstance(host, str):\n\t\thost = get_host(host)\n\t\n\ttry:\n\t\txrd = host.xrd\n\texcept:\n\t\treturn None\n\t\n\txrd.retrieve_diaspora_host_meta()\n\t\n\treturn xrd\n\t\n__all__ = ('RetrieveXRD_meta', 'RetrieveXRD_host',)\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn retrieve(host + '/' + DiasporaHostMetaResource)\n\ndef retrieve_meta_doc(meta_json_data, meta_dict):\n\t\"\"\"\n\tConvert meta_dict from 'dict' back to json.\n\tWhen a meta_dict has an '_links' field it is a list.  The '_links' must hold the\n\tfull path of the resource, and 'links' is the meta_dict item which contains\n\tthe \"rel\" header value.\n\n\t:arg meta_json_data: JSON dict\n\t:type meta_json"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\thttp_request, http_path = self._get_http_document(host)\n\tret = self._retrieve(http_request, http_path)\n\treturn ret if ret else XRD(self)\n\n#\n# The main module.\n#\n\nXRD = XRD()\n"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttry:\n\t\tif data:\n\t\t\tr = requests.post(url, data=data, timeout=timeout, **kwargs)\n\t\telif type(data) is bytes:\n\t\t\tr = requests.post(url, data=data.encode('utf-8'), timeout=timeout, **kwargs)           \n\t\telif type(data) is file:\n\t\t\tr = requests.post(url, data=data, files={\"file\": data}, timeout=timeout, **kwargs)\n\t\telif type(data) is list:\n\t\t\tr = requests.post"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t#TODO: implement timeout\n\t\n\tr = requests.post(url, data=data, data_type=\"json\", headers={\"Authorization\": f\"Bearer {token}\"}, timeout=timeout, *args, **kwargs)\n\treturn r.text if isinstance(data, dict) else r.json() if isinstance(data, bool) else None\n\n\n\t\n#------------------------------------------------------------------------------\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tbody = {\"data\": urlencode_entity(data)}\n\tparams = OrderedDict(**kwargs)\n\t\n\t# Add headers to the message and submit request\n\tmsg = Request(method, url, **params)\n\tmsg.headers[\"Accept\"] = \"application/json\"\n\tmsg.headers[\"Content-Type\"] = \"application/json\"\n\tr = requests.post(url, data=body, timeout=timeout, auth=requests_auth(*msg.headers))\n\tlogging.debug(r.text)\n\tstatus = r."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tdef _send(request, url, data, timeout=10, method=\"post\"):\n\t\tif type(timeout) is list:\n\t\t\ttimeout = self._add_timeout(timeout)\n\t\trequest.prepare(url + \"/\".join([\"\", \"POST\", \"%s\" % method, \"%s\" % url, data, kwargs]), input=data, timeout=timeout)\n\t\tstatus_code, error = __import__(method).from_iter(request.iter)\n\t\terror = getattr(error, \"status_code\", None)\n\t\t#print \""}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t# Send an xml header, to be used by send_document\n\tif 'XML' in kwargs:\n\t\tdel kwargs['XML']\n\tif 'format' in kwargs:\n\t\tdel kwargs['format']\n\t\n\treturn requests.post(\"{0}://localhost\".format(url), **kwargs)\n\t\n\t# Create a header from the data dict\n\tdata['Content-Type'] = 'application/xml'\n\t\n\ttry:\n\t\theaders = {}\n\t\tif 'Content-Length' in data or"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tsend = requests.post(build_url(url, data, method, *args, **kwargs), *args, **kwargs)\n\treturn send.json()\n\n#: Supported HTTP 200 OK responses\nok = lambda x: 200 == x.status_code\n\n#: Supported HTTP 400 errors\nunexpected_content_error = lambda x: 400 == x.status_code\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\theaders = {'Accept': 'application/json', 'Content-Type': 'application/json', 'content-type':'application/json; charset=utf-8'}\n\tresponse = requests.post(url, data=data, headers=headers, timeout=timeout, method=method, *args, **kwargs)\n\t\n\tif hasattr(response, 'ok'):\n\t\treturn (response.headers['status'], None)\n\t\n\treturn (None, response.json()['message'].lower())\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tresp = requests.post(url, data=json.dumps(data), timeout=timeout, **kwargs)\n\tcode = resp.status_code\n\terr = resp.text\n\tif err:\n\t\traise RuntimeError(\"Error {}: Send to {} failed; {}\".format(err, url, code))\n\treturn code, err\n\t\n@wrap_exc\ndef send(url, data=None, method=\"POST\", timeout=10, *args, **kwargs):\n\t_, err = send_document(url, data, method=method, timeout="}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\treq = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\tresponse = req.raise_for_status()\n\treturn (response.status_code, response.json())\n\t\n\t\ndef send_file(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tHelper method to send a file via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\tif isinstance(data, dict):\n\t\tdata = urlencode(data)\n\t\n\treturn requests.post(url, data=data, timeout=timeout, data_type=request_type, *args, **kwargs)\n\t\n\t\nif __name__ == '__main__':\n\t\n\tdef main():\n\t\t\n\t\timport threading\n\t\t\n\t\tfrom urlopen import urlopen\n\t\t\n\t\twith _blocking_lock:\n\t\t\t\n\t\t\turl = \"http://www.google.com/\"\n\t\t\t\n\t\t\twhile url:\n\t\t\t\t\n"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tpass\n\n\t\n\ndef dict_replace(dic, val, key, *keys):\n\t\"\"\"\n\t\treturn the original dictionary if the value is found\n\t\treturn the new (modifable) dictionary\n\t\treplace the keys of a dictionary\n\t\t\n\t\t:param dic: a dictionary object with new values\n\t\t:param val: a value to insert to the given dictionary\n\t\t:param key: first key in a chain of key that will store the value\n\t\t:param keys: sub keys in the keys chain\n\t\t\"\"\"\n\t\tpass\n\n\n\ndef dict_sub(dic"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic[key] = val\n\tfor i in range(len(keys)):\n\t\tkeys[i] = i\n\tif len(keys)!= 0:\n\t\tdic[keys[0]] = dic[key]\n\treturn\n\n################################################################################\n####################### PUBLIC API #####################################\n################################################################################"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor value in keys:\n\t\tdic = dict_update(dic, val, key, *value)\n\t\tdic_to_key = key + '.' + value\n\t\tif dic.get(dic_to_key) == None: \n\t\t\tdic.update({dic_to_key:val})\n\t\telse:\n\t\t\tdic = dict_update(dic, val, dic_to_key, *value)\n\treturn dic\n\ndef dict_insert_dup(dic, val, key, *keys):\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key in keys:\n\t\ttry:\t\t\n\t\t\tdic.__setitem__(key, val)\n\t\texcept:\n\t\t\tpass\t\t\t\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\treturn dic.__setitem__(key, val)\n\t\n\t\ndef dict_get(dic, key, default=None):\n\t\"\"\"\n\tget nested key\n\t\n\tthe value for a nested key is returned from the given dictionary only when\n\tthe key does not exist in that dictionary, or from which the key does not\n\texist.  If the value does exist but is not in the dictionary, this value is\n\treturned directly.\n\n\t:param default: the value if the key does exist but is not in the\n\t   dictionary\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys:\n\t\treturn\n\t\t\n\tif len(keys) == 1:\n\t\tpass\n\t\t# key is always the first key for nested keys\n\telif len(keys) == 2:\n\t\t# check if there is a common key\n\t\t# for now, just check first\n\t\tkey_chain = sorted(dic.keys())\n\t\tcommon_key = key_chain[0]\n\t\tval0 = list(k.value for k in keys)\n\t\tval0.append(val)\n\t\tfor index, key_name in enumerate(common_key"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tdic.update({key: val})\n\tfor k in dic.keys():\n\t\tfor k2 in dic[k].keys():\n\t\t\tdic[k2].update({val: dic[k]})\n\ndef dict_update(dic, values, update_keys):\n\t\"\"\"\n\tupdate a dictionary with a value or list of values separated using\n\tsemicolon sign \";\", return new dictionary with the updated values\n\n\t:param dic: a dict without a dictionary key\n\t:param values: a list of (key, value)"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdict_copy(dic, dic, key)\n\tif dic:\n\t\tdict_insert(dic, val, key, *keys)\n\"\"\"\n\tif not dic:\n\t\treturn\n\t\t\t\n\tfor key in keys:\n\t\tdic = dict2list(dic, val, key)\n\t\t\n\t\tdic = dict_insert(dic, val, key)\n\t\n\treturn\n\"\"\"\n\tFor an example, see other method's \n\t\n\tdic = {'val': '1', 'key1.val': '"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key in key:\n\t\tif not dic.has_key(key):\n\t\t\tdic[key]={}\n\t\tdic=dic[key]\n\t\tfor cnk in key.split('.'):\n\t\t\tvalue = dic\n\t\t\tif cnk!= '.':\n\t\t\t\tvalue = value.get(cnk)\n\t\t\tif not value and type(cnk) == type(''):\n\t\t\t\tvalue = dic{}\n\t\t\telif not value and type(cnk) == type(undefined):\n\t\t\t\tval.append(cnk)"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tresult = dict()\n\tsub_dic = {}\n\t\n\tfor key in keys:\n\t\tsub_dic[key] = dict()\n\t\tresult[key] = val\n\t\t\n\t\tfor subkey in key.split('.'):\n\t\t\tsub_dic = result\t\n\t\t\tsub_dic[key] = val[subkey]\n\t\n\treturn result\n\t    \ndef dict_remove(dic, val, *keys):\n\t\"\"\"\n\t    remove a value of a nested key\n\t\n\t:param dic: a"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfile_names = []\n\n\tfor x in settings_dirs:\n\t    new_file = '{}.ini'.format(spec_option + '.ini')\n\t    file_names.append(new_file)\n\n\treturn file_names\n\n# =========================\ndef main():\n\t\"\"\"Create the new IniType \"\"\"\n\t# Find out the location to save the file\n\n\tsettings_dir = '{}/{}'.format(CONFIG_DIR, SPEC_SETTINGS)\n\n\t# IniType has to be created in the settings directory (specify"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tnew_file_names = list_of_files(\n\t\t\tnew_filedir_name=settings_dirs,\n\t\t).map(parse_inifile_with_options)\n\tnew_file_names = [f() for f in new_file_names]\n\treturn IniType(new_file_names)\n\n\ndef validate_if_inifile_is_valid(settings_dirs, file_name, new_file_name):\n\t\"\"\"\n\tReturns True if ini file name is valid\n\t\"\"\"\n\ttry:\n\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfiles_dir = os.path.join(settings_dirs[spec_option['settings_key']])\n\tfiles = glob.glob(os.path.join(files_dir, \"*.ini\"))\n\treturn IniType(files, spec_option)\n\n\ndef build_ini_type(spec_option):\n\t\"\"\"\n\t\tReturns a complex type that can be serialized\n\t\"\"\"\n\tfiles = glob.glob(os.path.join(spec_option['settings_key'], \"*.ini\"))\n\tfiles = [build_ini_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t#TODO: implement the rest of list_of_file_names\n\t\n\t#make sure there are no spaces in the name\n\tif'' not in spec_option:\n\t\t\n\t\tif not spec_option.upper().endswith('_FILE_NAME'):\n\t\t\treturn None\n\t\t\t\n\t#name the group\n\tfile_name = 'F'\n\tif (settings_dirs is not None):\n\t\tfile_name = spec_option+'_FILE_NAME'\n\n\tgroup_tag = 'G'\n\tif (settings_dir"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tlogging.debug(\"list_of_file_names\")\n\tfile_names_to_export = []\n\tfor spec_name in spec_option:\n\t\tfile_names_to_exp = spec_parameter_type_to_file_names_for_spec(spec_name)\n\t\tfor fw in file_names_to_exp:\n\t\t\tfile_names = []\n\t\t\tlogging.debug(\"spec name: %s\", spec_name)\n\t\t\tlogging.debug(\"   file name: %s\", fw)\n\t\t\tlogging.debug(\""}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tif spec_option == 'ALL':\n\t\tspec_type = str(types) + '*'\n\telif spec_option == 'FILE':\n\t\tspec_type = str(types + '%') + '*'\n\tif spec_option=='TYPE':\n\t\t# list of all *.ini types\n\t\ttyp_str = str(types)\n\t\twhile not str(typ_str) in list_of_file_names:\n\t\t\ttyp_str = typ_str.replace('*.ini','%')\n\t\t# put the filename together\n\t\treturn re.sub"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tlist_of_file_names = []\n\tfile_name = None\n\tfor directory in settings_dirs:\n\n\t\t# make list of file_paths used for each type\n\t\tlist_of_file_paths_for_type_current_file = ['type{}'.format(directory_index) for directory_index in range(0, len(directory))]\n\t\tlist_of_file_paths_for_type_last_file = []\n\t\tif spec_option == 'default':\n\t\t\tlast_dir = \"\"\n\t\telse:\n\t\t\tlast"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\treturn IniType()\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfor dir in settings_dirs:\n\t\tfor files in os.listdir(dir):\n\t\t\tif files.endswith(spec_option):\n\t\t\t\tyield(files)\n\telse:\n\t\traise Exception('Cannot find %s in config dir' % spec_option)\n\ndef get_settings_dict(settings_dirs):\n\t\"\"\"\n\tGet the global settings\n\t\"\"\"\n\tsettings_dict = {}\n\tfor files in list_of_file_names(settings_dirs,'settings'):\n\t\tfilename = getpynames.get"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# check whether there is a sub directory\n\tfor iniFileName in os.listdir(settings_dirs['ini_location']):\n\t\tos.chdir(settings_dirs['ini_location'] + iniFileName)\n\t\t\n\t\t# iterate over sub directories\n\t\t# todo: change ini types in real life\n\t\tfor spec_file_name in os.listdir('spec'):\n\t\t\tspec_name = os.path.splitext(spec_file_name)[0]\n\t\t\tif spec_name == spec_option:\n\t\t\t\t"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\ttry:\n\t\treturn cls._lookup('ansible_config_manager')\n\texcept AttributeError:\n\t\ttry:\n\t\t\treturn cls._lookup(\"ansible_config\")\n\t\texcept KeyError:\n\t\t\traise AnsibleError(\"failed to find ansible_config manager, failed to get config, check your config and try again\")\n\nclass ConfigManager(object):\n\t\"\"\"\n\tInitializations\n\t\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\traise NotImplementedError()\n\nANSIBLE_CON"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tmanager = cls._manager\n\treturn manager.config_manager\n\t\ndef get_action(client, resource, action=None, **params):\n\t'''\n\t\tUsed to implement ansible actions\n\t\t'''\n\t\n\tmanager = cls._manager\n\treturn manager.get_action(client, resource, action=action, **params)"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls()\n\n\nclass CliClient(CLI):\n\t\"\"\"\n\tCLI class that allows you to build Ansible commands\n\t\"\"\"\n\n\tdef __init__(self, argument_spec_cls=dict(), ansible_config_manager=None):\n\t\tsuper(CliClient, self).__init__(argument_spec_cls)\n\t\tif ansible_config_manager is not None:\n\t\t\tself.config_manager = ansible_config_manager\n\t\telse:\n\t\t\tself.config_manager = CliConfigManager\n\n\tdef parse"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.get_object(cls.RESOURCE, cls.ANSIBLE_CONFIG)\n\n###### UTILITY FUNCTIONS ###########################################################\n\n\ndef ansible_version(module):\n\t\"\"\"\n\t\tThe `ansible-playbook --version` command returns the version of all Ansible core modules for this version of Ansible.\n\t\tThis is useful to check which release-branch the module has been ported from so that the user can identify any changes.\n\t\t@see:\n\t\t\thttp://docs.ansible.com/ansible/ansible"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls(\"ansible_config_manager\")\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls().config_manager()\n\n\n\nclass ModuleManager(object):\n\tdef __init__(self, *args, **kwargs):\n\t\tself.module_system = AnsibleModule(\n\t\t\targument_spec=dict(\n\t\t\t\tpath=dict(required=True),\n\t\t\t),\n\t\t\tsupports_check_mode=True\n\t\t)\n\n\t\tself.module_system.debug = self.module_system.debug + 1\n\t\tself._cls = cls(self.module_system)\n\n\t@property\n\tdef module_name(self):\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t# This was added to fix #4764\n\tglobal ansible_config_manager_manager\n\tif not ansible_config_manager_manager:\n\t\ttry:\n\t\t\tfrom ansible.playbook.play_context import PlayContext\n\t\t\tfrom ansible.playbook.task_include import TaskInclude\n\t\t\tfrom ansible.cli import CLI\n\t\t\tansible_config_manager_manager = AnsibleConfigManager()\n\t\t\tansible_config_manager_manager.load_playbook(cls.basedir + cls.ext)\n\t\t\tansible_config_manager"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tresult = ''\n\tcls.get_ansible_module()\n\tresult = cls.get_ansible_cfg()\n\n@register_cls('config_manager')\nclass AnsibleConfigManager(AnsibleConfigTransforms = []):\n\n\t\"\"\"\n\tAdds a new class for the ansible_config module\n\tNote: a normal AnsibleConfig class needs a get_ansible_config() method, but this\n\t\t\tdoes not exist in future versions of Ansible yet.\n\t\tAlso, config_manager depends on the ansible config to transform the data.\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tdef __init__(self, **kw):\n\t\t\"\"\"\n\t\tInitialize the ansible config manager.\n\t\t\"\"\"\n\n\t\t# Setup the required variables and parameters to use the call for the config manager\n\t\tself.__call_managers = dict()\n\n\t\tself.__set_host_params()\n\n\t\t# Get the hosts\n\t\tself.hosts = cls.hosts()\n\n        \t# get the config manager for the hosts\n\t\tself.host_manager = cls.host_manager(hosts=self.hosts)\n        \t\n\tdef _get_"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls(base=CLIExecutor())\n"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.__workspace_manager\n\n\n@workspace_manager.register(Module.NAME_MODULE, \"workspace manager\")\nclass ModuleWorkspaceManager(object):\n\t\"\"\"\n\t\tCreates the manager object.\n\t\"\"\"\n\n\tdef __init__(self, workspace_name):\n\t\t\"\"\"\n\t\t\tinitialise\n\t\t\t\"\"\"\n\t\tself.name = workspace_name\n\n\t@property\n\tdef modules(self):\n\t\t\"\"\"\n\t\t\tget modules\n\t\t\t\"\"\"\n\t\treturn self.__manager.modules\n\n\tdef __new__(cls,\n\t\t\t\t"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn getattr(cls, '_workspace_manager', None)\n\n\t\nclass Run(MetaEvaluation):\n\t\"\"\"\n\tMeta-evaluation of the run method.\n\t\n    In this case, a single workspace manager is present in the class attribute\n\t`WorkspaceManager`.\n\n\t:Parameters:\n\t\t- `cls` --\n\t\t\tThe class that the run is part of, or `None` if the\n\t\t\trun belongs to the current workspace manager.\n\t\"\"\"\n\tdef __init__(cls, **options):\n\t\t\"\"\"Constructor\"\"\""}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn kwid.workspaces()\n\n\t\nclass CellMimeXmlUtil(kwid.CellMimeUtil):\n\t\n\tdef _read_mime_xml_file(self, fname):\n\t\t\"\"\"\n\t\tReads a single file using the KWidget file system (this doesn't work on Windows).\n\t\t\n\t\tSee: http://www.codealliance.com/p/kwid/wiki/kfileutils\n\t\t\n\t\tParameters:\n\t\t\tfname - the file which is to be read\n\t\t\t\n\t\tRaises:\n\t\t\tIOError -"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._ptr\n\n\t\nclass Image(core.Image):\n\t\"\"\"\n\t\tGets/creates/deletes the specified image. Any changes to image file\n\t\tchanges will be written to the database immediately. You can still use\n\t\tthis to change the contents of an image only after you first created\n\t\tit.\n\n\t\tIMPORTANT:\n\t\t\n\t\tOn some systems this image is stored as an uncompressed uncompressed image\n\t\tfile. On all other systems an application must call create_image.\n\t\"\"\"\n\tdef __init__(self, path, file_name"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t@classmethod\n\tdef instance(cls):\n\t\t\"\"\"\n\t\t\tReturns the instance of this class\n\t\t\"\"\"\n\t\treturn cls\n\n\t@classmethod\n\tdef list(cls):\n\t\t\"\"\"\n\t\t\tReturns a list of workspace managers.\n\t\t\"\"\"\n\t\treturn [w for w in workspace_manager().list() if w.manager.name!= 'workspace_manager']\n\n\t@classmethod\n\tdef create(cls, manager):\n\t\t\"\"\"\n\t\t\tCreates a workspace manager with the given manager (as well as the\n\t\t\tname).\n\t\t\t\"\"\"\n\t\t"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tif type(cls) is not WorkspaceManager:\n\t\traise TypeError, \"not a WorkspaceManager class\"\n\treturn cls \n\n\n\n# TODO write docs\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.__manager\n\n\t\n\t\nclass workspace_manager(object):\n\n\tdef __init__(self, workspace_name, manager):\n\t\tself.workspace_name = workspace_name\n\t\tself.manager = manager\n\t\n\tdef create_new_workspace(self, workspace_name):\n\t\tself.manager.put_workspace(self.workspace_name)\n\t\t\n\tdef delete_workspace(self, workspace_name):\n\t\tself.manager.rm_workspace(workspace_name)\n\n\n\tdef get_workspace"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._workspace_manager or cls.create()\n\n\n@inherit_doc\nclass WorkspaceManagerMixin(Workspace):\n\t\"\"\"\n\t.. method:: workspaces_manager\n\n\t\tThis base mixin can get the global workspace manager.\n\t\"\"\"\n\n\treturn workspace_manager(cls)\n\n@inherit_doc\nclass BaseManager(object):\n\t\"\"\"\n\t.. method:: make_workspace(path, parent_path, **kwargs):\n\n\t\tCreate a new workspace manager given an initial path. The manager will\n\t\tnot work"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.__attr__(cls._get_property('_workspace_manager'))\n\n\n\n\nclass workspace_info(object):\n\t\"\"\"\n\tThe WorkspaceInfo class provides some useful info\n\tfor the workspace manager.\n\t\n\tThis object only takes care of reading the\n\tworkspace information from disk storage for\n\ta given workspace.\n\t\"\"\"\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\t\n\t\tParameters\n\t\t----------\n\t\tname : str\n\t\t\tThe name of the workspace.\n\t\t\"\"\"\n\t\treturn cls.__"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tif hasattr(cls, \"ws_manager\") and not cls.ws_manager:\n\t\tcls.ws_manager = cls.get_class(cls).ws_manager_factory(cls)()\n\treturn cls.ws_manager\n\n\ndef save_plugin_dependencies(plugin_manager, module):\n\t\"\"\"\n\tSaves dependencies of a list of Plugins. Each plugin depends on another module.\n\t\"\"\"\n\tfor plugin in plugin_manager:\n\t\tplugin.dependencies = getattr(plugin.dependencies, module)\n"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif not hasattr(os, \"path\") and os.name!= \"posix\":\n\t\tlog.warning(\"No path attribute for plugins\")\n\treturn plugins.get_plugin(\"Plugins\")\n\n\t\n\t\n\t\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif cls.__name__ == \"ExtPlugin\":\n\t\treturn cls.__plugin_manager()\n\telse:\n\t\treturn cls.__plugin_manager_instance\n\n\ndef get_instance(cls):\n\t\"\"\"\n\tGets the plugin manager instance.\n\t\"\"\"\n\treturn cls.__plugin_manager_instance\n\n#####################################################################################################\n\n\ndef enable(*args):\n\t\"\"\"\n\tEnables a plugin.\n\t\"\"\"\n\tcls = None\n\tfor cls in args:\n\t\tif not re.match('^ExtPlugin"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn get_plugin_manager_from_manager(cls._plugins_manager)\n\n\ndef _get_list(cls, search_string):\n\t\"\"\"\n\tGet all of the plugins with the provided pattern.\n\n\t:param search_string:\n\t\tThe pattern to search for.\n\n\t:returns:\n\t\tThe list of plugins with the provided pattern.\n\t\tIf the pattern cannot be found, an empty list will be returned.\n\n\t\tIt is advisable to create a filter for plugins that do not follow\n\t\tthe syntax as noted by the class documentation, as that will prevent\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tplugin_manager = plugins.Manager()\n\tplugin_manager.load_plugins(cls.PLUGINS_DIR)\n\treturn plugin_manager\n\n\t\nclass TestPluginManager(unittest.TestCase):\n\t\"\"\"\n\tRuns all tests that should be run\n\t\"\"\"\n\t\n\tdef run(self, plugin_manager):\n\t\t\"\"\"\n\t\tRuns all tests and returns the list of fail conditions\n\t\t:param plugin_manager:\n\t\t:return:\n\t\t\"\"\"\n\t\tplugins_manager = test.testmethod(plugin_manager)\n\t\tfailure"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.plugin_manager\n\n\ndef get_install_files():\n\t\"\"\"\n\t\tInstalls the plugin.\n\t\t\"\"\"\n\tplugins = [plugin.__module__ + '.plugins' for plugin in cls.plugin_manager.all_plugins()]\n\tmodules_to_install = set(['plugintools.scrapper'])\n\n\tfor module in plugins:\n\t\tmodules_to_install.add(module)\n\n\tif modules_to_install:\n\t\tlogger.getHandlersDecorator().install_files([plugins, modules_to_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn Class.get( cls.name )\n\n# ============= EOF ====================================\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls._data['plugins']\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn plugins_manager\n\t\n\n@plugins_manager\ndef get_plugin(plugin_name):\n\t\"\"\"\n\tGets plugin by name\n\t\"\"\"\n\treturn cls.load(plugin_name)\n\n\ndef start_plugin(*args):\n\t\"\"\"\n\tCalled when the module is loaded\n\t\"\"\"\n\t# Call the method that returns instance of cls\n\n\treturn cls(*args)\n\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\timport plugins\n\t\n\treturn plugins.plugins_manager()"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn getattr(cls,'manager')\n\nclass Plugins(PluginsBase):\n\t\"\"\"\n\t\tThe plugins.\n\t\"\"\"\n\tcls = plugin\n\tmanager = plugins_manager\n\t__init__ = PluginsBase.__init__\n\n\tdef registerPlugin(self, pcls):\n\t\tif hasattr(pcls,'registerManager'):\n\t\t\treturn getattr(pcls,'registerManager')(*self.plugin_mappings)\n\t\telse:\n\t\t\treturn self.plugin_mappings\n\n\tdef addPluginLoader(self,"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not spec_content:\n\t\traise ValueError(\"Spec file is needed\")\n\t\n\terror_dict = parse_yaml(spec_content)\n\t\n\tif error_dict:\n\t\traise IRValidatorException(\"YAML file can't be validated\")\n\treturn error_dict\n\n\ndef parse_yaml(yaml_file):\n\t\"\"\"\n\t\tParses a YAML spec file string, and returns a dictionary of dictionaries that could be used with the ir.validator.IRValidator class\n\t:param yaml_content: YAM"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\timport re\n\tre_data = re.compile(r\"(\\([^:]*\\))\")\n\tlines = spec_content.split('\\n')\n\thas_mandatory = False\n\tfor line in lines:\n\t\ttext = ''\n\t\tfor word in re.split(r\"%s:.*\" % re_data, line):\n\t\t\tcontent = line.replace(word, text).split(',')\n\t\t\tif len(content) > 1:\n\t\t\t\t# check all words\n\t\t\t\tfor keyword in re.findall(r\"%s(?! )"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif None == spec_content:\n\t\te_msg = 'A mandatory file is required: \"%s\"'\n\t\tcls.INVALID_SPEC_CONTENT_MESSAGE = e_msg%(CLS.PATH)\n\t\traise cls.INVALID_SPEC_CONTENT_MESSAGE\n\t\n\t# validate required fields\n\tdata = validate_parameter_set(spec_content)\n\t# validate the data\n\treturn cls.validate_by_data(data)\n\n##\n# Private Utility\n##\n\n\ndef validate"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not isfile(spec_content):\n\t\traise IRValidatorException(\"Spec file '{0}' does not exist!\"\n\t\t\t\t\t\t\t\t\t.format(spec_content))\n\tif not os.path.exists(spec_content):\n\t\traise IRValidatorException(\"Spec file '{0}' is not a valid file!\"\n\t\t\t\t\t\t\t\t\t.format(spec_content))\n\tspec_file = File(spec_content)\n\tspec_file_data = read_file(spec_file)\n\tspec = cls(spec_file_data)\n\tif not ('"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tmissing = []\n\tif spec_content is None:\n\t\tspec_content = cls.get_spec_YAML_content(cls.get_test_name(), cls.get_test_config(), cls.get_test_description()).get(\"file\")\n\ttry:\n\t\tcontent = spec_content.text\n\texcept AttributeError:\n\t\tprint\"The file '{' missing in spec.\".format(spec_content)\n\t\traise IRValidatorException(\n\t\t\t\t\"{'file':'{1}'} missing in spec.\""}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not spec_content:\n\t\treturn None\n\n\tspec2filere = [('spec_fullpath', r'.*\\<.*\\>', True)]\n\n\tspec_path = os.path.join(os.path.dirname(__file__),'spec.yaml')\n\t\n\tvalidate_spec(spec_path)\n\t\n\tspec2content = re.findall(r'{{(.+?)}}', spec_content, re.MULTILINE)\n\tif len(spec2content) > 1:\n\t\traise IRValidatorException"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not spec_content:\n\t\treturn {}\n\trequired_data = {}\n\t\n\t# load YAML specification from file name and content\n\tspec_path = os.path.abspath(spec_content)\n\tspec_file = open(spec_path, 'r')\n\tspec_file_content = spec_file.read()\n\tspec_file.close()\n\t\n\t# validate that everything is a string\n\tif not isinstance(spec_file_content, str):\n\t\traise IRValidatorException('Spec content for \"{0}\" must"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfrom_file = os.path.expanduser(\"~\")\n\ttry:\n\t\tspec_content = open(from_file + \"/{}\".format(os.path.basename(spec_content))).read()\n\t\treturn cls.validate_from_content(spec_content)\n\texcept IOError:\n\t\traise IRValidatorException(\"spec content '{}' missing\"\n\t\t\"\".format(from_file))\n\t\n\t\ndef validate_from_sphinx(filename, spec):\n\tif not os.path.isfile("}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t_spec_file = spec_content.get('spec')\n\tif not _spec_file:\n\t\traise IRValidatorException(\"No path to YAML specification file found.\")\n\t\n\t_spec_content=yaml.load(_spec_file, Loader=yaml.FullLoader)\n\t\n\t# validate name\n\tname=spec_content.get('name', None)\n\tif name is None:\n\t\traise IRValidatorException(\"No name specified in specification file\")\n\tvalidator_data=yaml.load(_spec_content, Loader"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\treturn cls(spec_content=spec_content) \n\t\t\t\n\nclass YamlParserMixin:\n\t\"\"\"\n\tYAML parser mixin class\n\t\"\"\"\n\t@staticmethod\n\tdef parseYAMLString(string):\n\t\t\"\"\"\n\t\tYAML string parsing utility function\n\t\t\"\"\"\n\t\t\n\t\tparser = YAML(object_pairs_hook=parse_object_pair)\n\t\tparser.load(string)\n\t\t\n\t\treturn parser\n\t\t\n\t@staticmethod\n\tdef validate_from_content(cls, spec_"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t# Get the path from the supplied path if passed\n\t# otherwise use a full path as default\n\tif yaml_file is None:\n\t\tyaml_file = cls.DEFAULT_YAML_FILE\n\t\t# Read the file if it's not there\n\t\tif not os.path.isfile(yaml_file):\n\t\t\tyaml_file = cls.READ_YAML_FILE\n\t\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"File not a file: \"+yaml"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfile_path = os.path.abspath(yaml_file)\n\tif not os.path.isfile(file_path):\n\t\traise IRValidatorException('File does not exist.')\n\t\n\tcls._from_file(file_path)\n\t\n\treturn cls\n\n\nclass IRValidatorException(Exception):\n\tdef __init__(self, values):\n\t\tself.values = values\t\n\tdef __str__(self):\n\t\treturn \"\".join(['Invalid data for field %r: %r' % (k,"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tdata = {}\n\t\n\tif not yaml_file is None:\n\t\t\tfd = open(yaml_file)\n\t\t\ttry:\n                    for n in yaml.load_all(fd):\n                        data[n['type']]=n['data']\n            except ValueError as e:\n                    print('Warning, YAML error while loading {0}'.format(yaml_file))\n                    raise IRValidatorException(str(e))\n    \n\telse:\n\t\traise IRValidatorException('No YAML file specified')\n\t#"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\traise IRValidatorException('Invalid Parameter: must construct a yaml_file')\n\t\n\twith open(yaml_file, 'r') as yaml_file:\n\t\troot = yaml.load(yaml_file)\n\t\n\t# check that all mandatory fields are present in file\n\tif root.keys() - set(('url', 'title', 'description', 'language',)):\n\t\traise IRValidatorException('Mandatory field missing from YAML file.')\n\t\t\n\t#check data in Y"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tcls.load(yaml_file)\n\treturn cls.validate()\n\nclass IRStringSchemaValidator(IRBaseValidator):\n\t\"\"\"\n\tThe class that will be used to validate the string schema with YAML\n\t\n\tThis validator validates the string schema with YAML, the first\n\tconcrete class being the string, which contains the string schema.\n\tThe validator will also validate using the IR validator provided to\n\tit, which may be more specific than the one provided to this validator.\n\t\n\tIf"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"Input YAML file doesn't exist\")\n\twith open(yaml_file) as f:\n\t\tyaml_text = f.read()\n\ttry:\n\t\tvalidators = dict([x.strip() for x in yaml_text.split(\n\t\t\t\t\t\t\t\t\", \" + \"'\\\"'\")])\n\t\tvalidators.update(\n\t\t\tdict([(k, v.split(': '))\n\t\t\t\tfor k, v in dir(validators) if k.startsw"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\timport yaml\n\t\n\tcls.validate_yaml_file(yaml_file)\n\t\n\ttry:\n\t\twith open(yaml_file, 'r') as fp:\n\t\t\tconfig = yaml.load(fp)\n\t\t\tfor k, v in config.items():\n\t\t\t\tchecker = config[k]\n\t\t\t\t\n\t\t\t\tif not checker or not getattr(checker, 'get_required_fields', []):\n\t\t\t\t\traise IrValidatorException(\"Invalid YAML/Config\")\n\t\treturn config\n\texcept ValueError:\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is not None:\n\t\tyaml = yaml_file.read()\n\t\ttry:\n\t\t\tyaml = yaml.get('tokens', {})\n\t\texcept:\n\t\t\traise IRValidatorException('Invalid parser settings for the given YAML file')\n\telse:\n\t\tyaml = yamlml().get('tokens', {})\n\treturn yaml\n\n\nclass JsonValidator(IRValidator):\n\t\"\"\"\n\tLoads validate on JSON files or on other files \n\t\"\"\"\n\tIMPORT_FILE_"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom yaml import YAMLError as e\n\ttry:\n\t\tvalidate_schema = cls.validate\n\t\twith yaml.RoundTripIterator(yaml_file) as stream:\n\t\t\tassert len(stream.tags) >= 1\n\t\t\tfor raw in stream.tags:\n\t\t\t\tif raw.get('type') == 'template':\n\t\t\t\t\traise yaml.BadYAML('Template files are not supported')\n\t\t\t\tif not (raw.get('title', 'Unnamed') in\n\t\t\t\t\t\t{'Tutorial Program', \n\t\t\t\t\t\t'Program', '"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\timport os\n\tfrom yaml.safe_load import SafeLoadError\n\t\n\ttry:\n\t\tf = open(yaml_file, 'r')\n\t\tyaml = yaml.safe_load(f)\n\texcept (SafeLoadError, IOError) as e:\n\t\traise IRValidatorException(\"%s:%d %s\" % (type(e), e.errno, e.filename))\n\t\n\treturn yaml\n\t\n\t\nclass IRValidatorException(Exception):\n\tpass\n"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor include_name in self.defines:\n\t\tkey = include_name\n\t\tparser_dict[key] = parser_dict[key].include()\n\t\tdel parser_dict[key]\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_directives = {}\n\n\t# First look for an include directive\n\tif parser_dict.get('include'):\n\t\tinclude_directives['include'] = parser_dict['include']\n\t\tparser_dict['include'] = parser_dict['include'].replace(\"#include\", \"\")\n\n\treturn include_directives\n\n# The list of tags that need to be imported\nimport_tags = [\"module\",\"cxx\", \"test\"]\nfor tag in import_tags:\n\ttag_path = \"{0}{1}\".format(tag_module_prefix, tag"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor include in self.spec.options['Include']:\n\t\tgroup = parser_dict['include_group'][include]\n\t\tparser_dict['include_list'].extend(group)\n\t\tparser_dict['include_list'].sort()\n\n\ndef _parse_include_files(path, parser):\n\tresult, params = [], {}\n\tfor key, spec in parser.iter_files():\n\t\tif spec.startswith('<'):\n\t\t\tfile = path + \"/\" + spec[2:]\n\t\t\tresult.append(file)\n\t\t\t"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# If an include directive is defined in the spec, check that first\n\t# to ensure the group is not already set.\n\tif \"include\" in parser_dict:\n\t\tfor key, directive in parser_dict[\"include\"].items():\n\t\t\tif key not in parser_dict:\n\t\t\t\tparser_dict[key] = directive\n\t\t\tgroup = next(k for k in parser_dict.keys() if k!= key)\n\t\t\tparser_dict[group] = parser_dict[key]\n\treturn parser_dict\n\ndef _subst_includes(template_dict,"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# Get the template specification from the parser\n\tparser = parser_dict['parser']\n\tself.default.append(parser.includeprefix)\n\tfor directive in parser.directives:\n\t\tif directive[0] == 'include' and directive[1]:\n\t\t\tif directive[1] in self.include_paths:\n\t\t\t\tself.default.append('import %s'%(parser.includeprefix+directive[1]))\n\t\telse:\n\t\t\tself.default.append(directive[0]+parser.includeprefix+"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser = parser_dict.get('include')\n\tif parser is None:\n\t\treturn {}\n\n\tinclude = {}\n\tincluded = [i for i in parser['dir'].choices if isinstance(i, list)]\n\tfor dname in included:\n\t\tfor include_dir in dname.choices:\n\t\t\tinclude.update(_include(self._group_prefix_cache, parser['parsers'], parser, include_dir))\n\tparser['include'] = include\n\treturn parser\n\n\nclass GroupDependencyParser(DependencyParser):\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroups = parser_dict['resolvegroups']\n\tif groups is None:\n\t\tparser_dict['resolvegroups'] = {}\n\t\treturn parser_dict\n\n\tall_includes = {}\n\tfor include in groups:\n\t\tkey = include\n\t\tif key[0] == '@':\n\t\t\tkey = key[1:]\n\t\tif key not in parser_dict['include_vars'] and (key[0]!= '@'):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tif type(parser_dict['include_vars']) == type({}):\n\t\t\t\t"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser = parser_dict.get('includes')\n\tif parser:\n\t\tparser = self.create_parser(parser)\n\t\tdirective = parser.rstrip().pop('include').lower()\n\t\tif directive in parser:\n\t\t\tparser.process_include()\n\n\treturn parser\n\nclass InlineFile(object):\n\tdef __init__(self):\n\t\tself.includes = set()\n\n\tdef add_include(self, dir):\n\t\tself.includes.add(dir)\n\n\tdef process(self, specs):\n\t\treturn specs\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict['include_groups']\n\n\tincluded_packages = {}\n\tfor include_group in include_groups:\n\t\tpkg_dict = parser_dict['packages'].get(include_group.name, {})\n\t\tif include_group.groups:\n\t\t\tpkg_dict.update(parser_dict['groups'].get(include_group.name, {}))\n\t\telse:\n\t\t\tpkg_dict.update(parser_dict['includes'].get(include_group.name, {}))\n\t\tincluded_packages.update(pkg_"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\tspec_file = self.spec_file\n\tparent_dir  = os.path.dirname(os.path.dirname(parser_dict['directory_name']))\n\t\n\tif not os.path.isdir(parent_dir):\n\t\traise RuntimeError('Can\\'t find folder'+ parent_dir + '.  Ick.')\n\t\t\n\troot_spec_file1 \t= os.path.join(parent_dir, 'inclusion_directives', 'all_includes_directives_1')\n\troot_spec_file"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tout = self.options\n\n\tout.defaults.update({ # make '*' default option if not set\n\t\t'*': None\n\t   })\n\n\treturn out\n\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# Get a function if we're running from files\n\tif hasattr(self.get_sources_func, \"__call__\") and not is_numpy():\n\t\tself.get_sources_func = self.get_sources_func()\n\n\tself.defaults = self.specification.get_defaults(self)\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tresult = dict(x for x in self.get_args())\n\n\tif 'description' not in result \\\n\tand 'help' in result \\\n\tand result['help']!= '':\n\t\tresult['help'] = (result['help']+' [--doc | help]'\n\t\t\t\t\t\tif 'help' in result else '').join(result['help']).lower()\n\n\treturn result\n\n\ndef get_commands(opts, parser):\n\tcmds = parser.parse_args(opts.commands)\n\n\t# Initialize command with spec\n\tcmd"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tpass\n\n\n\t### Arguments #################################################################\n\n\n\n\t### Optional arguments #######################################################\n\n\n\n\t### Parameters ############################################################################\n\t#\n\ndef params_common(in_, _, out_, _):\n\t'''\n\t\tCommon entry point for all other arguments\n\t'''\n\n\n\tdef params_common_2(in_, _, output):\n\t\t'''\n\t\t\tDo one common entry point for all non-common arguments\n\t\t'''\n\t\tout_(in_)\n\n\tdef params_common_1(in_, _,"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# Get config arguments\n\tget_arguments_from_namespace()\n\tget_defaults(self.spec, None)\n\t# Get arguments defined by args as configuration value\n\tget_arguments_from_values(self.args)\n\treturn self.spec, self.defaults\n\n\ndef get_arguments_from_name_dict(name_dict): # pylint: disable=unused-argument\n\tname_dict['namespace'] = name_dict.pop('namespace', name_dict.pop('namespace'))\n\treturn name"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tif self.config_source is False:\n\t\treturn {}\n\tvrs = {}\n\tfor l in self.argv.formatlist('vs'):\n\t\tvrs['vs'] = l.split(':', maxsplit=1)\n\t\t\n\tvs = self.config_source.format(**vrs)\n\tif self.config_target is not False:\n\t\tvs = self.config_target.format(**vrs)\n\t\t\n\treturn vs\n\n# Call methods of the argument processor associated with the\n# option parser instance.\nif __name"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tif self.get_type() =='spec' and not self.options.get('default', False):\n\t\tspec_defaults = self.get_arg('default')\n\t\tfor s in self._get_sections():\n\t\t\ts.d.update(spec_defaults)\n\ndef get_spec(file):\n\t\"\"\"\n\tRead the spec corresponding to a file.\n\t\"\"\"\n\tspec = None\n\tdefaults = {}\n\tfile = file\n\ttry:\n\t\twith open(file) as ifh:\n\t\t\tspec = yaml.safe_load"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\n\tif self.__spec is None or self.__dest is None:\n\t\traise ValueError(\"No specs supplied\")\n\t\n\tfor arg in self.__argv or []:\n\t\tval = self.__spec.get(arg)\n\t\tif val is None:\n\t\t\tcontinue\n\t\telse:\n\t\t\tif hasattr(val, \"__callable__\") and not isinstance(val, basestring):\n\t\t\t\tval = val(self)\n\t\t\tself.__argv.append(val)\n\t\t\tself.__defaults.append(val)\n\n\t\t"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\ttry:\n\t\tspec = self.spec\n\texcept AttributeError:\n\t\tspec = self.sources[0].spec\n\tif spec is Notimplemented:\n\t\tspec = None\n\n\t# Set defaults to values from spec for the arguments.\n\targargs = {}\n\tfor a in spec.parameters.parameters:\n\t\targargs[a.name] = a.defaults\n\tif len(argargs) > 0:\n\t\tself.argargs = argargs\n\n\t# Resolve source-specific defaults.\n\tret = self.sources["}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tif not argv:\n\t\treturn\n\t\t\n\treturn\n\n\tif 'config' in argv:\n\t\treturn\n\t\t\n\tif 'config-argv' in argv:\n\t\treturn\n\t\t\n\tif 'config' in argv:\n\t\tconfig = argv['config']\n\t\tdel argv['config']\n\t\t\n\t\tif config:\n\t\t\tself.parser = argparse.RawDescriptionHelpFormatter + argparse.SUPPRESS_RELATED_DESC_CACHE\n\t\telse:\n\t\t\tself.parser = argparse.RawHelpFormatter\n\t\t\t"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\td = {}\n\ttry :\n\t\tself.mv()\n\t\tself.rpc()\n\t\td['args'] = self.mv()\n\t\td['func'] = self.rpc\n\t\td['_cache'] = self._cache\n\texcept Exception,e:\n\t\td['func'] = self.rpc\n\t\td['args'] = self.func(self,\"%f\").func\n\t\td['_cache'] = self._cache\n\t\treturn d\n\treturn d\n\n \t\ndef get_args(self, args):\n\ttry:\n\t\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t#TODO: Return to avoid the use of get_deprecate_options() multiple times\n\treturn {\n\t\t\"categorize_entries\": False,\n\t\t\"fuzzy_match\": True\n\t}\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t# deprecating options that are no longer in the API\n\treturn {\n\t\t\"deprecated\": [\n\t\t\t\"allow_upgrade=False\", # upgrade and downgrade in next release\n\t\t\t\"allow_delegate=False\", # downgrade in next release, not all admins have upgraded\n\t\t\t],\n\t\t\n\t\t\"no_retry_url\": \"https://oslo.pocoo.org/api/v3/users/0\"\n\t}\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tresult = super(BaseTestCase, self).get_deprecated_args()\n\n\tresult['testcase__fullname'] = self.fullname\n\tresult['testcase__shortname'] = self.shortname\n\tresult['testcase__tags'] = self.tags\n\tresult['testcase__dependencies'] = self.suites\n\treturn result\n\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\targs_dict = collections.OrderedDict(\n\t\t(\n\t\t\ta, {\n\t\t\t\t'string': '',\n\t\t\t\t'args': ','.join(['--' + a])\n\t\t\t}\n\t\t)\n\t\tfor a in pynat.get_arg_dict()\n\t\t+ pynat.get_deprecated_args()\n\t)\n\targs_dict.update({\n\t\ta: (\n\t\t\t'--' + a, a in args_dict\n\t\t)\n\t})\n\treturn args_dict\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn self.options.get_deprecated_args()\n\n\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdeprecated = {}\n\treturn deprecated\n\n\t\ndef get_deprecated_args_and_other(args):\n\t\n\tdeprecated = get_deprecated_args()\n\tother = set(args)\n\t\n\tother &= deprecated.keys()\n\t\n\treturn other, deprecated\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn{\n\t\t\"class_name\": get_class_name(self),\n\t\t\"method_name\": get_method_name(self),\n\t\t\"is_deprecated\": self.is_deprecated(),\n\t\t\"deprecated_link\": self.deprecated_link\n\t}\n\ndef get_class_name(self):\n\treturn self.__class__.__name__\n\ndef get_method_name(self):\n\treturn self.__class__.__name__.replace('_', 'P')\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn self.get_args(['-WDEPOSITION'])\n\n\ndef get_new_args(cmd, argnames):\n\t\"\"\"\n\tReturn arguments which were deprecated.\n\t\"\"\"\n\tdeprecated = self.get_deprecated_args()\n\targs = []\n\tfor arg in argnames:\n\t\targ = arg in deprecated \n\t\tif not arg:\n\t\t\tcontinue\n\t\targs.extend([argname + str(i) + str(arg) for i, argname in enumerate(deprecated)])\n\treturn args\n\n\ndef print_"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tresult = {\n\t\t\"action\":\t\t\t'',\n\t\t\"action_list\":\t\t[],\n\t\t\"options\":\t{},\n\t\t\"requires\":\t[],\n\t\t\"suggest\":\t\t\t'',\n\t\t\"version\":\t\t'',\n\t\t\"dependencies\":\t'',\n\t\t\"author\":\t\t'',\n\t\t\"deprecated_by\":\t'',\n\t\t\"deprecated_until\": '',\n\t\t'class_path':\t\"\",\n\t}\n\n\treturn result"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\tif (any([cli_arg not in answer_file_args for cli_arg in cli_args])\n\t    and not isinstance(answer_file_args, list)):\n\t\tprint \"Invalid argument: %s cannot be used as a deprecation argument.\" % (\"-\"+cli_arg)\n\t\treturn\n\t\n\tif (getattr(self, \"version_not_yet_available\", True)): #if we are going to use pkgcheck for this thing\n\t\tif any([cli_arg in answer_file_args for cli_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tanswer_file_args['dep_args'].append(cli_args['--print-dep-warnings'])\n\t\n\targs, unknown = validate_arg(cli_args)\n\tanswer_file_args['dep_args'].append(cli_args['--print-dep-warnings'])\n\n\treturn args\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tanswer = parse_and_process(answer_file_args)\n\tdeprecated_args = { }\n\tfor cli_name in cli_args:\n\t\tdeprecated_args[cli_args[cli_name]] = cli_args[cli_name]\t\n\t\n\tdef print_answers(answers):\n\t\tfor k in answers:\n\t\t\tif k not in answer:\n\t\t\t\tprint k, answer[k]\n\n\t\treturn answer\n\n\tfor k, v in deprecated_args.items():\n\t\tprint 'Please remove this argument, it"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\tif self.verbose:\n\t\treturn cli_args\n\t\t\n\t# this could be a string with the command or multiple arguments\n\tfor arg in answer_file_args.keys():\n\t\tif sys.stdin in answer_file_args[arg]:\n\t\t\tfor opt in type(answer_file_args[arg]).__mro__[0]:\n\t\t\t\tif hasattr(answer_file_args[opt], 'valid'):\n\t\t\t\t\tanswer_file_args[opt].validation_str.deprecated = True\n\t\t\t\t\tbreak\n\t\n\tif not"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tlog.info('Found deprecation arguments: %s' % self.ARGUMENTS)\n\t\n\tfor k in self.ARGUMENTS:\n\t\tanswer_file_args[k] = k\n\t\n\tif len(answer_file_args)!= len(cli_args):\n\t\tlog.warning('Found incorrect number of arguments: have %s, but %s' % (len(cli_args), len(answer_file_args)))\n\t\t\n\t_remove = set(answer_file_args.keys())\n\tfor (k,v)"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif self.args.is_command_args_deprecation and not self.args.is_argument_deprecated:\n\t\tfile_list = []\n\t\tcommand_list = []\n\t\tfor arg in cli_args:\n\t\t\tif isinstance(cli_args[arg], list):\n\t\t\t\tfile_list.extend(cli_args[arg])\n\t\t\telse:\n\t\t\t\tfile_list.append(cli_args[arg])\n\t\tcommand_list = [arg for arg in command_list if not arg in self.args.deprecated + file_list]\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t#print \"Arg names are deprecated in favor of command line params\"\n\t\n\targument_deprecation = \"Argument \" + \" \".join(answer_file_args.keys())\n\t#print \"Argument %s\" % argument_deprecation\n\t\n\tresult = [argument_deprecation]\n\n\n\tresult.append(\"The output from run_cli_command('echo') depends \"\n\t\t\t\t    \"offly in the form of one or more non-string values, \"\n\t\t\t\t    \"but this function cannot validate a non-string value\");\n\t#result"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint (\"\"\"The argument '--version' has been deprecated.\n\n\t\tIt was originally a command that was called 'help'. Please switch to the \"command\" subcommand with a command like the following:\n\t\t\t\"command -v\" will show all the args that were available to the command.\n\t\"\"\")\n\tcli_args = copy.deepcopy(cli_args)\n\n\tmissing_arguments = []\n\n\tfor name in answer_file_args.keys():\n\t\tif name == \"version\":\n\t\t\tcontinue\n\t\tif name not in (arg for arg in cli_args."}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif '--' in cli_args:\n\t\tmsg = '`%s -d \"%s\"` --' % (answerer_cli_help, cli_args['-d'])\n\telse:\n\t\tmsg = '`%s -d \"%s\"' % (answerer_cli_help, cli_args['-d'])\n\t\tanswer_file = '-a %s' % answer_file_args['-d'].replace(' ', '').replace('-', '_')\n\t\tanswer_file = answer_file.replace('"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tanswer_file_args = dict([(v, answer) for v, answer in answer_file_args.iteritems()\n\t\t\tif hasattr(v, '__call__')))\n\tfor k, v in answer_file_args.iteritems():\n\t  if v is None:\n  \t    answer_file_args[k] = None\n  \t  elif k.startswith('--'):\n\t    k = k[2:]\n\t    if not (k in cli_args and '--' in cli_args[k]):"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t# Default to virsh command\n\t_opt_list = ['-c', self.virt_cmd]\n\n\tif command_name == 'oscap':\n\t\t_opt_list.append('-a') # all\n\t\t_opt_list.append('-sname') # all with sname\n\t\t_opt_list.append('-k') # no k, do k\n\t\t_opt_list.append('-m') # no m, do m\n\t\t_opt_list.append('-n') # no n, do n\n\t\t_opt_list"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn self._parser.command_options(command_name)\n\n\n   def run(self, command_name, args, user_flags):\n\t\"\"\"\n\t\tRuns a command using cmd\n\t\n\t:param command_name: name of the command\n\t:param args: arguments\n\t:param user_flags: special user flags (such as 'vtysh')\n\t:return: the return value of the command\n\t\"\"\"\n\treturn self._parser.args(command_name, args, user_flags)\n\n\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toption_specs = []\n\tfor key in self.__opt_dict.keys():\n\t\tif key in command_name:\n\t\t\toption_specs.append(self.__opt_dict[key])\n\treturn option_specs\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toptions = []\n\tget_parser_option_specs(command_name)\n\toptions += self._get_all_options(parser=self, command=command_name)\n\treturn options\n\t\ndef main():\n\tparser = get_parser()\n\tparser.description = 'The osc-image-manage command.'\n\tparser.add_argument('-v', '--verbose', action='store_true')\n\tparser.add_argument('--vendor', help='The image vendor', default='unknown')\n\tparser.add_argument('--"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn {\n\t\t\"command_name\" : command_name,\n\t\t\"option_groups\" : self.option_groups.keys()\n\t\t}\n\nregisterCmd(cmd)\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = super(Command, self).get_parser(command_name)\n\treturn parser.get_option_parser_values('--%s' % command_name)\n\n\nCommand = pprint.to_class(get_parser_option_specs(command_name=\"--%s\" % parser_mod.main))\n\n\ndef get_parser_option_specs(parser_mod=DefaultParser,\n\t\toptions_factory=ConfigParser.SafeBasicConfigParser):\n\t\"\"\"\n\tGets parser options from command line.  This includes --option and\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn self.options.get(command_name)\n\n# vim:set sw=2 sts=2 ts=2 et:\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn self._sub_parser.get_argument_spec(command_name)\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tif os.path.isfile(self.options_file) is True:\n\t\tcommand = self.options[command_name]\n\t\treturn command\n\telse:\n\t\treturn []\n\nclass CommandLine(object):\n    \"\"\"\n        A utility for generating the CLI interface to oscapi.cli.py\n        \n        This class allows you to run oscapi CLI commands with\n        python commands directly.   \n        \n        In a real environment where you need some other abstraction\n        like ssh proxy or something, the cli can be run directly from\n        within the script."}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tcmd_parser=re.compile(self.get_command_opt(command_name))\n\treturn [m.group(2) for m in cmd_parser.findall()]\n\n_cmd_specs = {\n\t\"virsh\": {\n\t\t\"help\": \"Show parameters of virsh commands\",\n\t\t\"version\": (\"%prog {0}\", __version__),\n\t\t\"alias\": {\n\t\t\t\"show_default\": {\n\t\t\t\t\"aliases\": [\"default\"],\n\t\t\t\t\"name\": \"default\",\n\t\t\t\t\"description\": _(\"Show the"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif is_string(self.options_spec) and argument_name in self.options_names:\n\t\treturn self.options_spec\n\t\t\n\traise ValueError(\"get_option_spec() called with unknown command \" + command_name)\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\tFor testing purposes. \n\t\tUsage: python get_option_spec.py --help\n\t\"\"\"\n\tpass\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn (\"%s [%s] \" % (self.command_name, command_name)).encode()\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tpass\n\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.option_spec\n\n\t\ndef get_option_definition(name):\n\t\"\"\"\n\tGets the definition of the option spec.\n\t\n\t@param\tname\toptional name for the specified option\n\t@return\tthe definition or None\n\t\"\"\"\n\tif hasattr(Configuration._options, name):\n\t\treturn getattr(Configuration._options, name)\n\treturn None\n\n\ndef set_option_value(name, value):\n\t\"\"\"\n\tSets the value for a keyword argument for a set_option command.\n\t\n\t@param\tname"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.__command_names[command_name].argspecs[argument_name]\n\n@command\ndef get_optparser(command_name=None, argument_name=None):\n\t\"\"\"\n\t\tReturns an optparser that will process the command(s) specified.\n\t\"\"\"\n\toptparser = CommandOptionParser(command_name or \"get_command_options\", argument_name or \"get_argument_options\", command_name or \"get_commands\")\n\tfor name in self.__command_names:\n\t\toptparser.add_option(self"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif \"--\" in argument_name and len (argument_name) > 2:\n\t    return self.options.options ()\n\telif \"--\" in argument_name and \"\\0\" in argument_name:\n\t    return self.options.options (\"-d\")\n\telif \"--\" in argument_name and len (argument_name) > 0:\n\t    return self.options.options (\"-v\")\n\telif \"--\" in argument_name and len (argument_name) > 1:\n\t    return self.options.options (\"--verb"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\n\treturn self._opts[command_name][argument_name]\n\n\t\n\nfrom core import config\nconfig.parser.debug('config', 'core.command_parser.get_option_spec.__file__')\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.__get_option_spec(\"command\", command_name, argument_name)\n\n\ndef get_option_value(self, option_or_command, option):\n\t\"\"\"\n\tReturns the value of the specified option that can be retrieved\n\tand modified from the configuration.\n\t\"\"\"\n\treturn self.__get_option_value(option_or_command, \"command\", option)\n\n\ndef get_argument_value(self, argument_or_command, argument):\n\t\"\"\"\n\tReturns the value of the specified option that can be retrieved\n\tand modified"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif argument_name == None:\n\t\treturn GetOptSpec(command_name, False)\n\t\t\t\n\telif argument_name in self.keywords:\n\t\tkeyword = self.keywords[argument_name]\n\t\tif command_name == keyword.func:\n\t\t\treturn keyword.spec\n\t\telse:\n\t\t\treturn keyword\n\t\n\telse:\n\t\treturn \"\"\n\t\t\n\n# Class to define a default function and argument spec\ndef GetDefaultFunctionSpec(func_name, argument_names = None):\n\t\"\"\"\n\t\tGets a default specification for one or"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn (command_name.lower(), self.options[self.option_name(command_name)][self.argument_name(argument_name)])\n\n#\n# get_option_value()\n#\n# Returns the value of the specified option.\n#\ndef get_option_value(self, command_name, argument_name):\n\t\"\"\"\n\tGets the value of the specified option.\n\t\"\"\"\n\treturn get_option_spec(command_name, argument_name)[1]\n\n#\n# set_option()\n#\n# Sets the"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif not self.isSet(args.silenced):\n\t\treturn []\n\n\tret = []\n\tfor a in self._silenced:\n\t\tif getattr(self, a) is None:\n\t\t\tcontinue\n\t\tif not isinstance(self, Verbosity):\n\t\t\tcontinue\n\t\tif isinstance(getattr(self, a), basestring):\n\t\t\tcontinue\n\t\telse:\n\t\t\tret.append(getattr(self, a))\n\n\treturn ret\n\n\nclass Verbosity(object):\n\t\"\"\"\n\t\tverbose function.\n\t\"\"\"\n\n\tSET = 0"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced = list(sorted(args.keys()))\n\tsilencer = [args[str(i)] for i in range(len(silenced))]\n\tfor i in silenced:\n\t\tsilencer[i] = [s for s in silenced if args[str(i)] == args[str(s)]]\n\treturn silenced + list(silencer)\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [ arg for arg in args if not arg.startswith(FLAGS_BASE) ] \n\nclass Config:\n\t\n\t\"\"\"\n\tThe class to contain all the configuration variables for ansible.\n\tIt implements a basic constructor for the basic configuration\n\t\n\t:param base : base for the config, e.g: ansible.run.{host,user,version}\n\t:return  : The config and options\n\t\"\"\"\n\t\n\tdef __init__(self, base):\n\t\tself.cfg = ConfigParser()\n\t\tself.parser ="}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif args is None or args == \"\" or len(args) == 0:\n\t\treturn []\n\telse:\n\t\tsilceed = set()\n\t\tfor s in args.split(\" \"):\n\t\t\tsilceed.add(s)\n\t\treturn silceed\n\t\n\t\n\t\n\t"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\targ_defs = []\n\tfor i in range(0, len(args.args)):\n\t\targ_defs.append(sys.argv[1])\n\t\targ = args.args[i]\n\t\tif arg.get_type().is_int() or arg.type == arg.type.type:\n\t\t\tif len(arg_defs) == 0:\n\t\t\t\tyield arg\n\t\t\telse:\n\t\t\t\tyield arg_defs[-1]\n\t\t\t\tfor arg_def in arg_defs[-1:]:\n\t\t\t\t\tyield \"{0"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tslimes = []# list of names of silenced arguments\n\tfor i in range(0, len(args)):\n\t\tif args[i] =='silence':\n\t\t\tslimes.append(i)\n\t\n\treturn slimes\n\t\t\t\ndef quiet(state):\n\t\"\"\"\n\tquiet argument for setting an quiet flag.\n\t\n\t:param state: The state of the method.\n\t\"\"\"\n\tlogger.info('%squiet is called!', state)\n\tglobal quiet\n\tdel quiet\n\ndef silent(state):\n\t\"\"\"\n\tsil"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [ arg.get_silent_class().__name__ for arg in args ]\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tres = []\n\tfor arg in args:\n\t\tif not arg.silent:\n\t\t\tres.append(arg.name)\n\treturn res\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tlist = [arg for arg in get_args(args) if \\\n\t\tnot isinstance(arg, (str, bool))]\n\treturn list\n\ndef call_fnc(func):\n\t\"\"\"\n\tCall a function,\n\t\n\t:param func: the function object to call\n\t:return: the function return value if success, else raise exception\n\t:raise: Exception if not\n\t\"\"\"\n\ttry:\n\t\tret ='return'+ func.__doc__.strip()\n\t\tvalue = subprocess.check_output(argv(2) +"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif args is None:\n\t\treturn None\n\n\tif type(get_silent_args_default)==type(args):\n\t\tret = [v for v in get_silent_args_default(args)]\n\telse:\n\t\tret = get_silent_args(args).split(',')\n\tret.sort(comparator=lambda x, y: cmp(x.lower()))\n\treturn ret\n\n\t\n\n\n"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args.version!= None:\n\t\tif not args.version.isdigit():\n\t\t\traise Exception(\"The `version` field must be a numeric value.\")\n\telse:\n\t\targs_str = str(args).split(\"|\")\n\t\tfor arg in args_str:\n\t\t\tif args.get(arg) is None:\n\t\t\t\traise ImproperlyConfigured(\"Missing required value \"+str(arg))\n\ndef get_config(name, section, prefix='config', default=\"\"):\n\tfrom pybb.settings import CONFIGURATIONS\n\treturn CONFIGURATIONS[name"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor required_arg in self.requires:\n\t    if not args.has_key(required_arg):\n\t        logger = RequiredArgsLogger(required_arg, args)\n\t        logger.exception(args)\n\t        raise RequiredArgsError(args)\n\n\tif not args.has_key(self.name):\n\t    logger = NameLogger(self.name, args)\n\t    logger.exception(args)\n\t    raise RequiredArgsError(args)\n        else:\n            if self.requires[0].name!="}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not args:\n\t\traise ValueError('You need to specify at least one argument.')\n\tif len(args) < 2:\n\t\traise ValueError('You need to provide at least \"arg1\" and \"arg2\".')\n\treturn (len(args) == 4 and args[:2] == ('arg1', 'arg2'))\n\n\ndef create_model_kwargs(model):\n\t\"\"\"\n\tConstruct any extra kwargs to pass on to a model.\n\t\"\"\"\n\tkwargs = {}\n\treturn kwargs\n\ndef init"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\treturn (check_required_args(self, args) and all)(args)\n\nclass CachedConfigFile(object):\n\t\"\"\"\n\tCreates an instance of CachedConfigFile, passing the file_path\n\tto the init function to load the config from.\n\n\t@type config: L{Mapper<L{CachedConfigFile}, C{str}};\n\t\tthe name of the config file.\n\n\t@type config_path: C{list} of C{str};\n\t\tthe absolute configuration path to load the config from the disk,\n\t\ti.e."}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif self.required_positional is None or len(self.required_positional)!= len(args):\n\t\traise KeyError(\"At least one required argument is needed for this command.\")\n\n\treturn self\n\ndef validate(*args, **kwargs):\n\t\"\"\"\n\tValidates arguments.\n\n\t\"\"\"\n\tvalidator_args = []\n\tvalidator = kwargs.get(\"validator\")\n\n\tif validator:\n\t\tvalidator_args.append(validator)\n\n\treturn cli_helpers.validate(*args, **kwargs).validate"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = [\n\t\t(\"dataset_location\", \"file\"),\n\t\t(\"input_file\", \"file\"),\n\t\t(\"output_pkl\", \"file\"),\n\t\t(\"max_clusters\", \"int\"),\n\t]\n\tfor arg in required_args:\n\t\t# Check that required arg is present for each one of them\n\t\tif not args.has_key(arg):\n\t\t\traise self.Error(\"Missing argument %s\" % arg)\n\nclass MinClustering(Minimize):\n\t\"\"\"\n\tMinimizes the model using the Clustering"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args.hostname is not None and args.hostname!= '':\n\t\tif self.hostname_exists(args.hostname) is False:\n\t\t\tself.output.error('Incorrect name provided.  You must provide a hostname')\n\t\t\tsys.exit(1)\n\tif args.password is not None and args.password!= '':\n\t\tif self.password_exists(args.password) is False:\n\t\t\tself.output.error('Incorrect password provided.  You must provide a password (with correct password)')\n\t\t\tsys.exit("}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\thasArgs = [arg for arg in self.requiredArgs if self.__get_type(arg)!='string']\n\trequiredArgsSet = set(hasArgs)\n\thasArgs.remove(\"_id\")\n\t# TODO: add validation message/bug\n\tif requiredArgsSet:\n\t\traise AttributeError(\"Required arguments: {args} have been provided. See help documentation for more info.\")\n\ndef __get_type(value):\n\t\"\"\"\n\t\tGets the value's type\n\t\tIf a class, returns a class (i.e. \"instance\" class"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args is None:\n\t\treturn (False, \"Required argument is missing.\")\n\n\treturn (args == True, \"Required argument is expected to be True.\")\n\n\ndef validate_exists(args):\n\tval = args.get\n\tif val is None:\n\t\treturn (False, \"Required argument missing\")\n\n\ttry:\n\t\tos.stat(val.str)\n\texcept OSError:\n\t\treturn (True, \"File doesn't exists.\")\n\n\ndef main(args):\n\t\"\"\"\n\tMain entry point for this test.  For example, you can"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not isinstance(args.filename, str):\n\t\traise TypeError(\"Arguments to check_requires_args must be str\")\n\tfor arg in args.filename.lower():\n\t\tif arg not in self.FILE_REQUIRES_ARGS:\n\t\t\traise ValueError(\"Argument %s invalid. Supported arguments: %s\" % (arg, ', '.join(self.FILE_REQUIRES_ARGS)))\n\treturn args"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t)\n\tif not options_spec and not self._get_required_args(command_name):\n\t\treturn []\n\treturn [\n\t]\n\t\n\t################\n\n\ndef _get_optional_args(command_name, args):\n\targs.setdefault('') # Set command name as optional argument\n\n\toptional_args = ['--'+name + '=' + value for name, value in args.items()]\n\t\n\trequired_when ='required when' in self._get_conditional_args(command_name)\n\treturn optional_args"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\ttry:\n\t\tif ('required_when' in options_spec) or ('not_required_when' in options_spec):\n\t\t\tif ('required_when' not in options_spec) and ('not_required_when' not in options_spec):\n\t\t\t\traise ValueError(\"must specified both arguments or specify 'not_required_when'\")\n\t\t\t# parse required_when argument as a list separated by comma\n\t\t\trequired = self.get_required_when(get=request.POST, request_with_args=request, option=options_spec)\n\t\t\tconditional = list"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) -> list[str]:\n\t\n\t# If command specification does not exist\n\tif not options_spec:\n\t\treturn []\n\n\t# If multiple options do not contain required arguments\n\tif len(options_spec) > 1:\n\t\tif'require'] in options_spec:\n\t\t\toptions_spec['require'].clear()\n\t\tif 'for_each'] in options_spec:\n\t\t\toptions_spec['for_each'].clear()\n\t\treturn []\n\t\n\t# If command specification is required\n\tcommand_spec = config.get_command_spec_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\treturn [name for name in args for _name in options_spec if not _name in self.required_when]\n\n\nclass BaseCommand(object):\n\t\"\"\"Implements Base Command.\"\"\"\n\tdef __init__(self, plugin, name, help, command_type, cmd_spec,\n\t\t\t\t options, default,\n\t\t\t\t requirements):\n\t\t\"\"\"Creates command from command base spec & default specs.\"\"\"\n\t\tself.command_type = command_type\n\t\tself.cmd_spec = cmd_spec\n\t\tself.options = options\n\t\tself.default = default\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tfor value, required_when in options_spec:\n\t\tif value:\n\t\t\tif not self.required_when_dict.get(command_name, None):\n\t\t\t\tself.required_when_dict[command_name] = []\n\t\t\tself.required_when_dict[command_name].append(value)\n\t\telse:\n\t\t\tif required_when:\n\t\t\t\tcontinue\n\t\t\tself.required_when_dict[command_name].append(value)\n\t\t\n\treturn self.required_when_dict\n\"\"\"\n\tArguments with ``required_when`` condition matched"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif options_spec:\n\t\treturn [arg for arg in args if not hasattr(command_name, arg)])\n\nclass Inputs(object):\n\t\"\"\"\n    :py:attr:`_input_class`\n    \n        :py:attr:`_input_class_kwargs`\n        \n        :py:attr:`_input_class_override`\n        \n        :py:attr:`_output_class`\n\n        :py:attr:`_input_spec`\n\n        :py:attr:`_input_spec_over"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\treturn self._get_valid_args(command_name, options_spec, options_valid=['!=\"None\"'])\ndef _get_valid_args(command_name, options_spec, options_valid=()):\n\t\"\"\"\n\tGets a validated command argument list.\n\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param options_valid:  the list of valid command arguments.\n\t:return: the validated command argument list.\n\t\n\t:rtype: list\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t)\n\treturn [key for key in dir(options_spec) if getattr(options_spec, key)(args)]\n\t\n\nclass Option(YAML):\n\tdef __init__(self, option_type, **kwargs):\n\t\tself._kwargs = kwargs\n\t\tself._value = None\n\n\tdef __attr__(self, key): return getattr(self._kwargs, key)\n\n\tdef value(self, val=None):\n\t\t\"\"\"\n\t\tA private __attr__ method for providing option value.\n\n\t\t:param val: name"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) -> Sequence[str]:\n\t\t\n\t\t\"\"\"\n\t\tList all mandatory arguments with ``required_when`` condition.\n\t\t\n\t\t:param command_name: the command name.\n\t\t:param options_spec: the list of command spec options.\n\t\t:param args: input arguments.\n\t\t:return: list, list of argument names with matched ``required_when`` condition\n\t\t\"\"\"\n\t\treturn [k for k in self.cmd.get_cmd_valid_kwargs(\n\t\t\tcommands=options_spec,\n\t\t\trequired_when=['is_required'],\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) -> list:\n\t\treturn [argument_regex.format(optional_when=re.escape(option)) % {\n\t\t\t'option': value\n\t\t} for attribute, value in options_spec\n\t\t]\n"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tvalid_string_args = (arg[0] for arg in self.argument_definitions)\n\tlength_args = sum([len(str(val)) for val in args if len(str(val))])\n\tif (len(args) > length_args):\n\t\tself.error(\"Length of `__str__` field is more than %d characters.\") % length_args\n\n# ===========================================================\n\n\nclass ValidationException(Exception):\n\tpass\n\n\nclass ArgumentsException(Exception):\n\n\tdef __init__(self, args):\n\t\tException."}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tlength = None\n\tif len(args):\n\t\tlength = args[0].validate()\n\t\tif length:\n\t\t\tif '-' in args[0]:\n\t\t\t\tif '1' <= args[0]['-'] <= '9':\n\t\t\t\t\tif len(args[0]['-']) < length:\n\t\t\t\t\t\traise Exception(\"Incorrect number of characters for arguments (length = %s). Please supply a larger number.\" % length)\n\t\t\t\telse:\n\t\t\t\t\traise Exception(\"Incorrect number of characters for arguments (length = %s).\" % length)\n\treturn length\n\t\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tself.validate_arg_length(args)\n\treturn\n\t\n\tcount = len(args)\n\tif count > self.max_length:\n\t\traise InvalidLength('Value has been taken from wrong length')\n\telse:\n\t\treturn \n\n#################################################\n#################################################\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\tself.get_length(args)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid value. Expected to be a string but got %s.\" % args.__class__)\n\ndef validate_length(self, value):\n\t\"\"\"\n\tValidate length argument using a regular expression.\n\t\n\t:param value: The length of the argument.\n\t\"\"\"\n\ttry:\n\t\tre.compile(value)\n\texcept re.error as error:\n\t\traise ValueError(\"The string entered cannot be matched. Expected to be a regular"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# Check type of arguments.\n\tif(type(args) is not tuple):\n\t\treturn False\n\t# Check arguments length.\n\tif(len(args) < 4):\n\t\treturn False\n\t# Check values for length.\n\tif(len(args) == 4 and len(args[1]) < 12):\n\t\treturn False\n\telse:\n\t\treturn True\n\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tvalue = args[0]\n\tif value > len(args):\n\t\traise TypeError('value must be < {}'.format(len))\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tval_length = len(args)\n\tfor arg in range(len(args)):\n\t\tval = arg_to_arg(val_length, self.args, args[arg], arg)\n\t\tif len(val) > len(self.args):\n\t\t\traise ValueError(\"The current argument is longer than the specified length.\")\n\t\n\treturn \n\t\n\t\n\t\n\t\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# check for arguments length\n\tassert 0 <= len(args) <= 200\n\ndef validate_width_args(self, args):\n\t\"\"\"\n\tCheck if value of arguments is not longer than width specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# check for arguments length\n\tassert 0 <= len(args) <= self.width\n\ndef check(message, args, kv = {}):\n\t\"\"\"\n\tCheck if passed parameters are valid and the user is within the\n\tcorrect area for their display.\n\t\n\t:param message:"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tself._check_param_length(args, self.param_length)\n\t\n\treturn\n\t\t\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass\n\t\n\nclass MyHandler(BaseHandler):\n\n\t@web.validate('name')\n\t@validate_length_args\n\t@WebError('Incorrect length arguments')\n\tdef validate_length_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is not longer than length specified.\n\t\t\n\t\t:param args: The received arguments.\n\t\t\"\"\"\n\t\treturn self.validate_value_args([(args, 'length'),])\n\t\n\t@web.validate(['name'])\n\t@validate_length_"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif 'order' not in args:\n\t\treturn False"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tif args[0] == 'choices' and len(args) >= 1:\n\t\tval = self.choices.match(args[0])\n\t\tif not val:\n\t\t\traise Exception('The choice \"'+ args[0 ] +'\" must be one of '+str(self.choices.choices))\n\t\treturn True\n\treturn False\n\n\nclass Choice(object):\n\t\"\"\"\n\t\tA choice argument.\n\t\"\"\"\n\t\n\tdef __init__(self, *args, **kwargs):\t\n\t\tself.choices = kwargs"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targs.append('--no_subprocess --no_shell', '--no_sigchld')\n\treturn args\n\n@command\ndef add(self, args):\n\t\"\"\"\n\tAdd a file to the filesystem\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpath = args.pop(0)\n\n\t# Create a list with the absolute file paths.\n\tpaths = [path]\n\n\tfor path in paths:\n\t\t# Try to create the directory\n\t\tfiles = self.find_files(path)\n\t\tcheck_failed ="}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.choices:\n\t\tavailable = True\n\t\t\n\t\tif self.verbose and (not args.verbose or not args.count()):\n\t\t\tavailable = False\n\t\t\n\t\tfor val in args.choices:\n\t\t\tchoices = val.choices\n\t\t\n\t\t\tif choices:\n\t\t\t\n\t\t\t\tfor elem in choices:\n\t\t\t\t\n\t\t\t\t\tif elem.text() == val.text():\n\t\t\t\t\t\tavailable = True\n\t\t\t\t\t\t# TODO: Do we need to check self.choices as well?\n\t\t\t\t\t\tbreak\n\t\t\t\t\t\n\t\tif not available:\n\t\t\t"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args[1].value is None:\n\t\treturn False\n\tif not hasattr(args[1].value, 'choices_set'):\n\t\treturn False\n\tif not isinstance(args[1].value.choices_set, set):\n\t\treturn False\n\treturn True\n\ndef validate_choices(self, args):\n\t\"\"\"\n\tValidate the choices argument.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\tchoices = self.validated_choices_args(args)\n\texcept Val"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\tchoices = [choices_dict[key] for key in args if key in choices_dict]\n\texcept KeyError:\n\t\treturn\n\n\tif sorted(choices) == args:\n\t\treturn True\n\treturn False\n\nif __name__ == '__main__':\n\t\n\tfrom ConfigParser import SafeConfigParser \n\t\n\tparser = SafeConfigParser()\n\tparser.read(sys.argv[2])\n\t\n\t\n\tprint parser.defaults()\n\tparser.read('config.ini')\n\tconfig = parser"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn args[0].is_enumerator\n\t\n\tdef default_arguments(self, args): \n\t\treturn args[1]\n\n\tdef validator_args(self, args): \n\t\treturn None\n\n\tdef args_validator(self, arg): \n\t\treturn False\n\n\tdef validate(self, args):\n\t\t\"\"\"\n\t\t\tCheck if it is any of the available choices.\n\t\t\t\n\t\t\t:param args: The received arguments.\n\t\t\t:return: A success code (valid or invalid).\n\t\t\t\"\"\"\n\t\tif args == self.default_arg"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif isinstance(args, list) and args:\n\t\treturn args[0] in self._choices\n\telse:\n\t\treturn False\n\n\ndef validate_tables(table):\n\t\"\"\"\n\tCheck if an argument can be a table object\n\t\"\"\"\n\t\n\treturn isinstance(table, Table)\n"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif \"choices\" in args:\n\t\tif args[\"choices\"] in range(len(self.choices_dict)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\treturn args[\"value\"] in self.choices_dict\n\n\n\tdef validate_choices_arg(self, arg_obj):\n\t\t\"\"\"\n\t\tCheck if value of choice arg is one of the available choices.\n\t\t\n\t\t:param arg_obj: The received arg value.\n\t\t\"\"\"\n\t\targ_str = arg_obj.split()\n\t\ttry:\n\t\t\targ_"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor c in args:\n\t\tif c.type == args.type and c.name in [\"label\", \"value\", \"choices\"]:\n\t\t\treturn True\n\t\tif c.default is not None:\n\t\t\tif not c.default is str \\\n\t\tand c.type == args.type and c.name == \"default\"\n\t\t\treturn True\n\t\t\t\n\t\t\t\n\t\tif c.value is not None:\n\t\t\tif c.value in args.choices:\n\t\t\t\treturn True\n\t\n\treturn False\n\t\t\n\t\t\n\t\n        \n"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tself.assertTrue(args[0] >= args[1].min)\n\tself.assertTrue(args[0] <= args[1].max)"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn args\n\t\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tsuper(MaxMinArgumentValidator, self).set_arguments(args)\n\tif self.min < self.max:\n\t\treturn _(\"arguments must be maximum values.\")\t\t\n\tif not self.min and not self.max:\n\t\treturn _(\"arguments must be between minimum and maximum.\")\n\n## ----------------------------------------------------------------------------------------\n##\n## ----------------------------------------------------------------------------------------\ndef setup_validators(validators):\n\t\"\"\"\n\tSetup the validators used to validate certain parameters on the page.\n\t\"\"\"\n\tfor validator in validators:\n\t\tvalidator.setup(valid"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin_value = max_value = None\n\tfor arg in args['arguments']:\n\t\tif arg is not None:\n\t\t\tval = arg.value\n\t\t\tif val < args['min']:\n\t\t\t\tmin_value = val\n\t\t\tif val > args['max']:\n\t\t\t\tmax_value = val\n\t\telse:\n\t\t\t\treturn \"Argument does not exist\"\n\treturn (min_value, max_value)\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args[0] > args[-1]:\n\t\tprint(\"Please choose the maximum value of the first argument.\")\n\telif args[0] < args[-1]:\n\t\tprint(\"Please choose the minimum value of the first argument.\")\n\telse:\n\t\tpass\n\n\ndef validate_bust_citation_arg(args):\n\t\"\"\"\n\tValidates a given argument which is a list of citations.\n\t\n\t:param args: The received arguments\n\t\"\"\"\n\tif not isinstance(args, list):\n\t\tprint(\"Please choose a list of citations.\")"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn args and min(args) <= max(args)\n\ndef validate_args(args):\n\t'''\n\tCheck if given arguments are integers.\n\t\n\t:param args: The value of arguments that should be valid integers.\n\t'''\n\treturn args and isint(args.min_min_max or args.max_max"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin_val = args[0]\n\tmax_val = args[1]\n\tif min_val!= '' and max_val!= '':\n\t\tif self.getSageMinMaxVal(min_val) == self.maxSageMinMaxVal and self.getSageMinMaxVal(max_val) == self.maxSageMinMaxval:\n\t\t\treturn True\n\n\tsage_log.debug('Sage:validate_min_max_args(<%r>,<%r>,...)' % (args[0], args[1]))"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tassert isinstance(args, int), \"Argument is not integer\"\n\t\n\tif args:\n\t\treturn int(args) >= 0\n\t\t\n\treturn False\n\n@returns_to_python\n@returns_to_json\ndef validate_array(*args, **kwargs):\n\t\"\"\"\n\tReturns the validated array of the object.\n\t\n\t*args - arguments of array. The values of the argument values will be validated and the object's type and class will be returned.\n\t*kwargs - other arguments for array. They will be passed directly through to"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\tmin_value = args.get('min')\n\t\tmax_value = args.get('max')\n\t\tif min_value is None:\n\t\t\traise ValueError(\"min value cannot be None\")\n\t\tif not max_value is None and min_value > max_value:\n\t\t\traise ValueError(\n\t\t\t\t\"min value must be less than max value, but is '{}'\".format(max_value)\n\t\t\t)\n\texcept (TypeError, ValueError):\n\t\traise SyntaxError('Invalid argument syntax')\n\n\tvalidator = self.__"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# The value is defined by a min (or 0) and max (or 1):\n\tif args[0] <= args[1] and args[0] > args[1] + 1:\n\t\treturn True # Success!\n\treturn False # Value is out of range!\n\t\n\t\n\t\n"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tdef __init__(self):\n\t\tComplexArgumentType.__init__(self, type_name)\n\t\n\t@property\n\tdef help(self):\n\t\t\"\"\"\n\t\tReturn the help of this command as a string\n\t\t\"\"\"\n\t\treturn 'the complex argument type\\n\\n' + ComplexArgumentType.help\n\n__plugin = ComplexArgumentType\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t)\n\t\n\targumeters_dict = { \n\t\t\"type_name\": type_name,\n\t\t\"option_name\": option_name,\n\t\t}\n\t\n\tt = ComplexComplexArgument(0, type_name, option_name)\n\td = argumeters_dict\n\treturn d\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t)\n\tself.complex_type_create(\n\t\tsubcommand, \n\t\t\"argument\", \n\t\ttype_name, \n\t\toption_name, \n\t\tdict(spec_option) if spec_option else None\n\t)\n\treturn self.complex_type_find(\n\t\ttype_name, \n\t\toption_name\n\t)\n\n\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\treturn get_complex_type(type_name, spec_option, subcommand)\n\n\n############################################################################\nclass ArgumentType(object):\n\n\t#############################################\n\t# arguments defined as a single argument\n\t#\n\n\tdef create_single_argumet_type(self, name, **kwargs):\n\t\treturn ArgumentString(*name, **kwargs)\n\n\n############################################################################\nclass IntArgumentType(ArgumentType):\n\n\tOPTION_TYPE = (Option, IntOption)\n\n\tdef create_single_argumet_type(self, name"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t):\n\t\ttype_spec = \"\"\n\n\t\tif type_spec!= \"\":\n\t\t\tif (type_spec == \"complex\"):\n\t\t\t\tcomplex = ComplexType(type_name)\n\t\t\t\ttype_spec = complex.subtype(type_name, subcommand, type_spec = option_name[1:], \n\t\t\t\t\tdescription = option_name[2:], argument = type_name,\n\t\t\t\t\tsuboptarg = option_name)\n\t\t\telif (type_spec == \"simple\"):\n\t\t\t\tcomplex = SimpleType(type_name)\n\t\t\t\ttype_spec = complex"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tself.current_type_name = None\n\tself.current_argumet_type = None\n\tself.current_option = None\n\tself.current_argumet = None\n\tself.current_complex = None\n\tself.argumet_type_name_array = None\n\tself.argumet_type_name_value = None\n\tself.option_name_array = None\n\tself.option_name_value = None\n\tself.complex_argumet_type = None\n\tself.complex_argumet_type"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t):\n\t\treturn types.ComplexArgument()\n\nclass ComplexArgument(types.Argument):\n\tdef create_element(self, index):\n\t\treturn types.ComplexElement()\n\n\tdef create_element_fromstring(self, string):\n\t\treturn types.ComplexElement()\n\n\tdef create_sequence(self, sequence, name):\n\t\treturn types.ComplexSequence(name)\n\nclass ComplexArgument(types.Argument):\n\tdef create_seqex(self, seq):\n\t\treturn types.ComplexSequenceEx()\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t   pass\n    \n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n    \n    parser = ArgumentParser(description='Create complex types using argument parsing')\n    \n    parser.add_argument(\n       'step', type=str, help='the step you want to run.'\n        )\n    parser.add_argument(\n        'type', type=type, help='the type. Valid types are: a string, a list of strings, or a complex number (numeric value)')\n    parser.add_argument(\n    "}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t):\n\t\n\t\t\n\tself.log('create_array_type is called')\n\t\n\t\t\n\ttype_list = []\n\tcomp_type_list = []\n\t\n\tdef extract_list(self, opts):\n\t\t\n\t\tif opts.type == 'complex':\n\t\t\t\n\t\t\t\n\t\t\ttype_name = opts.type_name\n\t\t\ttype_list = self.extract_complex_type(\n\t\t\t\t\t\t\ttype_name=type_name,\n\t\t\t\t\t\t\toption_name=opts.option_name\n\t\t\t\t\t\t\t\n\n\t\n\tdef extract"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t )\n\nclass ComplexArgumentOption(object):\n\t\"\"\"\n\tHelper class for creating ComplexArgument\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.opt_name = None\n\t\tself.sub_cmd = []\n\t\tself.complex_type_path = None\n\t\tself.complex_type_filename = None\n\t\tself.option = None\n\t\t\n\tdef create_complex_argumet_type_option(self, subcommand, type_name, option_name, spec_option):\n\t\t\"\"\"\n\t\tCreate a new option and"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\tfor arg in args:\n\t\t# check for nested sub-args\n\t\tif isinstance(arg, dict):\n\t\t\t# create sub args\n\t\t\tcontrol_args[arg['name']] = arg\n\t\t# if not nested it's a custom arg\n\t\telif not arg['name'].startswith('Control'):\n\t\t\tcontrol_args[arg['name']] = arg\n\t\t# otherwise set it\n\t\telse:\n\t\t\tnested_args[arg['name']] = arg\n\treturn (control"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args_dict = {}\n\tnested_args_dict = {}\n\tcustom_variables = set()\n\t\n\tif len(args):\n\t\t# collect all the custom args\n\t\tcustom_args = []\n\t\tfor arg in args:\n\t\t\t# put the parameter in a hash for searching later\n\t\t\tcustom_args = custom_args + [{\"param\": arg}]\n\t\t\n\t\tfor arg in custom_args:\n\t\t\tkey = arg[\"param\"]\n\t\t\tcontrol_args_dict[key] = arg[\"value\"]\n\t\t\t\n\t\t\t# add nested args"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\targ_dict = {}\n\tnested_args = []\n\tcustom_args = []\n\t\n\tif args:\n\t\tfor arg in args:\n\t\t\tif arg_dict.has_key(arg):\n\t\t\t\targ_dict[arg]['custom'] = 1\n\t\t\telse:\n\t\t\t\targ_dict[arg] = { 'dict': {}, 'custom': 0 }\n\t\t\tif not arg_dict.has_key(arg):\n\t\t\t\targ_dict[arg] = {}\n\t\t\t\t\n\t\t\tif arg['args'] in custom_args:\n\t\t\t\traise AnsibleError"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = []\n\tnested_args = []\n\tif args:\n\t\tif args[0]=='control' or args[0]=='nested':\n\t\t\tfor inner in args:\n\t\t\t\tif inner!='control': #ignore nested and control args\n\t\t\t\t\tcontrol_args.append(inner) #append control, arg to be used directly\n\t\t\t\tif inner=='control':\n\t\t\t\t\t#add the control as a nested argument\n\t\t\t\t\t#only nested args, with no args will give an empty arg.\n\t\t\t\t\t#this is used by the Ansible playbooks (when a"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\tcontrols_args = []\n\tcontrol_args = []\n\t\n\tfor arg in args:\n\t\tif \"control\" in arg:\n\t\t\tnested_arg = kwargs_get(arg, \"control\")\n\t\telse:\n\t\t\tnested_arg = None\n\t\tcontrol_arg = kwargs_get(arg, \"control\")\n\t\tcontrol_args.append([nested_arg, control_arg])\n\t\t\n\treturn control_args, controls_args\n\t\n\t\ndef nagios_output(spec):\n\toutspec ="}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\tcontrol_args = args.control.split(\",\")\n\tnested_args = args.nested.split(\",\")\n\t\n\treturn control_args, nested_args\n\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_arg_name\t= \"control_args\"\n\tnested_arg_name\t= \"nested_args\"\n\n\tcontrol_args\t= dict()\n\tnested_args\t\t= dict()\n\n\tfor arg in args:\n\n\t\tif arg[0].upper() == \"RUN_AS_PLAYBOOK\":\n\t\t\t\n\t\t\tif not \"NO_CLI\" in arg:\n\n\t\t\t\t# Add arg to control_args\n\t\t\t\tif arg[1] == \"as\":\n\n\t\t\t\t\tcontrol_args[\"as\"]\t= arg[2]\n  \t\t\t\t\t\n\t\t\t\telse"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\treturn {\"control\" : args, \"nested\" : args}\n\n\nclass HostConfigParser(object):\n    def __init__(self, spec_fname):\n        self.spec_fname = spec_fname\n        self.base = {\n            \"cmd\" : None,\n            \"command\" : None,\n            \"shell\" : None,\n        }\n\n        for line in open(self.spec_fname).readlines():\n            if line.startswith(\"HOST_\"):\n                attr_name = line.split(\"-\",1"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = self.get_nested_args(args)\n\tcontrol = {}\n\tcontrol['nested_args'] = control['control_args']\n\n\tfor sub_arg, control_arg in control_args.items():\n\t\tcontrol['control_args'].append(sub_arg)\n\t\tcustom_arg = control_arg.get(\"custom\", [])\n\t\tfor custom_arg_item in custom_arg:\n\t\t\tvar_path = custom_arg_item.split(\"=\", 1)\n\t\t\tvar_name = var_path.pop(0"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--nested_\"):\n\t\t\tnested_args[arg[len(\"--nested_\"):]] = arg.replace(\"--nested_\", \"\")\n\t\telse:\n\t\t\tcontrol_args[arg] = arg\n\n\treturn (control_args, nested_args)\n\ndef play_from_args(args, play):\n\t# if we got no args, just print an error message about no input provided\n\tif not args:"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = []\n\tif not isinstance(extra_vars, list):\n\t\textra_vars = [extra_vars]\n\treturn dict(merge_dicts(vars_dict, extra_vars))\n\ndef to_json(obj, **options):\n\treturn str(obj)\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict.update(extra_vars)\n\tvars_dict['__extrainfo__'] = extra_vars\n\n\ndef get_extras(extras_type=None):\n\t\"\"\"\n\n\t:param extras_type: Type of extras to add\n\t:return: extra(s) for {extras_type}\n\t\"\"\"\n\treturn {\n\t\t'name': 'externals{}'.format(extras_type.upper()),\n\t\t'description': 'Add extras to run'\n\t}\n\n\ndef add_extras_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict['VARS']=extra_vars\n\treturn vars_dict\n\ndef load_config(config=None, cfg_file=None, env=None):\n\timport json\n\timport sys\n    config_path = os.path.abspath(config or cfg_file or sys.argv[1])\n\tconfiguration = {config_path: {}}\n\tprint(\"Loading configuration from %s\" % config_path)\n\ttry:\n    \twith open(config_path) as config_file:\n\t\t\t\tconfiguration"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars is None:\n\t\textra_vars = []\n\tvars_dict.update(extra_vars)\n\treturn vars_dict\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\tdel vars_dict[self.extra_vars]\n\telse:\n\t\tvars_dict[self.extra_vars] = extra_vars\n\ndef add_required_attribute(cls):\n\t\"\"\"\n\t\tAdd the required_attribute and all required attributes to an\n\t\tobject\n\t\"\"\"\n\tcls.required_attribute = cls.attribute.name\n\tfor obj in getattr(cls, 'objects', []):\n\t\tsetattr(obj,'required', \n\t\t\t\t\tlambda obj: obj."}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or None\n\tfor k, v in vars_dict.items():\n\t\tif k in extra_vars:\n\t\t\tvars_dict[k] = extra_vars[k]\n\treturn vars_dict\n\ndef _get_from_env(env_key, default_value=None, type_=None,\n                  **kwargs):\n\tif not type_:\n\t\ttype_ = type(default_value)\n\n\tif hasattr(type_, '__call__'):\n\t\treturn type_("}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tif extra_vars:\n\t\tvars_dict.update(extra_vars)\n\n\treturn vars_dict\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\treturn vars_dict\n\t\n\tif not vars_dict:\n\t\tvars_dict = copy.deepcopy(extra_vars)\n\telse:\n\t\tfor k in vars_dict:\n\t\t\tif k in extra_vars:\n\t\t\t\tvars_dict[k] += extra_vars[k]\n\t\t\telse:\n\t\t\t\tvars_dict[k] = extra_vars[k]\n\t\n\treturn vars_dict\n\ndef merge_dicts(d1, d2):\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars is not None:\n\t\tvars_dict = vars_dict.copy()\n\t\tvars_dict.update(extra_vars)\n\n\treturn vars_dict\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = []\n\n\tif vars_dict:\n\t\tvars_dict = extra_vars + vars_dict\n\t\tfor extra_var, value in vars.items():\n\t\t\tif type(extra_var) is not list and type(sub_vars) is not list:\n\t\t\t\t# we want to create a list based on the value\n\t\t\t\tvars_dict[extra_var][value] = vars_dict.pop(extra_var)\n\n\ndef process_dependencies(toadl_"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t));\n\"\"\"):\n    pass\n\n\ndef check_for_playbook_file_exists(filename):\n    \"\"\"Returns True if the File exists.\"\"\"\n    return os.path.isfile(filename)\n\ndef check_for_playbook_file(playbook_path):\n    \"\"\"Checks a play with a playbook path.\"\"\"\n    run_plumed(['-p', playbook_path])\n\n\ndef run_plumed(cmd):\n    \"\"\"Executes an array of commands and returns the return value.\n\n    A simple form of `plumed-python"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\tdef exec_playbook(args, playbooks, play_name=None, extra_vars=dict()):\n\t\textra_vars.update(extra_vars=extra_vars)\n\t\treturn args.playbook(*args.play_vars(*extra_vars))\n\n\tdef playbook_info(module_path):\n\t\t\"\"\"\n\t\tReturn a dictionary of information for the current playbook (which should\n\t\tbe executed).\n\t\t\"\"\"\n\t\treturn { \n\t\t\t\n\t\t\t'ANSIBLE_PROMPT_TERM': 'y',\n\t\t\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\timport json\n\tfrom time import sleep as sleep_time\n\tfrom urllib.error import URLError\n\n\t# pylint: disable=invalid-name\n\timport ansible\n\timport requests\n\n\tdef get_result(success, result):\n\t\tif result:\n\t\t\treturn result\n\t\treturn '', False\n\n\tdef set_time_offset(offset):\n\t\t\"\"\" sets an offset for time for the plugin execution\n\n\t\t:param offset: a time offset as seconds (optional)\n\t\t\"\"\"\n\t\treturn offset\n\n\tdef plugin_result(success"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tdef runner_execute(**kwargs):\n\n\t\tfor variable in ansible_playbook_args:\n\n\t\t\ttry:\n\t\t\t\tkwargs['VARS'][variable.lower()] = kwargs[variable]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\n\t\ttry:\n\t\t\tfrom ansible.playbook.task_include import Include\n\t\t\ttask_include = Include(**ansible_playbook_args)\n\t\t\tti = task_include.run()\n\t\t\tti.add_ignore('run')\n\t\texcept ImportError:\n\t\t\ttask_include = Include(**ansible"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n        vcr = None\n        # The argument to playbook\n\tplaybook = {\"args\": extra_vars, \"stdout\": \"\"}\n\tresult = None\t\n\n        # If no extra_vars, then we need to see the arguments and\n        # use the default args of the current plugin\n        if extra_vars is None: \n        \targs_found = [a for a in playbook_path.strip().split('\\n') if a]\n        \textra_vars = self.plugin_args(ir_workspace, options.verb"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t) -> str:\n\n\t\t# create the Ansible CLI and run it\n\t\treturn play_cmd(ir_plugin, playbook_path, extra_vars=extra_vars, ansible_args=ansible_args, stdout=True, stderr=False, verbose=verbose) # noqa\n\n\t\t## Create args for ansible-playbook cmd:\n\t\t# These may need to be modified if you are using this command in the shell\n#\t\t#ansible_args = {'ANSIBLE_NAMESPACE': '/usr/bin/ansible',"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t),\n\tmodule_args: dict,\n)\n\ndef run_playbook(plugin_name, workbook_path, extra_vars={}):\n\t\"\"\"\n\tRun a playbook\n\t\tAn example playbook is as follows:\n\t\n\t\t.. code:: none\n\n\t\t\t# ansible 1.9.2\n\t\t\t# -*- coding: utf-8 -*\n\t\t\t- name: create_playbook.yaml\n\t\t\t- template: #!/usr/local/bin/ansible-playbook -i {playbook_file} -o {outputs}"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t), verbose)\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\n\t#\n\t# The Ansible base classes will take care of parsing the args and args string, so\n\t# avoid repeating that here.\n\t#\n\textra_vars = extra_vars or {}\n\targs, arg_options = parse_args(extra_vars, playbook_path=playbook_path)\n\tif arg_options:\n\t\tif not verbose:\n\t\t\traise AnsibleOptionsError('Api args do not include verbose flag.')\n\t\tcmd_args = {i:a for (i, a) in arg_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t);\n\n\n\ndef execute_playbook(action, ir_workspace, ir_plugin, playbook):\n\n\t# Find the command path from the playbook path where the call is passed\n\tfor playbook_path_element in playbook.split('/'):\n\t\tif playbook == playbook_path_element:\n\t\t\tbreak\n\telse:\n\t\tplaybook_path = './' + playbook\n\tplaybook_path_args = {\n\t\t'action' : {\n\t\t\t'ansible_playbook_args' : action,\n\t\t\t'playbook_path' : playbook_path"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t#print(\"in _run_playbook\")\n\t#print(\"vars_dict %s\" % vars_dict)\n\tassert cli_args, \"No command line arguments passed\"\n\t# Set the default options (all of which are required)\n\toptions = {}\n\tcli_args.extend(opt.add_default_option_for_subcommand(cli_args) for opt in default_options(cli_args))\n\t# We need to set the options for all the subcommands to be able to run other subcommands\n\toptions.update"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\tcli_vars = ''\n\t\n\tansible_result = dict()\n\t\n\tplaybook_data = ['{{vars_dict}}',\n\t\t\t\t\t  '-c', '-t', '{{default_playbook}}',\n\t\t\t\t\t  ir_workspace.filePath] + cli_args\n\t\n\t# Make sure all variables come together (will create a new var that won't be in the var_list in next loop)\n\tfor var in cli_args:\n\t\tplaybook_result_value = (playbook_data, i)\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t_args = cli_args\n\targv = _args\n\targs = []\n\t\n\t# Use an environment to override variables\n\tenv = None\n\tfor env in vars_dict:\n\t\t\n\t\tif env.has_key('ENV'):\n\t\t\targs = args + [env['ENV']]\n\t\telse:\n\t\t\targs = args + [env['VAR']]\n\t\n\t# set any additional args as cli args\n\targs = cli_args + ['--' + str(' '.join(args))]\n\t\n\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t_CLI_ARGS = cli_args[:]\n\n\t# Get CLI args from module\n\t# Get module parameters from modules\n\t# If required_vars_param is not present in the variables dict\n\t# then get it from cli args.\n\tif 'plugin' not in vars_dict:\n\t\tfor name in cli_args:\n\t\t\tif name in _CLI_ARGS:\n\t\t\t\tvars_dict['plugin'] = None\n\tif 'vars' not in vars_dict:\n\t\tfor name in cli_args:\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t# create the working directory if not already defined\n\ttry:\n\t\tos.mkdir(cli_args.work_dir)\n\texcept OSError:\n\t\tpass\n\n\n\t# the vars we are going to be going to run:\n\n\t# load the vars file\n\tvars_file = 'infrared.vars'\n\twith open(vars_file, 'rb') as vars_handle:\n\t\tvars_dict = json.loads(vars_handle.read())\n\t\n\t# convert the vars file data to"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tcmd = \"ansible \"+str(ir_plugin.command).replace(\" \",\"+\")+\" \"+str(ir_plugin.param)+str(ir_plugin.output_type)+str(ir_plugin.output)\n\t\n\tres = {}\n\trun_module(\"python \"+cmd)\n\tres[\"result\"] = run_module(\"python \"+cmd)\n\treturn res\n\t\n\ndef _playbook_playbook(ir_plugin):\n\tcmd = \"\".join([ir_plugin.playbook,\"--module\",str(ir_plugin.command),str"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\timport ansible\n\tresults = []\n\t\n\t# Check arguments\n\tif not (len(cli_args) == 8 and list(vars_dict.keys()) == list(cli_args.keys())):\n\t\traise errors.AnsibleError(\"Please provide the cli arguments, all you need to do is:\")\n\tif list(vars_dict.keys())().count(\n\t\t\tlist(vars_dict.values().keys())(((\"-v\", \"verbose\"),)\"|\")) > 0:\n\t\traise errors.AnsibleError(\""}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tenv = Environment(loader=PipEnvLoader())\n\tenv.playbook = env.prepend_path('{0}/playbooks'.format(ir_plugin.get_plugin_path(plugin='playbook')))\n\tenv.args = cli_args\n\tenv.variables = vars_dict\n\t\n\tcwd = os.getcwd().replace('\\\\','/')\n\tif(os.path.isdir(ir_plugin.get_plugin_path(plugin='playbook') + '/' + cwd + '/' + env"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\trun(cli_args, vars_dict)\n\t\n\t\n\t\ndef _get_results(cli_args, vars_dict, run_args=None, cli_output=None):\n\t\"\"\"\n\tReturns (result, error), which can be used to create a results file from\n\tthe AnsibleResults object.\n\t\n\t:param run_args: The command line arguments the Ansible Results\n\t:param cli_args: The list of command line arguments\n\t:param vars_dict: The Ansible Variables object\n\t:param cl"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\ttry:\n\t\tresult = vars_dict\n\t\tif '--dry_run' in cli_args:\n\t\t\tresult.update({\"ansible_rc\": 0,\n\t\t\t\t    \"ansible_playbook_contents\": \"\"})\n\t\telse:\n\t\t\tresult.update({\"ansible_rc\": 0,\n\t\t\t            # \"ansible_playbook_contents\": ir_playbook.name\n\t\t\t            })\n\n#\t\tif '--all' in cli_args:\n#\t\t\tresult.update({\"ansible_cmd\": cli_args"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor name, value in values_dict.items():\n\t\tif not isinstance(value, type(value)):\n\t\t\tvalues_dict[name] = str(value)\n\n\tparser = parser_name.lower()\n\tif parser == 'dns':\n\t\tparser = 'dns'\n\telif not parser in ('dns',):\n\t\traise AnsibleOptionsError(\"validator for %s is dns, but %s is given\" % \n\t\t\t\t('dns', parser))\n\n\treturn parser, values_dict\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif not values_dict:\n\t\treturn values_dict\n\tif parser_name == 'virsh':\n\t\treturn dict(value=values_dict)\n\tif parser_name == 'qmp':\n\t\tif 'output' in values_dict:\n\t\t\tvalues_dict['outputBinary'] = True\n\t\t\treturn values_dict\n\t\treturn dict(value='qemu-img')\n\tif parser_name == 'lvmi':\n\t\tif 'vcpupin' in values_dict:\n\t\t\tfor k,v in values_dict['vcpupin'"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tall_keys = values_dict.keys()\n\tall_keys.sort()\n\tfor key in all_keys:\n\t\tif key in self.valid_command_keys:\n\t\t\tvalue = values_dict[key]\n\t\t\tif isinstance(value, six.text_type):\n\t\t\t\tvalue = value.encode(\"ascii\", \"replace\")\n\t\t\t\tif sys.version_info >= (3, 5):\n\t\t\t\t\t# This is required by PY2 and other unversioned languages\n\t\t\t\t\tpass\n\t\t\telse:\n\t\t\t\traise Exception(\"The argument given as argument key"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif isinstance(values_dict['options.option1'], str) and 'options.option2' in values_dict:\n\t\tvalues_dict['options.option2'] = values_dict['options.option1']\n\tif isinstance(values_dict['options.option2'], str) and 'option1' in values_dict:\n\t\tvalues_dict['options.option1'] = values_dict['options.option2']\n\treturn values_dict\n\nclass LocalConfig(object):\n\t\n\tdef __init__(self, localconfig, filename):"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\tif values_dict.get('type'):\n\t\tdef convert_type(value_name):\n\t\t\tif value_name in ('str', 'int', 'bool', '_str', '_bool'):\n\t\t\t\treturn value_name\n\t\t\telse:\n\t\t\t\treturn value_name.replace(', ', '_')\n\t\tvalues_dict['type'] = convert_type(values_dict.get('type'))\n\treturn values_dict\n\n\nclass CommandHelp(CliCommand):\n\t\t\n\tUSAGE = \"\"\"\n\t\t<command> \n\t\t\t<usage"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tnew_values, _ = values_dict\n\tif self.__dict__ == {'__vlan__': int,\n\t\t\t\t'__clitag__': str,\n\t\t\t\t'__clitag_version__': int}:\n\t\tvalues_dict['__clitag_version__'] = 2\n\tfor v in new_values.keys():\n\t\tvalues_dict[parser_name + '_' + v] = new_values[v]\n\treturn values_dict\n\ndef _convert_cli_args(self, parser_name, values_dict):"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\tif 'values_dict' in values_dict:\n\t\tvalues_dict = values_dict['values_dict']\n\tif isinstance(values_dict, str):\n\t\tvalues_dict = values_dict.split(',')\n\t\n\tself.values.setdefault(parser_name, values_dict)\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tregex = parser_name + \" argument is a required and an object\"\n\tfor (key, value) in values_dict.iteritems():\n\t\tregex_obj = re.compile(key + r\".*\")\n\t\tif not regex_obj.match(value):\n\t\t\tvalues_dict[key] = value\n\treturn values_dict\n\n\ndef check_version(version):\n\t\"\"\" \n\t\tUtility function that can be used to check whether a given version is compatible.\n\t\t\n\t\t:param version: The version of yum to check\n\t\t:ret"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# Convert any non_cli argument values\n\tfor res in values_dict.keys():\n\t\tif res not in self._cli_subcmds.keys():\n\t\t\tvalues_dict[res] = values_dict[res]._convert(self._subcmds)\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser_values_dict = dict(values_dict)\n\tfor cmd_func_name in parser_values_dict:\n\t\tcurrent_value = parser_values_dict[cmd_func_name]\n\t\tif isinstance(current_value, basestring) or isinstance(current_value, collections.Mapping):\n\t\t\tcurrent_value = {cmd_func_name: current_value}\n\t\telif isinstance(current_value, list):\n\t\t\tcurrent_value = list(current_value)\n\t\t\tfor item in current_value:\n\t\t\t\tif is_"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn {\n\t\t\"type\": plugin_dir,\n\t\t\"name\": plugin_dir,\n\t\t\"args\": args.get_plugin_spec_plugin_args(plugin_dir), \n\t\t\"config_class\": config_class, \n\t\t\"plugin_spec\": plugin_spec,\n\t\t\"plugins_class\": plugins_class\n\t\t}\n\n\ndef get_plugin_spec_flatten_array(plugin_dir):\n\t\"\"\":returns array of dictionaries with properties for each plugin file\"\"\" \n\treturn[[[get_plugin_spec_flatten_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\timport os\n\tplugin_prop = dict()\n\n\tdirs = os.environ['PLUGIN_DIRS']\n\tplugin_path = os.environ['PLUGIN_PATH']\n\tplugin_spec = os.environ['PLUGIN_SPEC']\n\t\n\tplugin_configuration = dict()\n\n\tfor plugin_dir in dirs:\n\n\t\tif not os.path.isdir(plugin_dir):\n\t\t\tcontinue\n\n\t\tplugin_dir = plugin_dir + '/'\n\n\t\tdirectory_count = len(os.listdir(plugin_dir"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tlogger.debug('Flat dict to plugin spec:'+ plugin_dir)\n\tflat_dict = OrderedDict()\n\tfor key, value in get_plugin_spec_flatten_key_value.__dict__.items():\n\t\ttry:\n\t\t\tplugin_spec_flatten_key = key.decode('utf8')\n\t\t\tplugin_spec_flatten_value = value.decode('utf8')\n\t\texcept Exception as e:\n\t\t\tlogger.debug('FATAL ERROR:'+ re.sub('[:\\\\].*?"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tflattened = {}\n\tif os.path.isdir(plugin_dir):\n\t\tfor sub_dir in os.listdir(plugin_dir):\n\t\t\tif sub_dir.startswith(\".\"):\n\t\t\t\tsub_dir = plugin_dir + \"/\" + sub_dir\n\t\t\tflattened[sub_dir] = {}\n\t\t\tif os.path.isdir(sub_dir):\n\t\t\t\tflattened[sub_dir] = get_plugin_spec_flatten_dict(sub_dir)\n\t\n\treturn flattened"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tfactory = {\n\t \"name\" : plugin_dir,\n\t \"output_name\" : 'plugin_dir|output_name:',\n\t \"output_directory\" : 'plugin_dir|output_directory:',\n\t \"paths\" : {}\n\t }\n\n\tpaths =  plugin_dir.split(\"/\")\n\n\tfor path in paths:\n\t\tfactory.add_path(\"path\", path)\n\n\treturn factory\n\n\ndef get_plugin_spec_flatten_dict_ext(plugin_dir, extension):\n\t'''\n\tCreat"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn {name: list() for name in plugin_dir}\n\ndef get_plugin_spec_as_dict(plugin_dir):\n\t\"\"\"\n\tConverts the spec to a dictionary\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flat dict contains the plugin's properties\n\t\"\"\"\n\treturn dict(list(get_plugin_spec_flatten_dict(plugin_dir).iteritems()))\n\ndef get_version(plugin_dir):\n\t\"\"\"\n\tReturns the plugin's version from the dir\n\t\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\timport fnmatch\n\n\t# Loop over each plugin spec file\n\tfor fname in os.listdir(plugin_dir):\n\t\t# Check if the '*' or match against the patterns\n\t\tif fnmatch.fnmatch(fname, '*'):\n\t\t\tcontinue\n\n\t\tf = open(os.path.join(plugin_dir, fname), 'r')\n\t\tline = f.readline()\n\t\tproperties = {}\n\n\t\tfor p in line.split():\n\t\t\ttokens = p.split()\n\n\t\t\t# Check if the plugin's name contains dot or equals"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\tdict_plugins = {\n\t\t\"type\": plugin_dir,\n\t\t\"metadata\": {\n\t\t\t\"properties\": {\n\t\t\t\t\"flatten\": {\n\t\t\t\t\t\"items\": flatten_dict_property\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn dict_plugins\n\n\n\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tflatten_dict = {}\n\twith gfile.Open(plugin_dir, 'r') as stream:\n\t\tfor line in stream:\n\t\t\tline = line.rstrip()\n\t\t\tif line == '':\n\t\t\t\tcontinue\n\t\t\tsplit_line = line.split(':')\n\t\t\tflatten_dict[split_line[0].strip()] = { split_line[1].strip(), split_line[2].strip() }\n\treturn flatten_dict\n\ndef is_not_contain_spec_dict_value(dict_entry, spec_dict):"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t# Create a dict with all properties in plugin\n\tproperties = plugin_dir.__dict__.copy()\n\t\n\t# Get the list of properties for the plugin\n\tproperties = [ p.strip() for p in properties.replace('\\n', '').split(',') ]\n\t\n\treturn { properties[0] }\n"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif self.config == '':\n\t\tself.config = ''\n\t\tself.logger.debug('environment variable '+config_env_path+' = $'+self.config)\n\t\t\n\tif not self.config.startswith('$'):\n\t\tself.config = '${'+self.config+'}'\n\t\tself.logger.debug('environment variable '+config_env_path+' = $'+self.config)\n\t\t\t\n\treturn self.config"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tconfigure.set_config(_config)\n\ndef inject_environment():\n\t\"\"\"\n\tSet the environment variable for environment name, if it is undefined.\n\t\"\"\"\n\tenv = __config__.get_var(_env)\n\tenv = env or \"env\"\n\tsetattr(sys, env, getattr(sys, env))\n\ndef get_config(defaults=None):\n\t\"\"\"\n\tGet the environment variable for the default configuration file path, if it is None.\n\t\"\"\"\n\tenv = _config[(\"env\", None)]\n\treturn _"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv={}\n\tif self.config_path:\n\t\tenv[\"PATH\"]=self.config_path\n\tprint(\"Set the config path \" + str(env))\n\tos.environ = env\n\n\ndef main():\n\tparser = argParser()\n\targs = parser.parse_args()\n\ttry:\n\t\tconfigobj = ConfigParser.ConfigParser()\n\t\tconfigobj.read(args.config_path)\n\texcept OSError, err:\n\t\tprint \"Fatal error: \" + err\n\t\texit()\n\tif args.config_"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tconfig_path = \"\"\n\ttry:\n\t\tconfig_path = os.environ[\"CONFIG_PATH\"]\n\texcept Exception as exception:\n\t\tconfig_path = \"./config/conf/__dict__\"\n\n\treturn config_path\n\ndef inject_sys_vars(conf):\n\t\"\"\"\n\tTries to determine the sys_vars config path and loads the sys_vars or sys_vars_files\n\t:return dict: The loaded values (or None if no sys_vars_files were present)\n\t\"\"\"\n\n\tvars = None"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tconfigdir = os.environ.get('PILES_XML')\n\tif not configdir:\n\t\tconfigdir = os.path.expanduser('~/.config/pil')\t\t\n\tself.config = os.environ.get('PILERS_XML')\n\tif not self.config:\n\t\tself.config = os.path.join(os.path.expanduser('~'),'.config')\n\nclass ConfigLoader:\n\t\n\tdef configure(self):\n\t\tglobal ConfigDir\n\t\tif not os.path.isdir"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tconfigpath = self.getconfig(\"path\")\n\tif configpath == \"\":\n\t\tconfigpath = \"/etc/os/etc\"\n\tif is_windows:\n\t\t# If we are running in windows, add config.path to the path to override\n\t\t# the environment config.path\n\t\tconfigpath += \"\\\\\" + os.environ['APPDATA']\n\tos.environ['PATH'] = configpath\n\treturn\n\n#------------------------------------------------------------------------\nclass Base64(object):\n\t\n\tdef _encrypt(self,text):\n\t\treturn base64."}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tpath = os.environ.get('MULTIDNOPATH')\n\tif hasattr(os, '_environ'):\n\t\tos.environ['MULTIDNOPATH'] = path\t\n\tos.environ['MULTIDNOPATH'] = path\n\ndef test_multidnopath():\n\t\"\"\"\n\tTest whether the path is used by multidnopath\n\t\"\"\"\n\tfor module in os.listdir(__path__):\n\t\tif re.search('multidnopath', module) is not None:\n\t\t\tbreak\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv_config = self.config_handler.config[\"env\"]\n\tif env_config is None:\n\t\tenv_config = envdir + \"config\"\n\t\tfor var in self.vars:\n\t\t\tself.vars[var][\"env\"] = env_config\n\n\t\tself.logger.log(\"Set environment variable\", self.log.GREEN, [\"env\"])\n\n\tif self.vars.get(\"env\"):\n\t\tnew_name = env_config + \".ini\"\n\t\tself.config_handler.config.set(\"env\", new_name)"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv = self.args.get('-e')\n\tif env is not None:\n\t\t# if it specified a env then let's call it\n\t\texec env in globals(), locals()\n\telse:\n\t\t# otherwise use the environment variable\n\t\tconfig = os.environ.get('APP_HOME')\n\t\tif config is None:\n\t\t\treturn None\n\t\t# look for the last dot\n\t\tdot = config.rfind('.')\n\t\tif dot < 0:\n\t\t\tdot = -1\n\t\tif dot > 0:\n\t\t\tconfig = config[:dot]"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif \"config.path\" not in os.environ:\n\t\tself.__dict__[\"config.path\"] = __import__(\"{0}.config\".format(\n\t\t\tself.__module__))[\"PYTHONPATH\"].rstrip(\"/\") + \"/{0}.conf\".format(__name__)\n\t\tos.environ[\"PATH\"] = \\\n\t\t\tos.environ.get(\"PATH\",\n\t\t\t\tos.environ.get.call(os.environ,\n\t\t\t\t\tos.environ[\"PATH\"])) + \\\n\t\t\tself.__dict__"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\timport sys\n\t\n\tparser = root_subparsers.add_parser('spec', help='the Spec cli options')\t\n\tfor opt in root_subparsers.subparsers.keys():\n\t\tparent_parser = root_subparsers.subparsers._parent\n\t\tparent_parser.add_argument(opt, default = None)\t\n\t\tparser.add_argument(opt)\n\t\n\tparser.set_defaults(func = self.run)"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tdef _add_options(self, parser, base):\n\t\tnamespace = {}\n\t\tfor namespace_key, options in kwargs_to_namespaces(self.plugins):\n\t\t\tfor shortname, shortopt in namespace[namespace_key]:\n\t\t\t\tparser.add_argument(shortname, shortopt, required = True, help=shortopt)\n\n\t\tfor shortname, action in self.get_plugins(kwargs_to_namespaces(self.plugins)):\n\t\t\tparser.add_argument(shortname, action)\n\n\tdef create_argparser("}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tfor k,v in root_subparser.cli_aliases.items():\n\t\tsubparser = subparser.add_parser(k,help=v)\n\t\tsubparser.add_argument('--version', action='version', version = __version__)\n\t\tsubparser.add_argument('--help', action='help')\n\t\n\ndef get_args():\n\timport optparse\n\tparser = optparse.OptionParser(\n\t\tusage = \"%prog [options]\",\n\t\tusage = \"\"\"\n\t%prog [-h] [-w [--wiki]] [-"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\treturn add_cli(self, root_subparsers)\n\ndef add_cli(self, root_subparsers):\n\t\"\"\"\n\tCreates a new \"cli\" object with the same command line options as the current object.\n\t\n\t:param root_subparsers: list of the root subparsers to use in order of registration\n\t\n\t:returns: cli object\n\t\"\"\"\n\tself.register(root_subparsers)\n\treturn cli.make_cli(root_subparsers)\n\n\ndef add"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\timport cli\n\t\n\tclass ExtendedSliceParser(cli.SliceParser):\n\t\t\"\"\"\n\t\tAdds a \"extend\" attribute to the main entry point of the subparser\n\t\t\n\t\tThis parser adds cli options that make it easier to define custom sub\n\t\tparsers. Each cli option is a (name, (default value, description)) tuple\n\t\tspecification, where the value is the default or the option's value.\n\t\t\"\"\"\n\t\t\n\t\tdefault_parser = ( \"name\", cli.CmdLine )\n\t\tdef create_parser(self, name"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\troot_subparsers.add_parser('set', short_help='Set a key value\\nThe value can be either a string or an integer')\n\troot_subparsers.add_parser('get', short_help='Get a key value\\nReturns nothing if key not found\\n')\n\troot_subparsers.add_parser('update', help='Update settings.py\\n')\n\troot_subparsers.add_parser('delete', help='Delete settings.py\\n')\n\t\n\troot_subparsers.add"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers.add_argument_group('cli options', 'Configuration options related to'+\n\t\t'the cli cli runner.')\n\tsubparsers.add_argument(\n\t\t'--verbose',\n\t\tdest='verbose_log',\n\t\taction='store_const',\n\t\tconst=True,\n\t\tdefault=False,\n\t\thelp='log verbose calls')\n\tsubparser_clirec = subparsers.add_parser(\n\t\t'cli',\n\t\tdescription='cli runner to run cli cli commands.')\n\tsub"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t## Get base classes for extension\n\tmain_class = self.__class__.__name__\n\tsubparsers = root_subparsers.add_parser(main_class, help=\"The clilib specs\", prog=main_class)\n\textend_cli_options(self, subparsers, self.__class__,\n\t\toption_classes={\n\t\t\t\"extend\": self.__class__.__name__\n\t\t}\t)\n\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\troot_parser_name = self.arg.get('--cli')\n\tif root_parser_name is None:\n\t\troot_parser_name = None\n\telif len(root_parser_name) == 1:\n\t\troot_parser_name = '--cli'\n\t\n\t#print(\"Adding cli sub command to parser: %s\" % str(root_cmd))\n\t# Parse all other subcommands at entry point\n\tfor name, cli_sub_parser in inspect.getmembers(type(self), lambda m: m[1]"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\topts = root_subparsers.add_parser('extend', help='extend cli options')\n\topts.add_argument('cli', help='cli name')\n\topts.add_argument('--debug', default=False, action=\"store_true\", \n\t\t\t\t\thelp='enable debugging')\n\topts.add_argument('--version', action=\"version\", version=__version__)\n\t\n\treturn opts\n\t\ndef print_cli_args(cli_arg):\n\t\"\"\"\n\tAdds the c to to printing to stder"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not root: return  # no path\n\tif not path.endswith('/'): return # no trailing slash\n\tpath_length = len(root)\n\tif path[-path_length:].isdigit():\n\t\treturn  # doesn't need to be a root\n\tif path.endswith('/lib'):\n\t\troot = 'lib'\n\telse:\n\t\treturn\n\n\tif root == '':\n\t\treturn\n\treturn './' + path.rstrip('/')\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not path or not root: return path\n\tif len(path) > 1 and tree_has_root(path): path = tree_get_root(path)\n\tif root: path = path[1:] # first path element is always root, remove it\n\treturn path\n\t\t\t\nfrom rpython.rlib.objectmodel import _object_for_python_object\ndef split_cwd(path):\n\t\"\"\"\n\tSplit current working directory or home dir by path.\n\tThis can be useful when debugging on Windows.\n\t\"\"\"\n\t# remove all leading absolute"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif root == '':\n\t\treturn path\n\n\tif os.path.islink(path):\n\t\treturn os.path.realpath(path)\n\n\tif path == abspath(cwd) or path.startswith(cwd):\n\t\treturn path\n\n\traise Exception('Cannot remove %s from path %s'%(path, cwd))\n\n\n\nclass Config(defaultdict):\n\t\"\"\"\n\t\tA Config object stores all default parameters that\n\t\"\"\"\n\n\tdef __init__(self, **kwargs):\n\t\tself.defaults = dict((i"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\treturn path.strip(root).split(os.sep)\n\ndef find_file(root, path):\n\t\"\"\"\n\tSearch the path for the given filename, taking path_root as a candidate.\n\tIf successful, return the name of the file, else return None\n\t\"\"\"\n\tif not root.startswith(os.sep):\n\t\traise ValueError(\"File should be relative to the project root\")\n\tif len(os.sep)!= 1:\n\t\troot = root.replace(os.sep,\"/\")\n\tif root.end"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tassert root\n\tif path is None:\n\t\treturn None\n\n\tif root == '/' or root == '' or root == '':\n\t\treturn ''\n\n\tnewDir = path\n\twhile newDir!= '/':\n\t\tparent = os.path.join(newDir, '')\n\t\tif parent in path:\n\t\t\tbreak\n\t\tnewDir = os.path.abspath(newDir)\n\t\tif not os.path.isdir(newDir):\n\t\t\treturn None\n\tif path[:len(newDir)] == newDir:\n\t\treturn path[len("}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\treturn path.partition(root)[2]\n\texcept:\n\t\traise ValueError('Invalid root %r' % root)\n\ndef get_package_root(path):\n\t\"\"\"\n\tReturn path's parent directory (excluding root), throw exception on failure.\n\t\"\"\"\n\tpath = strip_root(path, package_root)\n\tpwd = os.path.join(os.path.dirname(path), 'package_root')\n\treturn os.path.abspath(pwd)\n\ndef is_package_root(path"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tlog.debug(\"Stripping root '%s'\" % root)\n\t# check for a '.', so it doesn't count as the root\n\tpath = path.rstrip(os.sep)\n\tif path == \"/\":\n\t\traise LookupError(\"Invalid or missing root\")\n\tdirs.remove(path)\n\tif len(dirs) == 0:\n\t\treturn os.path.join(root, path)\n\telse:\n\t\tlog.debug(\"%s already exists, returning %s\", root, path)\n\t\treturn path\n\ndef"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\tpos = path.rfind(root)\n\t\tif pos < 0:\n\t\t\traise ValueError\n\t\tif pos >= len(path):\n\t\t\tpath = path[:-1]\n\t\tpos = path.rfind('/', 0, pos)\n\t\tpath = path[pos:] + path[pos+len(root):]\n\t\tif not root.startswith(path):\n\t\t\traise ValueError\n\texcept ValueError:\n\t\traise ValueError(\"Path %s not recognized as a root (%s)\" %\n\t\t\t\t(path, root))\n\tpath ="}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\treturn _remove_re_root(path, root)\n\n\nif __name__ == \"__main__\":\n\t__dynload_ = True\n\tif \"--dynload\" in sys.argv or \"--dynload=none\" in sys.argv:\n\t\t__dynload_ = False\n\n\tif __dynload_:\n\t\tif \"--dynload\" in sys.argv or \"--dynload=none\" in sys.argv:\n\t\t\t__dynload_ = False\n\n\tif not check_python2"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\tos.remove(path)\n\texcept FileNotFoundError:\n\t\traise\n\texcept PermissionError:\n\t\traise\n\n\tif root is '':\n# if path.strip == ''\n# if path[-1] == '/':\n# if root is '/':\n\t\tpass\n\telse:\n\t\tif root[0] == '/':\n\t\t\tif path[-1] == '/':\n\t\t\t\troot = ''\n\t\telse:\n\t\t\tif path[-1] == '/':\n\t\t\t\troot = '/'\n\n# if root == '.':\n"}]}
