{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\ttz = tz if tz else local_time().tzinfo\n\treturn Time(\n\t\tseconds = nanoseconds * (24 * 60 * 60 * 1000),\n\t\tmicroseconds = nanoseconds * (24 * 60 * 60 * 1000),\n\t\tnanoseconds = nanoseconds,\n\t\ttimezone = tz.name()\n\t)\n\ndef timely(\n\tdays=None,\n\thours=None,\n\tminutes=None,\n\tweeks=None,\n\tyears=None,\n\tmonths=None,\n\tmin"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is None:\n\t\ttz = pytz.timezone(\"UTC\")\n\treturn pytz.utc.tzlocal().localize(tz.utcfromtimestamp(nanoseconds)).replace(tzinfo=tz)\n\nclass Time(object):\n\t\"\"\"Time in `TimeZone`.\n\t:param local_time: \tlocal time in UTC timezone.\n\t:param utc_time: \tutc time in the current timezone.\n\t:param milliseconds: \tmicroseconds in given `Time Zone`.\n\t\"\"\"\n\tdef"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\ttz = tz or local_tz\n\tassert isinstance(tz, datetime.tzinfo)\n\treturn datetime.datetime.fromtimestamp(nanoseconds, tz).time()\n\n\ndef local_time_from_timestamp(nanoseconds, tz=None):\t\"\"\"\n\tConvert `Time` and `LocalTime` values to local time.  The timezone is\n\tinterpreted by `tz`.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: LocalTime\t\"\"\"\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tnanoseconds = int(nanoseconds)\n\treturn Time(time.time() + 0.0, tz=tz) + 1.0 / nanoseconds\n\n\ndef timenote(dt, tz=None):\n\t\"\"\"\n\tReturns a string representation of the time component of the provided date time object:\n\n:param dt:\n:param tz:\n:return:\n\"\"\"\n\tdt = dt.replace(tzinfo=UTC)\n\ttry:\n\t\treturn timenote(dt.year, dt.month, d"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif type(nanoseconds) is types.FunctionType: return nanoseconds(tz)\n\telse:\treturn float(nanoseconds)\t\n\ndef to_s(msec):\n\t\"\"\"Turns a `sec` value into `s`.\"\"\"\n\treturn secs_to_millisecs(msec)\n\ndef secs_to_hours(seconds):\n\t\"\"\"Convert `seconds` into hours from the standard 60 minutes section.\"\"\"\n\treturn seconds // (60*60)\n\ndef seconds_to_hours(seconds"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif nanoseconds is None or float(nanoseconds) <= 0:\n\t\traise TypeError(\"%s is not an integer\" % (nanoseconds,))\n\tt = Time.from_nanoseconds(math.trunc(float(nanoseconds)))\n\tif t.tz is not None:\n\t\tif nanoseconds is not None:\n\t\t\tif tz is not None:\n\t\t\t\tt += tz\n\t\t\telse:\n\t\t\t\tif t.tzidx > -1:\n\t\t\t\t\tt += tz.utc(t."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\ttz = tz or DT_UTC\n\treturn Time(datetime.datetime.utcfromtimestamp(time.time() * nanoseconds), tz=tz)\n\ndef local_time(year, month, day, hour=0, minute=0, second=0, tz=None):\n\t\"\"\"Compute time using current time.\n\n\t:param year:\n\t:param month:\n\t:param day:\n\t:param hour:\n\t:param minute:\n\t:param second:\n\t:return: Time\n\t\"\"\"\n\tdt"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is None:\n\t\treturn math.ceil(math.pow(nanoseconds, 1.0 / 3600.0))\n\treturn math.ceil(math.pow(nanoseconds, 1.0 / 3600.0 * tz.utcoffset(None)) / 3600)\n\n##########################\n\n# A `Time` subclass from the `time` module, which is\n# created in Python by means of `time.time()`, and\n# added to the dictionary `get_time()` by adding\n# \""}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tresult = None\n\tif tz!= None:\n\t\t# Create the timestamps (only when tz is not None).\n\t\t# TODO: Check here, if there is a possibility,\n\t\t# that the timezone should be `tz`-specific and thus `tz` must be set.\n\t\tnow_tz = TZ(tz)\n\t\tnow_unix = unix(now_tz, round=True, precision=6)\n\t\tresult_unix = int(now_unix)\n\t\tresult_dt = datetime.date(now_dt."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\tdef _timedelta_to_time(delta_time):\t\n\t\treturn (delta_time // nanoseconds) * nanoseconds # nanoseconds\n\t\t\t\t\t\n\tdef _time_to_timedelta(time):\n\t\t# assume datetime.time can have only 1 microsecond unit\n\t\treturn time.replace(microsecond=0) - datetime.time.now()\n\n\tdef _date_to_timedelta(date):\n\t\treturn datetime.timedelta(days=date.weekday())\n\t\t\n\t"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t# convert `timedelta` value to int type\n\treturn int(value) if isinstance(value, timedelta) else value\n\n@csw.on_message(\n\t\"/time {command,args}: {[date]} [{hour|min:hourmin}|{day|month:month}]= {[weekday|weekdayname]}\"\n\t\" (no timezone; default is UTC) \",\n\tlambda channel, user, message: (message.text or \"\", message.channel, message)\n)\nasync def time(client, message"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn value"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tvalue = value.total_seconds()\n\treturn value\n\ndef convert_bool(value):\n\t\"\"\"\n\tDehydrator for `bool` values.\n\n\t:param value:\n\t:type value: bool\n\t:return:\t\"\"\"\n\treturn 1 if value else 0\n\ndef check_filepath(func_name, *args, **kwargs):\n\t\"\"\"\n\tCheck for a valid file path.\n\n\tRaises error if no argument string is provided as an argument.\n\tRaises error if file path is not entered.\n\n\t:param method:\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn value.total_seconds()\n\nclass CustomPacker:\n\t\"\"\"\n\tPipeline component that can be easily serialized with the ``timedelta`` package.\n\t\"\"\"\n\tdef __init__(self):\n\t\tsuper().__init__()\n\t\tself.pack = 0\n\n\tdef process(self, data):\n\t\tpacked_timedelta = datetime.timedelta(seconds = int(data))\n\t\treturn struct.pack('d') + packed_timedelta.to_bytes(length = 1, byteorder = 'little' )\n\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif value is not None:\n\t\treturn value.seconds\n\treturn value\n\ndel timedelta, _date_types\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn value.days * MILLISECONDS_IN_DAY + value.seconds + value.microseconds / 1E6\n\n\ndef dehydrate_timestamp(value):\t\"\"\"\n\tDehydrator for `datetime` values.\n\n:param value:\n:type value: datetime\n:return:\t\"\"\"\n\treturn value.timestamp()\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif not isinstance(value, timedelta):\n\t\traise TypeError('timedelta not a timedelta.')\n\thours, _ = divmod(value.seconds, 3600)\n\tdays, seconds = divmod(hours, 24)\n\tif seconds!= 0 and value.microseconds!= 0:\n\t\traise ValueError('timedelta must be a timedelta object without microseconds.')\n\tif value.days >= 0:\n\t\treturn f\"{days:02d}d{value.days}h{seconds}\"\n\telse:\n\t\treturn f\"{"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tsparse_str = str(value)\n\t# TODO: parse this out better\n\n\t# Remove the non-spacial parts of the date that we don't want\n\tdelim = \"_:\"\n\tsparse_str = sparse_str.split(delim)[0]\n\tsparse_str = re.sub(r\"\\s+\",\"\",sparse_str)\n\n\treturn sparse_str"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn value\n\n@admin.register(models.Tag)\nclass Tag(models.Model):\n\t\"\"\"Stores tag.\"\"\"\n\n\t_name ='models.tag'\n\t_description = 'Tag'\n\t_order ='sequence'\n\tname = fields.Char(required=True, help='Name of the Tag', index=True)\n\tactive = fields.Boolean(help='Tag is active', default=True)\n\n\t_constraints = [\n\t\t(models.Q(active=False), 'No active tag, exclude by default'),\n\t\t("}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif value is not None:\n\t\thours = timedelta(hours=value.days)\n\telse:\n\t\thours = 0\t\n\tminutes = value.seconds%60\n\tif value.microseconds:\n\t\tseconds_str = uf.timedelta_to_str(value)\n\t\tminutes, seconds = seconds_str.split(\":\")\n\t\tminutes += 10*0.0000036076\n\treturn (hours, minutes, seconds)\n\ndef dehydrate_list(value):\t\"\"\"\n\tDehydrator for `list` values.\n\n"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value.delta <= (60 * 60 * 24):\n\t\tvalue = value * value.div / (60 * 60)\n\telse:\n\t\tvalue = value.days * 24 * 60 * 60 * 1000\n\treturn value\n\ndef dehydrate_regex(value):\t\"\"\"\n\tDehydrator for `regex` values.\n\t\n:param value:\t\n:type value: regex\n:return:\t\t\"\"\"\n\treturn ('|'.join(\n\t\t\tmap(\t\t\n\t\t\t\tlambda x: x[x.find(\"*\")-1:], \n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif isinstance(value, six.integer_types):\n\t\tnow = datetime.datetime.now\n\t\treturn now(hour=0, minute=0, second=0, microsecond=0).time().strftime(\"%H:%M:%S\")\n\telse:\n\t\treturn str(_time(value, tzlocal('Europe/Mariehamn')))\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tvalue.time, unit = value.strip().split(\".\")\n\treturn time_unit(value.day, value.month, value.year, value.hour, 0, 0, unit)\n\n##\n@dataclass(frozen=True)\nclass Charge:\n\t\"\"\"\n\tClass to represent a charged charge\n\t\"\"\"\n\tvalue = field(default=0.0)\n\n\tdef __post_init__(self, value=0.0):\n\t\tif value > 0.0:\n\t\t\tself.value = value\n\t\telse:\n\t\t\traise ValueError"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value.hour % 3 == 0:\n\t\treturn f'{value.minute} AM'\n\tif value.hour % 3 == 1:\n\t\treturn f'{value.hour} AM {str(value.minute)}'\n\tif value.hour % 3 == 2:\n\t\treturn f'{value.hour} PM'\n\telse:\n\t\treturn f'{str(value.day)}-{str(value.year)}-{value.month} AM'\t\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn value.isoformat().replace(\"Z\", \"\")\n\ndef dehydrate_date(value):\t\"\"\"\n\tDehydrator for `date` values.\n\n:param value:\n:type value: Date\n:return:\t\"\"\"\n\treturn value.date().isoformat().replace(\"Z\", \"\")\n\n@provider(Depends(is_valid_data_request), name='v0.1.0')\nclass APIv0:\n\t\"\"\"\n\t\tA class to represent a VIN, VINID, or VINName value.\n\t\tSee https"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif isinstance(value, datetime):\n\t\treturn {\n\t\t\t'__class__': 'n:Dynamoclass', '_typevar': 'y:time', '_defaultvalue': datetime_convertor(value)\n\t\t}\n\telif isinstance(value, int):\n\t\treturn {\n\t\t\t'__class__': 'n:Number', '_typevar': 'y:time', '_defaultvalue': value\n\t\t}\n\telse:\n\t\traise ValueError('Invalid value in `dehydrate_time`')\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tvalue = value.isoformat()\n\treturn value\n\ndef dehydrate_date(value):\t\"\"\"\n\tDehydrator for `date` values.\n\n:param value:\n:type value: datetime\n:return:\t\"\"\"\n\tvalue = value.isoformat('','')\n\treturn value\n\n##\n##\n##\n##\n##\n\n\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value <= Duration.Days:\n\t\tvalue = duration(value)\n\telif value < Duration.Years:\n\t\tvalue = durationyear(value)\n\telif value < Duration.Months:\n\t\tvalue = durationmonth(value)\n\telif value < Duration.Weeks:\n\t\tvalue = durationweek(value)\n\treturn value\n\n\ndef build_dict():\n\tdict = {}\n\tfor key in TIME_KEYS:\n\t\tdict[key] = {}\n\tfor key in TIME_KEYS:\n\t\tdict[key]['type'] ="}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn value.hour * 3600 + value.minute * 60 + value.second\n\n\ndef get_time_from_dict(dict_):\n\t\"\"\"\n\tGenerate a `time` from the dictionary format\n\n\t:param dict_:\n\t:type dict_: object\n\t:return\t:\t`Time`\n\t:raises:\n\t\t**TypeError** if not is of type int\n\n:example:\n\t>>> import hydra as hs\n\t>>> dict_ = {'hours': 10,'minutes': 15,'seconds': 3, 'date"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn 0, str(value.hour)\n"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn value\n\ndef dehydration_point(point_data):\t\"\"\"\n\tDehydrator for Point data. Overdraft data if point is used.\n\n:param point_data:\n:type point_data: Point\n:return:\n:rtype:\n:exception:\t\n:see:\t\n\"\"\"\n\nclass Preamble(object):\n\t\"\"\"\n\tPreamble used to generate the 'preamble' block for the application. \n\t:param include: Set to True to add a 'include_preamble' block. Can't be overwritten"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn value.point\n\n\nclass Label(Model):\n\tdef __repr__(self):\n\t\treturn repr(self.type)\n\ndef point2label(point):\n\tif point==Point():\n\t\treturn point.label\n\tif point.type==\"C\":\n\t\treturn \"{}{}{}\".format(POINT_LABEL_BASE,point.label,POINT_LABEL_SUFFIX)\n\tif point.type==\"S\":\n\t\treturn \"{}{}{}\".format(point.label,POINT_LABEL_SUFFIX"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tpoint_data = {'type': 'Point', 'data': [value.x, value.y]}\n\tjson_point = json.dumps(point_data)\n\treturn json_point\n\ndef dehydrate_vector(value):\t\"\"\"\n\tDehydrator for Vector data.\n\n:param value:\n:type value: Vector\n:return:\t\"\"\"\n\tvector_data = {'type': 'Feature',\n\t\t\t   'properties':\n\t\t\t   {'name': str(value.name),\n\t\t\t   'subd': value.subd.point"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn value.x,value.y,value.z\n\nclass Point(BaseModel):\n    \"\"\"Generic Point (without an index, and no labels).\n\tThis subclass assumes that the Point data always came from a Point data file,\n\tbut it should handle all kinds of files (e.g. CSV or DB)\"\"\"\n\tx   = FloatField(validate=Validate(lb=0,ub=1))\n\ty   = FloatField(validate=Validate(lb=0,ub=1))\n\tz   = FloatField(validate=Validate"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn \"0\"\n\n\n\nclass DehydratorNil (Transition):\n\t\"\"\"\n\tHelper class to allow passing NaN or None to hydrate a numpy array.\n\t\"\"\"\n\ttype = \"nil\"\n\tdef getValue(self):\n\t\tif self.value is None:\n\t\t\treturn str(\"nan\")\n\t\treturn self.value \n\tdef getType(self):\n\t\treturn \"nil\"\n\n\ndef dehydrate_polynomial (value):\n\t\"\"\"\n\tDehydrator for Polynomial data.\n\t\n\t:param"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\ttry:\n\t\treturn int(float(value.z))\n\texcept ValueError as e:\n\t\traise PreventUpdate\n\n# \treturn f\"[z]{value.z}\"\n\n\ndef load_data_from_csv(filename: str) -> list[Point]:\n\t\"\"\"\n\tLoad point data from a csv\n\t:param filename:\n\t:type filename: str\n\t:return: list of Point data\n\t:rtype: list[Point]\n\t\"\"\"\n\tdata = []\n\n\twith open(filename) as csv_file:\n\t\t"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tres = value.get_x() + (value.get_y() * -1)\n\treturn res  # round(res,6)\n\n\ndef hydrate_point(point):  # add points from another Point in any form (or any other type)\n\t\"\"\"\n\tGet the current x,y (and x,y) coordinates from a Point and return it as a new Point object.\n\n\t:param point:\n\t:type point: Point\n\t:return:\t\"\"\"\n\tcoord = point.x, point.y\n\treturn Point(coord)  #"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn str('{0:,.0f}'.format(value.x),\",\") + str('{0:,.0f}'.format(value.y),\".\")"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tresult = \"%0.2f\" % value[1]\n\tif value[2] == None:\n\t\treturn result\n\telse:\n\t\treturn result\n\nclass Column(object):\n\t\"\"\" A Column in a Dataframe.\n\t\n\t:param name:\n\t:type name: str or None\n\t:param type:\n\t:type type: ColumnDataType\n\t:param key:\n\t:type key: ColumnDataType or str\n\t:param nullable:\n\t:type nullable: str\n\t:param sort:\n\t:type sort:"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tpt_point_string = \"\"\n\tfor pt in value:\n\t\tpt_point_string += pt[0].upper() + pt[1:] + \",\"\n\n\treturn pt_point_string[:-1]\n\n\ndef check_int(value):\t\"\"\"\n\tValidator for int datatypes.\n\tRaises an exception otherwise.\n\t:param value:\n\t:type value: int\n\t:return:\tNone\n\t\"\"\"\n\tif not isinstance(value, int):\n\t\traise InvalidInput(\"{} is not an int.\".format(value"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn map(_get_key, self.__all_keys)\t\t\t\n\ndef get_all_keys(self):\n\t\"\"\"\n\tReturn a flat string of the keys.\n\n:return: flattened keys string: e.g., ``'a,b,c'``\n\t\"\"\"\n\treturn ','.join(map(_get_key, self.__all_keys))\n\n# NOTE: this is a low-level function.\ndef keys_with_type(self, t):\n\t\"\"\"\n\tReturn dict of all keys with the given type.\n\n\t:"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn list(self.keys())\n\n# def get_record(name):\t\n# \treturn self.items().get(name)\n\n# def _isunique(dct, keys):\n# \tfor key in keys:\n# \t\tif key in dct:\n# \t\t\treturn False\n# \t\tdct[key] = 1\n\n# def insert(self, name, dict=None):\n# \tif dict is None: dict = {}\n# \tdict[name] = 1\n# \tfor key in dict: self[key"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn _pywin32.Popen_EnumKey(self)\n\n\t\n#------------------------------------------------------------------------------------------------------------------------------------------------\ndef enum(name, doc):\n\t\"\"\"doc\"\"\"\n\treturn win32com.client.CoClass(name, docs=[doc])\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------------\n\nclass __COM_PROCSESSES(__Win32Types):\n\t\"\"\"__COM_PROCSESSES: a collection of Processes in the Processes Registry.\"\"\"\n\n\t_cases_init = [\n\t\t2,\t\t# Process name and PID. This item is case-insensitive."}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tlst = []\n\tfor r in self.getList():\n\t\tif type(r)!== int  or type(r)!=str or r == None:\n\t\t\traise TypeError #raise exception TypeError\n\t\telse:\n\t\t\tlst.append(r)\n\treturn lst\t\t\n\n#   - This can be used as:\n#       self.getList().__getitem__(num)  # access with list()\n\ndef __getitem__(self, num):\t# get the key at a given index\n\t\"\"\"\n\tReturn the value of the"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tresult = sorted(self.keys())\n\treturn result\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\nclass TreeNode:\n\t\"\"\" A TreeNode represents the basic tree structure including the height and level info of the tree (depth(index, depth+1) and size of the whole tree).\n\t\n\tNote: TreeNode has the following properties:\n\t\t- Its parent/child are TreeNode objects\n\t\t- Node is initially not a leaf node\n\t\t- A reference to the"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tself._clear()\n\twhile True:\n\t\tk = self._getNext()\n\t\tif self._isOk(k):\n\t\t\tself._keys.add(k)\n\t\telse:\n\t\t\treturn list()\n\ndef keys(self):\t\"\"\"\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tself._clear()\n\twhile True:\n\t\tk = self._getNext()\n\t\tif self._isOk(k):\n\t\t\tself._keys.add(k)\n\t\telse:\n\t\t\treturn list()\n\ndef keys"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn cPickle.dumps(self)\n\n    def __getitem__(self, key):\t\"\"\"\n\n:param key: key to get\n:return: instance of a specific key\n\n:Note: in Python 3.3 this is a mutable mapping. Mutable mappings are first\nwrtue to prevent accidental mutation\n\n\"\"\"\n    return self._recv_buffer[key]\n\n    def pop(self, key):\t\"\"\"POP data from the record (without any return value).\n\tUseful if you don't want to track the record, just want to"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tif not self:\n\t\treturn []\n\tr = {}\n\tfor i, record in enumerate(self):\n\t\tr[record[0]] = i\n\treturn r.keys()\n\t\ndef __loaddata__(filename):\n\t\"\"\"\n\tLoad a file from the given filename.\n\t\n\t:param filename: the filename of a file\n\t:type filename: str\n\t:return: a list of lines of the file\n\t:rtype: list\n\t\n\t:raise TypeError: If data is unsupported\n\t:raise OSError: If"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.keys()\nclass Batch(object):\n\tdef __init__(self,\n\t\tdoc = '',\n\t\tfiles = [],\n\t\tkeys = [],\n\t\tname = '',\n\t\ttype = 'Batch'):\n\t\t## Initialize a batch of records\n\t\tself.doc = doc\n\t\tself.files = files\n\t\tself.keys = keys\n\t\tself.name = name\n\t\tself.type = type\n\t\t\n\tdef set_doc(self, doc = '', files = [], keys = [], name = '', type = '"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tif(self.has_key):\n\t\treturn list(self.__dict__.keys())\n\telse:\n\t\treturn []\n\n\treturn []\n"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = cls._supported_versions[cls.VERSION]\n\tprotocol = cls.protocol_version[0]\n\ttry:\tprotocol_tuple = protocol_version\n\texcept TypeError:\n\t\tprotocol_tuple = (protocol_version,)\n\tfor protocol in protocol_tuple:\n\t\ttry:\n\t\t\tprotocol_class = PROTOCOL_HANDLERS[protocol]\n\t\texcept KeyError:\n\t\t\traise TypeError(\"Not supported protocol version %s\" %"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif isinstance(protocol_version, tuple):\n\t\tprotocol_version, all_handler = protocol_version\n\t\tif all_handler is None:\n\t\t\treturn protocol_version\n\t\tall_handler_cls = protocol_version\n\t\tif protocol_version == (3, 5):\n\t\t\tall_handler_cls = Bolt_protocol.KbBitBuffer\n\telse:\n\t\tall_handler_cls = Bolt_protocol.KbBitBuffer\n\treturn {\n\t\t4: all_handler_cls(),\n\t\t5: all_handler_cl"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocols = cls._protocol_handlers.get(protocol_version)\n\tif (protocols is not None) and (protocol_version is not None):\n\t\treturn protocols[protocol_version]\n\telse:\n\t\traise TypeError(\"protocol_version must be a tuple of ints or None\")\n\ndef get_protocol_handler(protocol_version=None) :\n\t\"\"\"\n\tIf a protocol version is provided, then we retrieve that handler\n\tfrom the protocol dictionary, then return that. Otherwise,\n\twe will look"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif type(protocol_version) is not tuple:\n\t\traise typeerror(\"protocol_version must be a tuple of ints\")\n\tprotocol_versions = map(checkversion, range(3))\n\treturn {protocol_version[i]: type(protocol_handlers) for i, protocol_version in enumerate(protocol_versions)}\n\n#------------------------------------------------------------------------------#\n#                                                                             #\n#                                                                             #\n#                                                                             #\n#   Protocol-specific definitions                                                 #\n#                                                                        "}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tversion = (None,)\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, (tuple, list)):\n\t\t\traise TypeError('Protocol version must be a tuple or list')\n\t\tif not isinstance(protocol_version[0], (int, long)):\n\t\t\traise TypeError('Protocol version must be an integer or 1-octet integer')\n\t\tversion = protocol_version\n\n\thandlers = {}\n\tfor l in cls.protocols:\n\t\tif not l.available:\n\t\t\tcontinue\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tallowed = ()\n\tif protocol_version is not None:\n\t\tif(type(protocol_version[0]) == int and protocol_version[0] <= 3):\n\t\t\tallowed = {protocol_version}\n\t\telse:\n\t\t\traise TypeError('invalid version number: {0}'.format(protocol_version))\n\t# create handler dict\n\thandler_dict = {}\n\tfor k in cls.protocol_handlers.keys():\n\t\tif (k[0] & protocol_version)!= 0:\n\t\t\thandler_dict[k[1"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocol_version_tup = None\n\tif protocol_version is None:\n\t\tprotocol_version_tup = 0\n\t\tfor (major, minor) in enumerate(CLI_VERSIONS):\n\t\t\tif major == 3 or major == 4:\n\t\t\t\tprotocol_version_tup = (major, minor)\n\t\t\t\tbreak\n\telif type(protocol_version) is not tuple:\n\t\traise TypeError(\"protocol version must be a tuple of two integers\")\n\telse:\n\t\tif len(protocol_version)!= 2:\n\t\t\traise"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\timport types\n\tprotocol_type = type\n\n\tif protocol_version is None or isinstance(protocol_version, tuple) \\\n\t\tand 0 <= protocol_version.count(None) < 2:\n\t\treturn {'None': protocols.__get_handler_class__()}\n\telif type(protocol_version) == types.TupleType:\n\t\tif len(protocol_version) > 2:\n\t\t\traise TypeError('protocol_version tuple must contain only two'\\\n\t\t\t\t'elements not more than three')\n\t\telif not all"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\thandler_handles = cls.protocol_handles\n\tif protocol_version is not None:\n\t\ttry:\n\t\t\treturn dict(handler_handles[protocol_version])\n\t\texcept KeyError:\n\t\t\traise TypeError(\n\t\t\t\t\"Protocol handler for version (%d) unavailable\" % protocol_version\n\t\t\t\t)\n\telse:\n\t\treturn handler_handles\n\n\nclass Protocol(object):\n\tprotocol_handles = {\n\t\t# protocol name\n\t\t'1.0': getProtocolHandler('one_0'),\n\t\t'1.1"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocol_versions = OrderedDict()\n\tfor key in cls.protocol_names:\n\t\tif isinstance(protocol_version, tuple):\n\t\t\tif len(protocol_version)!= 2:\n\t\t\t\traise TypeError(\n\t\t\t\t\t'protocol_version argument of protocol.protocol_manager.{} must consist of a tuple, not \"{}\"'.format(\n\t\t\t\t\t\tcls.__name__, repr(protocol_version)))\n\t\t\tprotocol_versions[key] = get_protocol_class(\n\t\t\t\tprotocol_version)\n\t\tel"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\treturn Measurement.tx_meta(timeout)\n\n\n@Measurement.tx_meta(timeout=0)\ndef run_transaction_sync(tx, start=0, max_queue_size=0, timeout=None):\n\t\"\"\"\n\tRun transaction sync routine in the transactional thread.\n\tThis function takes a session and an :class:`Transaction` object,\n\tand performs some operations in a synchronous way and return the output.\n\n\t:param start:\n\t:type start: int\n\t:param max_queue_size:\n\t:type max_"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\timport math\n\timport sys\n\tfrom neo4j.exceptions import TransactionTimeout\n\n\tdef _make_timeout(val):\n\t\t# Returns current timeout limit\n\t\tif timeout is None:\n\t\t\treturn sys.maxint\n\t\telse:\n\t\t\t# If seconds are not in seconds. We might be on a server-side machine\n\t\t\tif type(val) in (float, int):\n\t\t\t\treturn val\n\t\t\telif isinstance(val, (int, float)):\n\t\t\t\treturn val\n\t\t\traise ValueError(\"timeout must be a float in seconds\")\n\n\tdef decorator("}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef inner(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\ttx = txmanager.transaction()\n\t\t\t\ttx.config(metadata=metadata, timeout=timeout)\n\t\t\t\tif'metadata' in kwargs:\n\t\t\t\t\ttx.config(metadata=metadata, timeout=timeout)\n\t\t\t\ttx.config(timeout=timeout)\n\t\t\t\ttx.run(func, *args, **kwargs)\n\t\t\t\treturn tx.is_active(), tx.run(\"MATCH (a:Person) RETURN"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef wrap(fn):\n\t\t@wraps(fn)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkwargs[\"metadata\"] = metadata\n\t\t\tkwargs[\"timeout\"] = timeout\n\t\t\treturn fn(*args, **kwargs)\n\t\treturn wrapper\n\treturn wrap\n\n\ndef has_tx_metadata(tx=None):\n\t\"\"\"This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\n    For example, a timeout may be applied::\n\n    from neo4j import has_tx_metadata\n    @has_tx_"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef set_timeout(tx, timeout):\n\t\ttx.options.add(\"timeout\", timeout)\n\t\ttx._set_timeouts(timeout)\n\treturn property(get_timeout, set_timeout)\n\n\nclass Transaction(object):\n\t@staticmethod\n\tdef create(tx, metadata=None, timeout=None):\n\t\t\"\"\"\n        :param tx:\n            Execution session.\n        :type tx: pyneo4j.graph.tx.exec.Transaction\n\t\t:param metadata:\n\t\t:type metadata: dict\n\t\t:param timeout:\n\t\t:"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\treturn TimedTransformer(func=self, metadata=metadata, timeout=timeout)\n\n@method(name='delete', docstring=r\"\"\"\nThe :meth:`delete` method allows to remove one or more Neo4j objects in the ``database`` (i.e., table or view).\nIf ``'table'``/``.view``/``.'schema'`` were specified, it must be either a :term:`table` or :term:`view` on a :term:`dataset` which must have been created with\n:meth:`"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\treturn _unit_of_work(self, timeout=timeout, metadata=metadata)\n\n_transaction_meta ='meta'"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef decorated(fn):\n\t\t@wraps(fn)\t\t\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif kwargs.pop('_expire', None) and timeout:\n\t\t\t\ttx = self.begin(timeout=timeout)\n\t\t\t\ttry:\n\t\t\t\t\tif metadata:\n\t\t\t\t\t\ttx.options(tx.write-merge(metadata))\n\t\t\t\t\tif metadata and timeout:\n\t\t\t\t\t\tself.end()\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tif dbms.transaction.log:\n\t\t\t\t\t\tlogging.info('Timeout for transaction.  %s',"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t# note: we can't use property setter on `decorator`, so the\n\t# decorator `set_timeout` need to be called first.\n\t# http://docs.python.org/reference/datamodel.html#object.__setattr__\n\t# note: there seems to be some issue with calling a class method\n\t# on a property, so we call getattr here instead of just defining\n\t# and modifying the set attribute\n\n\tdef set_timeout(transaction, timeout):\n\t\ttransaction.params['timeout'] = timeout\n\n\ttx_"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef _retry(tx, *args, **kwargs):\n\t\tt = time.time()\n\t\twhile True:\n\t\t\ttrans = dbms.transaction.Transaction(tx, *args, **kwargs)\n\t\t\tresults = trans.run()\n\n\t\t\tif 'count' in results: return results['count']\n\t\t\telif'started' in results: return results['started']\n\n\t\t\telif not isinstance(results, dict):\n\t\t\t\t# return default transaction timeout\n\t\t\t\tif timeout is None:\n\t\t\t\t\treturn None\n\t\t\t\telse:\n\t\t\t\t\ttime.sleep(timeout"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\twith open(os.path.join(os.path.dirname(__file__), \".txt\"), \"r\", encoding = \"utf-8\") as f:\n\t\tlist1 = f.readlines()\n\t\tfor x in range(len(list1)):\n\t\t\tif list1[x] == key:\n\t\t\t\treturn x\n\t\telse:\n\t\t\treturn -1\n\tf.close()\n\t\n\t\ndef delete(self, key=0):\t\"\"\"\n\tRemove an item from the dictionary. If key is specified, remove the item\n\tonly if it is"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self._data.hash(key)\n\n\nif __name__ == '__main__':\n\ta = Link(1)\n\n\tprint \"The linked list in the test: {}\".format(a)\n\n\t#a.insert_at_index(2, a)\n\t#a.insert_at_index(3, a)\n\t\n\t#print \"The linked list in the test: {}\".format(a)\n\n\t\n\tb = Link(1, 2)\n\n\tprint \"The linked list in the test: {}\".format(b)\n\n\tb"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn None\n\n\t\n\t\n\t\n\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\ttry:\n\t\treturn self.__index[key]\n\texcept KeyError:\n\t\traise ValueError('Key not found')\t\n\n\t\n# TODO: Add code here.\t\t\n\n\n# TODO: Add code here.\t\t\n\n\n\n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tif key not in self.map_:\t\treturn -1\t\n\tindex = self.map_[key]\t\treturn index\n\treturn 0\t\t\t\n\t\t\n\t\t\n\n\n\n\n\n\t\t\n\t\t\n\n\n\n   \t\n\n\n\n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.map[key]\t\n\ndef search(self, key):\t\n\t\"\"\"\n\tReturns the index of the item with matching key.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\t\n\t\"\"\"\n\t\n\tsearch_key = key+1\n\tfor index, element in enumerate(self.map): # search by key (self.map[0]), so we dont need to get the map()\n\t\tif element == search_key:\n\t\t\treturn index\t\t\t\n\traise ValueError('The key {}"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn hash(key) % len(self.items())\ndef listIndex(self, key): \n\t\"\"\"\n\tReturn the index of the given item.\n\n\t:param key: a key item\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif self.containsItem(key):\n\t\t# if key to index in dictionary is not present in the file\n\t\t# then return null\n\t\tif self.getItem(key) is None:\n\t\t\treturn -1\n\t\telse:\n\t\t\t# then return item index corresponding to key.\n\t\t\treturn"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\ttry:\n\t\treturn self.__search(_KEY, key)[0]\n\texcept Exception as e:\n\t\tprint(e)\n\t\traise Exception('Key does not exist')\n\ndef remove(self, key):\n\t\"\"\"\n\tRemoves the item with the given key from the container\n\t:param key: a key\n\t:return: item, removed from list\n\t:rtype: item\n\t\"\"\"\n\ttry:\n\t\treturn self.__search(_KEY, key)[2]\n\texcept Exception as e:\n\t\tprint(e)\n\t\traise Exception('"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self._hash[key]\n\n\treturn self._hash[key]\n\n\treturn self._hash[key]\n\n\treturn self._hash[key + 1]\n\n\n\n\t\n\t\nobj = HashDictionary();\n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t# your code here\n\treturn self.hmap.get(key, None)\n\ndef items(self):\n\t\"\"\"\n\tReturn all items in the map.\n\n\t:param: this function creates a dictionary mapping keys to indexes\n\n\t:return: items\n\t:rtype: dict\t\"\"\"\n\treturn self.hmap.items()\n\ndef clear(self):\n\t\"\"\"\n\tRemove all items from the map.\n\n\t:param: this function removes all items from the map\n\n\t:return: this function removes all items in the map\n\t:rtype: void"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn self._values()\n\n\tdef keys(self, *keys):\t\"\"\"\n\tReturn the keys of the records, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all keys will be included\n:return: list of keys\n:rtype: list\t\"\"\"\n\treturn self._keys()\n\n\tdef unique(self, keys):\t\"\"\"\n\tReturn a list of the indices of items that contain at least \none of the entries in keys\n\n:param keys: the"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tr = list(self.browse(self._ids, context=self.context).write({}, {}, ['value']+keys))\n\tif keys:\n\t\tres = []\n\t\tfor record in r:\n\t\t\tif not record[2] in ['value']:\n\t\t\t\tcontinue\n\t\t\tfor i in range(len(keys)-1):\n\t\t\t\trec = records.new(record[self].browse(self._ids).with_context(rec_create_context))\n\t\t\t\trec.values_list('%s[%d]' % (record[self],i), many"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tvalues = self._recorder.value(self._id, *list(keys))\n\n\tresult = list()\n\ttry:\n\t\tfor key, value in zip(keys, values):\n\t\t\tif key in ('id', 'created'):\n\t\t\t\tresult.append(float(value))\n\t\t\telif key in ('updated'):\n\t\t\t\tresult.append(datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%fZ', '%Y-%m-%dT%H:%M:%"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn self.values().sort(sort_by_index=True)\n\telse:\n\t\treturn values(self, *keys)\n\nclass DBRecord(object):\n\t\"\"\"Represents a record of an item in the database, including\n\tinfo like name and description, date, place,...\n\t(although this object may not be an actual Item instance)\"\"\"\n\tdef __init__(self, name, date_changed, description):\n\t\t# TODO: use a more sensible class name\n\t\tself.__name = name\n"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn list(self.as_list(*keys))\n\nclass RecordIterator(object):\t\"\"\"\t\"\"\"\t\"\"\"\t\"\"\"\t\n\tdef __init__(self, records, **params):\n\t\t\"\"\"\t\"\"\"\t\n\t\t# parse `params`\n\t\tfor k in params.keys():\n\t\t\tif not params[k].get().startswith('|'):\n\t\t\t\tparams[k] = params[k].replace('|', '\\\\|')\n\t\t# parse records\n\t\trecords = [rec.values(**params) for rec in records]\n\t\tself.rec"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t_keys = []\n\tif len(keys) == 0:\n\t\treturn self._values.copy()\n\tfor (k, v) in self._items_iter(keys):\n\t\t_keys.append([k, v])\n\n\treturn [v for (_, v) in _keys]\n"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\n\t# NOTE: I.\n\tassert keys is None or all([isinstance(x, int) for x in keys]), \"Keys are must be int\"\n\t#assert all([isinstance(x, int) for x in keys]), \"Must be non-negative (integer).\"\n\t\n\treturn [self._records[l][x] for l,x in enumerate(keys)]\n\t\t\n\t\n\t\n\t\nclass BaseCsv(object):\n\t\"\"\"\n\t:type :class:`csv.writer`\n\t:param csv_table: c"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif keys:\n\t\treturn self.__records.get(keys[0]).values(*keys)\n\treturn self.__records.values()\n\n@total_ordering\ndef setValues(self, values, *keys):\n\t'''Make the specified instance of self into a new instance of this instance\n\twith all of the values specified.  All keys and indices used must be in\n\tthe list of keys used in the constructor.  Returns an object with only\n\ta subset of current data that has been subsequently set.  Returns the\n\tcorresponding new instance of"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tvals = list(self.select(*keys))\n\tif len(keys) < 1:\n\t\treturn vals\n\t\n\t# filter on the correct index of the value to include\n\tif isinstance(keys, (tuple, list)):\n\t\tfilter = reduce(lambda x, y : x | y, keys)\n\telse:\n\t\tfilter = keys\n\t\t\t\t\t\n\t# filter on values in the required filter\n\tvals = filter(lambda v: filter(lambda d : not isString(v) and isNumber(d), vals))\n\treturn vals if"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn [self._values[key] for key in keys]\n"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn self.__get_dictionary(*keys, self.items.get(keys[0], ()))\n\ndef record(kind, index, *args, **kw):\n\t\"\"\"\n\tReturn a record of the given type (fields, value)\n\t\"\"\"\n\trecord_type = RECORD_TYPES[kind]\n\tkw['kind'] = record_type(*args, **kw)\n\treturn Record(kind, *args, **kw) \n\t\nclass _Record(object):\n\t\"\"\"\n\tbase record constructor\n\t\"\"\"\n\tdef __init"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\ttry:\n\t\td = self._data.items()\n\texcept:\n\t\td = self._data.items()\n\t\td.extend((k, None) for k in keys)\n\telse:\n\t\tif len(keys)!= len(d):\n\t\t\traise IndexError\n\t\td.extend((k, v) for k,v in d if k not in keys)\n\treturn dict(zip(self._fields, d))\n\ndef sort(self, sort, ascending=False, field=None):\t\"\"\"\n\tSort this record and return the sorted :"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t# default case: just return the keys\n\tif len(keys) == 1:\n\t\tassert isinstance(keys[0], int), str('%s is not an index' % str(keys[0]))\n\t\tt = {}\n\t\n\t\tkv_list = []\n\t\tfor index, item in enumerate(self):\n\t\t\tif isinstance(index, int):\n\t\t\t\tif index >= len(keys):\n\t\t\t\t\traise IndexError('%d out of bounds (%d)' % (index, len(keys)))\n\t\t\t\telse:\n\t\t\t\t\tt[self.keys[index"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t# check keys\n\tif keys:\n\t\tfor key in keys:\n\t\t\tif key in self.__dict__.keys():\n\t\t\t\traise IndexError(\"Record indexed by '%s' already exists\" % key)\n\treturn {key: self[key] for key in self if key not in self.__dict__.keys()}\n\ndef data(self, *keys):\n\t\"\"\"\n\tReturn the keys and values of this record as a dictionary, including\n\tonly certain values by index or key. Keys provided in the items that\n\tare not in the record will be inserted"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\ttry:\n\t\trecord = self[keys[0]]\n\texcept KeyError, e:\n\t\traise IndexError(\"no keys in %s: %s\" % (self.filename, repr(e)))\n\tif len(keys) == 1:\n\t\tnewarray = {}\n\t\tfor field in self.fields:\n\t\t\tnewarray[field] = self[field][keys[0]]\n\t\treturn newarray\n\t# handle lists\n\tif not is_sequence(keys[0]):\n\t\tnewarray = {}\n\t\tfor field in self.fields:\n\t\t\tnewarray"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn {field_name: self.data(index) for field_name, index in range(self.n)\n\t\t\tif index in keys}\n\n\t#####\n\n\tdef data(self, index=None):\n\t\t\"\"\"\n\tRetrieve the fields at given indices of the record data.  If\n\tindex is not provided, data for all records will be returned.\n\t\n\tNote that all values have to be retrieved in order to use the\n\tdata of the record using ``[field_name]`` access. Otherwise the\n\tunneccessary information will get lost.\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn {field1: self[field1].extract(index) for index  , field1 in enumerate(self)} if isinstance(keys, type(None)) else {index: value\n                                                                                                               for index, value in self.items()\n                                                                                                               if index[0] <= keys\n                                                                                                          if index[0] >= 0}\n\t\n\t\ndef field(self, value, index=None):\n\t\"\"\"Create the field object indicated by the value.\n\tThe field is indexed by the field named"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tvalue_dict = {}\n\t\t\n\tfor key in keys:\n\t\ttry:\n\t\t\tvalue_dict[key] = self[key]\n\t\texcept KeyError:\n\t\t\tvalue_dict[key] = None\n\t\t\t\n\treturn value_dict\n\t\n\t\n\t\nclass Model(object):\n\t\"\"\"Abstract base class for database objects\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\tpass\n\t\n\t\n\tdef save(self):\n\t\t\"\"\"\n\t\tPersist this record's:\n\t\t\n\t\t* :field:`id` to new"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn self._getValuesAsDict('data', keys)\n\ndef field_by(self, field):\t\"\"\"\n\tReturn the field (``type`` + field name) that maps this record to.\n\n:param field: field name of the record to return\n:return: field name of the record mapped to\n:raises: :exc:`NotFound` if no record exists with the given\n           field name\n:raises: :exc:`InvalidType` if record with field name field and \n           value type mismatch\n:raises: :exc:`KeyError"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn dict.fromkeys(self.keys())\n\telif len(keys)==1:\n\t\tdata = dict.fromkeys(keys, self.getlist(keys[0]))\n\t\tfor i in range(len(data)):\n\t\t\tif i not in data:\n\t\t\t\tdata[i] = None\n\t\treturn data\n\tif len(keys)==0:\n\t\treturn dict(self.items())\n\telse:\n\t\tif len(keys) > len(self):\n\t\t\traise IndexError(\"maximum index out"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn chr(self.__data[len(self.__data)-1])\n\nclass u32(object):\t\"\"\"\n\tThis class implements an univocal\n\t32-bit unsigned integer with a\n\tvariable size.\n\t\"\"\"\n\tdef __init__(self, value):\n\t\t\"\"\"\n\tCreate the 32-bit unsigned\n\tinteger using an unsigned\n\tlong value.\n\t\"\"\"\n\t\tself.__value = value\n\n\tdef unpack(self):\n\t\tself.__value = struct.unpack('<L', str(self.__data"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn self.__unpack_dword_32bit(self._data[self._nBits-2:self._nBits-1])\n\n\n\nclass I2cMessageType(IntEnum):\n\t\"\"\"\n\tType of message.\n\t\"\"\"\n\tPROGRAM_RESULT_ACK     = 0x01\n\tPROGRAM_RESULT_NOACK  = 0x02\n\n\nclass I2cRequest(CodedRequest):\n\tdef __init__(self, command:CodedRequest, data:bytes, is_valid=True):\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn self.pop(2)\n\t\n\t\t\t\t\ndef pop_u32(self):\t\"\"\"\n\tRemove the four byte of data, returning them as a big-endian 32-bit\nunsigned integer.\t\"\"\"\n\treturn self.pop(4)\n\t\n\t\t\t\t\ndef pop_u64(self):\t\"\"\"\n\tRemove the eight byte of data, returning them as a big-endian 64-bit\nunsigned integer.\t\"\"\"\n\treturn self.pop(8)\n\t\n\t\t\t\t\t\t\ndef pop_i16(self):\t\"\"\"\n\tRemove the two bytes"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn (self.__pop_u32() << 16) + self.__pop_u32()\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack('!H',self.data[-2:])[0]\n\n  def pop_u32(self):\t\"\"\"\n\tRemove the fourth bytes, 8-bit unsigned integer, and return them as a\n\tbig-endian 32-bit unsigned integer.\t\"\"\"\n\treturn  struct.unpack('!I',self.data[-4:])[0]\n\nif __name__ == '__main__':\n\ts = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\taddr = ('"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn self.__data[-2:]\n\n\treturn None\n\nclass BH32:\n\n\t__hashvalue = 0\n\n\tdef __init__(self, key: int, iv: bytes):\n\t\tself.__iv = iv\n\t\tself.__key = key\n\n\t\ttry:\n\t\t\tBH32.__hashvalue = BH32.getHash(key)\n\t\texcept:\n\t\t\treturn 0\n\n\t@staticmethod\n\tdef getHash(key: int) -> int:\n\t\treturn int.from_bytes(zlib.adler32(key.to"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn self.data_buffer[-2:]\n\t\t\n\t\ndef read_v4(self, address, length):\n\t\"\"\"\n\tReturn a sequence of 8-bit V4 bytes starting at address, and the sequence\n\tlength read from a data file.\t\"\"\"\n\tu16 = u32(self.data_buffer[2:6])\n\tself.data_file.seek(self.address_base+address)\n\tself.data_file.seek(u16)\n\treturn u16, self.data_file.read(length)\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tindex = super().pop()\n\treturn self._read_u16(index)\n\t# if index := self.find(b'\\040\\015\\n'), index = super().pop()\n\t# return self._read_u16(index)\n\n\t# return super().pop() + 12\n\n\t# # return (super().push(b'\\040\\015\\n') + super().push(b'\\0\\015'), 3)\n\n\t# # return (super().push(b'\\0'))\n\n\t# result = bytearray(6)\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack('!H', self.buffer[-2:])[0]\n\t\n\t\n# \t@staticmethod\n#   def parse_byte_l16(byte):\n#     \"\"\" Parse the 16-bit unsigned integer from a byte. \"\"\"\n#     return (struct.unpack('!H', struct.pack('!h', byte))[0]) \n\n\t\n\t\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\ta = self.get_data()\n\tself.set_data([a[:2] + a[-2:]])\n\treturn self.get_data()\n\n  def put_str(self, s):\n\t\"\"\"\n\tUse a string as an input.\t\"\"\"\n\ta = [ord(c) for c in s]\n\tself.set_data(a)\n\ndef run():\n  filename = sys.argv[1]\n  a = AES(filename)\n  print \"Contents of file:\", filename\n  while True:\n    "}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t# If hydration hooks are defined and hydration hooks\n\t# have been called first (with default dehydration hooks)\n\t# then hydration hooks are called.\n\tif dehydration_hooks:\n\t\thydration_hooks.update(hydration_hooks)\n\telse:\n\t\thydration_hooks = {k: hydration_hooks.get(k, hydration_hooks.get('_default_', None)) for k in dehydration_hooks.keys()}\n\tdehydration_hooks = dehydration_hooks.copy()\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n\tif n in (None, '-1'):\n\t\tn = self.__count\n\t\t\n\tif qid in (None, -1):\n\t\td = {}\n\t\ttry:\n\t\t\tqid = self.query_map[n]\n\t\texcept KeyError:\n\t\t\tqid = -1\n\telif n not in self.query_map:\n\t\tif n == 3: qid = -1\n\t\telse: raise KeyError(\"query %d does not exist\" % n)\n\telse:\n\t\tqid = self.query_map["}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t)\n\t\n\tif type(n) is not int:\n\t\traise TypeError(\"n must be an int; provided was: {}\".format(type(n)))\n\t\n\tfor f in self.all_funcs:\n\t\tif type(f) == str:\n\t\t\tif f.startswith('hydration') or f.startswith('hydration_'):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif f.startswith('dispatch'):\n\t\t\t\tcontinue\n\t\t\n\t\tf.extend([n], hydration_hooks)\n\t\tf.ext"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tdef setTypeHooks(self):\n\t\tH = self.hydration\n\t\td = self.dehydration\n\t\tif d:\n\t\t\thandler = getattr(d, 'hydration', None)\n\t\t\tif handler:\n\t\t\t\tif not hasattr(type(H), '__call__'):\t\t\t\t\t\n\t\t\t\t\tself.response.add_hydration_handler(H(*H.args, *H.kwargs))\n\t\t\tif d:\n\t\t\t\thandler = getattr(d, 'dehydration', None)\n\t\t\t\tif handler:\n\t\t\t\t\tif not hasattr(type"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t)\n\t_, msgid = udp.sendRequest(udpObjType.DISCARD, [discontentId, hydration])\n\t_, msg = udp.getData(msgid)\n\tdiscontent = udp.recvData(msg)\n\tif len(discontent)!= 0:\n\t\tudpErr, excName, excDesc = udp.findExceptionName(msgID=msgid)\n\t\treturn _utils._response(msgid, udpErr, \"Error: Disconnection to DB\")\n\tdiscontents = dict"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\timport types\n\tfrom Dehydrant.PackStream.message import PackStreamMessage, EOL\n\n\tif n < -1:\n\t\timport sys\n\t\tn = len(\"\".join(eol for eol in sys.stdin))\n\telif n == -1:\n\t\tn = 0\n\tif qid < 0:\n\t\tqid = - 1\n\telif qid == -1:\n\t\tqid = 'last'\n\telif qid!= - 1 and qid > qid:\n\t\t# if an EOL marker is present\n\t\tq"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tqid = self.handle_new_message\n\tself.dehydration_hooks = dehydration_hooks  # default: no-op hydration\n\tself.dehydration_handlers = hydration_hooks.setdefault(self.type, []).add(self.dehydration_callback) # type: DehydrationHandler\n\tself.dehydration_handlers.insert(0, self.data_handler)\n\tself.data_queue = self.data_queue.appendleft(Record(n=n, qid=qid, type"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n\t\n\tDEHTRATE_MESSAGE_REQUIRED = 5\n\t\"\"\"\t\n\tMessage is an array of values required by any message that\n\tapplies to a given type. When there is a mix of type\n\tattributes and a dehydrated type, this is set and has one\n\tmessage per dehydrated type.\n\t\"\"\"\n\t\n\t\n\tDEHTRATE_NODUPLICATE_RECORD = 6\n\t\"\"\"\t\n\trecord is deleted from a pack, but can't be purged from packet\n\tafter it's discarded."}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tif n <= 0:\n\t\treturn\n\ttry:\n\t\tdehydration_object = hydration_hooks[n + -1](dehydration)\n\t\tif deletion_hooks is not None:\n\t\t\twhile n + -1 < len(dehydration_object.decayed_types):\t# While there is still some to hydrate\n\t\t\t\tself._addItem(dehydration_object.getItem(n + -1)) # Adds the item to the Dehydration message\n\t\t\t\tn = len(dehydration_object.decayed_types)\t\t# Set"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\treturn Response(\n\t\thydration_message(\n\t\t\t[(id, n, qid, i, w)\n\t\t\tfor (id, n), qid, w in self.samples],\n\t\t\thydration_message(\n\t\t\t\t[(id, n, dehydrated), (id, n, dehydrated)]\n\t\t\t\tfor (id, y, n) in self.samples\n\t\t\t\tif len(y) > 0\n\t\t\t),\n\t\t))\n\t\n\tdef process(\n\t\tself,\n\t\tsample: list,\n\t\tdehydrated: np.array\n\t)"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn self._protocol.begin(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, handlers=handlers)\n\n@event('transaction.begin.received')\ndef begin_received(self, payload, mode, bookmarks, metadata):\n\t\"\"\"Incoming Begin Event received from the client. Transforms into a Begin message.\n\n:param payload: message containing information describing information for\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tpass\n\nclass Transaction(object):\n\t\"\"\"Transaction class\n\tThis class describes a transaction. A transaction is started as a\n\tlist of messages executed concurrently, with each in turn the same\n\t(or more!) of the same message. When the transaction is done, it\n\twill also receive the response of the last message.\"\"\"\n\n\tdef __init__(self, db, app_id):\n\t\t\"\"\"Initialize a new transaction.\"\"\"\n\t\tself.db = db\n\t\tself.app_id = app_id\n\t\tself.messages = []\t# messages\n\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tif mode not in (None, 'READ', 'WRITE', 'NONE', 'DELETE', 'ALL'):\n\t\traise NotImplementedError('mode is not supported:'+ str(mode))\n\n\tif db not in self.meta.accessed_db:\n\t\traise NotImplementedError('\"meta.accessed_db[' + db + ']\" not found; none available.')\n\n\tif \"meta\" not in self.meta:\n\t\tself.meta = Meta(self_db = {'dbs': [db]},\n                         de"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\trequest = PackstreamTransactionRequest(\n\t\tmode=mode,\n\t\tbookmarks=bookmarks,\n\t\tmetadata=metadata,\n\t\ttimeout=timeout,\n\t\tdb=db,\n\t\timpersonation=impersonation,\n\t\tdhydration_hooks=dhydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\thandler_functions=handler_functions )\n\t_execute(\n\t\tself.transaction_queue,\n\t\ttransactions.begin,\n\t\trequest,\n\t\t**{\n\t\t}\n\t\t)\n\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tmode = (mode if mode is not None else \"READ\")\n\tbookmarks = bookmarks if bookmarks is not None else []\n\tmetadata = metadata or {}\n\ttimeout = timeout if timeout is not None else DEFAULT_TRANSACTION_MAX_TIMEOUT\n\timport binascii\n\tmetadata.update({\t'BEGIN': binascii.b2a_base64(bytearray(packmsg, 'utf-8')[:64]),\n\t                   'BEGIN_METADATA': 'true'\n                   })\n\timprint, body,"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn self.app.exec(\"BEGIN\", {'mode':mode,\n\t\t'bookmarks':bookmarks,'metadata':metadata or {},\n\t\t'timeout':timeout or self.timeout,'db':db, 'dephyte':dephying_hooks,\n\t\t'hydration':hydration_hooks, 'handler': handlers})\n\n\t\n@register(u'BEGIN_STMT')\n@doc(begin)\nclass BEGIN_SEM(App):\n\tdef begin(self, mode=None, bookmarks=None, metadata=None, timeout"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tif timeout is not None and not (300 < timeout <= 300000):\n\t\traise ValueError('timeout must be <= 300 seconds')\n\n\tif mode is None: mode = 'WRITE'\n\tif mode == 'READ' and not (4.0 <= py_ver < 4.5):\n\t\traise ValueError('mode must be \"WRITE\" and <= 4.5 of Bolt')\n\n\tself.mode = mode\n\tself.bookmarks = bookmarks\n\tself.metadata = metadata\n\tself.timeout = timeout\n\tself.db = db\n\tself.imp_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\n\tdef __init__(self, mode=\"READ\", bookmarks=None, metadata=None, timeout=1000, **kw):\n\n\t\tdef _hook(type, value):\n\t\t\timport types\n\t\t\tif type in hydration_hooks:\n\t\t\t\tvalue = hydration_hooks[type](type, value)\n\t\t\tif types.isclass(value):\n\t\t\t\tvalue = value()\n\t\t\treturn value\n\n\t\tdef _delete_hook(type, value):\n\t\t\timplementations.extends(handler, type, \"delete\")\n\n\t\timplementations.bind(BOT"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn self.execute('BEGIN', mode, bookmarks, metadata, timeout, database=db, \n\t\timp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, handlers=handlers)\n\n# COMMON TRANSACTION HELPER FUNCTIONS\t\n\ndef drop_transaction(self, mode, bookmarks=None, metadata=None):\n\t\"\"\"\n\tPerforms a drop transaction.\n\t\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\theaders = { \n\t\t'CONTENT_TYPE' : 'text/xml',\n\t\t'EXPIRY'  : int(time() + timeout)\n\t}\n\n\tif (mode==\"READ\") and (bookmarks is None) and (metadata is None):\n\t\traise BoltError(ERR_BAD_MODE, \"begin() requires metadata or bookmarks.\")\n\n\tif (mode==\"WRITE\") and (params):\n\t\traise BoltError(ERR_BAD_MODE, \"begin() requires no params.\")\n\n\tif metadata is not None and not isinstance"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn n*0.5\n\n\n#print(round_half_to_even(4))       # should print out the same as the input round half to even, regardless of the decimal number\nprint(round_half_to_even(1000))    # should print out 1000, because the decimal number is 1000.12\nprint(round_half_to_even(25.5))    # should print out 25.5, because the decimal number is 25\n\n\n\n\n#  The difference between the following and the following code sample is that your goal is to avoid \n# this code. You"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn round(1 + math.sqrt(1 + 8 * n))\n\n\ndef f(x):\n\t\"\"\"\n\t>>> f(math.exp(x))\n\te**x + 1\n\t>>> f(math.sin(x + 10))\n\te**(x + 10) + e**(x + 1)\n\t>>> f(math.sin(-x))\n\te**(-x)\n\t\"\"\"\n\treturn 1 / x + math.exp(x)\n\n\ndef test_f():\n\t\"\"\"\n\t>>> round(sum(map(f"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t#return(round_to_even((n)))\n    return (n - round_to_even(n))\n\n\n\n\n\n\n\ndef round_to_even(n):\n\t\"\"\"\n\t>>> n = 3\n\t>>> round_to_even(n)\n\t3\n\t>>> n = 3.2\n\t>>> round_to_even(n)\n\t4\n\t>>> n = 3.5\n\t>>> round_to_even(n)\n\t4\n\t>>> n = 3.7\n\t>>> round_to_even(n)\n\t5"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif isinstance(n, Number) and n>=0:\n\t\tfloor=int(ceil(n)/2)\n\t\tif floor==ceil(n):\n\t\t\tfloor=floor-1\n\t\treturn floor\n\t\n\telif n <= 0:\n\t\traise ValueError(\"Number must be positive.\")\n\telif type(n)! = int:\n\t\traise TypeError(\"Must give the numerator to the function\")\n\telse:\n\t\tfloor= ceil(-math.log10(n))\n\t\tif floor==floor:\n\t\t\tfloor=floor-1\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\treturn round(n, 0)"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn round(n,2)\n\n\ndef round_half_to_even_recursion(n):\t\"\"\"\n\t>>> round_half_to_even_recursion(3)\n\t3\n    >>> round_half_to_even_recursion(3.2)\n    3\n    >>> round_half_to_even_recursion(3.5)\n    4\n    >>> round_half_to_even_recursion(3.7)\n    4\n    >>> round_half_to_even_recursion(4.0)\n    4"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n < 0:\n\t\treturn 0\n\tif 0 < n < 2:\n\t\treturn n % 2\n\tif n < 2 and n < 6 and n >= 3:\n\t\treturn n % 3\n\tif 3 <= n < 9:\n\t\treturn n % 9 + 1\n\ndef countDigit(num):\t\"\"\"\n\t>>> countDigit(9)\n\t1\n\t>>> countDigit(-5)\n\t1\n\t>>> countDigit(924)\n\t3\n\t\"\"\"\n\ttotal = 0\n\tfor i in range(num, 10"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn n\n\n\n\"\"\"\n>>> import cmath\n>>> cmath.pi\n\n\n\"\"\"\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn round(n)/2\n\ndef round_half_to_even_2(n):\t\"\"\"\n\t>>> round_half_to_even_2(3)\n    3\n    >>> round_half_to_even_2(3.2)\n    3\n    >>> round_half_to_even_2(3.5)\n    4\n    >>> round_half_to_even_2(3.7)\n    4\n    >>> round_half_to_even_2(4)\n    4\n    >>> round_half_to_"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\treturn math.ceil(n/2) if n%2==0 else n // 2\n\t\ndef round_decimals(n, places=0):\n    \"\"\"\n\t>>> round_decimals(3.5,2)\t# => 1.4\n    3.0\n\t>>> round_decimals(3.7,2)\t# => 1\n    3.0\n\t>>> round_decimals(4.6,2)\t# => 1.8\n    2.6\n\n\t:param n:\n\t:param places"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tfrom srsly.types import srsly_factory\n\tfrom.lazy import (\n\t\tLazyList,\n\t\tLazyMap,\n\t\tLazyString,\n\t\tLazySet,\n\t\tLazify,\n\t\tLazyClassAttributeLookup,\n\t\tLazifyLazy,\n\t\tLazyClassAttribute,\n\t\tLazyClassAttributeWrap,\n\t)\n\n\t__all__ = [\n\t\t\"GeometryCollection\",\n\t\t\"Point\",\n\t\t\"MultiPoint\",\n\t\t# Point is really point in RFP format, so just point_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\treturn POINT_TYPE(name, fields, srid_map)\n"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\treturn Point(name, fields, srid_map)\t\t\n\n@bp_type.defer('_point__clamped').clone_fields = True\ndef point_clamped(name, fields, srid_map):\t\"\"\"\n\tDynamically create a PointClamped subclass.\n\t\"\"\"\n\treturn PointClamped(name, fields, srid_map)\t\t\n\n@bp_type.defer('_point__polyline').clone_fields = True\ndef point_polyline(name, fields, srid_map):\t\n\t\"\"\"\n\tD"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tpoint_cls = _point_cls_dict[name]  # use an existing point class\n\tfields = map(str, fields)\n\tif srid:\n\t\tfields = ['sd','s','stere' ]\n\tcls_def = 'class {0}: {1}(\\n    {2},'.format(name, ','.join(fields), point_cls.__init__.__doc__)\n\tcls_def = '%s\\n   '% cls_def\n\tfor i, field in enumer"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tclass Point(shapeobject):\n\t\t\"\"\"\n\t\tPoint subclass.\t\n\t\t\"\"\"\n\t\tdef __init__(self, x, y, z, srid=None):\n\t\t\t\"\"\"\n\t\t\tInitialize a new Shape object.\t\n\t\t\t\n\t\t\tThe point has a name string.\t\n\t\t\tThe point geometries and geometry type fields are\n\t\t\tset in the parent.\t\n\t\t\t\n\t\t\t@param x The numeric coordinates of the point.\t\n\t\t\t@param y The numeric coordinates of the point.\t\n\t\t\t@param z The numeric coordinates of the point.\t\n\t\t\t@param"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tclass Point(object):\n\t\t\"\"\"A 2D point.\"\"\"\n\t\t__slots__ = ['point']\n\n\t\t@property\n\t\tdef point(self):\n\t\t\t\"\"\"Return a representation of the point.\"\"\"\n\t\t\treturn Point2D(*self.value)\n\n\t\t__contains__ = getattr(Point, '__contains__', str.__contains__)\n\t\t__hash__ = getattr(Point, '__hash__', None)\n\t\t__eq__ = getattr(Point, '__eq__', None)\n\n\t\t@classmethod\n\t\tdef from_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tdef f(cls):\n\t\treturn type(name, (point,), fields)\n\treturn property(f)\n\ndef get_gml(obj):\n\t\"\"\"\n\tFactory function for GML objects.\t\t\n\t\"\"\"\n\tif obj._srid!= -1:\n\t\t# For compatibility with old data sources,\n\t\t# srid=None is also valid.\n\t\tobj.ogr.SetSpatialFilter(obj._filter)\n\t\tobj.ogr.SetSpatialFilter(obj.srid)\n\treturn obj.ogr\n\n#----------------------------------------------------------------"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tattr_list = [name]\n\t\t\n\tdef __init__(self, name):\n\t\t'''\n\t\tConstructor. Name is a valid Python identifier.\n\t\t'''\n\t\tself.name = str(name)\n\tattr_list += [self.name]\n\t\n\tdef name(self):\n\t\treturn self.name\n\tattr_list += [name]\n\nclass point_wkb(object):\n\t'''\n\tWKB type; represents the set of points as a sequence of floating point numbers as\n\tconverted into a polygonal,"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tdef __init__(self, type_id, field_names, field_types, srid_map, geom_type, spatial_indexes):\n\t\t# Set the SID attribute.\n\t\tself.type_id = int(type_id)\n\t\tself.srid_map = srid_map\n\t\tself.geom_type = geom_type\n\n\t\tsuper(Point, self).__init__(field_names, field_types)\n\t\tfor f in self.fields:\n\t\t\tsetattr(self, f.name, _get_point_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t# -- Check arguments.\n\tif not name in fields:\n\t\traise ValueError('`name` argument must be one of:\\n\\t- \"Point\"\\n\\t- \"Point2D\"\\n\\t- \"Point3D\"')\n#\tif not isinstance(srid_map, (dict, dict)):\n#\t\traise TypeError('`srid_map` must be either a dictionary from SRID to index-into type (int or str'),\n#\t\t\t\t\t\t)\n\n\t# -- Get fields.\n\tfield_names, field_constraints"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n    @deprecated(message='foo has been deprecated in favour of \"bar\"')\n    def foo(x):\n        pass\n\n    @deprecated(\"'foo' has been deprecated 'baz'\")\n    def baz(x):\n        pass\n\n    @deprecated(\"foo has been deprecated in favour of 'quux'\")\n    def quux(x):\n        pass\n\n    @deprecated('foo has been deprecated in favour of'* 'quux')\n    def quux(x):\n        pass\n\n    @deprecated('foo has been deprecated, '"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\traise functools.wraps(arg[0], deprecated)(arg[0], message)\n\t\n\t\n    @deprecated('bar has been deprecated in favour of'+ 'foo')\n    def bar(x):\n        pass\n\n\nclass VersionError(Exception):\n\t\"\"\"\n\tRaised when attempting to use a version in setup.py that has unknown\n\tfeatures.\n\t\"\"\"\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\t\t\n\tdef __str__(self):\n\t\treturn self.msg\n\t\n\t\n\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdef decorator(func):\n\t\tdef new_func(x):\n\t\t\t\t'''Function is called with new arguments, but it's not a new function because the docstring\n\t\t\t\tfor it has the format:\n\t\t\t\t'The arguments to this function are:'\n\t\t\t\t{x}'''\n \t\t\t\t'''print(\"The arguments to this function are: \"+ func.__doc__ )\n\t\t\t\tfunc.__doc__ = \"\"\"The arguments to this function are: \"+ x+\" \"\"\"\n\t\t\t\tfunc.__name__ = \"\"\n \t\t\t\t'''Add some documentation for this function but will only do"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\t@deprecated(message=\"'foo' has been deprecated in favour of 'bar'\")\n\tdef foo(x):\n\t\tpass\n\t\n\tprint('Printing x.')\n\tx=1\n\tprint(x)\n\t\n\tdef bar(x):\n\t\treturn x\n\tprint(bar(x))\n\t\n\tbar(1)\n\t\n\tprint('Printing x.')\n\tx=1\n\tprint(x)\n\t\t\n\nif __name__ == \"__main__\":\n    print(\"\\n\\tThis is"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n    return foo\n#-----------------------------------------------------------------------------\n\n@deprecation\ndef test_deprecations():\n\t\n\tclass Foo(object):\n\t\t\n\t\tif_deferred = None\n\t\t\n\t\tdef foo(self, x):\n\t\t\tpass\n\t\t\t\n\tclass Bar(Foo):\n\t\t\n\t\tif_deferred = \"shouldn't be deferred\"\n\n\t\tdef foo(self, x):\n\t\t\tpass\n\t\t\t\n\tdef test(x, y=None): pass\n\t\t\n\tbar = Bar()\n\t\t\n\ttest1 = test\n\t\t\n\ttest"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef main():\n\timport platform\n\timport sys\n\timport re\n\timport time\n\tfrom optparse import OptionParser\n\timport itertools\n\tclass Options:\n\t\tdef __init__(self):\n\t\t\tself.version = (1, 15, 4)\n\n\t\tdef parse_options(self, args):\n\t\t\t\"\"\"\n\t\t\tParse user options, and return them as a dictionary.\n\t\t\t\"\"\"\n\t\t\treturn dict((a.lstrip(\"-\").replace(\"-\", \"_\").upper(), v) \\\n\t\t\t\t\t\t\tfor a, v in opts.__dict__"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\treturn 'Please see here for deprecation warning: %s' % message\n\t\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\tdef decorate(function):\n\t\treturn type(function.__name__, (DeprecatedModuleType, function), {})\n\t\n\treturn decorate\n\t\n@deprecated(\"'foo' has been deprecated in favour of 'bar'\")\ndef bar(x):\n    pass\n\t\ntry:\n\tbar()\nexcept DeprecationWarning as e:\n\tprint(e.args[0])\nexcept Exception as e:\n\tprint(str(e))\n\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\t__all__ = [ 'deprecated' ]\t\n\t\ndef deprecated_name(name, error=\"'{0}' has been deprecated in favour of '{1}'\".format):\t\n\t\"\"\"\t\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\n\t\t@deprecated_name('foo', 'bar')\n\t\tdef foo(x):\n\t\t\tprint('x is {0}'.format(x))\t\"\"\"\n\t\n\t__all__ = [ 'deprecated' ]\t\n\t\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\tdef deprecate_function(func):\t\n\t\treturn lambda *args, **kw: func(*args, **kw)\n\t\n\tdef deprecated_function(message_fn):\t\n\t\tassert isinstance(func, DeprecationWarning), \"Cannot decorate function that is not deprecating\"\t\n\t\t# Get the original function in our original caller\n\t\toriginal_fn = sys._getframe(2).f_back.f_code\t\n\t\t# Now make it deprecate it\n\t\tnew_function = deprecate_function(func) \n"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t#TODO : Make this optional? Could be implemented by providing something to\n\t#R to configure.\n\treturn code\n\t\ndef _inline_r_run(code: str) -> str:\n\t\"\"\"\n\tHelper function for R to run some code.\t\"\"\"\t\n\t#print(f\"inline_r_run: {code}\")\n\treturn code\n\t\t\ndef inline_r_command(filename: str) -> str:\n\t\"\"\"\n\tInline code on a code object for R\n\t\n\t:rtype: str\n\t\"\"\"\n\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn Code(\"\"\"import time, sys;\nimport matplotlib.pyplot as plt; plot(''')\nset_global(plot)\nset_global(cbook.savefig, n=1)\nsys.exit(0);\"\"\").rstrip()\n\nclass Logger(object):\n\t\"\"\" Logger object for saving stats in CSV format \"\"\"\n\tdef __init__(self) -> None:\n\t\tself.records = []\n\t\tself.header=['type','time','function','args','message']\n\tdef append(self,record: '"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn \"\"\n\n\t# if the command contains a special value like \"Rscript\", the special \"Rscript.exe\" is the first of the list.\n\t# It also prevents any R command in your REPL from being invoked, like so:\n\t#\n\t# if isdefined( \"Rscript\" ) then\n\t#\n\t# @{\n\t# \t%Rscript\n\t# }\n\t#\n\t# which is incompatible with this rule, so ignore it\n\t#\n\t# The \"Rscript\" special value is a special value which is not documented in R, but"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\ts = ''\n\tif code == 'r': s +='-e '\n\tif code == 'R':\n\t\ts +='-e '\n\t\tfor k in sorted(R_vars):\n\t\t\tif k and k in OTHEN_var:\n\t\t\t\tif OTHEN_var.index(k) > 5: s +='-o {}\\n'.format(R_vars[k])\n\t\t\telse: s +='-v 1\\n'\n\treturn s\n\ndef _inline_r_run(script: str, code: str) ->"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn \"\"\"\n[R] data.package.name = \"@package\"\n[R] data.package.repos_path = \".\"\n[R] data.package.data_path = \".\"\n[R] data.package.file_path = \".\"\n[R] data.package.file_extension = \".\"\n[R] print(list(data.package.file_path))\n\"\"\" + code\n\n\n@depends(__before__, __after__, __before_r_add__, __after_r_add__)\ndef __setup_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t_r = _pyR()\n\t_r.Rscript('-e \"var $d=$([graphicsDisplay][0]); while { for (i in $d; i < 10) { for (j in $D[i].keys() && $D[i]['\n\n\t# set R options for some plotting features\n\t# see the R documentation for more information\n\tif R_OPTIONS_MODE == '1':\toptions_1={'colorBarMode': 'auto',\n\n\t\t\t\t\t\t\t'colorLevelGrayscale':0,\n\n\t\t\t\t\t\t\t'colorAxis':'y"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn r.eval(code)\n\n    async def _inline_r_exec(self, code: str) -> str:\n        exec(code)\n\n\nasync def get_d_r_exec(code: str, config: Dict[str, Any]) -> str:\t\n    return await _inline_r_setup(code)\n\ndef get_r_exec(code: str, config: Dict[str, Any]) -> str:\t\n    exec(code)\n\nasync def set_r_exec(code: str, config: Dict"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif env.R_STARTED:\n\t\tcode = code.replace('\\n','')\n\treturn \"\"\"\n\t\t%R\n\t\tset -R\n\t\tset -e\n\t\tlet %R_SUFFIX = '$R_SUFFIX'\n\t\tlet %R_VERSION = '$R_VERSION'\n\t\tset $R_SUFFIX 1\n\t\tset $R_VERSION 1\n\t\t{}\n\n\t\tif [ 1!= $R_VERSION ]\n\t\tthen\n\t\t\treturn false\n\t\tfi\n\t\treturn true\n\t\"\"\".format(code)"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\timport os, glob, re\n\timport sys, io\n\timport traceback\n\timport pylab as py\n\tpy.logging.set_verbosity(pylab.DEBUG)\t# Print debug messages to screen\n\tpy.show_config()\t\t\t\t\t# Initialize interactive config before setting environment variables\n\tpy.interactive(True)\t\t# Interactive mode is always on, and this is only a global config\n\n\timport numpy as np\n\timport seaborn as sn\n\t\n\t# Initialize Python environment\n\tos.environ['PYTHONPATH"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t# first, get the \"global\" environment variables\n\t# - \"C\", \"R\", \"LDIR\", \"R_HOME\", etc.\n\tenv = get_env_var(code, r_envs)\n\n\t# add the \"R_HOME\" key to the env object\n\tenv.update(env_vars.from_os_envvar(env, \"R_HOME\"))\n\n\treturn generate_r_code(code, env)\n\ndef _generate_s_r_setup(c: str, r_envs: T.Dict["}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t).replace(\"\\n\",\"\")\n\t\t.replace(\"\t\",\"\"))\n\treturn\n\tif sys.platform == \"win32\":\n\t\tfrom _winreg import OpenKey, QueryValueEx, CloseKey,HKEY_LOCAL_MACHINE,\\\n\t\t\tHKEY_CURRENT_USER,KEY_READ,KEY_WOW64_32KEY,\"KEY_WOW64_64KEY\",KEY_ALL_ACCESS\n\telif sys.platform == \"linux2\":\n\t\tfrom os import readlink as readlink_l\n\t\tfrom os import sytem as _"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t),\n\txargs=\"\"\"\n\tUse this package instead of the GNU xargs because it is easy\n\tto deal with the problem of using ptys on old Python versions that use only\n\tasynchronous I/O.\n\t\"\"\"\n)\n\n\ncmd(\n\tcommand=\"./setup.sh {0} -n {1} --workdir {2}/results/ {3} --use_py_api_trees=1\".format(\n\t\txargs_flag, threads_count, xargs_path, workdir\n\t),\n\tuser_name="}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t))\n)\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t):\n\t\targs = xargs_helper.generate_args()\n\t\targs[\"color\"] = not args.get(\"color\", False)\n\t\targs[\"target_concurrency\"] = not args.get(\"target_concurrency\", False)\n\t\treturn xargs_helper.run(args=args)\n\n@click.command(short_help=\"Convert python source file to pty-file\", help=\"Convert source file or output dir to pty-file and run it.\")\n@click.argument(\"input_code\", type=str, nargs=1)"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t).tags(py=5)\n\n\nclass XargsPTY(PTY):\n\t# pty, no_stdin, no_stdout, no_stderr, no_log\n\tdef setup(self, target=None, worker_class=None,\n\t           tty=None, color_log=None, workdir=None, cwd=None,\n\t           loglevels=None, logfile_filter=None, work_path='.',\n\t           logfilename_filter=None, logfilename='stdout',\n\t           argv="}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\t@property\n\tdef xargs_command(self) -> str:\n\t\treturn '%s -i' % self._get_xargs_command()\n\t@property\n\tdef xargs_command_encoding(self) -> str:\n\t\treturn self._xargs_command.encode()\n\t@property\n\tdef xargs_command_encoding_size(self) -> int:\n\t\treturn len(self.xargs_command_encoding)\n\t@property\n\tdef xargs_result(self) -> List[SgRepr"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\t\n@task(\n\tdepends_on=(\"setup_depends.sh\", \"xargs_setup.sh\"),\n\tretries=2,\n)\ndef setup_yabbis(*args, **kwargs):\n\t\"\"\"\n\tDepends on setup_depends.sh to run all its dependencies with xargs and make a pty.\n\t\"\"\"\n\txargs = get_cmd(f\"setup-{set_up_xargs_args(args)}\")\n\ttry:\n\t\txargs(xargs, *args, **kwargs"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t__target_concurrency__=\"1\"\n\t__arguments_separator_regex__=r\"(?:\\;\\$|\\$) \"\n\t__arguments_end_regex__=r\"(?:\\:)$\"\n\t__arguments_regex__=r\"%arguments_separator %arguments_end\"\n\t__xargs_arguments_set_regex__=r\"%(?P<value><?([^<>]*)</?>: (:?[^']*|\\(.*\\)+)/%(?"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn [se[random.randint(0, len(se))] for i in seq]\n\n# TODO: Implement this function :)\n\ndef read_line_format(path: Path, line_format: str) -> None:\n\t\"\"\"\n\tRead line with file-format `path`\n\tReturn a dict with all columns separated\n\twith `line_format` and a `{column name: data}` line seperator\n\t\"\"\"\n\t\n\tinp = []\n\tfin = None\n\tfor line in open(path,'r'):\n\t\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn sample(seq, k=len(seq))\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tout = seq[:]\n\tfor i in reversed(range(0, len(seq))): \n\t\tindex = (seq[i] + seq[i+1]) % 2\t\n\t\tout[i] = out[i+1]\n\t\tout[i+1] = seq[index]\t\n\treturn out\n\n\ndef _partition_at_part_size(seq: Sequence[str], size: int) -> list[str]:\n\t\"\"\"\n\tPartition a seq into several seq_sizes\n\n\tParameters\n\t----------\n\tseq: Sequence[str"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn list(np.random.permutation(seq))"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t# TODO -- change back to shuffle within a single function\n\treturn seq * seq.reverse().copy()[:]\n\n\t\n# TODO -- create a function that takes list \ndef random_walk(starting_state: str, \n\tdestination_state: str,\n\ttarget_num_states: int = 1000,) -> list[str]:\n\t\"\"\"\n\tRandom walk with target_num_states states going back to the starting state\n\t\"\"\"\n\t# TODO (part 2) - Implement the random walk\n\t\n\tinitial_states = [starting_state]"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn seq[random.randint(0, len(seq) - 1)]\n\n\n\t# https://stackoverflow.com/questions/56121289/random-in-order-to-shuffle-a-bunch-of-strings\n\t# def _ordered(self, l: Sequence[str]) -> list[list[str]]:\t\"\"\"\n\t# \tOrder the items\t\"\"\"\n\t# return [list(self._shuffle(i)) for i in l]"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn random.sample(seq, len(seq))\n\n\ndef write_file(file: str, content: list[int]):\n\t\"\"\"\n\tWrite a list of items to a text file.\t\"\"\"\n\twith open(file, 'a') as out_file:\n\t\tout_file.writelines(content)\n\n\n\n\ndef main():\n\t\"\"\"\n\tCommand line program.\n\t\"\"\"\n\t_, txt = read_file(\"input.txt\")\n\tshuffled_txt = _shuffled(txt)\n\t_, txt_sh"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tshuffle(seq)\n\treturn seq\n\n\t\ndef get_sentence_data(files: Union[list, None], batch_size: int = 8, seq_len: int = 8) -> tuple:\n\t\"\"\"\n\t\tFunction returns the data to train.\n\t\tArgs:\n\t\t\tfiles: a list of txt files containing the data of each review.\n\t\t\tbatch_size: int: the size of the batch.\n\t\t\tseq_len: int: the length of the input sequence.\n\t\tReturns:\n\t\t\tx: The sentence ids\n\t\t\ty: The sequence "}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn [seq[i] for i in rng.sample(range(len(seq)),len(seq))]\n\n\ndef _normalize(text: str,) -> str:\n\t\"\"\"\n\tNormalize text: remove links and convert to lowercase.\n\t\"\"\"\n\ttext = re.sub(r\"https?://.+?\\\\/\", '', text).lower()\n\ttext = re.sub(r\"www\\S+?\\\\\", '', text)\n\n\treturn text\n\n\ndef _build_conversation(conversation: Any) -> any:"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tshuffle(seq)\n\treturn seq\n\t\n\t\ndef test_shuffled_p():\n\t\"\"\"\n\tTest permutation function\n\t\"\"\"\n\tseq = [\"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"d\", \"d\", \"e\", \"e\", \"f\", \"f\", \"g\", \n           \"g\", \"i\", \"i\", \"j\", \"j\", \"k\", \"k\", \"l\", \"l\", \"m\", \"m\", \"n\", \"n\", \n           \"o\", \"o"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\treturn tuple(int(i) for i in re.split('(\\d+)', s, 3)[:3])\n\n#------------------------------------------\ndef write_config(config):\t\n\t\"\"\"\n\twrite configuration file\t\n\t:\n\t* keys: \n\t  - path\n\t  - config_dic\n\t\"\"\"\n\tpath = config[\"path\"]\n\tlogging.info(\"write config: %r \", path)\n\tpath_filename = os.path.join(path, \"config.json\")\t\t\n\tlogging.info(f\"write config save"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tres: list = s.split(\"+\")\t\t# split into a list of strings\t\t\t\n\tfor i in range(0,len(res)-1):\t\t# check each substring\t\t\n\t\tfor j in range(i+1,len(res)):\t# find the '+' in the end\n\t\t\tif res[i] >= res[j]:\t# if the first element is greater, than it is second and the rest are more than \n\t\t\t\tres[j],res[i] = res[i],res[j]\t\t# swap the two values\n\t\t\t\tbreak\t\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif len(s) > 3:\n\t\tx, s = s[:3], s[3:]\n\tif len(s) == 0:\n\t\tpass\n\telif len(s) == 1:\n\t\tif ord(s) == 0xd0:\n\t\t\treturn (4,0)\n\t\tif ord(s) == 0xd1:\n\t\t\treturn (3, 0)\n\t\tif ord(s) == 0xd2:\n\t\t\treturn (2, 0)\n\t\tif ord(s) == 0xd3:\n\t\t\treturn (1, 0)\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif s is None:\n\t\treturn tuple()\n\tv = split_version.split('.')\n\treturn tuple(map(int, v))\n\ndef split_version(v: str) -> tuple[str,...]:\n\t\"\"\"\n\tversion format: e.g. 2.0.0 (2.0.0) \n\t* version as a numeric version * - version as a (non-v) numeric value\n\t\"\"\"\n\tif v is None:\n\t\treturn tuple()\n\tl = list(map(int, v.split('.')))\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\timport re\n\tm = re.search(r'\\d+', s) #version number of 'b''\\xfc\n\tif m:\n\t\treturn int(m.group(), 16) * 100000\n\traise ValueError(\"Couldn't convert string %r\" % s)\t\n\t\t\t\t \n\n\n# ================================================================================================================================\n# ==============================================================================================================================\n\nclass Lazy(object):\n\t__slots__ = ()\n\t_cache = {}\n\tdef __getitem__(self, k):\n\t\tif k in self._cache:\n\t\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tver_l = s.split('.')\n\tif len(ver_l) == 1:\n\t\treturn int(ver_l[0]), 0\n\tparts = list(map(int, ver_l[0].split('.')))\n\tparts += ver_l[1].split('.')\n\treturn parts[0], parts[1], int(parts[2])\n\n\ndef check_version(\n\tversion: Version,\n\tmin_version: dict,\n\tmax_version: dict\n) -> tuple[Union[int, Version], Union[int,"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ts = s.replace('GitHub', 'PyCrust').replace('-', '').split('.')\n\tif len(s) == 2:\n\t\treturn int(s[0]), int(s[-1])\n\t\t\n\treturn (int(s[0][:2]), int(s[1]), )\n\n\n\n@click.command()\n@click.option('--project', '-p', help='Your GitHub repository')\n@click.option('--branch', default='main')\n@click.option('--debug', is_flag=True"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\treturn tuple(map(int, re.search(r'[ \\d\\.\\:]', s).group(0).split('.')))\n\n\t# version_list[0] = '{0}.{1}.{2}'.format(nxos._version.release.major, nxos._version.commit.status, nxos._version.patch)\n\t# version_list[1] = '{0}.{1}.{2}'.format(nxos._version.release.minor, nxos._version.commit.status, nxos"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tv1, v2 = ('0', '0')\n\tfor i in s.split('.'):\n\t\tif not i:\n\t\t\tcontinue\n\t\telif int(i) > 255 or  len(i) > 32 or not i.isdigit():\n\t\t\traise ValueError('%s is not a valid version.' % s)\n\t\telse:\n\t\t\tv1 = (int(i,16)+2) * (256**(2-len(i))) + (int(i,16)+2)\n\t\t\tv2 = (int(s.split('.')["}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\treturn int(s), int(s[2:])\n"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif cmd[0][-1] == \":\" and cmd[1]!= \"\":\n\t\tcmd = cmd[0] + \":\" + cmd[1]\n\treturn cmd\n\ndef unquote(value: str) -> Union[bool, int, float, str, bytes, str, bytes, dict, str, dict]:\n\t\"\"\"\n\tNormalizes a string and converts it to it's unicode representation.\n\t\"\"\"\n\tif isinstance(value, dict):\n\t\treturn {k: unquote(v) for k, v in value.items()}\n\tel"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd1 = \" \".join(cmd)\n\treplace_cmd1 = cmd1.replace(\"\\\\\", \"_\")\n\treplace_cmd2 = replace_cmd1.replace(\".\", \"_\")\n\tif os.path.normcase(re.sub('[^\\\\w_]', '_', replace_cmd1.upper()))!= re.sub('[^\\\\w_]', '_', replace_cmd2.upper()): \n\t\t#print('here')\n\t\treturn cmd1\n\telse:\n\t\treturn cmd1\n\nimport subprocess\n\ndef run"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\ts = cmd[0]\n\tif '-' in s or '+' in cmd[0] or '=' in cmd[0]:\n\t\t# the following might parse well\n\t\tif '=' in cmd[0]:\n\t\t\treturn tuple(cmd[0].split('=', 1))\n\t\tif cmd[0] == \"%TEMP\" and '|' not in cmd[0]:\n\t\t\treturn tuple(cmd[0].split('|',1))\n\t\ttemp_dirs = None if not os.environ.get('TEMP', None) else os.environ['"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\timport os.path\n\n\trootdir, path = cmd[0].split(os.path.split(cmd[1])[0], 1)\n\tif cmd[1].startswith(\"$(\"):\n\t\tnew_command = cmd[1][3:-1]\n\t\tif cmd[1].startswith(\"\"\"#\"\"\") or (cmd[1] in [\"\\\"\"\"\", \"\"]):\n\t\t\tnew_command = commandsort.uniquify_script(\n\t\t\t\tsubst_strs = [cmd[1][1:-1]],\n\t\t\t\tpath = os."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd = list(cmd)\n\ts_num = cmd.count('/')\n\tfull = ['/'] * (s_num + 1) + cmd\n\tindex = full.index('*')\n\tc = full[index + 1:]\n\td = ''.join([d for d in c])\n\n\ta_cmd = ''\n\tfor c in c:\n\t\tif c not in ['.'] and c not in ['(', ')', '[]', '{}']:\n\t\t\ta_cmd += c\n\ta_cmd = a_cmd[1"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd = [c if c.startswith('#') else c.split('=')[-1] for c in cmd]\n\t# we don't have the special `$` (which makes it the first)\n\t# this also makes any `=' in both shell commands\n\t# and bash, etc.\n\tfix_cmd = []\n\tfor x in cmd:\n\t\tif x.startswith('#'):\n\t\t\tcontinue\n\t\t# use the last part of the command\n\t\t# because we don't need it anyway\n\t\tfix_cmd.append(x["}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\tcmd = list(map(str.strip, cmd))\n\tcmd.append('-h')\n\tcmd.append('-d')\n\tif platform.system() == 'Windows':\n\t\t_, s, _ = platform.python_compiler()\n\t\t_, a, b = platform.uname()\n\t\tcmd = ['cmd', '/C', '\"{}/installer{}\" {}'.format(s, _, cmd)]\n\t\tif platform.system()!= 'Linux':\n\t\t\tcmd.append('-D')\n\treturn tuple(map(str"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\ttargets = dict()\n\tfor i in range(len(cmd)):\n\t\tif '(' in cmd[i] or '.exe' in cmd[i] or '.cmd' in cmd[i]:\n\t\t\tpath = f'{winreg.HKEY_CURRENT_USER}\\\\Software\\\\{cmd[i].split(\"\\\\\", 2)[0]}.cmd'\n\t\t\t# noinspection PyPep8\n\t\t\twith open(path, 'rb') as f:\n\t\t\t\ttargets[path] = f.read().decode()\n\n\tcmds = cmd"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\tif '\\\\' in cmd[0]:\n\t\t# convert to absolute path\n\t\tcmd[0] = str(pathlib.Path(cmd[0]).resolve())\n\n\t\t# change to absolute path\n\t\tcmd[0] = str(pathlib.Path(cmd[0]).absolute())\n\n\tfixed_path = []\n\t# add current dir\n\tfixed_path.append(cmd[0])\n\n\tfor i in range(1, len(cmd)):\n\n\t\t# the sub cmd does not work on windows\n\t\tif os.path.sep in cmd["}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\ttoks = cmd.upper().split() \n\tret_cmd = \"\"\n\tfor word in toks: \n\t\tword_normalized = \"_\".join(word[::-1])\n\t\tif word[0] == \"Z\": # Z is reserved word\n\t\t\tword_normalized = \"../\" + word_normalized\n\t\tif word.isdigit(): # valid number\n\t\t\tret_cmd += word + \" \"\n\t\t\tcontinue\n\t\telif word.isalnum() or word[1:2] in \"0123456789\": # a-z/"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef decorator(f):\n\t\tif hashkey in cache:\n\t\t\treturn cache[hashkey](f)\n\t\telse:\n\t\t\tcache[hashkey] = cached_func(f, cache, key, lock)\n\t\t\treturn cache[hashkey]\n\treturn decorator\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n# Internal class: the result of a thread\n#   (thread has an ID and it is a threading.Thread instance)\n#\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nclass result():\t\n\tdef __init__(self, id, func, args, k"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tfrom functools import wraps\n\timport gc\n\n\tif lock is None:\n\t\tlock = dict()\n\n\tdef cache_fn(func, *args, **kwargs):\n\t\timport threading, time\n\t\tst = time.time()\n\t\tckey = hashkey(args, kwargs)\n\t\tif ckey in cache:\n\t\t\tresult = cache[ckey]\n\t\telse:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[ckey] = result\n\t\telif not lock.get():\n\t\t\tgc.collect()\n\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tassert getattr(cache, key) is None, \"You have already computed a cached function.\"\n\t\n\tdef wrapper(fn):\n\t\tif cache.get(key) is not None:\n\t\t\tif lock is None:\n\t\t\t\traise RuntimeError(\"%s is already compiled in a cache.\" % key)\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\texcept:\n\t\t\t\t\tlock.release()\n\t\t\t\t\traise\n\t\telse:\n\t\t\tresult = fn()\n\t\t\tif lock is None:\n\t\t\t\tcache[key] = result\n\t\t\telse:\n\t\t\t\ttry"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tassert callable(key)\t\t# verify that `key` is callable\t\n\tassert callable(cache)\t\t# verify that `cache` is callable\t\n\tassert lock is None or isinstance(lock, threading.Lock)\n\t\n\tdef cached_fn_wrapper(fn):\t\t\t\t# cache the result\n\t\tkey = fn.func_code \t\t\t# cache the key\n\t\tret = None\t\t\t\t\t# store results in this cache\n\t\targs = ()\t\t\t\t\t\t# cache them as 1d or 2d\t\t\n\t\tif lock is None:\n\t\t\tlock = threading."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef wrapper(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = None\n\t\t\ttry:\n\t\t\t\tcache_key = get_cache_key(func.__name__, args, kwargs)\n\t\t\t\tresult = cache[cache_key]\n\t\t\texcept KeyError:\n\t\t\t\ttry:\n\t\t\t\t\tcache_key = hashkey(func, args, kwargs)\n\t\t\t\t\tresult = cache[cache_key] = func(*args, **kwargs)\n\t\t\t\texcept:\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tif isinstance(_excinfo.value.__traceback"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef new(f, *args, **kargs):\n\t\tkey = lambda args: hashkey(args) % (len(cache)<<1)\n\t\tif lock is not None:\n\t\t\tlock.acquire()\t\t\t\n\t\tv = cache.get(key(args))\n\t\tif not v:\n\t\t\ttry:\n\t\t\t\tc = f(*args, **kargs)\n\t\t\texcept:\n\t\t\t\tc = f(*args, **kargs)\n\t\t\t\tc = v\n\t\t\tcache[key(args)] = c\n\t\t\treturn c\n\t\telse:\n\t\t\ttry:\n\t\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef memoizer(*args, **kw):\n\t\ttry:\n\t\t\tcache['args'] = args\n\t\t\tcache['key'] = hashkey\n\t\t\tcache['kw'] = kw\n\t\t\tcache['args_hash'] = hash(args)\n\t\t\tcache['kw_hash'] = hash(kw)\n\t\t\ttry:\n\t\t\t\tresult = cache['result']\n\t\t\texcept:\n\t\t\t\tresult = _None()\n\t\t\t\tcache['result'] = result\n\t\texcept:\n\t\t\tif lock is None:\n\t\t\t\traise\n\t\t\telse:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tcache"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n\tdef _wrapped(*args, **kwargs):\n\t\n\t\t# Don't actually memoize the return value.  The function\n\t\t# we're wrapping will be able to change the return value\n\t\t# directly without saving the state of the function.\n\t\twith cache as __cached__:\n\t\t\treturn __cached__[key]\n\n\t\t# Don't decorate this function.\n\t\treturn func(*args, **kwargs)\n\n\t# Cache it so we can access it from multiple times with the same\n\t# key.  This avoids making a round trip for expensive GET"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef memoer(**kwargs):\n\t\treturn cache.get(key, memoer(**kwargs))\n\treturn memoer\n\ndef timed(sfunc):\n\t\"\"\"\n\tDecorator to delay the execution of the function, as a\n\tdecorator.\t\n\t\"\"\"\n\tdef _(*args, **kwargs):\t\n\t\tfrom threading import Timer\n\t\ttiming = Timer(round(args[0].exec_time), sfunc, args, kwargs)\n\t\ttiming.start()\n\t\treturn timing.output()\n\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tif lock:\n\t\tlock.acquire()\n\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# We'll assume that func(*args, **kwargs) is\n\t\t\t# expected to return the result from cache.\n\t\t\tresult = f(*args, **kwargs)\n\n\t\t\t# If no result was calculated, or we return an\n\t\t\t# unexpected result, we have a cache hit.\n\t\t\t# Try to acquire the lock and increment the number of unique results.\n\t\t\tif isinstance(result, (int,long)):\n\t\t\t\tif result in"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorator(func):\n\t\tcache = LRU(maxsize, ttl=ttl, timer=timer, typed=typed)\n\t\tcache.set(func.__name__, func, ttl=ttl)\n\t\treturn cache.get\n\treturn decorator\n\t\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef wrapper(func):\n\t\twrapper.fns[func.__name__] = fns[func.__name__] = LRU(maxsize=maxsize, ttl=333, typed=typed)\n\t\treturn func\n\twrapper.fns = fns\n\treturn wrapper\n\ndef ttl_cache_with_typed(\n\t\tfns=None, \n\t\tmaxsize=128, \n\t\ttyped=True,\n\t\ttimer=time.monotonic \n): \n\t\"\"\"\n\tDecorator that wraps a method with a"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tttl_cache = Timedict(ttl=ttl)\t# TTL dictionary is cacheable\n\n\tdef wrapper(fn):\n\t\t@wraps(fn)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tcached = ttl_cache.get(fn)\n\t\t\tif cached is False: # function not available\n\t\t\t\tret = fn(*args, **kwargs)\n\t\t\t\tcached = True\n\t\t\t\tttl_cache.set(fn, ck_value(c) if ck_value is not None else None)\t# cache is now complete\n\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t@_memorem_time_cache(maxsize, ttl, time.monotonic, typed, timer)\t\n\tdef memoizer(func):\n\t\t\"\"\"\n\t\tA decorator which memoizes the function and its result up\n\t\tto `maxsize` items per second.\n\t\tTo prevent recomputation of this value, pass in an explicit\n\t\t`ttl=` value of zero or lower.  This function will be\n\t\tcalled only each invocation of `func()` until the number of\n\t\titems in `maxsize` has been called (up to `tt"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tif not typed:\n\t\t# Note, `timer` should be called once per TTL.\n\t\t# See https://www.python-future.org/3/tutorial/example/timing.html for\n\t\t# an alternate solution that runs several separate invocations in\n\t\t# a single timer.\n\t\tdef memoize_ttl_with_timer(function):\n\t\t\t@functools.wraps(function)\n\t\t\tdef memoize_function(args, *args1, **args2):\n\t\t\t\tttl = timer() - args[0].start_time\n\t\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef wrapper(user):\n\t\tcache = []\n\n\t\tdef _ttl_cache_func(ttl=ttl):\n\t\t\tcache.append((ttl, user))\n\t\t\tprint(f'{user} retrieved: {ttl} minutes')\n\t\t\treturn cache\n\n\t\t@functools.wraps(user)\n\t\tdef _ttl_cache(*args, **kwargs):\n\t\t\tcache.append((int(round(timer-time.monotonic()+args[0][0])), user))\n\t\t\tprint(f'Fired call to TTL (with {"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef wrapper(func):\n\t\t@functools.lru_cache(maxsize=maxsize)\n\t\tdef wrapped_func(*args, **kwargs):\n\t\t\tttl = timer()-ttl\n\t\t\tif ttl<ttl_cache:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\telse:\n\t\t\t\tif not typed:\n\t\t\t\t\treturn max(func(*args, **kwargs), key=lambda x: -ttl_cache[x] if isinstance(x, Exception) else None)\n\t\t\t\telse:\n\t\t\t\t\titems = max_cache_items"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\t@wraps(func)\n\tdef wrapper(*args, **kwds):\n\t\tcached = LRU(maxsize, timer, typed)\n\t\trv = ctx.cache_value_to_dec(typed, func, *args, **kwds)\n\t\tcached.set_value(rv)\n\t\treturn rv, cached\n\t\t\n\treturn wrapper\n\n#-----------------------------------------------------------------------------------\n# Class LRU(data structure)\n#-----------------------------------------------------------------------------------\nclass LRU(object):\n\t\"\"\"\n\tAn Lru linked list implementation. Allows us"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\timport time, pickle\n\td = dict(ttl=ttl)\n\tdef lru(fn):\n\t\tfn.__code__ = fn.__code__.replace(CODE_SETTING, CODE_SETTING_KEY)\n\t\tfn.__tb_frame__ = fn.__tb_frame__\n\t\tfn.__dict__ = fn.__dict__.copy()\n\t\tdef inner(*args, **k):\n\t\t\tobj = fn(*args, **k)\n\t\t\tresult = obj.predictor(d, obj.model)\n\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef ttl_wrapper(wrappedfn):\n\t\t@wraps(wrappedfn)\n\t\tdef decorated(item, *args, **kwargs):\n\t\t\ttimeelapsed = timer()-time.monotonic()\n\t\t\tttl_count = (timeelapsed//10)%item_count or 1\n\t\t\tttl = next_ttl(ttl_count, ttl)\n\t\t\t\n\t\t\tttlval = ttl_cache(maxsize, ttl, timer)\n\t\t\tresult = wrappedfn(item, args, kwargs)\n\t\t\t\n\t\t\tif"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t\tdef mru_cache_func(func):\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapper(self):\n\t\t\t\tresult = func(self)\n\t\t\t\tif self._cached_data is None or result is None or self._cached_data[\"updated\"]:\n\t\t\t\t\tnow = time.time()\n\t\t\t\t\tif (self._cached_data is not None) and (now > self._cached_data[\"updated\"]):\n\t\t\t\t\t\tself._cached_data = None\n\t\t\t\t\t\tself._update_mru(self._data_cache"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef _mru_cache(func):\n\t\t@functools.wraps(func)  # to allow for argument passing in arg\n\t\tdef wrap(maxsize=maxsize, typed=typed):\n\t\t\tcache = {}\n\t\t\tdef _wrapper(*args, **kwargs):\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[args, kwargs]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tcache[args, kwargs] = value = func(*_args, **_kwargs)\n\t\t\t\t\treturn cache[args, kwargs]\n\t\t\tif typed:\n\t\t\t\treturn _wrapper"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef fun(func):\n\t\tdef inner(*args, **kwargs):\t\n\t\t\tfrom functools import lru_cache\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\texcept MemoryError:\n\t\t\t\tout = lru_cache(maxsize)(func)\n\t\t\t\tout()\n\t\t\t\treturn out(*args, **kwargs)\n\t\tinner.func_name = getattr(func, '__name__', func)\n\t\tinner.func = func\n\t\tinner.maxsize = maxsize\n\t\tinner.typed = typed\n\t\treturn inner\n\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef mru_cache_decorator(func):\n\t\tcache = LRU(maxsize=maxsize)\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry: return cache[args]\n\t\t\texcept KeyError:\n\t\t\t\tcache[args] = cache_result = func(*args, **kwargs)\n\t\t\t\tcache[args] = cache_result\n\t\t\t\treturn cache_result\n\t\t\texcept TypeError: return cache_result\t\n\t\treturn wrapper\n\treturn mru_cache_decorator\n\n# ----------------------------------------------------------------------------------------------------\n\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef cached_mru(wrapped):\n\t\t@wraps(wrapped)\n\t\tasyncm = wrapped\n\t\t@asyncm.register_cache(maxsize)\n\t\tdef _checkIfCache(r):\n\t\t\tr._mru_cached = r.__dict__.get('_mru_cached', False)\n\t\t\tr.__dict__['_mru_cached'] = True\n\t\t\treturn r\n\t\t@asyncm.register_cache_cleanup(None)\n\t\tdef _cleanUpCache(r):\n\t\t\tif r._"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tkey = _MemoKey(typed) \t\t\t\t# make the key\n\tvalue = _NonCallableMutableMapping()   # make a empty dictionary for holding results\n\tdef apply(func, *args, **kwargs): \t\t  # decorate the callable\n\t\tresult = value.__getitem__(key)   \t\t  # retrieve from the existing dictionary\n\t\tif result is not None and id(result) not in value:\n\t\t\tvalue[key] = func(*args, **kwargs)\t\t# call function\n\t\t\tvalue[id(result)] = None"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef decorator(fn):\n\t\t\n\t\t# We'll get a decorator, we assume that all the decorated function's output\n\t\t# is to be hashable. Here we just store its unique index. The reason is that\n\t\t# hashlib.sha1 operates like str.hash, so Python 2 uses the default hash\n\t\t# function.\n\t\tfn_id = id(fn)\n\t\tfn_hash = hash((fn_id, fn_id, fn_id))\n\n\t\t# We can't call fn(...), because otherwise the following would fail:\n\t\t# 'ModuleNot"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrap(func):\n\t\tfunc.cache_mt = None\n\t\t\n\t\tdef cache_mt(func):\n\t\t\tfrom.mrup import mru as cache_mrup\n\t\t\t\n\t\t\tfunc.cache_mt = FuncMru(func, cache_mrup, typed=typed)\n\t\t\tfunc.cache_mt.func_name = func.__name__\n\t\t\t\n\t\t\treturn func\n\t\t\n\t\ttry:\n\t\t\timport pylru\n\t\texcept ImportError as err:\n\t\t\traise import_error_exception(\n\t\t\t\t'pylru is required to"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t# Use a dictionary of the most recently used values, so that we don't\n\t# waste our function work on getting multiple results even for the\n\t# same argument.\n\tmemo = _cache = {}\n\n\tdef wrapper(function):\n\t\t# Handle the `typed` argument specially if a callable is typed,\n\t\t# and we need to store it as a dictionary.\n\t\tif typed:\n\t\t\t@functools.wraps(function)\n\t\t\tdef decorator(*args, **kwargs):\n\t\t\t\tif kwargs:\n\t\t\t\t\tkwargs = {kwarg:"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t# we cannot use named_tuple because if this code is run twice on the same\n\t# task, the name will be wrong and you will be unable to re-use the\n\t# function for several tasks.\n\tdef memoizing(func):\n\t\tname = None\n\t\tif hasattr(func, \"__original_name__\"):\n\t\t\tname = func.__original_name__\n\t\telif hasattr(func, \"__name__\"):\n\t\t\tname = func.__name__\n\t\telif typed:\n\t\t\tname = str(func)\n\t\tkey ="}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(func):\n\t\tcache = LRUCache(maxsize)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tif'return_as_list' in kwargs:\n\t\t\t\treturn list(cache.cache[_i] for _i in args)\n\t\t\telse:\n\t\t\t\treturn cache.cache[_i]\n\n\t\t\ttry:\n\t\t\t\t# try to do this first time to make sure the cache is NOT\n\t\t\t\t# full\n\t\t\t\tdel cache.cache[_len]  # for mypy; is not an instance method\n\t\t\t\treturn func(*args, **kwargs)"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef wrapped(f):\n\t\tif maxsize <= 0:\n\t\t\treturn f\n\t\tdef memoized(*args, **kwargs):\n\t\t\tif not typed:\n\t\t\t\treturn f(*args, **kwargs)\n\t\t\tcache = {}\n\t\t\tfor k, v in kwargs.items():\n\t\t\t\tcache[k] = v\n\t\t\tdef cache_result(n):\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[n]\n\t\t\t\texcept KeyError: \n\t\t\t\t\tcache[n] = result = f(n)\n\t\t\t\treturn result\n\t\t\tcache_result.cache = cache\n\t\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorated(fn):\n\t\t# create memoization object for LRU (least recently used)\n\t\t# functions.\n\t\tfn._memo = d = _LRU_Cache(maxsize = maxsize, type=typed)\n\t\t# this function is designed to be used whenever one wants to\n\t\t# keep a cache for a specific kind of object (i.e. a `UserProfile`),\n\t\t# instead of a standard class.\n\t\tif fn._self_reserved:\n\t\t\tdef cache_wrapper(*args, **kwargs):\n\t\t\t\t# memoizer.get"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef real_wrapper(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcache = LRUCache(maxsize=maxsize, typed=typed)\n\t\t\ttry:\n\t\t\t\treturn cache[func]\n\t\t\texcept KeyError: # the key has expired\n\t\t\t\targs = tuple(args)\n\t\t\t\tkwargs = dict(kwargs)\n\t\t\t\treturn wrapper(*args, **kwargs)\n\t\tsetattr(wrapper, '__doc__', func.__doc__)\n\t\twrapper.__module__ = func.__module__\n\t\treturn wrapper\n\treturn real_"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(function):\n\t\t# TODO: Use cache to reduce cache complexity to `maxsize`\n\t\t# TODO: Use maxsize to control the complexity to avoid\n\t\t# infinite recursions\n\n\t\t@functools.wraps(function)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\t# TODO use the memoizer to control the expected complexity\n\t\t\t# of the recursion itself\n\t\t\t# TODO create a memoizer with a max_size=`maxsize` if not `None`\n\t\t\tcache = LRUCache(maxsize)\n\t\t\ttry:"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t# TODO: Add support for caching to functions based on arguments\n\t\"\"\"lru = {\n\t\t# Initialize a key dict\n\t\t\"key\" : None,\n\t\t# Initialize a dict indexed by unique key as an argument\n\t\t\"arguments\" : {}\n\t}\n\n\tdef decorator(fn):\n\t\tif not hasattr(lru, \"key\"):\n\t\t\tlru = {\n\t\t\t\t\"key\": None,\n\t\t\t\t\"arguments\": {}\n\t\t\t}\n\t\t_dict = lru[\"arguments\"]\n\t\t_dict[\"key\"] = fn.__"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(original_function):\n\t\tcache = defaultdict(int) # defaultdict()=dict_of_functions=defaultdict(int)\n\n\t\tdef wrapper(argument):\n\t\t\tif cache[f\"{argument}\"] == maxsize:\n\t\t\t\tcache[f\"{argument}\"] = argument\n\t\t\telse:\n\t\t\t\tcache[f\"{argument}\"]=argument\n\t\t\t\treturn original_function(argument)\n\n\t\twrapper.cache = cache\n\t\twrapper.typed = typed\n\n\t\treturn wrapper\n\treturn decorator\n\n@lru_cache()\ndef get"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef inner(a, *args, **kwargs):\n\t\t\tresult = func(a, *args, **kwargs)\n\t\t\tif result is None or isinstance(result, Exception):\n\t\t\t\tresult = func(a, *args, **kwargs)\n\t\t\telif result is not None and len(result) == 1:\n\t\t\t\tresult = result[0]\n\t\t\telse:\n\t\t\t\tresult = tuple(result)\n\t\t\tmemo = dict()\n\t\t\tfor r in result:\n\t\t\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef decorator(fn):\n\t\tif typed:\n\t\t\tcache = None\n\t\t\tdef cached_get(*args):\n\t\t\t\tnonlocal cache\n\t\t\t\tif cache is None:\n\t\t\t\t\tcache = {}\n\t\t\t\tif len(args) == 1 and isinstance(args[0], dict):\n\t\t\t\t\tresults = cache.setdefault(args[0], [])\n\t\t\t\t\tif len(results) >= maxsize:\n\t\t\t\t\t\tresults.pop(0)\n\t\t\t\t\treturn fn(*args)\n\t\t\t\telse:\n\t\t\t\t\tif args in cache:\n\t\t\t\t\t\tresults = cache[args]\n\t\t\t\t\telse:\n\t\t\t\t\t\tresults"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\tdef decorator(original_function):\n\t\t@functools.wraps(original_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tmemoizer = LRUCache(maxsize)\n\n\t\t\tif str(original_function.__name__) in ['show':\n\t\t\t# show() does something to show the state of the system when run\n\t\t\t#  see https://docs.python.org/library/functions.html#print-and-sending-output for help\n\t\t\t#  see https://docs.python.org/2/library/"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef f(fn, *args, **kwargs):\n\t\tfn.cache_info_tries = 0 \n\t\tfn.cache_info_t = 0 \n\t\tfn.cache_info = None \n\t\tfn.cache_t = time() \n\t\tfn.maxsize = maxsize \n\t\tfn.typed = typed \n\t\tfn.args = args \n\t\tfn.kwargs = kwargs \n\t\t# Make sure to initialize a dictionary that will contain a hash/value of fn's arguments\n\t\tfn_args_dict = {}\n\t\tfor"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tif not typed:\n\t\tdef _lfu(*args, **kwargs):\n\t\t\tl = lrucache(maxsize)(tuple.__getitem__)\n\t\t\tl(*args, **kwargs)\n\t\t\treturn l\n\telse:\t\n\t\tdef _lfu(*args, **kwargs):\n\t\t\treturn tuple.__getitem__(tuple(args), **kwargs)\t\n\tdef wrapper(f):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tif not isinstance(tuple, tuple):\n\t\t\t\t_ = lambda x: tuple(x"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef lfu_deco(func):\n\t\tif not typed:\n\t\t\t@wraps(func)\n\t\t\tdef cached_lfu_function(*args, **kwds):\n\t\t\t\tkey = (args, kwds)\n\t\t\t\tif key in lfucache.keys_cache:\n\t\t\t\t\t# We already have cached the result from `kwds`...\n\t\t\t\t\t# Return it as usual.\n\t\t\t\t\treturn lfucache.get(key)\n\t\t\t\tr = func(*args, **kwds)\n\t\t\t\tlfucache[key] = r\n\t\t\t\treturn r\n\t\t\treturn"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef lazyUFunc(cachefn):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tfilt = cachefn.mframe(args)\n\t\t\t\tif len(filt) > maxsize > 1:\n\t\t\t\t\t#print \"LFUCache with id\", cachefn.id(args, **kwargs)\n\t\t\t\t\treturn cachefn.lget(filt, id=cachefn.id(args, **kwargs))\n\t\t\t\telse:\n\t\t\t\t\treturn cachefn(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tcachefn.mreset(args)"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef decorator(func):\n\t\timport functools\n\t\timport time\n\t\tfrom functools import wraps\n\t\tfrom queue import PriorityQueue\n\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*arg, **kwargs):\n\t\t\tif (\n\t\t\t\tnot kwargs \n\t\t\t\tor (\n\t\t\t\t\tlambda s: kwargs.get(s.key,None) not in s.vals\n\t\t\t\t\tand len(s.vals) < maxsize\n\t\t\t\t\t)()\n\t\t\t):\n\t\t\t\tprint(\n\t\t\t\t\t\"\"\"\n\t\t\t\tFunction {} called by user {} has exceeded the"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef cache_(f):\n\t\t@functools.lru_cache(maxsize=maxsize, typed=typed)\n\t\tdef g(x):\n\t\t\trv = f(x)\n\t\t\t_,_,index = lfucache.index.get(f, None)\n\t\t\tif index is None or lfucache.index[f][1] < index:\n\t\t\t\tlfucache.add(f, rv, index=index)\n\t\t\t\treturn rv\n\t\t\telse:\n\t\t\t\treturn lfucache.replace(rv, rv +"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef decorator(f):\n\t\tif typed:\n\t\t\tf.__lfu_cache__ = dict()\n\t\texec(\"f.__lfu_count__ = 0 # cache entry count; only valid after object is bound to func\"\n\t\t\t  \" and has been used by the cache_module call.\" if typed else  \"\"\n\t\t\t\t  \"f.__lfu_cache__ = {}\"\n\t\t\t\t  \"# Cache Entry = (idx, f, func)\"\n\t\t\t\t  \"lfu_count={} for _ in range(*range(1024));\".format(maxsize"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef lfu_wrapper(f):\n\t\tdef lfu_recur(args, func, d, n):\n\t\t\tnonlocal maxsize\n\t\t\tif len(args) == 2 and isinstance(args[1], str) and args[1].startswith('lu'):\n\t\t\t\ti = func(args[0]) + (d % maxsize == 0)\n\t\t\t\tif i > n:\n\t\t\t\t\tfunc(args[0], d, n)\n\t\t\t\t\tf(args[0], i - n)\n\t\t\t\t\td += func(args[0], i, n)\n\t\t\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef decorator(fun, **kw):\n\t\tdef memoizer(x):\n\t\t\t# TODO: consider caching results\n\t\t\tres = None\n\t\t\ttry:\n\t\t\t\tres = fun(x, **kw)\n\t\t\texcept (IndexError, KeyError) as e:\n\t\t\t\tfor i in range(0, maxsize):\n\t\t\t\t\tx = e.__getstate__()[0];\n\t\t\t\t\tif x is None:\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\ttry:\n\t\t\t\t\t\tres = fun(x, **kw);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\texcept (IndexError, KeyError) as e:"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef decorator(fun):\n\t\t@wraps(fun)\n\t\tdef wrapper(*args,**kwargs):\n\t\t\timport time\n\t\t\ttimestamp = time.time()\n\t\t\tmemo = dict()\n\t\t\tmemo[args] = timestamp\n\t\t\targs = tuple(args)\n\t\t\tstamp = (timestamp - timestamp % 1000000) / 1000000\n\t\t\twith Memo(maxsize, stmt='func(%s)' % str(fun)):\n\t\t\t\tmemo = memo[args] = memo.get(args, 0) + stamp\n\n\t\t\tdef ret(*"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tresult = self.head\n\tself.head = self.head.next\n\treturn result\n\t\n\t\n\t'''\n\tRemove the first occurrence of the given key.\n\n\tUsage:\n\t\t\ta = LinkedList()\n\t\t\ta.add(1)\n\t\t\ta.add(2)\n\t\t\ta.popitem()\n\t\t\tprint a\n\t\t\tprint a.get(0)\n\t\t\tprint a\n\t\t\ta[0]\n\n\t\"\"\"\n\t\t\n\tdef pop(self, key):\t# returns the value or a `IndexError`\n\t\tif self.head"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\treturn self.__contents.pop()\n\n  ### Accessor ###\n  def __iter__(self):\t\n    \"\"\"Return an `Iterator` object for the tuple represented by this dictionary.\"\"\"\n    return iter(self.items())\n\n  def __contains__(self, key):\t\n    \"\"\"Check if the given key is a member of the dictionary.\"\"\"\n    return key in self.__contents\n\n  def __getitem__(self, key):\t\n    \"\"\"Return the given key, element, or `None`. Raise an error if\n    the given key isn"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\tself.popitem()\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\treturn (self.keys()[0], self.values()[0])\n\t\nclass DynamicMenuView(Tk.Frame):\n\tdef __init__(self, config, gui_master, default_key_values):\n\t\tsuper().__init__(orient = \"vertical\", bg = \"blue\", width = 200, height = 100)\n\t\tself.config = config\n\t\tself.gui_master = gui_master\n\t\tglobal default"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\treturn self.heap.popitem()\n\nclass MaxHeap(object):\n\t\"\"\"\n\tImplementation of a Max-Heap\n\n\t>>> from heapq import *\n\t>>> maxheap = MaxHeap([9,2,1,0.5,4,6,3,8,5,7])\n\t>>> for i in range(len(maxheap)):\n\t... \tprint('%d %.2f %d %f %f' %\n\t... \t\t(i, maxheap[i])\n\t... )\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\treturn super(dictionary, self).popitem()\n\texcept KeyError as e:\n\t\t# No data\n\t\traise KeyError('Dictionary is empty:'+ repr(e))\n\nclass reverse_dict(dict):\n\t\"\"\"\n\tMaintains reverse_dict[key] order invariant.\n\t\n\t>>> d = reverse_dict()\n\t>>> d['c'] = 4\n\t>>> d['a'] = 3\n\t>>> sorted(d.keys())\n\t['a', 'c', 'd'] \n\t\"\"\"\n\tdef"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttmp = list(self.items())\n\ttmp.sort(reverse=True)\n\tfor i in range(1, len(tmp)):\n\t\ttkey, tvalue = tmp[i]\n\t\tyield tkey, tvalue\n\n\nclass HashDB(MutableMapping):\n\t\"\"\"\n\tA mutable dictionary implemented via a hash map. Each entry is a tuple ``(key, value)``.\n\tIf the key already exists in the dictionary, the updated value is appended. If it doesn't\n\texists, the entry is inserted into the mapping.\n\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\treturn remove(self._data, self.first()[1])\t\t\n\n \n    def update(self,delta):\n\tfor key in delta.keys():\n     \tself._data[key] = delta[key]\n     \t\n\n    def _get_all_keys(self):\n\treturn list(set([key for (key, value) in self._data.items()]))"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tself.__table_name.insert(self.__index_of_key, self.__value)\n\tself.__index_of_key += 1\n\tif self.__index_of_key >= len(self.__table_name):\n\t\tself.__table_name.insert(0, [1, 0, 0])\n\treturn ((self.__table_name[self.__index_of_key - (self.__index_of_key % 2)]), self.__value) \n\n\t\n\nclass LRU_Cache(Dict):"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\treturn self.items.popitem()\n\nclass HashNodeItem(object):\n\t\"\"\"\n    Node.item\n    \n    Inherits from `dict`, a key must pass an `item` argument.\n    \n    Can be used as `a = HashNodeItem` to get a :term:`hash` index.\n\n    \"\"\"\n\n\tdef __init__(self, item, hash_function):\n\t\t\"\"\"\n\t\t@param item: item is a `tuple`, where the key is assumed to be\n\t\t  the index in `self.items`.\n\t\t@"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\treturn  self._dict.popitem    ( )\t         # This doesn't work for Python3\n\t\n\t\n\t\n    def replace(_=None, *args) :\t\n        \"\"\"\n        Replace a pair or a mapping with another pair or\n        mapping.\t\n        \"\"\"\n        _, new = _get_key(_set_key, args[0], args[1], _set_key_copy)\n        return self.__dict__ if new is _self else new\n    \n        \n    def copy( self ) :\n        \"\"\"\n        Returns"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn self.__getitem__(key,[default,default,default,default])\n\n  def copy(self):\n    \"\"\"D.copy() -> D  deeply copy the dict\"\"\"\n    newD = dict()\n    newD.update(self)\n    return newD\n\n  def items(self):\n    \"\"\"D.items() -> D.iteritems()    return a an iterator of (key,value) tuples.\"\"\"\n    keys = list(self.keys())\n    for key in keys:\n      yield key, self[key]\n\n  def values"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn D.setdefault(key,default)\n\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key not in self.keys():\n\t\tself[key] = default\n\treturn self[key]\n\n\t\n\t\n\t\nclass Test:\n\t\"\"\"\n\ttest\n\t\"\"\"\n\t\n\t\n\tdef debug(self, data):\n\t\t\"\"\"\n\t\tdebug method\n\t\t\"\"\"\n\t\tprint(data)\n\t\n\t\n\tdef test(self, data):\n\t\t\"\"\"\n\t\ttest method\n\t\t\"\"\"\n\t\tprint(data)\n\t\n\n# test function\n\ntry:\n    T = Test()\n    T.data = 'k1'\n"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn default\t\n\n  def update(self, E): # assumes that E is a dictionary-like object; don't use it in your solution, use D.update() instead.\n    for k, v in E.items():\n        self[k] = v\n\n\n\t\"\"\"\n\tfor key, value in E.items():\n\t\tD.setdefault(key, value)\t#D.setdefault(k,d)\n\t\"\"\"\t\n\n\n\tpass\n\n\n\n\n\n# End of file\n"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn dict.setdefault(self, key, default or ())\n\ndef bfs(G, s, t):\n\t\"\"\"Shortest path tree.\n\tReturns a list of edges,\n\tand a dictionary:\n\tD[(edge to vertex_1, vertex_2)] = distance\"\"\"\n\tvisited = {s : 0}\n\tpred_dist = {s : 0}\n\tedge_visited = {}\n\tg = defaultdict(list)\n\t\n\tdef dist(node, pred, v):\n\t\treturn dist[v] * pred[v]"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn default if isinstance(key, int) else dict.setdefault(key, default)\n\n# ______________________________________________________________\n\n\n# ______________________________________________________________\n\n# ______________________________________________________________\n# ___________________________________________________________________\ndef print_dictionary(d):\n    \"\"\"\n    Returns a dictionary printed out as the string 'key : value'.\n    \"\"\"\n    if len(d) == 0:\n        return \"Empty Dictionary\"\n    else:\n        return \"Key   \" + str(len(d)) + \"\\nValue \" + str(sorted(d))\n"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key in self: return self[key]\n\telse: self[key] = default\n    def __getitem__(self, k): return self.get(k, self.__missing__(k))\n    def __missing__(self, k): return self[k]\n    def setdefault(self, key, default=None):\t\"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n    def __init__(self, iterable=\"\", *args,"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif (key in self)and(key not in D):\n\t\tD[key]=default;\n\telse:\n\t\tD[key]=default;D[key]=default;return D[key]\n\t\t\n\n\treturn D[key]\n\n#\n# example of class\n#\n\nD = {}\na = {'a1': 2, 'a2': 2, 'c_b': 3}\na.setdefault('a_r', 3)\na.setdefault('a_r_2', 1)\n\nprint(a)\nprint(a."}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\ttry:\n\t\treturn D[key]\n\texcept KeyError:\n\t\tD[key] = default\n\t\treturn default\n\ndef readfile(file):\t# read a txt file\n\twith open(file, 'r') as f:\n\t\treturn f.read()\t\t\n\ndef getword(file,word):\n\twdd=\"\"\n\ttry:\n\t\twdd=str(word)\n\t\twdd=wdd.replace(\"'\", \"\")\n\t\twdd=wdd.replace(\" \",\"\")\n\t\twdd=wdd.lower()"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tretval = self.get(key,{})\n\tif not isinstance(retval, dict):\n\t\tretval = {}\n\tretval.update( {key : default if key not in retval else default} )\n\treturn retval\n\n\t\n\ndef main():\t\n\tif len(sys.argv)<2:\n\t\tsys.stderr.write(f'Usage: {sys.argv[0]} <program>  ')\n\t\treturn 1\n\t\n\tif sys.argv[1]=='--getenv' :\n\t\t"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t\treturn self._dict[key][1]\n\texcept KeyError:\n\t\tif default == None:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n# ----------------------------------------------------------\n#\t\tD[key] = (k, default)\n# ----------------------------------------------------------\nclass Node:\n\t\"\"\" A node in a linearized AST. \"\"\"\n\tdef __init__(self, key, val, left=None, right=None):\n\t\tself.key = key\n\t\tself.val = val\n\t\tself.left = left\n\t\tself.right = right\n\tdef"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t\treturn self.data[key]\n\texcept KeyError:\n\t\treturn default\n\nif sys.version_info[0] >= 3:\n\tb = bytes\n\tx = xrange = range\n\n##\n# This module is deprecated and has no backward compatibility. Use pyspark.sql\n# module instead to read, convert, write Spark dataframes to Parquet files.\n#\n# The original module was implemented by the Spark PySpark package. PySpark\n# does not have that module because it does not allow you to write it to a\n# file"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif key not in self:\n\t\tself[key]=default\n\treturn self[key]\n\t\n#===================================================================================\n# \n#===================================================================================\n\n\nif __name__ == '__main__':\n\tfrom Tkinter import *\n\troot = Tk()\n\n\te = Entry()\n\tprint 'default:', e.default()\n\tprint 'get(key, default):', e.get(key=0), e.get(key='-1')\n\n\tdef key(e):\n\t\treturn e.cget('validator')\n\n\tprint"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry: return self.__dict__[key]\n\texcept KeyError: return default\n\texcept TypeError:\n\t\ttry:  return d[key]\n\t\texcept KeyError: return default\n\ndef add(self, key, *args):\n\tfor an in [i for i in args if i]:\n\t\tif hasattr(self,key):\n\t\t\tself.__dict__[key] = getattr(self,key) + an\n\t\telse:\n\t\t\tself.__dict__[key] = an\n\ndef addBool(self, key, value):"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif key in self: return self[key]\n\tif default == None:\n\t\tif k:\n\t\t\traise TypeError('get() argument must be dict, not str')\n\t\telse:\n\t\t\treturn default\n\tif default == \"\":\n\t\traise TypeError(\"No value provided for argument %r\" % key)\n\treturn default\n\t# def __not_mutable__(self,key,default=None): return self.__setitem__(key,default)\n\t# def __getitem__(self,key): return dict.__getitem__(self,"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.__data.get(str(key), default)\n\t\n\t\n\t\n\t \t\n\t\t\nclass Memory(object):\n\t\"\"\"\n\tThis is a helper object that is instantiated by the constructor of\n\tthe Memory object, representing all of the memory contents.\n\tThe Memory object has the following attributes:\n\t\t\n\t\t\n\t\tmemory:\n\t\t\t\tA dictionary that maps address strings to values for each\n\t\t\n\t\t\t\n\tThe memory object also includes additional attributes for\n\tinspection. The following are the attributes:\n\t\t\n\t\t\n\t\t\n\t\"\"\"\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif not key or key in self:\n\t\treturn self[key]\n\telse:\t\t\n\t\treturn default\n\t\t#raise KeyError, \"must be unique\"\n\nc = Counter('abcde')\n#c.update([1,2])\nc.update({2,3,4})\nprint c.most_common(3)\n\n#c.update( {2,4,5,6,7,8}:)\t\n    #c.update([(1,2), (2,3), (3,4), (4,5), (5,"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t\tret = self.maps[key]\n\t\treturn ret[0]\n\texcept KeyError:\n\t\tif default is not None:\n\t\t\treturn default\n\t\traise KeyError(key)\n\n\tdef __getslice__(self, i, j):\n\t\ttry:\n\t\t\treturn self.maps[i:j]\n\t\texcept KeyError:\n\t\t\treturn tuple(self.maps[i:j])\n\n\tdef __getslice__(self, i, j):\n\t\treturn self.__getslice__(slice(i, j), slice("}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t    return self.__dict__[key].__get__(self)\n\texcept KeyError:\n\t    return default\n\n@dataclass(frozen=True)\nclass ConfigParser:\n    __slots__ = ['config_dir', 'config_path', 'config_type']\n    _config_path: str = Path(__file__).with_suffix('.config.toml')\n    _config_dir: str = Path.home() / '.config'\n    config_type = 'toml'\n    def __init"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t\treturn self.D[key]\n\texcept KeyError:\n\t\treturn default\n\t\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\nif __name__ == '__main__':\n\tpass\n\t\n\t\n\t\n\t\n\t\n\t"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tmethod_name = 'cachedmethod%s' % key\n\tif method_name not in __CACHE_UNCACHED_METHODS:\n\t\tdef _cache(instance):\n\t\t\tcache_instance = cache\n\t\t\ttry:\n\t\t\t\tcache_instance = __CACHE_UNCACHED_METHODS[key](instance)\n\t\t\texcept AttributeError:\n\t\t\t\tdef __cache_instance(inst):\n\t\t\t\t\tcache_instance = cache\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcache_instance = __CACHE_UNCACHED_METHODS[key](inst)\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef memoizedmethod(fn):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tfuncargs = kwargs.pop('funcargs', ())._as_memoized()\n\t\t\twith funcargs['cache'].setdefault(key, []).append(fn):\n\t\t\t\tcache[key] = fn(*args, **kwargs)\n\t\t\treturn cache[key]\n\n\t\twrapped.cache = cache\n\t\twrapped.funcargs = funcargs\n\t\treturn wrapped\n\treturn memoizedmethod\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef do_decorator(wrapped_class):\n\t\tcache = cache if cache is not None else HashCache()\n\t\tf = cache.get(key, lock)\n\t\tif f: return f\n\t\t# Create a decorator that memoizes the result and calls it on the wrapped\n\t\t# function with the key used in the decorator (for cache-specific rules)\n\t\t@wraps(wrapped_class)\n\t\tdef memoizedcache(*args):\n\t\t\t# NOTE: We need to have cache for class and instance methods, see:\n\t\t\t# https://www.sql"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t@wraps(obj.__dict__.get)\t# Wrap all method __dict__ with memoization calls\n\t@cache(cache)\n\tdef method(obj):\t\t# Pass the instance to the memoizer method.\n\t\tif not hasattr(obj, 'cache'):\n\t\t\tobj.cache = {}\n\t\ttry: return obj.cache[cache][key]\t\t# Return the cached value.\n\t\texcept KeyError: pass\t\t\t\t\t\t\t# Ignore the KeyError exception.\n\t\tv = obj.__dict__.get(key)\t\t\t# Get value for the key requested.\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tclass MemoizedMethod(object):\n\t\tdef __init__(self, func):\n\t\t\tself.cache_name = None \t\t# the unique cache name associated with an instance and a function\n\t\t\tself.func = func \t\t\t# (e.g., get_client_ip)\n\t\t\tself.lock = None\t\t\t\t# (e.g., request.lock)\t\t\n\n\t\tdef __get__(self, obj, type_):\n\t\t\tif obj is None:\n\t\t\t\treturn self.func\n\t\t\t# the (possibly) cached name of the function, but if it's been changed"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif not isinstance(cache, dict):\n\t\tif isinstance(cache, int):\n\t\t\tdef cachedmethod(self, *args, **kwargs):\t\n\t\t\t\t# Call superclass to find cached attribute\n\t\t\t\treturn getattr(self, key)(*args, **kwargs)\n\t\telse:\n\t\t\tcachedmethod = methodcaller(key, *args, **kwargs)\n\telse:\n\t\tcachedmethod = cachedmethod\n\tdef cachedmethodstatic(instance, *args, **kwargs):\t\n\t\tif lock:\n\t\t\tlock.acquire()\n\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif cache is None:\n\t\traise ValueError('cachedmethod needs a pointer to a cached class instance')\n\tcache, lock = check_key(key, cache, lock)\n\tdef wrapped_method(self, *args, **kwargs):\n\t\tkey = (hashkey(self), args, tuple(sorted(kwargs.items())))\n\t\ttry:\n\t\t\tresult = cache[key]\n\t\texcept KeyError:\n\t\t\tresult = wrapped_method(self, *args, **kwargs)\n\t\t\tcache[key] = result\n\t\treturn result\n\treturn"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t# Make a copy of the instance, so we can safely modify it while we're\n\t# processing the original arg-value\n\tinstance = key(weakref.proxy(instance), {key: [key for key in dir(instance)\n\t\tif not key.startswith('_') and key not in ('__init__', '__repr__')\n\t\tand callable(getattr(instance, key))]})(\n\t\tinstance)\n\tif lock is not None:\tcache['locker'] = (\n\t\t_thread_global_lock, _thread_lock)\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef cachedmethod(self, obj):\t# cache the signature(cls, self, obj)\n\t\tcachekey = hashkey(obj)\n\t\thash = cache._cache.get(cachekey, None)\n\t\tif cache := hash is None:\t# no cache\n\t\t\tif cache and len(cache) == 2:\t# check that self / obj are the same\n\t\t\t\tcache[0] = self.get_object(obj)\n\t\t\t\thash = cache[1]\t# store in the cache the second-party\n\t\t\telse:\t# no cache: put on a set\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tcache_ = {}\n\t@wraps(f)\n\tdef wrapper(*args, **kwargs):\t\n\t\tkey2 = cache_[key] = (f.__name__, args, kwargs)\t\t\n\t\treturn f(*args, **kwargs)\n\twrapper.cache = cache_\n\tif lock:\n\t\twrapper.lock = lock\n\treturn wrapper\n\n#------------------\t\n\n@cachedmethod(lock=True)\t\t\t# use this for debugging only\ndef fbcalc(a, b, m, func):   # this must be"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\traise e\n\treturn str(e)"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\toutstr = \"\"\n\toutstr += colored(str(type(e)), attrs=['green','bold','underline'], fg='white', bg='yellow')\n\toutstr += \"{}{}\".format(outcol(5), e) # Add max level on the top and at the bottom.\n\toutstr += extolist(5, max_level=max_level)\n\toutstr += extolist(5, max_level=30)\n\toutstr += extolist(5, max_level=max_level, addsubs="}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ttry:\n\t\treturn repr(e)\n\texcept:\n\t\tif not cls.stacklevel.get(\"showtraceback\"):\n\t\t\terror(f\"cannot format exception: '{e}'\")\n\t\ttraceback.print_exc()\n\ndef extract_stack(frame, limit=None):\n\t\"\"\"\n\tExtract a call stack for the error object. The stack depth will be\n\tindicated by \"maximum_line_no\" and the \"maximum_file_line\" of the\n\tframe object, respectively.\n\t:param frame: Frame object containing"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t# Get the last error\n\tt = e\n\tif str(t).find('BaseException has not been ') > -1:\n\t\ts = str(t)[str(t).find('BaseException has not ') + 5:]\n\t\tr = re.findall(r'\"([^\"]*)\",\"([^\"]*)\",\"([^\"]*)\"', s)\n\n\telse:\n\t\tr = []\n\t\tfor i in e.__traceback__:\n\t\t\tif i[0].find(\"self\") == 0:\n\t\t\t\ti[0] = i[0"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tret = \"\"\n\ttry:\n\t\tfor line in traceback.format_exception(type(e), e, e.__traceback__): #print the stacktrace (from e._traceback.        \n\t\t\tif \"ErrorType\" not in line:\n\t\t\t\tret += f\" \\n {line}\"\n\texcept Exception: #catch uncaught errors\n\t\tpass\n\t\t#raise Exception(str(e))\n\treturn ret\n\t\t\n\t\n# A helper function for printing stacktrace's info    \n#  (e._format_tb)\ndef _format_t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ttry:\n\t\tif _DEBUG: print(\"[LOG]:[ERROR] [extostr] @extostr: [{}]\".format(file), end=\"\\n\\n\")\t\t\n\texcept: print(\"[LOG]:[ERROR] [extostr] [ERROR]\", end=\"\\n\\n\")\t\t\t\t\t\n\t\n\t\n\tex = e\n\tm = None\n\t_is_list = False\n\t_path = [ ]\n\t_pathid = ''\n\t_str = None\n\t_path_error_msg = []\n\t\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ttry:\n\t\te.msg = \"from \" + e.filename + str(e.lineno) + \":\" + e.message + \"\\n\"\n\t\te.srcline = sys.exc_info()[2].tb_frame.f_code.co_firstlineno\n\t\te.srcfile =  sys.exc_info()[2].tb_frame.f_code.co_filename\n\tfinally:\n\t\tprint (e.msg)\n\treturn e\n\ndef write_to_file(file, dic):\n\t\"\"\"\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\treturn cls.repr(e, max_level, max_path_level)\n\n\ndef traceback(e):\n\t\"\"\"\n\tPrint details about specific exception.\n\t\"\"\"\n\tprint(\"\\n### Traceback ###\")\n\tprint(\"[traceback] Exception occured:\", extostr(e))\n\n\ndef getException(e):\n\t\"\"\"\n\tGet the exception object.\n\t\"\"\"\n\treturn e\n\ndef issubclass(klass,konecs):\n\t\"\"\"\n\tCheck if an object is subclass of another (klass,k"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tfrom traceback import format_exception_only\n\tfrom sys import exc_info\n\n\ttbl = list()\n\t# This dict maps str to int.\n\tstack_info = dict()\n\n\tdef dump(e, n=20, tbl=tbl):\n\t\ttbl.append([format_exception_only(e), \"\"])\n\t\ttry: tbl[tbl[n][0]] += [format_exception_only(e)]\n\t\texcept TypeError: tbl[tbl[n][0]] = [format_exception_only"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t#print('extostr',extostr(e, max_level, max_path_level))\t\n\tdef getrepr():\n\t\tline = str(sys.exc_info()[1])\n\t\t#if getattr(e, '__traceback__', None):\n\t\t#\treturn \"\".join(traceback.format_tb(sys.exc_info()[2][1]))\n\t\t#else:\n\t\t#\treturn \"[\"+line.strip(\".\").strip(\"\\n\")+\"]\"\n\t\t#return line.replace(\"\\"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\ttry:\n\t\tif text_buffer is None:\n\t\t\traise ValueError\n\t\tif encoding is None:\n\t\t\traise Exception('No encoding provided.')\n\t\twith open(file_name, 'w', encoding=encoding) as file_obj:\n\t\t\tfile_obj.write(text_buffer)\n\t\t\tif not overwrite:\n\t\t\t\treturn len(text_buffer)\n\t\t\telif file_name == 'text_buffer.txt':\n\t\t\t\treturn os.path.getsize(file_name)\n\t\traise IOError('failed to write to file.')\n\texcept ("}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\twith open(file_name, 'rb') as buffer:\n\t\tif encoding == 'ansi':\n\t\t\tif is_ansi_ansi(buffer.read(4)):\n\t\t\t\treturn _format_ansi_text_buffer(buffer, text_buffer)\n\t\t\telif is_ansi_ansi_binary(buffer.read(4)):\n\t\t\t\tansi_type = _format_ansi_text_buffer(buffer, bytes(text_buffer))  # type: bytes\n\t\t\t\treturn ansi_type.encode('utf-8') + bytes("}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t# Open the file in write mode.\n\ttry:\n\t\tfile = open(file_name, \"wb\")\n\t\tfile.write(text_buffer.encode(encoding))\n\t\t# Flush the buffer now, it shouldn't be in memory until it's written.\n\t\tfile.flush()\n\t\tfile.close()\n\t\t# Write success and return the number of bytes written.\n\t\tif overwrite:\n\t\t\treturn len(text_buffer)\n\t\treturn 0\n\t# If anything goes wrong, open the file in read mode.\n\texcept Exception as ex:\n\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not file_name:\n\t\treturn 0\n\tif overwrite or not os.path.isfile(file_name):\n\t\ttry:\n\t\t\tfile_handle = open(file_name, 'a', encoding=encoding)\n\t\t\tfile_handle.write(text_buffer)\n\t\t\tfile_handle.close()\n\t\t\treturn 0\n\t\texcept Exception as e:\n\t\t\tprint(e)\n\telse:\n\t\ttry:\n\t\t\tfile_handle = open(file_name, 'w', encoding=encoding)\n\t\t\tfile_handle.write(text_buffer)"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tbuffer_length = len(text_buffer) if isinstance(text_buffer, type(0)) else max(buffer_length, 0)\n\tif ((buffer_length <= 0) and (encoding!= 'utf-8')):\n\t\traise ValueError('Encoding should be one of '\n\t\t\t\t'[\"utf-8\",\"iso-8859-1\"]')\n\tif (file_name.endswith('.pdf')):\n\t\tfile_name = file_name.replace('.pdf','')\n\tdestination_file_object = open(file_name"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not os.path.isfile(file_name):\n\t\twith open(file_name, 'w', encoding=encoding, newline='') as file_obj:\n\t\t\tfile_obj.write(text_buffer)\n\telse:\n\t\tif text_buffer:\n\t\t\ttry:\n\t\t\t\tos.rename(file_name, file_name + '/renamed_' + str(os.path.basename(file_name)) + '.txt')\n\t\t\t\twith open(file_name + '/renamed_' + os.path.basename(file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\twith open(file_name, \"ab\" if overwrite else \"wb\") as f:\n\n\t\tbuffer_size = len(text_buffer) \t\n\t\tnum_written = 0\n\t\ttry:\n\t\t\tf.write(text_buffer) # write text to file\n\t\texcept Exception as e:\t\t\t\t\t\t\t\t\t\t\t \n\t\t\tprint(e)\t\t\t\t\t\t\t\t\t\t\t\n\treturn num_written\n\ndef read_file(file):\n\t\"\"\"\n\t:param file:  File to retrieve data from.\n\t:type file: str\n\t:return text_buffer: \t\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\ttry:\n\t\tif os.path.isfile(file_name):\n\t\t\tif overwrite==False:\n\t\t\t\traise ValueError(f'{file_name} already exists/already exists')\n\t\t\tos.remove(file_name)\n\t\tfile = open(file_name,'wb')\n\t\tfile.write(text_buffer)\n\t\tfile.close()\n\texcept OSError as os_error:\n\t\tif os_error.errno == 13:\n\t\t\traise IOError('IOError')\n\t\t\tfile\n\t\t\t\twith open(f'{file_name"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\ttry:\n\t\tif file_name!= '' and (not overwrite or not exists(file_name)):\n\t\t\twith open(file_name, mode=\"wb\") as handle:\n\t\t\t\treturn handle.write(text_buffer)\n\t\telse:\n\t\t\treturn write(text_buffer, encoding, overwrite)\n\texcept:\n\t\tfailure = \"Error in append_text_to_file: \"\n\t\tfile_name = get_file_name(file_name)\n\t\tlogging.error(failure + file_name)\n\t\traise ValueError\n\ndef read_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tlogging.debug(text_buffer)\n\ttry:\n\t\tif not overwrite:\n\t\t\tf = open(file_name, 'w', encoding=encoding)\n\t\telse:\n\t\t\tf = open(file_name, 'w', encoding=encoding)\n\texcept IOError:\n\t\tlogging.error('can\\'t open file %s ', file_name)\n\t\treturn 0\n\t\n\tif f.write(text_buffer)!= len(text_buffer):\n\t\tlogging.error('Could not write data to file (%s) because there was a"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n    with open(file_name, 'rb') as filepath:\n\t\tfile = filepath.read()\n\t\t\n    return file.decode('utf-8', 'backslashreplace') # This will convert UTF-8 to ascii.\n\ndef get_file_bytes(file, data_type):\n\t\"\"\"\n\tGet a file bytes.\n\n\t:param file: The name of the file to get, or the path or file object to open.\n\t:type file: str\n\t:param data_type: The data type of the file, in"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\treturn open(file_name, 'rt', encoding=encoding).read()\n\ndef texttofile_from_textbuffer(textbuffer, file_name, encoding):\t\"\"\"\n\tConvert a text buffer to an array of bytes, in a specified encoding.\n\t:param textbuffer: Text buffer to convert.\n\t:type textbuffer: str\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to convert to.\n\t:type encoding: str\n\t:return: A file object to read text"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\tif not encoding:\n\t\t# default encoding is UTF-8\n\t\tencoding = 'UTF-8'\n   \n\ttry:\n\t\tresult = open(file_name, 'rt',  encoding=encoding)\n\t\t#result = open(file_name, 'r',  encoding=None, errors='ignore')\n\t\t\n\t\t# read file in binary mode, not text mode\n\t\tdata = result.read()\n\texcept Exception as e:\n\t\treturn None\t\n\t\t\n\treturn data\n\t\t\n\n#-----------------------------------------------------------------------------------\n# Function: is"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\tf = open(file_name, \"r\", encoding=encoding)\n\texcept:\n\t\tf = None\n\t\te = sys.exc_info()[0]\n\t\tprint(\"File '\"+file_name+\"' was not found. \")\n\t\tprint(e)\n\t\tprint(\"Program terminated\")\t\n\t\tsys.exit()\n\tfile_length = 0\n\ttry:\n\t\twith f as file:\n\t\t\tfor line in file:\n\t\t\t\tfile_length += len(line)\n\tfinally:\n\t\tf.close()\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t# Open an IO stream and wrap it in the BOM tokenizer?\n\tf = StringIO('')  # TODO: Check to see if it works.\n\tfor line in file( file_name, 'r' ) :\n\t\tline = line.strip()\n\t\tif line :\n\t\t\tf.write(line.encode(encoding)+b'\\r\\n')\n\n\treturn f.getvalue()[:-1] \n\t# return file_to_textbuffer(file_name, encoding)\n\ndef textbuffer_to_file(textbuffer"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tif encoding == \"UTF-16\":\n\t\tfile = open(file_name, \"rb\")\n\telse:\n\t\tfile = open(file_name, \"rb\")\n\ttry:\n\t\ttext = file.read()\n\texcept:\n\t\treturn None\n\telse:\n\t\tfile.close()\n\t\treturn text\n\ndef textbuffer_to_file(textbuffer, file_name, encoding, is_json=False):\n\t\"\"\"\n\tWrite a text buffer TO a file, using the specify encoding and decoding method.\n:param textbuffer: Text buffer we want"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as f:\n\t\t\tdata = f.read()\n\t\t\treturn data\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\t\ndef read_char_file_from_file(file_name, byte_num):\n\t\"\"\"\n\tRead one character from file.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param byte_num: Number of byte to be read and return.\n\t:type byte_num"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\terror_fh = io.StringIO()\n\tfile = open(file_name, \"r\", encoding=encoding)\n\n\ttry:\n\t\tfor line in file:\n\t\t\terror_fh.write(line.encode(encoding))\n\texcept UnicodeDecodeError as e:\n\t\tif e.object.start > 0:\n\t\t\tline_error_start = e.object.start - 1\n\t\t\t# Check if message contains the expected error\n\t\t\tif e.reason == \"strict_map\":\n\t\t\t\tline_error_end = e.object.end"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\tif not file_exists(file_name):\n\t\t\traise (Exception (\"Input file does not exist.\"))\n\t\tdata = open(file_name, 'r', encoding).read()\n\t\tbuffer = StringIO(data.decode(\"utf-8\"))\n\t\treturn buffer\n\texcept Error, e:\n  \t\ttry:\n\t\t\traise (Exception(\n\t\t\t\t\"ERROR: {}\".format(e.args)))\n\t\t\traise\n\texcept IOError, e: # pragma: no cover\n\t\tif e.args.errno == 2:\t\t# EN"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\twith open(file_name, mode='rb', encoding=encoding) as text:\n\t\t\treturn text.read().decode()\n\texcept UnicodeDecodeError as e:\n\t\t# file is corrupt or badly encoded\n\t\tif e.reason == \"surrogateescape\":\n\t\t\traise AttributeError(\"File name does not contains surrogate escape characters.\")\n\t\t# file is malformed\n\t\tif e.reason in ['replace']: # or e.reason in ['backslashreplace']:\n\t\t\traise AttributeError(\"Invalid or corrupted file name: \" + e"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\ttry:\n\t\tos.path.exists(\"file_name.txt\")\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True\n\nif is_file_exist(\"file_name\"):\n\tpass\nelse:\n\tprint(\"Please input a valid file name.\")\n\tsys.exit()\n\n# Reading\nf = open(\"file_name.txt\", \"r\")\n\n# Searching\ntext = f.read()\nprint(text)\nif '@' in text:\n\tf.close()\n\tpass\nelse:\n\tprint"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tfile_name = file_name.strip()\n\n\treturn not (file_name in Path(sys.path[0]))\n\ndef get_file_path(filename):\t\"\"\"\n\tGet filepath.\n\t:param filename: file name. (filename must not begin with '.' or './')\n\t:type filename: str\n\t:return: return filepath.\n\t:rtype str\n\t\"\"\"\n\treturn Path(sys.path[0] + '/' + filename).resolve()\n\n\ndef save(data):\t\"\"\"\n\tSave to file."}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn os.path.exists(file_name) and os.path.isfile(file_name)\n\n\nif __name__ == \"__main__\":\n\tfile_data = \"../text_generation/datasets/charset_10k/train/train.conll\"\n\tis_file_exist(file_data)\n\n"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\ttry:\n\t    f = open(file_name, 'r')\n\t    f.close()\n\texcept: # file not exists\n\t    print('File does not exist: \"' + file_name + '\"')\n\t    return False\n\treturn True\t\t\ndef read_file_data():\n\t\"\"\"\n\tRead current data file into a dictionary.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Returns a dictionary, where key is the name of the data and its corresponding value is value\n\t:r"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tfile_list = []\n\tfile_name = file_name.strip().split('/')\n\tfor path in file_name:\n\t\tif path not in file_list:\n\t\t\tfile_list.append(path)\n\tfor file in file_list:\n\t\tif os.path.exists(f'./{file}'):\n\t\t\tpass\n\t\telse:\n\t\t\treturn False\n\treturn True\n\ndef is_valid_parameter(param, value):\n\tif param == 'log4jParam':\n\t\tval = value\n\t\tparams = dict("}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\ttry: \n\t\t# file exists\n\t\tif os.path.isfile(file_name):\n\t\t\treturn True\n\t\telif os.path.isfile(os.path.join(os.getcwd(), file_name)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception as e:\n\t\tsys.error(e)\n\t\treturn False\n\t\t\n\t\n\ndef get_folder_list(base_dir, return_dirs=False, maxdepth=20):\n\tsys.stderr.write('\\nsending:"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tfile_exists = False\n\ttry:\n\t\tos.stat(file_name)\n\t\tfile_exists = True\n\n\texcept AttributeError:\n\t\tfile_exists = False\n\n\treturn file_exists\n\ndef get_files_name_list(files_path):\n\t\"\"\"\n\tGet files names list from source directory path.\n\t:param files_path: Absolute path\n\t:type files_path: str\n\t:return: List of files names\n\t:rtype: [str]\n\t\"\"\"\n\tfile_name_list = os"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif os.path.exists(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\t\ndef is_file_writeable(file_name):\n\t\"\"\"\n\tCheck File Existence (read)\n\t:param file_name: File name.\n\t:type file_name: str\n\t :return: Return true (File exists/readable), false (File not exists/writeable)\n\t:rtype bool\n\t\"\"\"\n\tif os.access(file_name, os.R_OK):\n\t\treturn True\n\telse:"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn os.path.exists(file_name)\n\t\n\"\"\"\nGet a data source.\n\n:return: data source info.\n:rtype data_source info\n\"\"\"   \ndef get_data_source(f):\n\t\"\"\"\n\tGet a data source.  This is a convenience function.\n\t\n\tParameters\n\t----------\n\t\t* f: File/path name to be loaded.\t\n\t\n\tReturns\n\t-------\n\t\t* data_source \n\t\"\"\"\t\n\tif os.path.isfile(f):\t\t\n\t\treturn {\"data"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn os.path.exists(file_name)\n\n\ndef file_delete(file_name):\n\t\"\"\"\n\tRemove file. \n\n:param file_name: File name.\n:type file_name: str\n:note : If file does not exist, it always returns True.\n:return: Return true (success), False (failure), or None (error)\n:rtype: Union[bool,None]\n\"\"\"\n\n\treturn os.unlink(file_name)\n\n\ndef file_read(file_name):\n\t\"\"\"\n\tGet string"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\n\tconfig.LOG_LEVEL = logging.CRITICAL + 10\n\tif hasattr(logging, 'FATAL'):\n\t\tlogging.FATAL = logging.CRITICAL\n\t\tconfig.LOG_FATAL = True\n\t\tconfig.LOG_LEVEL = logging.CRITICAL - 5\n\t\t\n\tif hasattr(logging, 'ERROR'):\n\t\tlogging.ERROR = logging.CRITICAL\n\t\tconfig.LOG_FATAL = False\n\t\tconfig.LOG_LEVEL = logging.INFO - 5\n\t\t"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\ttry:\t\n\t\t# Remove Logging\n\t\tfor (key,value) in cls._Logging.items():\t\n\t\t\tif value:\t\n\t\t\t\tif (key.startswith('__') and not key.endswith('__')):\t\n\t\t\t\t\tvalue.remove()\t\n\t\t\t\telse:\t\n\t\t\t\t\tdelattr(cls,key)\t\n\texcept Exception as e:\t\n\t\tprint(f'Exception: {key}')\t\n\t\tprint(f'Exception: {e}')\t\n\t\tpass\t\n\treturn\n\ndef _open_log"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tlogger = logging.getLogger()\n\tlogger.setLevel(logging.CRITICAL)\n\thandler = logging.StreamHandler(sys.stdout)\n\tformatter = logging.Formatter('\\n%(asctime)s | %(levelname)-8s  | %(message)s')\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\treturn logger\n\n\t\ndef _setup_logging(cls):\n\t\"\"\"\n\tsetup logging for python to log to console\n\t\"\"\"\n"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tlogging.basicConfig(format='%(message)s')\n\tlogging.captureWarnings(True)\n\tlogging.installHandler()\n\ndef _clear_logging(cls):\t\"\"\"\n\tClean up the log file\n\t\"\"\"\n\tlogging.shutdown()\n\tlogging.basicConfig(format='%(message)s', filemode='a' )\n\tlogging.captureWarnings(True)"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tlogging.basicConfig(filename='main.log',\n                        format='%(asctime)s - %(levelname)s - %(message)s',\n                        datefmt='%Y-%m-%d %H:%M:%S',\n                        level=logging.INFO,\n                        filemode='a')\n\nclass Server(threading.Thread):\n    \"\"\"\n    A server thread for receiving messages from clients.\n    \"\"\"\n    def __init__(self, host, port, username):\n        super().__init__()\n        "}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls._logging = LOGGER\n\tlogging.basicConfig(filename=\"main.log\", format='{}  {}'.format(date_time_and_time(), '{:.0f}'.format(time.perf_counter())))\n\ndef _setup_client(server='', ssl_cert_path='cert.crt', ssl_key_path='key.pem', ssl_ca_path='ca.pem', local_path='/path', auth_mode='basic', auth_username='username', auth_password='"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls.__log_file = \"\"\n\n\n\t#####\n\t#\n\t#\n\t#####\n\t\n\t\n\t#####\n\t#\n\t#\n\t#####\n\n\t\n\t#####\n\t#\n\t#\n\t#####\n\n\n\t\n\t#####\n\t#\n\t#\n\t#####\n\n\n\t\n\t#######\n\t#\n\t#\n\t#######\n\n\n\t\n\n  \n\n"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tfile_handler = logging.FileHandler(cls.LOG_FILE_NAME)\n\tfile_handler.setFormatter(cls.LOG_FORMATTER)\n\tfile_handler.setLevel(cls.LOG_LEVEL)\n\n\tlogger = logging.getLogger(cls.LOG_NAME)\n\tlogger.setLevel(cls.LOG_LEVEL)\n\tlogger.addHandler(file_handler)\n\n\treturn logger\n\nclass _Database(object):\n\n\tdef __init__(self, db=None, host=\"localhost"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tif cls.logging_type is LoggingType.BASIC.value: \n\t\tbasic_config = cls.logging_config\n\t\tbasic_config['name'].value = cls.name\n\t\tbasic_config['level'].value = cls.level\n\t\tbasic_config['format'].value = cls.format\n\n\nclass Color:\n\t\"\"\"Set the colors\"\"\"\n\t@classmethod\n\tdef red(cls):\n\t\tcls.COLORNUMBER_RED = \"#ffffff\"\n\t@classmethod\n\tdef te"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls._logger = get_logger('logger','main.log', 'python_execution_history_tool')\n\tcls._loglevel = get_loglevel(cls._loglevel)\n##\n#--------------------------------------------------------------------------------------\n#\t\tConstructor\n#--------------------------------------------------------------------------------------\n#\n\ndef _constructor(args):\n\t\"\"\"\n\tReturn a Process object with the specified command line parameters.\n\t\"\"\"\n\tglobal _instance, _thread\n\tif _thread == None:\n\t\tcreate_process(_command_line, *args)\n"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn _xmlschema_builtin.SchemaNodeItem\n\tdef _getSupportedFormats(self):\n\t\t\"\"\"\n\t\t_getSupportedFormats()\n\n\t\tThis method has been extended to support the 'list' output\n\t\ttype.\t\"\"\"\n\t\treturn [_xmlschema_builtin.xsdDateTime, _xmlschema_builtin.xsdNumeric]\t\t\n\nclass SchemaItemSubstitution(BaseItem, SchemaNodeItem):\n\t\"\"\"\n\tA substitution element holds the substitution values of the item with a\n\tsubstitution group name"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn PyUnittestClass\n\nif __name__ == \"__main__\":\n\t#import xmlrunner\n\timport unittest\n\tfrom testcase import testcase_generator\n\n\t# the package has access to the test suite, but it shouldn't be imported as\n\t# a test method.\n\tunittest.main_ = lambda *args: unittest.TestSuite()\n\n\toutf = open(\"testcase.html\", 'w')\n\trunner = xmlrunner.XMLTestRunner(\n\t\tstream=outf, descriptions=True, verbosity"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn self._target_class\n\nclass C_Babel(object):\n\t\"\"\"\n\tAbstract bundle class to be used as a base class.\n\t\"\"\"\n\n\t_class_attribute_key = None # this class attribute should be a string that belongs to the implementation\n\t_keywords_sorted = [ 'treat',\n\t\t\t\t\t\t'ignore',\n\t\t\t\t\t\t'type',\n\t\t\t\t\t\t]\n\n\tdef create_class_obj(self, klass):\n\t\traise NotImplementedError\n\n\tdef get_class_keywargs(self):\n\t\tkwc = {}\n\t\t"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn _FallbackSubclass\n\nclass MappingSubclass(dict):\n\t\"\"\"\n\tMappingSubclass(mapping, *, **kw)\n\n\tA Mapping subclass whose '__iter__' always returns an iterator.\n\t\"\"\"\n\tdef __init__(self, mapping, *, **kw):\n\t\tself._iter_fields = getIteritems(mapping)\n\t\tdict.__init__(self, **kw)\n\tdef __dict__(self):\n\t\treturn vars(self).copy()\n\tdef __setitem__(self, key"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn _Fallback\n\n\n# Register _Fallback in _PySpin:\n_PySpin._Fallback = _Fallback(globals(), '__Pyx_fallback')\n\n# Register an implementation:\n_PySpin.c_classdef('namespace', [\n    ('namespace', [\n        ('namespace_prefix', cbschema.WildcardType(type=cptype.Node(cbschema.WildcardTypeScope(type=cptype.Node(c_string), optional=True),\n         node_type=c"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn c_short\n\t\n\t\nclass _Py(object):\n\t\"\"\"\n\tImplement this to return a python class object.\n\t\"\"\"\n\t_instances = {}\n\tdef __iter__(self):\n\t\t# for some reason I think that I'm calling __iter__ \n\t\t# on a _Proxy instance.\n\t\treturn self.__iter__()\n\t\n\tdef new(self, target=\"_\"):\n\t\tkey = \"_\".join([_Py._makeTarget(target), str(id(self))])\n\t\ttry:\n\t\t\t# there"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\timplementedBy = getattr(self.__getTargetModule(),\n\t               self.__class__.__name__ + 'Implements')\n\treturn getattr(implementedBy, \"ImplementedBy\" + self.imports.get(\"\", \"\"))\n\n\n\nclass PyImplements(object):\n\tBuiltinImplements = _PyGetImplementation()\n\tImplements = BuiltinImplements.implementation\t\n\n\tclass __ImplementsBase:\n\t\timplementation = None\n\tclass Implements(Implements.__"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn SmartClass\n\n\t\n########################################################################\n##  FooBarDocument class\n########################################################################\n\nclass FooBarDocument(Document):\n\t\"\"\"\n\tThe FooBarDocument defines the 'Document' interface.\n\tThe following methods are over-written to give more functionality.\n\t\"\"\"\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tInit a FooBarDocument instance.\n\t\tUse the keyword argument 'base_url': the base relative URL, \n\t\t\teg. 'fooBar/index.html'.\n\t\t:param base_"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn PyQt4\n\n_isDEVICE = (hasattr(sys, \"pypy_device\") and\n\t\t\thasattr(sys, \"pypy_qemu_info\"));\n\nif _isDEVICE:\n\tdef _getQemuDeviceName(sDevice):\n\t\tif hasattr(sys, \"pypy_device\"):\n\t\t\tdev = sys.pypy_device;\n\t\telif hasattr(sys, \"pypy_qemu_info\"):\n\t\t\tdev = sys.pypy_qem"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn PyPyCPy\n\nif __name__ == \"__main__\":        # pragma: no cover\n\t# Simple test -- accept all parameters, no need to parse args.\n\tfor p in options.simpleopts:\n\t\tassert isinstance(p, collections.abc.Iterable)\n\tfor p in options.simpleopts:\n\t\tp = list(p)\n\t\tassert all(isinstance(i, tuple) for i in p)\n\n\tprint()\n\n\tprint(\"### Cython \")\n\n\tprint()\n\tprint(\"####### Py implementation\")"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tordered = []\n\tsuffix = ''\n\tfor x in orderings:\n\t\tif isinstance(x, tuple):\n\t\t\tname = _legacy_mergeCategories(x)\n\t\telse:\n\t\t\tname = x\n\t\tsuffix += '(' + name + ')'\n\t\tfor i in range(len(x)):\n\t\t\tif isinstance(x[i], string_types):\n\t\t\t\tif suffix: suffix = ',' + suffix\n\t\t\t\tsuffix += '(' + x[i] + ')'\n\t\t\telse:\n\t\t\t\tlist(x)\n\t\t\t\tlist"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\t# first make sure all orderings is a list\n\torderings = self.__orderingsFromList(orderings)\n\t\n\t# now make sure they're a list and only have their orderings\n\t# added together by a string operation.\n\tfor ordering in orderings:\n\t\tassert isinstance(ordering, list) and len(ordering) == len(orderings)\n\t\t\n\t\t# add each ordering (list) into a tuple\n\t\tfor i in range(len(ordering)):\n\t\t\tif not isinstance(ordering[i], list):\n\t\t\t\tordering["}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\tmergingOrderings = []\n\tfor ordering in orderings:\n\t\tsubOrdering = []\n\t\tfor orderingElement in ordering:\n\t\t\tif str(orderingElement) not in _orderedForMerge():\n\t\t\t\tsubOrdering.append(orderingElement)\n\t\tmergingOrderings.append(subOrdering)\n\treturn mergingOrderings\n\t\ndef _legacy_splitOrderings(orderings):\t\"\"\"\n\tSplits a single orderings into orderings contained within two\n\tother orderings, so that ordering with x is the new first.\n\nOrderings"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tif len(orderings) < 2:\n\t\treturn orderings\n\telif orderings[0][:3] == orderings[1][:3] and orderings[1][:4] in 'Zx':\t# p<=3\n\t\treturn orderings[1]\n\telse:\n\t\tif orderings[0][:3] >= orderings[1][:3] or orderings[0][:3] == orderings[1][:3]:   # p>=2\n\t\t\treturn orderings[0]\n\t\telse: # p>=2"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\treturn (\n\t\t_orderedSetString(orderings, lambda i: i[1]))\n\ndef _orderedSetString(items, key):\n\t\"\"\"\n\tRecursively sorts by key from left to right (and then by string\n\tversion of key, if present). All elements in items should have\n\tstrings in lists instead of lists of tuples.\n\t\"\"\"\n\t# Create a temporary map of key -> list.\n\t# This can be done recursively without the trick you would do\n\t# via \"__rebuildMap\" (see below).\n\n\tkeystring"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\torders = [] # list where all the objects are stored\n\tfor order in orderings:\n\t\tif order[0][0] in ('x', 'y', 'z'):\n\t\t\torders.append('|'.join(order[1:]))\n\t\telse:\n\t\t\torders.append(order)\n\treturn orders\n\ndef set_legacy_match(match):\n\t'set match in the old _libmp_match_type'\n\tif hasattr(match, '__iter__'):\n\t\tfor m in match:\n\t\t\tm.legacy_match ="}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tdef recurse(orderings, old, new):\n\t\t# Check if ordering is valid\n\t\tif len(orderings) > 1 and orderings[-1][0] not in old:\n\t\t\t# Uniquify orderings using prefixes\n\t\t\torderings[-1] = [old.index(i) * -1 for i in orderings[-1]]\n\t\t\t# Get recurse for each group\n\t\t\tfor group in old:\n\t\t\t\trecurse(orderings, group, new[group])\n\t\t\treturn\n\t\ti = 0\n\t\t# For each group that ends earlier"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tif orderings:\n\t\t# Get a list of the original ordering and all indices in that order.\n\t\tordering_list = []\n\t\tfor ordering in orderings:\n\t\t\tfor idx in xrange(len(ordering)):\t# [x, y, z, q, z]\n\t\t\t\tif '_' + str(ordering[idx]) not in ordering\\\n\t\t\t\t\t\tor ('_' + str(ordering[idx]) == 'q' and '_' + str(idx+1) in ordering\\\n\t\t\t\t\t\t\t\tor ordering[idx] == 'z'):\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\torderings = list(orderings)\n\tfor i in xrange(len(orderings) - 1):\n\t\tif orderings[i][0] not in orderings[i + 1]:\n\t\t\torderings[i] = [ orderings[i + 1][-1] ]\n\n\treturn orderings\n\n  \n  \n  \n  \n  \n  \ndef _clean_nested_list_string(lst):\n\treturn \" \".join(lst)\n        \ndef _legacy_cleanLineString(lst):\n\t\"Keeps track of the"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t#\n\t# The original orderings are already in fixed list orderings in the\n\t# mergeOrderings function above.\n\t#\n\t# To figure out each ordering in list, do\n\t#\n\t# for i in range(len(orderings)):\n\t# if\n\t\t# for j in range(len(orderings[i])):\n\t\t# if\n\t\t#\n\t#\n\t# and insert it in the appropriate order.\n\t#\n\t# The list is always sorted in a new order, so we can make\n\t# sure both are sorted.\n"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tdef directlyProvidedBy(a):\n\t\treturn directlyProvidedBy(a.__class__)\n\treturn directlyProvidedBy(object)\n\n# Added for debugging\n#def directlyProvidedBy(object):\n\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn object # __IGNORE_WARNING__\n\n\ndef directlyProvidedByAll(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by all interfaces.\n\t\"\"\"\n\treturn [\n\t\tobject # __IGNORE_WARNING__\n\t] # __IGNORE_WARNING__\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tpass\n\n\n\n# Class for constructing the class-specific metaclass\nclass InterfaceMaker(type):\n\t\"\"\"Class for creating the metaclass and calling its various magic methods.\n\t\n\tThe meta class is a class-level object which will be created when a\n\tclass is instantiated or when the class name is taken from the meta\n\tclass's __name__ attribute. This allows the given metadata (e.g.\n\t``metaclass=InterfaceMaker``) to be overridden by a new Meta class. There's\n\tmuch, however, more than it's true,"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tfrom zope.schema.interfaces import IDeclaration, IInterfaceDeclaration\n\tif _use_decl_interfaces(object):\n\t\treturn IDeclaration\n\telif _use_decl_interfaces(object): # pylint:disable=non-idiomatic-typecheck\n\t\treturn object\n\telse:\n\t\treturn InterfaceDeclaration\n\n\n@implementer(_Zope2InterfaceDeclaration)\ndef directlyProvidedByObject(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tfrom zope.interface import Interface\n\timport Zope2\n\n\t# pylint:disable=R0201\n\tif isinstance(object, InterfaceMeta):\n\t\treturn object\n\n\t# pylint:disable=R0201\n\tif isinstance(object, _directlyProvidedByMeta):\n\t\treturn object\n\t# pylint:disable=R0201\n\n\traise TypeError(\n\t\t'Expected a name from %s or the object passed is an instance '\n\t\t'of the InterfaceMeta or by_name decorator. Got %r.' % ("}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn directlyProvidedBy(object)\n\n## ###################################### Interface Class Interface ##############################################\n\nclass IInterfaceClass:\n\t\"\"\"A base interface class that provides one-to-one correspondence.\n\t\"\"\"\n\t__allow_setting_default_value = frozenset(('__set_defaults__',))\n\t__allow_replacing_default_value = frozenset(('__set_defaults__', '__set_defaults_methods__', '__set_defaults_attributes__'))\n\n\tdef __new__(cl"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn object.__directlyProvidedBy__()\n\nclass _ICountObjectManager(Interface):\n\t\"\"\"Interface definition that provides `ir.count` for the interface.\"\"\"\n\n\tdef count(object): # pylint:disable=redefined-builtin\n\t\t\"\"\"Return the count (number of objects) for this object.\n\t\tThe count is the maximum number of objects that exist associated\n\t\tto this object.\"\"\"\n\t\traise Forbidden('this interface is not of interest to me')\n\nclass _IDeclarationIteratorItem(object):\n\t\"\"\"Helper for the interface declarations"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn objectsToDeclarations(self._objectDeclarations.values(), classImplemented=\n\t\t\t\t\t\tobject)\n\ndef directlyProvidedBy(object, *args, **kw): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by 'object' (notably\n\t`object` itself)\n\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn objectsToDeclarations(\"\", classImplemented=object, *args, **kw)\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t# This would be a very important concept to understand, it's not\n\t# essential.\n\tdef directlyProvidedByMethod(object):\n\t\ttry:\n\t\t\t# pylint:disable=redefined-builtin\n\t\t\tinterfaces = iter(getattr(object, '__implements__'))\n\t\texcept AttributeError: # Not a class, not a class instance\n\t\t\treturn None\n\t\ttry:\n\t\t\treturn next(interfaces)\n\t\texcept StopIteration:\n\t\t\treturn None\n\tdirectlyProvidedBy = property(\n\t\t_directlyProvided"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tzopeTag(__name_=__name__, __author_=__author__, __email__=__email__,\n\t\t__doc__=__doc__, __license__=__license__, __url__=__url__)\n\nfrom.Z_Interfaces import * # pylint:disable=wildcard-import\nfrom.Z_Interface import * # pylint:disable=wildcard-import\nfrom.Z_Interface import IZopeInterface\n"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treturn sorted(classes, key=lambda x: classes.count(x))\n\n\ndef minimumBases(classes):\t\"\"\"\n\tCount the minimum unique base classes for each class\t\"\"\"\n\treturn sum(dict.fromkeys(classes, 0))\n\ndef minimumBasesCumulative(classes):\t\"\"\"\n\tReturn the cumulative amount of minimum bases for each class\t\"\"\"\n\t\t\n\t# Initialize result variable, zero if zero\n\tresult = zero\n\n\t# Initialize the total amount of all classes, zero if zero\n\ttotalAmount = zero\n\n\t# Iterate through all"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tdef isMin(classes, m):\n\t\treturn classes[0] == m #return boolean indicating the given minimal base classes are equal to itself\n\tminim = []\n\tfor elem in classes:\n\t\tif isMin(classes, elem):\n\t\t\tminim.append(elem)\n\treturn minim \n\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tdef b(lst):\n\t\tif len(lst) == 1:\n\t\t\treturn '.'\n\t\tm = []\n\t\tfor i in lst:\n\t\t\tif not i is None:\n\t\t\t\tx = i\n\t\t\t\ti = x.get_bases_classes()  # TODO Check the return type\n\t\t\t\tidx = [a for a, c in enumerate(i) if c is not None]\n\t\t\t\tfor a in idx:\n\t\t\t\t\tm.append(i[a])\n\t\treturn m\n\toutput = []\n\tfor c in classes:\n\t\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treturn [classes[i] for i in range(classes.__len__())]\t\t\ndef removeNestedClasses(classes,classesToRemove=None):\t\"\"\"\n\tRemove every class based on its nested class.\n\tEx.\tclass A(object):\n\t\ta = A()\n\t\tb = A()\n\tThe resulting object is a.a,b\n\t\"\"\"\n\t# First class\n\tif classesToRemove == None:\n\t\treturn classes\n\telif classesToRemove == []:\n\t\treturn classes\n\t\n\tclassesToRemove = minimalBases("}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t# TODO: Implement\n\t\n\tmin_bases_classes = {x: set() for x in classes}\n\tbases = set(classes)\n\tminimum_bases = 0\n\t\n\t# Create a list of the base classes ordered on the list of classes, with no duplicates\n\t#\n\tfor index, k in enumerate(classes):\n\t\tbases_l = list()\n\t\tfor v in bases:\n\t\t\tif k.issubclass(v):\n\t\t\t\tbases_l.append(v)\n\t\t\t\tif not (index+1)"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tdef rec_reduce(baseclasses, a, b, newclass):\n\t\t\"\"\"\n\t\tBase class a of class newclass in class b is considered to be one of the\n\t\tequivalent of bases of a class in baseclasses\n\t\t\n\t\tRecursive function used to reduce list of base classes into a minimum\n\t\tequivalent of those base classes\n\t\t\"\"\"\n\t\t#check for trivial cases\n\t\tif len(a) == 1 and baseclasses == []:\n\t\t\tif not newclass in newbaseclasses:\n\t\t\t\treturn baseclasses\n\t\t\telse:\n\t\t\t\treturn newclass\n\t\tif"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tdct = {}\n\tfor c in classes:\n\t\tdct[c] = 0\n\tclasses = [c for c in dct.keys()]\n\tclasses.sort()\n\treturn classes\n\n\n#==============================================================================\n# Generalized Base Class Hashing\n\ndef groupBaseClass(classes, includeZero=False):\n\t\"\"\"\n\tHashes class list by the order in which class is located, and includes zero counts\n\n\tParameters:\n\t\t\tclasses -> list of class.\n\t\t\t\t\tIf classes is a list or array, then it should be a list of such lists, "}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treturn sorted(classes, key=lambda base: base._baseprotocol)\n\n\t\nclass UUIDObject(object):\n\tdef __init__(self, uuid=None, raw=False):\n\t\tself.raw = False\n\t\tif raw:\n\t\t\tself.setRaw(uuid)\n\t\telse:\n\t\t\tself.setUUID(uuid)\n\n\tdef setRaw(self, raw):\n\t\tself.raw = raw\n\n\tdef setUUID(self, uuid):\n\t\tself.uuid = uuid\n\n\tdef isUUID(self"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tclasses = list(set(classes))\n\tclass_to_base = {}\n\tfor c in classes:\n\t\tclass_to_base[c] = list(set(classes) - set(classes[classes == c]))\n\t\tfor b in class_to_base[c]:\n\t\t\tclass_to_base[b].append(c)\n\tclasses = sorted(class_to_base.keys(), key=len)\n\treturn classes\n\t\n\t\ndef minDistance(string1, string2):\n\t\"\"\"\n\tCompute the minimal distances between two strings"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tif classes == []:\n\t\traise Exception(\"This function doesn't know all of your classes\")\n\telse:\n\t\tclasses = sorted(classes)\n\t\tfor clas in classes:\n\t\t\tif not clas in classlist:\n\t\t\t\tclasses.remove(clas)\n\t\treturn classes\n\ndef sortClasses(classes, alphabet):\n\t\"\"\"\n\tSort a list of classes from lower to higher\t\"\"\"\n\tsortedClasses = []\n\tfor clazz in sorted(classes, reverse=True):\n\t\tif clazz in classlist:\n\t\t\tsortedClasses."}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tpass\n\nclass MethodDescriptor(object):\n\t\"\"\"\n\t\tWraps one or more method descriptors, each of which contains a method name,\n\t\tclass name and description.\n\t\"\"\"\n\t\n\tdef __init__(self, name, selfImpl, parentDesc, methodDescriptor=None)->None:\n\t\tself.name = name\n\t\tself.parentDesc = parentDesc\n\t\tself.methodDescriptor = methodDescriptor\n\t\t\n\tdef getMethod(self, docStringName):\n\t\t\"\"\"\n\t\t\tReturn the method described in this descriptor."}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn [name for name, attr in self._get_all_attributes(all)]\n\n\nclass AttributeAccessor(ABC, _AbstractAttribute):\n\t\"\"\"Abstract interface for an attribute accessor.\n\n\tAn attribute accessor is a descriptor that may access an AttributeInterface.\n\n\tFor any other interface type, the class that inherits AttributeAccessor should derive\tof this class.\n\t\"\"\"\t\n\n\t@property\n\t@abstractmethod\n\tdef access(self):\n\t\t\"\"\"Returns the name of the interface the attribute accessor resides within.\"\"\"\n\n\t@property"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\traise NotImplementedError('''<p>You need to extend this module in order to use the public interface in your model''')\n\nclass Model(object):\n\t\"\"\"A python model class.\t\"\"\"\n\n\t#\n\t# __init__\n\t#\n\n\t#\n\t# Class Variables\n\t#\n\n\t## public class parameters used in this class\n\t##\n\tpNamesAndDescriptions(all=False) = {}\n\n\t## public class methods\n\t##\n\t# _attributes_\n\t#\n\t# _attributes_methods\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t_interface.NamesAndDescriptions.call(self)\n\treturn self.__namesAndDescriptions\n\n\tdef __init__(self):\n\t\t_interface.Interface.__init__(self)\n\n_interface = Interfaces([], [Interface()])\n\n# ===============================================================================================\n\ndef buildProtocol(transport, protocol factory):\n\n\tp = protocolFactory()\n\tp.setProtocol(transport)\n\ttransport.protocol = p\n\treturn p\n\n# ===============================================================================================\n\nclass Interface(BaseInterface):\n\n\ttype"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tpass\n\n\nclass IFormField (Interface):\n\n\tdef __init__(self): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tIFormField.\n\t\tInitializes a form field.\n\n\t\t\"\"\"\n\t\tpass\n\n\tdef getInputFrom(self): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tA form field gets its value from user.\n\n\t\t\"\"\"\n\t\tpass\n\n\tdef getValueFrom(self): # pylint:disable=redefined-builtin,useless-suppression\n\t\t\"\"\"\n\t\tReturns"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif not self._names:\n\t\tself._findCapsules()\n\tif not all:\n\t\treturn self._names, self._descriptions\n\telse: # for performance reasons\n\t\treturn self._names.keys(), sorted(self._descriptions.keys())\n\t\t\n\t\n# pylint:disable=too-many-branches\ndef findRoutinesOfFile(path): # pylint:disable=too-many-locals\n\t\"\"\"\n\tFinds routines of a python file and stores them in a list.\n\t\n\t@param\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t# pylint: disable=too-many-return-statements\n\t# pylint: disable=too-many-branches\n\t# pylint: disable=too-many-instance-attributes\n\t# pylint: disable=too-many-locals\n\t# pylint: disable=too-many-public-methods\n\t# pylint: disable=too-many-return-statements\n\t# pylint: disable=too-many-statements\n\treturnVoid = 0\n\treturnVoid = 8\n\ttype"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn getObjects(self._interface.interface, namesAndDescriptions)\n\t\t\n\tdef removeInterfaceAttribute(self, attr):\n\t\t\"\"\"Removes an attribute from the interface\n\t\t\n\t\tInterface interfaces are identified using type \n\t\tidentification strings of the following types:\n\t\t\"Class\", \"Proto\", or \"Enum\"\n\t\t\"\"\"\n\t\tinterface = getInterface(self._interface.interface)\n\t\tinterface._removeAttribute(attr)\n\t\t\n\t\n\tdef removeInterfaceNode(self, node):\n\t\treturn self._interface.interface._removeNode("}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif all: return self.ATTR_TYPE.keys()}\n\n\treturn {d: [e.name for e in self.__attrs if e.name in self.ATTR_TYPE[d]] for d in self.ATTR_TYPE}\n\n#////////////////////////////////////////////////////////////\nclass XMLObjectInfo(object):\n\t\"\"\"\n\tBase class for converting XML-based objects (e.g. comments) into a\n\t:class:`TreeBuilder`, :class:`NPOI`, or :class:`XHTMLElement`.\n\n\t:param name, or name="}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn self.interface.GetProperties()[\"interfaceName\"].namesAndDescriptions() if not all else self.interface.GetProperties()['interfaceName'].namesAndDescriptions().iteritems()\n\n\t\nclass InterfaceViewer(Viewer):\n\t\"\"\"View class for a specified interface view \"\"\"\n\t\n\tdef Get(self):\n\t\tif not hasattr(self, \"__interfaces__\"):\n\t\t\tself.__interfaces__ = {}\n\t\t\t\n\t\tif not hasattr(self, \"__viewer__\"):\n\t\t\tself.__viewer"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tpass\n\n\t\n\t\ndef getAttribute(self, *args, **kwargs): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn an attribute whose value can be retrieved using the attribute name\n\tpassed by the interface with the ``name`` argument.\t\"\"\"\n\tpass\n\n\t\n\t\ndef removeAttribute(self, attr): # pylint:disable=redefined-builtin\t\"\"\"\n\tRemove the specified attribute of a tag.\t\"\"\"\n\tpass\n\n\t\n\t\ndef _getInterface(self): # pylint:disable="}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self.names\n\n\t# def items(self):\n\t# \"\"\"\n\t# Return the attribute names and values for the interface, keyed by key\n\t# @return iter of (attributeName, attributeValue)\n\t# \"\"\"\n\t# return iter(self.names)\n\n\t# def keys(self):\n\t# \"\"\"\n\t# Return the attribute names and values for the associated keys, keyed by\n\t# key\n\t# @return iter of keys\n\t# \"\"\"\n\t# return itervalues\n\t# def values(self, all=False):\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\traise NotImplementedError\n\treturn self._names\n\n\tdef getAttribute(self, attr):\n\t\"\"\"\n\tReturn an Attribute named <attr> from the namespace.\n\n\t<attr> is a string name.\n\n\t@param all:\n\t\tIf true, then get the attribute from all.\n\t@return\n\t\"\"\"\n\traise NotImplementedError\n\treturn self._attributeNames.get(attr, None)\n\n\tdef hasAttribute(self, name):\n\t\"\"\"\n\tReturn if the class has the named attribute.\n\n\t@param name:\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self._interfaces\n\n  def set(self, name, value):\n   self._interfaces[name] = value\n\n  ### Properties ###\n\n  @property\n  def interfaces(self):\n    return self._interfaces\n\n  @interfaces.setter\n  def interfaces(self, value):\n    self._interfaces = dict(value)\n\n  @property\n  def name(self):\n    return self._name\n\n  def __repr__(self):\n    return self.__str__()\n\n  def __bool__(self):\n    return self."}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif all:\n\t\tfrom pygtk.glade import name_from_pointer\n\t\treturn list(name_from_pointer(self.__dict__[\"@\"]).values())\n\telse:\n\t\treturn self.__class__.__dict__.values()\n\nclass PropertyInterface(InterfaceBase):\n\t\"\"\"Interface that provides a set of special methods for handling\n\tproperties and interaction with pygtk objects. \"\"\"\n\tdef __getattr__(self, name):\n\t\tdef call(*args, **kwargs):\n\t\t\tsetattr(self, name, self.__"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self.__class__.attrs.keys()\n\n  def keys(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the key attributes of the interface.  \"\"\"\n\treturn self.__class__.attrs.keys()\n\n  def iteritems(self, value=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tGenerator method yielding the interface attributes as items.\r\n\tThis method is used for iterating attributes.\r\n\tThe default value of *value* is True for all other"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self._interface.names()\n\n  def get_field(self, attname, type):\n\t\"\"\"\n\tReturn a field with the given attribute and return the real data\n\t(which will in turn may be an instance of that interface's field).\t\n\t\"\"\"\n\treturn self._interface.get_field(attname, type)\n  def set_field(self, attname, type, value, clear=False):\n\t\"\"\"\n\tSet the field with the name provided.\t\n\t\"\"\"\n\tself._interface.set_field(attname, type, value"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self._keys\n\n\t\nclass Attribute(SingletonSingleton): # pylint: disable=too-many-ancestors,too-many-public-methods\t\"\"\"\n\tBase class for all Attributes. This class defines the interface for all other classes.\n\t\n\tThis class defines the interface for all other classes that are members of the interface.\n\t\n\tAttributes defined here are used for both the *instance* and *class* types. They do not\n\thave to be available to every class; only the *member* type can be accessed. The other *data*"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\traise NotImplementedError()\n\n\nclass AbstractView(AbstractAbstractObject):\n\t\"\"\"A view of an abstract module/class.\"\"\"\n\tname = \"Abstract View\"\n\n\tdef info(self, all=False):\n\t\t\"\"\"\n\t\tDisplay information about a view and its parameters.\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\n\nclass AbstractScript(AbstractView):\n\t\"\"\"An abstract class defining a generic view of an interactive REPL.\n\tIt allows one to interact directly with the REPL.\"\"\"\n\tname = \"Abstract Script\"\n\n\tdef __init__(self"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif self._flags[\"_name\"]:\n\t\treturn list(self.__class__.mro())[1:-2]\n\treturn list(itertools.chain(self.mro()[1:-2], _cname_names(self)))\n\n\ndef _cname_names(self):\n\t\"\"\"Iterate over the attributes of interface.\"\"\"\n\tattrs = self\n\tif isinstance(self, classmethod):\n\t\tattrs = itertools.chain.from_iterable(a.mro()[1:-2] for a in attrs"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif type(sequence) is tuple and isinstance(sequence[0], Interface):\n\t\tsequence = sequence[1:] # remove if multiple interfaces\n\tif output is None: output = sequence\n\ti=0\n\t# Remove empty string if present\n\tif type(sequence) is dict and'sequence' in sequence:\n\t\tvalue = sequence['sequence']\n\t\tif isinstance(value, dict):\n\t\t\tfor key in value:\n\t\t\t\tif value[key] is None:\n\t\t\t\t\tdel seq[i]\n\t\t\t\t\ti-=1\n\t\t\t\telse:\n\t\t\t\t\tseq[i] ="}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tfrom. import __all__\n\tnormalizeargs = dict(__all__.list(sequence) +\n\t                    interface.list(sequence) +\n\t                    struct.list(sequence) +\n\t                    enum.list(sequence) +\n\t                    interface.list(sequence))\n\n\tif output is not None:\n\t\tassert not isinstance(output, tuple), \"output must be a tuple for now\"\n\n\treturn [normalizeargs.get(argtype, argtype) \\\n\t\tfor argtype in normalizeargs \\\n\t\tif argtype not in (\""}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output is None:\n\t\toutput = sequence[:]\n\tif isinstance(sequence, (list,set)):\n\t\toutput.extend(sequence)\n\telif isinstance(sequence, tuple):\n\t\toutput.append(*sequence)\n\telse:\n\t\toutput[1] = sequence\n\treturn output\n\ndef _normalizemethods(sequence):\n\t\"\"\"\n\tNormalize methods of an interface specification.\n\tThis will always make lists and sequences of lists the same size;\n\tand if no base interface exists on the method specification, the\n\tspecified interface will"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tdef_ = []\n\tif isinstance(sequence, (tuple, list, set)):\n\t\tsequence = tuple(sequence)\t\t\n\t\ttype_ = sequence[0][2]\n\t\tfor argument in sequence[1:]:\n\t\t\tif argument[2]!= type_:\n\t\t\t\tassert False, \"Type %s with value %s doesn't match interface %s\"%(type_, argument[2], getattr(interface, argument[2], 'Undefined'))\n\t\t\telse:\n\t\t\t\ttype_ = argument[2]\n\t\treturn dict(sequence)\n\tel"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif not sequence: return sequence\n\telif isinstance(sequence[0], base.interface.Interface):\n\t\tif len(sequence) == 1: return sequence[0]\n\t\tp0 = sequence[0]\n\telif isinstance(sequence[0], tupletuple.tuple):\n\t\tp1 = sequence[1]\n\t\tif output:\n\t\t\tp1 = output\n\t\telif len(sequence) == 1:\n\t\t\tp1 = sequence[0]\n\t\t\tl = [p0]\n\t\t\twhile not p1.is_classifier():\n\t\t\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\treturn sequence\n\n\ndef _normalizetuis(sequence, output=None):\t\"\"\"\n\tNormalize TU interface declarations\n\"\"\"\n\treturn sequence\n\n\ndef _normalizeteams(sequence, output=None):\t\"\"\"\n\tNormalize Ethernet TU declarations\n\t\nTU includes Ethernet Protocols, the standard, and any other definitions \nthat the TU may define.\n\nAnything but individial interfaces or implements specs will be expanded.\n\"\"\"\n\treturn sequence"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tresult = []\n\tfor s in sequence:\n\t\tif isinstance(s, tuple):\n\t\t\titems, = s\n\t\telse:\n\t\t\titems = (s,)\n\t\tfor i in items:\n\t\t\tif isinstance(i, tuple):\n\t\t\t\tif len(i)!= len(items):\n\t\t\t\t\traise Exception(\"Unequal number of spec values. %s vs %s\"\n\t\t\t\t\t\t\t\t\t % (len(i), len(items)))\n\t\t\t\titems = i\n\t\t\telif isinstance(i, interfaces.Interface):\t\t\n\t\t\t\titems = i.normalize() \t\t\t\t\t\t\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif isinstance(sequence[0],Declarion) and isinstance(sequence[0].decl,Interface) and \\\n\t\tnot isinstance(sequence[0].decl,Specification):\n\t\t# Single interface or multiinterfaces\n\t\tfor i in range(len(sequence)):\n\t\t\tif isinstance(sequence[i],Declarion) and \\\n\t\t\t\tnot isinstance(sequence[i].decl,(tuple,set)) and \\\n\t\t\t\tnot isinstance(sequence[i].decl,specifier):\n\t\t\t\tif output and sequence[i].decl:# and isinstance("}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output is None:\n\t\toutput = []\n\tif isinstance(sequence, (str, int, float, Decimal, bool)):\n\t\tp = sequence\n\telif isinstance(sequence, tuple):\n\t\tp = seq_to_tuple(sequence)\n\telif isinstance(sequence, tuple) and len(sequence)>0:\n\t\tp = list(sequence)\n\telse:\n\t\traise TypeError(\"Illegal expression to normalize: %s\" % sequence)\n\tfor i, arg in enumerate(p):\n\t\ta = arg if"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output == None:\n\t\toutput = sequence[:]\n\telif not isinstance(output, list):\n\t\toutput = [output]\n\telse:\n\t\tfor i in range(len(output)):\n\t\t\tif not isinstance(output[i], list):\n\t\t\t\toutput[i] = [output[i]]\n\tfor i in range(len(sequence)):\n\t\tif seqtype(sequence[i]) in [Interface, enuminterfaces, implementspecifiers]:\n\t\telse:\n\t\t\tif isinstance(output[i], tuple): # if an iterator\n\t\t\t\t"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tglobal _CPointer\n\ttry:\n\t\t_CPointer.value()\n\texcept:\n\t\traise ImportError\n\treturn _CPointer.value()\n\ndef _dtype_available():\n\t\"\"\"\n\tCheck whether the DType module is available.\n\t\"\"\"\n\tglobal _DType\n\treturn True\n\ndef _real_dtype(dtype):\n\t\"\"\"\n\tdtype should be of the form (name, dtype) which can be used to find a dtype\n\tfor a given name.  If not found, a ValueError is"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\tfrom Cython.Build import cythonize, cythonize\n\texcept ImportError:\n\t\treturn False\t# No Cython\n\telse:\n\t\treturn cythonize()\n\ndef _c_crossversions_available():\t\"\"\"\tReturn the C crosscompilation module, if available, otherwise\na false value.   \"\"\"\n\ttry:\n\t\tfrom Cython.Build import cythonize, cythonize_optimize, cythonize_c_compiler\n\texcept ImportError:\n\t\treturn False\n\telse:\n"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t# This is necessary on python 2.8\n\timport importlib\n\tfrom importlib import reload\n\twith_optimizations = importlib.util.find_spec(\"py2lib._optimizations\") is not None\n\tif with_optimizations:\n\t\tif with_optimizations.loader is not None:\n\t\t\t# Load it in\n\t\t\tmod = reload(with_optimizations.loader).load_module()\n\t\t\treturn getattr(mod, \"optimizations\", False)\n\t\telse:\n\t\t\t# Don't worry, but probably should be an error\n\t\t\treturn False\n\t"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn False\n\ndef _c_optimizations_not_available():\t\"\"\"\n\tReturn the C optimization module, if not available,\n\totherwise raises an ImportError.\n\t\"\"\"\n\traise ImportError(\n        'The C runtime support for Python is not available\\n'\n        'This includes: the C runtime for C source code,\\n'\n        'the C runtime for Python itself\\n'\n        'You can specify which versions of Python's Python\\n'\n        'are to be used with the C runtime\\n'\n        'with the following command:\\"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t# If no optimization is provided, we assume that they're required.\n\tif not _c_optimizations_available:\n\t\traise ImportError(\"Not available: C options are not available in '%s'\"\n\t\t\t\t\t\t% os.path.join(os.path.dirname(__file__),'site-packages'))\n\treturn _c_optimizations_available()\n\ndef _c_version_string():\t\"\"\"\n\tReturn the version of gcc from which we are.\"\"\"\n\t\t# Import late to fail before local imports due to dependency cycle\n\t\t# importlib doesn't"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\tfrom _getopt import getopt, StrictGetOpts, get_get_opts\n\texcept ImportError:\n\t\treturn False\t# Python 3\n\tif StrictGetOpts():\n\t\topt_list = getopt(__doc__, \"o:d:c\")\n\t\tget_get_opts = StrictGetOpts\n\tfor opt in opt_list:\n\t\tif opt[1] == \"o\":\n\t\t\treturn True\t\t\t\n\treturn False\n\n\ndef _f_optimizations_available():\n\t\"\"\"\t\n\tReturn"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tfrom.backend import C, _optimize\n\tif getattr(_optimize, \"_optimized\", False):\n\t\treturn True\n\t_optimize._optimized = True\n\ttry:\n\t\timport ctypes\n\t\tctype = ctypes.c_char_p\n\texcept:\n\t\t# There are no ctypes.\n\t\tctype = None\t\t# no c_char_p type if ctypes wasn't installed\n\tif ctyp:\n\t\t\t_optimize.c = ctypes\n\t\t\t\t\n\t# For non-CPython implementations, just pass ctypes through"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tglobal _COPHIC_OPT_AVAIL\n\t_c_optimizations_available = True\n\tif type(__loader__) == type(Importer):\n\t\ttry:\n\t\t\tmod = __import__('optimizations', globals(), locals(), ['optimizations'])\n\t\texcept:\n\t\t\t_c_optimizations_available = False\n\telse:\n\t\ttry:\n\t\t\tmod = __loader__.find_module('optimizations', _path_info)\n\t\texcept ImportError:\n\t\t\t_c_optimizations_available = False\n\treturn _"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tc = _lib.ZONE_COPY\n\tif c == _lib.ZONE_COPY_ONLY: \treturn False\n\treturn _lib.ZONE_OPEN\n\ndef _c_optimizations_required():\t\"\"\"\n\tReturn the C optimizations module needed, if available,\n\totherwise a false value.\n\nIf the optimizations are not required but cannot be used in\nZONE, this raises the ImportError.\t\"\"\"\n\tc = _lib.ZONE_OPEN\n\tif c == _lib.ZONE_OPEN_"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn _c_optimizations_available\n\n\n# end of _c_optimizations_available\n\n##########################################################################\n#\t\t\tObjects\n##########################################################################\n\nclass _c_constants_class:\n\tdef __init__(self, name):\n\t\t\"\"\" \"\"\"\n\t\tself.__name__ = name\n\t\n\tdef get_name(self):\n\t\t\"\"\" \"\"\"\n\t\treturn self.__name__\n\n\tdef get_value(self):\n\t\t\"\"\" \"\"\"\n\t\ttry:\n\t\t\treturn self._c_constants[self.get_name()]"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\n\t# We're on PyPy.\n\tpython_version = sys.version_info[0]\n\t\n\treturn ((not PY_VERSION_TESTING) and python_version >= 3) or \\\n\t\t((hasattr(sys, 'pypy_version_info')) and PY_VERSION_TESTING)\n\ndef _should_attempt_numpy_impl():\n\t\"\"\"\n\tEvaluate ``np`` as we'll be using it on either PyPy (and Py3) than CPython.\n\t\"\"\"\n\t\n\tn"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn bool(os.environ.get('PURE_PYTHON'))\n\n\n# -----------------------------------------------------------------------------\n\n\nif _use_c_impl():\n\t# -----------------------------------------------------------------------------\n\n\t# TODO: In the long run, this file is very buggy and may not function correctly.\n\t_is_cython_module_valid = False\n\n\t# The Cython backend is responsible for doing something reasonable.  For\n\t# now, we just have to import as PyPy just fine.\n\n\t# Make sure cython is available in PATH in this case\n\t__import__('"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif hasattr(sys, 'pypy_version_info'):\n\t\treturn False\n\n\tif sys.platform.startswith('win'):\n\t\t# On Windows, C includes the CUO_DLL function although its name\n\t\t# includes \"coreuso\" as a prefix. Windows C includes both the\n\t\t# library \"coreuso\" and the CUBO_DLL. It's really difficult to know\n\t\t# where it will come.\n\t\tif os.path.exists('/usr/bin/env'):\n\t\t\t# Python 3.5-"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn PY_BETA_INPROGRESS and not (config.get(\"run_fast_math\", False)\n\t\tand config.get(\"native_pycretorizing\", \"\") == \"\"\n\t\tand PY_LIB_DIRS and \"PYTHONPATH\" not in os.environ)\n\n\ndef _use_c_impl():\n\t\"\"\"\n\tReturn a true value if we should use the C implementation.\n\n\tThis is normally only set if we're on a PyPy compatible machine; it's\n\tan idea to pass some PyPI packages that don"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\n\tmyenv = os.environ.copy()\n\tif not myenv[\"PURE_PYTHON\"] == \"1\":\n\t\treturn False\n\n\t# Make sure we're in the local environment - if not, this could just be False\n\ttry:\n\t\tif not sys.version_info[0] >= 3:\n\t\t\tsys.stdout = StringIO()\n\t\tmyc_version = get_local_module_cversions().get(2, None)\n\texcept ImportError:\n\t\tmyc_version = None\n\n\tif myc_version is"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn os.environ.get('PURE_PYTHON') == '1' or os.environ.get('PYTHONANYWHERE')!= '1'\n\n\ndef has_sockets():\n\t\"\"\"Return a boolean indicating whether sockets should be disabled.\"\"\"\n\treturn _use_c_impl or 'PYTHON' in os.environ, os.environ.get('CFLAGS')\n\n\ndef disable_opinion_of_deprecations():\n\t\"\"\"Force deprecation warnings to be avoided on failure.\"\"\"\n\tif not _"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif \"PYTHON_LESS_CONN_WANT\" in os.environ:\n\t\t# only c is available for python 3, we need c-only compiled\n\t\t# functions (c.co_flags has the extra overhead)\n\t\treturn False\n\telse:\n\t\t# don't attempt c optimizations if c is not available\n\t\treturn _USE_CPYTHON_FOR_NOT_ATTEMPTING_C\n\n\ndef _get_c_import_str(module):\n\t\"\"\"\n\tReturn the C import string to use for a module.\n\n\tThis"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tuse_c_optimizations = (\n\t\tos.environ.get('PURE_PYTHON') and\n\t\tos.environ.get('PYTHONSTARTUP')!= 'original'\n\t)\n\n\tif ((not sys.platform.startswith('darwin')) and\n\t\t(not use_c_optimizations)):\n\t\treturn None\n\n\treturn not is_pypy_optimized()\n\ndef _is_c_pure(path):\n\t\"\"\"\n\tReturn true if the given path is pure-Python.  This relies"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t# If PY37+ and we're on PyPy, check the result of the 'optimize' function to see if PYPY\n\t# is installed.\n\tif sys.version_info >= (3,6) and os.environ.get('PY_IMPLY', None):\n\t\t# It looks like there is PyPy 3.6 or newer - let's try to optimize things.\n\t\tif 'optimize' in _exec_environment(__file__):\n\t\t\t# PyPy-specific: not PY3 and not PyPy\n\t\t\treturn False\n\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t# NOTE: This may be `True` if a failure to detect C impl has occurred.\n\tif \"PYPYPROJ\" in os.environ and \"PYTHON\" in os.environ:\t\n\t\tif \"PYTHONIMPL\" not in os.environ:\n\t\t\t# NOTE: \"PYTHONIMPL\" is defined only in environments like Jython which\n\t\t\t# assume that the Python implementations are all pure Python.\n\t\t\t# See https://github.com/giampaolo/pygtk/issues/16 for details.\n\t\t\treturn"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn [\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t'',\n\t\t]\t\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t_c_optimizations_required.append((True, '__setattr__'))\n\t_c_optimizations_required.append((True, '__getattr__'))\n\t_c_optimizations_required.append((True, '__delattr__'))\n\t_c_optimizations_required.append((True, '__getattribute__'))\n\t_c_optimizations_required.append((True, '__setattr_noun'))\n\t_c_optimizations_required.append((True, '__getattroname__'))"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn ((),())\n\ndef _c_optimizations_optional():\t\n\t'''\n\tIf the following expression has a chance to be used, don't ignore\n\tit.  \n\t'''\n\n\treturn ()\n\ndef _c_optimizations_required_not_removed(x):\n\t'''\n\t(A function which takes a function as a parameter which returns an indicator\n\twhich means this expression has a chance to be used.\n\t\n\tThis function will print a message stating why the expression can't be used.\n\t'''\n\n\treturn (False"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tdef add_compile_args(compiler):\n\t\tcompiler.set_compile_args(pre=[b\"-O3\"])\n\tdef gen_glibc_glfc_arg_line(glibc_glfc_version):\n\t\tif glibc_glfc_version == \"2\":\n\t\t\tcompiler_glibc_versions = globals()\n\t\telse:\n\t\t\tcompiler_glibc_versions = {\n\t\t\t\t\"glfc2\": \"3\",\n\t\t\t\t\"glfc3\": \"6\",\n\t\t\t\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn {'c_cleanup', 'c_cleanup_once', 'cflags', 'cxxflags'}\n\n# end of _c_optimizations_default\n\n\ndef _set_pthread(use_pthread):\n\t\"\"\"\n\tSets the pthread.\n\t\"\"\"\n\t# Set it for the first time or by running the setup once.\n\tos.environ[\"PYTHON_PTHREAD_SEMAPHORE_OPTIMIZATION\"] = \"2\" if use_pthread else \"0\"\n\tset_p"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t_c_optimizations_required = {\t\n\t\t# C++ interfaces.\t\n\t\tb\"WGL_SWAPMENT_OVERLAY1\": b\"KHR_pixmap\",\n\t\tb\"WGL_SWAP_OVERLAY1\": b\"KHR_framebuffer_object\",\n\t\tb\"WGL_SWAP_OVERLAY2\": b\"KHR_framebuffer_object\",\n\t\tb\"WGL_SWAP_OVERLAY2_EXT\": b\"KHR_framebuffer_object\",\n\t\tb\"wglSwap"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn _c_optimizations_required() & _c_optimizations_ignored()\n\n\ndef _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn _c_optimizations_required() & ~_c_optimizations_ignored()\n\n\ndef _c_get_c_error_msg(cError, errorText, fatal=False):\n\t\"\"\"\n\tReturn a convenient error message to include in a C code snippet, when\n\tfails instead of a C source file or an error."}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn [_c_optimizations_required[i] for i in 'EOW']\n\n_c_optimizations_optional = {'F': 0x1,\n                            'O': 0,\n                            'B': 1}\n\ndef _c_optimizations_enabled():\t\"\"\"\t\n\tA tuple of the current conditions under the user's control.\t\n\t\"\"\"\n\treturn _c_optimizations_optional[optimizations_mode]\n\n\nclass _OptimizeModes(int):\n\t\"\"\"\n\tRepresents the various optimization modes of the optimization toolbox.\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\traise SystemError(\"TODO\")\n\treturn ()\n\n\nclass Optimizations:\n\t\"\"\"A group of optimizations, that are intended for all extensions to\n\tXSAT.\n\t\"\"\"\n\n\n\t__slots__ = (\n\t\t\"_opt\"\n\t\t, \"__weakref__\"\n\t\t, \"_enabled\"\n\t\t, \"_ignored\"\n\t\t, \"_requiredItems\"\n\t\t, \"_requestedItems\"\n\t\t, \"_ignoredItems\"\n\t\t, \"_injectiveItems\"\n\t)\n\n\n\tdef __init__(self) -> None:\n"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn (\t'\\\\\"nocopy\\\"'\n\t\t+ '\\\\\"gcc\"\\n'\n\t\t+ '\\\\\"mixed\"\\n'\n\t)\n\n#----------------------------------------------------------------------------\n\n# -- Help/usage ---------------------------------------------------------\n\ndef usage():\n\t\"Print program's long help (global usage)\"\n\timport sys\n\timport textwrap\n\timport os\n\thelp_text = \"\\nUsage: \\\"%s [options] \\\"\\nOptions:\\n  -h/--help                       Show this help\\n  -c/--cargs=PATH                     Show"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t#print('_c_optimizations_required')\n\treturn 'C_OPTIMIZATIONS_REQUIRED' in os.environ\n\ndef _c_optimizations_available():\t\"\"\"\n\tReturn a true value if the C optimizations available.\n\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl` in `\n\t_use_c_impl`.\n\n\tThere are 3 supported optimizations that can also be used in C:\n\t\t* C_OPTEN_STRIMES_NOT_AVAILABLE"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t# if the C code uses PURE_PYTHON or the implementation is using C, the\n\t# ``PURE_PYTHON`` flag will always be true, so return true.\n\treturn (\tc_flags.get('PURE_PYTHON')\t\tor\n\t\tc_flags.get('PYTHON_IMPL')\tor\n\t\tc_flags.get('PYTHON_CONVERSION')\tor\n\t\tc_flags.get('PYTHON_HAS_CONSTANT_CHECKS')\n\t)\n\n"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tif _use_c_impl(PurePython) and PurePython.PURE_PYTHON:\n\t\treturn True\n\treturn False\n\ndef _get_flags():\t\"\"\"\n\tReturn a Flags object containing some information about the compilation\n\tdone.\n\nIf this method is called, ``_c_optimizations_required()`` should return a\n``bool`` value indicating that the compilation is required. The result of\nthis function indicates whether an optimization is required.\n\nThe return value of this function can be used to check whether the compilation\nhas detected any errors.\n\"\"\"\n"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn (\n\t\tsys.platform == \"win32\"\n\t\tor PURE_PYTHON\n\t\tor __builtin__.getenv(\"COVERAGE_PROC\", True)\n\t\tor t.PY27\n\t)\n\n\ndef _c_optimizations_used():\t\"\"\"\n\tReturn an ordered list of the current CPython-specific optimizations.\n\nAlthough CPython uses the PURE_PYTHON/COVERAGE_PROC vars to configure\nPyConfig, we do add that at the beginning, so that we can use them\nbefore we"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tif PYTHON is PURE_PYTHON:\t\t\t# Python >= 2.3\n\t\treturn True\n\telif PURE_PYTHON > 1.0:\t\t\t# Python >= 1.1\n\t\treturn fpstate.is_optimizeable(fopts.STDC_OPTIMIZE, fopts.STDEVPX_OPTIMIZE,\n\t\t\t\t\t\t\t\t\t fopts.CPP_OPTIMIZES)\n\telif PURE_PYTHON >= 0.7:\t\t\t# Python >= 0.6\n\t\treturn"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tif \"PURE_PYTHON\" in os.environ:\n\t\treturn False\n\treturn True\n\n#--------------------------------------------------------------------\n#   Test\n#--------------------------------------------------------------------\n\ndef test():\n\t\"\"\"\n\tA helper function for tests, which returns True if the test can run.\n\t\"\"\"\n\tif _c_imports_required():\n\t\tif not is_pypy and not is_freebsd and not is_win32 and not is_cygwin \\\n\t\t\tand not is_darwin and not is_unix or _c_optimizations_required"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tif _has_native_lib_compiler():\n\t\t_libc.check_call(_have_libc)\n\t\tlibc.check_call(_have_libc)\n\t\tif sys.platform == 'darwin':\n\t\t\t_libc.check_call(_have_libc_system_darwin)\n\t\telif sys.platform == 'win32':\n\t\t\twin32utils.check_os2_lib_dll()\n\telse:\n\t\tuse_c_impl_ = _get_compiler_required(True)\n\t\tlibc.check_"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tif 'use_c_impl' in kwds:\n\t\treturn kwds['use_c_impl']\n\telse:\n\t\traise ValueError('_c_optimizations_required() not called')\n\ndef _c_optimizations_required_default():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required, if not already \n\tconfigured.\n\n\tIn a future version of Python, this can be accomplished based on using the \n\t``PYTHON_OPT`` global variable.\n\t\"\"\"\n\tif 'PYTHON"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn PURE_PYTHON\n\ndef _c_optimizations_is_enabled():\t\"\"\"\n\tReturn a true value if the C optimizations are enabled.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn ENABLE_CPYTHON_ORG_OPT\t\n\ndef _use_c_impl():\t\"\"\"\n\tReturn the C implementation to use.  This is a function passed to the\n\t``check_c_impl()`` function that is used internally by this"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn getenv('PURE_PYTHON') == 'True'\n\ndef _c_debug():\n\treturn bool(globle('debug'))\n\nclass _PythonPYCompiler(compiler.BaseCompiler):\n\t\"\"\"Base class for different implementations of C\"\"\"\n\tdef compiler(self, *args, **kwargs):\n\t\tself._c_compiler = self.find_compiler()\n\t\treturn self._c_compiler(*args, **kwargs)\n\tdef compiler_algorithm(self):\n\t\t\"\"\"Method to choose compiler algorithm"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.hist2d = {}\n\t#TODO(jw) -- allow histograms to be dynamically created without reinitialising the histogram.\n\t#TODO(jw) -- create a dictionary of a fixed size with zero bin centres and no bin edges -- and set the bins\n\t#to the values passed by the user? Make it possible (i.e. it allows to control the sampling).\n\t#TODO: allow the user to specify a bin type, and/or an alternative, as well as a fixed minimum and/or maximum.\n\t#TOD"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.count = 0\n\tself._hist = {'min':float('inf')}\n\tself._hist['max'] = float('-inf')\n\tself._hist['avg'] = 0.\n\tself._hist['sum'] = 0.\n\tself.normalized = False\n\tself.histogram = {'min':0.,'max':float('inf'), 'avg':0.,'sum':0.}\n\t\n\tself._initial_value = None\n\tself._initial_hist = False\n\tself._s_before = 0.\n\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n\t## NOTE: the default bin structure is:\n\t## [ (center_value, [lower,upper,midbin])...\n\t##   (center_value, [lower,upper,midbin1])...\n\t##   (center_value, [])...\n\t## ]\t\n\t\n\tself._reset_histos(self._histos)\n\tself._histos[0].Clear()\t# reset histos_array\n\tself._histos_index = {}\n\tself._histos_sum = {}\n"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.__histogram = dict() # reset the dictionary\n\tself.__bins = [] # reinitialized with *initialize_bins()*\n\tself.__histogram = dict() # reset the dictionary\n\n\tself.__set_histogram_value(self.__initialize_value(), 0.0)\n\n\treturn self.__bin_index\n\n\n\t\n\n\ndef make_a_histogram(initial_value, make_bins=True):\t\"\"\"\n\tInitialize the histogram.\n\n\tinitial_value: float\n\t\tValue that the histogram"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n\tself.histogram={}\n\tself.bins = []\n\tself.hist_total = 0\n\tself.histogram['min'] = []\n\tself.histogram['max'] = []\n\tself.histogram['hist'] = []\n\n\tself.bins_total = 0\n\tself.bins_interval = 0\n\tself.bins_interval_new = 0\n\ndef set_xlim(self,lim):\n\t\"\"\"\n\tSet the plot limits.\n\n\tLimits are returned as *xlim* (min,"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.histo = {}\n\treturn self.reset()\n\n\t\n\t\n\t\n\n#############################################################\t   #\n#  Functions for creating the histograms            #\t #\n#############################################################\t\n\n\n\ndef make_barplot(data, xlabel, ylabel, cmap):\n\t\"\"\"\n\tThe bars are calculated by the data from :term:`pandas`.plot.\t\n\n\t:param pd.DataFrame data: The data to plot. \n\t:param str xlabel: The label for the x-axis (`"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.reset_args(initial_value=self.kw['initial_value'])\n\tself.clear_bins()\n\tself.hist_count.pop('histogram', None)\n\t\n\tif self.kw['show']:\n\t\tfrom.text.Text import Text\n\t\tself.add_text(Text(\" Histogram\", 'center',\n\t\t\t\t\t\t   ('bold', 'blue')))\n\t\tself.add_text(Text(\" Number of bins  (default 50) \",\n\t\t\t\t\t\t   ((10,10), (0,1,30,500))))\n\t\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\thist = {}\n\tfor key, val in self.__dict__.items():\n\t#if ((type(val) == type(self.__dict__[key])).__name__ == 'list' or (type(val) == type(self.__dict__[key])).__name__ == 'dict'):\n\t#if key.find('_') >= 0:\n\t\thist[key] = 0\n\tself._reset_bins(hist)\n\treturn hist\t\n\t\t\n    def _reset_bins(self, bin_vals=None):"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself._bins = {}\n\tself._total = 1\t\n\tself._x = None\n\t\n\tself._widths = np.empty(self.nbins, numpy.float64)\n\tself._slices = {}\t\t\n\tself._y = None\n\tself._fmt = ''\t\t\t# Formatstring (only used if *y=None*).\n\tself._colors = {}\t\t# Color name (key) and hex string (value).\n\tself._stderrs = {}\t\t# Error estimate (key) and formatstring (value).\n"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself._bins = dict(initial_value=self.make_bins())\n\tself._bins['upper'][:0] = self.hist.cumsum()\n\nclass HistOptimizer(object):\n\tdef __init__(self, initial_value):\n\t\tself._initial_value = {'normal' : initial_value['normal'][:]}\n\tdef __call__(self, hist:Hist):\n\t\tself._hist = hist\n\t\tself._initialize()\n\t\treturn self._do_hist(self._initial_value)\n\n"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tfrom lena.util import issequence\n\n\tfor graph in self:\n\t\tif graph.is_multigraph():\n\t\t\traise NotImplementedError(\"multigraph not implemented yet\")\n\n\t\t# if (graph.is_directed() or graph.is_topologically_sorted()) and\n\t\t# lena.graph.graph_is_undirected():\n\t\t#     raise NotImplementedError(\"directed topologically sorted not implemented\")\n\n\t\ts = graph.get_list_of_nodes()\n\t\tfor i, v in enumerate(s):\n\t\t\trow ="}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tdef as_csv(g, csv=True):\n\t\twith ln.ln(g):\n\t\t\tif len(g)==0: return ['']\n\t\t\telse:\n\t\t\t\treturn ['\"%s\"'%(str(x))for x in g]\n\tdata  = list(map_(Data, graph))\n\theader = list(map_(Header, data)) if header is None else header\n\tif separator!=\",\": separator = '(',\n\tif (not csv and isstring_(separator)) or csv:\n\t\treturn map_ ('"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\n\tif isinstance(separator, str):\n\t\tseparator = (separator,)\n\telse:\n\t\tif len(separator)!= 1:\n\t\t\traise RuntimeError(\n\t\t\t\t\"separator must be one character, not %r\" % separator)\n\tif not isinstance(header, (str, type(None))):\n\t\traise TypeError(\"header must be a string or None\")\n\t\n\treturn _to_csv(self, separator, header, None)\n\t\t\ndef from_png(png):\n\t\"\"\"\n\tIterate over the"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\timport numpy as np\n\ts, b, to_csv = self.points_as_str(),self.bary_coordinates(),self.to_csv()\n\treturn np.array([s.values],dtype=np.str).T, b.values.T,[separator]  # np.array[np.str_]->np.str_ = np.str_ object\n\n\n\n\n\n\nclass Graph:\n\t\"\"\"\n\tA class that represents an individual :class:`Graph`\n\tobject.  The use of this class can be used many ways,"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tdef _write_csv(fn, obj):\n\t\tif not fn.startswith(\"/\"):\n\t\t\treturn \"w\" + fn\n\t\tis_array = True if fn.endswith(\".png\") else False\n\t\tcsv_writer = open(fn, \"wt\")\n\t\tfor v in obj.vertices:\n\t\t\tfor i in range(4):\n\t\t\t\tcsv_writer.writerow([v.x, v.y, v.z])\n\t\t\tfor i in range(8):\n\t\t\t\tcsv_writer.writerow([obj.graph[v].nodes"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t#if not hasattr(self._graph, \"to_csv\"):\n\t\t#raise TypeError(\"expected a graph with the 'to_csv' method\")\n\n\tif self._graph.num_dims == 1:\n\t\tpoints = list(zip(*self._graph.coords))\n\telse:\t\n\t\tpoints = zip(*_convert_to_coords(self._graph))\n\n\treturn self._graph.to_csv(output=out, separator=separator, header=header)\n\n\ndef to_matrix(graph):\n\t\"\"\"\n\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\timport collections\n\n\tif isinstance(self, collections.MutableSequence):\n\t\tself = map(lambda g: g.to_csv(separator=separator, header=header), self)\n\telse:\n\t\tself = [g.to_csv(separator=separator, header=header) for g in self.list()]\n\treturn '\\n'.join(self)\n\nclass LenaError(Exception):\n\tpass\n\ndef lena_error(**kwargs):\n\treturn LenaError(*(kwargs.iterkeys() +\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\n\timport csv\n\timport math\n\t\n\tif len(self.vertices) > 1:\n\t\t\n\t\tprint \"***Graph:\", self\n\t\n\t\tdef convert_to_str(v, s, e):\n\t\t\ts = s.encode('utf-8')\n\t\t\te = e.encode('utf-8')\n\t\t\treturn [u'{:.{}f}'.format(u[0], 8).encode('utf-8')\n\t\t\t\t\tfor u in self.x.get(v, (0,0)) ] + ["}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\n\tseparator=_ast.literal_eval(separator)\n\tif isinstance(header,type(None)):\n\t\t_f = lambda x: str(x).replace('\\n',' ')\n\t\theader=lambda x: ',' +','.join(_f(n) for n in x)\n\tfrom astropy.io import ascii\n\treturn ascii.tabular.to_csv(self, _ascii, header, separator)\n\t\n\t\n#####################################################################\n#####################################################################\n#####################################################################\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\timport lena\n\timport numpy as np\n\t# from scipy.constants import eps\n\t# eps = eps*10**(-6) * 10**(-18)\n\teps = 1*3.1852\n\tdef to_list(v): return v._as_tuple()\n\tfor graph in self.g:\n\t\tnp.insert(graph.points, 2, graph.points,\n\t\t\t\t  axis=0)  # append as vector\n\t\tnp.insert(np.ones(graph.g_dim, dtype=int),"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\ttry:\n\t\treturn [\n\t\t\tcoord_name,\n\t\t\tlen(np.unique(coord_name)),\n\t\t\trange(0, len(self.get_data_array()))\n\t\t]\n\texcept:\n\t\tpass\n\t\t#return [coord_name, ]\n\n\t\n\n#------------------------------------------------------------------------------\n# Get the number of elements and their statistics.\ndef get_number_elements(data1, verbose=False): \n\tif verbose: print(f\"Getting number of elements: {data1}\")\n\tl1 = len(data1)\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tlogger.info(\"\"\"[{0}]: Extracting coordinates {1} in {2}\"\"\".format(\n\t\tdatetime.datetime.now(), coord_name, self.coord_labels.index(coord_name)))\n\tcolnames = self.coord_labels[self.coord_labels==coord_name].values\n\tlogger.info(\"{0:<30} {1:<3} --> {2:<4} {3:<20} {4:<20}\"\n\t\t.format(colnames[0], colnames["}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\treturn self._get_index('error_indices', coord_name)\n\t\n\t\nclass Interval(object):\n\t\"\"\"\n\tA class that provides a range of intervals for the time domain \n\tof a signal.  Intervals are initialized as defined by their \n\tmin and max times. \n\t\"\"\"\t\n\t\n\tdef __init__(self, min=None, max=None):\n\t\t\"\"\"\n\t\tIf a range is initialized as None (default), no intervals need\n\t\tto be initialized.\n\t\t\"\"\"\n\t\tif min is not None:\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tindices=[0, 1, 4, 5, 9, 12, 13, 14, 17, 31, 34, 36, 41, 45, 58, 63, 61, 65, 68, 75,\n\t\t 81, 93, 124,126, 127, 128, 132, 135, 137,\n\t\t139, 143, 144,\n\t\t146, 151,154,156,157,159]\n\tif coord_name.endswith(\"X\"):\n\t\tprint(\"coordX\",coord_name,self.error_indices[coord_name])\n\t\treturn self.error_"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\n\t# find index of the coord with the given name.\n\t# get error indices of this coord's type\n\terr_idx = -1 # default value to -1 because this coord is unknown\n\tfor indx in range(len(self.coord_info)):\n\t\ttype_name=self.coord_info[indx][2]\n\t\tif coord_name.lower()==type_name.lower():\n\t\t\terr_idx=indx\n\t\t\tbreak\n\t\t\n\treturn err_idx\n\nclass coord_extractor(object):\n\t\"\"\""}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tindices_list = list(range(len(self.err)))\n\tindices_dict = dict(zip(self.err_names, indices_list))\n\treturn [(indices_dict[indices_name], indices_dict[neighbor_name]) for indices_name in self.err_names for neighbor_name in self.neighbors[coord_name]]\n\n\n#============================================================================================================= #\n#                                  Function definition                                   #\n#============================================================================================================= #\n\ndef _read_geo_files(filename_list):\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t# get unique error values in the coordinates\n\tflds=['F', 'E']\n\terr_inds,err_strs=np.unique(self.fields[coord_name], return_inverse=True, axis=0,\n\t\treturn_counts=True, return_inverse=True, return_index=False)[1:]\n\terr_vals=np.array(err_inds)\n\t# create a pandas dataframe for error and index values on the same column\n\terr_df=pd.DataFrame(np.column_stack"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\n\tind_e0 = {'hf12': 0, 'e13': 1, 'gf13': 2, 'rf13': 3} #\n\tind_ee = {'hf12': 4, 'e13': 5, 'rf13': 6}\n\t\n\tind_er = {'hf12': 1, 'e13': 2, 'rf13': 3}\n\treturn (ind(coord_name) & (x == float(self._corrcoords['corr'][0]) )\n\n\nclass x_y_corr"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif self.c_err_indices is None:\n\t\t# error indices are already calculated when self.c_err_indices is specified from a parent.\n\t\treturn []\n\telse:\n\t\t# error indices need to be accumulated.\n\t\treturn self.c_err_indices[coord_name]\n\nclass _GeoJson2CSV_Handler(object):\n\tdef __init__(self):\n\t\tself.c_err_indices = None\n\t\tself.geom_handler = _GeoJson2CSVGeomHandler()\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tx2, y2 = coord_name.split('_')\n\tx1, y1 = x2.split(self.sides)+y2.split(self.sides)\n\n\treturn int(x1), int(y1)\n\n\t\nclass QuadTree:\n\t\"\"\"\n\tThis class represents the Quad Tree representation and a Node which contains \n\tonly points with the corresponding x, y coordinates of the bounding box \n\trectangle. The class will not have a representation for all nodes.   \n\t\"\"\"\n\tdef __init__(self, quad_"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tself.context = context\n\tself.value = context['value']\t\t\t# dict containing fields \"E,t\"\n\t#if x - y < self.__x_low - self.__y_low:   # (x-y) < self.__x_low - x_low:\n\tif x  <= self.__x_low:\t    # (x-y) <= self.__x_low:\n\t\tself.context.pop(\"error\")['x'] = self.__x_low  # remove x\n\t\tself.context.pop(\""}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor edge in list(context.edge[:-1]):\n\t\tcontext.edge[edge[\"e\"]][\"e\"][\"edge_index\"] = context[edge[\"e\"]][\"index\"]\t\t\n\tfor e in context.edge: context.edge[e][\"n\"][\"edge_index\"] = context[e][\"index\"]\t\t\n\tfor v in context.vertex: context.vertex[v][\"vertex_index\"] = context[v][\"index\"]\t\t\n\tcontext.vertex[list(context.vertex)\n\t\t[0"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor key, val in context.to(self).items():\n\t\t# We need values here, not data, as a subclass of `dict`\n\t\tsetattr(context, key, val)\n\tcontext.error.update(context.value)\t# Make an extra layer\n\n\n#################################################################\n# Classes: Graph\n#################################################################\n\nclass Graph(Graph):\n\t\"\"\"\n\tClass containing all the attributes needed to draw a graph.\n\n\tThe graph object consists of a list of subgraphs (defined\n\tby a graph structer like structure), indexed by its name.\n"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tif not self.value:\n\t\tself.value = {}\n\t\tfor key in context.property:\n\t\t\tcontext.value[key] = context.property[key]\n\tif \"error\" in context:\n\t\tcontext.update_all(**context[\"error\"])\n\n\ndef _from_context(context):\n\t\"\"\"\n\tConvert the *context* into the flow.\n\tReturn value is passed to\n\t:func:`.to_context`.\n\t\"\"\"\n\treturn to_context(context)\n\n\ndef _to_context(value, context=\"\"):\n"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tself.toCSV()\t\t\t\t\t# convert the original graph to CSV\n\ttry:\n\t\tvalues = context.value\n\texcept Exception as e:\t\t# catch invalid graph *(context,values)\n\t\t# and all its subcontexts which may be empty\n\t\t# by _get_subcontexts_index\n\t\tlogging.error('_get_subcontexts_index returned an empty array: %r', e)\n\telse:\n\t\tx,y,z = self.getCoordinates()\n\t\t# Remove invalid coordinate types\n\t\tif len(values) > 0"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tif \"E\" in context.value:\n\t\tif \"t\" in context.value[\"E\"]:\n\t\t\tcontext.value[\"E\"][\"t\"] = {\"x_low\": context.value[\"E\"][\"t\"]}\n\t\tif \"low_errors\" in context.value[\"E\"]:\n\t\t\tcontext.value[\"E\"][\"low_errors\"].update({} if context.value[\"E\"][\"low_errors\"] is None else context.value[\"E\"][\"low_errors\"])\n\t\t\tcontext.value[\"E\"][\"low_errors"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\ttry: x, y, z = context.flow\n\texcept (TypeError, IndexError, AttributeError): return\n\tfor name in self.properties:\n\t\tvalue = getattr(context.flow, name)\n\t\tvtype=type(value)\n\t\tif vtype is dict or vtype is tuple:\n\t\t\tfor key, value in value.items():\n\t\t\t\tdelattr(context.flow[name], key)\n\t\telif vtype is str:\n\t\t\tdelattr(context.flow[name], name)\n\t\telif vtype is int:\n\t\t\tdel"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t# In order to have a graph to have an initial data\n\t# of it's own we create a copy of the context,\n\t# which keeps track of individual errors so that\n\t# we can have them correctly displayed.\n\tother = copy(context)\n\tother.extend(context.error.copy())\n\tother.extend(context.node.get(\"error_elements\", {}))\n\tother.extend(context.edge.get(\"error_elements\", {}))\n\tgraph = self.new(\n\t\tcontext, other, self.graph_type"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tif self.value:\n\t## remove the existing values in the context.\n\t\tif \"message\" in self.value:\n\t\t\ttry:\n\t\t\t\tdel self.value[\"message\"]\n\t\t\texcept KeyboardInterrupt:\n\t\t\t\tlogger.warning(\"GraphError: ctx.value=%s\", self.value[\"message\"][\"data\"], exc_info=True)\n\t\telif \"data\" in self.value:\n\t\t\ttry:\n\t\t\t\tdel self.value[\"data\"]\n\t\t\texcept KeyboardInterrupt:\n\t\t\t\tlogger.warning(\"GraphError: ctx.value="}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\ttry:\n\t\tp_error = context.error\n\t\tif not p_error:\n\t\t\treturn\n\texcept:\n\t\treturn\n\tkey, index, dt, e_low=(p_error[\"error\"] or \"\").split(sep=\"_\")\n\t#print context.info(key, index, dt, self.name, e_low=e_low)\n\t#print \"update_context():\\n context.error=%s\\ncontext.value=%s \" % (str(context.error), str(context.value))\n\t#"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn histogram(bins, edges, hist = True)\n\ndef integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges*\nform the mesh\nfor the integral.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn histogram(bins, edges, hist = False)\n\nclass fit:\n\t\"\"\"\n\t:arg bool_stats: plot mean, min, and max values\n\t:kw args:\n\t    *fname"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\treturn integralB(bins, histogram(edges, hist=0)[0]*(edges[1]-edges[0])/edges[1]-\n\t\t\t\t\tedge0)/(edges[1]-edges[0])\n\n\ndef integralB(bins, x):\n\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *x* form the mesh\nfor the integration.\n\nThe integration algorithm computes:\n\n>>> intB = integrate(bins, x)\n\nIt returns a"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\tdef scale(value): \t\t# compute integral of an intensity\n\t\tnumerator = np.prod(bins[:-1] + (edges[:-1] - bins[:-1])/2.0)\n\t\tdenominator = edges[1] - edges[0]\n\t\treturn numerator/denominator\n\t\n\treturn np.array([np.arange(b[0],b[1]) if scale(b[0]) >= 0 else \\\n\t\tnp.arange(b[0],b[1],1)*(b["}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tn = len(bins)\n\th = n*(n-1)/2\t\t\t\t\t\t\t\t# total number of elements\n\t# total number of bin edges\n\tb = n*edges\t\t\t\t\t\t\t\t# bin edges\n\ta = [0]*(h - 1)\t\t\t\t\t\t# bin values, one for each bin\n\tfor i in range(0, h-1):\t\t\t\t\t# iterate over each bin\n\t\ta[i] = b[i] / (h - 1) - a[i]\t\t# update bin values for next iteration\n\treturn sum(a[-1:])"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn histogram(bins, edges, *args, **kwargs)\n\ndef bin(bins, edges, *args, **kwargs):\t\"\"\"\n\tCompute distribution function `f(x)` for given histogram.\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn histogram(bins, edges, *args, **kwargs)\n\ndef dif(left, right, *args, **kwargs):\t\"\"\""}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn _integrand(bins, edges, 'integral')\t\t\t\t  \ndef smooth(bins, edges):\n\t\"\"\"\n\tCompute smooth (sum with weighting for the histogram).\n\n\t*bins* contain values, and *edges* form the mesh\n\tfor the smoothing. Their format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn _integrand(bins, edges,'smooth')\t\ndef histogram(x, width=None, bin=False):\n\t\"\"\"\n\tCompute histogram along the given dimension"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tN    = len(edges)-2\n\tbins   = _bin(bins)\n\treturn binmap(*(_integrate(a, bins * 2 ** N) for a in edges))\n\ndef binmap(A, B=None, C=None, bin_size=None, mask=None):\n\t\"\"\"Create a binned map.\n\n*A*, *B*, *C*, and *bin_size* are arrays representing the bin edges,\nrespectively.   They must be of the same length and contain non-negative numbers\nfor integer values.  Each"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\tn = sum([len(edges.get(x/10.)) for x in bins])\n\n\tfor i in sorted(bins):\t\t\n\t\tedges[i/10.] = cumsum(np.concatenate([[0], edges.get(i/10.)])) + edges.get(i, [0]).take(n+1)\n\t\tn -= len(edges.get(i/10.))\n\tdel edges\n\tdel bins\n\t\n\tc = integrate_cubic(edges[0], edges[1"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\treturn histogram(bins, edges, data[:,np.newaxis])\n\ndef integrate(a, b, data):\n\t\"\"\"\n\tIntegrate from ``(a, b)`` to ``(x, y)`` using the histogram\n\tfunction\n\t\"\"\"\t\n\n\tedges = a < b\n\tb = b[edges]\n\ta = a[edges]\n\n\ts0 = histogram(b[:0], a[:0], data, bins = len(a))\n\ts1 = histogram(b[:1], a"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn np.trapz(bins[:-1], edges[1:]-edges[:-1])\n\ndef max_over_bin(values):\n\tvalue = np.unravel_index(np.argmax(values), values.shape)\n\treturn values[value], value+1\n\n@numba.njit\ndef count_in_histogram(values, step):\n\tcount = 0.0\n\tfor i in range(step):\n\t\tstart = i*step\n\t\tstop = start + step\n\t\tif start>=len(values):"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tfrom.seq_to_fill_request import seq_to_fill_request\n\treturn len(seq) == 4 and not isinstance(seq[3], (fill_request_type, Source_Seq, Frame_Seq) )\n\t\n\ndef is_fill_request(req):\n\t\"\"\"\n\tTest whether given FillRequest is a valid FillRequest element.\n\t\"\"\"\n\tif not isinstance(req, fill_request_type):\n\t\treturn None\n\treturn req.params is not None and (not req.params is None or \"nodeprint"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\ttry:\n\t\tfillreqseq = ElementTree.fromstring(seq)\n\texcept VersionError:\n\t\treturn 0\n\n\tif isinstance(fillreqseq, ElementTree._element.SubElement):\n\t\tif fillreqseq.tag!= 'fill':\n\t\t\treturn 0\n\n\tif fillreqseq.text == '':\n\t\treturn 0\n\n\tif len(fillreqseq) == 0:\n\t\treturn 0\n\n\telse:\n\t\treturn 1\n\n\treturn 0\n\ndef is_fill_seq(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn True if seq == FillRequest.SequenceNumber or (len([s for s in seq if s.type == FillRequest.SequenceNumber]) > 0) else False\n\ndef is_fill_seq(seq, seq_end, seq_type):\n\t\"\"\"\n\tTest whether *seq* can be converted to a FillSeq.\n\t\n\t*seq* can be:\n\t\n\t* a PN object\n\t* a string representing a fully qualified SequenceNumber.\n\t* a int, or a tuple/list of such\n\t* a PN object,\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tis_valid = is_seq(seq) and seq.is_fill_request() or (is_str(seq) and is_fill_seq(seq.data))\n\tif not is_valid:\n\t\tLOGGER.warning(\"Can't find Fill request/source sequence %s in given sequence %s\", seq.tag, seq.data)\n\treturn is_valid\n\ndef is_fill_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillSeq.\n\nTrue if not is of class FillSeq or contains the sequence object"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tseq = seq.value\n\tif seq not in (None, ENCODING_INFORMATION, None):\n\t\treturn False\n\tseq_type = type(seq)\n\tif len(set(seq_type.__dict__.get('__slots__', ())))!= 1:\n\t\treturn False\n\tif not isinstance(seq, dict):\n\t\treturn False\n\tfor k, v in seq.items():\n\t\tif not isinstance(k, str):\n\t\t\treturn False\n\t\tif not isinstance(v, str):\n\t\t\treturn False\n\ttry"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\n\tseq_length = len(seq)\n\tif(seq_length < seq_length_required): return False\n\t\n\tif(not seq[seq_length_required] in ['E', 'C', 'I', 'H', 'P', 'S', 'X']) : return False\n\tif(not isinstance(seq[seq_length_required], str)): return False\t\n\t\n\t# A FillRequestSeq is either (1) a FillRequest or an Error-level fill request, of which (i.e. it contains only the correct one"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\ttry:\n\t\tseq_type = seq.get_type()\n\texcept AttributeError:\n\t\treturn False\n\ttry:\n\t\tseq_data = seq.data\n\texcept AttributeError:\n\t\tpass\n\treturn seq_type == TypeEnum.Elements and not isinstance(seq_data, IntSQProteinID) # and seq.get_idseq().is_segment_unpaired() #\n\ndef is_fill_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillSequence.\n\nTrue"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\n\tassert seq.kind() == 1 and seq.tid().kind() == 1\n\tseqlist = seq.seqlist()\n\tassert len(seqlist) == 1\n\tseqseq = seqlist[0].seq()\n\tif type(seqseq) == FillRequest and seqseq.src().tid().kind() == 1:\n\t\treturn True\n\telif type(seqseq) == QXseq and (seqlist[0].src().tid().kind() == 1 or len(seqlist) == 0):\n\t\treturn True\n\telse:\n\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif seq in [fill_type.source_seq.value for fill_type in FillType if fill_type is not None]:\n\t\treturn False\n\tif len(seq) < 7:\n\t\treturn False\n\t\t\n\treturn True\n\t\nclass Read(namedtuple('Read','sensor start_offset read length real_length')):\n\t\"\"\"A read record\n\t\"\"\"\n\tpass\n\n# ###################################################################################################### #\n\nclass FrameTypeSequence(nptyping.NestedSequence[fill_type.frame_type_seq]):"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn isinstance(seq, FillRequest) or seq.seq.is_sequence\n\ndef _split_fill_type_and_request_type(seq, i):\n\t\"\"\" Return SplitFillType,\n\t\tand requestType as strings. \"\"\"\n\ti = i + len(_FillRequest)\n\tif isinstance(seq, FillRequest):\n\t\tseqtype = seq.seq.type\n\telse:\n\t\tseqtype = seq[i:i+5]\n\treturn seqtype, structs[seqtype.tag]\n\ndef _split_id_or_seq"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn True\n"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif obj!= None and not isinstance(obj, dict):\n\t\treturn False\n\tif not hasattr(obj, '__len__'):\n\t\tobj_ = [obj]\n\telif obj.count(None) > 0:\t\t\t\t\t\t\t\t\t\t# if no None value, not None else empty, so it's a list\n\t\tobj_ = list(map(lambda obj_i: obj_i[0] if obj_i[1] is None else obj_i[1], enumerate(obj)))\n\telse:\n\t\tobj_ = list(map(lambda obj_"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn callable(obj) or isinstance(obj, types.FunctionType)\n\n\ndef is_fill_type(typ):\n\t\"\"\"\n\tCheck whether object is an object that is not (generic) python object.\n\tCython code will consider objects to be cython.\tCython code, it is\n\tbetter to use this function, than to define any generic object, as the\n\tresulting cython.\n\t\"\"\"\n\treturn not (is_fill_request_el(typ)) or (is_dummyvar_el(typ) or is_cy"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tfrom.is_s_x_e_s import is_s_x_e_s\n\tfrom.is_s_e_s import is_s_e_s\n\tfrom.is_s_e import is_s_e\n\timport os, subprocess\n\ts_t_e_o_e_=obj.__dict__.keys()\n\tfor key in s_t_e_o_e_:\n\t\tif \"fill\" in key and obj.is_fill_request_el(obj._elements):\n\t\t\tm_result"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif not hasattr(obj, '_fill'):\n\t\tobj._fill_method = getattr(obj.fill, '__call__', None)\n\t\tobj._request_method = getattr(obj.request, '__call__', None)\n\ndef check_value_is_integer(value, dtype, name, strict=True):\n\t\"\"\"\n\tHelper for checking that passed value has passed the type\n\tconstraint.\n\n\tParameters\n\t----------\n\tvalue : np.float\n\t    value to test\n\tdtype : object\n\t    data"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t# create class\n\tclass FillRequestEl(object):\n\n\t\t# method isFill\n\t\tdef isFill(self):\n\t\t\treturn True\n\t\t\n\t\t# method getId\n\t\tdef getId(self):\n\t\t\treturn str(-1)\n\t\t\n\t\t# method saveAsFile\n\t\tdef saveAsFile(self):\n\t\t\treturn False\n\n\t\t# method getAsFile(self)\n\t\tdef getAsFile(self):\n\t\t\treturn False\n\n\t\t# method update(self, param):\n\t\tdef update(self, param):\n\t\t\treturn 0\n\n\t\t# method delete(self"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tprint(\".is_fill_request_el()\")\n\tprint(f\"The method is_fill_request() was called.\", file=sys.stderr)\n\tprint(f\"obj: {obj}\", file=sys.stderr)\n\ttry:\n\t\tprint(f\"The method {obj.__class__.__name__} was called.\", file=sys.stderr)\n\texcept AttributeError:\n\t\tpass\n\ttry:\n\t\tprint(\"The method request was called.\", file=sys.stderr)\n\texcept Attribute"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tobj = str(obj).lower()\n\tif obj == \"fill\": return ('fill', None)\n\tif obj == \"request\": return ('request', None)\n\tif \"fill\" in obj:\n\t\t# this is a generic method\n\t\treturn (\"fill\", eval(obj.partition(' ')[0]))\n\tif \"request\" in obj:\n\t\t# generic method\n\t\t#\n\t\t# e.g. <method '__init__' of '_str_translit' objects>\n\t\t#\n\t\t# should be:\n\t\t#\n\t\t# fill"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tis_fill = True\n\ttry:\n\t\tobj.fill\n\texcept:\n\t\tprint 'Can\\'t get \\'fill\\'. Please use \\'fill_request\\' method.'\n\t\tis_fill = False\n\ttry:\n\t\tobj.request\n\texcept:\n\t\tprint 'Can\\'t get \\'request\\'. Please use \\'fill_request\\' method.'\n\t\tis_fill = False\n\tif not is_fill:\n\t\treturn obj.is_fill\n\treturn obj.is_fill_request\n\nis_fill_request = is_fill_request_el"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn hasattr(obj, 'fill') and callable(obj.fill) and not hasattr(obj,'request')\n\n\n\"\"\"\nFunctions related to the generation of XML documents\n\"\"\"\n\ndef _check_str(string):\n\t\"\"\"\n\tIf the string param is None, it returns the parameter.\t\n\tIf 'x' is None it returns 'y'.\t\t\n\t\"\"\"\n\tif string == None:\n\t\treturn string\n\telse:\n\t\treturn str(string)\n\ndef doc_to_xml(doc):\n\t\"\"\"\n\tGenerates an"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn getattr(obj, 'run_') is not None\n\n\ndef is_run_el_or_file(obj):\t\"\"\"\n\tObject contains executable method 'run_'.\t\"\"\"\n\treturn getattr(obj, 'run_') or getattr(obj, 'run_or_file_') is not None\n\n\ndef is_run_el_or_file_or_var_file(obj):\t\"\"\"\n\tObject contains executable method 'run_or_file_'.\t\"\"\"\n\treturn getattr(obj, 'get_run_or_file_')"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tobj_type = type(obj)\n\tif obj_type is Tuple_types:\n\t\treturn 'run' in obj[1].__name__\n\treturn run in obj.__class__.__name__\n\n\n#-----------------------------------\n#\tFunctions\n#-----------------------------------\ndef get_input_data(inputs):\n\t\"\"\"\t\n\t\tGet the input data which is the one passed by the user to the code.\n\t\t\n\t\tinputs : \t\t\t\t\tData (a list) of tuples/dicts or lists. \n\t\t\t\t\t\tList contains four of the types:"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\ttry:\n\t\texecs = [obj.run]\n\texcept AttributeError:\n\t\texecs = []\n\treturn any(map(lambda f: hasattr(f, 'run'), execs))\n\n##########################\n### Check if a method 'run()' has an associated method'set_job()' in file'model.py'\n\ndef check_if_run_el(func):\n\t\"\"\"\n\tCheck if a method 'run()' has an associated method'set_job()' in file'model.py'\n\t\"\"\"\n\tif not"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn obj.__class__.__name__ == 'Run'\n\nclass Run(object):\n\tdef __call__(self, *args, **kwargs):\n\t\treturn self.execute(*args, **kwargs)\n\nclass Executable(object):\n\t# Defines the parameters of a certain execution in the Python interpreter.\n\tname = None\n\tdef __call__(self, args, kwargs):\n\t\t# Override method of any class instances.\n\t\tself.params = args\n\t\tself.kwargs = kwargs\n\n\tdef execute(self"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif hasattr(obj, 'run'):  #obj[name]\t\n\t\tfor attr in dir(obj):\n\t\t\tif attr[0]!= '_' and attr[-1]!= '_':\n\t\t\t\tif callable(getattr(obj, attr)):\n\t\t\t\t\tif not attr == 'run':\n\t\t\t\t\t\treturn True\n\t\t\t\t\treturn False\n\n\treturn False\n\ndef validate_args_list_inclusive(args):\t\"\"\"\t\n\tList of arguments has to be one-2-many and mutually exclusive.\n\tThis function is to determine"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif sys.version_info[0] == 2 and not hasattr(obj, '__func__'):\n\t\treturn False\n\treturn getattr(obj, '__func__', None) is not None\n\ndef is_run_el_globals(obj):\t\"\"\"\n\tObject contains executable method 'run_globals'.\t\"\"\"\n\tif sys.version_info[0] == 2 and isinstance(obj, basestring):\n\t\treturn False\n\treturn getattr(obj, '__globals__', 0) is not None\n\ndef"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn (type(obj) == classmethod)\n\ndef run(method):\n\t\"\"\"\n\t\tRun method provided by an executable class.\n\n\t\t:param method:   A method of the class. Should have a self as\n\t\t\t\targument.\n\t\t:exception: Exception.\n\t\"\"\"\n\ttry:\n\t\trun_obj = method() # Run the method\n\t\tif (is_run_el(run_obj)):\n\t\t\tprint(colorama.Fore.GREEN+\\\n\t              \t\"[ SUCCESSFUL ] - \\t\\t\"+\\\n\t              \t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn hasattr(obj, 'run') and not getattr(obj, 'hide', False)\n\n\ndef get_runlist():\n\t\"\"\"Returns the executable object as a list, or None if no runlist is found.\"\"\"\n\tif os.path.exists(\"runlist\"):\n\t\twith open(\"runlist\", \"rb\") as fh:\n\t\t\trv = json.load(fh)\n\t\t\treturn rv['runlist']\n\telse:\n\t\treturn None\n\n\ndef runlist_to_xml(obj, *, show_empty_children):"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn _el == object\n\n#\tdef run(self):\n#\t\tprint \"Run!\"\n#\t\tself.__method = getattr(self, \"Run\")\n#\t\t# If the method has been invoked, don't run the method\n#\tdef Run(self):\n#\t\tself.__Run()\t\t\n#\t@classmethod\n#\tdef Register(cls, name, run, func ):\n#\t\tcls.func[name] = func\n#\t\n#\tdef __Run(self):\n#\t\tname = self.__name\n#\t\tset"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn eval(obj[0]) == b\"run\"  # or object contains 'popen' module\n\n\ndef get_popen(obj):\t\"\"\"\n\tReturns binary shell command object.\t\"\"\"\n\treturn obj[0]\n\n\ndef run_command(obj):\t\"\"\"\n\tExecutes binary shell command passed as arguments.\t\"\"\"\n\treturn run(obj[0])\n\n\n#\n#  Methods to handle binary shell commands\n#\n\ndef add_run(name, binary):\t\"\"\"\n\tStores the name and path of input binary command as a tuple.\t"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn callable(obj.fill)\t\n\nif hasattr(__builtins__, '__file__'):\t__file__ = __builtins__.__file__\n__file__ = __file__.replace('.c','_compute.c',1)\t\t# replace '.c' with '.cpp'\nexec(__file__, globals())\n\t\n\t\n\t\nif __name__ == '__main__':\n\t###\tToDo\tCheck if 'pyramid' can be changed as Python.\t\n\t###\t\tUse 'eval' as the"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\nclass FillSolver:\n\tdef __init__(self):\n\t\t\"\"\"\n\t\t\tInitialization.  You should not need to assign anything.\n\t\t\tAny of these methods can be called without any parameters.\n\t\t\"\"\"\n\t\tself.solver = _S_FillSolver()\n\t\tself.fill = _S_Fill()\n\t\tself.fill_fun = _S_Fill_FUN()\n\t\tself.solve = _S_Solve()\n\t\tself.compute_fun"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn any((hasattr(ob, 'compute'), hasattr(ob, 'fill')))\n\nfiller = object()\t# a filler value for the fill function\ncompute = object()\t# a computation value for the fill function\ndef _fill_compute(obj):\n\t# obj is passed to the 'compute' function; it is a variable type, and\n\t# must be defined before fill() is defined and a value of fill() could\n\t# be obtained.\n\tfill(obj, f(obj))\n\nfill = def_func(_fill_comp"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif not hasattr(obj, 'fill_'):\n\t\treturn  # Not a valid fill executable, return a string\n\n\t_mf = object()\n\t_comps = [_mf]\n\t_pops = [_mf, _mf]\n\t_ops = ['__contains__', '__eq__', '__lt__', '__gt__', '__le__', '__ge__']\n\t_num_sizes = [0, 1, 2, 3, 4, 5, 6]\n\t_attrs = ['compute"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn \"__fill__\" in dir(obj) and obj.__class__.__name__ in (\"Fill\", \"Compute\")\n\n############################################################################\n#                                                             Misc.\n############################################################################\n\t\ndef parse_string_to_list(s):\n\t\"\"\"\n\t\tThis function parse a string to a tuple containing the\n\t\tsplit string separated by whitespace.\n\t\"\"\"\n\tl = []\n\tfor s in s.split():\n\t\tl.append(s)\n\treturn tuple(l)\n\ndef"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\texec_obj = getattr(obj, 'exec_', False)\n\texecs = [exec_obj] if exec_obj is not False else get_names(obj,'methods')\n\tfor exec_name in execs:\n\t\texec_obj = getattr(obj, exec_name, '')\n\t\texec_obj = _get_exec_and(exec_obj)\n\t\ttry:\n\t\t\texec_obj()\n\t\texcept Exception as e:\n\t\t\tlog('Error while'+exec_name+': {}',''.join(e.args))\n\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif obj.type == 'PIPE':\n\t\tpipes = obj.inputs\n\t\tif len(pipes)!= 2:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn obj.is_fill() or obj.is_compute()\n\ndef is_fill_model(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn obj.is_fill_model()\n\ndef is_fill_model_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn obj.is_fill_model_el()\n\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tobj._is_compute_compute_el = 0\n\treturn obj._is_compute_compute_el == 0\n\ndef is_fill_runnable_methods_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and 'run'.\t\"\"\"\n\tobj._is_runnable = 1\n\treturn obj._is_runnable == 1\n\ndef is_fill_runnable_methods_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and 'run'.\t\"\"\"\n\tobj._"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tfor name in dir(obj):\t\t#This allows the process to pass along methods.\t\n\t\tif name[0]!= '_':\t\t#No other non-command identifiers.\t\n\t\t\tif callable(getattr(obj, name)):\n\t\t\t\tif ( name == 'fill' ) or ( name == 'compute' ):\n\t\t\t\t\tyield name, getattr(obj, name)\n\n#################################################\n#\tCONVERT METHODS TO LIST\t\t             \n#################################################\n\n# Converts a Numeric object to an List.\t\ndef listify(obj,"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\td = {} if level>0 else {}\n\td.update(difference(d1, d2, level+1))\n\treturn d\n\ndef difference_update(d1, d2):\n\t\"\"\"Update dictionary *d1*, which is a sub-dictionary *d2* plus\nits keys.\"\"\"\n\tfor k in d1:\n\t\td1[k] = difference(d1[k], d2, -1)\n\treturn\n\ndef intersection(d1, d2, *args, **kwargs):\n\t\"\"\"'Inverse to"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tfrom copy import deepcopy\n\td1 = d1.copy()\n\td1.update(d2)\n\tif level is not None:\n\t\tdeepcopy(d2, d1, level+1)\n\treturn d1\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tresult = {**d1, **d2}\n\tif level > 0:\n\t\toldkeys = newkeys = d1.keys() ^ d2.keys()\n\t\tfor key in oldkeys:\n\t\t\tnewdata = d1[key] - d2[key]\n\t\t\tif not newdata:\n\t\t\t\tdel result[key]\n\t\t\telse:\n\t\t\t\tresult[key] = newdata\n\treturn result\n\n################################################################################\n#####\u00a0MAIN\u00a0FUNCTION\t<--- Main Function\t                            <----\n################################################################################\n\nif __name__ == '"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif type(d1)!= type(d2):\n\t\treturn difference(d1, d2, level + 1)\n\tres = dict([elt for elt in d1 if elt not in d2])\n\tif level:\n\t\tres.update(difference(d1.get(level, []), d2.get(level, []), level + 1))\n\treturn res\ndef intersection(d1, d2, level=-1):\n\t\"\"\"Return a dictionary with items common to *d1* and *d2*.\n\n*level* sets the"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\treturn difference(d1.copy(), d2,\n\t        level=level+1)\n\ndef intersection(d1, d2, levels=0, depth=-1):\n\t\"\"\"\n\tIntersect two dictionaries, preserving the ordering\n\t(or the least common super type).  Items found in both\n\td1 and d2 are added to the result.\n\n*d1* and *d2* remain unchanged. The return value may be two\ndictionaries in the standard way; or alternatively they may\nbe modified to maintain order and return a new dictionary.\n\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif level <= -1:\n\t\tlevel = -1\n\td1bk = d1.copy()\n\td2bk = d2.copy()\n\n\tif not d2:\n\t\tfor d in [d1, d1bk]:\n\t\t\tdiff = {x:x for x in d.keys()} in d\n\t\t\tfor x in d.keys():\n\t\t\t\tif x in d2:\n\t\t\t\t\td[x] = d[x] + d2[x]\n\t\t\t\t\tdiff = not diff\n\t\t\t\tif not diff:\n\t\t\t\t\tdel d[x]"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tresult = {}\n\tif type(d1) is not dict or type(d2) is not dict:\n\t\traise TypeError('d1 and d2 must be dictionaries')\n\n\tif level > -1:\n\t\tstack = [(d1, d2)]\n\t\twhile len(stack) > 0:\n\t\t\t(d1, d2) = stack.pop()\n\t\t\tif d1 in d2:\n\t\t\t\tfor k, v in d1.items():\n\t\t\t\t\tif k not in result:\n\t\t\t\t\t\tresult[k] = v\n\t\t\t\t\tif v not in d2"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif level == 0:\n\t\treturn {} if len(d1) == 0 else \\\n\t\t{xrkc : d1[xrkc] for xrkc in d1 }\n\n# \td = deepcopy_dict(d1)\n\tif level:\n\t\tk_vals = []\n\t\tif k_set.intersection(d1):\n\t\t\tk_vals = d1.keys()\n\t\tfor k in d:\n\t\t\tif k_val is not None and k in k_vals:\n\t\t\t\td[k] = None\n\t\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\tresult = {}\n\tfor k, v in d1.items():\n\t\tif  level >= 1:\n\t\t\tif k in d2 and (d1[k]!= d2[k]):\n\t\t\t\tresult[k] = difference(d1.get(k, None), d2.get(k, None), level + 1\n\t\t\t\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\t\telse:\n\t\t\tresult[k] = v\n\treturn result\n\t\n\t\n@PY3K\ndef diff(*args):\n\t\"\"\"\n\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\timport copy\n\n\tif level >= 0:\n\t\tif not d2:\n\t\t\treturn difference(d1, {})\n\t\tresult = {}\n\t\tfor k in d1:\n\t\t\tif k not in d2:\n\t\t\t\tif len(d1.get(k)) == 0:\n\t\t\t\t\tdel d1[k]\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = difference(d1[k], {})\n\n\t\t# do not return d1, but d2.\n\t\treturn result or d2\n\n\td1_deep = d1 is not None\n\td2_"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tind = coord[0]-0.5*(coord[2]-self.edges[0][1])\n\tif coord[1] < self.edges[0][0]:\n\t\tind += 0.5*(self.edges[1][1]- self.edges[0][1])\n\tfor i in range(0, len(self.edges[1])-1):\n\t\tif ind >= self.edges[1][i] and ind < self.edges[1][i+1]:\n\t\t\tself.hist[coord[0]]["}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tfor x in self._ax[coord].flatten():\n\t\tif x < 0:\tx = x + len(self._ax[coord])\n\ti = 0\n\tfor x in self._ax[coord].flatten():\n\t\tif weight * abs(x) < 0:\n\t\t\ti += x\n\t\t\tself._ax[coord][i] = 0\n\t\t\tself._h[coord][i] += 1\n\t\telse:\n\t\t\t# histogram edge\n\t\t\ti += 1\n\treturn [i, x]\n\n# -----------------------------------------------------------------------\n#\n# Classes\n#\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.data[coord, :] = weight*self.data[coord-1:coord+1, :]\n\tself.bins[coord, :] = self.hist_data + (weight*self.bins[coord-1:coord+1, :]-self.bins[coord-1:coord+1, :].min())\n\ndef smooth(self, scale=1.0):\n\t\"\"\"\n\tMultiply data by a scaling factor and add some noise.\n\n\tThis is a smoothed version of the weighted arithmetic (histogram\n\tweighted"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif coord not in self.bin_edges[0]:\n\t\tlogger.debug(\"ERROR while filling histogram with incomplete interval (bin is not set %f)\" % coord)\n\t\treturn\n\n\tif coord in self.inliers:\n\t\t# already inliers\n\t\tif self.inlier_weights[self.inliers.index(coord)] == weight:\n\t\t\treturn\n\t\t# make a copy because no-copy data\n\t\thist = np.copy(self.weights[self.inliers == coord])\n\t\tself.inliers[self.in"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\treturn fill_ndarrays(coord, self.dims, self.data, weight=weight)\n\ndef filled_hist(self, coord, weight=1, edgecolor=None, alpha=1, cmap='RdYlBu',\n\t\t\t\tvmin=None, vmax=None, fill_value='black', xscale=None, yscale=None,\n\t\t\t\tclip=(0, 1), **kwargs):\n\t\"\"\"\n\tMake a histogram of the values at *coord* and with the given *weight*.\n\t\n\tThe bars and patches extend on"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif coord < 1 or coord > self.count():\n\t\treturn\n\tif self[coord] < weight or weight <= 0:\t\t\t\t\t\t\t\t### if the histogram weight is <= 0...\n\t\tself[coord] = 0\n\telif weight < self[coord]:\n\t\tself[coord] = weight\t\t\t\t\t\t\t\t\t\t\t### if the histogram weight is lower than the histogram weight\n\telse:\n\t\tself[coord] = max(0, self[coord] - weight)\n\ndef histsum(hist, norm=None):\n\t\"\"\"\n\tReturn a cumulative sum of the values"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t# This function fills a histogram with the given *weight* and\n\t# calculates the edges of the bin.\n\tif weights is not None:\n\t\tweight = numpy.asarray(weights, float)\n\tif not weight.flags.contiguous:\n\t\tweight = weight.copy()\n\t\tweight = numpy.append(numpy.inf, weight)\n\tcentre = coord + 0.5 * (coord[1] - coord[0])\n\t#print centre\n\t#print coord\n\t#print weight\n\twidth = 0.01 * (coord["}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\treturn fill(coord, self[:,coord], weight)\n\n########################################################################\ndef max(self, axis=None):\n\t\"\"\"Return the maximum of each row of the distribution.\"\"\"\n\tmaxVals, _ = histogram(self.flatten(), normed=True, weights=None)\n\tmaxIdx = zeros(len(maxVals), int)\n\tfor tidx in arange(len(maxVals)):\n\t\tmaxIdx[tidx] = argsort(-ravel(maxVals[0:tidx]))\n\t"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.data.append([])\n\ttry:\n\t\thistogram = self.stat\n\texcept AttributeError:\n\t\thistogram = self\n\txvals = list(histogram.xlow) + [c+1 for c in coord] + list(histogram.xup)\n\tfor i in range(len(xvals)):\n\t\tif xvals[i] < 0 or xvals[i] > histogram.xbins.shape[0]-2:\n\t\t\tcontinue\n\t\tif ((not histogram.xbins.has_data(xvals[i"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\treturn sum(self.d.hist(weight=weight, x=coord).flatten())\n\ndef get_bin_edges(hist, hist_weight=1):\t\"\"\"\n\tReturn *hist_weight* bin edges.\n\n\tReturns bin edges of *hist*.  If the histogram has a weight,\n\tthis function also returns the bin edges of the weighted\n\thistogram.\n\t\"\"\"\n\timport numpy as np\n\tw = max(hist.flatten())\n\tif w==0:        # empty bucket\n\t\treturn [0], [0,"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t# Validate labels against the regex given.\n            for lbl in labels.keys():\n                if not _is_match(labels[lbl], regex):\n                    raise\n\n\nclass BaseDecoder:\n\tdef parse(self, output_file):\n\t\traise NotImplementedError\n\n\nclass BaseEncoder:\n\tdef __init__ (self, **kwargs):\n\t\tself.format = \"json\"\n\t\tself.output = None\n\n\tdef parse(self, output_file):\n\t\t\"\"\"\n\t\tParse a given path to an output file.\n"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t    errors: dict = {}\n\t    for label in labels:\n\t\tregex = label.get(label.rfind(\".\"))\n\t\tif regex is None:\n\t\t\terrors[label] = 'expected string-like object'\n\t\telif regex.findall(labels[label]):\n\t\t\terrors[label] = 'expected string-like object'\n        else:\n            errors[label] = 'valid string-like object'\n            for regex in re.finditer(regex, labels[label]):\n                if regex.group():\n                    messages:"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\n\tdef _is_valid_key(k, error=\"\"):\n\t    if not error:\n\t\treturn True\n\t    if \"_\" in k:\n\t\treturn False\n\t    error += f\"- '{k}' found in an unexpected format. (Must be a valid key.) (Regex '[{_RE_LEGACY_KEY_PATTERN}]' + is optional)\"\n\t\n\t# Check values in the labels dictionary separately.\n\tfor label, value in labels.items():\n\t\tif label.find(\"_\") > -1:\n\t\t\traise Val"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\traise NotImplementedError(class_name +'must implement.')\n\n\tlabels = dict((k, v) for k, v in labels.items() if isinstance(v, type(None)) == False)\n\tlabels = dict((k, v) for k, v in labels.items() if isinstance(v, list))\n\tlabels = dict((k, v) for k, v in labels.items() if any(isinstance(e, tuple) for e in v))\n\n\tif labels!= dict():\n\t\t# Check the type of the given"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tif not _is_label_valid(labels):\n            return\n\tfor k,v in labels.items():\n\t    match = (\n\t\tre.match(r\"\"\"((?:(?:\\w|\\d+\\.\\d+)\\s*\\(?))(?:\\|\\\\\\'|\\\\\\w+\\||(?:[^\\\"\\'])\\|\\\\\\w+)?\\s*\\S+\\s*\\|?(?:\\|\\\\\\w+)(?:\\|\\\\\\w+)?)\\s+(\\S+\\s*\\"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tif \"name\" not in labels: return labels\n\tfor key, label in labels.items():\n\t    if not isinstance(label, string_type):\n\t        raise ValidationError(\"labels must be a dictionary containing a list of values.\")\n\t    for regex, message in label.items():\n\t        if not re.fullmatch(regex, key):\n\t            raise ValidationError(message)\n\treturn labels\n\ndef _parse_labels(labels):\n\t\"\"\"Determine whether to check the labels.\n\n    Args:\n        labels"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\treturn False\t# Default behavior\n\n\t\ndef _label_match(labels, label):\t\n\t\"\"\"\n\tHelper method which validates that the input argument has the given label, valid.\n\t\n\tNOTE: this function can be used to validate all the labels defined in the input\n\tlabels arguments. This function uses recursion, so to get the same labels, use\n\tthe same label function as another function calls a validator.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t    label (any): the label to validate."}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t# TODO - check if all keys are also keys in the given labels\n\t# for keys, values, valid and invalid in labels, check errors only for valid ones\n\t# the messages contain just the error messages the list contains, in addition\n\t# to the error messages\n\t# for each regular expression search the key.value, as regular expressions\n\t# and strings of keys against the matching regular expressions\n\t# check if the keys match\n\t# raise error if not true-false errors, key matches should either only one\n\t# key on a given label, one value that is either a string"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tvalid = False\n\tfor key,label in labels.items():\n\t\tr_p = re.compile(type(key).__name__ + \":*\" + type(label).__name__)\n\t\tif (r_p.match(key)):\n\t\t\tprint(key,'invalid')\n\t\t\tvalid = True\n\t\telif (type(label)!= list):\n\t\t\tprint(key,'is not a list type')\n\t\t\tprint(label.message)\n\t\t\tvalid = False\n\n\tif (not valid):\n\t\traise ValidationError(labels)\n\n"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t# check error messages for each label key/value pair\n\tfor key, value in labels.items():\n\t\tif isinstance(value, str):\n\t\t\ttry:\n\t\t\t\tre.compile(value)\n\t\t\texcept re.error:\n\t\t\t\t# this could be a bytes() error in python3\n\t\t\t\t# we have to check for this a second time to make sure it matches\n\t\t\t\tpass\n\t\t\texcept TypeError:\n\t\t\t\tassert False, value\n\t\telif key in labels and isinstance(value, (str, bytes)):\n\t\t\tassert key not in labels or value not in labels"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn(\n\t\tre.compile(r'([a-zA-Z\\-\\.:\\[`\\(\\)_\\-!()\\/;\\],]+)\\.krake(?:-r\\d+|-r\\d+)\\.krake([a-zA-Z\\$\\-\\%\\:\\,\\(\\)\\]]?)', re.I),\n\t)\n\ndef _get_krakel_resources():\t\t# Internal method for accessing Krakel resources.\n\t\"\"\"Returns a list of resources from Krakel.\"\"\"\n\tkrakel_resources"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn None\n\n\ndef _is_resource_name_valid(result, resource_name):\n\t\"\"\"\n\tValidate the input value and return whether or not it has the proper\n\tregular expression.\t\n\n\tArguments:\n\t\tresult (str): the value to match against the regular expression\n\t\tresource_name (str): the resource name to verify against.\n\t\"\"\"\n\treturn match(result, _get_resource_name_regex(resource_name))\n\n\n_resource_name_regex = re.compile(r'^(-?\\d+)$')"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t# Reject the special characters { and } in resource name.\n\treturn re.compile(r'[{:<>|][\\/{}()\"][$_]+')\n\n\ndef _get_resource_names(data_table):\n\t\"\"\"\n\tGet the list of resource names.\n\n\tArgs:\n\t\t(str): data_table: a text table describing the information\n\t\tabout the resource (each row is a document)\n\n\tReturns:\n\t\t(list): list of resource names as strings.\n\t\"\"\"\n\t# Build the regex that will get all the name"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\t# Build the list of regular expressions\n\tregex_list = []\n\n\t# Get the pattern object\n\tpattern = re.compile(\"^{{([^}]*)\\})$\", re.VERBOSE)\n\t\n\t# Add any whitespace to the end to match anything that comes out of the template\n\tpattern = '{{((^|^)(?:                                    |(?:\\r\\n)+)\\s*)\"' + pattern + \"}\"\n\n\t# Loop through each element in the string, which will be matched, and\n\t# compile any regular expressions"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t# Build regular expressions.\n\tresources = [\"resources\"]\n\tif \"krake-resources\" in Config.get_variable(key=\"Environment\").upper():\n\t\tresources.append(\"resources\")\n\t\tresources.append(re.escape(Config.get_variable(key=\"Environment\")))\n\n\treturn re.compile(\"|\".join(resources), re.UNICODE)\n\n\ndef _build_name_match(re_pattern, name_regex_pattern):\n\t\"\"\"\n\tBuild or return the compiled regular expression for the match of a name. \n\tUsed"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn re.compile('^.*(\\d+)[A-Z].*$')  # Example output: '000111'\n\n\ndef _validate_path_name(name, valid_paths):\n\t\"\"\"\tValidate a resource's path (i.e., resource name).\n\t\n\tPossible inputs:\n\t    * `name`\t\t: The resource name (string) (e.g., '1f5').\n\t\t\t\t\t   Allowed values must match the _get_resource_name_regex() call.\n\t\t\t\t    For a resource name that starts with"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tvalidator = re.compile(r'\\w+_?(?:[0-9]|[_a-z]+)(_)?[0-9]*)$')\n\treturn validator\n\ndef _get_resource_class(resource, name=None, parent_class=None):\n\t\"\"\"\n\tBuild a `ResourceClass` instance from the name and all parent classes of\n\t`resource` and the name of the parent class's resource class.\n\n\tThis function will use the `GetClassNameFromResourceName()` logic\n\tgenerated by `get_class"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn _get_resource_name_regex_mappings()\n\n\n\ndef _get_resource_name_regex_mappings():\t\"\"\"\tBuild or return the compiled regular\n    expressions for matching resource name.\t\"\"\"\n\tresource_name_regex_mappings = []\n\tfor pattern, groups in _resource_name_replacements.items():\n\t\tif isinstance(pattern, tuple):\n\t\t\tt = {}\n\t\t\tfor subpattern in pattern:\n\t\t\t\tt[subpattern] = '(' + subpattern + ')'\n\t\t\tfor group in groups:\n"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn { 'name': re.compile(r'([a-z\\d\\-\\_\\_\\.\\:\\.])', re.IGNORECASE),\n\t\t\t'formatName': re.compile(r'([a-z\\d\\-])([a-z\\d\\-\\.])?$'),\n\t\t\t}\n\n\n\n# -----------------------------------------------------------------------------\n\n# Functions for defining and returning validator functions.\n\n# -----------------------------------------------------------------------------\n\n\ndef validate_format(d, name=None, filename=None):\n\t\"\"\"\n\tCheck that the d structure and the filename provided are"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tpass\n\n\ndef _read_xsl_file(path):\t\"\"\"\n\tRead the text file and convert it to a list of\n\tsubstrings.\t\"\"\"\n\tpass\n\n\ndef _write_json_file(path):\t\"\"\"\n\tWrite the file to a file and convert it to a JSON.\t\"\"\"\n\tpass\n\n\ndef _write_xml_file(path):\t\"\"\"\n\tWrite the XML root element to a file and convert it to\n\ta JSON.\t\"\"\"\n\tpass\n\n\ndef _write_yaml_file(path):\n\t\"\"\"\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    if len(value) <= 4:\n        raise ValidationError(\"Field '{}' has the invalid length: \"\n                              \"it must be a maximum of 5.\".format(value))\n    matched_chars = re.findall(r\"(?:\\d|[a-z])[^\\d\\w]*|\\d\", value)\n    if len(matched_chars) > 1 and len(matched_chars) == int(len(matched_chars)) and int(len(matched_chars)) < 4:\n        raise Val"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tregexp = re.compile(fr'^[^\\s]*({value})[^\\s]+\\s\\1*$','re.IGNORECASE')\n\tif(re.match(regexp, value)):\n\t\treturn 'Valid'\n\telse:\n\t\t\traise ValidationError('Invalide input value')\n\n@app.route('/add', methods=['POST'])\ndef add_item():\n    \"\"\"\n    Receive the POST request data from a JavaScript query string.\n    Store the incoming string as input variable."}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tmatches = re.search(value, value, re.MULTILINE)\n\tif matches:\n\t\traise ValidationError(\"{} is not a regular expression.\".format(value), code=ValidationError.INVALID)\n\t\n\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\treturn re.fullmatch(value, r'\\w{0,30}$', re.ASCII)\t\n\n###################################################\t\n#\t\t\t\t\t\t      FUNCTIONS   \t\t#\n################################################### \n# TODO: Implement \"validate_size\" to check a string against the following regular expression:\n# ^\\d+\\s$\n#\n# HINT: You'll need this regular expression so that if the \"sz_type\" key is not present, that\n# string is not going to be checked.\t\ndef validate_sz_type(data):"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tregex = REGEXP_VALIDATION_REG\n\ttry:\n\t\tregex.lookall(value, REGEX_TYPE)\n\texcept TypeError:\n\t\traise ValidationError(\"Input data is not a valid format.\")\n\texcept Exception as e:\n\t\traise ValidationError(e)\n\n\ndef validate_type(type):\n\t\"\"\"\n\tValidate the given type against the corresponding regular expression.\n\t\nArgs:\n    type: the string to validate against a string.\t\n\nRaises:\n    ValidationError: if the given type is not conform"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\ttry:\n\t\tm = re.search(r'\\b[0-9]+\\b', value)\n\texcept re.error:\n\t\traise ValidationError('Not a valid number')\n\t\treturn [ \"Invalid\"]\n\tvalid = \"\"\n\ttry:\n\t\tif m.group(0)==value:\n\t\t\tvalid=\"Valid\"\n\texcept ValueError:\n\t\tvalid=\"Invalid\"\n\treturn [valid]\n\n\ndef remove_value(value):\t\"\"\"\tUtility function which removes the given value.\n\n\tA value can be passed in as a string in"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tm = re.match(\"^{}$\".format(value), value)\n\tif m:\n\t\tif (\tre.search(\"[^A-Z]\", m.group(0)) \n\t\t\tor (m.group(0) in [\"-\",\"-e\",\"-I\",\"/\",\"*\",\"%\"]) \n\t\t\t\t) and len(m.group(0)) < 6 :\n\t\t\traise ValidationError(\"invalid phone number format\")\t\n\t\telse :\n\t\t\traise ValidationError(\"phone number should follow the format: XXXX-XXXX-XXXX\")\t\t\n\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t#TODO: Check the format\n\tpass\n\n\nclass ValidateTextElement(object):\n\n\tdef __init__(self, xpath, elements, label, value, description):\n\t\tself.xpath = xpath\n\t\tself.elements = elements\n\t\tself.label = label\n\t\tself.value = value\n\t\tself.description = description\n\n\tdef validate_data(self, value):\n\t\t\"\"\"\n\t\tValidates the given value against the corresponding data type.\n\t\tNOTE:\n\t\tThis function is only called for type: \"text\" and \"select\"; types"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t# Use the validateRegex method for python2\n\t# This code exists so that we can check syntax after running some\n\t# code.\n\ttry:\n\t\tis_valid = re.compile('^-?(?=%s)' % value).match(value)\n\texcept Exception, err:\n\t\traise ValidationError(err)\n\n\tif is_valid is None:\n\t\traise ValidationError('invalid literal for validateRegex: %s' % value)\t\n\t\n\treturn True\t\n\t\n\ndef validate_date(value, valid_"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t# Create regular expression\n\t# to match a particular part of value\n\tpat = re.compile(r'^(?:[^'+match[0]+'])'+match[1], re.M)\n\n\t# Check whether the regular expression matches\n\tif pat.fullmatch(value): return\t# Matches if one or more of the parts matches\n\traise ValidationError(\"Validation errors: %s\" % pat.pattern)\t\n\n\ndef validate_value_list(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tres = re.match(key, key)\n\tif res is None:\n\t\traise Error(\"Invalid key\")\n\treturn res\n\nclass Key:\n\t\"\"\"A container for a key string.\"\"\"\n\t\n\tdef __init__(self, key):\n\t\t\"\"\"Create a Key object.\n\n\t\tThe key can be either a string or a callable that returns a bool.\t\"\"\"\n\t\tif not callable(key):\n\t\t\traise Error(\"key must be a callable\")\n\t\tself.key = key\n\n\tdef __call__(self):\n\t\t\"\"\"Apply the"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t# TODO: validate the given key according to the corresponding regular expression.\n\t#raise Exception(\"No ValidationError Exception\")"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t# valid for the given key\n\tif not re.match(PATTERN_KEY, key) is not None:\n\t\traise ValidationError(\"Key must conform to the regular expression: \\\"\" + PATTERN_KEY.format(key) + \"\\\"\")\n\t"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tvalid = key.string.strip().lower()\n\tif len(key)>1 and valid.endswith(\"*\"):\n\t\tallowed = re.compile(key[1:len(key)]).match\n\t\tif not allowed(key):\n\t\t\tval_error = \"{} is invalid!\".format(key)\n\t\t\traise ValidationError(val_error)\n\treturn key\n\n\t\n\nclass FileObject(object):\n\tdef __init__(self, file_object):\n\t\t\"\"\"\n\t\tInitialize the class.\n\t\t\n\t\tArgs:\n\t\t\tfile"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tresult = re.match(regexp + \"\\b\", key) # return object of a pattern\n\tif not result:\n\t\traise ValidationError('key cannot be {}.'.format(key))\n\t\n\treturn result\n\ndef validate_value(value):\n\t\"\"\"\n\tValidate the given value against a regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tresult = re.match(regexp, value) # return object"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tkey = str.upper(key)\n\tmatch = re.match(exp(key), key)\n\t\n\tif match is None:\n\t\traise ValidationError('Key must match {}'.format(key))\n\t\n\t# if match.groupdict()['valid']!= 'Y':\n\t# \tvalid = match.groupdict()['valid'];\n\t\t\n\treturn match.groupdict()['valid']\n\n\ndef validate_key_and_value(key, value, validate_func):\n\t\"\"\"\n\tValidates the given key-value pair against the"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not isinstance(key, string_types):\n\t\traise TypeError('key must be type string')\n\tif not re.match(r'^[^/-]+$', key):\n\t    raise ValueError('key must conform to regexp')\n\t\ndef is_number(x):\n    try:\n\tfloat(x)\n    except:\n\ttry:\n\t    int(x)\n\texcept:\n\t    return False\n    return True\n    \nclass DateObject(object):\n\t\"\"\" Object storing date and time information for an object.\n        "}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t# If a regular expression does not validate well enough, raise a error.\n\tif not re.escape(key).match(key):\n\t\traise ValidationError(\"The key must be a valid key\")\n\ndef validate_keyid(keyid):\n\t\"\"\"\n    Validate the given key id against a corresponding regular expression.\n\n    Args:\n    keyid: the key id as a string\n\t\t\t\t\n    Raises:\n    KeyError: if the given keyid does NOT start with \"1:\", or if it already exists.\t\n\t\t\t\t\"\"\"\n\n\t# If"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t# define regular expressions.\n\t# to help us determine how to handle validations for the match/regex\n\t# both match/search and sub pattern of regular expressions.\n\tregexes = {\n\t\t\"A\" : {\n\t\t\t\"pattern\": \"^[a-z]+\",\n\t\t\t\"result\": 'A'\n\t\t},\n\t\t\"B\" : {\n\t\t\t\"pattern\": \"^[0-9]+$\",\n\t\t\t\"result\": 'B'\n\t\t}\n\t}\n\n\t# perform regular expressions matching on the given string and get the corresponding Python"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\ttry:\n\t    return re.compile(key).match(key,None)\n\texcept TypeError:\n\t    raise TypeError(\"The key must be a string.\")\n\texcept AttributeError:\n\t    raise AttributeError(\"The key must be a string.\")\n\texcept re.error as e:\n\t    # An error message will be constructed from the given error\n\t    msg = u'The key does not conform to the regular expression: {0}'.format(e[0])\n\t    raise ValidationError(msg,data=key,params={\""}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tobservers = manifest_dict['observers'] if first_level else manifest_dict['manifest']\n\tfor observer in observers:\n\t\tobserver_type = get_observer_type(observer)\n\t\tif observer_type:\n\t\t\tobserver_schema = {}\n\t\t\tobserver_schema[observer_type] = observer \n\n\t\t\tif first_level and observer_type not in observer_schema:         # recursive structure\n\t\t\t\tgenerate_default_observer_schema_dict(manifest_dict['sub']['observers"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\treturn {'manifest': {i: str(manifest_dict[i]) for i in manifest_dict if i!= 'MANIFEST'}}\n\ndef add_default_manifest_schedule_list(self, manifest_list=None, first_level=False):\n    \"\"\"\n    Like :func:``add_default_observer_schema_list``, this function simply replaces\n    empty fields which are not specified in the manifest file (only ``manifest_list``).\n    \"\"\"\n    if first_level:\n        self.update(generate"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tobserver_schema_dict = {}\n\tif isinstance(manifest_dict, collections.OrderedDict):\n\t\ttry:\n\t\t\t# Dictionary with nested lists\n\t\t\tif first_level:\n\t\t\t\titem_type = next(iter(dict(manifest_dict)))\n\t\t\t\tif not isinstance(manifest_dict[item_type], collections.OrderedDict):\n\t\t\t\t\tobserver_schema_dict[item_type] = manifest_dict[item_type]\n\t\t\t\t\tmanifest_dict.pop(item_type)\n\t\t\t# Dictionary with nested lists"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tobserver_schema_list = []\t\n\tobj_dict = {}\n\tobj_data = {}\n\t# The dictionary is based on the definition of the resources.\n\tfor component in manifest_dict:\t\t\t\t\t\t\t\t\t\t\t\n\t\tresource_definition = manifest_dict[component]\n\t\tresource_definition['observer'] = '''\\\n# This is a list of valid resources, as defined by the schema files.\n\t'''\n\t\tresource_definition['observer']['spec'] = '''\\\nresources:\n\t'''\n\t\t'''\n\t\tThe `"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t# Check validity of manifest_dict and manifest_string\n\t# If error, return None\n\t# If valid, create a new dict and return it\n\t# If invalid, return None\n\t# print(manifest)\n\tif not isinstance(manifest_dict, dict):\n\t\treturn None\n\tif not isinstance(manifest_dict, dict):\n\t\treturn None\n\tif not first_level: return None\n\tfor field, value in manifest_dict.items():\n\t\tif not isinstance(value, dict):\n\t\t\treturn \"Missing type: {}\".format"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tobserver_schema = {\n\t\t('app', 'api'): None,\n\t\t('app', 'api','read'): {},\n\t\t('app', 'core','read'): {},\n\t\t('apps', 'api','read'): {},\n\t\t('api','read'): None,\n\t\t('app','static','read'): {},\n\t\t('metadata', 'labels'): {},\n\t\t('metadata', 'ownermetadata','read'): None,\n\t\t('service','spec'): None,\n\t\t('resources',"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tdefault_observer_schema = {}\n\tnon_list_fields = set(['node', 'pod_name', 'pod_spec', 'persistent_volume_claim', 'ingress'])\n\n\t# First, replace all non-list and non-dict:\n\tfor name in non_list_fields:\n\t\tvalue = manifest_dict[name]\n\t\t# If it is a list, the list is assumed to be the complete schema:\n\t\tif isinstance(value, list):\n\t\t\t# This must be empty, meaning the manifest is still consistent \n\t\t\t#"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\tobserver_schema = {}\n\t\n\tif \"namespaced\" in manifest_dict.keys():\n\t\tobserver_schema[\"namespaced\"] = manifest_dict.pop(\"namespaced\")\n\t\n\tobserver_schema_str_no_tokens = \"\"\n\t\n\tobserver_schema_str_no_tokens += \"resource_group\"\t\t# Add `resource group` to the schema\n\tobserver_schema_str_no_tokens += \"kubernetes_kind\"\t\t# Add `kuber"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tresult = {}\n\tif first_level:\t\t# in case the manifest contains only a schema (key: value).\n\t\tfor key,value in manifest_dict.items():\n\t\t\tif type(value) is list:\n\t\t\t\tresult[key] = [generate_default_observer_schema_dict(i,first_level) for i in value]\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\t\treturn result\n\telse:\n\t\tif \"@context\" not in manifest_dict:\n\t\t\treturn dict(result)\n\n\t\tfrom urllib.request"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\td = defaultdict(dict)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\tif first_level and \"observer_schema\" in manifest_dict: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tfor key, value in manifest_dict.items():\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\tif \"default\" in key:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tvalue = manifest_dict[key][\"default\"]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tif isinstance(value, dict):\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\td[key][\"default\"] = generate_default_observer_schema_dict(value, first_level=True)\t\n"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n# end update_last_applied_manifest_from_resp\n\n\n# end def update_last_applied_manifest\n\ndef update_last_applied_manifest_dict_from_resp(\t\"\"\"\n    Together with :func:``update_last_applied_applied_manifest_dict_from_resp``,\n    this function is called recursively to update a dictionary of ``last_applied_\n    manifest`` from a partial Kubernetes response\n\nArgs:\n    last_applied_manifest_dict"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t, resp, schema_path,\n\t, lambda: last_applied_manifest,\n\tlambda: observer_schema,\n\tlambda: response)\n\tasync def create_next_applied_latest_manifest(\n\tobs_name: str, updated_manifest: dict,\n\tobserver_schema: schema.List[Any], response: list,\n\tschema: schema.List[Any]}\n\nasync def create_next_applied_manifest_list_from_resp(observer_schema: schema.List[Any"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\ndef update_last_applied_manifest_list_from_resp_dict(\t\"\"\"\t\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp_dict``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes API response dictionary\n\t\n\tArgs:\n\t\tlast_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t\t\t\tupdated\n\t\tobserver_schema (dict):"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\tdef update_last_applied_manifest_dict_from_response(self,\n\t    last_applied_manifest,\n\t    observer_schema,\n\t    response\n\t):\t\n\t\tvalue_for_applied_manifest = response.get('last_applied_manifest', {})\n\t\tfor obj in value_for_applied_manifest:\n\t\t\t# If a key exists in the current object\n\t\t\t# Add the corresponding value if not present\n\t\t\t# In the second iteration, change the object \n\t\t\t# if"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t#TODO remove these helper codes at some point. \n\t\n\tdef append_partial_manifest(\tresp_manifest, manifest):\n\t\t\t#TODO Remove this part: update the value in last_applied_manifest. \n\t\t\tif manifest not in resp_manifest: \n\t\t\t\t\t\t\t\t\t\t\tresp_manifest += manifest\n\t\t\treturn resp_manifest\n\t\n\tdef get_observer_schema_from_resp(resp):\n\t\tobserver_schema = set()\n\t\tfor x in resp['items']:\n\t\t\tobserver"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t  + K8S_API_METHOD_PREFIX + \"update\", last_applied_manifest, observer_schema, response)\n\n\n@K8SMetadata.action(\n    action=lambda_names,\n    metadata=lambda_names[\"metadata\"],\n    code=lambda_names[\"code\"],\n    handler=\"create\",\n    handler_input=lambda_names[\"handlerInput\"],\n    k8s_obj=lambda_obj)\t\ndef create_metadata_value(\t\n        metadata: dict,\n        observer_schema,\n        "}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\treturn update_last_applied_manifest_list_from_resp(**kwargs)\n\n# ------------------------------------------------------------------------------\n# update_last_applied_manifest_dict_from_resp (update_last_applied_manifest_list_from_resp)\n# ------------------------------------------------------------------------------\n\n# ------------------------------------------------------------------------------\n# update_last_applied_manifest_dict(update_last_applied_manifest_list_from_resp)\n# ------------------------------------------------------------------------------\n\n# ------------------------------------------------------------------------------\n# clear_last_applied_manifest (clear_last_app"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t, update_last_applied_manifest,\n\tobserver_schema, jsonapi.observe_list_update_response\n\tfrom flask import current_app\n\t# from flask.json import jsonapi.observe_list_update_response # may only called from main thread\n\t# maybe in the event of multiple threads\n\t)\n\n\ndef update_last_applied_manifest(\n\tlast_applied_manifest: list\n\t) -> None:\n\t\"\"\" Updates ``last_applied_manifest`` from the current app's dict"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t):\n\t\t# update values from the response\n\t\tfor o in observer_schema:\n\t\t\t# retrieve the value from the dict\n\t\t\to_value = getattr(self.last_applied_manifest_info, o[0])\n\t\t\t# if the value is None -> that means it has been updated\n\t\t\t# if not None -> it's not None, but we should do nothing\n\t\t\t# if any other stuff do nothing, that means the value has been\n\t\t\t# already correctly set\n\t\t\tif not o_value:\n\t\t\t\tsetattr(self.last_applied_man"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\ndef update_last_applied_resource_list_from_obj(\t\"\"\"\n\tTogether with :func:``update_last_applied_resource_dict_from_obj``, this\n\tfunction is called for each object being updated on the Kubernetes API's\n\tlast_applied_resource that can be found in the list of\n\t:mod:``kubernetes.resources``:\n\n\tIf a :mod:`~kubernetes.resources.Container <kubernetes.resources.Container>`\n\tis found in the"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\treturn last_applied_manifest\n\n\n@task\ndef update_last_applied_manifest_list_from_resp(\n\tobserver_schema: Optional[Dict] = None,\n\tresponse: Optional[Dict] = None,\n) -> List[Dict]:\n\t\"\"\"Updates the last_applied_manifest list with a Kubernetes response.\n\n\tArgs:\n\t\tobserver_schema (Optional[Dict[str, Any]]): partial ``observer_schema``\n\t\tresponse (Optional[D"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t,\n\t__examples=None,\n\t__refs=None,\n\t__config=\"config/update_last_applied_manifest_dict.json\"\n)\ndef update_last_applied_manifest_dict_from_resp(\n\tlast_applied_manifest: dict, observer_schema: dict, response: dict,\n\texact=False,\n\t**kwargs\n) -> dict:\n\t'''\n\tUpdate a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n\tArgs"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t, update_fun=update_last_applied_manifest_dict_from_resp)\n\ndef update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp__func``, this\n\tfunction is called recursively to update a partial\n\t``last_applied_manifest_list`` from a partial Kubernetes response\n\nArgs:\n    response (dict): partial response from the Kubernetes API.\n    observable"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t, last_applied_manifest):\n\t\t\"\"\"Update a partial ``last_applied_manifest`` from a given Kubernetes\n\t\tresponse. These observed fields are then updated to the value of the\n\t\tpartial ``last_applied_manifest``. This function go through all\n\t\tobserved fields, and initialized their value in last_applied_manifest\n\t\tif they are not yet present.\n\n\t\tArgs:\n\t\t\tlast_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t\t\t\tupdated\n\t\t\tobserver_schema"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\tself.get_last_applied_manifest(self.observed_field, observer_schema)\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\tdef update_last_applied_manifest_list_from_resp(self, last_applied_manifest, observer_schema, response):\n\t\tself.updated_list = [last_applied_manifest.setdefault(x, self.get_json_data(response, observer_schema[x])) for x in observer_schema]\n\nupdated_v2_async = _UpdatedV2AsyncHelper()\nupdated_v2 = _UpdatedV2()\n\nclass _UpdatedV2Helper(object):\n\tdef __"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t, kubernetes_client\n\t, observables_client\n\n\t, metadata_obj\n\t, observed_fields_map\n) -> None:\n\t\"\"\"\n\tThis function updates both the ``last_applied_manifest`` field on a Kubernetes\n\tmanifest object, and the ``observer_schema`` field of each observed object,\n\tto be updated in a more consistent manner, but taking into account the\n\t``observables_client`` which is the one requesting the Kubernetes API\n\tvia metadata_obj.\n\t\"\"\"\n\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t  ):\n    \"\"\"Update an existing  observed field with a partial k8s response.\n\n    This function updates a field within ``observed`` from a partial k8s response.\n    Args :\n        last_applied_manifest (dict): the observed field to update\n        observer_schema (dict): the fields from the observed schema to update\n            with k8s response\n        response (dict): the k8s response from the Kubernetes API\n    Returns: dict: the updated k8s response\n    \"\"\"\n    if key not in last_app"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t),\n    update_last_applied_manifest_list_from_resp)\n\n#==============================================================================\ndef apply_manifest(obj, name, last_applied_manifest, observe_api):\u00a0\n\t\"\"\"`\n    Function updates a ``last_applied_manifest`` for this object by applying\n    the kubernetes manifest.\n\n    Args:\n    obj (object): The object from the Kubernetes API.\n    name (str): The name of the object or container of the object (e.g. 'containers')\n        "}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\t@apply_defaults\n\tdef update_last_applied_manifest_dict_from_resp(self, last_applied_manifest, observer_schema, response):\n\t\tobserver_fields = ['clusterName', 'kind','metadata.name','metadata.resourceVersion','metadata.annotations','metadata.ownerReferences', 'labels']\n\t\tif any(observer_field in response for observer_field in observer_fields):\n\t\t\tself.last_applied_manifest = last_applied_manifest\n\t\t\t#"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\treturn [\n\t\t{ \"name\": \"status\", \"description\": \"The status of each Kubernetes resource\" }\n\t]\n\ndef generate_observer_schema(app): \"\"\" Generate the data schema for the data\n       relationship ``kubernetes.status.observed``.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate an\n       observer schema\t\"\"\"\n\treturn [\n\t\t{ \"name\": \"status\", \"description\": \"The status of each Kubernetes resource\""}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tdef _gen_default_observer(spec):\n\t\tdef build():\n\t\t\tsc = generate_kubernetes_connection(spec, None)\n\t\t\tschema = sc.schema(None)\n\t\t\tif schema:\n\t\t\t\tsc.get_observer().build_observer_schema(schema)\n\t\t\telse:\n\t\t\t\tsc.get_observer().build()\n\t\t\treturn jsonify(sc.data.observer), 204\n\t\treturn _gen_default_observer, build\n\t@app.route(\"/api/schema/<string:spec"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t# TODO, add \"--custom-observer\" when we have an initial observer\n\t# but we need to have a list of initial observers before we get the right\n\t# spec\n\tobserver_schema = app.k8s_resources['spec']['observer'].output_from_dict(app.resources[0].spec['spec'])\n\tif observer_schema['kind'] == 'POD':\n\t\tassert not hasattr(observer_schema,'metadata')\n\t\t# TODO, add support for pods - which we need to generate"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\t# The generated schema is designed to match the specification of kubernetes itself\n\t# and should thus ignore missing resources, and also suppress any missing labels\n\tapp.kube_client.expect(\n\t\tspec.manifest.resource(\"persistent-secrets\").filter(name_contains=\"config\"),\n\t\t\"Missing objects from manifest, expected kubernetes objects but got %s\" % str(app.kube_client.expect(spec.manifest.resource(\"persistent-secrets\").filter(name_contains=\"config\"),\n\t\t\t\t\t\t\"No"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    for path_part in Path(APPLICATION_DIR).parts:\n        observable_objs = app.observables[path_part]\n        if not isinstance(observable_objs, list):\n            if len(observable_objs) == 0 and \"podman\" not in path_part.upper():\n                observable_objs = [\"Podman\"]\n            elif \"docker\" in path_part.upper():\n                observable_objs = [\"Podman\"]\n            else:\n                raise Exception(\""}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tdefault_observer_schema = kubernetesHelper.generate_default_observer_schema(app)\n\tapp.observer_schema = default_observer_schema\n\n\n\ndef generate_deployed_observer_schema(\n\tdeployed_app_or_clusterid,\n\tdeployed_resource\n):\n\t\"\"\"\n\tGenerate the deployed observer schema for a Kubernetes resource present in\n\t``spec.manifest``.\n\n\tArgs:\n\t\tdeployed_app_or_cluster"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\tobserver = app.observers\n\t# Generate a default observer schema if a default observer schema was not specified\n\tobservers = []\n\tif observer is None:\n\t\tobserver_schema = {\n\t\t\t\"kind\":\"CustomObjectReference\", \n\t\t\t\"properties\": { \"apiVersion\": \"api/v1\", \"kind\": \"O\" } }\n\t\tobservers.append(observer_schema)\n\telse:\n\t\tobserver_schema = { \"kind\": \"CustomObjectReference\", \"apiVersion\":\"alpha1\", \"properties"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\treturn generate_default_observer_schema(app.get_managed_v1(\n\t\t\tobserver_name=\"default-observer-1\"))\n\ndef generate_default_observable_schema(app):\t\"\"\"\n\tGenerate the default observable schema for each Kubernetes resource present\n\tin ``spec.manifest`` for which a custom observable schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observable schema\t\"\"\"\n\treturn generate_default"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\tobservers = {}\n\t\n\tfor resource in sorted(app.api_resources.values(), key=lambda r: sorted(r.path.parent)):\n\t\t\n\t\tresource_names = getattr(resource,'resource_names', None)\n\t\t\n\t\tobservers_meta = resource.observable_schema\n\t\t\n\t\tif not _is_none(observers_meta):\n\t\t\tif observers_meta['schema'].get('observer')!= None:\n\t\t\t\t\n\t\t\t\tobserver_def = _generate_default_ob"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\ttry:\n\t\tfrom kubernetes import client\n\texcept ImportError:\n\t\tlog.error(\"Unable to import the kubernetes module. Aborting.\")\n\t\traise SystemExit(0)\n\n\ttry:\n\t\tapi_resp = client.V1Pod(status=client.V1Status(phase='Running'))\n\t\tif len(app.spec.manifest.observers) >= 0:\n\t\t\tfor obj in app.spec.manifest.observers:\n\t\t\t\tif obj.api_version == 'ingress.kubernetes.io"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\n__all__ = (\n\t'FormatMixin',\n\t'FormatStringMixin',\n\t'FormatStringParamMixin',\n\t'format',\n\t)\nfrom.FormatStringParamMixin import FormatStringParamMixin\nfrom.FormatStringParamMixin import format\n\nclass FormatStringParamMixin(FormatStringParamMixin):\n\tdef format(self, params):\n\t\t\"\"\"\n\t\tThis method is required because :class:`~FormatStringParamMixin` \n\t\tmixins it own `format` method internally and will create \n\t\ta"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t  )\n\t\n\tparams = None\n\tif params is not None:\n\t\t# Convert them to collections.abc.Mapping objects.\n\t\tif isinstance(params, dict):\n\t\t\tparams = params.values()\n\t\telif isinstance(params, list):\n\t\t\tparams = list(params)\n\t\t\n\t\t# If *params* is a :class:`collections.abc.Mapping`, then\n\t\t# convert them to :class:`list` in the correct order.\n\t\tif (\n\t\t\tparams is not None and \\\n\t\t\t\tisinstance(params["}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\n\tif isinstance(params, dict):\n\t\ttparams = params\n\telse:\n\t\ttparams = {str(i):val for i, val in enumerate(params)}\n\n\tif len(tparams) == 0:\n\t\treturn (\"\", ())\n\n\tfrom sqlalchemy import select, table, literal_column\n\timport re\n\n\t# For a named style style, we can do anything\n\tif kw.get('style') and kw['style'].lower() == 'named':\n\t\tfparams = list(tparams.values())\n\t\tassert"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\n\tclass OutQuerySender(object):\n\t\tdef __init__(self):\n\t\t\tself.current = 0\n\n\tdef encode(self, params):\n\t\tself.q_sender.current = self.current\n\t\tself.q_sender.params = params\n\t\tself.q_sender.next()\n\t\tself.current = 1\n\n\tdef send(self):\n\t\tself.q_sender.current = self.current\n\t\tself.q_sender.next()\n\t\tself.current = 1\n\n\tdef decode(self, out_"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\treturn (\n\t\t\"%(sql)s\",\n\t\t_format_sql_params(params)\n\t)\n\n\ndef _format_sql_params(params):\n\tresults, in_style = format_params(params)\n\tif not in_style:\n\t\t# Remove trailing parameters from dict, which it may add via.pop().\n\t\tresults = tuple([dict(result).pop() for result in results])\n\treturn results\n\n\ndef format_params(params):\n\t# :meth:`str.format` has a different logic.  Call"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t),\n\t_str_params\n\t)\n\nget_params_factory = Function(\n\t\"\"\"\n\t:class:`SQLParams` is an object from which other classes will access\n\tthe parameters as attributes.  This factory can be used in place\n\tof :func:`sql.execute()` to generate any SQL query from the\n\t:class:`SQLParams` object.\n\t\"\"\"\n\t)\n\nget_first_result = Function(\n\t\"\"\"\n\t:class:`tuple` of :class:`dict` or :class:"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t),\n\n\t\"format_params_join_char\":\t(\t\"\"\"\n\tFormat parameters with a specified field separator.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\t_SQLExpression.__init__(self, format=True)\n\tself._params = {}\n\tif hasattr(params, \"items\"):\n\t\tself._params.update(params.items())\n\telif hasattr(params, \"keys\"):\n\t\tself._params.update((n, idc) for n, idc in params)\n\tif in_style:\n\t\tself._in_style = format(in_style)\n\t\tif not isinstance(self._in_style, str):\n\t\t\tself._in_style"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t)\n\n\t@decorator\n\tdef out_style(\n\t\t\tself,\n\t\t\tsql: Sequence[Union[str, Decorator[None, str]]], params: Dict[str, Sequence[Union[int, str]]],\n\t\t\t*extras: Any,\n\t\t\t**options: Any,\n\t) -> Sequence:\n\t\tparams = _handle_conversion(params)\n\t\treturn format_qt(query(sql, params, extras, **options)[0], options)\n\n\t_out_style = out_style\n\tdel out_style\n\n\t@class"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t) \n\tcls =\tself.__class__.__name__\n\n\tparams = self.params\n\tparams = params and self.cls.parse(params) or params\n\tparam_columns = list(params.keys())\n\n\tq = \"{c}({t})\"\n\tparam_columns_to_q = list(dict.fromkeys(param_columns).keys())\n\tsql = tuple()\n\tstmt =\t\"(c := c.values({cols}))\"\n\n\tfor k in params.keys():\n\t\tv = params["}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t)\n\n\n@implements(IParameters)\nclass params(Mapping):\n\t\"\"\"\n\t:class:`params` behaves like ``params[1]`` if ``params[0]`` is a :class:`int`\n\tand ``params[1]`` is a :class:`str` otherwise.\n\t\"\"\"\n\tdef __init__(self, sql):\n\t\tif not isinstance((sql), bytes):\n\t\t\tsql = sql.encode('utf-8')\n\t\tself._sql = sql\n\t\tself._value = None\n\t\tself._"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t))\n\n\nlist.clearformat.arg_types = _params_to_dicts(\n\t\"\"\"(:class:`~collections.abc.Iterable` or :class:`~collections.abc.Iterator`)->:class:`str`\n\nAdds :attr:`collections.abc.Mapping.items` to the :meth:`MutableDict.clearformat` argument\n(``clearformat=[]``), adding each in-style (key, value) pair that has is treated\nthe same way as it would happen with arguments via the :"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t.format(in_style=SQLParams.in_style), SQLParams.sql_param_type(),\n           format_map=True\n\t.join([repr(o) for o in params]), *(list(params) if hasattr(params, \"__iter__\") else [params]))\n\ndef format_params(sql=None, one_param_set=True):\n\t\"\"\"Returns the formatting params as a sequence with a formatter which\n\tdisplays the values to :func:`string.format`.\n\n\t- *sql* (:class:"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t)\n\ntable_template = \"\"\"\n%(head)s\n\n{% for sql in sql_list %}\n{% if sql.in_style == 'dict' %}\n<tr>\n    <td>{{sql.name}}</td>\n    <td>{{sql.get_value(values)}}</td>\n</tr>\n{% else %}\n    <td>{{sql.get_value(values)}}</td>\n{% endif %}\n</tr>\n{% endfor %}\n\n{% for sql in"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t),\n\t'many_params':\n\t\"\"\"\n\tConvert each SQL parameter to a tuple (key, value).\n\n\t*params* (:class:`collections.namedtuple`) is a parameter set used to\n\tformat the SQL query (:class:`tuple`).\n\n\tReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of in-style parameters\n        (:class:`dict"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t)\n\nclass SqlParamError(Exception):\n\t\"\"\"Used internally to indicate an error in converting the SQL query to a string.\n\t\"\"\"\n\tdef __init__(self, param_name:str) -> None:\n\t\tself.param_name = param_name\n\tdef __str__(self) -> str: return f\"Parameter '{self.param_name}'\"\n\n\nclass SQLParams:\n\t'''Convert the query to use a specific set of in-style parameters.\n\n\tFor example, use :func:`SQLParams.formatmany"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t)\n\n\n# :config:`db_params` is an extension to :config:`params` but is rather\n#    customised to convert DB parameters to SQL query parameters.\n#    Additional keys are added to default list and provide a more\n#    convenient config format, if needed. This way this database can have\n#    different parameters than the parameters :attr:`.paramstyle`.\n#    \"\"\"\n#    db_params = dict(params)\n#    if 'params' in db_params:\n#        db_params['params'].update(params)\n#"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t)\n\n\tdef __init__(\tself, sql, many_params=None):\n\t\tself.sql = sql\n\t\tself.params = self._convert(many_params)\n\n\t@property\n\tdef params(self):\n\t\treturn self._params\n\n\t@params.setter\n\tdef params(self, value):\n\t\tself._params = value\n\n\t@property\n\tdef params_list(self):\n\t\treturn self._params\n\n\tdef _convert(self, many_params):\n\t\tif many_params:\n\t\t\tfrom sqlalchemy.orm"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t)\n\n\t_check_in_style(PARAMS_TYPES.in_style)\n\t_check_in_style(QUERY_PARAMS_TYPES.in_style)\n\n\tif (\n\t\t\t\t\tparams_in := list(params)\n\t\t):\n\t\tif list(params_in) == []:\n\t\t\treturn (sql % [], [])\n\t\telse:\n\t\t\treturn (make_in_style_format(sql_in, params_in, params))\n\telse:\n\t\treturn (sql % [(PARAMS_TYPES.in"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t)\n\n_parsers = {}\n\ndef formatparser(name, attr, parser, *,\n\t\t\t    in_style = SQLParams.in_style,\n\t\t\t\t \n\t\t\t\t \n\t\t\t\t parsed_params = None):\n\t\"\"\"\n\tA decorator used to register a string format parser to the :mod:`sqlparse`\n\tlibrary. Given a formatter/parser name it will be available as a\n\tproperty of the :func:`format_parameters` function. Available format\n\tparameter styles are in the :mod:`sqlparse` library"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t# First, validate the OCFL at path.\n\tocfl_path = Utilities.check(path)\n\tif not ocfl_path:\n\t\treturn False\n\t\n\t# This is a pyfs root. It's the file's name.\t\n\tfname = Utilities.file_name(path)\n\tif fname!= ocfl_path:\n\t\treturn False\n\n\t# Verify that the file exists.\t\t\n\tfname = Utilities.get_fs_location('%s/%s' % (path,fname))\t\n\tif not os"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tglobal __DEBUGING__\n\tif __DEBUGING__:\n\t\tlogger.debug('validating OCFL object at %s' % path)\n\tif __DEBUG_OCFL__:\n\t\tlogger.debug('validating OCFL object at %s' % path)\n\n\ttry:\n\t\tos.path.exists(path)\n\texcept IOError, (errNum, errString):\n\t\tlogger.exception(errString)\n\t\treturn False\n\n\tif not os.path.isfile(path):\n\t\tlogger.warn('{0} not"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t# The 'ok' flag is deprecated, see 2129\n\t# pylint: disable=too-many-arguments\n\trval = False\n\t# pylint: enable=too-many-arguments\n\tif path.startswith('pyfs:'):\n\t\tpath = path[5:]\n\tpath = self.getPath(path)\n\ttry:\n\t\treturn validator.validate(path)\n\texcept osutils.InvalidFile as err:\n\t\trval = True\n\t\tmessages = '\\trawarning: {}' # rval"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n\t# validate object at pyfs root \n\t# check that the specified object has a valid OCFL class\n\tpyfsObject = isFileSystem(path)\n\n\tif not pyfsObject:\n\t\t# path not a dir + root\n\t\t# warn user of error\n\t\t# return False as we don't recognize object at path\n\t\treturn True\n\t\n\t# validate object class\n\t# get a list of subclasses\n\tvalidClasses = [ OCFL_Data, OCFL_DataSet, OCFL_Dataset, OCFL_Logger, OCFL_"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n\t# Validate the OCFL object at path.\n\t# Return True if valid, False otherwise.\n\tvalid, error = self.validate_pyfs(path, self.OCFL_FILE)\n\t\t\n\tif not valid:\n\t\t# Return False if the error (e.g. invalid OCFL object)  \n\t\t# is given, else return False.\n\t\treturn False\n\t\t\n\treturn True\n\n\t\nclass InvalidDirectoryError(Exception):\n\t...\n\t\n\t\n\t\nclass InvalidDirectory(Exception):\n\t...\n\t\n\t\n\t"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\ttry:\n\t\tvalidate_file(file_path=path, root_path=self.root, ignore_missing=True, ignore_warnings=True)\n\t\treturn True\t# file found\n\texcept (TypeError, ValueError) as e:\n\t\tif str(e) not in ('is a directory', 'does not exist', 'does not exist', 'does not have a file', 'FilePath not found', 'is a file', 'does not have a directory', 'No filename has been given', 'does not have an attribute named is_file', 'is not a"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tvalid = False\t# will be returned as soon as a file is validated, if any\n\t\n\t# try to open the file, if not valid path\n\t# try and find it with a valid path\n\ttry:\n\t\tf = open(path)\t# open the file as a pointer\n\t\tvalid = True \t# set valid to True with success\n\texcept IOError:\n\t\t# try for non-existing path\t\t\n\t\ttry:\n\t\t\tself.fs.walk(path, invalid=True, recursive=True)\n\t\t\tvalid = True\t# invalid if no path provided"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\ttry:\n\t\tvalid = False\n\t\tpyfs_obj = pywf.open_pyfs(path)\n\t\ttry:\n\t\t\twcfg = pyfs_obj.check_wcfg(validate=True)\n\t\t\tvalid = True\n\t\texcept pywf.InvalidWfException as e:\n\t\t\timport sys\n\t\t\tprint(\"\\r\\nOCFL check warnings: %s %s\" % (sys.exc_info()[0], str(e)), file=sys.stderr)\n\t\t\twarnings = warn.Warnings.from_exception"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n\t# First create an empty map of object names\n\t(names, objects)=self.createObjectsMap({})\n\t# Now populate the mapping with our path to the raw OCFL\n\t(name, subname)=urllib.splittype(urllib.parse.urljoin(self.fsroot, path))\n\tif not name.endswith('.py'):\n\t\tpath=urllib.parse.urljoin(self.fsroot, path)\n\tif not path.endswith('/'):\n\t\tpath=path+'/'"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\ttry:\n\t\tif ocfl.get(\"root\") == ocfl.get(\"working_dir\"):\n\t\t\tself.root = ocfl.get(\"working_dir\").get(\"path\")\n\t\t\treturn True\n\t\telse:\n\t\t\tself.root = ocfl.get(\"path\").get(\"working_dir\")\n\t\t\treturn True\n\texcept ocfl.ConfigurationError as e:\n\t\tprint(\"Error with configuration in %s at %s: %s\" \n\t\t        % (self, e.path, e.message) )\n\t\treturn False\n"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tself._check()\n\ts = \"Validator\" + prefix + self.status_str(prefix=\" \" + next(self.fmt)).lstrip() + \"\\n\"\n\treturn s\n\t\n\t\t\n\t#\n\t#\n\t#\n\t\n\t\nclass Generator():\n\t'''\n\tInterface used to generate unique tokens\n\t'''\n\tdef __init__(self, prefix=\"\"):\n\t\t'''\n\t\tInitializes new instance\n\t\t'''\n\t\tself.prefix = prefix\n\t\tself.saltterm = None\n\t\tself.salt"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn \"\".join(['OK' if self.valid() else 'FAILED','' + prefix])\n\t\n#_______________________________________________________________________________________\t\n\ndef make_func_tuple(t = ''):\n\t\"\"\"\n\tCreate a tuple from this line of code -- to be used as a function's argument\n\tin the function.\t\n\t\"\"\"\n\treturn (t, )\n\n###===================================================================\n###                                                                    \n###===================================================================\n###_______________________________________________________________________________________\t\n\ndef generate_func(meth, kwargs = tuple(), name = ''):\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tis_valid = self.validation_state()\n\t# print(is_valid)\n\n\tif not prefix:\n\t\ttry:\n\t\t\treturn str(is_valid)\n\t\texcept:\n\t\t\treturn is_valid\n\n\treturn str(is_valid) +'' + prefix\n\n\treturn str(is_valid) +'' + prefix\n\n\treturn str(is_valid)\n\n\nclass InputError(Exception):\n\tdef __init__(self, message, validation_error=None):\n\t\tself.message = message\n\t\tif not isinstance(validation_"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn 'Validators active{}'.format(' '*(prefix==''))\n\ndef status_delta(self):\t\"\"\"\n\tReturn time elapsed to last validator status change.\t\"\"\"\n\tif not self.status:\n\t\treturn 'Not active'\n\treturn self.__delta_ms()\n\ndef status_delta_elapsed(self):\t\"\"\"\n\tReturn elapsed time remaining to last validator status change.\t\"\"\"\n\treturn (self.__last_validator_status_ms() - self.__status_ms())/1000\n\n#"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif not prefix:\n\t\tprefix = self.__class__.__name__\n\tstatus_str = '%s' % self.__class__.status(\n\t\t'valid', 'active', 'inactive')\n\treturn str(' %s%s' %\n\t\t(prefix, status_str))\n\n\n@six.add_metaclass(ValidatorMetaClass)\nclass Validator(object):\n\t\"\"\"Class representing an instance of an instance of a Validator.\t\n\t\t\n\tProvide a method to validate an amount. It is not mandatory to call this"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\n\tstatus = ''\n\t\n\tif self.validator.is_initialized():\n\t\tif (not self.validator.is_valid()   ):\n\t\t\tstatus += 'validation error'\n\t\telif (self.validator.has_input_schema()):\n\t\t\tstatus += 'validation error (input)'\n\t\telse:\n\t\t\tstatus += 'validation success'\n\telse:\n\t\t# this should not happen\n\t\tstatus += ', validation is disabled'\n\t\n\tif self.error_handlers:\n\t\tfor handler in self.error_hand"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn''.join([prefix, self.status])\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn status.format(prefix=prefix)\n\n\nclass Validator(object):\n\t\"\"\"\n\tA base validator.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tsuper(Validator, self).__init__()\n\n\t\tself.errors = list()\n\t\tself.names = None\n\n\tdef __contains__(self, value):\n\t\ttry:\n\t\t\tname = self.names[value]\n\t\texcept KeyError:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\tdef __getattr__(self, name):\n\t\tif name in"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn f'{prefix}{getattr(self, \"validator\")}: {getattr(self, \"status\", \"N/A\")}'\n\t\n \nclass ModelValidator(Validator):\n\t\"\"\"\n\tValidation model object base class. Uses a base class: `ModelValidator` used to validate incoming\n\tdata, as well as the `ModelValidator.model_name` attribute as a class name and name\n\tin the base class' `name` member.\n\t\n\tParameters\n\t----------\n\tvalidator: str\n\t\tThe name of the"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn prefix + (\"PASS\" if self.is_valid else \"FAIL\")\n\n\nclass EAPTLSKeyInvalid(ESMEAPICodeResult):\n\t\"\"\"\t\n\tFor EAPTLS Key Invalid.\n\t\"\"\"\n\tdef __init__(self, code=None):\n\t\tsuper().__init__(code, None)\n\t\tself.code = code\n\n\t@property\n\tdef error_code(self):\n\t\t\"\"\"\n\t\tReturns the status string for error code.\n\t\t\"\"\"\n\t\treturn str(self.code)\n\n\t@property\n\tdef"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn '<status message=\"{0}\". status=\"{1}\">'.format(prefix, self.status)\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\theader = ['{prefix}{tag} = value'.format(prefix=prefix, tag=self.tag),\n\t\t'{prefix}{tag} = {value}'.format(prefix=prefix,\n\t\t\ttag=self.tag),\n\t\t'{prefix}{tag} = {value:.3f}'.format(prefix=prefix,\n\t\t\ttag=self.tag),\n\t\t'{prefix}{tag} = {value}'.format(prefix=prefix, tag=self.tag),\n\t\t'{prefix}={true}={false}'.format(\n\t\t\ttrue=('true'"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn ''.join(prefix + (self.__class__.__name__, '_')).replace('.', '__')\n\n\n# Register status class\nRegisterStatus.status_str = status_str\nRegisterStatus._add_method('_add_status_', self._add_status)\nRegisterStatus.__module__ = RegisterStatus.__module__.upper() + '__'\nRegisterStatus.__name__ = RegisterStatus.__name__.lower() + '_'\nRegisterStatus.c_args = (RegisterStatus, )\nRegisterStatus._init_metadata"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn \"status = %s, status_str = %s; date =  %s;\"%(self.status, type(self).__name__, datetime.datetime.now().strftime(\"%D %b %Y %H:%M:%S\")) if self.status!= \"error\" else \"status = error, date =  None;\"\n\n# ==============================================================================\n    #\n    def error_str(self):\t\"\"\"\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n    \treturn self.error_str2, type("}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tif self._status:\n\t\t# if there are changes, it can contain information about the validation process.\n\t\treturn prefix + self.status + '\\n' + self.reason + '\\n'\n\telse:\n\t\t# otherwise, it can contain information about the overall validation process.\n\t\treturn prefix + 'WARNING: Unable to process this change, you should consider checking it against your own data. This changes will be lost.'\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tresult = \"\"\n\tif self._is_validated:\n\t\tstatus = self._is_validated\n\t\tresult += prefix + 'valid' if status else \"\"\n\t\tresult += '[' + self._name + ']' if self._name else ''\n\t\tresult += ':'+ prefix + str(status)\n\t\tresult += '\\n'\n\telse:\n\t\tresult += prefix + 'not in valid log'\n\treturn result\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tlog = self.get_status_log()\n\tif log is not None:\n\t\tstatus = '%s%' % (prefix, self.status)\n\t\tstr = getstr('Status', '') % (self.get_status(), self.get_status_string())\n\n\t\tif str.startswith('%s' % status):\n\t\t\treturn str\n\t\telse:\n\t\t\treturn '%s %s%s' % (prefix, str, status)\n\telse:\n\t\tstatus = ('%s[INACTIVE]' % prefix) if self.is_"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tdef pad(s, l):\t# Pad string to be on least-lengthened to l letters\n\t\tif len(s) >= l:\t# If current string must be to l chars, return asl\n\t\t\treturn s;\treturn str(s[:l])\n\t\tl += len(s[len(s)]);\treturn str(s[:l]);\t\n\ts = self.name;\tmsg = '';\tfor m in self.validations:\t\t\n\t\ts = s.replace('|','| ');\t\tmsg = msg.replace('"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tmsg = ''\n\tif self.valid is True:\tmsg += 'Success'\n\telif self.valid is False:\tmsg += 'Failed'\n\tmsg += ':'+ ','.join(self.errors)\n\treturn '%s%s [%s] %s' % (prefix,\n\t\t\t  '(%s)' % (self.max_errors if self.valid else '',\n\t\t\t   self.max_faults if self.valid else '',\n\t\t\t   self.max_successes if self.valid else '',)) \n\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tstr = prefix\n\tfor (name, value) in self.data().items():\n\t\tif name.upper() not in [key.upper() for key in self.validators]:\n\t\t\tcontinue\n\t\tstr += f'{prefix}{name} \"{value}\" '\n\treturn str\n\n# end class ValidationLog\n\n\n# class ValidationFailure\nclass ValidationFailure(Exception):\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\n\tdef __str__(self):\n\t\treturn self.msg\n# end class ValidationFailure\n\n\nclass"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\traise NotImplementedError\n\n  def get_type(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn type or None, always None if identifier is an opaque type.\"\"\"\n\traise NotImplementedError\n\n  def get_type_by_name(self, name):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn type by name or None, always None if identifier is an opaque type.\"\"\"\n\traise NotImplementedError\n\n\nclass IsIdentifier(Operator):\n  \"\"\"A wrapper of"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\traise NotImplementedError(\"is_valid(%r)\" % (identifier,))\n\n\nclass User(BaseModel):\n\t\"\"\"\n\tUser represents the actual user that exists on the server, and is set to use\n\tthe API.\n\t\"\"\"\n\n\tname: str\n\temail: str\n\trole: str\n\tpassword: str\n\n\tusername: str = Field(..., regex=r\"[A-Za-z]+\")\n\temail_verified: bool = Field(True, alias=\"emailVerified\")\n\tpassword_hash: str\n\n\tdef verify"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\nclass Node(object):\n\t\"\"\"\n\tAbstract node class.\n\t\"\"\"\n\tdef __init__(self, children):\n\t\t\"\"\"\n\t\tCreates a node object.\n\t\t\"\"\"\n\t\tself.children = []\n\t\tself.children = children\t\t\n\n\tdef add_child(self, child):\n\t\t\"\"\"\n\t\tAdds a child to this node.\n\t\t\"\"\"\n\t\tself.children.append(child)\n\n\tdef remove_child(self, child):\n\t\t\"\"\"\n\t\tRemoves a child from this node.\n\t\t\"\"\"\n\t\tself."}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n  def get_id(self):\t# pylint: disable=invalid-name\n\t\"\"\"Returns system identifier for the current map.\n\tThis identifier is used in naming.\t\n\t\"\"\"\n\treturn mapId\n\n  def get_identifier(self):\t# pylint: disable=invalid-name\n\t\"\"\"Returns identifier for this map.\n\tThis identifier is used in naming.\n\t\"\"\"\n\treturn self.mapId\n\n  def get_source_ref(self):\t# pylint: disable=invalid-name, unused-"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif identifier is None:  # pylint: disable = unset-context\n\t\treturn True\n\telif isinstance(identifier, six.integer_types):  # pylint: disable = no-else-return\n\t\tif -1 < identifier < len(self.identifiers):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn identifier < len(self.identifiers)\n\telif isinstance(identifier, six.moves.range):\n\t\tindex = identifier.start\n\t\tif index >= len(self.identifiers):  # pylint: disable"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True              \n\nfrom.._compat import text_type\nfrom..common import validator\n\n# pylint: disable=too-many-arguments\n# pylint: disable=too-many-branches\n# pylint: disable=too-many-instance-attributes\n# pylint: disable=too-many-lines\n# pylint: disable=too-many-locals\n# pylint: disable=too-many-public-methods\n# pylint: disable=too-many-return-statements\n# p"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\tpass  # pylint: disable=unused-argument\t# noqa: F821 pylint: disable=unused-argument\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\n"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tv = self.inventory[self.inventory_index]\n\tif not v['type']:\n\t\treturn \"Invalid inventory: non-valid type\"\n\t\n\tif self.specversion == \"1\":\n\t\tif v['type']!= \"string\":\n\t\t\treturn \"Invalid inventory: Type'string' is a supported type.\"\n\t\tif extract_spec_version:\t\n\t\t\tv['type'] = \"\"\n\t\t\tv['spec_version'] = self.specversion\n\t\t\treturn \"Saved to the file: {type}\".format(\n\t\t\t\ttype='string',\n\t\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tresult = True\n\tif not hasattr(inventory,'versionHierarchy'):\n\t\tif self.skip:\n\t\t\tresult = False\n\t\telse:\n\t\t\tif not hasattr(self.spec,'versionHierarchy'):\n\t\t\t\t# No version specified\n\t\t\t\ttry:\n\t\t\t\t\tif self.skip:\n\t\t\t\t\t\t# If the test is used, we need to go ahead and\n\t\t\t\t\t\t# extract this version\n\t\t\t\t\t\t_, spec_version = split_spec_version_from_spec(self.spec)\n\t\t\t\t\t\tif self.version_type:\n\t\t\t\t\t\t\tversion_type,"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t# First perform all tests.\n\tvalidate_spec_version = super(InventoryItem, self).validate(inventory, spec_version)\n\tif not validate_spec_version:\n\t\treturn valid, valid\n\tvalid, valid = self.validate_inventory(inventory)\n\tif not valid:\n\t\tlog.debug(\"No 'valid' result\")\n\t\tif not any(entry for entry in inventory.keys() if entry!= self.type):\n\t\t\tlog.debug(\"No 'valid' result, inventory key '%s' not match with '%s'\""}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\n\tif hasattr(inventory, \"groupby\"):\n\t\tinventory = inventory.groups\n\t\n\t\tself.spec_version  = 2\t\t\t\t# use Version 2\n\t\tself.extract_spec_version = True\t\t# use version 1 or even False (optional)\n\telse:\t\n\t\tif hasattr(inventory, \"version\"):\n\t\t\tself.spec_version = inventory.version # assume a VersionSpec()\n\t\telif inventory:\n\t\t\tself.spec_version  = 1\t\t\t\t# use Version1\n\t\t\tself.extract_spec_version = False\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tversion = 0\n\tif self.spec_version!= -1:\n\t\tversion = self.spec_version\n\n\tspecified = {}\n\tfor entry in inventory.entries:\n\t\tif entry.inventory_mode:\n\t\t\t# We are dealing with an 'environment' - this is a thing that we must \n\t\t\t# convert to a string by checking if it has been put into a str \n\t\t\tspecified[str(entry.inventory_mode)] = {str(entry.name):str(entry.type)}\n\n\t# Special cases for things that are not recognized as valid."}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\timport datetime\t\n\n\tinventory_name = inventory.name\n\tinventory_ver = inventory.version\n\tif inventory_name == \"*\" or inventory.spec == \"*\" or inventory.spec == \"\":\n\t\treturn True\n\ttry:\n\t\tinventory_ver = extract_spec_version\n\t\tif \",\" in inventory_ver:\n\t\t\ttry:\n\t\t\t\treturn self.validate_spec_version(inventory_ver)\n\t\t\texcept:\n\t\t\t\tpass\n\t\treturn True\n\texcept AttributeError, e:\n\t\treturn inventory_ver == \"\" and self.valid"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif not inventory: return 0\n\tif inventory.metadata.get('inventory_type')!= 'inventory': return 0\n\tspec_version_str = self.metadata.get('version')\n\tif not spec_version_str:\n\t\tspec_version_str = inventory.metadata.get('type')\n\ttry: # this is a little hack: the specification version is always\n\t\t# a valid integer value, but it's pretty close to 1\n\t\tspec_version = int(spec_version_str)\n\t\tif extract_spec_version is not True:\n\t\t\textract"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t# If it's valid, we can proceed.\n\tif self._valid or self.version in self._extract_spec_versions:\n\t\treturn True\n\n\t# In our case it's not valid then we need to look for the type value.\n\t# Get information about that, and go to the next condition for testing.\t\n\tif inventory._is_valid() or extract_spec_version:\n\n\t\t# Check to see if the type value is correct\n\t\ttype_value = inventory.get('type')\n\t\tif self._valid and type_value!= 'Unknown' and type"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tversion_type = self.spec_version\n\tif extract_spec_version:\n\t\tversion_type = version_type.split(None,1)[0].rsplit('.',1)[0]\n\n\t# If either group has a version specification or there is no version\n\t# specification then attempt to figure out if it is valid.\n\tfor sub_group in inventory:\n\t\tif isinstance(sub_group, list):\n\t\t\traise ValueError(\"%s has multiple groups: %r\" % (self.name, inventory))\n\t\tif isinstance(sub_group"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\ttry:\n\n\t\t# check to see if anything has been added\n\t\tif Inventory._added_items:\n\t\t\tfor item in Inventory._added_items:\n\t\t\t\tinventory.append_item(item)\n\n\t\t\tif isinstance(inventory, type):\n\t\t\t\tif inventory.spec_version == \"v\" and inventory.spec_version is not None:\n\t\t\t\t\tinventory.spec = inventory.spec.replace(\"{v}\", inventory.spec_version)\n\t\t\t\telse:\n\t\t\t\t\tinventory.spec = \"{v}\"\n\n\t\t\t# we should have something to validate\n\t\t\tif inventory.spec and (is"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor f in manifest_files:\n\t\tm = open(f)\n\t\tname = f.split(\"/\")[-1].split(\".\")[0]\n\t\tversion = name.split(\"_version\")[0]\n\t\tif version not in self.ver:\n\t\t\tself.ver[version] = 1\n\t\tif name not in self.used_ver:\n\t\t\tself.used_ver[name] = 1\n\t\tfor line in m:\n\t\t\tline = line.strip(\"\\r\\n\")\n\t\t\tif not line:\n\t\t\t\tcontinue\n\t\t\tif \"digest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tcheck_list = self.check_manifest_digests(manifest_files)\n\n\tfor file in check_list:\n\t\tname, file_data = file\n\t\tif (name, file_data) not in digests_used:\n\t\t\tdigests_used.append((name, file_data))\n\n\n\t# return check_list\n\treturn check_list\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest in manifest_files:\n\t\tdigests_used.add(get_sha1(manifest))\n\t\tfor infilename in manifest.get_included():\t\n\t\t\tif os.path.isfile(infilename) and infilename not in self.files:\n\t\t\t\tself._check_digests_present_or_used(infilename, manifest_files,\n\t                                               [], [], [], [])\n\ndef get_sha1(path):\n\t\"\"\" Get SHA1 hash of a file. \"\"\"\n\twith open(path,"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tself.log_debug(f'Checking all digests.')\t\n\tis_digests_present = False\n\tmanifest = {str(k): str(v) for k,v in manifest_files.items()}\n\tif len(manifest) == 0:\n\t\tself.log_warn(f'Did not find any manifest_files.')\t\n\t\tis_digests_present = False\n\telif len(manifest) == 1:\t\n\t\tself.log_warn(f'Found one manifest_files item, but has only one manifest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tmanifest_files.sort()\n\tif(self.__manifestFile and self.__manifestFile[2]):\n\t\tself.__manifestFile[3].append(\"\")\n\tfor manifest_file in  manifest_files:\n\t\tmanifest_file = manifest_file[0]\n\t\troot = ET.parse(manifest_file)\n\t\tdoc = root.getroot()\n\t\tall_file = doc.findall(\"manifest/file\")\n\t\tif(len(all_file)>0):\n\t\t\tself.__manifestFile["}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor file_path, digest_name in zip(manifest_files, digests_used):\n\t\tif file_path not in self.manifest_files_seen:\n\t\t\tself.manifest_files_seen.add(file_path)\n\t\t\tfile_hash = hashlib.blake2b(digest_name.encode()).hexdigest()\n\t\t\tif not self._check_hash(file_hash, self.manifest_files_seen):\n\t\t\t\tsys.stderr.write(\"Unsupported manifest hash %s\\n\"\n\t\t\t\t\t\"T"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tchecksum_list_present = 0\n\tchecksum_list_used = 0\n\n\tfor file in sorted(manifest_files):\n\t\t# 1 File is included\n\n\t\t# Read manifest\n\t\tlines = read_xmlrpc_filedata_from_file(file)\n\n\t\t# 2 'FileName' should match\n\t\t# First check matches\n\t\tif 'FileName' in lines[0] and lines[0]['FileName']!= 'No Digest' :\n\t\t\tchecksum_list_used += 1\n\t\t\t# print('checksum_list_used -> {}"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor file in manifest_files:\n\t\t# Checks to make sure that the hash of the manifest file doesn't change\n\t\t# or that we're already using it.\n\t\t# This will handle many issues where we still want to keep things\n\t\t# separate in the source file and the build directory in-between.\n\t\t# Currently, we can't just check the hash of the file because it\n\t\t# might come from a different build tree (possibly the.whl).\n\t\t#\n\t\t# Note: We want to check this *after* the SHA1 hash. This is because\n\t\t# the"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_entry in manifest_files:\n\t\ttry:\n\t\t\tdigest = self.find_digest_in_manifest(manifest_entry['path'])\n\t\t\tif digest and digest not in digests_used:\n\t\t\t\tself.process_file_in_path(manifest_entry['path'], digest)\n\t\texcept OSError as err:\n\t\t\tos.stderr.write(\"Error: File {0} not present.\\n\".format(manifest_entry['path']))\n\t\t\tos.stderr.write(\"Error:  "}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\treturn True\n\n   # Get digests for files from the list of manifests.\t\n\tfor manifest_file in manifest_files:\t\t\n\t\tdigest = hashlib.sha1()\n\t\tif os.path.exists(manifest_file):\n\t\t\ttry:\n\t\t\t\tf = open(manifest_file)\n\t\t\t\tdigest.update(f.read(os.path.getsize(manifest_file)))\n\t\t\t\tf.close()\n\t\t\texcept:\n\t\t\t\t# Something went wrong.\t\n\t\t\t\traise SystemExit(\"Error in %s.  \"\n\t\t\t\t\t\"Could"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tself._validators[PriorVersion].check(prior)\n\tif not isinstance(prior, InventoryValidator):\n\t\traise TypeError(\"'prior' must be an instantiated InventoryValidator object\")\n\tif not prior.is_compatible(self):\n\t\traise ValueError(\"The version of the supplied inventory is not \"+\n\t\t\t\t\t\t\"compatible with the version of the inventory\")\n\n\tnfv_version = prior.nf_version\n\tif nfv_version > self.nfv_version:\n\t\traise ValueError(\"The inventory is incorrectly configured as"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tpval = prior.check()\n\tif type(pval)!= InventoryValidator:\n\t\traise ValueError(\"Invalid prior version\")\n\n\t# check that we have at least one object in inventory\n\tif not len(pval.objs)==1:\n\t\traise ValueError(\"Only on-disk inventory objects supported\")\n\n\t# We were validated that the name of one of these objects is\n\t# the correct suffix for the name of the self objects\n\tif not pval.objs[0].suffix==\"\":\n\t\traise ValueError(\"Internal naming conflict of '"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tversion_msg = f'prior version {prior.version} is the earliest supported version of {self.inventory.name}'\n\tself._assert_internal(version_msg, PriorVersion, 'PriorVersion')\n\n\tinventory_version_msg = f'This inventory is in version {self.version} and will not be used'\n\tif self.inventory.version!= prior.version:\n\t\tinventory_version_msg += f' because {prior.inventory_version}. New version can be calculated'\n\t\tprior.inventory.update(self.inventory, inventory_version"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\n\treturn (prior.validate() and isnotNone(prior.invertior) and\n\t\tisinstance(prior, InventoryValidator) and\n\t\tisinstance(prior.invertior, InventoryValidator))\n\t\n\t\n################################################################################\n\nclass InventoryValidator(object):\n\t\"\"\" An inventory object is valid if:\n\tIt has been checked for internal consistency.\n\tIt is an ordered and ordered mutable collection of strings.\n\t\t\"\"\"\n\t\n\t\n\tMAX_COMBO_VALUES = 100\n\t\n\tdef __"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tassert isinstance(prior, InventoryValidator), \"Invalid prior version\"\n\tif self.object is not None:\n\t\tassert self.object[\"_version\"] < \", \".join(prior.versions), \"Unmatched prior version %s\" % (prior.versions)\n\ndef validate_as_version(self, v2d, v1=0, vn=None):\n\t\"\"\"\n\tValidates that version n, vn, and/or v2d are available on disk in the\n\tinventory.  If not, a warning is issued detailing version-ins"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tassert type(prior) is InventoryValidator, \"invalid prior object: %s\" % str(prior)\n\tassert (type(self) is Inventory) and (type(prior) is Inventory), \\\n\t\t\"%s and %s must both be Inventory\" % (self, prior)\n\n\tfor a in ['name', 'base','version', 'product', 'description']:\n\t\tif not hasattr(prior, a):\n\t\t\traise Exception, \"invalid %s attribute of prior: %s\" % (a, str(prior))\n\n\t\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif isinstance(prior, InventoryValidator):\t\n\t\tdef validate_version(version):\n\t\t\t\"\"\"\n\t\t\tCheck a version of a prior object against a version of the current\n\t\t\tinventory object.\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\treturn prior.is_compatible(self, version)\n\t\t\texcept Exception:\n\t\t\t\treturn False\n\t\tif validate_version(self.version):\n\t\t\tif validate_version(prior.version):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\telse:\n\t\treturn False\n\n\ndef validator(typefname, datat"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif (prior!= self and\n\t   not isinstance(prior, InventoryValidator) in (self.pipeline, prior.pipeline)):\n\t\traise ValueError(\"prior must be an InventoryValidator or\"\n\t\t              \" one of {self, prior}\")\n\treturn prior\n\n\t\nclass Inventory:\n\t\"\"\"\n\tContainer object that stores an inventory of objects.  This object uses\n\ta list of classes for storing information like name, type and class members.\n\t\"\"\"\n\tname = \"\"\n\ttype = \"string\"\n\tmembers ="}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tvalidating_function = None\n\tvalidating_function_name = None\n\tif validating_function == None:\n\t\traise ValueError(\"Please pass a valid validator object to validate as an argument.\")\n\tif validating_function_name == None:\n\t\traise ValueError(\"Please pass a valid validating function to validate as an argument.\")\n\tif not issubclass(validating_function, InventoryValidator):\n\t\traise TypeError(\"\\'\" + validating_function_name + \"\\' must extend `inventory.InventoryValidator`.\")\n\tif not isinstance"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tinventory = self.getInv()\n\tprior_version = self.getVersion()\n\tif prior_version > inventory.getVersion():\n\t\traise Item.Item.InvalidVersion(\"version of {} ({}) must be greater \"\n\t\t\t\t\"than version of the current inventory ({})!\".format(\\\n\t\t\t\tprior.getName(), prior_version, inventory.getName()))\n\n\tif (prior.isCompatibleWith(inventory) == False or\n\t\tprior.getCompatibilityCheck() < 1.0 or prior.getCompatibilityCheck() > 2.5):\n"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t# Start with a set of files from disk\n\tdisk_files_set = inventory.get_logical_path_set(version)\n\n\t# Now add out-of-date files\n\tdisk_files_set.add_files_to_disk(inventory.get_out_of_date_files())\n\n\t# Add files in the disk that aren't in a state in previous versions\n\tlogical_path_map = {}\n\tfor logical_path in inventory.get_current_state[version].split(','):\n\t\tdisk_f = inventory.get_log"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t# Check that we have write permissions for all users.  The user has\n\t# write permission for the top-level directory of inventory, a subdirectory\n\t# of the version in inventory, an entry in the map, and any subdirectories\n\t# of the map.\n\tgroup = uid_to_group(self.user.user_id)\n\tassert group.gr_name == \"root\"\n\tu = self.user\n\tif (group.gr_owner!= u):\n\t\traise PermissionDenied(__(\"Permission denied: %(user)s"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_map = {}\n\tlogical_paths = inventory.get(version, ())\n\tfor logical_path in logical_paths:\t\t\t\n\t\tif \"*\" not in logical_path:\n\t\t\t# Skip files which are not listed in all\n\t\t\t# earlier versions of the inventory.\n\t\t\tlogical_path = pathnames[version] + \"*\" + logical_path\n\t\t\tif os.path.exists(logical_path):\n\t\t\t\tcontent_files = [f for f in os.listdir(logical_path) if os.path"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\t# Initialize a new dictionary:\n\tlogical_path_dict = {}\n\t\n\t# Iterate over logical_paths\n\tfor logical_path in inventory:\n\t\t\n\t\t# Get the logical path\n\t\tlogical_path_split = logical_path.split('/')\n\t\t\n\t\t# Try to split the version\n\t\ttry:\n\t\t\t_, version_split = logical_path.split('v', 1)\n\t\texcept ValueError: # not a viable option # not a logical path\n\t\t\tlogical_path_split = []\n\n\t\t# Add"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tpaths = set()\n\tfor action in inventory.data['actions']:\n\t\t\tfull_path = os.path.join(action['path'], action['file'])\n\t\t\tif any(map(lambda x: full_path.startswith(x), version.get_logical_paths(action['in']))):\n\t\t\t\tpaths.add(full_path)\n\t\treturn paths\n\t\n\n#\n# Check if a file reference to a logical path is valid and can be referenced by a file\n#\ndef map_logical_path_validation"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_files =  {}\t\t# key is a logical path from state in inventory\n\tcontent_files = {version:set()} # key is a contentfile from state in inventory\n\n#####\tget_logical_path_file_by_state(state, content_file)\t\t\t\t\t\t\t\t\t\t\t\n\tfor state in inventory:\n\t\tfor content_file in state.get('file_lists', None):\n\t\t\tif content_file.get('state', None) in inventory[state][\"_files_to_consider\"][version]:\n\t\t\t\tfor path in"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_map = {}\n\tlogical_path_map[''] = inventory.get_logical_paths('')\n\tfor key, value in inventory.get_version_files(version, '').items():\n\t\tlogical_path_map[key] = value\n\treturn logical_path_map\n\n\ndef get_file_logical_path(path, version, type='content'):\n\t\"\"\"\n\tGet a file mapping of logical location to state for logical path and type \n\tto the version \n\t\"\"\"\n\tfile_"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_in_state = {}\n\tall_files = inventory.all_files\n\tlogical_path_in_state['state_0'] = set([item for item in all_files if\n\t\t\t\t\t\t\t\t\t\t  version in inventory.get_state_0(item)])\n\tlogical_path_in_state['state_1'] = set([item for item in all_files if version in inventory.get_state_1(item)])\n\tlogical_path_in_state['state_2'] = set([item for item in all_files"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\tlogical_path_map = { \"base\" : {}, \"next\" : {}, \"previous\" : {}, \"previous_p\" : {} }\n\tcurrent_version = version.split(\".\")[0]\n\t\n\tif current_version in inventory.base:\n\t\tcurrent_files = inventory.base[current_version].split(\"/\")\n\t\t\t\t\n\t\tfor i in range(0, len(current_files) - 1):\n\t\t\tif current_files[i] in inventory.next:\n\t\t\t\tlogical_path_map[\"next\""}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tfiles_in_state_dict = {}\n\tcurr_state = None\n\tnew_state = None\n\tfor root, dirs, files in os.walk(inventory):\n\t\tinventory_dir = os.path.join(root, inventory_dir)\n\t\tstate = os.path.join(inventory_dir, version.state_dir_prefix)\n\t\tfiles = [f for f in files if\n\t\t\t (os.path.isfile(os.path.join(state, f))\n\t\t\t\tand not f.startswith('.' + os.sep))"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\ts_failures = 0\n\ts_changes = 0\n\ts_files = 0\n\t\n\t# Validate fixities\n\tfor fix in fixity:\n\t\tif fix['type'] == 'File':\n\t\t\t\ts_files += 1\n\t\t\telif fix['type'] == 'Error':\n\t\t\t\ts_failures += 1\t\t\t\t\t\n\t\n\t# Check files and files to delete if they are not in manifest\n\tif s_files!= len(manifest_files):\n\t\ts_changes += 1\n\tif s_files!= 0 and s_files"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\tvalid_ids           = {}\n\tinvalid_ids        = {}\n\n\tif fixity.id in manifest_files:\n\t\tfor f, e in fixity.id_to_id.items():\n\t\t\tpath_elem    = manifest_files[fixity.id]\n\t\t\tdata_elem    = manifest_files[f]\n\t\t\tif e.parent.is_dir():\n\t\t\t\tdata_elem, path_elem = path_elem, data_elem\n\t\t\t\n\t\t\tif data_elem.is_file():\n\n\t\t\t\tvalid_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfiles = []\n\tmissing_files = []\n\tfor manifest_file in self.manifest_files.values():\n\t\tif manifest_file['type'] == file_type.fixity and manifest_file['filename']!='manifest.xml':\n\t\t\tfiles.append(manifest_file['filename'])\n\t\n\t# Check all files\n\tfor filename in files:\n\t\ttry:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tif f.read() =='manifest.xml':\n\t\t\t\t\tmissing_files.append(filename)\n\t\t\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tif fixity:\n\t\tif \"FIXATION\" in fixity:\t\t# FIXATION must be present in manifest\n\t\t\tfixity_files = fixity[\"FIXATION\"]\n\t\telse:\n\t\t\tfixity_files = []\n\telse:\n\t\tfixity_files = []\n\tfor file in fixity_files:\n\t\tfor manifest in manifest_files:\n\t\t\tif manifest[\"key\"] == file:\n\t\t\t\tbreak\t# found a manifest with the same name of a file\n\t\tif manifest:\n\t\t\tbreak\n\t# if any file was not found\n\tif not fixity"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t#print 'Validate fixity...'\n\n\tall_files = []\n\tfor file in self.files:\n\t\ttodefile = file.name.lower()\n\t\tfix = fixity.strip()\n\t\tignore = set(['include', 'exclude', 'comment'])\n\n\t\tif tdofile.startswith('_') or (tdofile.startswith('.') and tdofile[1:] in manifest_files):\n\t\t\tif ignore:\n\t\t\t\tif tdofile.endswith('.py'):\n\t\t\t\t\tif not fix.start"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\terrors = 0\n\tfor fixity_file in fixity.get_files().values():\n\t\t# If file does not exist, it will be skipped.\n\t\tif not os.path.isfile(fixity_file):\n\t\t\tcontinue\n\t\t# If the file is not a template, it could be a binary.\n\t\tif not fixity_file.is_bin():\n\t\t\tcontinue\n\t\t# The file may be missing any required elements.\n\t\tfor fixity_ref in fixity_file.get_references():\n\t\t\tif not fixity_ref in get_inventory"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\tinventory_files = manifest_files[\"inventory_files\"]\n\t\n\t# check that files with the same names as fixities exist\n\tfor fixity in fixities:\n\t\tif fixity.filename not in inventory_files:\n\t\t\tprint \"Invalid fixity: {}: File {} doesn't exist in manifest.\".format(fixity.id, fixity.filename)\n\t\t\traise SystemExit\n\t\t\t\n\t# if we can find the block, we know it has at least one file, it's valid.\n\t# if we cannot find the block, it might have an extra"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfix_type = fixity['type']\n\tfixity_type = fixity['type'].split('|')\n\n\t# Check that this block is in the inventory\n\tif fixity['type'] in manifest_files:\n\t\t# Check that this block is valid for all types and that there is\n\t\t# at least one file in the manifest.\n\t\tif fixity_type[0] not in {'c', 'c#', 'cpp', 'objc'}:\n\t\t\tself.fail(\n\t\t\t\t('Missing'+ fixity['type'] +'file in manifest.'"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tvalid_fixities = set()\n\tseen = set()\n\tfor f in fixity['fixities']:\n\t\tfor v in f['valid']:\n\t\t\tif manifest_files[v['path']]:\n\t\t\t\tseen.add(f['path'])\n\tfor f in fixity['fixities']:\n\t\tif f['valid']!= sorted(list(set(f['path']) - seen)):\n\t\t\tvalid_fixities.add(f)\n\n\tfixity['fixities'] = valid_fixities\n\nclass MockFixity:\n\tdef __"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\t# We make a copy of the fixity as this may be passed to run_fixity\n\t# as the fixity object.\t\n\tfixity_copy = self.fixity_copy.copy()\n\t# Update the fixity object with what we expect, and only make it\n\t# available to override the file/dir names\n\tfixity_copy.update(fixity)\n\t\t\n\t# Now we're going to do a check for all the files in the manifest.\n\t#\t\n\t# This is probably a little funky because there's still one set of"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfiles = []\n\tfor file in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, file)):\n\t\t\tfiles.append(os.path.join(path, file))\n\t\t\n\t\telif os.path.isdir(os.path.join(path, file)):\n\t\t\tfiles.extend(files_list(os.path.join(path, file)))\n\t\n\treturn files\n\ndef is_absolute(path):\n\t\"\"\"\n\tCheck whether `path` is an absolute path and\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn [os.path.join(path, name) for name in os.listdir(path) if os.path.isfile(os.path.join(path, name)) and not os.path.islink(os.path.join(path, name))]\n\t\n#\n# \t\tdef dir_list(path=None):\t\t\t\t\n# \treturn  os.listdir(path)\n\ndef make_dir(dirname):\t\t\t\t\t\n\tpath = tempfile.mkdtemp() \t\t\n\t# print (\"path: %s\" %path)"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn [name for name in listdir(path) if isfile(join(path, name))]\n\ndef count_words(list):\n\t\"\"\"Count the number of words in a list of words.\"\"\"\n\treturn len(words(list))\n\ndef count_words_for_words(text):\n\t\"\"\"Count the number of words in a text.\"\"\"\n\treturn len(words_list_from_string(text))\n\t\ndef count(text, mode='n'):\t## if 'q' or 'Q', end. \n\t\"\"\"Count the number"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tf_out = open(path, 'r').read()\n\tfiles = f_out.split('\\n')\n\treturn files\n\n#--------------------------------------------------------------------------------------------------------#\n\ndef files_info(path):\t\"\"\"\n\tReturn the file names\t\"\"\"\n\tf_out = open(path, 'r').read()\n\tfiles = f_out.split('\\n')\n\treturn files\n\n#--------------------------------------------------------------------------------------------------------#\n\ndef files_contains(path, f):\t\"\"\"\n\tReturn the files that match `f`\t\"\"\"\n\tf_out = open"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfiles = []\n\tfor root, directories, filenames in os.walk(path, topdown=False):\n\t\tfor filename in filenames:\n\t\t\tfilepath = os.path.join(root, filename)\n\t\t\tfiles.append(filepath)\n\treturn files\n\n\nasync def download(url, local_dir, filename=None):\n\t\"\"\"\n\tDownload an URL to a local file\n\t\"\"\"\n\tdata = urllib.request.urlopen(url)\n\tlocal_path = os.path.join(local_dir, filename)\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfiles = os.listdir(path)\n\tfiles_list = []\n\tfor file in files:\n\t\tif file.endswith(\".h\"):\n\t\t\tfiles_list.append(file)\n\treturn files_list\n\ndef get_parameters(parameters):\n\t\"\"\" \n\t\tParse parameters into a dict in the following format:\n\t\t\n\t\t{\n\t\t\"name\": variable_1, \n\t\t\"unit\": variable_2\n\t\t}\n\t\"\"\"\n\treturn_params = {}\n\tfor param in parameters:\n\t\treturn_params"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn os.listdir(path)\n\ndef get_md_files(path):\t\"\"\"\n\tExtract and return all markdown files in `path` in the same order\t\"\"\"\n\tdef file_ext_to_int(f):\n\t\text = f.split(\".\")\n\t\treturn int(''.join(filter(str.isdigit, ext[-1])))\n\treturn sorted(files_list(path), key = file_ext_to_int)\n\ndef mark_file(path = \"mark_files\"):\t\"\"\"\n\tMake markdown"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfiles = []\n\tfor r in os.listdir(path):\n\t\tfiles.append(os.path.join(path, r))\n\treturn files\n\ndef get_file(path, ext):\t\"\"\"Return file with extension `ext` in path. If no match is there, return None\"\"\"\n\tfor f in files_list(path):\n\t\tif os.path.normcase(f).endswith(ext):\n\t\t\treturn f\n\treturn None\n\ndef get_file_size(path):\n\t\"\"\"Returns size of files in `path`\"\"\"\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn os.listdir(path)\n\n\ndef read(path):\n\treturn np.load(path)\n\ndef save(z, dest_dir):\n\t\"\"\"\n\tSave `z` as an array in `dest_dir`\n\t\"\"\"\n\tif not os.path.isdir(dest_dir): os.mkdir(dest_dir)\n\tnp.save(dest_dir+os.path.sep+os.path.basename(path_dir), z)\n\n\ndef sigmoid(x):\n  # TODO Implement this function!"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfilelist = [ f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\tfilelist.sort()\n\treturn filelist\n\n\ndef run_command(commandargs):\n\tif len(commandargs) == 1:\n\t\targs = commandargs[0].split(' ')\n\telse:\n\t\targs = commandargs\n\tsubprocess.Popen([eval(\"subprocess.Popen(\"+args[0]))]+args[1:])\n\n\ndef run_subprocess_cmd("}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\tfiles_list = []\n\tfor xmli in files:\n\t\ttry:\n\t\t\tfiles_list.append(str(xmli.name))\n\t\t\ttry:\n\t\t\t\tif len(xmli.extras) > len(files_list):\n\t\t\t\t\textras = xmli.extras\n\t\t\t\t\tfor k in extras.keys:\n\t\t\t\t\t\tif all(v is not None for v in vars(extras[k])):\n\t\t\t\t\t\t\textras[k]\n\t\t\tfinally:\n\t\t\texcept:\n\t\t\t\tpass\n\t\texcept:\n\t\t\tcontinue\n\n\n\tkey = []"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\t## Initializations\n\tfile_count = [] ## count of files in the folder or zipfile\n\tfile_dict = {} ## {name: [data]}\n    \n    \n    ## Process files\n\tfor f in files:\n\t\t\t\n\t\t\t## get filename \n\t\t\tfilename = os.path.basename(f[0])\n\t\t\t\n\t\t\t## Get data\n\t\t\twith open(f[0], 'r') as read:\n\t\t\t\twith open(f[0].replace('.zip', ''): 'r') as read_zip:\n\t\t\t\t\twith open(f["}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\tgroupings = defaultdict(list)\n\tfor xml_fn, filename in zip(_files, files):\n\t\t# We use this to avoid the wrong reading. This is still a bit awkward.\n\t\txmlfile = open(xml_fn, \"rt\")\n\t\txmldoc = minidom.parse(xmlfile)\n\t\tfor cnt, tag in enumerate(xmldoc.getElementsByTagName(\"filename\")):  \n\n\t\t\t# Check that the filename is present.\n\t\t\t# There's no great way than to check for equality"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\tif is_zipfile(source):\n\t\tfiles = zip_contents(files)\n\n\tif not xml_filename in files:\n\t\traise Exception('xml_filename does not exist in source')\n\t\t\t\n\telif not is_archive_filename(source):\n\t\tfiles = [source + filename for filename in files]\n\n\telif not is_zipfile(source):\t\n\t\tfiles = [source + filename for filename in files.split(sep=os.path.sep)]\n\t\t\n\tgrouped = {os.path.basename(filename):"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\timport zipfile\n\tzip_file = zipfile.ZipFile(source)\n\tif not xmls:\n    \t\txmls = _find_all_XML_files(os.path.dirname(source))\n\treturn (_find_files(filename, os.path.split(filename)[1], xmls, files) and\n\t\t_group_files_by_xml_filename(filename, xmls, [\n\t\t\t\t\t\t\t\tfile.filename for file in _sort_file_list(files)]))\n\n\ndef _find_files(filename, root, packages, files):"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n    def group_by_xml_filename(files):\n\t\n        # dictionary to be returned\n\t\t\n        all_groups = {}\n        \n        # for each element\n\t\t\n        for filename in files:\n\t\t\n            # read in elements\n\t\t\t\n            xml = open(filename, \"rb\").read()\n\t\t\t\n            # decode the XML string to a string\n            xml = ElementTree.XML(xml)\n    \n\t\t\t# extract the root element\n\t\t\n        group_root = xml.find(\"document/core-component\")\n\n##\t\t# split"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\tgroupedFiles = GroupsByXML.ParseGroupsBy(xmls, files)\t\n\tfilenameDict = GroupsByXML._groupsByXMLFilename(groupedFiles)\t\n\t\n\treturn filenameDict\t\n\t\n\t\ndef get_files_list(directory, filter = None, files = None, xmls = None):\n\t\"\"\"\n\tGet a list of files in directory based on a filter.\n\t\n\tParameters\n\t----------\n\tdirectory : str\n        Path directory to search for XML files\n\tfilter : list\n        optional"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\td = {}\n\tif isinstance(files, list):\n\t\tif not files:\n\t            raise ValueError('No file provided')\n\t\tfor file in files:\n\t\t    name = os.path.splitext(os.path.basename(file))[0]\n\t\t    d[name] = file\n\t\n\telif isinstance(files,  str):\n\t\td[files] = files\n\telse:\n\t\traise TypeError(\"files should be a list or str, not {}\".format(type(files)))\n\t\t\n\tfor xml"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\timport zipfile\n\t# Extract all files in the folder or zipfiles\n\t# for xml_filename in files:\n\t# \tif not zip_filename:\n\t# \t\t# Extract only the XML files\n\t# \t\t# if not zip_filename:\n\t# \t\tzip = zipfile.ZipFile(xml_filename,\"r\")\n\t# \t\tzip.extractall(src)\n\n\timport os\t\n\txml_files = []\n\txmls_name = []\n\tos.chdir(source)\t\n\ttry:\n\t\tsource"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\td = {\"filenames\": []}\n\n\tfor filename in files: \n\n\t\tif os.path.isfile(filename):\n\t\t\t\t\t\t\n\t\t\tbasename, _ = os.path.splitext(filename)\n\n\t\t\tif basename:\n\n\t\t\t\tif basename not in d[\"filenames\"]:\n\t\t\t\t\td[\"filenames\"].append(basename)\n\t\t\n\treturn d\n\n\ndef _parse_xml_data(filename):\n\t\"\"\" parse XML file contents \n\n\tParse XML file, read files and return a dict. \n\n\tThe keys will"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tprint(f'Looking for: {file_path}')\n\tfilename = os.path.basename(file_path)\n\tprint('filename:'+ file_path)\n\tpath_to_file = os.path.join(root_path, prefix, filename)\n\tprint('path_to_file:'+ path_to_file)\n\treturn os.path.isfile(path_to_file)\n\n\n\ndef load_package_metadata(package, dir_name, version_string, name, root_path):\n\t\"\"\"\n\tLoad"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tlogging.info(\"Looking at '{}' for {}\", file_path, prefix)\n\tif os.path.isfile(file_path):\n\t\tif (prefix.upper().endswith(\".xml\")):\n\t\t\twith open(file_path, \"r\") as f:\n\t\t\t\tfor line in f:\n\t\t\t\t\tlines = line.split(\"\\t\")\n\t\t\t\t\tif prefix in lines[0]:\n\t\t\t\t\t\treturn lines[3].strip()\n\t\treturn False\n\n\ndef match_file_by_path(parent_path, file_path):\n\t\"\"\"Test"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\treturn os.path.isfile(file_path) and os.path.splitext(file_path)[0].startswith(prefix)\n\n\n########################################################################################################################################\n\ndef extract_regexp(text, pattern):\n\t\"\"\"\n\tExtract the text matching a regex `pattern` from `text`\n\nRetorna la rexpr\t\n\"\"\"\n\tresult = None\n\tif pattern!= None:\n\t\tfor pat in glob.glob(os.path.dirname(pattern) + '/' +  pattern + '/' + pattern):\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\treturn os.path.isfile(os.path.join(package_dir, prefix, file_path))\n\ndef has_docstrings(doc) :\n    soup = BeautifulSoup(doc, 'html.parser')\n    return soup.desc.string!= ''\n\n#######################################################################\n###       Fun\u00e7\u00f5es que realizam o tokenizer       ###\n#######################################################################\n\ndef split_token(doc, tokenize=True):\n    soup = BeautifulSoup(doc, 'html.parser')\n    line      = []\n    token"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n    for _file in [f for f in os.listdir(file_path) if '.' not in f and f.startswith(prefix)]:\n        try:\n            # We need an absolute path so we can use Path.joinpath\n            file_path_p = Path(file_path).absolute()/Path(prefix)/'.'\n            # And now check the path against the package\n            doc_path = file_path_p / doc_path_pattern.get(prefix, '.')\n            if doc_path.is_dir():\n                "}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tpattern = re.compile('{}'.format(prefix.strip('.')))\n\tfile_is_file_matches = bool(bool(re.search(pattern, file_path)))\n\treturn file_is_file_matches\n\n\ndef process_document(pdf, file_path):\t\n\t\"\"\"\n\tIdentify if a given document belongs to a given package if available.\n\t\n\t:param pdf:\n\t:param file_path:\n\t:return: `True` if the document is a package, otherwise `False`\n\t:rtype\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\t# Load the package\n\ttry:\n\t\tpackage = Package.get_package(f\"../models/modules/models/{prefix}\")\n\texcept Exception as e:\n\t\traise Exception(\"{0} does not exist. {1}\".format(prefix, e.args))\n\treturn package.file_name.find(file_path) >= 0\n\n@model_collection(model_type=\"datasets\").register\ndef file_paths(package: Model, files: Set[str]):\n\t\"\"\"Return a list of `files` of `package"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tstart = 0\n\tfilt = False\n\tif prefix == '': raise ValueError('Empty prefix')\n\tif '\\\\' in prefix: filt = True\n\tfor start, char in enumerate(prefix):\n\t\tif filt:\n\t\t\tif char == '\\\\': break\n\t\t\t\t\n\t\tfile_path = file_path.replace(prefix, '')\n\t\tif '\\\\' in file_path:\n\t\t\tfilt = True\n\t\t\tbreak\n\n# Testado ______________________________________________________________\n\telif filt:\n\t\tstart = file_path.rfind('"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\t#Identify if the file is in a package\n\tif prefix.partition(\"/\")[2] in file_path: \n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef search(pkg, file_path):\t\n\t\"\"\"Identify if the file is a package and get to its name.\n\n\tReturns\n\t-------\n\tstr\n    A string of the package found and returned.\n\nRetorna `None` se ao identificador `file_path` fora do `pkg`\ne retorna `None` se ao"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t# TODO: Replace me\n\twith lopen(file_path, \"r\") as f:\n\t\tfor line in f:\n\t\t\tif re.match(r\"[a-zA-Z]+([.])?[a-z]+\", line.lower()):\n\t\t\t\tf.seek(0)\n\t\t\t\treturn line.lower().startswith(prefix.lower())\n\treturn False\n\ndef get_file_identifier(file_path):\n\t\"\"\"\n       Return the filename to this file.\n\t\"\"\"\n\tfilename = file_path.rstrip(\"/\")"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfilenames = [str(x) for x in files]    \n\tdef filterende(filt, paths):\n\t\tif filt == 'name':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\treturn list(map(filterende, list(set(filenames)), filterende(prefix, paths)))\n \n#Get a list of files in a subdirectory. \ndef get_files_in_dir(dir):\n    files = []\n    for root, dirs, files in os.walk(dir"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tif not len(files):\n\t\tfiles = glob.glob(prefix+\"*.\")\n\treturn [x.replace(prefix, \"*\") for x in files if x.startswith(prefix)]\n\ndef check_version_for_unversioned_archives(output_filename_directory, directory_to_build, unversioned_archive_suffix, exclude_archives_to_unversioned_path=None):\n\t\"\"\"\t\n\tGet files for which the version of the `package` is unversioned.\n\n\tIf a file has exactly `unversion"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t# check: check filepath with prefix\n\tif len(files) < 5:\n\t\traise Exception('files is not enough')\n\t# build paths for every files\n\tfor filename in files:\n\t\tfilepath, ext = os.path.splitext(filename)\n\t\tif len(prefix) < 1:\n\t\t\t# ignore the filename and return\n\t\t\tfiles.pop(files.index(filename))\n\t\t\tdirpath = prefix + os.sep + filepath\n\t\t\t# create new path, return\n\t\t\treturn [ prefix + os.sep +file for file in ["}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tprefix = prefix.replace('.', os.path.sep).lower()\n\tprefixPatterns = [[m for m in nf.glob(prefix + '/' + f)\n\t\t\t\t\t\tif m.endswith('.' + p[-1:])] for p in files]\n\treturn [x for sub in prefixPatterns for x in sub]\ndef select_filenames_by_filename(filename, path, files):pass\ndef select_filenames_by_suffix(file, files, extensions=[]):\n\n\t\"\"\"\n\t\tGet files which"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\timport os\n\timport subprocess\n\t\n\tfiles = [f.format(prefix) for f in files]\n\t\n\treturn os.path.join(*((os.path.split(f)[0]).split(os.path.sep)) + os.path.split(files[0])[1][1:]) \t\t\t\t\t\t\n\ndef generate_pk(filename):\n\n\t'''\n\tGenerate PK from a PDF file \n\t\n\tParameters\n\t----------\n\tfilename : str\n    \tFull path of a PDF file\n    \n\tReturns"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfilesDir = os.listdir(files)\t\t# List directory files in this directory if it contains any filenames\n\tfiles = list(filter(lambda name: os.path.splitext(name)[1] == suffix, filesDir))\t\t# Remove extensions\n\tfiles.sort(key= lambda filePath: os.sep + os.path.basename(filePath))\t# Sort filenames by path and name\n\tfiles = list(set(files))\t\t# Remove duplicates and sort files\n\treturn files\n\ndef download_packages(packages, packageName"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\tfile = '*' # Filename prefix is not used for all files\n\t\n\tif prefix!= '':\n\t\tfile = prefix + '.' # Add a filename of prefix\n\t\t\t\t\n\t\tfiles = files.rglob(file)\n\t\treturn files\n\t\n\telse:\n\t\tfiles = file + \"*\" # No prefix is used\n\t\t# files = files.rglob(file)\n\t\treturn files\t\n\n\n\ndef select_filenames_by_prefixv2(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfiles = []\n\tfor filename in files:\n\t\text = os.path.splitext(filename)[1]\n\t\tif exts.match(ext):\n\t\t\tfiles.append(filename)\n\treturn files\n\ndef select_filenames_by_path(paths, recursive=True):\t\"\"\"\n\tGet files which belong to documents in a path.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `paths`\n\nParameters\n----------\npaths : list of str\n    Paths\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfiles_lst = files.split(',')\n\tfiltered_lst = [l for l in files_lst if prefix in l and l.startswith(prefix)]\n\treturn filtered_lst\n\ndef find_file(prefix, start, end):\n    \"\"\"\n    Get file matching a prefix.\n\n    Retorna um file path, ocorrendo, em `start`..`end`, ocorrendo dentro das\n    ficheiros que ainda n\u00e3o forem filtradas pelo prefix.\n\n    Parameters\n    ----------"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn sorted([x.stem for x in files if  # pylint: disable=too-many-nested-blocks\n\t\t\t\tprefix in x.stem]) # pylint: disable=g-complex-comprehension\n\ndef get_doc_prefix(doc):\n\t\"\"\"\n\tReturns prefix of the document.\n\nRetorna o prefix do documento baseado no nome do arquivo\n\nParameters\n----------\ndoc : ncbi.Document object\n    Document object.\nReturns\n-------\nstr\n    prefix of the document\n\t\"\"\"\n\treturn"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\tfile_list = os.listdir(folder)\n\tgroups = dict()\n\t\t\n\tfor file in file_list:\t\t\t\t\t# For each file in the folder\n\t\t\t\t\t\t\t\t#\n\t\tfile_name = file[:-4]\t\t\t        # Name of the XML file\n    \t\t\t\n\t\ts = '<file id=%s type=%s title=%s>\\n' # XSD file data\n\t\ts = s % (file_name, file_name[-4:], '<file>') # XSD data\n\t\t\n\t\twith open(folder+"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tf = []\t\t\t# empty dict\n\tc = 0\t\t\t# counter\n\tfor file in os.listdir(folder):\t# folder path\n\t\tif file.endswith((\".xml\")): # file's extension\n\t\t\twith open(folder+\"/\"+file) as xml_file:\t\n\t\t\t\tif not xml_file.closed:\t\t# checks if file is a closed file (usually file that's open)\n\t\t\t\t\tf.append({'root_file':folder+\"/\"+file, \t\n\t\t\t\t\t'sub_folder':'', \t\t\t\t\t\n\t\t\t\t\t'label_"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfrom pathlib import Path \n\timport xml.etree.ElementTree as ET\n\timport os\n\timport requests\n\timport zipfile\n\timport re\n\n\tif not os.path.isdir(folder):\n\t\traise ValueError(\"Folder for {} not found\".format(folder))\n\n\t# Parse folder xml\n\txml = ET.parse(os.path.join(folder, \"package.xml\"))\n\n\tversion_re = re.compile(\"\\\\d+\\.?\\\\d*\")\n\tpackages = {}\n\tfiles = {}\n\tcount_files"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\timport os\n\timport pathlib\n\timport xml.etree.ElementTree as ET\n\tfrom glob import glob\n\tdef get_file_size(file):\n\t\treturn os.path.getsize(file)\n\n\tfiles = []\n\tif os.path.isdir(folder):\n\t\tfiles = [file for file in os.listdir(folder) if not (file.startswith('.')) and file.endswith('.py') and os.path.isfile(os.path.join(folder, file)) and not os.path.isdir"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tf = folder + \"/\"\n\tprint(\"Reading data from %s: \" + f, flush=True)\n\tfiles_to_explore = glob.glob(f+'*')\n\tprint(\"\\t%i files collected\" % len(files_to_explore), flush=True)\n\t\n\t# Read XML file into a dict\n\tpackage_dict = {}\n\tfor k,v in file_explorer.data_parser(files_to_explore).read_file(f):\n\t\tpackage_dict[k] = v\n\treturn"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\timport os\n\timport xml.etree.ElementTree as ET\n\timport sys\n\tfrom glob import glob\n\timport pkg_resources\n\timport time\n\n\t# Create a folder listing the files in given folder\n\ttry:\n\t\tif not os.path.exists('./'+folder):\n\t\t\tos.makedirs('./'+folder)\n\texcept:\n\t\tpass\n\tos.chdir('./'+folder)\n\tpkgs_list_dir = [x[:-4] for x in os.listdir(os"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfolder = os.path.join(pathlib.Path.cwd(), folder)\n\tfolders = []\n\twith os.scandir(os.path.join(folder, '*')) as entries: \n\t\tfor entry in entries: \n\t\t\tfolders.append(f'{entry.name}')\n\t\n\treturn {f: read_tree(os.path.join(folder, f), os.path.join(folder, '*')) for f in sorted(folders)}\n\n##############################################################################\n#\n# MAIN\n"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tglobal _packages\n\tfilename = folder.split(os.sep)[-1].replace(os.sep,\"/\")\t# Get filename\n\tfilename = filename.split(\".\")[-2]\t\t\t\t# Split filename if there are multiple \n\tbasename = filename.upper()\t\t\t\t\t\t# Get basename of package\n\tbase = os.sep\t\t\t# Directory separator\n\n\t# Read in data\n\tpack_xml = os.path.join(folder, filename + \".xml\")       # Specify package\n\tdata = ET.parse(pack_xml)\t\t\t\t \t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfile_names = []\n\n\tdata = {}\n\t\n\tfor file in sorted(glob(os.path.join(folder, \"*.xml\"))):\n\t\tfile_name = os.path.split(file)[1]\n\t\tfile_name = file_name.split(\".xml\")[0]\n\t\tdata.update({file_name: {}})\n\n\t\tf = open(file, encoding='utf-8')\n\t\tdom = xml.dom.minidom.parseString(f.read())\n\n\t\tdataall = dom.documentElement.getElements"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t# Get data\n\tdata = []\n\tfor dir_path, _, files in os.walk(folder):\n\t\tfor filename in files:\n\t\t\tif os.path.isfile(os.path.join(dir_path, filename)):\n\t\t\t\tfile_name = os.path.basename(filename)\n\t\t\t\tinfo = {\"file_name\": file_name, \"data_type\": \"xml-file\"}\n\t\t\t\tfile_data = readFile( os.path.join(dir_path, filename) )\n\t\t\t\tdata.append( {\"info\": info, \"data"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t# check if file and folder exist, not dir\n\tif len(file_folder.strip())!= 0:\n\t\t# make directories in directory given folder\n\t\t# the path given to the directory is given\n\t\tdir_path = os.path.join(file_folder, prefix)\t\n\t\tdir_exists_path = check.path_exists(dir_path=dir_path)\n\t\tif not dir_exists_path:\n\t\t\t# directory doesnt exist, create it\n\t\t\t# check if it alreay exists\n\t\t\t# the path given to the directory is given"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif not os.path.exists(file_folder):\n\t\tos.makedirs(file_folder)\t\n\tfiles = os.listdir(file_folder)\n\tif filename.endswith('xml') and 'asset' not in filename.lower():\n            files = [f for f in files if not f.endswith('.bmp')]\n    #     if 'Renders' not in filename:\n    #         files = [f for f in files if f.endswith('.jpeg')]\n    # for f in files"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tfile_folder = '/'.join(str(i) for i in range(int(file_folder)+1))\n\tfile_name = file_path.split('/')[-1].split('.')[0]\t#get rid of extension\n\tif 'asset' in file_name and file_folder not in file_name:\n\t\treturn {file_type: asset_type}\n\telif'rendition' in file_name and file_folder not in file_name:\n\t\treturn {file_type: rendition_type}\n\telse:\n\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tglobal current\n\tif current == 'asset' and not file_folder.endswith('.txt'):\n\t\tr, f = prefix.replace('.xml', '.asset'), file_folder + f'\\{prefix.split(\".\")[1]}'\n\n\t\tif not os.path.exists(os.path.normpath(f)):\n\t\t\tr, f = r.strip(), f.strip()\n\t\t\tcurrent.append(f'{prefix.split(\".\")[1]}\\n\\t\\t\\t<package>\\n\\t\\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif file_path.endswith('.xml'):\n\t\txml = file_path.replace(file_path[len(prefix) + 1:filename],'')\n\t\treturn {\n\t\t\t\"type\":\t'asset',\n\t\t\t\"extension\":\t'xml',\n\t\t\t\"filename\":\txml\n\t\t\t}\n\telif file_path.endswith('.json'):\n\t\tjson = file_path.replace(file_path[len(prefix) + 1:filename],'')\n\t\treturn {\n\t\t\t\"type\":\t'sound',\n\t\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\timport glob\n\tfiles = glob.glob(f'{prefix}/**/*.xml$', recursive=True)\n\tfor file in files:\n\t\twith open(file) as f:\n\t\t\tsoup = BeautifulSoup(f.read(), \"xml\")\n\t\tpackages = soup.find_all('package')\t\t\n\t\tfile_folder = prefix.rpartition('/')[2].strip( '/' )\n\t\tpackages_info_filename = f'{prefix}/{file_folder}/{file_folder}_{file}.xml'\t\t\n\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\txml_base = \"\"\"<dts:asset xmlns:dts=\"http://www.w3.org/2000/04/dattransforms\" xmlns:dts:dte=\"http://www.w3.org/2001/XMLSchema\" xmlns:dt\">\n\t<dt:definitions>\n\t<dt:definition>\n\t...\n\t</dt:definition>\n\t</dt:definitions>\n\t<dt:package>\n\t<dt:packagedescription \"dts:asset\"/>\n\t<dt:description"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif not os.path.exists(file_folder):\n\t\tos.makedirs(os.path.join(file_folder, \"content\"))\n\treturn {\n\t\t\"asset\"           : f\"./{dirs[0]}Assets/\"+filename+\".\"+extension,\n\t\t\"rendition\"      : f\"./{dirs[0]}Assets/\"+filename,\n\t\t\"packages\"        : f\"./{dirs[0]}content/\"+prefix+\".\"+extension,\n\t\t\"file_folder\"     : file_"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tarquivo_xml = file_path.replace(prefix+file_path.split('.')[-1], '') + '.asset'\n\treturn os.path.join(config[\"ROOT_DIR\"],\n\t                   os.path.dirname(arquivo_xml), \n\t                   'assets', \n\t                   os.path.splitext(os.path.basename(file_path))[0] + '.asset')\n\n\ndef _get_file(config, filename):\n\t\"\"\"\n\tBusca o arquivo e retorn"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\timport os.sep\n\timport xml.etree.ElementTree as ET\n\tfrom datetime import datetime as dt\n\tfrom os import path\n\timport time\n\tif (file_path==\"/\"):\n\t\tif (\".asset\" not in path.basename(file_path)):\n\t\t\tfile_path += os.sep + \".asset\"\n\t\tif not(os.path.exists(\"assets\")):\n\t\t\tos.makedirs(\"assets\")\n\t\twith open(file_path, 'r') as stream:\n\t\t\ttree ="}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\treturn self.send(data={ \"lang\": lang,\"file_path\": file_path })\n\n@app.route(\"/add_author\")\ndef add_author():  \n\tlang = request.args.get(\"lang\")\n\t\n\tif str_regex.search(lang) :\n\t\tauthor = str_regex.search(lang).group(0)\n\n\t\treturn json.dumps({\n\t\t\t\"result\": 0,\n\t\t\t\"author\": author\n\t\t})\n\telse:\n\t\t\n\t\tauthor_en = randint(1,2)\n\t\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tresponse_add = \"Artigo\"\n\t#en = filePath.split(\"/\")[5]\n\tresponse_add = base.response_model_handler.get_response(method=get, path=file_path, lang=lang, additional=response_add,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  status_code=[200,201,204], error_code=[40104, 40101, 40100])\n\tresponse_add_en = \"\"\"\n\t<!doctype html>\n\t<html>\n\t{$response$}\n\n</html>\n\n\t\"\"\"\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n\tdoc = {lang: file_path}\n\t\n\tdata.set_data(self.config, doc)\n\t\n\treturn data\n\n\n\n\nif __name__ == \"__main__\":\n\tparser = argparse.ArgumentParser(description=\"Generate PDFs for given config\")\n\t\n\tparser.add_argument(\"-p\", \"--path\", default=\"\", type=str, help=\"Path to assets\")\n\tparser.add_argument(\"--config\", default=\"config.txt\", help=\"Path to config file\")\n\targs = parser.parse"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tpdf = pdfplumber.open([file_path])\n\tpdf_file = pdf[self.file_title]\t\n\tself.pdf_file = pdf[self.file_title].pdf_file[file_path].page_array\n\tself.pdf_title = pdf[self.file_title].pdf_file[file_path].title\n\tself.pdf_content = pdf[self.file_title].pdf_file[file_path].content\n\tself.pdf_page_index = pdf[self.file_title].pdf_file[file_"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\trequest = {\n\t\t\"data\": {\n\t\t\t\"files\": {\n\t\t\t\t\"original\": file_path\n\t\t\t},\n\t\t\t\"lang\": {\"lang\": lang}\n\t\t}\t\n\t}\t\n\treturn self.__send(\"https://api.s3cdn.com/buzz-upload/upload/\", request).text['data']\n\n\tdef __write_to_text(self):\n\t\t\"\"\"Write the PDF to text\"\"\"\n\t\treturn \"\"\"*Artigos recibidos:*\\n\\n\n    <br>\n    <br>\n    <"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tpath_to_file = os.path.abspath(file_path)\n\tf = open(path_to_file, \"rb\")\n\tbyte_string = f.read()\n\tf.close()\n\tcipher_suite = self.__encryption(secret_key, byte_string).decode()\n\tdata = {\n      \"original\": byte_string,\n      \"en\": cipher_suite\n    }\n\t\n\turl = 'https://api.wptalk.us/recipes/add'\n\theaders = {\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tfile_format = self.get_file_format(file_path)\n\tif os.path.exists(file_path) and self.lang == 'en':\n\t\text = file_path.split(\"-\")[1]\n\t\tif ext in self.ext and self.get_extension(file_format) == '.pdf':\tself.add_file(file_path.replace(ext, ''))\n\tself.write({\n\t\t'name' : self.name,\n\t\t'thumbnail' : self.thumb,\n\t\t'artition'"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n\tself.language = lang\t\t# default setting\n\tself.filepath = file_path \n\t\t\n\t#print(\">>>\",self.filepath)\n\t#print(\">>>\",self.language)\n\t\n\twith pdfplumber.open(self.filepath) as pdf:\n\t\t\t\t\t\t\n\t\t#print(pdf)\n\t\t#pdf.close()\n\t\ttext = pdf.pages\n\t\t\n\t\t# Get text from each page\n\t\ttext = str(text.extract_text())\n\t\tif(self.language == \"all\"):\n\t\t\tself."}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tif os.path.isfile(file_path):\n\t\tos.remove(file_path)\n\n\tif os.path.isfile(self.cache_file):\n\t\tres = self._fetch_file(file_path)\n\t\tif res:\n\t\t\twith open(self.cache_file, \"w\") as cache_file:\n\t\t\t\tjson.dump(res, cache_file, indent=4)\n\n\tself._create_cache_file()\n\tfilename_with_lang = f\"{self.language}.{file_path.split('.')[-"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n\t# check if the language is in the same country as the document\n\t# if not, search the documents' folder for the languages in its country\n\tresult = self.get_data()\n\tfor i in result['continent']:\n\t\tif result['continent'][i] == env.LANGUAGE_FULL_COUNTRY.get(lang):\n\t\t\ten_lang = env.LANGUAGE_FULL_COUNTRY.get(lang)\n\t\t\tbreak\n\telse:\n\t\tfor i in result['continent']:\n\t\t\tif"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tadditional_files = set()\t\n\tfor tiff in [os.path.join(file_path, f) for f in os.listdir(file_path)]:\n\t\ttry:\n\t\t\twith Image.open(tiff) as image:\n\t\t\t\timage.load()\n\t\t\t\tif image.width > image.height or not image.get_data_format() in IFFT_FORMATS:\n\t\t\t\t\tadditional_files.add(tiff)\n\t\texcept IOError:\t# EIO: read only\n\t\t\tif os.path.isfile(tiff"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\treturn self.save(file_path, basename, \"image/asset\")\ndef add_asset2(self, basename, file_path):\t\n\t# TODO add this as part of the asset creation logic?\n\t# Return the new asset\n\treturn self.save(file_path, basename, \"image/asset2\")\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\toutput = os.path.join(self.output_dir, basename)\n\tif not os.path.exists(output):\n\t\tos.makedirs(output)\n\ttemp_file = os.path.join(self.output_dir, basename)\n\twith open(temp_file, 'w') as tmp_file:\n\t\ttmp_file.write(file_path.replace('/', '~'))\n\treturn temp_file\n\n\ndef assign_artigo02(src_filename, out_destination):\n\t\"\"\"\n\tAss"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tbase,file = os.path.split(file_path)\n\text = os.path.splitext(file)[1]\n\tif ext in ['.tiff','.png','.jpg','.jpeg','.png']and base!= '':\n\t    asset_name = base + '_' + ext\n\t    os.mkdir(path=asset_name)\n\treturn asset_name\t\n\nif __name__ == '__main__':\n\t# Add the path to the asset\n\tartigo02_path = input(\"Path to"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tassetpath = f\"{self.asset_folder_path}{basename}\"\n\timg = Image(file_path)\n\tif img.file_extension == \".tiff\":\n\t\ttry:\n\t\t\twith io.open(assetpath, \"rb\") as fp:\n\t\t\t\tbuffer = fp.read()\n\t\t\t\twith open(assetpath, \"wb\") as fp2:\n\t\t\t\t\tfp2.write(buffer)\n\t\t\t\tself.image_loaded = Image(assetpath)\n\t\texcept Exception as e:\n\t\t\traise e\n\t\treturn"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tbase, file_name = os.path.split(file_path)\n\tfile_name.replace(os.sep, '-').replace('./', '')\n\tdir = base + os.sep + file_name.replace(os.sep, '-')\n\tos.makedirs(dir)\n\tshutil.copyfile(file_path, dir + os.sep + basename + '.tiff')\n\tshutil.copyfile(file_path, dir + os.sep + basename + '.jpg')\n\tsh"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\twith open(file_path, \"wb\") as file:\n\t\t# Write the TIFF header\n\t\t# This is the first TIFF in the file\n\t\ttiff.write_tiff(file, self.header, dpi=(self.width,self.height))\n\n\t\t# Write the band\n\t\ttiff.write_band(file, self.data) \n\nclass GIS_LICENSE:\n\t\"\"\"\n\tContains a TIFF metadata structure\n\n\t\"\"\"\n\tdef __init__(self, gis_metadata):\n\t\tself.header = g"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\timport os, uuid, glob\n\timport numpy as np\n\tfrom utils import imread, imwrite # noqa\n\timport os\n\timport subprocess\n\n\t# TODO: create and clean folder in tmp folder of new folder (should run as a separate cmd)\n\t# TODO: make sure that gf03 is clean\n\n\t# TODO: move gf03 into assets folder and clean it\n\t# TODO: move gf03.py to assets folder?\n\n\t# TODO: create a subfolder of gf03 and make it under assets folder\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\n\tself.assets.append({\"path\": basename, \"file\": file_path})\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tfile_path = os.path.join(os.path.dirname(__file__), file_path + basename)\n\t# check file exist and if not create\n\tif os.path.exists(file_path) == False:\n\t\traise FileNotFoundError(\"file \\'{}\\' does not exist\".format(file_path))\n\t# if directory is not exist create\n\tif not os.path.exists(os.path.dirname(file_path)):\n\t\tos.makedirs(os.path.dirname(file_path"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\tzpath = zip_path.replace('/', 'zip')\n\tfile_list = glob.glob(zpath)\t# get list of file names in that zip\n\tprint('The following files are in zipfile:')\n\tfor file in file_list:\n\t\tprint(file,)\n\t\twith zipfile.ZipFile(file) as zip_ref:\n\t\t\tprint(zip_ref.namelist())\t# get names of sub-files/directories in zipfile\n\n\treturn dict(zip(file_list, file_list))\n\n\ndef _"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tdircache = dict()\n\tzipf = None\n\tzf = None\n\texts = ['.xml', '.gz', '.zip', '.bz2']\n\ttry:\n\t\tzipf = zipfile.ZipFile(zip_path, 'r')\n\texcept:\n\t\ttraceback.print_exc()\n\tdircache['xml'] = [{bdir: None for bdir in os.listdir(os.path.join(BASEDIR, 'xm', name))} for name in os.listdir(os.path.join("}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\timport sys, getopt\n\timport zipfile\n\timport os\n\t\n\tif not zip_path:\n\t\terrortype, errorname = \"\", \"Invalidzip\"\n\t\tsys.stderr.write(\"ERROR: zip_path not passed\\n\"\n\t\t\t\t    + \"Usage: {}, -l, --list        lends data on the list in given zipfile name: <zipfile>\\n\"\n\t\t\t\t    + \"Example: {} -h       help on the help option\\n\"\n\t\t\t\t    + \"Example: {}, -f, --force"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tcontent_as_dict = {}\n\tfor filename, info in zipfile.ZipFile(zip_path).infolist():\n\t\tname = info.filename\n\t\tif name!= 'zips.zip':\n\t\t\tcontinue\n\t\tif name in ['topics']:\n\t\t\tif info.st_mode & 0o7700 == 922337203685477580:\n\t\t\t\tcontinue\n\t\titem = filename.split('/')\n\t\tcontent = \"files/{}/{}\".format(item[1][:-4], filename)\n\t\tcontent_as_dict."}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\twith zipfile.ZipFile(zip_path) as zin:\n\t\troot = os.path.splitext(os.path.basename(zip_path))[0]\n\t\tdata = zin.infolist()\n\t\tfiles = []\n\t\tfor xmlfile in data:\n\t\t\t# print(xmlfile.filename)\n\t\t\tif xmlfile.filename[-3:] == \".xml\":\n\t\t\t\tfiles.append(xmlfile.filename[:-3])\n\t\tpcs = {}\t\t\n\t\tfor file_name in files:"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t# Get list of files in zip directory\n\tfile_names = glob.glob(join(temp_dir,zip_path.name, '*.zip'))\n\tif len(file_names) == 0:\n\t\twarnings.warn(\"No file found.\")\n\t\treturn {}\n\t# Get last item in zipfile in order from 0\n\tzip_filename = file_names[len(file_names)-1]\n\t# Get path to temp file with extension.xml\n\ttemp_fn = join(temp_dir,zip_filename[:-3]+'zip')"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\timport os\n\tfrom os.path import isfile, join\n\timport zipfile\n\t# get list of basenames in the specified zip file\n\tbasenames = [f.filename[f.filename.rfind('/')+1:].split('.')[0] for f in os.scandir(zip_path) if f.is_file() and f.filename.endswith('.zip')]\n\twith zipfile.ZipFile(zip_path, 'r') as z:\n\t\twith open(\"info.ini\", 'w') as out"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t# Create target name: {package/group/item}\n\t# Note: There may be multiple names of each package/group/item\n\tpackage_to_group_item = {}\n\tbase_name_list = [int(os.path.basename(path)[12:]) for path in os.listdir(zip_path) if os.path.isdir(path)]\n\troot = Et.parse(zip_path).getroot()\n\tfor group, group_item in enumerate(root):\t\t\t# root node is a group\n\t\tif group_item."}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tinfo = []\n\twith zipfile.ZipFile(zip_path) as zipf:\n\t\tzipf.extractall(path = '.')\n\t\tfor filename in os.listdir(zipf.namelist()[0]):\n\t\t\tpkgfile = filename\n\t\t\tpkgfile = os.path.join(zipf.namelist()[0], filename)\n\t\t\tmime_map = {}\n\t\t\tfor mode in zipf.namelist():\n\t\t\t\tif filename in mode:\n\t\t\t\t\twith zipfile.ZipFile(mode, 'r') as modezip:\n\t\t\t\t\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\timport os, zipfile\n\timport xml.etree.cElementTree as ET\n\timport sys\n\tfrom csv import writer\n\tfrom glob import glob\n\n\t# Initialize empty file\n\tfile = open(zip_path, \"r\")\n\tfile.close()\n\n\t# Extract all xml files into a list\n\txml_files = []\n\tfor name in os.listdir(zip_path):\n\t\tif is_zipfile(zip_path + \"/\" + name):\n\t\t\txml_files.append(zip_path + \"/\" + name)\n\n\t"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t# Open Zip file as an iterator.\n\n\tdire = os.path.dirname(zip_path)\n\tpath_prefix = []\n\tf = open(zip_path, 'r')\n\tfor line in f:\n\t\tpath_prefix.append(line[:-4])\n\tf.close()\n\n\t# Check the directory structure.\n\n\tprint(\"Directory structure:\")\n\tdirectory_list = [\"\", \"\", \"\", \"\"]\n\tfor file in path_prefix:\n\t\tprint(file + (\"[0->Z]/\", \"\", \"[1->"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\twith ZipFile(zip_path) as z:\n\t\t#print \"files_list from zipfile:\", z.namelist()\n\t\treturn z.read(z.namelist()[0]).decode('utf-8')\n\n@click.argument('tinf_dir', type=click.Path())\n@click.argument('outputs_dir', type=click.Path())\n@click.argument('pdfs', type=click.Path())\n@click.option('-p', '--passive', is_flag=True, default=False)\ndef run"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tfor file in glob.glob(zip_path + '/*/*.pdf'):\n\t\tyield file\n\n\tfor file in glob.glob(zip_path + '/*/**.xml'):\n\t\tyield file\n\n\tfor file in glob.glob(zip_path + '/*/*.xml'):\n\t\tyield file\n\ndef files_text_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path` separated by new line delimiter\n\nExample:\n\n```\n[\n    '2318-08"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t# Get the list of all files in a zipfile\n\tif os.path.isfile(zip_path) == False:\n\t\traise Exception(_(\n\t\t\t'Error: the `download_extracted_files_from_zipfile(zip_path)'\n\t\t\t+ '` should be `True` if it is the first run, e.g. when building a new `convert'\n\t\t\t+ '` dataset with: `python generate_csv_files.py --make_csv=True`'\n\t\t).replace('/', '//'))\n\n\tif os.path"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\twith zipfile.ZipFile(zip_path,'r') as f:\n\t\tfiles = [ filename.filename for filename  in f.filelist ]\n\t\tfiles.sort()\n\treturn files\n\ndef file2list(file, delimchar=',', with_id=True):\t\"\"\"\n\tConvert file `file` to a `list` of `file` and `str`,\n\totherwise return a `str`.\n\t\"\"\"\n\treturn f'{file}{delimchar}{os.linesep.join(sorted([str(f) for f in file"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t# https://stackoverflow.com/questions/26140962/how-to-get-file-paths-from-zipfile\n\timport zipfile\n\tzipfileObj = zipfile.ZipFile(zip_path)\n\tfileList = zipfileObj.namelist()\n\treturn fileList\n\n\ndef read_image_from_tif(tif_path):\n\t'''\n    Read an image from a directory that was generated using python_to_tiler\n    Args:\n        tif_path (str): The relative path to a.tif file in"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tdef find_files_match(pattern, f_list):\n\t\t\"\"\"\n\t\tReturn list of the files matching `pattern`\n\t\t\"\"\"\n\t\tfiles_found = []\n\t\tmatched_files = set()\n\t\tfor f in f_list:\n\t\t\tif not re.search(pattern, f):\n\t\t\t\tcontinue\n\t\t\tfiles_found.append(f)\n\t\t\tmatched_files.add(f)\n\t\treturn match_files(files_found)\n\t\t\n\tlist_of_files = _gzip_filewalk([zip_path], find_files_match)\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\t# Build a list of the files\n\treturn files_list_from_zipfile(zip_path)\n\t\t\n\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tfiles_list = []\n\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tzip_ref.extractall(records=zip_path.split(\"\\\\\")[-1])\n\n\tshutil.move(\n\t\tpath=zip_path + \"/\" + \"2318-0889-tinf-33-0421/\",\n\t\tto_path=data_temp + \"/\"\n\t)\n\tos.remove(zip_path + \"/\" + \"2318-0889-tinf-33-0421/"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tprint(\"\\nReading \" + zip_path.name.replace(\".zip\", \"\"))\n\twith zipfile.ZipFile(zip_path, \"r\", zipfile.ZIP_DEFLATED) as f:\n\t\tfor name in f.namelist():\n\t\t  if 'labfile' in name:\n\t\t    yield name[10:].split('/')[2]\n\t  elif 'labfile' in f.namelist(name=\"../2\"):\n\t     yield name[10:].split('/')[2]\n\t\n"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn remove_name_prefix(content).replace('w:st=\"',\"w-st=\"\")\n\ndef add_to_id_from_namespace(content, name):\n\tif name.startswith(\"w:st=\"):\n\t\treturn name\n\treturn add_name_prefix(content.replace('\\r',' ')) + \"-\" + name\n\ndef parse_content_ns(content, prefix):\n\t\"\"\"\n\t\tGiven text with namespace notation, convert it into a proper format.\n\t\t:param content:\tThe file text content\n\t\t:param prefix"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn''.join(str.split(content, 'w:'))\n\n\ndef fix_namespace_prefixes(e):\n\t\"\"\"\n\tAdd the namespace prefixes to all element lists that it's found.\n\n\tIn case the list is empty of children it's added the last non-None tag,\n\teven if there is an empty list of children it's also added the parent list.\n\n\tParameters\n\t----------\n\te : element tree\n\t\"\"\"\n\ttry:\n\t\te_list = list(e)\n\texcept TypeError:\n\t\treturn e\n\n\tif"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tname_to_text = name_to_text(content)\t# get content without double escapes (with double brackets)\n\treturn content.replace(name_to_text, \"w-st=\\\"%s\\\"\" % name_to_text)\t# strip text from content_name and return\n\ndef fix_namespace_path(content):\t\"\"\"\n\tConvert text that defaults to'st=\"' to'st'\t\t\"\"\"\t# fix'st=\"' at end of content_name\n\tp = content.find('\"')\t# find at end of string ("}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn 'w-' + text.replace(' ', '').replace('. ', '-')\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tfrom xml_util import clean_name\n\t# content = remove_markup(content)\n\tif 'w-st=\"0\"' in content:\n\t\tcontent = content.replace('w:st=\"0\"', 'w-st=\"0\"')\t\t\n\t# return clean_name(content)\n\treturn fix_name(content)\n\n# def check_format(content, name, lang=None):\n#\n# \t\"\"\"\n# \tGet a fixed formated string with its replacements.\n# \t\"\"\"\n# \t# content = remove_mark"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t# https://stackoverflow.com/a/26138949\n\treturn content.replace(\"'\", \"\")\n\ndef fix_namespace_prefix(content):\t\"\"\"\n\tConvert text that defaults to 'a:st=\"' to 'a-st=\"'\n\t\"\"\"\t\n\treturn content.replace(\"'\", \"\")\n\ndef fix_namespace_prefix_r(content):\t\"\"\"\n\tConvert text that defaults to 'r:st=\"' to 'r-st=\"'\n\t\"\"\"\t\n\treturn content.replace(\"'\", \"\")"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tif content.startswith('w:st=\"') == 0:\t# is there an initial w:st=?\n\t\treturn content\n\telse:\t# is there one or more w:st=?s?\n\t\tmatch = re.search(r'(?P<st>(.+))',content)\t# get rid of any whitespace before and after\n\t\tif match:\t# if there is something\n\t\t\treturn match.expand(\n\t\t\t\tr'w-st=\"(%s)?\"' % ( match.group(\"st\").lower() ) )\n\t\t\t\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn ''.join(x if 'w:' in x else 'w-'  for x in content).replace('-st', '') if content else ''\n\n\ndef load_cx(self, content):\n\t\"\"\"\n\tLoad a list of nodes from Excel.\n\t\"\"\"\n\treturn [x for x in content if isinstance(x, list) and isinstance(x[0], int) and isinstance(x[1], int) and get_type(x[0]) not in ('cx:cxst',)] # ignore if invalid\n\n\ndef append_line"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tparts = content.split()\n\tfor idx, part in enumerate(parts[1:]):\n\t\tif part.startswith('\"'):\n\t\t\ttext = content\n\t\t\tcontent = text[1:(idx)]\n\t\telif part.startswith('w:'):\n\t\t\tst = part[2:]\n\t\t\ttext = text+' w-st=\"'+st+'\"'\n\treturn content+text \t\n\ndef convert_namespace(namespace):\n\tglobal n3m\n\tfor part in (namespace.rstrip('_')).split"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_text = WebDriverWait(node, 5, 1).until(\n\t\t\tEC.presence_of_element_located((By.XPATH, pubdate_xpath))\n\t\t)\n\t\tpubdate_text.clear()\n\t\tpubdate_text.send_keys(Keys.TAB)\n\t\tpubdate_text = WebDriverWait(node, 5, 1).until(\n\t\t\tEC.presence_of_element_located((By.XPATH, pubdate_x"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tmatched = None\n\tfor xpath in pubdate_xpaths:\n\t\tmatched = node.find(xpath)\n\t\tif matched:\n\t\t\treturn matched\n\treturn matched"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tmatch = next((node for node in pubdate_xpaths if node.find('pubdate').text.strip().lower() == pubdate_xpaths[0].lower()), None)\n\treturn match.getparent()\n\n@register.simple_tag\ndef render_tags(tags='', **kwargs):\n\t\"\"\"\n\tRenders the HTML-tag list view_template using a template tags to generate the \n\ttemplate template based on the context in kwargs.\n\t\"\"\"\n\treturn render_template(os.path.join(tags_dir,"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tpubdate_attr = node.attrib.get('pubdate')\n\tif pubdate_attr is not None:\n\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\tpubdate_value = eval(pubdate_xpath)\n\t\t\tif pubdate_value in pubdate_attr:\n\t\t\t\treturn pubdate_xpath\n\n\treturn None\n\ndef match_date_time_format(date_xpath_list, date_format_xpath):\n\t\"\"\"\n\tRuns the test_dates_formats.formats_xpath file"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor key, value in node['pubdate_xpath']['publication_date'].items():\n\t\tmatch_pub_xpath = \"/html/body/div[5]/div/span/input\"\n\t\tprint( match_pub_xpath + \" = \" + key \\\n\t\t\t+ \" match on pub_date\" )\n\n\tnode_pubdate = node.xpath( match_pub_xpath + \" | /html/head| /html/body\")\n\n\tfor elem in node_pubdate:\n\t\tif elem.text:\n\t\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tif pubdate_xpaths:\n\t\ttry:\n\t\t\tp1 = node.xpath('//*[@id=\"%s\"]/td' % pubdate_xpaths[0])\n\t\t\tif p1:\n\t\t\t\tp1 = p1[0].text.strip().split(\",\")\n\t\t\t\treturn p1[0]\n\t\texcept:\n\t\t\tpass\n\treturn None\n\n\t\n###\t\t\n# This is the function that will be called in order to \n# match pubdate and return a string with the match\t\n# return format: \"a, where"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tpubdate_xpath = None\n\t# Go through all xpath elements and first match each one\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = re.findall(pubdate_xpath, node.text)\n\t\tif(pubdate[0]):\n\t\t\tpubdate = pubdate[0].replace(\"&nbsp;\", \" \")\n\t\t\tif(pubdate == pubdate_xpath): #If we have found a match\n\t\t\t\treturn pubdate.strip()\n\t\telse:\n\t\t\t#If we don't have any"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t# Look for a match, unless the \"all\" option is provided\n\tif ( \"all\" not in pubdate_xpaths ):\n\t\t# Try to identify the first match, unless it's a single date\n\t\tif (\"@firstDate\" not in node.attrib):\n\t\t\treturn node.text\n\t\ttry:\n\t\t\tfor xpath in pubdate_xpaths:\n\t\t\t\tnode = get_first_match(node, xpath)\n\t\t\t# Return the first match, which will come directly after the pubdate element\t\n\t\t\treturn node.text\n\t\texcept NoSuch"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tif isinstance(pubdate_xpaths, list):\n\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\txpath = \"{}//span[contains(text(),'{}')]\".format(\n\t\t\t\tpubdate_xpath,\n\t\t\t\tnode.get_text().encode('ascii', 'ignore')\n\t\t\t)\n\t\t\tstatus = int(\n\t\t\t\tnode.find(xpath).get('status', 0))\n\n\t\t\tif status == pyld.DISPLAYED or status == pyld.DMTIME:\n\t\t\t\tbreak\n\n\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tif node.xpath(f'//div[@class=\"content\"]/span[@class=\"dtd\"]/text()'):\n\t\t\treturn node.xpath(pubdate_xpath)[0].text\n\t\telse:\n\t\t\tcontinue\n\treturn 'No matching result'\n\n\t\ndef find_title(node):\n\t\"\"\"\n\tReturns the first value of the class \"title\" which could not be loaded by css\t\"\"\"\n\tfor divitem in node.cssselect('.title span'"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# Extract number of the number field value\n\tnumber_of_issue_number = issue.find(\"number-of-issues-section\",\n\t\t\t\t\t\tparent=issue.select(\"number-of-issues-section\")[0])\n\t# Extract first and second number from the number field for the number.\n\tnumber_of_issue = number_of_issue_number.select_one(\n\t\t\t\t\t\t\"number\").text.strip()\n\t# Extract first and second number from the number field for the number.\n\tfirst_number = number_of_issue[0:number_of_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# Extract number and suppl elements in the issues elements\n\t# @TODO: Need to pass 'issue' argument for this function\n    possible_int = []\n    possible_str = []\n    possible_float = []\n    possible_bool = []\n    for tag in issue_elements:\n        # Try tag.text as not a number\n        try: \n            this_text = tag.text.strip()\n            # Try tag.text.split('\\n') or tag.text[2:].strip().split(' ')\n            this"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\ttry:\n\t\treturn int(re.sub(r'\\D', '', issue.xpath('./h4[1]/text()').get())), \\\n\t\t\t\tissue.xpath('./p[1]/div[2]/text()').get()\n\texcept ValueError:\n\t\treturn '', ''\n\treturn '', ''"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\ttry:\n\t\t# Finds all values of type text and stores them as a list.\n\t\t# The numbers values must be separated by hyphens (--), with an optional space between the value and the value sign (--,-)\n\t\tcontents = issue.find_all(text=True, recursive=False, attrs={'id':'rp'})\n\t\t# If there are no contents, then return a `None` so it doesn't break the filter algorithm.\n\t\tif (contents is None or len(contents) == 0):\n\t\t\treturn None, None\n\n\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t#print(\"Inside extracter from issue_element_number and number support\\n\")\n\textracted_values = issue.text.replace(\",\",\"\").split(\":\")\n\tvalues = []\n\tfor value in extracted_values:\n\t\ttry:\n\t\t\tvalue.strip()\n\t\t\tif value:\n\t\t\t\tif value.isdigit():\n\t\t\t\t\t#print('digit value')\n\t\t\t\t\t#print(value)\n\t\t\t\t\tvalues.append(True)\n\t\t\t\telse:\n\t\t\t\t\t#print('Number')\n\t\t\t\t\t#print(value)\n\t\t\t\t\tvalues.append(False)\n\t\texcept"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnum_from_issue = issue.find(\"Number\")\n\tissue_num = issue.getText().replace(\"\\n\", \"\")   # Remove white space\n\tissue_num_cont = issue.getText()[:num_from_issue].replace(\" \", \"\")   # get first text: replace white space\n\n\t# Get number values from number element:\n\tissue_number = issue.find(\"IssueNumber\").getText().replace(\"-\", \" \").replace(\"/\", \" \").replace(\"\\(\", \"\").replace(\",\", \"\").replace(\"\\)\", \""}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tcontent = issue.findchildren(name = \"div\")\n\tnum_dict = {}\t# for each variable in the content we will hold all the possible number \n\tfor field in content:\n\t\t# there are 6 possible values of this field: ['Int', 'Non-negative', 'Negated', 'Boolean', '[Not]', 'Any']\n\t\t# for example 'Int' will look like '_id', 'name', 'is_open', 'issue_id', 'fields'\n\t\tcur = field\n\t\twhile not cur.name.startswith(\"_\"):"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# There are many ways of parsing a number of attributes in a XML element,\n\t# as discussed in the following article:\n\t#\n\t# <http://pythonexample.org/numbers/text/1.1.html>\t\n\t#\n\t# To extract the number, a comment is extracted from the string: '''<number value=\"123\"/>'''.\n\t# The value of the number is the extracted value, and the type is\n\t# the attribute of the number, if it exists. Otherwise, we should\n\t# expect it to be a string:"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber =  int(issue['number'])\n\t\n\tsuppl = [\"{!r}\".format(supp) for supp in issue['suppl'].split(',')]\n\t\n\tif \"1\" in suppl:\n\t\tsuppl.remove(\"1\")\n\treturn number, suppl\n\t\ndef _change_issue_comment_element(issue_element):\n\t\"\"\"\n\tChange issue_element to contain the right fields.\t\n\tReturn issue_element as modified.\n\t\"\"\"\n\tassert('author' in issue_element.keys()), \"Author"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber = ''\n\tsuppl = ''\n\tissue_number = None\n\n\tif element := issue.find('number'):\n\t\tissue_number = element.text\n\t\tfor num in number_l_list:\n\t\t\tif issue_number[0:4] == num:\n\t\t\t\tnumber = issue_number[5:]\n\t\t\t\tif '+' in issue_number:\n\t\t\t\t\tnumber = number.replace('+', '')\n\n\t\t\t\tsuppl = ''\n\t\t\t\tfor _ in range(len(issue_number) // 2):\n\t\t\t\t\tsuppl += issue_number[len("}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tfrom nltk.tree import Tree\n\tfrom operator import itemgetter\n\tfrom string import ascii_letters, digits, punctuation, whitespace\n\n\tprint(''+indent*'|')\n\tfor key, value in tree2str(self).items():\n\t\tif isTree(value):\n\t\t\tprint(\" \"+indent*\"|\",\"Tree: \" + value.pretty(indent=indent+3) + \",\", key.get_label(indent=indent+3, debug=debug)+\":\")\n\t\t\tself.pretty(indent"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\toutput = []\n\tfor k in sorted(self.__dict__, key=lambda k:k[0]):\n\t\ts = k[0] + \": \"\n\t\tv = self.__dict__[k]\n\t\tif type(v) is dict:\n\t\t\tif type(v) is OrderedDict:\n\t\t\t\ts+=\"\\n\"\n\t\t\t\ts = k[0] + \": \"\n\t\t\t\tk = k[0] + \": \"\n\t\t\t\tfor k2 in sorted((self.keys() + v.keys())):\n\t\t\t\t\tselfk ="}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\timport pprint\t\n\ts = \"  \" + self.__class__.__name__\t\n\tif self.__size__ is not None:\t\n\t\ts +='size: '+ (self.__size__).__repr__()\t\n\tif self.__depth__ is not None:\t\n\t\ts +='depth: '+ self.__depth__\t\n\ts += \" size:      \" + str(self.__element__)\t\n\treturn s.replace(repr(self.__element__),'element of class \"t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tindentString = \" \"*indent\n\tif debug: print(\"\\t\"*indent, end=\"\")\n\treturn \"\\n\".join([indentString,\"{}{}{}{}{}\".format(\"<{}>\".format(self.__name__), indentString+indentString, \" \"*indent, \"</{}>\".format(self.__name__), indentString)])\n\n\n\nclass Cifar100(Dataset):\n\t\"\"\"\n\tThis class implements a set of datasets for CIFAR-100.  It provides \n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tfor name, val in sorted(self.items()):\n\t\tif debug:\n\t\t\tprint('{0:#^{1}}{2!r}'.format('=' * indent, indent, name))\n\t\tif isinstance(val, dict):\n\t\t\tprint '{0}{1}- {2}'.format('\\t' * indent, 'dict', name)\n\t\t\tfor sub_val, sub_indent in divmod(indent - 1, 2):\n\t\t\t\tprint '\\t' * sub_indent + pretty(sub_val, sub_indent, False"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tindent_lines = [\"\"]\n\tif not self.is_list() or not self.is_empty():\n\t\tself.show()\n\t\tindent_lines = [\"\"]\n\t\tindent_lines.append(_indent(indent, True))\n\n\tdef _indent(indent, wrap_last):\n\t\tif indent == 0:\n\t\t\tindent_lines[-1] += \"  \"\n\t\tif indent_lines[-1]:\n\t\t\t\tindent_lines[-1] = indent_lines[-1]\n\t\treturn indent_lines if not"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif debug:\n\t\tindent = 4 * indent\n\t\tindent2 = 4 * indent\n\t\tprint '\\n========='\n\t\tprint 'CUSTOM'\n\t\tprint self.id\n\t\tprint '========'\n\t\t#print \"\\n\\n\\n\\n\"\n\t#print 'ID: ', self.id\n\t\n\t\tif not getattr(self, 'name'):\n\t\t\treturn self.id\n\t\tif self.name :\n\t\t\tprint \"Name: %s\" % self.name\n\t\tfor var in self.var:\n\t\t\tname, type ="}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif self.debug: print(self.__xref_to_xref)\n\treturn \" \"*indent + \"*   \"+ \\\n\t\t\"\\n\".join([self.__headings[int(i)]+self.__doc_desc if i <= len(self.__doc_desc) else self.__doc_desc for i,s in enumerate(self.__headings)])+\\\n\t\t\"\\n\"\n\ndef indent(self, indent=0):\n\t\"\"\"\n\tPretty print self.\t\n\t\"\"\"\n\treturn self"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif debug:\n\t\tself.debug(indent)\n\treturn self.__pretty\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tdef pad(y, indice):\n\t\ty = y.upper()\n\t\treturn ('  '*indice) + y\t\n\tif debug:\n\t\tprint(('\\rDEBUG: pretty('))\n\t#print indice\t\n\n\tdef print_list(self, indent=0, indent_type='\\t'):\t\n\n\t\t#print indent\n\t\theader = ('\\r'*(indent+1)).lstrip()\n\t\t#print header\n\t\t#print indent_type; indent_type.zfill(4)\n\t\tind  = indent+1"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t# TODO This is very hacky as it depends on the number of arguments\n\n\t# Check if args can match in number of args:\n\tif not isinstance(args[0], list) or isinstance(args[1], tuple):\n\t\t# Not a list, not a tuple\n\t\traise ValueError\n\n\t# Check if args are 2d:\n\tif len(args)!= 3:\n\t\t# Not a 2d arg.\n\t\traise ValueError\n\n\t# TODO Implement:\n\tnewLhs = []\n\tlhs = args[0]\n\tif not"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tresult = None\n\tif args:  # if there is an expression in the args list, absorb the expression directly\n\t\tres = self.absorb(args[0])     # absorb the expression first, and then its arguments directly\n\t\tif args[0] not in self.expressions:\t\tres = res[1:]\n\t\tfor exp in args[1:]:  # absorb all the remaining arguments recursively, then combine\n\t\t\tres.append( self.absorb(exp)[0] )\n\t\tresult = res[-1].copy()\n\n\treturn result\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tfrom sympy import * \n\texp = args[0] \n\tres = exp.expand_func().subs(args[1])\n\treturn res\n\n\t#print( \"=>\\nabsoluti\",abs(*exp)) \n\t#print( \"=>\\nNabla:\",abs(*res))\n\nclass Locus:\n\t\"\"\"\n\tRepresents a locus of interest based on the expression obtained from the `terms` property.\n\t\n\tThe initial values are given by the `exp` term, but the `locus_idx`"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n\tif len(args)==0:\n\t\treturn self\n\telif len(args) < 3:\n\t\tresult = self\n\t\tfor i in range(1, len(args)):\n\t\t\tresult = result.__or__(args[i])\n\t\treturn result\t\n\telse:\n\t\ta, b = args[:3]\n\t\treturn (a & b), (not (b & a))\t\n\n\nclass Or(AbstractExpression):\n\t\"\"\"Or class.\"\"\"\n\tdef expand(self, args):\t\"\"\"\n\tGiven an `args`"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tlogger.debug('In absorb: {}'.format('A|B') + str(args))\n\tlogger.debug('Absorption: {}'.format(' AND '.join(' {} | {}'.format(args[i],args[-i]) for i in range(len(args)))) )\n\tif any(A[i]!=A[-i] for i in range(len(args))):\n\t\treturn args\n\telse:\n\t\treturn args[:-1]+[A[-1]]\n\nclass Absorbed(SyntaxTree):\n\tdef _"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif len(args)!= 2:\n\t\traise ValueError(\"Must have exactly two arguments, got:\", args)\n\telse:\n\t\t# A & (a | b) = a, a | (a & b) = a\n\t\tA, B = args\n\t\treturn (A| B, A& B)\n\t\ndef absorb_seq(args):\n\t\"\"\"\n\tGiven a sequence of strings, return a new sequence with\n\tapplied absorption and negative absorption. Note that the\n\tabsorption is always applied to the last one. This routine\n\tapplies to any"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\targs[:] = map(ApplyExpression, map(Expression, args))\n\targs.reverse()\n\targs[:0] = args\n\tif None in args:  # if any args have zero length lists\n\t\treturn args\n\t# remove zero length lists\n\targs = map(Filter, args)\n\tif False in args:  # if any args are True or False\n\t\treturn args\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t# TODO: Add support for more than one arg list (i.e., arguments and\n\t# expression lists of the same size)\n\t# NOTE: The return list isn't in strictly quantifiers, but is possible!\n\n\t# Assume all arguments have been absorbed\n\tif all(isinstance(arg,list) for arg in args):\n\t\treturn args\n\n\t# Calculate the expression with the absorbing factor given\n\tresult = self.calculate_absorbing_expression(args)\n\targs = [arg if isinstance(arg,list) else [arg] for arg"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n\t# First make a boolean list of A and B.\n\tA = args == 'A & (A | B)'; # A & A & A = A\n\tB = args == 'A & (~A | B)'; # A & A & B = A * B\n\t\n\t# Create a new list of A and B with True, and False as False's\n\tout = list(A & B)\n\t\n\t# Remove False's from A and B, then make them True\n\tout.remove(False)\n\tout.append(True)\n\t\n"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tresult = []\n\tfor item in args:\n\n\t\t###\n\t\t###\n\t\t### TODO: figure out what is better, may want args or args and should work with n args? \n\t\t###\n\t\t### TODO: update with the more correct interpretation.\n\n\n\n\treturn result\n\t\t\n"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef decorator(func):\n\t\tself.hooks[hook] = func\n\t\treturn func\n\treturn decorator\n\n\t"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn _decorator(hook, _register, True, self.__registry_)\n\n\ndef get_registry():\n\t\"\"\"\n\tget_registry returns a global registry for the current session.\n\n\tReturns:\n\t\tregistry (RegistryType): A registry instance.\n\t\"\"\"\n\treturn _registry_globals()\n\ndef set_registry(registry):\n\t\"\"\"\n\tset_registry sets a global registry for the current session.\n\n\tArgs:\n\t\tregistry (RegistryType): A registry instance.\n\t\"\"\"\n\t"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn lambda x: add_to_registry(hook, self)\n\nclass RegistryManager(object):\n    def __init__(self):\n\tself.registered_hooks = set()\n\tself.registered_hook_types = {}\n\n    def register(self, hook):\n\tself.registered_hooks.add(hook)\n\tself.registered_hook_types[hook.type] = hook\n\n    def unregister(self, hook):\n\tif hook in self.registered_hooks:\n\t    del self.registered_hooks[hook]\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t#return lambda func: self.hooks.append(func)\n\tdef wrap(item):\n\t\treturn self.registerListener(hook, item.get('value'))\t\n\t#def wrap(item):\n\t\t#return self.registerListener(hook, item.get('value'))\n\treturn wrap\n\t\ndef clearHooks():\n\twhile len(self.hooks) > 0:\n\t\th = self.hooks.pop()\n\t\th()\n\t\t\ndef get_hook_class(hook)->HookType:\n\t\"\"\"Parse registered"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tif hasattr(hook, 'on'):\n\t\traise Exception('Expected hook to be a class')\n\thook.on = self\n\tself.hooks.append(hook)\n\treturn hook\n\n#==========================================================================\n\t\nclass TABUDAHookType(object):\n\t\"\"\"\n\tTABUDAHookType is a type for registration of TACTabularUI\n\tHooks.\tThis is implemented as a decorator with two arguments:\n\t1. An object that provides a Handler\n\t2. An object that contains data for registering\n\thook"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn {\n\t\t\t'on': [\n\t\t\t\tlambda e, h, *arg, **kw: app.events.trigger('hook', e, h, *arg, **kw)\n\t\t\t\t],\n\t\t\t}[hook]\n\ndef on_events_event(event):\n\t\"\"\"\n\tEvent callback implemented by hook to trigger when a given event\n\toccurs.\t\"\"\"\n\tapp.events.trigger('event', event, *lambda e, h, *arg: on(h, e, *arg))\n\ndef on_setup(app):\n\t\"\"\"\n\tSetup call"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef register(reg):\n\t\t\"\"\"\tMethod to register listeners for the specified\n\t\thook.\t\"\"\"\t\n\t\treturn reg(hook.__name__ + '_listener')\t\n\treturn register\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef on_hook(f):\n\t\tif getattr(hook, 'lazy', False):\n\t\t\tdef hook_wrapper(self, *args):\n\t\t\t\tif getattr(hook, 'is_enabled', False):\n\t\t\t\t\tif not self.listeners:\n\t\t\t\t\t\tself.listeners = hook._args()\n\t\t\t\t\t\tif hook._config_list:\t\n\t\t\t\t\t\t\tfor config in hook._config_list:\n\t\t\t\t\t\t\t\tself.listeners.append(\n\t\t\t\t\t\t\t\t  _add_config_to_list(self._reg_hook_config, {\n\t\t                        config: f(self,"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\timport inspect2hook\n\tdef _register(func):\n\t\t'''\n\t\tRegistration hook function.\n\t\t'''\n\t\timport inspect2hook\n\t\tif not hasattr(inspect2hook, '_global_hooks'):\n\t\t\tinspect2hook._global_hooks = []\n\t\ttry: # handle if the function does not require a module\n\t\t\tnew_module = module = inspect2hook.import_module('sys')\n\t\texcept (ImportError, SyntaxError, ModuleNotFoundError):\n\t\t\tnew_module = new_func = lambda obj, *args"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn self.connect(_handle__handle__on,hook)\n\n\n#-------------------------------------------------------------------------------\nclass __handle__on__register(object):\n\t\"\"\"\n\tDecorator for an application handler.\n\n\tAttributes and methods created with this object are also created in\n\t:meth:`~.handler.Handler.__init__` and :meth:`~.handler.Handler.register`.\n\n\tArgs:\n\t\thook (HookType): An instance of :class:`KeystoneHook` used to register\n\t\t\tcallbacks when this handler is called.\n\t\tpriority (int"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tbase = {}\n\tbase[\"USER\"] = user\n\tbase[\"CONNECTED_USERS\"].add(user)\n\tbase[\"ETCHED_HOST\"] = etcd_host\n\tbase[\"ETCHED_PORT\"] = etcd_port\n\n\treturn base\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig_file = os.path.join(\"etc\",\n            \"jail\", \"user-{}.json\".format(user))\n\treturn dict(user=user, etcd_host=etcd_host, etcd_port=etcd_port,\n\t            config_file=config_file, database_dbname='etco-database')\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {'host': etcd_host, 'port': etcd_port}\n\n\"\"\"\ndef config(user, etcd_host=None, etcd_port=None):\n\tconfig_base = base_config(user)\n\tif etcd_host is None:\n\t\t# assume they dont have a database.\n\t\tetcd_port = None\n\t\tetcd_host = \"localhost\"\n\telse:\n\t\t# they dont have a database.\n\t\tif etcd_port is None:\n\t\t\tetcd_port = 2379\n\tconf = {}"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\t# Create the default config.\n\tconf = {\n\t\t\"id\": user,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port\n\t}\n\n\treturn conf\n\n\ndef user_exists(config, service):\n\t\"\"\"\n\tChecks if a given User exists in a dictionary.\n\n\tArgs:\n\t\tconfig (dict): the current configuration.\n\t\tuser (str): name of the User to check.\n\n\tReturns:\n\t\tbool: True if it exists, False otherwise.\t\"\"\"\n\n\tuser"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = {\"username\": user, \"password\": \"1234\", \"password_encoding\": \"ascii\", \"host\": etcd_host, \"port\": etcd_port}\n\treturn {\"config\": config}\n\n#\n# Function to create the database to store all posts; it's a JSON file that stores a single object\n#\ndef build_db_load_json(file=\"post_feed.json\", file_format = \"json\"):\t\n\n\t\"\"\" Create a json file that contains a single object, in the form of a single dictionary\n\t\tfor all posts"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {\n\t\t\t\"user\": user,\n\t\t\t\"etcd_host\": etcd_host,\n\t\t\t\"etcd_port\": etcd_port,\n\t\t\t\"mongo\": \"mongodb://localhost:{0}\".format(etcd_port),\n\t\t\t}\n\n# \ndef get_etcd_config():\n\treturn {\"user\": \"\", \"etcd_host\": \"localhost\", \"etcd_port\": 2379}\t\n\ndef get_mongo_config():\n\treturn {\"user\": \"test\", \"etcd_host\":\"localhost\","}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = {\n\t\t\"BaseConfig\": {\n\t\t\t\"Key\": \"base_config\",\n\t\t\t\"Name\": user+\"_base_config\",\n\t\t\t\"Value\": { # a string value\n\t\t\t\t\"EtcdHost\": etcd_host,\n\t\t\t\t\"EtcdPort\": etcd_port,\n\t\t\t\t\"StaticKey\": etcd_host+\":\"+etcd_port\n\t\t\t      }\n\t\t}\n\t}\n\n\treturn config\n\t\ndef _generate_config_entry(host, etcd): # pylint: disable=invalid-name"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = createConfig()\n\tconfig['auth']['user_name'] = user\n\tconfig['auth']['pam_home'] = user\n\n\t# set the hosts\n\tconfig['auth']['etcd_hana'], config['auth']['etcd_etl'], config['auth']['etcd_crdb'] = etcd_host, etcd_host, etcd_host\n\tconfig['auth']['etcd_cred_home'] = etcd_host\n\n\tconfig['auth']['etcd'"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn { \n\t\"Base\": {\n\t\t\"User\": {\n\t\t\"Name\": user.lower()\n\t\t},\n\t},\n\t\n\t\"Static\": {\n\t\t\"EtcdHost\": etcd_host, \n\t\t\"EtcdPort\": etcd_port,\n\t\t\n\t\t\"KeyIn\": \"Configs\",\n\t\t\"KeyOut\": \"Configs\",\n\t\t\"KeyOutExpiration\": \"0\",\n\t\t\n\t\t\"FileExpireInSeconds\": \"3600\",\n\t\t\n\t\t\"FileExpireExpression\": \"expire"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig_dict = {\n\t\t\"user\": user,\n\t\t\"etcd_host\": \"localhost\",\n\t\t\"etcd_port\": 2379,\t\n\t}\n\n\tif etcd_host!= None:\n\t\tconfig_dict['etcd_host'] = etcd_host\n\n\tif etcd_port!= None:\n\t\tconfig_dict['etcd_port'] = etcd_port\n\n\treturn config_dict\n\ndef auth_config(user:str, etcd_host=None, etcd_port=None):\n\t\"\"\"\n\tReturns the"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# Based on the UTC definition in `datetime.datetime.utcfromtimestamp`.\n\n\t# Handle \"fold\" (see the docs for `dateutil.tz.tzfile` for more\n\t# information.  I tried but it ended up saving some filepaths, though\n\t# for a huge amount of data. It didn't hurt to save them, so I'll\n\t# probably shave off the \"timezone file\" for now.\n\ttz_date = tzfile(self.datetime_str).gettzinfo()\n\tif tz_date is None"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# Create a new timezone object assuming this timezone is the one\n\t# when we started converting to UTC.\n\ttz = pytz.timezone(dt.tzinfo.tzname(dt))\n\n\t# Convert the UTC time string to the timezone of the new timezone.\n\tdt = pytz.utc.localize(dt, is_dst=dt.dst())\n\n\t# If this is a fold-timezone, convert it to the new UTC timezone.\n\ttry:\n\t\t# Create an identical object after converting all ambiguous times.\n\t\ttd"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\tif getattr(dt, 'fold', False):\n\t\t# Timezone-aware datetimes have already been\n\t\t# converted to local dates, so we're done\n\t\treturn\n\t\t\n\t# Convert to UTC timezone\n\tutc = self._utc\n\tdt_utc = dt.astimezone(utc)\n\ts_dt_utc\t= (dt_utc - dt.zone).seconds\n\t\n\t# If it's ambiguous and in a \"fold\" state, calculate the effective\n\t# time difference, using the current time"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# XXX: This isn't perfect, the timezone info will be saved as a\n\t# different timezone (e.g. UTC+30, we'll add 30 back)\n\t# XXX: if an ambiguous datetime is fed back then that assumes we have a\n\t#      \"fold\" state (that is, if the datetime is ambiguous and we\n\t#      examined whether the AM/PM is 0, the second time is ahead of\n\t#      00:00:00/00:00:00, so we should expect it to be a fold) and we\n\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tfrom pytz import timezone\n\n\t# If this is already a timezone-aware datetime, don't bother with this.\n\tif not (issubclass(dt.__class__,\n\t                    datetime.tzinfo.tzinfo) or hasattr(dt, 'tzinfo')):\n\n\t\t# Calculate the timezone offset from UTC.\n\t\tt = dt.utcoffset()\n\n\t\t# If timezone information is available, adjust offset accordingly.\n\t\tif t is not None:\n\n\t\t\t# If t is positive, adjust the offset to match UTC.\n\t\t\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tfold = dt.fold\n\tif fold == dt.fold or (not fold):\n\t\tdelta = dt - dt.utcnow()\n\t\t# This may have been ambiguous or a state of ambiguous and the\n\t\t# datetime is incorrect.\n\t\tif delta.year is not None and (delta.year!= dt.year or not delta.month):\n\t\t\tif dt.month is not None:\n\t\t\t\t# Most ambiguous dates.\n\t\t\t\tymd = time.strftime(\"%Y%m%d\", time.gmtime(dt.year *"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tassert hasattr(dt, 'tzinfo') and hasattr(dt, 'tzname')\n\treturn dt.astimezone( self.cls.dt.tzinfo.tzoffset(self, dt.minor) )\n\n_datestr = '%Y-%m-%dT%H:%M:%S.%fZ'\n_datestr_date = \"%Y-%m-%d\"\n\nclass _LocalTimeZone(utc):\n\n\tdef utc2jd(self, datetime):\n\t\t\"\"\"\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn dt.replace(tzinfo=self)\n\n\n# -----------------------------------------------------------------------------\nclass _time_zone_utc(tzinfo):\n\t\"\"\"\n\tA timezone that can calculate unambiguous datetimes in a \"utc\" time.\n\tIn particular, does not recognize the datetime as being ambiguous.\n\tSee also :class:`_timezone_naive_utc`.\n\n\tThe datetime returned by this class can be used as the unambiguous\n\tdatetime of other functions in this module.\n\t\"\"\"\n\n\tdef tzname(self, dt):\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tself._tzinfo = None\n\t\treturn dt\n\n\t_time = dt.tzinfo\n\t_offset = _time.utcoffset()\n\n\tutc_dt = dt.astimezone(_time)\n\n\t# If this is the special case, if either of these are set then we return an ambiguous\n\t# datetime, otherwise return the ambiguous datetime\n\tif not ((_offset is None or _offset.days == 0 and _offset.seconds == 0)\n\t\tor utc_dt.tzinfo"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn self._timezones.fromutc(dt)\n\n\n\n# Some examples\n# -----------------------------------------------------------\n#\n# A Python programmer and a Python interpreter implementation\n#\ndef print_line(file=None, lnum=\"<unknown>\"):\n\tfile = sys._getframe(1).f_code.co_filename\n\tmsg = \"Line: %s on %s\\n\" % (lnum, file)\n\tprint msg\n\ndef print_msg(filename, linenumber=\"<unknown>\", msg=\"\", context=None):\n\tlines, curline"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tfrom calendar import isleap\n\t#from future.utils import PY3\n\n\tif (hasattr(self, 'normalized_to_date') or\n\t\thasattr(self, 'normalized_from_date') or\n\t\thasattr(self, 'from_date') or\n\t\thasattr(self, 'to_date')):\n\t\treturn None\n\tif self.days < 0:\n\t\treturn relativedelta(months=-self.months,\n\t\t                years=-self.years,\n\t\t                days=-days,\n\t\t                hours=hours)\n\t"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tfrom dateutil.relativedelta import relativedelta\n\treturn relativedelta\n\treturn type(self)(self._days,self._hours)\n\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(days=self.normal, hours=self.normal, minutes=self.normal, seconds=self.normal, microseconds=self.normal, milliseconds=self.normal, weeks=self.normal, weeks_from_1970=self.normal)\n\ndef add(self, other):\n\t\"\"\"\n\tAdd a date or datetime to a timedelta.\n\n\tConvert the other date or datetime to the same calendar\n\n\tThe :class:`dateutil.relativedelta.relativedelta` constructor for\n\t:class:`dateutil."}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t# Return a timedelta from a dateutil relativedelta\n    return relativedelta(**self.__dict__)\n\n\nclass weekday(object):\n\t\"\"\"\n    Represents the seven days of the week with default values as 0, tuple() are also valid values.\n\n    :attributions:\n\t\"\"\"\n\t# Attribute representing the seven days of the week.\t\n    weekday ='sunday'\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn dateutil.relativedelta.relativedelta(\n            days=self.isend,\n            hours=self.isendend,\n            minutes=self.isdur,\n            seconds=self.isdurexecs,\n            milliseconds=0,\n            microseconds=0\n        )\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(months=self.months,\n\t\t\t\tdelta=self.days,\n\t\t\t\tyears=self.years,\n\t\t\t\tmilliseconds=self.milliseconds,\n\t\t\t\tseconds=self.seconds,\n\t\t\t\tmicroseconds=self.microseconds,\n\t\t\t\tleapdays=self.leapdays,\n\t\t\t\tdayslate=self.dayslate,\n\t\t\t\thourslate=self.hourslate)\n\ndef relativedelta(dt1, dt2):\n        \"\"\"\n        Return a relativedelta object between two :class:`dateutil.relativedelta"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(days = int(self.days),\n                             seconds = int(self.seconds),\n                             microseconds = int(self.microseconds),\n                             hours = self._hours,\n                             minutes = self._minutes,\n                             months = self._months,\n                             years = self._years,\n                             leapdays = self.leapdays,\n                             is_leap_year = self.is_leap_year)\n\t\n\t\n@staticmethod\ndef timedelta(days=0, hours=0, minutes=0, seconds=0,"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(months=self.months, minutes=self.minutes)\n\t\n# Local variables:\n# tab-width:4\n# tab-completion:open\n# end:\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tfrom dateutil.relativedelta import relativedelta as rdelta\n\tif self.days is None or self.hours is None:\n\t\treturn rdelta(**self.__dict__)\n\telse:\n\t\treturn rdelta(**{k: self[k]/float(self.days) for k in ['days', 'hours','minutes']})\n\nif __name__ == '__main__':\n\n\timport unittest\n\tprint '# Generated by python...'\n\tclass MyTestCase(unittest.TestCase):"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn self._build_relativedelta(self, False, False, False, False, False, False, False, False)\n\n@contextlib.contextmanager\ndef with_reloader(main=None, use_signals=False, **kwargs):\n    \"\"\"\n    Context manager that takes standard argments and config file arguments\n    and passes them into a ``with_*`` block for a configured :class:`TestProgram`\n    instance.  It does *not* call the :meth:`~HookimplFailed` runs.\n\n    The `main"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\ttry: return''.join(namefunc(locale.getlocale()[1]))\n\texcept:\n\t\t# python 2.7\n\t\ttry:\n\t\t\treturn str(namefunc(namefunc().encode('utf-8')))\n\t\texcept UnicodeError:\t# python 3.4 bytes strings are returned normally\n\t\t\t# FIXME: get this string (note that it's in locale string repr) without using a \"\".join\n\t\t\treturn str(namefunc(namefunc().encode('ascii','replace')))\n\texcept:\n\t\t# python 2."}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif PY3:\n\t\treturn namefunc\n\telse:\n\t\treturn six.text_type(namefunc())\n\ndef format(record):\n\t\"\"\"\n\tReturn a tzoffset-aware datetime string for the specified record. This\n\tmay be displayed in local time if the record doesn't have a timezone.\n\t\"\"\"\n\ttzname = getattr(record, 'tzname', None)\n\n\t# We want the datetime string as a UTC-like date time string in\n\t# local time.  For example, the 'CST' zoneinfo value will"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\treturn (u''.join(x for x in namefunc()))\n\t\t\n\t\ndef localtime(hhmm, locale = None):\n\t\"\"\"localtime(hhmm) --> yyyy, mm, dd, hh, mn, ss, wd, yy, %a, %A, %b, %b\n\t   -hhmm    localtime at specified hour (24-hour clock)\n\t   -locale  uses Python 2 only ('de_DE', 'en_US', 'fr_FR')\n\t   -mm     MM (1-"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tfrom builtins import chr\n\tfrom operator import itemgetter\n\tfrom array import array\n\timport tzname\n\timport string\n\n\tdef tzname_in_python2(namef):\n\t\ttry:\n\t\t\tx = tzname.gettz(namefunc())\n\t\t\treturn array('b', \"%+03d\" % tzname.abbr.index(namef)).tostring()\n\t\texcept ValueError:\n\t\t\treturn string.ascii_lower_2(namef).encode('ascii')\n\nclass localtime(dat"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\timport sys\t\n\tcode = (\"import sys, %s\\n\"\n\t\t\"def tzname_in_python2(namefunc):\\n\"\n\t\t\"\ttry:\\n\"\n\t\t\"\t\treturn str(namefunc());\\n\"\n\t\t\"\texcept UnicodeError, error: t = str(namefunc()).split(',');\\n\"\n\t\t\"\treturn '<encodings.tearray.Encoding Standard>' in t[1] or '<encodings.tearray.Encoding Standard>' in t[2];\\"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif sys.version_info < (3,):\n\t\treturn namefunc\n\telse:\n\t\treturn namefunc.encode('utf-8')\t\n\ndef tzoffset_in_python2(namefunc):\n\t#tzoffset_in_python2() is faster than calling it directly\n\treturn str(time.strptime(ticks[0],\"%Y-%m-%dT%H:%M:%S\")[3]*60-(ticks[1] + ticks[2]))\n"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif PYTHON2:\n\t\tdef namefunc2(strfunc):\n\t\t\tif strfunc.decode('ascii')    == namefunc:\n\t\t\t\treturn strfunc.decode('ascii')\n\t\t\telse:\n\t\t\t\treturn '%r' % tzname(namefunc.decode('ascii'))\n\telse:\n\t\tdef namefunc2(strfunc):\n\t\t\tif isinstance(strfunc, unicode):\n\t\t\t\tstrfunc = strfunc.encode('ascii')\n\t\t\treturn tzname(strfunc)\n\t"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\n\tfrom java.lang import String\n\t\n\tdef tzname(name=None):\n\t\treturn namefunc(name or String(time.tzname[0].upper() + time.tzname[1:]))\n\t\n\treturn tzname\n\t\n\t\nif __name__ == '__main__':\n\t\n\tdef _create_instance(fn):\n\t\tdef instance(*args, **kwargs):\n\t\t\treturn fn(*args, **kwargs)\n\t\tinstance()._instance = fn\n\t\treturn instance\n\n\tclass Timer:\n\t\t\"Time"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif (sys.version_info[0] < 3):\n\t\treturn namefunc('')\n\telse:\n\t\treturn namefunc('\\'')\n\nif (sys.version_info[0] < 3):\n\tdef time_repr(t):\n\t\treturn t.strftime('%Y-%m-%d %H:%M')\nelse:\n\tdef time_repr(t):\n\t\treturn t.strftime('%Y%m%d%H%M')\n"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif sys.version_info[0] == 2:\n\t\tdef tzname(name, zulu=False):\n\t\t\tif name == \"Zulu\":\n\t\t\t\treturn namefunc(tzfile.getname(zone))\n\t\t\telse:\n\t\t\t\treturn namefunc(name)\n\telse:\n\t\tdef tzname(name, tzname, zulu=False):\n\t\t\tif name == \"Zulu\":\n\t\t\t\treturn tzname(namefunc(zone), tzfile.getname(zone))\n\t\t\telse:\n\t\t\t\treturn tzname(tzname, name"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# Set assume made on the local file as True so that the version_info\n\t# attribute can search for files whose names start with \"version\"\n\ttry:\n\t\tfd = open(os.path.join(here, \"cffi/_version.py\"), \"r\")\n\t  \tfd = fd.read()\n\t\tfd.close()\n\texcept IOError:\n\t    return \"unknown\"\n\tm = re.search(r\"version_info = (.*)\", fd, re.MULTILINE)\n\tif m:\n\t    return \"$"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# I did not make any changes to this method, but it's to be really relevant\n\t# to the status of this method\n\treturn get_versions\n"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# Set up some paths\n\tversionFile = os.path.join(os.path.dirname(__file__),'version.py')\n\tif os.path.exists(versionFile):\n\t\tif os.path.isfile(versionFile): \n\t\t\tglobal __version__\n\t\t\timport imp\n\t\telse:\n\t\t\tos.rename(versionFile, versionFile + '.py')\n\t\t\tspec = imp.new_module('spec')\n\t\t\tspec.VCS = 'git'\n\t\t\tspec.VCS_URL = 'git+https://github.com"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# I am unable to reliably guess the version from search path\n    versionFile = None\n    try:\n        return _py2app.VERSION\n\n    except AttributeError:\n        from distutils.version import LooseVersion as _LooseVersion\n        return _LooseVersion.parse_version(application().__path__[0])\n\n\n\n\ndef add_dll_directory(filename):\n\t\"\"\"\n\tAdds a file to the list of directories that may contain dlls if they do not exist.\n\tThe filename should be relative to the application.py extension\n\t\"\"\"\n"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# Verify handling of version argument (sys.version_info or\n\t# importlib.util.version)\n\tif getattr(sys,'version_info', None) is None:\n\t\ttry:\n\t\t\tfrom importlib.util import version  # Util from importlib\n\t\texcept ImportError:\n\t\t\tfrom importlib import version  # Lib by default\n\n\t\tfinally:\n\t\t\tdel version\n\n\t\tversion_info = sys.version_info  # save version for later\n\n\t# Try to get version from python builtin `sys.version_info` or cpython.p"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# Get the short version\n\ttry:\n\t\tfor version in short_version:\n\t\t\tpath = os.path.join(get_dir(), version)\n\t\t\tif os.path.isfile(path):\n\t\t\t\twith open(path, encoding='utf-8') as f:   #open for reading and encoding\n\t\t\t\t\ts = f.readline()\n\t\t\t\tversion_info = s.split('-')\n\t\t\t\ttry:\n\t\t\t\t\tversion = version_info[0]\n\t\t\t\t\tif LooseVersion(version) > LooseVersion('18.02.0'):\n\t\t\t\t\t\traise"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# I'm in _version.py, so can just test here\n\timport _version\n\treturn _version.get_versions()\n\n\t# I'm version_info.py, the major version identifier\n\n\t# PEP 440:\n\t# http://www.python.org/dev/peps/pep-0440/\n\n\t# If using pip, get the feature subpackage information:\n\tfrom pip._internal.req import parse_requirement_arguments\n\treq = parse_requirement_arguments(sys.argv[1:])\n\n\t# Handle requirements"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# I am a bit clean and backward compatible and allow for breaking change\n\t# without making any changes to nikola.\n\t# Only update the last version to ensure that Ghostscript\n\t# on non Unix will invoke this after making changes.\n\tp = subprocess.Popen([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tstdout, stderr = p.communicate()\n\tre_result = stdout.strip().dec"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# I am in _get_svn_version only because most of the installation is\n\t# already done (does not need ugh).  If anything breaks here b/c\n\t# maybe someone does a 'cannot install' fix then the src\n\t# commit/bug card in the repo may not be what we want (and\n\t# our version string should fix the problem).  So return a string\n\t# match.  But, if there is SVN 1.5.2 in front of us, ask the user\n\t# if soill I get the default.\n\tif"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t# I wanted to import it using importlib, but it doesn't work for us because our package uses it inside of\n\t# the namespace package and the version number does not change, we have to manually set the version\n\t# of the package.\n\t# \n\t# (This seems to be very particular to the importlib library that distutils uses. This will work. Its\n\t# actual import fails.)\n\timport importlib.metadata as metadata\n\n\t# If we have a main package that we need to load only the same version, we can do that now.\n\t# However, using"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]: return {\"version\": \"unknown\",\n\t\t\"error\": pieces[\"error\"],\n\t\t\"pieces\": []}\n\n\tpath = os.path.join(style[\"src_path\"], *pieces[\"full\"])\n\n    # compute the relative version of the source given authority,\n\t# e.g. \"release\" returns \"7.0.3\"\n\tif pieces[\"error\"]: return {\"version\": \"unknown\",\n\t\t\"error\": pieces[\"error\"],\n\t\t\"pieces\": []}\n\n\n\tvc_ident = vcs.split_rev_list"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]: return {\"version\": \"unknown\",\n\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\"dirty\": None,\n\t\t\"error\": pieces[\"error\"],\n\t\t\"date\": None}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\telif"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\t\"date\": None}\n\n\tif not style or style == \"default\":\t\n\t\tstyle = \"pep440-post\"\t\n\n\tif style == \"pep440-old\":\t\n\t\trendered = render_pep440_old(pieces)\t\n\telif style == \"pep440\":\n\t\trendered = render_pep440(pieces)"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\t\"date\": None}\n\n\tif not style or style == \"default\":\t\n\t\tstyle = \"gnu\"\t\n\n\tif style == \"default\":\n\t\tstyle = \"fancy_01\"\n\n\tif style == \"gnu\":\n\t\tstyle = \"gnu2\"\t\n\t\tvf(\"\\n\".join([\"set -symbolic-offsets 0"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"distance\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\t\"date\": None}\n\n\t# get best-fit state from our version\n\troot = \"@\"[0:8]\n\tif pieces[\"full-revisionid\"].startswith(root):\n\t\tpieces[\"full-revisionid\"] = pieces[root][len(root):].lstrip('.pc.')\n\t# or else, try to"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\trendered += \".post%d\" % pieces[\"distance\"]\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".dev0\"\n\t\t\trendered += plus_or_dot(pieces)\n\t\t\trendered += \"g%s\" % pieces[\"short\"]\n\telse:\n\t\t# exception #1\n\t\trendered = \"0.post%d\" % pieces[\"distance\"]\n\t\tif pieces[\"dirty\"]:\n\t\t\trendered"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]: return {\"version\": \"unknown\",\n\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\"dirty\": None,\n\t\t\"error\": pieces[\"error\"],\n\t\t\"date\": None}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"\t# the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\telif"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t      \"full-revisionid\": pieces.get(\"long-revisionid\"),\n\t\t      \"dirty\": pieces[\"dirty\"],\n\t\t      \"error\": pieces[\"error\"],\n\t\t      \"date\": None}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\trendered += \".post%d\" % pieces[\"distance\"]\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".dev0\"\n\t\t\trendered += plus_or_dot(pieces)\n\t\t\trendered += \"g%s\" % pieces[\"short\"]\n\telse:\n\t\t# exception #1\n\t\trendered = \"0.post%d\" % pieces[\"distance\"]\n\t\tif pieces[\"dirty\"]:\n\t\t\trendered"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\trendered += \".post%d\" % pieces[\"distance\"]\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".dev0\"\n\t\t\trendered += plus + \" g%s\" % pieces[\"short\"]\n\telse:\n\t\t# exception #1\n\t\trendered = \"0.post%d\" % pieces[\"distance\"]\n\t\tif pieces[\"dirty\"]:\n\t\t\trendered += \".dev0\"\n\t\trendered += \"+"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif \"+\" in pieces.keys():\n\t\treturn \"+\"\n\telse:\t\n\t\treturn \".\\n\" if pieces[\"+\"] == \".\" else \".\"\n\t\ndef run(data):\t\n\t\n\t#Get filepath\t\n\tfilepath=\"./\"+data[0]+\"\\n\"\n\t#File data\t\n\tfiles=[\"0-1\\n\",\"1-2\\n\",\"2-3\\n\",\"3-4\\n\",\"4-5\\n\",\"5-6\\n\",\"6-7\\n\",\"7-8\\n"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif \"+\" in pieces.pegs:\n\t\treturn \"+\"\n\treturn \".\"\n\ndef moveTo(pieces, to_move, pieces_moved, board, row, column):\n\t\n\t# Get pieces for squares and move them.\n\tsquareToMove = [row, column]\n\tif to_move == \"Q\":\n\t\tsquareToMove = [row//2, column//2]\n\t\tboard[squareToMove].pieces.append(pieces[pieces_moved[0]])\n\t\tboard[squareToMove].pieces.append(pieces[pieces"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\"\"\"\n\tpieces_copy = pieces.copy()\n\twhile '.' in pieces_copy:\n\t\tif not pieces_copy.get('.'):\n\t\t\treturn '.'\n\t\tfirst_piece = pieces_copy.get('.')\n\t\tlast_piece = pieces.get(first_piece[0])\n\t\tif last_piece is None:\n\t\t\treturn first_piece[0] + \".\"\n\t\tpiece_type = last_piece[-1]\n\t\tif piece_type == '.':\t## not in 'Aa1.. a.'\n\t\t\tpieces_"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\"\"\"\n\tpieces = list(map(pinned, pieces))\n\tif None not in pieces:\n\t\treturn '.'\n\tif not all(map(lambda x: x is None, pieces)):     # There is only None in the list map, then this is bad\t\n\t\treturn '.'\n\treturn '..'\n\t\"\"\"\t\n\treturn '.' if not any(map(lambda x: x in (\"\", \".\", \"..\"), pieces)) else \".\"\n\ndef render_pep440_post(pieces, options):\t\n\t\"\"\" Render the given pieces as"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif \"..\" in pieces.intersection([\"..\", \"...\"]):\n\t\treturn \"..\"\n\telse:\n\t\treturn \".\"\n\t\t\nfrom os import system\nsystem(\"clear\")\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif \"+\" in pieces.keys():\n\t\treturn \"+\"\n\treturn \".\"\n\ndef get_version():\t\"\"\"\n\tReturn the version of the current Koji build tree.\t\"\"\"\n\timport datetime\n\tp = os.popen(\"git log -r --format=%H%cd-%m*\")\n\tfor l in p.readlines():\n\t\tif \"+\" in l:\n\t\t\tdate = l\n\tdatetimeT = datetime.datetime.strptime(date, '%H%cd-%m*')\n\treturn int("}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\"\"\"\n\t>>> plus_or_dot('a.exe')\n\t'a.exe'\n\t>>> plus_or_dot('a.exe.b')\n\t'a.exe.b'\n\t>>> plus_or_dot('a/b')\n\t''\n\t>>> plus_or_dot('..')\n\t''\n\t>>> plus_or_dot('abc.def')  # \"\".join(set(pieces))\n\t'abc.def'\n\t\"\"\"\n\tif pieces[0].startswith('.'):\n\t\treturn '.'\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\"\"\"\n\tThe first value in the PEP release list looks like:\n\t    0.4.0.dev0+2.2.0.gtheorov\n\tLists multiple.dev,.dev0, and.dev2 releases will be delimited with\n\t    a plus.\n\n\tIf the.dev release does not start with a tilde, the first\n\tmatching second value. Assuming we skip the first value like above,\n\tthis should not be anything but _dot_. Note that we always\n\twant to check for.dev--not-dev0 as"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif \"+\" in pieces.keys():\n\t\treturn \"+\"\n\treturn \".\"\n\ndef train_test(test_test):\n\tif not os.path.splitext(test_test + \".train\")[1].lower()  in [\".wav\", \".npy\", \".npz\"]:\n\t\tos.makedirs(test_test)\n\t\ttrain_data = np.load(train_test + \"/train.npy\")\n\t\ttest_data = np.load(train_test + \"/Test.npy\")\n\n\t\ttrain_data = train_"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif '+' in pieces.keys():\n\t\treturn '.'\n\treturn '+'\n\n\t\ndef prefixToPostfix(word, start='', end=''):\n\tr = [('N', 0, 0)]  # start and end position\n\tstack = []\n\ti = start\n\tfor p in word:\n\t\tif p == '$':\n\t\t\tr.append(r[i])\n\t\t\tr = stack.pop()\n\t\t\ti += 1\n\t\tr.append((p, i, i))\n\tif len(r) > 0:\n\t\t"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif env is None:\n\t\tenv = os.environ\n\telse:\n\t\tenv = env.copy()\n\tenv['DJANGO_DEBUG'] = 'true'\n\tp = None # position argument\n\te = None # error level argument\n\tfor e,f in error_code:\n\t\tif f(args):\n\t\t\tp = e\n\t\t\tbreak\n\tif p is not None:\n\t\ttry:\n\t\t\tproc = subprocess.Popen(commands, universal_newlines=True, cwd=cwd, stdout=subprocess.PIP"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tenv = env or dict()\n\ttry:\n\t\tfor cmd,args in commands:\n\t\t\ttry:\n\t\t\t\tcall_args = args\n\t\t\t\tif type(call_args) == types.StringType:\n\t\t\t\t\tcall_args = shlex.split(call_args, posix=False)\t# in case command takes bytes\n\t\t\t\tcall_args = list(call_args)\n\t\t\t\tif len(args) >  0:\n\t\t\t\t\trun_input =''.join(args)\n\t\t\t\telse:\n\t\t\t\t\t# if args==[], then only show the env\n\t\t\t\t\trun_"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif isinstance(commands, basestring):\t# Only one argument was passed\n\t\tcommands = commands,\n\tif not isinstance(commands, list):\t# Any number of arguments other than strings and lists were specified\n\t\traise TypeError('Argument 2 must be a list or a string.')\n\tnsignal = signal.signal(signal.SIGINT, signal.SIG_DFL)\n\tfor c in commands:\n\t\tif isinstance(c, string_types):\n\t\t\tif not os.path.exists(c):\n\t\t\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t# load the known results of the failed commands\n\tif args:\n\t\tenv = env or run_command([get_config_filename(), args[0]], quiet=True)\n\t\tenv = env or {\"SHELL\": os.getenv(\"SHELL\")}\n\t\tenv = {k:v for k,v in env.items() if v}                \n\t\t\t\t\t\n\t# run the commands\n\tstdout = None\n\tif hide_stderr:\n\t\tstderr = subprocess.STDOUT\n\telse:\n\t\tstderr = None\n\tchild = None"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tassert isinstance(args, (list, tuple))\n\tif isinstance(args, basestring):\n\t\targs = args.split()\n\treturn map(cwd.run, commands) if cwd is not None else map(None, commands)\n\t\n\n#----------------------------------------------------------------------------\nclass Popen:\n\t_registry = {}\n\n\t@staticmethod\n\tdef run(cmd, cwd=None, verbose=False, hide_stderr=False, env=None, text=False):\n\t\tassert isinstance(cmd, (list, tuple))\n\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmd = []\n\tfor k, v in iteritems(args):\n\t\tif v is None:\n\t\t\tcmd.append(k)\n\t\telse:\n\t\t\tcmd.append(k + \"=\" + v)\n\n\tif cwd is not None:\n\t\tcwd = os.path.abspath(os.path.expanduser(cwd))\n\n\tif env is None:\n\t\tenv = {}\n\n\t# return output of''.join(cmd)\n\texec_param = ''\n\tfor k in iterkeys(env):\n\t\texec_param += \"-"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\tcmds = args[:]\n\tif cwd is not None:\n\t\tcmds += ['..']\n\tproc = None\n\t#print cmds\n\tif (args and len(args):\n\t\tif (args[0] == '--version'):\n\t\t\tproc = _run_command(['git', '--version'], args, cwd, verbose, hide_stderr, env)\n\t\t\treturn proc\n\t\tif (args[0][:8] == 'cd'):\n\t\t\tcwd = args[0][8:]\n\t\t\tif ("}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif isinstance(commands, basestring):\n\t\tcommands = commands.split(' ')\n\tif isinstance(args, basestring):\n\t\targs = args.split(' ')\n\tif env is None:\n\t\tenv = os.environ\n\t# for some reason this needs to be replaced by the command module\n\tif args and cwd:\n\t\tp = cwd\n\t\tcwd = os.getcwd()\n\telse:\n\t\tp = None\n\ttry:\n\t\tfor cmd in commands:\n\t\t\tif verbose:\n\t\t\t\tprint"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif env is None:\n\t\tenv = [] \n\n\t# Start suppressing output\n\tp = subprocess.Popen(commands,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.STDOUT,\n\t\tcwd=cwd,\n\t\tenv=env,\n\t\tstdin = None\n\t\t)\n\t# Wait for output to be expected.\n\tif hide_stderr:\n\t\tstdout_f = p.stdout\n\telse:\n\t\tstdout_f = p.stdout\n\tif verbose:\n"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tassert isinstance(commands, list), \"expanded_commands must be a list\"\n\tassert isinstance(args, list), \"arguments must be a list\"\n\tret = None\n\tfor command in commands:\n\t\tkwargs = {}\t# arguments for command\n\t\tif args:\n\t\t\tkwargs['args']  = args\n\t\tif silent:\n\t\t\tkwargs['env']   = env\n\t\tret_val = call(command, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, **kwargs"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t# these are parsed from self.version file or the CLI arguments\n\tcfg = VersioneerConfig()\n\tcfg.versionfile_source = self.PACKAGE_NAME\n\tcfg.versionfile_build = self.OPTIONS.get('build.versionfile.build')\n\tcfg.tag_prefix = self.OPTIONS.get('tag_prefix')\n\tcfg.parentdir_prefix = self.OPTIONS.get('parentdir_prefix')\n\tcfg.verbose = self.OPTIONS.get('verbose')\n\treturn cfg\t\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()\n\n\ndef get_model_name(config):  # TODO: Change this to 'yolo' argument.\n\tif config.dataset in ['coco', 'cityscapes', 'deeplab']:\n\t\treturn 'cityscapes'\n\telif config.dataset in ['pascal_voc', 'pascal_voc_test', 'ade20k']:\n\t\treturn 'voc'\n\telif config.dataset in ['stl10', 'imagenet']:\n\t\treturn 'imagenet'"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t# First create the tb config.\n\tcfg = VersioneerConfig()\n\tcfg.VCS = \"git\"\n\tcfg.style = \"pep440\"\n\tcfg.tag_prefix = \"v\"\n\n\t# Every new tag begins a new branch.\n\tcfg.branch_create = False\n\n\t# Remove already config branch if it exists, mark as conflicted if not\n\t# yet.\n\tcfg.branch_detach = cfg.branch_create\n\n\t# Don't set both version and release\n\tcfg.version = \"\"\n\tcfg."}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tconfig = get_config()\n\tconfig.VCS = \"git\"\n\tconfig.workflow = \"flit\"\n\tconfig.tag_prefix = \"\"\n\t#config.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"pip\"\n\treturn config\n\ndef main():\n\t\n\t\n\tglobal CONFIG_FILE\n\t\n\t\n\tif not CONFIG_FILE==None:\n\t\t\tCONFIG_FILE=convert()\n\t\t\t\n\tsetup(root=get_root(),  \n\t\t\t install_requires=install_requirements(),\n\t\t\t"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t# [edit] config filename  changed\n\tglobal _configure_pt, _configconfig\n\tuser_dir = os.path.abspath(os.getcwd())\n\twith  open(user_dir + '/pepe/config.py', 'r') as f:\n\t\t_configure_pt = f.read()\n\t_configconfig = pepe.pepe._get_latest_version_str()\n\n\t# [edit]\n\tglobal _version_info, _major_version, _minor_version\n\tv_line = [ _"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()\n\n\ndef get_command_line():\t\"\"\"\n\tRuns the command line for comparing Versioneer to \"A\" (adapted from pydevd) using \"pytest\".\n\t\"\"\"\n\n\tcmd_line = ['pytest']\n\tif 'tests' in sys.argv:\n\t\tcmd_line.append('--doctest-modules')\n\n\tglobal __version__\n\ttry:\n\t\twith open(os.path.join(os.path.dirname(__file__), 'VERSION_FILE')) as f:\n\t\t\t"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VEERC_CONFIG\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()\n\n\nclass VersioneerConfig:  # pylint: disable=too-many-public-methods\n\t\"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\t_CMD_PREFIX ='versioneer'\n\t# Variables used to hold Versioneer configuration parameters.\n\t# This is a string containing all the following variables, which need to be\n\t# filled in using values from setup.cfg.  The keys must be the\n\t# correspondings values from versioneer.  Common keys from setup.cfg\n\t# are ['version', 'full"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tbase_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\t# Directory where this module is (relative to) its packaged package.\n\tmod_dir = os.path.join(base_dir, \"donnees\")\n\tos.makedirs(mod_dir, mode=0o775, exist_ok=True)\n\n\tconfig = VersioneerConfig()\n\t\n\treturn config\n\n\ndef get_requirements(filename, requirements, session):\n\t\"\"\"\n\tParse'requ"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\teprint(f'- config file set to {versioneer.tool.run(\"git lsb release\").split(\".\")[0]}')\n\tfile = open(versioneer.cfg)\n\treturn versioneer.Config(file.read())\n\ndef getkey(name, key_file):\n\t\"\"\"Read and return a single key from a file in the Versioneer's \"variables\" directory.\"\"\"\n\ttry:\n\t\topt = get_config().variables['%s' % name]\n\texcept KeyError:\n\t\topt = None\n\tif opt:\n\t\topt"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(func):\n\t\tsetattr(func, '_can_be_run_as_vcs', True)\n\t\tsetattr(func, '_can_be_run_as_module', True)\n\t\tsetattr(func, '_can_be_run_as_pip', VCS.vcs.is_registered(vcs))\n\t\tif method =='record_poetry_hook':\n\t\t\tregister_poetry_hook(func)\n\t\t\treturn\t# allow inlining of hooks in register_poetry_hook\n\t\telif method"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef handler(func):\n\t\tsetattr(vcs, method, func)\n\t\treturn func\n\n\treturn handler\n\n\ndef is_zipfile(path):\n\t\"\"\"\n\tDetermine if the file at path is a zipfile.\n\t\"\"\"\n\tif not zipfile.is_zipfile(path):\n\t\treturn False\n\treturn path.endswith('.zip')\n\n\n@py34compat\ndef check_version(py26_only=False):\n\t\"\"\"\n\tCheck if the version of Python is Python 2.6 or greater.\n\t\"\"\""}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tsetattr(vcs, method, _vcs_handler_wrapper(method))\n\treturn method\n\n\ndef get_vcs_scanner(vcs):\n\t\"\"\"Return a Node class for VCS and unittest (usually)\"\"\"\n\ttry:\n\t\tmyver = LooseVersion(vcs.__version__)\n\texcept AttributeError:\n\t\tmyver = LooseVersion('6.0.0')\n\tmyver = LooseVersion('6.5.4')\t\n\tclass Scanner(vcs.vcs.BaseVCS,"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef handler(self, *args, **kwargs):  # @staticmethod\n\t\t\"\"\"\n\t\tThe default handler.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn method(self, *args, **kwargs)\n\t\texcept Exception as e:\n\t\t\tif logger.isEnabledFor(logging.DEBUG):\n\t\t\t\tlogger.exception(e, exc_info=True)\n\t\t\tmsg = '''The method register_vcs_handler failed. The {vcs} vcs is not supported.'''\n\t\t\tmsg = msg.format(vcs=vcs.capitalize"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef handler(self, path, **kwargs):\n\t\tmethod_handler = getattr(vcs.server, method)\n\t\t# Allow both name and url to be `.`-internal paths\n\t\tif path.startswith('.'):\n\t\t\tself.registry.set_default('.', path, **kwargs)\n\t\t\tvcs.server.register_handler(path, method_handler, **kwargs)\n\t\t\treturn\n\t\t# if url starts with '/' (internal), it's an actual VCS path\n\t\tif path.startswith('/'):"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tif vcs in ('git', 'hg', 'bzr', 'bzr2'):\n\t\tdef vcs_handler(self, config_params, content_handler):  # pragma: nocover\n\t\t\tmethod(self, config_params, content_handler)\n\t\t\treturn\n\n\traise ValueError(\"Unknown VCS: '%s'\" % vcs)\n\n\n@pytest.fixture\ndef runserver(pytester, request, runner):\n\t\"\"\"Fixture for running the GitClient instance.\n\n\tIf `pytestmark` is applied to"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef register(handler):\n\t\t\"\"\"\n\t\tif the given handler is a `~github.Repository.GitHubOAuth2Mixin`\n\t\tthen this method has been decorated by this one\n\t\t:param handler: The GitHubOAuth2Mixin class that will handle the request.\n\t\t:param method: The method to be decorated.\n\t\t\n\t\tif handler is a `~github.ContentFile.ContentFile` then this method has been wrapped by this method to add a @content.\n\t\t:return: Returns the decorated `method`\n\t\t\"\"\"\n\t\tif handler =="}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tif method not in ['checkout', 'checkout_packagelist', 'checkout_prerelease', 'checkout_tagged', 'checkout_all', 'checkout_tagged_prerelease']:\n\t\traise RuntimeError(\"method must be 'checkout', 'checkout_packagelist', 'checkout_prerelease', 'checkout_tagged', 'checkout_all','checkout_tagged_prerelease'\")\n\tif vcs not in VCS:\n\t\traise RuntimeError(\"Unknown version control instance: \" + vcs)\n\tHAND"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(handler):  \t\t# @<--\n\t\thandler.register(vcs.__name__, getattr(vcs, method)(handler))\n\t\treturn handler  # @<--\n\n\treturn decorator\n\n# vim: ts=4 sw=4 sts=4 expandtab\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tif hasattr(method, \"vcs_handler_display_prefix\"):\n\t\treturn method\n\tsetattr(method, \"vcs_handler_display_prefix\", \"vcs_\")\n\treturn method\n\n#\n# vcs -> run_command(cmd)\t\n#\n# Implements os/posix path:\n#./repo/subdir/path/to/some/file.py\n# and also:\n#./repo/subdir/path/to/some/file.py:\n#   <includes>\n#\n#\n"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\t## Validate tree structure.\n\n\tif self.tree.is_empty():\n\t\tself.logconsole.critical(\"The following tree is empty:\")\n\t\tself.logconsole.critical(\"tree.is_empty() = {}\".format(self.tree.is_empty()))\n\t\tif not self.tree.valid():\n\t\t\tif show_warnings:\n\t\t\t\tself.logconsole.warning(\"The following tree seems to be empty!\")\n\t\t\t\tself.logconsole.warning(\"tree.valid() = {}\".format(self.tree.is_empty()))\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t# get storage\n\tstorage = self.getStorage()\n\t# get all objects\n\tnum_objects = 0\n\ttry:\n\t\tnum_objects = len(storage.objects(validate=True))\n\texcept:\n\t\tif show_warnings is False:\n\t\t\tprint('')\n\t# check the hierarchy\n\tif self.hierarchy == None:\n\t\traise Exception('No hierarchy found!')\n\t# if there are no objects and hierarchy exists, nothing is good, return all objects\n\tif len(storage.objects(hierarchy=self.h"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\tstorage_tree = StorageTree(None)\n\tfor stg in self.stg:\n\t\tif storage_tree.validate_object(stg):\n\t\t\tlogging.info(f\"validating {stg=}\")\n\t\t\tpass\n\t\telse:\n\t\t\tmessage = f\"Error: Failed to validate {stg=} on hierarchy. Aborting.\"\n\t\t\tlogging.warning(message)\n\t\t\tself.validation_errors_all_level[stg].append(message)\n\t\t\tif show_warnings:\n\t\t\t\tif (self.show_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\t# Validate hierarchy.\n\tif not validate_hierarchy(self._tree, self.__parent): \t\n\t\tif self.__parent is None or show_warnings:\n\t\t\tprint(\"Storage object %s: Hierarchy is NULL\" % self.name)\n\t\t\tprint(\"Skipping...\\n\")\n\t\treturn 0, 0, 0\n\t\n\tnum_objects = 0\n\tgood_object = 0\n\tfor obj in self._tree:\n\t\ttry:\n\t\t\tnum_objects += 1\n\t\t\tif check_digests and obj.__"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\tfor fk_object in self.fk_objects:\n\t\tfk_object[\"object_is_valid\"] = True\n\t\tnum_objects += 1\n\t\t\n\t\t# This object had parent. So need to check parent\n\t\tfor parent_object in fk_object[\"parent\"].get(\"children\"):\n\t\t\tif parent_object[\"object_is_valid\"]:\n\t\t\t\tif parent_object.get(\"hierarchy_is_valid\", \"false\").lower() == \"true\":"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tself.data_stream.setLevel(logging.DEBUG)\n\tself.log.addHandler(StreamHandler(self.log))\n\n\tif hasattr(self,'store_type'):\n\t\tif self.store_type == 'object':\n\t\t\tself.object_store_loader.validation_errors += self.object_store_loader.__init__(self, self.object_store_loader.getAllObjects(self.file_path))\n\t\telif self.store_type =="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\timport logging\n\tfrom.common_validation import validate_checkpointing\t\t\t\n\tfrom.common import get_all_sub_dirs, list_of_files, files, create\n\tfrom.common import create_dir, remove\n\tfrom.common import check_file\n\t\n\tif show_warnings == True:\n\t\tlogging.warn('WARNING: Checkpointing functionality can be unstable!')\n\t\n\t# check all dirs\n\tfile_names = []\t\t\t\t\t\t# list of all file names (for logging purposes)\n\twhile self.dir_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\tbad_objects = []\n\tfor o in self.storage_root.values():\n\t\tif o.isdir() or o.islink():\t# is link\n\t\t\tnum_objects += 1\n\t\telif o.isdir():\t\t# is directory in a repository\n\t\t\tif self.compare_paths(o.path, '/'):\n\t\t\t\tgood_objects += 1\n\t\t\telse:\n\t\t\t\tbad_objects.append(o)\t\n\t\telse:\t\t\t\t# is other object\n\t\t\tif ("}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\tvalid_objects = 0\t\n\tobject_count = 0\t\n\t\n\tif not show_warnings:\t\n\t\ttry:\n\t\t\ttree = ET.parse(self.storage_root + \".xml\")\t\n\t\t\t\n\t\t\troot = tree.getroot()\t\n\t\t\tvalid_objects = (len(root.findall('object')) > 0)\t\n\t\n\t\texcept Exception as e:\t\n\t\t\tif show_warnings:\n\t\t\t\tlogging.error('An error occurred while parsing file!')\t\n\t\t\t\tlogging.error(repr(e"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\t# Check all parents at a given level (e.g. root of new hierarchy)\n\twhile 1:\n\t\tthis_level = self.levels[-1]\n\t\tif len(this_level)==0:\t# No more children\n\t\t\tbreak\n\n\t\t# Go up until the last element (and not a null element) on the level by adding\n\t\t# that element to the object and checking all that point on the same ancestor\n\t\tprev = this_level[-1]\n\t\tprev_childs = self."}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t# Set up a pool of available resources\n\tself._ocfl_pool = self.pool(self.qos)\n\t# Set up a dictionary where the keys are strings of the form \"hostname:port\"\n\tself.osflow = dict()\n\tfor host in self.clients:\n\t\tif (hasattr(self.clients[host], 'log') and self.clients[host].log):\n\t\t\tself.osflow[host] = self.clients[host].log.connections\n\t\telse:\n\t\t\tself.osflow[host] ="}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\n\tself.root     = self.stor.new_stor()\n\tself.root     = self._enforce_read_mode(self.root)\t\n\n\tself.storage = self.stor(self.root)\n\tself.storage = self._enforce_read_mode(self.storage)\n\n#---------------------------------------------------------------------------\t\n\n#  Define type definitions and initialise common definitions.\t\nclass types(object):\n\t\"\"\"Define the type definitions for the OCEXT classes and types.\"\"\"\t\n\t\n\t__receipt_number"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\troot = self.root()\n\tif root:\n\t\treturn root\n\telse:\n\t\traise Exception(\"The storage root was not created! Attempt \" + __doc__[__AID__])\n\n\tocfl_storagetype = [('ocfstoragetype', ocfstoragetype.encode())\n\t\tfor ocfstoragetype in ocfstortypes]\n\tstoragetype_table = [('storagetype', storagetype.encode())\n\t\tfor storagetype in"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.ocfl.InitStorage('OCFL',\n\t\thostAddress=self.root_host,\n\t\tpwd=self.root_pwd,\n\t\tversion=8,\n\t\tuseDefaultCache=True,\n\t)\n\tself.ocfl.CheckStorageFile(self.root_host+'/user.json')\n\tself.ocfl.CheckStorageFile(self.root_host+'/ocf/'+self.ocfstype)\n\tif not self.ocfl.check_compat_version():\n\t   raise Exception('OCFL version"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t# We have two ways to define OCRFL to use:\n\t# 1) Create a new OCFL root (a default path + type = root)\n\t# 2) Specify paths to new folder, to use for all existing OCFLs\n\t# 3) Specify default folders on the path\n\t#\n\t# Create OCFL root for each use of the file:\n\t# We can specify that, for example, to create a root with a specific\n\t# folder structure within it (specify folders, not by default).\n\t# \n\t# We can specify a prefix"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.objContext.appendCode(opencode(\"#ifdef OCFL\\\\n\", \"\\n\"))\n\tself.objContext.appendCode(opencode(\"#pragma ocfl_storage_readonly\", \"\\n\"))\n\tself.objContext.appendCode(opencode(\"#endif\", \"\\n\"))\n\n\tself.objContext.appendCode(opencode(\"#ifdef __cplusplus\\n\", \"    \\n    \\n    // \"))\n\tself.objContext.appendCode(opencode(\n\t\t\"    /* OCFL Base class */\\"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself._file_handle.file_handle().makedirs_if_needed()\n\tself._fid = self._file_handle.file_handle()\n\tself._dofd = self._file_handle.open('dolfin-py-3.23', 'w')\n\tself._dolffd = self._file_handle.open('dolfin-py-3.23', 'r')\n\n\t# Create the base domain.\n\t# All meshes are assumed to have their own base domain (since they would then be related).\n\t# Set the"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t# Create the root:\n\tself.ocflroot = ocfl.OCFL(\"ocfl\")\n\tself.ocflroot.set(init(config))\n\t# Set the root-user configuration, but we'll re-use this during this script.\n\tself.ocflroot.set(config, True)\n\n\tself.ocflroot.exec(\"user create ocfl.config\", interactive=True)\t\t\t# Create ocfl.config.\n\n\tif os.path.exists(config.ocflconfigpath):\n\t\tself.logger.debug"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t# Create the output directory.\n\tos.makedirs(self.outputDir.split(\"/\"))\n\n\t# Create the directory structure for initial data.\n\tself.initDir = os.path.join(self.outputDir, \"dummy\")\n\tfor d in [self.dataDir, self.outputDir]:\n\t\tos.makedirs(d, exist_ok=True)\n\t\n\t# Update the initial data.\n\tself.get_dbs = {\n\t\t\t# \"dummy\"\n\t\t\t\"database\": self.dbs,\n\t\t\t#"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.init = True\n\tself.cluster_name = ''\n\tself.pool_size = 0\n\tself.container_base_path = ''\n\tself.storage_base_path = ''\n\tself.host_group_name = ''\n\tself.clustergroup_name = ''\n\tself.storage_prefix = ''\n\tself.data_prefix = ''\n\n    def get(self):\n        return {\n          'default' : {\n            'pool_size': \"0\",\n            'cluster_name' : 'DefaultCluster"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tpattern = r\"0(\\d+|-*[\\d\\+\\-\\+~\\*]*[\\d\\+\\-\\+~\\*]+)$\"\n\treturn pattern.match(version) or version\n\n\ndef find_current_version(current_pattern, path=None, all=True):\n\t\"\"\"\n\tFind the path to the version in the given\n\tpattern, given the filename as a string.\n\t\"\"\"\n\tversion = current_pattern.groups()[0]\t\n\tversion = next_version(version)\n\tif version:"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tversion = version.lower()\n\tif version.endswith(\"0\") or not version[0].isdigit():\n\t\treturn \"\"\n\telse:\n\t\tnext_number = int(version[1:]) + 1\n\t\tnum = ''\n\t\tif next_number > 19:\n\t\t\tnext_number = 19\n\t\twhile not version[0].isdigit():\n\t\t\tif version[0] == '0' and version[1] == '0':\n\t\t\t\tversion = str(next_number)\n\t\t\telse:\n\t\t\t\tnext_number += 1\n\t\t\t\tif num"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tversion = version\n\tnver = (number(version) - 1)\n\tif version == '0':\n\t\treturn version\n\tif nver == 0:\n\t\treturn version + '0'\n\tif nver == 1 and version == '0':\n\t\treturn version + '0'\n\tif nver == 1 and version.startswith('0') or version in ['1.1', '1.2']:\n\t\t# if the string is an odd integer, i.e., it has been pre-processed previously\n\t\t# as '1_1' was"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tassert version-1 >= 0 and version-2 >= 0\t# ensure this is true\n\t\t\n\t# this is always the default, which will be derived.\t\n\treturn [0]*1\t# just return []\t\n\n\ndef nn_checkpoint_dw(state_dict):\t\n\t\"\"\"\n\t\tHelper to extract nn.init_xxx_from_state_dict\n\t\tArgs: \n\t\t- state_dict (dict): a loaded checkpoint file \n\n\t\treturn extracted value\n\t\"\"\"\n\tif'model.' in state_dict.keys() and '"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\treturn ('{0}.{1}'.format(*version))\n\n\ndef next_patch(patch_v):\n\t\"\"\"\n\tGet the corresponding version number.\t\"\"\"\n\tif any([d in patch_v for d in ['0.0', '0.0.0']}):\n\t\treturn ''\n\treturn '{0}.{1}.{2}'.format(*patch_v)\n\n\ndef version_patch(patch_v, next_v=None):\n\t\"\"\"\n\tReturns the version patch of the current version, or returns version\n\tpatch for next version"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tnxt = {4: 'KF', 5: 'JF', 6: 'KS',\n\t      7: 'KSP', 8: 'JT', 9: 'KSV',\n\t\t\t 10: 'KSP', 11: 'JT', 12: 'KF',\n\t\t\t 13: 'JF', 14: 'KT', 15: 'JT',\n\t\t\t 16: 'KO', 17: 'KG', 18: 'JG',\n\t\t\t 19: 'KSV', 20: 'JG', 21: 'KG'}\n\n\tif version=="}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t# If version is an empty string or not None,\n\t# then the last version is\n\t# considered to be a valid\n\t# pattern.\n\tif version == None or version == \"0\":\n\t\treturn None\n\n\t# There are no leading 0s. So create a version\n\t# string without them, which\n\t# will be the result after the current\n\t# pattern to be matched\n\t# against.\t\n\tif version[0] == 0:\n\t\tversion_string = version[1:]\n\telse:\n\t\tversion_string = str(int(version"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version.startswith('v'):\n\t\treturn version\n\n\tif version.startswith('0'):\n\t\treturn version[2:-1]\n\n\treturn str(version[2] + version[4:])\n\n\n#\n#\tFunctions for identifying whether a token can occur in this environment.\n#\ndef can_process(token, env, error=False):\n\t\"\"\"\n\tWhether or not this token can occur in a given environment.\t\"\"\"\n\t# Can the token begin with a number (aka a digit)?\n\tif (not token.val."}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tver_dict = {\t0x0000: \"X0\",\t0x0001: \"X1\",\n\t\t\t\t0x0010: \"X2\",\t0x0011: \"X3\",\n\t\t\t\t0x0100: \"X4\",\t0x0101: \"X5\",\n\t\t\t\t0x0110: \"X6\"}\n\tif (version[0] in range(0, 256) and\n\t\t\tversion[1] in range(0, 256)):\n\t\tif (version == ver_dict[0x0000] and len(version) == 1):\n\t\t\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tversion = version.lower()\n\tversionPrefixes = '000'\n\tversionPrefix,versionSuffix = versionPrefixes.split('x')\n\tif version < versionPrefix.encode():\n\t\tif version > 0x01000000:\n\t\t\treturn '(X).'\n\telse:\n\t\tif version <= 0x01000000:\n\t\t\treturn '(0).'\n\treturn versionPrefix + (''.join([chr(x) for x in (version, versionSuffix) if x])).encode()\n\ndef format_version(ver"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\tdef validate_version_inventory(self, version_directory, version):\n\t\n\t\t# Make sure the inventory is well formed\n\t\t(path, size) = os.path.split(version_directory)\n\t\tif not os.path.isdir(path):\n\t\t\traise Exception(\"Version directory '\" + version_directory + \"' does not exist\")\n\t\tif os.path.exists(self.inventory_dir):\n\t\t\traise Exception(\"Version path '\" + version_directory + \"' exists and does not contain a valid version\")\n\t\ttry:\t\t\n\t\t\tos."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\tversion_inventories = []\n\t\n\tfor version_dir in version_dirs:\t\n\t\tversion_inventories.append(version_dirs[version_dir])\n\t\n\tversion_dirs_as_str_list = []\n\t\t\n\tfor i in range(len(version_inventories)):\n\t\tversion_dirs_as_str_list.append(str(i))\n\t\t\t\t\n\tversion_dirs_as_str = ','.join(version_dirs_as_str_list)\n\t\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t# First look for any version directory at end of the path name list.  If found\n\t#   a version directory should exist.\n\t#\t# If found the list is valid.\n\t#\t#\t# Add this version to valid list.\n\t#\t# Do it for all other versions.\n\t#\t#\t# Assume all versions have been in the list are valid at this place.\n\t#\t#\t#\tbreak\n\n#\tfor version_dir in range(len(version_dirs) - 1, 0, -1):\n#\t\tif True"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tself.inventory.inventory._check_version_inventories(version_dir,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.get_content_digests())\n\ndef _check_files(files):\n\tfor f in files:\n\t\tdirname, basename = os.path.split(f)\n\t\tbasename = os.path.basename(basename)\n\n\t\tif not os.path.exists(f):\n\t\t\traise ValueError('The following file does not exist: %s' % f)\n\n\t\tif not os"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\tinventory = {}\n\t# Make the version inventory\n\tfor version_dir in version_dirs:\n\t\tversion = self.version(version_dir)\n\t\td = self.version_inventory.get(version_dir)\n\t\tif d is None:\n\t\t\tinventory[version_dir] = inventoryDict.empty()\n\t\t\tinventory[version_dir][\"digest\"] = version_dir\n\t\t\tinventoryDict[version_dir] = {}\n\t\t\tinventoryDict[version_dir][\"digests\"] = self.version_digests\n\t\t\tself.version"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\tinventory = Inventory.get_instance()\n\tfor version_dir in version_dirs:\n\t\tvalid_versions = inventory.validate_version_inventory(version_dir)\n\t\tif inventory.current_version not in valid_versions:\n\t\t\traise Exception(\"Unrecognised version number [%s] in inventory\" % inventory.current_version)"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\ti = 0 # current version \n\tn = len(version_dirs) # total number of versions \n\t\n\tfor version in version_dirs:\n\t\tif self.version < i:\n\t\t\tprint('ERROR: Invalid version number \"%s\".  Please provide a valid'% version)\n\t\t\tsys.exit(1)\n\t\telse:\n\t\t\ti = i+1\n\t\t\t\n\tif i < n: # if there's more items, then it's all good.\n\t\treturn \n\t\n\tprint('ERROR: Unable to find inventory for version %"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\tinventory_d = {}\t\t# map of inventory items\n\tfor directory in version_dirs:\n\t\tinventory_d[directory + \"/\"] = os.path.join(self.rootdir,directory)\n\t\n\t# Now we have a dict from directory names to inventory file paths.\n\tfor (key, value) in inventory_d.items():\n\t\t\n\t\t# First find items under that directory\n\t\titems = []\n\t\tif os.listdir(value):\n\t\t\tfor root, directory, files in os.walk(value):\n\t\t\t\tfor f in files:"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\t# Ensure the inventory directories exist.\n\tif version_dirs == []:\n\t\treturn\n\n\t# Ensure there is a unique root inventory\n\tif \"ROOT\" not in version_dirs:\n\t\tprint(\"Found no version %d root inventory files.\" % len(version_dirs))\n\t\treturn\n\n\t# Keep track on the file/directory names which have already been validated.\n\tvalidated_inventory_filenames = []\n\n\tfor version_dir in version_dirs:\n\t\t# Create a file/directory named in the current directory with\n\t\t#"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\tfor version_dir in version_dirs:\n\t\ttry:\n\t\t\tfname = version_dir.replace('version', 'inv').replace('.json', '.json')\n\t\texcept:\n\t\t\tprint('Version {!r} not found.'.format(version_dir))\n\t\t\tcontinue\n\t\t\t\n\t\tif not self.is_valid_version(fname):\n\t\t\tprint('{!r} has an inventory file {!r}. Aborting.'.format(version_dir, fname))\n\t\t\treturn\n\t\n\t\twith open(fname, 'r')"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif not path or len(path) < 1:\n\t\tpath = ''\n\telif path[0] == '*':\n\t\t# assume a file if it has '*' appended\n\t\tpath = path[1:]\n\t\t# then a collection if it's a directory\n\t\tif path[0] == '/' and len(path) > 0:\n\t\t\tpath = path[1:]\n\telif path[0] == '0':\n                path = path[1:]\n\telif re.match(r'^\\d+$',path):\n\t\ttry"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif os.path.isdir(os.path.join(path,'0=*')):\n\t\treturn 'root'\n\tif os.path.isdir(os.path.join(path,'0=*',path.split(path.split('=')[0])[1])):\n\t\treturn 'object'\n\tif os.path.isfile(os.path.join(path,path.split(path.split('=')[0])[1])):\n\t\treturn 'file'\n\t#print(\"Other type : {}\".format(type(tree"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n\t# check for invalid entries\n\tif path.endswith(\"*\"):\n\t\terror_list.append(\"Invalid Entry: Path may not end in '*'\")\n\t\tprint('Error:')\n\t\tprint(\"Path entry is invalid. \")\n\t\tprint('Path:')\n\t\tprint(path)\n\t\treturn(\"path\")\n\t\n\t# check for valid characters in this string\n\tfor letter in path:\n\t\tvalid_char = re.sub('[^0-9]', '', letter)\n\t\tif valid_char.isalpha():\n\t\t\t"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n\t# First line is just informational.\n\troot_path = path.upper()[:-1]\n\t\n\t# check if root directory/file\n\tif len(root_path) == 0:\n\t\t# if not, get only the name\n\t\troot_name = [i for i in os.listdir(str(path)) \\\n\t\t\t\t\t   if os.path.isdir(str(path) + '\\\\' + i)][0]\n\t\t\n\t\t# and not a file\n\t\tif len(root_name) == 0:\n\t\t\t# if not"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif path[0] == \"/\":\n\t\tpathlen = len(path)\n\t\treturn \"root\"\n\tif path[0:2] in [ \"//\", \"\\\\\\\\\" ]:\n\t\tpath = path[1:]\n\t\treturn \"object\"\n\tif path[0:3] == \"\\\\\":\n\t\tpath = path[3:]\n\t\treturn \"file\"\n\telif path[0:1] == \"@\":\n\t\tpath = path[1:]\n\t\treturn \"other\"\n\telse:\n\t\treturn \"other\"\n\t\n\t#print 'find"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n\t# Try to find a type.  Can't use name as a path because the name is a string\n\ttype,name = path.split(\"=\")[0].split(\"-\")\n\t\n\tif type == 'object':\n\t\t\n\t\treturn 'object'\n\telif type == 'object-root':\n\t\t\n\t\treturn 'root'\n\telif type == 'file':\n\t\treturn 'file'\n\telif type == 'file-root':\n\t\treturn 'root'\n\telif type == 'file-inventory' or type == 'inventory':\n"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n\t# Search all of the paths for a * or *=\n\tresult = \"root\"\n\tfor path in path.split(\":\"):\n\t\tif \"=\" in path:\n\t\t\tpart = path.partition(\":=\")\n\t\t\tif part[0] == \"0\":\n\t\t\t\tresult += \"object\"\n\t\t\t\tbreak\n\t\t\tif part[1] == \"=\":\n\t\t\t\tresult += \"file\"\n\t\t\t\tbreak\t\t\t\n\t\n\treturn result\n\ndef find_filename(path):\n\t\"\"\"\n\tReturn a string indicating the filename at the given path.\n\t"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tres = os.listdir(path)\n\tif '*' in res:\n\t\tres.pop(res.index('*'))\n\t\troot = \"0=*\"\n\t\tfile = True\n\telif '*' not in res:\n\t\t# Not something to be done\n\t\troot = '0='\n\t\tfile = False\n\telse:\n\t\ttry:\n\t\t\tfor fil in res:\n\t\t\t\tif os.path.isfile(path + '\\\\' + fil):\n\t\t\t\t\tfile = False\n\t\t\t\tif os.path.isdir(path + '\\\\"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tstr_type = ''\n\tif path[:4] == '0':\n\t\tpath = '/'+path[4:]\n\t\tif path[-1]!= '/': path += '/'\t\n\tfor sub_dirname in path.split('/'):\n\t\tif sub_dirname[:1] == '#': \n\t\t\tcontinue\n\t\tstr_type = 'object'\n\t\tfor name in sub_dirname[1:].split('='):\n\t\t\tname = name + '='\n\t\t\ttry:\t\t\t\t\n\t\t\t\tf = open(name)\n\t\t\t\tfound"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t# Handle the default 'root'\n\tif path not in ['/', '.', '.']:\n\t\treturn 'root'\n\t# Handle file paths\n\tif path.endswith('/'):\n\t\treturn 'file'\n\telif path.endswith('\\\\'):\n\t\treturn 'file'\n\telif path.endswith('/'):\n\t\treturn 'file'\t\n\n\t# Check for OCFL\n\tif path.startswith('//'):\n\t\tiname = path[1:]\n\telse:\n\t\tiname ="}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t#\n\t# Overwrite this for a new Bugzilla parameter to add.\n\t# \n\tprint \"Amend bug params for {}: {}\".format(self.project.username,params)\t\n\tparams['bug'] = self.parameters.format(self.project.bug_name)\n\n\tprint \"params.combo: {}\".format(params['combo_ticker'])\n\tif params['combo_label'] and self.combo_ticker!= '':\n\t\tparams['combo'] = self.combo_ticker\n\n\tif params"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tprint '***amend_bzparams***'\n\tbzparams = params\n\tfor bzparam in sorted(bzparams, key=lambda x: x['value']):\n\t\tbzparam['value'] = str(bzparam['value'])\n\t\tif len(bzparam) > 2:\n\t\t\t# We're setting the params.bz.name so things like -t and -n may confuse the user\n\t\t\tbzparam['name'] = \"%s_%s\" % (bzparam['bz.name'], bzparam['value"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tbzparams = os.path.join(self.working_dst, 'bzparams')\n\tif os.path.exists(bzparams):\n\t\twith open(bzparams, 'r') as f:\n\t\t\tlines = f.read().split(';')\n\t\tbzparams_new = dict(lines[0].split('=')[1].split(','))\n\t\tbzparams_new['BUGFILL_REPO'] = 'false'\n\t\tbzparams_new['BUGFILL_REVISION'] = ''\n\t\tfor"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t# Amend the bzparams\n\tfor bug_id in bug_ids:\n\t\ttry:\n\t\t\tif len(self.__bzparams[bug_id]) > 0:\n\t\t\t\t_params = self.__bzparams[bug_id]\n\t\t\t\tif len(params) > 0:\n\t\t\t\t\t_params.extend(params)\n\t\t\t\telse:\n\t\t\t\t\t_params = params\n\t\t\t\tself.__bzparams[bug_id] = _params\n\t\t\telse:\n\t\t\t\tself.__bzparams[bug_id] = params\n\t\texcept KeyError:\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tself._fix_params(\"amend\", \"bug_ids\", bug_ids)"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tassert bug_ids\n\tparams.append('$bug_ids')\n        params.append('%s' % str(bug_ids))        \n        self.write_file(params)\n\n        #####\n        # set up file\n        #\n        # This file is supposed to start with this\n\t\"\"\"\n        # The first 2 lines contain the parameters we need\n        # (with their default values)\n        params =  ('title', '')\n        write_file(''.join([ '\\n' * 3, '# The following parameters to modify', '\\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tprint('amend bugzilla params')\t\n\tfor bug_id in bug_ids:\n\t\tprint('\\tdebugging bug: {}'.format(bug_id))\n\t\twith self.cursor() as cursor:\n\t\t\tcursor.execute('select id, bzname, bzparams from bugbugs where id= %s', (bug_id,))\t\n\t\t\tparams['bzname'] = str(cursor.fetchone()[0])\n\t\t\tparams['bzparams'] = (''.join(cursor.fetchone()[1:]))"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t_, files, directories = self.do('search', params, fname, '-type', 'file')\n\tfiles = self.do('select', files[:])\n\tfiles = [{\t'success': f, \\\n\t\t\t\t'domain': '*' + f, \\\n\t\t\t\t'filesize': f.split('/')[-1].split('_')[4], \\\n\t\t\t\t'bzs': [], \\\n\t\t\t\t'display': '' } for f in files]\n\tfor d, _, fnames in directories:\n\t\t_,"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tself.bzparams = {\"all\":params, \"bz-user\":params.get(\"bz-user\",\"\"), \"bz-password\":params.get(\"bz-password\",\"\"), \"bz-url\":params.get(\"bz-url\",\"\"), \"bz-api-key\":params.get(\"bz-api-key\",\"\"), \"bz-bugfilter\":params.get(\"bzfilter\",\"\"), \"bz-bugtags\":[\"{self.user}<br>{self.build_name}\", \"{{self.build_number}}\"], \""}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\treturn False\t\n\n# TODO: Do we need this method? It would be great if we could set the\n# value of the 'fixed_version' param to True, with all of the default\n# values set and then set the 'fixed_version' param and not the hard-coded\n# values\n# class Customise_Bugzilla_params(Customize_Bz_params):\nclass Customise_Bugzilla_params(Customize_Bz_Params):\n\n\t\"\"\"\n\tCustomize all of the parameters for the Bugzilla API\n\n\tA subclass of Customize_"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdef deep_merge_nodes(nodes):\n\t\ttry:\t# If any node is a mappings node, attempt to merge it with it's value.\n\n\t\t\tfor val in nodes: # Check each node value to see if it has a scalar value that is a scalar in either \n\t\t\t\t# list or set format that is appropriate to deep merge to its value.\n\t\t\t\tif isinstance(val, list):\n\t\t\t\t\t# Iterate over each child node in order to check each element within the loop\n\t\t\t\t\tfor elem in val:\n\t\t\t\t\t\tif not isinstance(elem,"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\t\n\t\n\tdef merupre(self):\t\n\t\t\"\"\"\n\t\tPerforms a recursive merge of the given dictionaries using the values of its children as the\n\t\tfirst position in each new list. Child nodes that have been used before have their order\n\t\treplaced within them, if any.\n\t\t\n\t\t\"\"\"\n\t\tif not self.value_dict_used_in_merge:\n\t\t\treturn None\n\t\t\n\t\tif len(self.value_dict_used_in_merge) == len(self.value_dict_used_in_parents):\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\timport yaml\n\tfrom ruamel.yaml.scalarstring import SafeLoader, SafeDumper\n\tfrom ruamel.yaml.comments import CommentedMap\n\tfrom ruamel.yaml import ScalarToken, MappingNode\n\n\tnodes = []\n\n\ttry:\n\t\tfor key, node in zip(nodes, nodes[1:]):\n\t\t\tif isinstance(node, MutableMappingNode):\n\t\t\t\tif key:\n\t\t\t\t\tnodes.append((None, node.anchor, node))\n\t\t\t\tnodes.append((key, None))\n\t\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# TODO: This should be the only method that I found useful to use\n\t# I used a new method that has already simplified this for simplicity. For instance, if\n\t# node values are a list such as:\n\t#\n\t# [\n\t#     (\n\t#         ScalarNode(tag='retention'),\n\t#         ScalarNode(tag='retention')\n\t#     ),\n\t#     (\n\t#         ScalarNode(tag='retention'),\n\t#         MappingNode(tag='retention')),\n\t#"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tnew_config_data = self._deep_merge_nodes(nodes=nodes)\n\treturn new_config_data\n\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdeep_merge_keyed = {}\n\tdef _merge(dict, key, merge, deep_merge_keyed, merge_deep_map_keyed):\n\t\tif dict.__contains__(key):\n\t\t\tdeep_merge_keyed[key] = dict[key]\n\t\t\tif merge:\n\t\t\t\tmerge_deep_map_keyed.setdefault(key, {}).setdefault(merge_deep_map_keyed[key][:-6], []).append(deep_merge_keyed[key])\n\n\t\telif"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdef deep_merge_nodes(self, nodes):\n\t\t\"\"\"\n\t\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n        (\n            ruamel.yaml.nodes.ScalarNode as a key,\n            ruamel.yaml.nodes.MappingNode as a value,\n        ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t#TODO: Implement in a more elegant way\n\t#nodes = []\n\t#current_node = nodes\n\t#prelude = borgm.prelude\n\t#for node in nodes:\n\t\t#is_duplicate_key = False\n\t\t#duplicate_keys = []\n\t\t#for key in prelude.get_children():\n\t\t\t#try:\n\t\t\t\t#nodes[current_node].set(key, node[0])\n\t\t\t\t#current_node = nodes[current_node].get(key)\n\t\t\t#except:"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdef deep_merge_nodes(nodes):\n\t\tnodes = deepsort_nodes(nodes)\n\t\tfor node in range(0, len(nodes)):\n\t\t\tfor key, value in nodes[node].items():\n\t\t\t\tif value is None:\n\t\t\t\t\tcontinue\n\t\t\t\telif isinstance(value, MappingNode):\n\t\t\t\t\tnodes[node][key] = deep_merge_nodes(nodes[node][key])\n\t\t\t\telif isinstance(value, ScalarNode):\n\t\t\t\t\tnodes[node][key] = Scalar"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# Build a dictionary to hold all leaf nodes\n\tleaf_values = {}\n\tfor root_node in nodes:\n\t\tif isinstance(root_node, MappingNode):\n\t\t\tfor leaf_node in root_node.value:\n\t\t\t\tleaf_values[leaf_node.tag] = leaf_node.value\n\t\telse:\n\t\t\tleaf_values[root_node.tag] = root_node.value\n\n\tleaf_keys = set(leaf_values.keys())\n\n\t# Build an intermediate copy of leaf_values for further merge\n\tleaf_values_intermediate"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\timport sys\n\n\t# Create an ArgumentParser instance\n\tparser = ArgumentParser(description='')\n\n\t# Loop over all arguments provided by the user\n\tfor argument in arguments:\n\t\t# Check that argument is a valid choice\n\t\tif not check_choice(argument.choices):\n\t\t\tprint 'ERROR: argument \"{0:s}\" invalid, expected one of: {1!s}'\\\n\t\t\t\t.format(argument.name, ', '.join(argument.choices))\n\t\t\traise SystemExit(2)\n\n\t\t# Remove leading and trailing whitespace from argument names, since they"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\timport os\n\timport sys\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n\n########################################################################\n\ndef main():\n\t\"\"\"This is a main function which runs all the other functions.\"\"\"\n\targs = parse_arguments()\n\n\t\t\n\tif args.verbose:\n\t\tsys.stdout.write(' '.join(arguments))\n\t\tsys.stdout.write('\\n')\n\n\tmain_work()\n\ttry"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description = 'Script for extracting the information in the current state of the network')\n\t\n\tsubgroup = parser.add_mutually_exclusive_group()\n\t\n\tsubgroup.add_argument(\n\t\t'-n',\n\t\tnargs = '?',\n\t\thelp = 'Name of the network',\n\t\ttype = str\n\t\t)\n\t\n\tsubgroup.add_argument(\n\t\t'-i',\n\t\tnargs = '',\n\t\thelp = 'Name of the input',\n\t\ttype = str\n\t\t)\n\t\n\tsubgroup"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# Parse command-line argument\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument('-p', '--profile_file',\n\t\t\trequired=False,\n\t\t\ttype=pathlib.Path,\n\t\t\tdefault=Path.home().joinpath('.profiles').joinpath('idle.prof').as_posix(),\n\t\t\thelp='Path to profiling log file (either set explicitly by running '+\n\t\t\t'idle with -P or via Profiler.prof or idle.prof)')\n\tparser.add_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"\"\"Given a path to a\n\t\t\t\t\t\t\t\t\t\t\t\t\tlog file containing one or more process logs,\n\t\t\t\t\t\t\t\t\t\t\t\t\tparse this log file and return a list of log\n\t\t\t\t\t\t\t\t\t\t\t\t\tfiles in which each log file is contained.\n\t\n\t\t\t\t\t\t\t\t\t\t\t\t\tThe log file is parsed using the Python 3 standard\n\t\t\t\t\t\t\t\t\t\t\t\t\tlibrary: CVS, Git, and Mercurial.\t\"\"\")\n\treturn parser.parse_args(arguments)\n\n\ndef main():\n\t\"\"\"\n\tMain function that parses user arguments and calls the appropriate script.\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\targuments = argv[1:]\n\t\n\tif arguments:\t\n\t\tprint \"Invalid command line arguments.\"\n\t\n\tparser = ArgumentParser(description = description)\n\t\n\t(options, args) = parser.parse_args(arguments)\n\t\n\treturn options\n\n\ndef read_file(filename):\n\t\"\"\"\n\tGiven a file name (without the extension), read it into a string, return it.\t\"\"\"\n\t\n\twith open(filename, 'r') as f:\n\t\treturn f.read()\t\t\n\ndef add_args"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description='A tool for analysing and evaluating chemical compound relations.')\n\tparser.add_argument('-q', '--quiet', action='store_true', help=\"Only print status messages.\")\n\tparser.add_argument('-s', '--standard', action='store_true', help=\"Prints status messages for the standard library.\")\n\tparser.add_argument('-j', '--jar', help=argparse.SUPPRESS)\n\tparser.add_argument('-i', '--interactive', action='store_true', help=\"Runs"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=argparse.SUPPRESS)\n\t# Create an ArgumentParser() instance: arg_parser\n\targ_parser = argparse.ArgumentParser(parents=[parser], add_help=False)\n\t# Create an argument group (argument group) named 'general' using the parent (parent).\t\n\t# 'general' defines the commands (options) that were supplied to this script.\t\n\t# 'general' is a list containing all the 'command' command-line arguments specified with the '-h' flag or"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=sys.argv[0], usage=\"[%(prog)s] command [script directory]\")\n\tparser.allow_abbrev = False\n\t# parser.parents = [parser]\n\tparser.add_argument(\"-h\", \"--help\", default=argparse.SUPPRESS, action=\"help\", help=\"show this help message and exit\")\n\tparser.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"turn on error verbose mode, will also log to stder"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\treturn argparse.ArgumentParser(description=__doc__).parse_args(arguments)\n\t\n\ndef create_directory_tree(data_dir, base_dir):\n\t\"\"\" Create a directory tree with a list of base directories as parent dirs. \"\"\"\n\tif not os.path.exists(data_dir):\n\t\tprint(\"Make sure the data directory exists or create it with the'make_data_dir' command!\")\n\t\treturn\n\t\n\t# Create base directory\n\tif not os.path.exists(base_dir):\t\n\t\t"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.flags.__str__().replace('_','')\n\ndef list2string(alist):\t\"\"\"\n\tGiven a tuple/list of ints, return a string representation of the list.\n\tFor example, '[]' or \"[1, 2, 3, '4']\" means '[] 1 2 3 4'.\n\t\"\"\"\n\tout = ''\n\tfor item in alist:\n\t\tout+=str(item)+' '\n\treturn out[:len(out)-1]\n\ndef check_version(version):\t\"\"\"\n\tCheck that version"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\timport argparse\n\t# Create the parser.\n\tparser = argparse.ArgumentParser(description=parser.description, formatter_class=argparse.RawTextHelpFormatter,\n\t\t\t\t\t\t\t\t\t  prog='myparser')\n\tparser.add_argument('--flag', default=\"default\",\n\t                    help='Help text for the option')\n\targs = parser.parse_args()\n\t# Return the argument flags in a space-separated string.\t\n\tif args.flag!= \"default\":\n\t\treturn \" \"*len(args.flag) + args.flag"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tspaceARG = parser.add_argument_group(title=\"Flags\")\n\tdef parse(argument):\n\t\t\"\"\"\n\t\tParse the argument, and return a dictionary of option flags for that argument.\n\t\tIf the argument is a string, it will be converted into a dictionary.\n\t\t\"\"\"\n\t\tif argument: # if there is a flag, parse\n\t\t\t# '--o' is an alias for '--output'\n\t\t\toption_flags = { }\n\t\t\tflags = []\n\t\t\tif len(argument)>0:\n\t\t\t\tflags=[x[1] for x in parser._"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags_str = \"\"\n\tfor flagname in sorted(flagdefinition.flagnames):\n\t\tif not flagname in parser._flagnames:\n\t\t\tif FLAGS.verbose:\n\t\t\t\tprint(\"Unrecognized flag (inheriting from parent): %s\"%flagname)\n\t\t\tflagname = \"--no-%s\"%flagname\n\t\tif FLAGS.__dict__[flagname] is not None:\n\t\t\tflags_str = flags_str + \" %s \" % (flagname)\n\treturn flags_str\n\n#-----------------------------------------------------------------------\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\tfor option in parser.option_strings:\n\t\tflags += argparse.SUPPRESS + option + \" \"\n\treturn flags\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tif not parser or not parser.flags:\n\t\treturn \"\"\n\n\t# convert all flag fields to comma-separated strings\n\tdef flag_set(flagvals):\n\t\treturn \",\".join(\"{}={}\".format(*flagval) for flagval in flagvals)\n\n\tdef flag_to_str(flags):\n\t\tif not flags:\n\t\t\treturn \"No flags\"\n\t\treturn \",\".join(str(flag) for flag in flags)\n\n\t# create flaglist and return\n\tflags = []\n\tfor arg in parser.flags:\n\t\tflags."}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \" \".join((flags[i] for i in parser.flags))\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\tif parser.formatter_class is not None:\n\t\tformatter = parser.formatter_class(**parser.format_help())\n\telse:\n\t\tformatter = parser.formatter_for_help()\n\tfor option in parser._actions:\n\t\tif option.dest is not None:\n\t\t\tflags += \"-\" + option.dest + \"=\"\n\t\t\tfor option_i in option._option_string_actions:\n\t\t\t\tif option_i.help is not None:\n\t\t\t\t\tflags += \" \" + option_i.help\n\t\t\t\telse:"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags=sys.argv[1:]\n\tflag_str=\" \".join([(\"-D \")+(\" \")+flag for flag in flags])\n\treturn flag_str\n\t#\n\t# xtract_flags()\n\t# xtract_flags()\n\n\n\n\n\n\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.flags.write().replace('-', '').rstrip()\n\ndef parser_config(logfile):\n\t\"\"\"\n\tGiven a logfile object containing filenames for the logs, parse its contents,\n\tconstruct appropriate arguments, add standard argument defaults, and return the\n\targument flags and logging module args.\n\t\"\"\"\n\tconf = None\n\targfile = None\n\tfname = None\n\tlogfilestatfile = None\n\tlogfilestatfile = None\n\tlogfilewarn = 0\n\tif not logfile:\n\t\treturn conf, None"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparsed_arguments = dict()\n\n\tparsed_arguments[\"--version\"] = get_version(logger=logger)\n\tparsed_arguments[\"--description\"] = ''\n\n\tif len(unparsed_arguments) > 0:\n\t\tfor k, v in unparsed_arguments.items():\n\t\t\ttry:\n\t\t\t\tparsed_arguments[k] = v.parse_args()\n\t\t\texcept argparse.ArgumentError as e:\n\t\t\t\tlogger.debug('Error when parsing option %s: %s"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# Note: This function requires argparse.\t\n\t# See https://docs.python.org/3.5/library/argparse.html#argparse.ArgumentParser.add_argument.\t\n\t\t#\n\t\t# TODO (cdeh): Why this function needs to recursively walk through command-line parameters.\t\n\n\targuments = {}\n\tfor argument in unparsed_arguments:\n\t\t#print(\"------------------\",argument, len(argument), \"------------------\")\t\t\n\t\tif \"=\" in argument:\n\t\t\tsubparser = argument.split(\""}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tfrom argparse import Namespace\t\n\t\n\tall_subparser_names = dict()\t\t\t\t\t# dict mapping from subparser name to subparser object.\t\n\tall_subparser_names.update(dict.fromkeys(subparsers.keys(),\n\t\tNone))           # create new dictionary of the subparsers\n\t\t\t\t\t\t\t\t\t\t\t# so that argparse will search the subparsers\n\t\t\t\t\t\t\t\t\t\t\t# for an argument parser, rather than creating\n\t\t\t\t\t\t\t\t\t\t\t# a subparser object for each argument.\t\n\t\t\t\t\t\t\t\t\t\t\t\n\tdef add_argparse_arguments_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\timport argparse\n\tfrom ntpath import basename\n\tresult= {}\n\tif len(unparsed_arguments)!= 0:\n\t\t_unparsed_arguments = sorted(unparsed_arguments)\n\t\tif _unparsed_arguments is None:\n\t\t\t_unparsed_arguments = []\n\t\telse: #_unparsed_arguments is a list\n\t\t\t_unparsed_arguments = []\n\t\t\tfor i in range(len(_unparsed_arguments)):\n\t\t\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\"\"\"\n\tThis function is a wrapper around argparse.parse_args() which allows us to create and parse command-line arguments.\"\"\"\n\tfrom argparse import Namespace as myargparse\n\t\n\tglobal subparsers\n\t\n\t# Create command-line subparsers\n\t# The subparsers list may not exist if this script already passed the argument parsing function.\t\t\n\tsubparsers = []\n\tfor arg_name in unparsed_arguments:\n\t\tif arg_name!= 'name':\n\t\t\tsubparser = subparsers.add_parser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# parse arguments\n\toptions = argparse.Namespace()\n\tfor option_name, option_string in unparsed_arguments:\n\t\tparts = option_name.split('.')\n\t\tassert parts\n\t\tassert len(parts) in [0, 1]\n\t\tassert parts[-1] in ['store','store-local','store-global'], parts[-1]\n\t\tif len(parts) == 1:\n\t\t\tparts.append('store')\n\t\toption = parts[-1]\n\t\tif parts[0] in options.__dict__"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\treturn { subparser.name: subparser.parse_arguments(*unparsed_arguments) for subparser in sys.argv[1:] }\n################################################################################\n# Main\n################################################################################\nif __name__ == '__main__':\n\t# parse arguments and populate namespace\n\targs = parse_arguments()['parser']\n\tconfig = args.config\n\targs.verbose = config.verbose\n\targs.debug = config.debug\n\targs.quiet = config.quiet\n\tconfig.root_dir = os.path.abspath"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targs = {}\n\n\t# Parse subparsers\n\t# Get default subparsers\n\tparser_dict = dict() # Dict of parser instance for default subparsers only\n\tfor parser in argparse.ArgumentParser._name2cmdline.values():\n\t\tif parser in parser_dict.keys():\n\t\t\tparser_dict[parser] = parser\n\t\telse:\n\t\t\tparser = parser[0]\n\t\t\tparser_dict[parser] = parser if parser.startswith('-' ) else None\n\t\t\tparser_dict[parser] = argparse.ArgumentParser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\timport argparse\n\t# Initialize parse and return values\t\n\targs\t\t= {}\t\t\t\t\t\t# Argument dictionary, as passed around as a dict(args) in main()\n\n\t# Create argument parser.\n\tparser = argparse.ArgumentParser(add_help=False, allow_abbrev=False)\n\tparser.allow_abbrev    \t= False\n\n\tif len(unparsed_arguments) > 0:\n\t\tparser.add_argument(*unparsed_arguments.pop() )\n\n\t# Parse arguments.\n\toptions, remaining"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targs = dict()\n\t\n\tif (\"--verbose\" in unparsed_arguments[0:2]):\n\t\tverbose = True\n\telse:\n\t\tverbose=False\n\t\targs[\"verbose\"] = (verbose == \"True\")\n\n\t# Assert that we have arguments\n\tif len(unparsed_arguments) == 2 and unparsed_arguments[1] in [\"--help\", \"?\"]:\n\t\thelp_text = subparsers.format_help(UnparsedHelp())\n\n\t\tif unparsed_arg"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tsubparsers.clear()\t\t\t\t\t\t\t\n\tfor name, subparser in subparsers._actionss.items():\n\t\tnargs = subparser.add_subparsers(dest=\"action\")\n\t\t(a, args) = parser_name(name, subparsers, unparsed_arguments, nargs).match_subparser_parser_name(name)\n\t\tsubparsers = subparser.add_parser(name)\n\t\tsubparsers.set_defaults( **dict(a = a, action = name))\n\t\targs_to_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# TODO: add subparsers to list with a sub-subparser object\n\tsubparsers_dict = {}\n\tremainder = []\n\tfor arg_dict in unparsed_arguments:\n\t\tif not 'parent' in arg_dict:  # Sub-subparser:\n\t\t\tsubparser = subparsers.add_parser(arg_dict['sub'],\n\t\t\t\t\t       \t\t parents=[arg_dict['parent']])\n\t\telse:\n\t\t\tsubparser = subparsers.add_argument_group(arg_dict['group'],\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tsubparser_parser = subparsers.add_parser('parse-subparser')\n\tsubparser_parser.set_defaults(handler=parse_subparser_arguments)\n\n\tunparsed_arguments = unparsed_arguments.copy()\n\tunparsed_arguments['--help'] = False\n\tsubparser_parser.add_argument(*argv[1:], action='help', help='parse arguments for this subparser',\n\t\t\t\t\t\t\t\trequired=unparsed_arguments.get('--args'))\n\treturn subparsers"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresult = {}\n\tother_subparsers_found = {}\n\tother_subparsers = subparsers.get_subparsers(dest='other_subparsers_arguments')\n\tif subparsers.prog not in other_subparsers:\n\t\tsubparsers = subparsers.add_parser(subparsers.prog, help='subparser description')\n\t\tother_subparsers[subparsers.prog] = subparsers\n\n\tfor subparser in subparsers:\n\t\tresult"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\targparse_action_map = {}\n\tremaining_arguments = []\n\trequired_namespace = dict()\n\targ_defaults = dict()\t\t\t# to be set by subparsers\n\trequired_namespace_defaults = {}\n\tfor arg in unparsed_arguments:\n\t\tparser_name = arg.get_parser_name()\n\t\t\n\t\tif not parser_name in argparse_action_map:\n\t\t\targparse_action_map[parser_name] = py_argparse_actions.ArgumentParserAction()\n\t\targgroup"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparsed_namespace = {}\n\t# For now, each subparser is only defined once.\n\t# Maybe make subparsers separate (e.g. with subcommands in separate subparsers like\n\t# \"file\", or by other subparsers)\n\tsubparsers = {}\n\tsubparsers_as_dict = {}\n\trequired_subparsers_as_dict = {}\n\ttry:\n\t\tsubparser, subparser_name = unparsed_arguments.pop(0)\n\texcept IndexError:\n\t\traise System"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tnms = {}\n\ta2ns = {}\n\tfor optname, opt in unparsed_arguments.items():\n\t\tfor subp in subparsers.values():\n\t\t\tif optname in subp.option_strings:\n\t\t\t\ttry:\n\t\t\t\t\tif getattr(subp, \"help\") is not None:\n\t\t\t\t\t\thelp_text = ''\n\t\t\t\t\telif isinstance(opt, (int, float)) or isinstance(opt, str) and _is_tuple(opt):\n\t\t\t\t\t\thelp_text = \"\".join(opt)\n\t\t\t\t\telif isinstance("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparsed_namespace = {}\n\tsubparsers = subparsers.copy()\n\tfor ap in subparsers.values():\n\t\t# Find all arguments present in all subparsers\n\t\tall_args = []\n\t\tfor ap_key in ap.required_args:\n\t\t\tif ap_key not in unparsed_arguments:\n\t\t\t\traise HelpException(\"Missing argument \"+ap_key+\"\")\n\t\t\tall_args.append(unparsed_arguments[ap_key])\n\t\t# Make a new parser class and add all the arguments into"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tconfiguration = {x.opt:x for x in subparsers.arguments}  # Copy of 'args', plus possibly some kwargs\n\tparsed_args = {}\n\tremaining_args = []\n\tfor i, arg in enumerate(unparsed_arguments):\n\t\twhile arg[0].casefold() in subparsers.arg_names:\n\t\t\targs = []\n\t\t\targ, remaining_args = unparsed_arguments[i + 1:i + 2], unparsed_arguments[i + 2:]\n\t\t\tif"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\toptions = argparse.Namespace()\t\n\tfor namespace, arg_parser in subparsers.items():\t\n\t\toptions.__dict__.update(arg_parser.parse_args(unparsed_arguments).__dict__)\n\t\tfor arg_name, arg_value in vars(arg_parser.parse_args(unparsed_arguments)):\n\t\t\tnamespace_args[arg_name] = namespace.replace('.', '__').replace('/', '__').replace('-', '__').replace('_', '__')"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t# Build an argument parser for your user-defined function.\n\tparser = argparse.ArgumentParser(prog='gogame_auto_parser.py',\n\t\t\t\tusage='Usage: %(prog)s [options] [args]'\n\t\t  \t\t\t       '<auto_game_game_file> [args]\\n'\n\t\t\t\t       'Optional arguments: see example_gogameweek_auto_data.py\\n'\n\t\t\t\t       'Example usage:\t%(prog)s my_gogame_game.pyo\\n\\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\ttry:\n\t\tfrom parser import Parser\n\texcept ImportError:\n\t\traise ImportError('Module \\'parser\\' not found. Please file a bug against Parse.')\n\treturn Parser, parser\n\n##########\n# main\n##########\ndef main():\n\ttry:\n\t\targs, unknown = make_parser()\n\t\toptions = args.get_options()\n\texcept argparse.ArgumentError as e:\n\t\tprint >>sys.stderr, args.get_usage()\n\t\traise e\n\texcept getopt.GetoptError as e:\n\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparsers = []\n\tfor module in modules:\n\t\tfor spec in dir(module):\n\t\t\tfull_path = module_full_path(module, spec)\n\t\t\tif full_path.startswith('parsers.') and full_path.endswith('.py'):\n\t\t\t\tparsers.append(full_path)\n\treturn tuple(parsers)"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tpass\n\n\ndef make_subparser(parser, command=None, subparser=\"\", args=\"\", description = \"\", help_text=\"\", minargs=0, mutuallyexceptions=0):\n\t\"\"\"\n\tCreate a base sub-command parser (that will be initialized on the first call) and a sub-parser for that sub-command.\t\n\t\"\"\"\n\t# Add any subparser\n\tif subparser!= \"\":\n\t\tparser.add_argument(subparser, nargs=\"?\", help=description, metavar=description)\n\n\t# Add the supplied arguments.\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparsers = []\n\tcommand_prefixes = [\"\", \"--verbose\"]\n\tfor cmd_name in command_names:\n\t\tcommand = cmd_name.upper().replace(\"-\", \"_\")\n\t\tif command not in command_names + [\"COMMAND_SRC\"] + command_prefixes:\n\t\t\tcontinue\n\t\t# A simple top-level parser.\t\n\t\tparser = ArgumentParser(\n\t\t\tprog=f\"{command}\",\n\t\t\tdescription=f\"{cmd_name} command.\",\n\t\t\tadd_help=False,\n\t\t\tadd_help"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = make_parser(0, \"parse\")\n\tmy_parser = parser.add_subparsers(dest=\"parser_name\")\n\n\t# make all subparsers for parsing\n\tfor i in range(3):\n\t\ti = my_parser[i]\n\t\tnew_subparser = new_parser(i, f\"subparser_{i}\", \"\", 3)\n\t\ti.add_parser(new_subparser)\n\n\treturn (my_parser[0], new_subparser)\n\ndef main():\n\t\"\"\"Parse, parse it,"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Utilities for the DataMining web')\n\tparser.add_argument('input_csv', \n\t\t\thelp = 'path to the input csv to process',\n\t\t\tmetavar = '<path-to-csv>')\t\n\tparser.add_argument('output_csv', \n\t\t\thelp = 'output csv',\n\t\t\tmetavar = '<path-to-csv>')\n\targs = dict(parser.parse_args())\n\tinput_csv = args['input_csv']\n\toutput_"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser.add_argument(\n\t\t('--port', '-p', type=int, nargs='?', default=8050, \n\t\t help='The port to run the server on (default %(default)s)')\n\t)\n\n\t# --host option\n\tserver_args = parser.add_argument_group('server arguments')\n\tserver_args_group = server_args.add_mutually_exclusive_group(required=False)\n\tserver_args_group.add_argument('--host', dest='host', action='store', \n\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t# Build the parser\n\tparser = argparse.ArgumentParser(description=\"Generate a lexed text file out of a given file\")\n\n\t# Collect the arguments\n\targs = parser.parse_args()\n\n\t# Process arguments\n\tdocuments = args.directory\n\tparser = make_parser(documents)\n\n\t# Parse the input file\n\tsrc_parsers = parser.parse_args(sys.argv[1:])\n\n\t# Return the input parser\n\treturn src_parsers\n\n\nif __name__ == \"__main__\":\n\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\timport string\n\tparser = argparse.ArgumentParser(description='Search a collection of PDF files and output results')\n\tparser.add_argument('file', metavar='FILE', help='File to parse', type=str, nargs='?', default='.')\n\t# parser.add_argument('extension', metavar='EXTENSION', help='Extension to process', type=str, nargs='?', default='.pdf')\n\tparser.add_argument('-D', default='TTF', metavar='D', help='Force PDF decoding"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# TODO: allow a config dictionary to be passed in as an input\n\t# TODO: this function can also be used with list, tuple to reduce the overhead of\n\t#\t\tadding nested configurations, e.g.:\n\tmerged_node =  deep_merge_nodes(nodes)\n\t# TODO: return the merged nodes?\n\t# TODO: create the key/value/key/value-node: list - we'll want this to come after that for\n\t#\t\ta key/value/key, for that key/value to use\n\t#"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tmerged_data = []\n\tlast_node = None\n\tfor next_node, value in nodes:\n\t\tif not isinstance(value, MutableMapping):\n\t\t\tvalues = [value]\n\t\t\tkey = value\n\t\t\tnext_node = value\n\t\telse:\n\t\t\tvalues = value\n\t\t\tkey = next(value.iterkeys())\n\t\t\tnext_node = value\n\t\ttry:\n\t\t\tif next_node.tag == 'key' or not next_node.flow_style and key!= last_node.tag:\n\t\t\t\tmerged_data += values\n\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfor node in nodes:\n\t\tnode_key, node_val = node\n\t\tnode_key_parts = node_key.split(\",\")\n\t\tif len(node_key_parts)!= len(node_val.split(\"=\")):\n\t\t\traise ValueError(\"Nested borgmatic config data structure in {} has invalid format.\".format(deep_merge_nodes.__name__))\n\t\tfor kk, vv in node_val.split(\"=\"):\n\t\t\tif kk.startswith(\"tag\"):\n\t\t\t\tnode_key_parts."}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfor node in nodes:\n\t\tif type(node) is (): #ignore empty nodes\n\t\t\tpass\n\t\telif type(node) is not tuple: #ignore the non-Scalar node\n\t\t\treturn None\n\t\telse: #for nodes with a key-less value\n\t\t\ttry:\n\t\t\t\tnode = deep_merge_nodes(node)\n\t\t\texcept: #the node is invalid\n\t\t\t\tpass #don't raise error again\n\t\t\tnew_node = node #the last returned node had to be a tuple of ScalarNodes\n\t\t\tfor key, value in node: #"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tcur_node = (None, None)\n\tnode_dictionary = {}\n\n\tcur_node = update_nodes(cur_node, nodes)\n\n\tdeep_merge_node(cur_node, node_dictionary)\n\n\treturn node_dictionary\n\n\ndef deep_merge_mapping(node, merge_node=False):\n\t\"\"\"\n\tGiven a mapping as a list of values of the form as:\n\n    {\n        tag:\n        {\n            ScalarNode(value=A):\n            {\n                node:\n            }\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tret = list(nodes)\n\tfor i in range(len(nodes)):\n\t\tif type(ret[i]) is not type(nodes[i]):\n\t\t\tif type(ret[i]) is int or type(ret[i]) is float:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tret[i] = ScalarNode(\n\t\t\t\t\tnodes[i].tag,\n\t\t\t\t\tvalue = nodes[i].value\n\t\t\t\t)\n\t\t\t\tret[i].set_prefix(nodes[i].val_prefix + nodes[i].idx_list[-"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tif isinstance(nodes[-1], MappingNode): # if it's a mapping, it will be a sequence\n\t\tnested_items = []\n\t\tfor node in nodes[:-1]: # there's a sequence as well\n\t\t\tnested_items.append([node[0], node[1].merge_nodes(nodes[-1][1:])])\n\t\tnodes += nested_items\n\telse:\n\t\tfor node in nodes:\n\t\t\tnode_in_nested_merge = False\n\t\t\tif isinstance(node, MappingNode):"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# first make sure we've created a node\n\ttry:\n\t\tnode = [n for n in nodes if not isinstance(n, ScalarNode)][0]\n\texcept IndexError as e:\n\t\traise YAMLConfigurationError(\"Invalid config. Nodes of a non node are not supported.\"\n\t\t\t\" The data is %s\" % e)\n\n\t# first make sure the result is not a sequence\n\tval = [] if isinstance(nodes, Sequence) else nodes\n\n\t# now we have a list of tuples, where each tuple is a (node, value"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdef merge_into_file(base_config_file, merged_config_file):\n\n\t\tif merged_config_file._merged:\n\t\t\t# if the parent already has an entry in the merged file, return a warning\n\t\t\tif base_config_file._merged[0]._parent:\n\t\t\t\tbase_msg = \"<warn: %s.%s is already in the output file. This will override the parent's attributes, which results in a new config file.\" % (base_config_file._merged[0]._parent.__class__.__name__, base"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tmerged = None\n\tfor (c1, n1) in nodes:\n\t\tif not (type(n1) == ScalarNode and len(n1)==1):\n\t\t\traise ValueError('inmerge_nodes: nested borgmatic configuration data structure %r is not of the same structure' % (n1,))\n\n\t\tif not isinstance(n1[0], ScalarNode):\n\t\t\traise ValueError('outmerge_nodes: nested borgmatic configuration data structure %r is an element of another value' % (n1[:1],"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description=__doc__, prog='nemlcd', add_help=False)\n\tparser.add_argument('-i', '--input', metavar='file', \n\t\thelp='Read only the input file to write to, and do not modify it.', required=True)\n\tparser.add_argument('-o', '--output', metavar='file', \n\t\thelp='Write to a file, and do not modify it.', required=True)\n\tparser.epilog = 'Type \"python nemlcd\""}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\t# Add parents and their names as arguments.\n\tparser.add_argument(\n\t\t'-t', '--title', type=str, default=\"\", \n\t\thelp=\"Title of input files to include in the returned result.\"\n\t)\n\tparser.add_argument(\n\t\t'-s', '--source', default=\"\", type=str,\n\t\thelp=\"Directory containing source file to run on, which will be \"\n\t\t\"automatically copied and renamed.  If this argument is not given, \"\n\t\t\"no files will"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tprint('Argument list:',''.join([str(a) for a in arguments]))\n\tparser = ArgumentParser(description=\"Generates a.mf file with model information.\")\n\tparser.add_argument('-f', '--formulae_file', default=Formulae.formulae_path, help=\"\")\n\tgroup = parser.add_mutually_exclusive_group(required=True)\n\tgroup.add_argument(\"-s\", '--super-structs', action='store_true',\n\t\t\t\t\thelp=\"Load super-structures from"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom argparse import RawTextHelpFormatter\n\tif len(arguments) > 1:\n\t\traise ValueError('Expected one argument: one argument to parse.')\n\treturn Arguments(*arguments)\n\n\nclass Arguments:\n\t\"\"\"\n\tThe Arguments instance is a container to the arguments defined in a program.\n\tThis class provides a couple functions for accessing and parsing arguments, and to\n\tparse them from a command line.\t\n\t\"\"\"\n\n\targuments = argparse.Namespace()\n\n\tdef __init__(self"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\tfrom argparse import ArgumentParser\n\t\n\tparser = ArgumentParser(description = __doc__)\n\t\n\tparser.add_argument(\n\t\t\"-m\",\n\t\t\"--model\",\n\t\trequired=True,\n\t\ttype=str,\n        help=\"model\",\n\t)\n\t\n\tparser.add_argument(\n\t\t\"-r\",\n\t\t\"--routers\",\n\t\trequired=True,\n\t\ttype=str,\n        help=\"routers\",\n\t)\t\n\t\n\ttry: \n\t\tparser.add_argument(\n\t\t\t\"-"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description='Get information on Google Books in one or more Google Books Search.')\n\tparser.add_argument('--search',\n\t\t\t\t\t\taction='store',\n\t\t\t\t\t\tdefault='',\n\t\t\t\t\t\thelp='A Google Books Search query or term to search on. Defaulting to empty.')\n\tparser.add_argument('--output-file',\n\t\t\t\t\t\tdest='output_file',\n\t\t\t\t\t\thelp='A file to write the output to. Defaults to the current directory but can be overridden by the value of --output-directory.'"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description=__doc__.replace('\\n', '\\n' +'' * len(arguments[0].replace('-', '\\n')[0].replace(':', ''))))\n\tparser.add_argument('-i', '--interface', dest='interface',\n\t\thelp='Interface to make changes into (e.g. wlan0)',\n\t\tmetavar='interface', required=True)\n\n\tparser.add_argument('--host', dest='host', help='IP Address of host',\n\t\tmetavar='host',"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description='A wrapper for a text analysis script.')\n\tparser.add_argument('--id', '-i', nargs=1, type=Path, help='A (relative or absolute) filepath prefix for this script.')\n\treturn parser.parse_args(arguments)\n\nif __name__ == \"__main__\":\n\targs = parse_arguments()\n\toutput_dir = args.id[0]\n\tif not output_dir.is_dir():\n\t\tprint(f\"The output directory {output_dir} doesn"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\targparser = ArgumentParser(description=argparse_help)\n\targparser.add_argument('-m', '--match', metavar='MATCH', type=str, nargs=1)\n\targparser.add_argument('-r', '--regions', metavar='REGIONS', nargs='*', const=all_regions, type=str, default=None)\n\targparser.add_argument('-f', '--fields', metavar='FIELDS', nargs='+', type=str)\n\targparser.add"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\ttry:\n\t\targs = docopt(r\"\"\"\\\n\n\t\tUsage: %(prog)s [options] <path-to-python> [files]\n\n\n\t\tOptional arguments:\n\n\t\t  -f:\tFor --py3, parse arguments as attributes  (requires --py2 or --py3)\n\t\t  -A:\tFor --py2 --py3 arguments parse as attribute access (requires --py2)\n\t\t  --list: List all files for the specified name or directory\n\n\t\tExample:\n\t\t--list:\n\t\t--py3\n\t\tPython C"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn''.join([str(x)\n\t\t\tfor x in parser.argtypes + parser.defaults + parser.destargs])\n\ndef parser_doc(parser, parser_flags):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return the help text for every\n\tcommand if it is marked as a document, else the string empty string \"\".\t\"\"\"\n\th = parser.formatter_help + parser_flags + ''\n\th = parser.get_help(h)\n\treturn h\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \" \".join([f.help for f in parser.flags.values()])\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\toptions = ['--pct_ref_id', '--pre_selected', '--selected', '--unselected', '--mesh_select', '--unmesh_select']\n\tprint 'Flags:',\n\tret = ''\n\tfor i, option in enumerate(options):\n\t\tflag = '--%s_'%(option.upper())\n\t\tmgr = parser.add_argument_group('')\n\t\tmgr.add_argument(flag, \n\t\t\t\t\t\t\t\tnargs='?',\n\t\t\t\t\t\t\t\tdefault=True,\n\t\t\t\t\t\t\t\tconst=False,\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags_string = ', '.join('{0.name}={0.default}'.format(_flag) for _flag in parser._flags.values())\n\treturn '.+?' + (flags_string or 'None')\t# If no argstring, use space_separated_flag()\n\n\t\n\ndef create_and_initialize_logging(loglevel, loglevel_filehandler):\n\t\"\"\" create_and_initialize_logging(str, str)\n\n\tTakes a string of the form \"{loglevel} {sys.prefix}\" (and a"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.get_default_values().__str__()\n\n#################################################################\n\nclass Cmd(object):\n\tdef __init__(self):\n\t\tself._parser = argparse.ArgumentParser()\n\t\tself._parser.add_argument('args', \n\t\t        type=argparse.FileType(mode='rb'), nargs='*', \n\t\t        default=[], required=_IS_LEFT_IS_MORE_THAN, \n\t\t        help='File or folder to create or overwrite')\n\n\tdef run(self):\n\t\targs ="}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\tif not parser.is_enabled():\n\t\treturn flags\n\n\targv = str(parser.sysargs())\n\tflagzs = re.findall(\"=.*\", argv)\n\tflagzs = \"-\".join(flagzs)\n\tflags += flagzs\n\treturn flags\n\n\ndef parser_print_version(parser):\n\t\"\"\"\n\tDetermine the appropriate version of Python, and print a warning warning.\t\"\"\"\n\tif not parser.is_enabled():\n\t\treturn\n\n\tflagzs = parser_flags(parser"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = []       # string form of flag flags\n\tfor f in parser.flags:\n\t\t# a space-separated flag must not include '-' which has been already part \"--\",\n\t\t# since these would have been accidentally combined with flags with '_'.  So\n\t\t# just remove the leading whitespace.\n\t\tif not f.startswith('-') and f[0:1].isalpha() and f.count('=') <= 0:\n\t\t\t# remove trailing whitespace, and all space-characters after them which are not '_'\n\t\t\tf = '/"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags_string = \"\"\n\tfor key, value in vars(parser.flags).items():\n\t\tif len(value) > 0 and value[0]!= \"-\" and value[0]!= \"_\" and value[0]!= \"_\":\n\t\t\tflags_string += \"{}\".format(key) + \" \"\n\t\t\tif type(value) is bool:\n\t\t\t\tif value:\n\t\t\t\t\tflags_string += u\"T\"\n\t\t\t\telse:\n\t\t\t\t\tflags_string += u\"F\"\n\t\t\telse:\n\t\t\t\tflags_string += str(value)\n\t\telse:\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = parser.format_help()\n\treturn(flags.split('\\n'))\n\ndef save_args(args):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, store the contents of the args object in a json file.\n\t\"\"\"\n\targs.nodb_root = ROOTDIR\n\targs.nodb_directory = os.path.join(args.nodb_root,args.targetnode)\n\twith open(os.path.join(args.nodb_root,args.targetnode)+'_args"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn''.join(parser.flags.keys())\n\n\ndef help_markdown_html(text, html):\n\t\"\"\"\n\tReturns the given in a simple way... with help markup embedded into\n\ta single HTML block with HTML structure containing the given text.\n\t\"\"\"\n\treturn '<pre><pre>%s</pre></div>\\n%s</div>\\n<blockquote>\\n%s\\n</blockquote>\\n' % (text, html, text)\n\n\ndef help_markdown_text(text, text_type, html="}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tscript = \"\"\n\tfor name, value in globals().items():\n\t\tif type(value) == str:\n\t\t\tscript += f\"bash.io '{value}'\\t\"\n\treturn script\n\n# ------------------------------------------------------------\n#\n#\t\tCommand\n#\t\t\n# ------------------------------------------------------------\n\ndef borgmatic():\n\t\"\"\"Launch the borgmatic command on the shell\"\"\"\n\tprint(bash_completion())\n\tprint(\"\"\"Command: python3 -m borgmatic {command}\"\"\")\n\tprint(\"\"\"\n\tCommands you may use:\n"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tbashp = \"bash_completion -l --path=$PATH \" + bash_path_prefix # + bash_path\n\tp = subprocess.Popen(bashp, stdout=subprocess.PIPE)\n\tp.wait(10)\n\treturn []\n\n\ndef help():\n\treturn 'The bot commands.'\n\nhelp = help\n\n\n"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n\tbash_completion_list = [\n\t\t\"\\\\\",\t# this is a subgroup that uses bash command with argument \"\n\t\t\"||,  # or \"\n\t\t\"%&&,  # or \"\n\t\t\"&&&&,  # or \"\n\t\t\"&>>\",  # or \"\n\t\t\"&\",\t# or \"\n\t\t\"%\",\t# or \"\n\t\t\"|>>\",  # or \"\n\t\t\"|||\",  # or \"\n\t\t\"||&&\",  # or \"\n\t\t\"||&\",\t# or \"\n\t\t"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t# Get the borgmatic command\n\tcmd = (arg.replace(\"--\", \"\") for arg in sys.argv[1:])[0]\n\treturn \"\\n\".join((\"#Borgmatic argument --%s\" % c for c in cmd))\n\ndef main(argv = ()):\n\t\"\"\"Entrypoint for the borgmatic command-line parser.\"\"\"\n\tprint(bash_completion())\n\tparser = argparse.ArgumentParser(\n\t\t\t\tprog=\"borg\", description=__doc__,\n\t\t\t\tadd_help=True, allow_"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tif borg_completion is None:\t\t\t\t\t\t\t\t\t\t# don't execute if no complete script exists.\n\t\tprint \"ERROR: Please launch the bat file to generate bash_completion script.\"\n\t\treturn 0\n\tbscript = borg_completion\n\tcomplete_script = \"\"\"\n/usr/bin/\nbash-completion -l -l <PATH> -e 'bash -i -c \\'\\n'\nbash-completion --help <PATH>\n\n\"\"\"\n\tcomplete_script = complete_script.replace('\\\\', '\\\\\\\\')\n\treturn complete_script\n\n"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tscript, args = get_borgmatic(\n\t\tcommand_line_parser=command_line_parser,\n\t\tlog_handler=logging_handler,\n\t\targ_parser=arg_parsers\n\t)\n\tset_shell_completion(script)\n\treturn script\n\n\ndef arg_parser():\t\"\"\"\n\tDefine a parser to be used for shell arguments.\t\"\"\"\n\t# Defaults for arguments in the bash completer\n\toptions = {}\n\t# Arguments in command line parser\n\tcmd_line_parser.add_argument(\\\n\t\t\""}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\ts\t=\tbash_script()\t+ \"\\n\"    \t\t\t\t# bash_script for cmdline syntax\n\ts = s + \"\\n\\t\"    \t\t\t\t\t\t\t\t\t### <!> - for a comment\n\ts = s + chr(0x0a)                                   # put our bash escape chr marker\n\ts = s + chr(0x0a)\t\t\t\t\t\t\t\t\t# on a windows machine\n\ts +=  \" \\t\\t# %s\\n\"                              # for a command (no -f in a shell)\n\ts +=  \"   -b $"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n\timport sys\n\timport os\n\timport shlex\n\timport cmd\n\tfrom cmd import cmdline_parse\n\timport textwrap\n\timport os.path\n\ttry:\n\t\tbasename = __file__\t# gets the file name without extension\n\t\tif basename == '': basename = None\n\t\telse: basename += '.'\t\t# the file extension is not empty\n\t\ttry: path = '' if basename == None else '{}'.format(basename)\n\t\texcept UnicodeDecodeError:\n\t\t\tprint('File {} does no fit in {}.'"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tbash_completion_file = os.path.join(\"resources\", \"bash_command_parser_completion.bash\")\n\treturn bash_completion_file.replace(\".sh\", \".bash\")\n\ndef docker_image_search():\t\"\"\"\n\tSearch the Docker images in the filesystem to match the borgmatic docker image.\n\t\"\"\"\n\tdocker_file = os.path.join(\"resources\", \"DockerImages.txt\")\n\tdocker_file_match_check = False\n\tfor fname in os.listdir():\n\t\tif docker_file in f"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t# TODO: Complete the bash-completion command to return an array representing possible completions for all commands and arguments. These will be used by Bash's 'args' magic-function.\n\ndef add_command_args(parser):\n\t\"\"\"\n\tFor each command, add the command-line arguements to the parser.\t\"\"\"\n\t# TODO: Implement add_command-args in the add_command-args decorator. \n\t\"\"\"\n\tAdd command-line arguments for all commands and all arguments.\n\t\n\tIf the argument is not used, it will not appear"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument(\n\t\t\"command\",\n\t\tnargs=\"*\",\n\t\thelp=\"\"\n\t)\n\tparser.add_argument(\n\t\t\"-h\",\n\t\t\"--help\",\n\t\taction=\"help\",\n\t\thelp=\"show this help information and exit\"\n\t)\n\treturn parser.parse_args(*unparsed_arguments)\n\ndef print_subparser_help():\n\tprint(\"Subparsers are named as short-names, such that the long"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tfrom argparse import Namespace\n\treturn dict(parse_known_args(*unparsed_arguments) or vars(local_args))\nmain = parse_arguments\n\nfrom. import __copyright__\nfrom. import __version__\nfrom. import __license__\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal_ns = Namespace(**locals())\n\tglobal_ns.verbose = global_ns.verbose or config.verbose\t\t\t\t\n\treturn subparsers_arguments_dict(global_ns, sys.argv)\n\ndef parsers_arguments_dict(*unparsed_arguments):\n\t\"\"\"\n\tGiven positional arguments (that have been supplied in the command-line) are unparsed into a dict for use as kwargs for the corresponding subparsers, parse for each subparser's arguments and return them as a dict.\t\"\"\"\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# Parse arguments\n\targv = sys.argv\n\t# Get subparser and parse options\n\tparser = get_argparser(*unparsed_arguments)\n\t# Get config data\n\tconfig = parser.parse_args(argv[1:])\n\t# Load config\n\tconfig.init(argv[:1])\n\t# Log config\n\twrite_config(config)\n\t# Get and print config, log, and quit arguments\n\targs = get_commandline_arguments(config, argv[1:])\n\tif args"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\tglobal_arguments = argparse.Namespace(**unparsed_arguments)\n\t\n\t# Get the subparser that is currently being called;\n\t#   - Check that it's actually one in list of parsers\n\t#   - Check the top level subargument parser in order of declaration of module name\n\t#   - Check that it matches the name of a known subparser\n\tsubparser_name = name\n\twhile subparser_name in subparsers.keys():\n\t\tif subparser_name == \"global\":\n\t\t\treturn global_arguments"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Runs an experiment.')\n\tparser.add_argument('--mode', type=str, help='Run the specified experiment, but display results in console as interactive text on screen.', default='validation', choices=('validation', 'validation_augment', 'test', 'test_augment'))\n\tparser.add_argument('--debug', action='store_true', help='Run the experiment, but only display progress bar messages.', default=False)\n\treturn parser.parse_args(unparsed_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\tfrom argparse import ArgumentError\n\t\t\n\tglobal options\t\t\t\n\toptions = {}\n\t\n\t# Set subparser usage message to just the command line\n\t# options so a subparser can be loaded with the command-line arguments that were given\n\t\n\t# Convert subparser arguments from global option format to argument format from command line arguement\n\tsubparser_global_args = {}\n\tif len(unparsed_arguments)!= 0:\n\t\tfor optionName, arguement in options.items():\n\t\t\t# In case there are subparser options with"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tunparsed_arguments = sorted(unparsed_arguments, key=lambda arg: arg.name)\n\tif unparsed_arguments:\n\t\tparser = subparsers.add_parser('arguments', help=None, parents=[p])\n\t\tfor argument in unparsed_arguments:\n\t\t\tparser.add_argument(*argument.args, **argument.kwargs)\n\t\treturn vars(parser.parse_args())\n\treturn {}\nif __name__ == '__main__':\n\tmain()\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=os.path.basename(sys.argv[0]),\n\t\tusage=\"usage: %(prog)s [options] --dir=dir\\noptions: --quiet | --verbose | display\",\n\t\tdescription=\"Parses sys.argv[0] for use as subparser names.\",\n\t)\n\tgroup = parser.add_argument_group('subcommand options')\n\tfor subcommand in subcommands():\n\t\tgroup.add_argument(\"--\" + subcommand.dest, dest="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# Initialize argparse\n\tparser = argparse.ArgumentParser(description='Run a simple, deterministic, stable reinforcement learning algorithm.')\n\t\n\t# Add arguments\n\tfor command in subparsers._actions:\n\t    parser.add_argument('subparsers'.join([command.dest,command.default_metavar]),**command.metavar_options)\n\n\t# Run parser, save argparse args, return the remaining args\n\targuments = parser.parse_args()\n\targuments = vars(arguments)\n\targs = Names"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tgrouping_done = False\n\tsubparsers_dict_vars = {}\n\tsubparsers_remaining = []\n\tfor unparsed_arg in unparsed_arguments:\n\t\tsubparser_mappings = subparsers.parse_args([y[0] for y in unparsed_arg], namespace=True)\n\t\tif subparsers_mappings is None:\n\t\t\traise ValueError(\"unrecognized unparsed \" +str(unparsed_arg))\n\t\tsubparser_name = subparser_mappings.name"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tsubparser = subparsers.pop(unparsed_arguments[0])\n\t# Create a dict of subparsers and their associated lists of arguments.\n\targuments = parser_arguments = {}\n\tfor subparser_name in subparsers:  # We need a temporary mapping, in case some subparsers are required/forgetting a subset/removing some subparsers.\n\t\tsubparser_arguments = {}\n\t\targuments_to_parse = subparser.add_subparsers(destination=subparser_name)\n\t\tfor unp"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\targ_dict = {}\n\n\t# Assume that we were passed an iterable of unparsed args as a list of strings.\n\tunparsed_arguments = list(unparsed_arguments)\n\n\t# Make a special namespace for all subparsers so we can handle subparser name-based\n\t# unparsing.\n\tsubparsers = {}\n\tfor subparser_name in subparsers.keys():\n\t\t# If argument name is a nested subparser, then we want it to map to the\n\t\t# subparser's subparsers dict"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# print(\"parse_subparser_arguments\")\t\n\tparser = subparsers.add_parser(\n\t\tsubparsers.get_prog() + \" \" + \", \".join([\"--\" + sub.replace(\"-\", \"_\") for sub in unparsed_arguments[1:]]),\n\t\tdescription=\"\"\"%s <subcommand> <name> <args>\n\nSub-command must either be followed by a \" --\" (the first argument), followed by any\nother arguments (followed by <name> and any arguments following), or an option ("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tglobal subparsers_dict\n\n\t# Create a dict to store parsed subparsers and their names in\n\tglobal_names = {\"__main__\": None}\n\n\t# Convert the list to a dict as required by argparse\n\tunparsed_arguments = dict([(p, a) for a, p in unparsed_arguments])\n\tsubparsers_dict = {}\n\tfor name in subparsers.keys():\n\t\tsubparsers_dict[name] = argparse.ArgumentParser(\n\t\t\tconflict_handler='resolve',"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tconfigurations_by_subparsers = [None] * len(unparsed_arguments)\n\treturn configurations_by_subparsers\n\nif __name__ == '__main__':\n\t# Get arguments\n\targuments =  sys.argv[1:]\t\n\tunparsed_arguments = parser.parse_args(arguments)\n\n\t# Configuration. Initialize all subparsers\n\tsubparsers = parser.add_subparsers(dest='parser_name',\n\t\t\t\t\t\thelp='sub-command help')\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# FIXME(jeremy-papier): do not keep a nested mapping of subparsers and subparsers, because there may be multiple sub-parsers, which are of a type or different subparsers.\n\tconfig = {}\n\tparser_names = []\n\tfor argument in unparsed_arguments:\n\t\tname, args = argument.split(maxsplit=1)\n\t\tname = name + \".\"\n\t\tif name not in config:\n\t\t\tconfig[name] = {}\n\t\tconfig[name][\"arguments\"] = args\n\t\tif len"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tsubparsers = get_subparser_names(unparsed_arguments, subparsers)\n\toptions = {}\n\tfor subparser in subparsers:\n\t\t# The arguments, not the default arg_values, were previously stored in _get_args\n\t\t# in get_all_arg_values(**kwargs).\n\n\t\t# Remove argument values from the subparser's namespace.\n\t\tnamespace = _get_args_namespace(subparser, unparsed_arguments)\n\t\t# Remove argument values from the list of remaining arguments because they were\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\taction_name_list = []\n\tfor arg in unparsed_arguments:\n\t\taction_name = arg.pop('action').lower()\n\t\tif action_name not in actions:\n\t\t\taction_name_list.append(action_name)\n\t\n\tparserdict = {} # the dictionary of subparsers\n\tfor action in subparsers:\n\t\t# print('\\n', args, [arg.name for arg in unparsed_arguments], [str(action.default)])\n\t\tif action.name in action_name_list or"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\tresult = {}\n\tremaining_arg_names = []\n\n\tfor i,argdict in enumerate(unparsed_arguments):\n\t\tif len(argdict) is None:\n\t\t\tremaining_arg_names.append(argdict.keys())\n\t\telse:\n\t\t\tif not isinstance(argddict, dict):\n\t\t\t\tbreak\n\t\t\tfor k in argdict.keys():\n\t\t\t\tremaining_arg_names.append(k)\n\n\t\t\tif '__missing_args__' in argddict:\n\t\t\t\tkwargs = argddict\n"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t# We'll also have the sub-parser classes in the parser class dict.\n\t# This will be used for all the command names.\n\tparser_classes = {}\n\n\t# First fill up the parser class dict with sub-parser classes\n\t# Build a class for each top-level parser\n\tfor parser_class in parser_classes:\n\t\tif hasattr(parser_classes[parser_class], 'add_subparsers'):\n\t\t\t# Check to see if this parser allows a subparser\n\t\t\tsubparser_class = parser_classes[parser_class].add_sub"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tclass Parse(argparse.Action):\n\t\tdef __call__(self, parser, namespace, values, option_str=None):\n\t\t\tsetattr(namespace, self.dest, values)\n\t\t\t\n\tparser = argparse.ArgumentParser(description='Coding exercise')\n\tsubparser = parser.add_subparsers()\n\n\tdef action_echo(*args):\n\t\t\"\"\"The echo function just prints the given string through the parser.\"\"\"\n\t\tprint(args[0])\n\n\tdef action_count(*args):\n\t\t\"\"\"The count function counts the occurrences"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser_parsers = {}\n\tfor type in _ALL_DOC_TYPES:\n\t\tparser_parsers[type] = DocParser()\n\n\tparser_parsers[\"unknown\"] = UnhandledDocParser()\n\tparser_parsers[\"html\"] = HtmlParser()\n\tparser_parsers[\"text\"] = TextParser()\n\treturn parser_parsers\n\ndef init(sub_parsers, spec,\n\t\tsubparser_overrides, docstrings):\n\t\"\"\"\n\tProcess and check a spec and subparser dictionary."}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\tpf = ParserHelper()\n\tpf.enable_subparsers()\n\t\n\t# Add the parser to every top-level parser of the top-level lexer class.\t\n\tpf.add_lexer(LexerParser)\n\t\n\treturn pf.get_subparser_names()\n\t\t\t\t\t\t\t\t\t# [parser name for parser_name in parser_names]\t\t\n\n\nclass ParserHelper:\n\t\"\"\"\n\tA helper class that will help creating command line argument values.\n\t\"\"\"\n\t\t\t\t\t\n\tdef __init__(self,"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\timport argparse\n\timport re\n\tfrom tqdm import tqdm\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Argument parsing for --models, --train_test, --config_file argument.\")\n\tparser.add_argument(\"--model\", help=\"model to parse\", metavar=\"NAME\")\n\tparser.add_argument(\"--train_test\", help=\"train/test\", metavar=\"BOOL\")\n\tparser.add_argument(\"--config_file\", help=\"config file to use\", metavar=\"PATH\")\n\tparser"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser('pytest')\n\tparser.add_argument(\n\t\t'-x',\n\t\taction='append',\n\t\tdefault=[],\n\t\thelp='exclude files matching one of these extensions. Multiple formats are supported.')\n\tparser.add_argument(\"-s\", \"--skip\",\n\t\taction=\"append\",\n\t\tdefault=[],\n\t\thelp='skip this file or directory test. Can specify multiple times.')\n\tparser.add_argument(\n\t\t'--exclude-module',\n\t\tdefault=[],\n\t\taction='append',\n\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser()\t\n\tparser.add_argument('-i', '--input', type=str, default=None, help=\"input file\")\n\tparser.add_argument('-o', '--output', type=str, default=None, help=\"output file\")\n\n\treturn parser, [args.input, args.output]\n\t\n\n\ndef create_argument_parser(args: argparse.Namespace):\t\"\"\"\n\tCreate the parser for the argument values.\t\"\"\"   \n\tparser, subparsers = make_parsers()"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Generate a list of valid XML namespaces.')\n\tparser.add_argument('--namespaces-file', '-N', metavar='file',\n\t\t\t\t\t\thelp='the filename.  If this file is provided then the'+\n\t\t\t\t\t\t'receiving command line argument --ns-prefix-file will be'+\n\t\t\t\t\t\t'used to get the namespace prefix for --ns-file.')\n\tparser.add_argument('--ns-prefix-file', '-N', metavar='file',\n\t\t\t\t\t\thelp='the filename"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(description = 'Utilities to work with documents.')\n\t#parser.add_argument('-d', dest=\"doc\", default='docx', help=\"Document.\")\n\tparser.add_argument('-doc', dest=\"doc\", default='mp4.txt2g.doc', help=\"Document.\")\n\tparser.add_argument('-dir', dest=\"dir\", default='data/', help=\"Directory\")\n\tparser.add_argument('-i', dest=\"info\", default='info.txt', help=\"Name and path of the input"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tsubparsers = []\n\tparser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n\n\t# subparsers for subcommands\n\tparser_sub = subparsers.add_parser(description=\"Subcommand information\", prog=\"v2_parser\")\n\n\t# ArgumentParser subparsers\n\tparser_sub.add_argument('sub-command', action='store', help=\"Enter the subcommand you would like to run.\")\n\n\t# subparsers for each arguemnts\n\tparser_sub.set_default"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tsitemap_links = response[5].find_all(\"li\", {\"class\": \"sitemap\"})\t\n\tfor sitemap_link in sitemap_links:\t\n\t\tsitemap_title = sitemap_link.find(\"div\", {\"class\": \"sitemap-content\"}).find(\"div\", {\"class\" : \"thumbs\"}).find(\"span\").text\t\n\t\tsitemap_link = \"https://mapis.com/maps/search?query=\" + sitemap_title.replace(\" \", \"+\")\t\n\t\ty"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tpage = 2 #2 because of 2paging in page 1st and 2nd\n\tfor response_item in response:\n\t\tyield(page, response_item)\n\t\tpage += 1\n\ndef format_string(format_type):\n\t\"\"\"\n\tConvert a werkzeug.http.Formatter(string) to a regular formatter(string) using passed types\nArgs:\n\tformat_type (string): Type you wish to convert\nReturns:\n\tFormatter(string).\t\t\"\"\"\n\tif format_type == str:\n\t\tformat_type = str"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tpages=1\n\tcount=0\n\t\n\twhile True:\n\t\trequest='GET /'+path+'?$top='+str(max_results)+'&$skip='+str(count)+'&$num='+str(pages*max_results)\n\t\tyield _call_request_with_cookies(request, response)\n\n\t\tcount+=1\n\n\t\tif(count >= max_results):\n\t\t\tbreak\n\n\t\trequest='GET /'+path+'?$top=-1&$skip=-1&$num='+str(count)\n\t\ty"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tpage = 0\n\twhile True:\n\t\tpayload = response[(page + 1) * max_results: min((page + 1) * max_results, len(response))]\n\t\tfor page_response in payload:\n\t\t\tyield [\n\t\t\t\tresponse_from_response(page_response)\n\t\t\t]\n\t\t\tpage += 1\n\t\tyield(\n\t\t\tresponse[(page + 1) * max_results:]\n\t\t)\n\t\tif((len(response)) % max_results == 1):\n\t\t\tbreak\n\t\telse:\n\t\t\tcontinue\n\n\n"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\t# Iterate through list\n\tfor item in response:\n\t\t# Get only the 'pageSize' for each 'list' entry\n\t\tpage_size = getattr(item, 'pageSize')\n\n\t\t# Stop if the pageSize exceeds the desired size\n\t\tif (item.pageSize <= max_results):\n\t\t\tbreak\n\n\t\t# Use the pagination offset as pageCounter \n\t\tyield item.page_counter, item\n\n\t\t# Reset the pageSize value\n\t\tpage_size = 0\n\n\t\t# Set the previous pageSize value as the next pageSize value\n\t\t"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tfrom bs4.element import Tag\n\tstart = response[0]['paging']\n\tcounter = 0\n\tpage = \"\"\n\twhile True:\n\n\t\tif not isinstance(start, Tag):\n\t\t\tbreak\n\n\t\tstart_str = start['href'].split('?')[0]\n\t\tend_str = start_str\n\t\tif start['href'].startswith(\"/user/<username>\"):\n\t\t\tend_str = \"/users/\"\n\t\telif start['href'].startswith(\"/user/<username>\"):\n\t\t\tend_str"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\tfor item in response:\n\t\tyield item \n\t\tmax_results -= 1\n\n\tif max_results > 0:\n\t\tif max_results > len(response):\n\t\t\traise Exception ('Max Number of Results exceeded.')\n\t\telse:\t\n\t\t\treturn paging(response[max_results:],max_results)\n\telse:\n\t\treturn response\n\t\t\ndef api_version_list():\t\n\t\"\"\"\n\tReturns a dictionary of Api versions.\n\t\n\tReturns:\n\t\tDictionary of API version's dictionary.\t\n\t\"\"\"\n\n\t"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\t# Define base URL based on response.\n\t# It is required to identify the actual content location by the base URL\n\tbase_url = 'https://api.github.com'\n\t\t\t\n\t# Get current page number and validate page count\n\tcurrent_page = str(response[0]['pageInfo']['totalCount'])\n\tvalidated_page = int(current_page,10)\n\t\n\twhile validated_page <= max_results - 1:\t# Loop until end of list\n\t\t# For each page, we need to retrieve each entry and"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tfor item in response:\n\t\titem.update({'_paging': max_results})\n\t\tyield item\n\ndef check_login(req, user, password):\n    \"\"\"\n    Function to verify user and password exists and in local host.\n    Args:\n        req (req): Client request to be redirected to.\n        user (str): username to be verified.\n        password (str): user password.\n    Returns:\n        [LoginStatus]: Returns a Boolean on success, if fail\n            to verify user or password.\n    Raises:\n        [RequestException"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\treturn response\n\ndef wapi_response(status=None, reason=None, reason_description=None, result=None):\n\t\"\"\"\n\tA function for creating WAPI responses for requests.\n\t\"\"\"\n\tresponse = {\n\t\t\"status\": status,\n\t\t\"reason\": reason,\n\t\t\"reason_description\": reason_description,\n\t\t\"result\": result\n\t}\n\n\treturn response\n\ndef json_response(status=None, reason=None, reason_description=None, result=None):\n\t\"\"\"\n\tA function for creating JSON and WAPI responses"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tassert str.isprintable(size)\n        if size == \"\":\n            return 0\n        try:\n            size = float(size)\n        except:\t\n            raise ValueError(\"Invalid file size: %s\" % size)\t\t\n        elif size <= 0:\n            raise ValueError(\"Invalid file size: %s\" % size)\t\n            \n        value = 1.0 * size / 1000.0 # Convert to bytes, and then adjust for unit of 1000(KiB)\n        return round(value * 1000)\n        \n    def parse_m"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\ttry:\n\t\treturn float(size) * 1024\n\texcept Exception as e:\n\t\tprint(\n\t\t\t\"Error - Unknown type of file/size. \"\n\t\t\t\"Check whether the value provided is a float as \"\n\t\t\t\"float/ratio.\\n\\n\"\n\t\t)\n\t\traise e\n\n# ===============================================================\n#\n#                    MODULES\n#\n# ===============================================================\n\nclass Module(object):\n    def __init__(self, name: str, **extras):\n        self.name = name\n        self.extras"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n\tsize = size.lower()\n\tsize_as_units = {\n\t\t\"bytes\" : 1,\n\t\t\"Kb\" : 1024\n\t}\n\t\n\ttry: \n\t\treturn size_as_units[size]\n\texcept:\n\t\treturn len(size)\n\ndef bytes_to_size_string(file_size: int) -> str:\t\"\"\"\n\tConvert a non-human readable file size to human readable string.\n\n\tResulting value is in most case formatted as the human readable format, such as \"1.5 KiB\"."}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn int(float(size))\n\n\ndef unix_get_time_str(year, month, day) -> str:\t\"\"\"\n\tGet Unix time with the current date/time (yyyy/mm/dd hh:mm).\n\t\n\tReturned value is a string in the format of 'YYYY/MM/DD hh:mm'.\n\t\n\tArgs:\n        year: Year in the str format\n        month: Month in the str format\n        day: Day in the str format\n\t\t\n\tReturns:\n\t\tA str representing Unix time\n\t\"\"\""}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t   \n\t\n\t   \n    if not size:\n        return 0 \n    elif size.isnumeric():\n        return int(size)\n    else:\n        return 0\n\t \n\ndef hash_md5(data: str):\n    \n    \n    \n    dataHash = hashlib.md5(data.encode()).hexdigest()\n    hex_bytes = str(dataHash[:8])+'...'+str(dataHash[-8:])\n    \n    \n    return hex_bytes\n\t \n\t\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tif isinstance(size, str):\n\t\tmatch = r\"([0-9]+(\\.[0-9]+)?(\\.\\d*)?K|[0-9]+)K\"\n\t\tmatch = m.group(0)\n\t\treturn int(match[match.index('.') + 1:])\n\telse:\n\t\tprint(\"ERROR: Not a float value!\")\n\t\texit()\n\n@check_argument(\"path\")\n@check_argument(\"file_type\", str, \"binary\", False)\ndef process_upload(path: str,\n\t\t\t\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t#TODO\n\tif size not in [\"bytes\", \"KB\", \"kB\", \"MB\", \"B\", \"GB\", \"GB\", \"TB\", \"Tb\", \"Ti\", \"Ti\", \"Pi\"]:\n\t\tsize = \"n/a\"\n\n\tsize = size.lower().split(\" \")\n\tsize_unit = {}\n\tfor unit in [\"bytes\", \"KB\", \"kB\", \"MB\", \"B\", \"GB\", \"GB\", \"TB\", \"Tb\", \"Ti\", \"Ti\", \"Pi\"]:\n\t\tsize_unit[unit] ="}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\timport math\n        bytes = math.floor(math.log(size, 10) * 10**6)  # 1k = 10^6 bytes\n        return bytes\n\ndef get_size(string: str) -> float:\n    \"\"\"\n    Convert a human readable string representation to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n    \n    Args:\n        string: A human readable string representation (eg: '6M')\n      \n    Returns:\n        A decimal representation of size\n        \n       Example::\n\n            >>> get_"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t# size is converted to an integer\n\tsize = float(size)\n\t# Convert to 2x power of 10 by rounding up\n\tpower = math.floor(math.log(size // 1, 1024))\t\n\t# return the resulting integer multiplied by 1024\n\treturn math.pow(1024, power)\n\n\ndef parse_ip_addresses(ip: str) -> Iterator[str]:\n    \"\"\"\n    Return a generator of IP addresses in the given input string.\n\n    Args:\n        ip (str): A string representing one or more IP addresses\n                (eg"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn int(size)\n\n\tpass\n\n\n\ndef is_file_empty(filename: str) -> bool:\t\"\"\"\n\tDetermine if the contents of file passed into is_file_empty method are empty.\n\n\tInputs:\n\t   filename: A file name string.\n\n\tReturns:\n    \t  True if the contents of the file is empty, False otherwise.\n\n\"\"\"\n\n    try:\n        # Open a file\n        open(filename, 'r')\n        return False\n    except:\n        return True\n\n\tpass\n\n\tpass\n\n\n\ndef"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tfor dictionary in dicts:\n\t\tfor key in dictionary.keys():\n\t\t\tdicts[key] += dictionary[key]\n\treturn dicts\n\ndef test(parsestr=False):\n\timport sys; print 'test:', sys.argv\n\tprint 'test:', __file__\n\tprint '\\n>>> test()'\n\t# The next four lines are just for dev/testing, and only serves\n\t# as an example for users.\n\tif parsestr:\n\t\timport sys\n\t\tprint parsestr\n\tf = StringIO("}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn sum(dicts, lambda a, b: (a or b), _dictsum)\n\n\ndef _filteritems(items, key=None, func=None):\n\t\"\"\"\n\tReturn list of items for which func should apply to each list item.\n\n\tIf key is supplied, it searches in sorted list (sorted first)\n\tand in case key already present, it skips.\n\tOtherwise, it selects items using func (provided by key).\n\t\"\"\"\n\t# TODO: This is probably not the most efficient implementation\n\t# but it is as close to a brute"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tif len(dicts)==0:\n\t\treturn dicts\n\tif len(dicts)==1:\n\t\treturn dicts[0]\n\telse:\n\t\treturn dict(dictsum(_dictsum(dicts[1:])))\n\t\t\t\n\ndef _convert_to_bytes(x):\n\t\"\"\"Convert x -> bytes or similar.\n\n\tNote: _convert_from_bytes is used in multiple tests to convert\n\teither the \"expected\" result of _convert_from_bytes into the\n\t\"actual\" result if it is desired.\n\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn reduce(set.update, dicts)\n\n#==============================================================================\ndef _find_intersection(lst1, lst2, fn=set.intersection):\n\t\"\"\"Given two lists: [a, b], find the intersection of all elements in both\nit.\n\n\t>>> _find_intersection([1,3,5],[2,4,2],'==')\n\t{2,4}\t\n\t\n\t\"\"\"\n\t\n\treturn fn(lst1, lst2)\n\t  \n\n#==============================================================================\ndef _split("}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tvalues = []\n\tfor d in dicts:\n\t\tfor k, v in d.items(): values.append(v)\n\treturn dict(values)\t\n\n\ndef _compress(text):\n\t\"\"\"\n\tCompress string text.\n\t>>> _compress('123123123123')\n\t'1 2 3'\n\t\"\"\"\n\treturn'1'+''.join(str(i) for i in len(text)\n\t\t\t\t\tif text[i]!='') \n\t\t\n\t\t\ndef _compress_punct(text):\n\t"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn {x: sum(y.values()) for x, y in dicts}\n\n\n#------------------------------------------------------------------------------\n# Test functions\n\ndef test_dictsum():\n\t# The two dictionaries should be equal\n\tassert _dictsum(dicts=[{'a': 1, 'b': 2}, {}]) == {'a': 1, 'b': 2}\n\t# The values must be counted at least 3 times\n\tassert _dictsum(\n\t\t[{'a': 1, 'b': 2, 'c': 1},\n\t\t{'a': 1, 'b"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\td = {}\n\tfor dict in dicts:\n\t\tfor key, val in dict.items():\n\t\t\tif key not in d:\n\t\t\t\td[key] = []\n\t\t\td[key].append(val)\n\t#return {val: sum(d[k] for k in d) for k,d in d.items()}\n\treturn dict(map(lambda x: (x['a'], x['b']), filter(None, list(d.items()))))\n\ndef dict_merge(dicts, merge_mode='strict'"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn dict([(k, sum(v.values())) for k, v in dicts])\n\nif __name__ == \"__main__\":\n\timport doctest\n\tif doctest.testmod().failed == 0:\n\t\tprint \"ALL TESTS PASSED.\""}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn reduce(lambda value, dict_: value | dict_, dicts, {})\n\n\n#------------------------------------------------------------------\ndef get_version():\n\t\"\"\"return the current version of this build\"\"\"\n\treturn VERSION\n\n\n#------------------------------------------------------------------\n# deprecated-rename.  Just to replace pyproject.toml\n\ndef update_deps(project_deps):\n\t\"\"\"Updates a dict containing all project dependencies from a pyproject.toml\n\tfile.  It assumes that the pyproject.toml file is in a directory named\n\t`builddir`, in which the package"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn {key: sum(values) for key, values in dicts}\n\n\n#______________________________________________________________________________________\n\n\n# Test:\ndef test():\n\tx = [5, 10, -3, 0, -1, -4, -7, -9, 2]\n\ty = [-10, 200, 200, -2, 100, -4, -5, -6, -7]\n\n\tprint _sum(x, y)\n\tprint sum(_sum(x, y))\n\n\nif __name__ == '__main__':\n\timport doctest\n\tdo"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif url is not None and not isinstance(url, string_types):\n\t\traise TypeError('Invalid type for url')\n\tif url:\n\t\turl = url.replace(\"#\", \"$\")\n\t\turl = url.replace(\"#\", \"%23\")\n\t\turl = url.replace(\"#\", \"%5B\")\n\t\turl = url.replace(\"#\", \"%5D\")\n\t\turl = url.replace(\"\\n\", \" \")\n\t\turl = _handle_encodings(url)\n\t\turl = re.sub(r\"\\s+\", \" \","}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\trepl_dict = {'&': '%26', '=': '%3A', '#': '%23'}\n\tfor k, v in repl_dict.items():\n\t\turl = url.replace(k, v)\n\treturn url\n\n#----------------------------------------------------------------------------------------------\ndef _download_file(url, filename, output):\n\t'''\n\t\tDownloads a URL, filename and saves it to the output directory\n\t'''\n\twith io.open(output, 'wb') as f:\n\t\ttry:\n\t\t\tfile_path = _download_url"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif url.rpartitions('?')[1][0] in url_args:\n\t\turl_args = url_args.replace(url.rpartitions('?')[1][0],\n\t\t\t\t\t\t\t\t\t\"{}\".format(url_args[url.rpartitions('?')[1][0]]))\n\treturn url_args\t\n\n\ndef parse_args(arg_string):\n\t\"\"\"\n\tParse any string URL arguments and replace any values in args\n\t:param str arg_string: the string arguments to parse\n\t:return: parsed args from arg_string\n"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif url_args:\n\t\targs_list = url.split('&')\n\t\tfor i, entry in enumerate(args_list):\n\t\t\tvalue = entry.split('=')[-1]\n\t\t\turl_args[i] = url_args[i].replace(value, url_args[i])\t\t\t\t# replacing value in url in args\n\treturn url_args\n\ndef get_files(root_dir):\n\t\"\"\"\n\tIteratively return a list of.json files in the given Directory structure\n\t\"\"\"\n\textensions = {'.json','.json"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\ttemp_string = ''\n\tfor arg in url_args:\n\t\ttemp_string = temp_string + quote(arg + \"=\" + urlargs[arg], \"replace\") + '%20'\n\ttemp_string = temp_string.strip('%\\n/')\n\treturn temp_string \n\n\ndef _parse_query_url():\n\t\"\"\"\n\tMethod returns url query arguments as a dictionary\n\t\"\"\"\n\tquery_params = {} # create a dictionary \n\tquery_args = {} # create a dictionary \n\n\ttry:\n\t\turl_list ="}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\treturn url.replace(\"?\", url_args, 1)\n\ndef _filter_url(url_str):\n\t\"\"\"\n\t\tReturn a copy of URL with a '?*' replaced with '**kwargs' \n\t\"\"\"\n\tfiltered_url = url_str\n\tindex = 0\n\twhile index < len(filtered_url):\n\t\tif url_str[index]!= '?' and url_str[index]!= '?' and not url_str[index] == '/':\n\t\t\tif url_str[index]!= '&':\n\t\t\t\tfiltered"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tnew_args = {}\n\tfor k, v in url_args.items():\n\t\tnew_args[k] = re.sub('[%s]*' % k, '%s' % v, v)\n\n\treturn new_args\n"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif  url.isdigit():\n\t\tif len(get_args(url, 'user', 'pass')) > 0 :\n\t\t\treturn [get_args(url, 'user', 'pass')]\n\t\telse:\n\t\t\treturn ['','']\n\tif  url =='movies':\n\t\treturn ['','']\n\telif  url =='personnel':\n\t\treturn ['','']\n\telif  len(url) == 0:\n\t\treturn get_args(url, 'user', 'pass')\n\telse:\n\t\treturn get_"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\turl_dict = {}\n\ttemp_url = re.sub('[\\[]', '', url)\n\tfor arg in url_args:\n\t\tif temp_url.startswith(arg[1]):\n\t\t\ttemp_args = re.findall(r'(//.+)', temp_url)\n\t\t\tif temp_args:\n\t\t\t\ttemp_url = url.replace(temp_url.index(arg[1]), \"{%s}\" % temp_args[0])\n\t\t\telse:\n\t\t\t\ttemp_url = url.replace(temp_url.index("}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t# Replace any URLs in custom string items with their values\t\t#\n\tfor k, v in list(url_args.items()):\n\t\tif isinstance(v, str):\n\t\t\turl_args[k] = v.format(**url_args)\n\t# Return the replaced value\t\t#\n\treturn url_args\t\t\n\n\t\n# --------------------------------------------------------------------------\n# Functions are defined here to allow for proper import of classes and variables\n# to import libraries (e.g. tk)\n# --------------------------------------------------------------------------\t\ntry:\n\tfrom. import Text\n\tfrom.Text import *"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn val == \"None\"\n\n\ndef is_true(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a boolean value.\t\"\"\"\n\treturn not is_none_string(str(val))\n\n\ndef bool_to_str(val: any) -> str:\t\"\"\"\n\tConvert a boolean value to a string value.\n\t\"\"\"\n\treturn \"True\" if is_true(val) else \"False\"\n\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn str(type(val)) in ('NoneType', 'FalseType', 'TrueType','str', 'uuid') or isnewchar(str(val))\n\ndef set_class_type(class_: Type) -> NoneType:\n\t\"\"\" Set the type from which the subtypes are found and stored. \"\"\"\n\tif not hasattr(class_, '_sub_types'):\n\t\tclass_._sub_types = {}\n\tclass_._sub_types[class_] = {}\n\t\ndef get_type(typ: Type) -> Union[Type"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn val == \".\"\n\n\ndef parse_number(val: any) -> float:\n\ttry:\n\t\treturn float(val)\n\texcept ValueError:\n\t\treturn None\n\n\ndef parse_nested_list(val: list[any]) -> List[any]:\n\tret = []\n\n\tfor elem in val:\n\t\tret.append(parse_number(elem))\n\n\tif len(ret):\n\t\treturn ret\n\telse:\n\t\treturn ret\n\n\ndef parse_nested_dict(val: dict[any]) -> dict[any]:\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn val == None\n\n\ndef is_bool(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a boolean value.\t\"\"\"\n\treturn val.lower().startswith('t') or is_none_string(val)\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn type(val) is str and True in str(val.lower()).replace('-', '').strip().split() and str(val).strip() not in C_S\n\n\n#####################################################################################################################\n\n_BEGIN = '\\n    |\\n'.join([\n    '|',\n    '| ', # 0 - 8\n    '|',\n    '| ', # 9 - 15\n    '| ', # 16 - 19\n    '| ', # 20 - 23\n    '| ', # 24 - 27\n    '| ', # 28 - 31\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn str(val) == 'None'\n\n\ndef is_true(val: any) -> bool:\t\"\"\"\n\tCheck if a number represent a True value.\t\"\"\"\n\treturn bool(val)\n\n\ndef is_bool(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represent a boolean value.\n\t\"\"\"\n\treturn str(val).lower() == 'true'\n\n\ndef is_float(val: any) -> bool:\n\t\"\"\"\n\tCheck if a number represent a float value.\n\t\"\"\"\n\ttry:\n\t\treturn is"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn not val or val == \"None\"\t\n\ndef is_number(val: any) -> bool:\t\"\"\"\n\tCheck if a string contains a numeric value.  \"\"\"\n\treturn not is_none_string(val) and isinstance(val, (int, float, complex))\t\n\ndef is_nonzero_float(val: any) -> bool:\t\n\t\"\"\"\n\tCheck if a string contains a valid non-zero float value.  \"\"\"\t\n\ttry:\t\n\t\tval = float(val)\t\n\texcept ValueError:\t\n\t\treturn"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn \"None\" == str(val)\n\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn type(val) is str and val.strip() and val!= 'None'\n\n\ndef _is_int_or_float(val: any) -> bool:\t\"\"\"\n\tCheck if a value is an integer or a float.\t\"\"\"\n\t_type = type(val)\n\tif _type is str and is_none_string(val): return False\n\tif _type is float and is_none_string(val): return False\n\treturn _type.__name__ == 'int' or _type.__name__ == 'float'\n\ndef is_"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn type(val) == type(\"\")\n\n\ndef is_str_list(val: list,\n\t) -> list[any]:\n\t\"\"\"\n\tCheck if a list is a string-object.\t\n\t\"\"\"\n\tif len(val):\n\t\tres = [is_none_string(element) for element in val]\n\t\tif not all(element is not None for element in res):\n\t\t\traise ValueError(f\"{val} should contain only None or strings.\")\n\treturn res\n\n\ndef is_nonzero_iterable_of_integers(val"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\toptions = parser.flags.as_dict(all=True) # convert all flag/help strings to dicts\n\trepr_flags = ''\n\tfor section, flags in sorted(options.items()):\n\t\tif len(flags.keys()) > 0:\n\t\t\trepr_flags += f\"{section}={str(flags).replace('_','')}\\n\"\n\treturn repr_flags\n\n\t\ndef format_timestamp(ts):\n\t\"\"\"\tFormats a local time-stamp as an RFC 3339 timestamp string.\t\"\"\"\n\treturn '"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = []\n\tfor member in parser._get_optional_actions():\n\t\tflags.append( member.option_strings )\n\treturn''.join(flags)\n\nclass ArgParser:\n\n\t__instance = None\n\n\tdef constructor(self,*args):\n\t\t\"\"\"\n\t\tClass constructor with required arguments\n\t\t\"\"\"\n\t\tif ArgParser.__instance is not None:\n\t\t\traise TypeError('Class instance has been initialized already.')\n\t\t\n\t\tArgParser.__instance = self\n\t\tself.__args = args\n\n\tdef __str__(self):\n\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tif parser is None:\n\t\t# Nothing to do, return None\n\t\treturn None\n\n\t# Add the flag and its value from the flags instance variable.\n\tvalue = parser.flags.get()\n\tif not value.is_flag:\n\t\t# Flag not defined in flags, return None\n\t\treturn None\n\n\t# Add the flag value from the arguments.\n\tvalues = parser.parse_args()\n\n\t# Filter out a set of problematic flags with their value set to 'True'.\n\tif values.is_flag == 1 and values.boolean_value == True:\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tlines = [x.strip() for x in parser._actions[1].option_strings.split(',') if x.strip()!= '---']\n\treturn''.join(lines)\n\ndef add_flags(parser):\t\t# use _action for its options\n    for flag in parser._actions[1].option_strings.split(','):\n        # TODO: should these be added by themselves\n\t\tparser._actions[0].option_strings.add(flag)\n        #print('  ',flag)\n\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = parser.flags.as_header()\n\treturn'' +''.join([option for option, short, flag in parser.flag_values]) +''.join(flags) \n\n\n\t\n\t\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags_string = \" \".join([ \" -\" + x for x in sorted(getattr(parser._actions, \"flags\", []))])\n\treturn flags_string\n\n@numba.jit(nopython=True)\ndef _parse_parser_args(args):\n\t\"\"\"\n\t\tGiven an argparse.ArgumentParser instance, return its arguments.\n\n\t\tInputs:\n\n\t\targs\tthe passed in arguments for argparse\n\t\"\"\"\n\t# Create a parser\n\tp = argparse.ArgumentParser(add_help=False)\n\t# Now"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tres = \"\"\n\tfor flag in parser._flag_string_actions_:\n\t\tif isinstance(flag, Arg):\n\t\t\tres += flag.value\n\t\telif isinstance(flag, Flag):\n\t\t\tres += '{}'.format(flag.value)\n\t\telse:\n\t\t\tres += '{})'.format(flag.name)\n\treturn res\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tout =''\n\tfor flag in parser.flags :  \n\t\tout += flag+': '+parser.flag_values[flag][0]+' '\n\tfor key, val in parser.flag_values.items() :\n\t\tout+='--'+key+': '+val[0]+' : '+val[1]+' '\n\treturn out\n\n\ndef define_parser(arguments, arguments_def) :\t\"\"\"\n\tGiven an argument list and flag definitions, returns a\n\tparser instance, passing in to the `arguments` argument.\t\"\"\"\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \"\\n\".join([flag.help for flag in vars(parser.flags).values() if flag.short_name!= \"callback_exception\"])\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\targs = parser.parse_args()\n\treturn args.flags\n\n\ndef check_argtype(arg, argtypes):\n\t\"\"\"\n\tIf arg.type is not a valid option type, raise a TypeError.  Valid options\n\tare booleans, integers, and strings.  The rest of the function checks\n\tthat the provided type is valid option type.  If arg.type is string, it\n\tmust be valid option type, because it can't convert the string value\n\tinto an int/float before it is converted into bool.  There is no way of\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\tdef __was_processed(pathname, dir_path_list):\n\t\t\"\"\"\n\t\tCheck if a pathname is already in the process of processing - but\n\t\tdon't do that if the name has already been put in the set.\n\t\t\"\"\"\n\t\t\n\t\tif processed.get(pathname) is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\tprocessed.add(pathname)\n\t\t\tfound = False\n\t\t\tfor dir_path in dir_path_list: # Process all contained subpaths!\n\t\t\t\tfound = __was_processed(path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tpath_name_str = str(path_name)\n\ttry:\n\t\tprocessed[path_name_str].append(processed.pop(path_name_str[len(path_name_str)-1]))\n\t\treturn True\n\texcept KeyError:\n\t\tprocessed[path_name_str] = [[path_name_str]]\n\t\treturn False\n\t\t\ndef path_name_is_relative(path_name, directory_path):\n\t\"\"\"\n\tCheck if a directory or file name has been processed since\n\ta previous one.  Returns"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t# if it's already been processed, return True\n\tif processed.get(path_name) is not None:\n\t\treturn True\n\t# if it's NOT a directory or file, return False\n\telif not pathlib.Path(path_name).exists():\n\t\treturn False\n\telif verbose is True:\n\t\tprint(\"{} has already been processed.\".format(path_name))\n\t# expand the relative pathname to an absolute path\n\telse:\n\t\tprocessed[path_name] = processed.get(processed.get(path_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif os.path.exists(path_name):\n\t\t# We found a file or a directory. We try and iterate over all items\n\t\t# in the set to see if a folder exists and add those to the set.\n\t\tfor entry in os.scandir(path_name):\n\t\t\tif entry.is_dir():\n\t\t\t\tassert not was_processed(processed, entry.path, verbose), \\\n\t\t\t\t\"Error in was_processed(%s, %s, %s)\" \\\n\t\t\t\t% (entry.path, entry_name, verbose"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif os.path.isdir(os.path.join(path_name, file_name)):\n\t\treturn True\n\n\tprocessed.add(path_name)\n\n\ttry:\n\t\tshutil.rmtree(os.path.join(path_name, file_name))\n\texcept OSError: # Ignore broken symbolic links\n\t\tpass\n\n\tif verbose:\n\t\tprint(f\"[{file_name}] recursed to, removing\")\n\n\ttry:\n\t\tshutil.rmtree(path_name)\n\texcept OSEr"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\treturn path_name in processed or path_name in processed_abspaths(path_name)\n\n\ndef unquote(s):\n\t\"\"\"\n\tRemoves the quotes in a file.\n\tArgs:\n\t\ts: String to be processed\n\tReturns:\n\t\tThe unquoted string\n\tRaises:\n\t\tAssertionError: If the string couldn't be processed\n\t\"\"\"\n\tassert type(s) == type(u'')\n\tif type(s) == type(()):\n\t\ts = s[0]\n\t\tfor i in range(len"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif verbose:\n\t\tprint('\\n...')\n\tcurr = set()\n\tif path_name not in processed and path_name.startswith('\\\\\\\\'): # not a directory path...\n\t\tcurr.add(path_name)\n\telse:\n\t\tres = True if path_name in dirct_names else False\n\t\tif curr:\n\t\t\tfor i in curr:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint(i)\n\t\t\t\tif isinstance(i, os.PathLike):\n\t\t\t\t\tres = was_processed(curr,"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\t# Check the path_name:\n\tif not(os.path.isdir(path_name)):\n\t\tpath_name = os.path.dirname(path_name)\n\tpaths_in_set = 0\n\tfor filename in os.listdir(path_name):\n\t\tif os.path.isfile(os.path.join(path_name, filename)):\n\t\t\tpaths_in_set += 1\n\t\t\tif verbose: print(Fore.YELLOW + Back.BLUE + 'Processed'+ filename + Fore."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\t#Check if the entry already in the set\n\tif pathname in processed:\n\t\treturn True\n\n\t#Add the entry to the set and test it again\n\tprocessed.add(pathname)\n\tif path_name.split(os.path.sep).__eq__(pathname.split(os.path.sep)) or path_name.upper().startswith('.'):\n\t\tpath_name += '*'\n\tif path_name.split('.').__eq__(processed.__contains__(path_name)):"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif not isinstance(processed, set):\n\t\tprocessed = set([processed])\n\ttest = (path_name in processed)\n\treturn test\n\n\ndef check_processing(processed, path_name, verbose):\n\t\"\"\"Check if a file or directory has already been processed.\n\n\tIf the entry is already present, do nothing. If not, check for\n\ta missing entry is in the processed set. If it is present, move\n\tto a directory and process the file. If the process did not\n\tprocess the file then report the issue.\n\n\tIf"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tif(len(vertices)!= 3):\n\t\traise Exception('Vertices should be as [A,B,C]')\n\n        #convert 3-tuple to a list.\n\treturn vertices\n\ndef vertexcount(vertices):\n\t\"\"\"\n\tcount the # of vertices in the polygon.\n\tthe number of vertices should be three in total.\n\t\"\"\"\n\tcount = 0\n\tfor index in range(len(vertices)):\n\t\tcount =count+1\n\n\treturn count\n\n\n#test function\ndef area(polygon):"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tv = []\n\tfor vertice in vertices:\n\t\tfor vertex in vertice:\n\t\t\tv.append(vertex)\n\treturn tuple(v)\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\timport numpy as np\n\timport math\n\timport sys\n\tfrom OpenGL.GL import *\n\tfrom math3d.polygons import Polygon \n\tcoords = np.array(vertices, type = np.float32)\n\t\n\t#create 3d point \n\t#vertices = coords#np.array([[ 1.0, 1.0, 0.0], [ 1.0, 0.0, 1.0], [ 0.0, 0.0, 1.0] ])\n\t#vertex = vertices["}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tdef create_vertices(point_list):\n\t\t\"\"\"Creates 3 points for points in vertex_list\"\"\"\n\t\tx1,y1=[point_list[0][0],point_list[0][1]]\n\t\tx2,y2=[point_list[1][0],point_list[1][1]]\n\t\tx3,y3=[point_list[2][0],point_list[2][1]]\n\n\t\tmy_list = [ [x1,y1], [x1,y1],[x1,y1],[x1,y"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tpts = [(vertices[v[0]] + vertices[v[1]]) / 2 for v in zip(vertices[:-1], vertices[1:])]\n\treturn pts\n\ndef main():\n\tfrom random import random\n\tfrom time import timeit\n\n\t#print(\"Running time on 2x polygon\")\n\tsolution = solution2()\n\t#print(solution)\n\tprint(\"Solution: {}\\nExecution time: {}s\".format(\n\t\tsolution, timeit(\n\t\t\tcode=solution,\n\t\t\tglob"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tn = len(vertices)\n\t\n\t#initialize output dictionary with default values (we use 0 in both sides of the vertex)\n\toutput = {\"v\":[0,0,0],\n\t\t\t\t\t\"e\" :{ \"v\":[0,0,0],\n\t\t\t\t\t\t\t\t\"e\":[0,0,0],\n\t\t\t\t\"f\":[0,0,0]}}\n\t#iterate over vertices and compute area\n\tfor v in vertices:\n\t\toutput[\"v\"].append(v[0]*(1-v[2]))\n\t\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\timport sys\n\timport math\n\timport os\n\timport numpy as np\n\timport numpy.linalg as line\n\n\tpointList = []\n\tpointNumber = float(len(vertices))\n\tfor vertex in vertices:\n\t\t# for xi in range(-60, 61):\n\t\txi1, xi2 = map(lambda x: x * pointNumber/4 + 60, list(range(-60, 61)))\n\t\tfor j in range(-60, 61):\n\t\t\tfor i in range(2):\n\t\t\t\tfor k in range(2):\n"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tpoints=[]\n\tfor vertex in vertices:\n\t\tpoints+=list(vertex)\n\treturn points\ndef vertex2tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\npolygon with vertices ABCD\nWill return\nA    D    B    E\nD    A    C    B\nB    D    C    E\n#C    B    A    D\n\t\"\"\"\n\t\tpass\ndef polygon_intersection(poly1, poly2"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t# create a new array by using \"zip\" method\n\ta = [ (vert[0], vert[1], vert[2]) for vert in verts ]\n\treturn tuple(a)\n\ndef rotatePolygon(poly, angle, point1 = None, point2 = None):\n\t\"\"\"\n\tReturns a rotated polygon. If point1 is None then the first two points (the two sides of the polygon) are used. If point2 is None then it is also used.\n\t\nPolygon is such that each [p1,p2,p3,...] is"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\tdef vertex(point):\n\t\t\"\"\"return vertex (vertex 2-tuples) for vertices\"\"\"\n\t\t\n\t\tif points[point] == None:\n\t\t\treturn []\n\t\telse:\n\t\t\treturn [point,points[point][0], points[point][1]]\n\t\t\t\n\tpoints = {}\n\tfor v in vertices:\n\t\tfor i in range(3):\n\t\t\tpoints[v[i]] = None\n\t\tpoints[v[i+1]] = tuple([v[i+2],v[i+1]])\n\t\t\n\t\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t# TODO: write your solution here\n\tdigits_before_padding = padding + 1\n\tret_str = ''\n\tif padding is not None and len(message) > digits_before_padding:\n\t\tret_str = message[padding:]\n\t\tmessage = message[:padding]\n\n\tstr_index = len(alphabet)\n\ti = 0\n\tj = 0\n\tfor c in message:\n\t\tj += len(alphabet)\n\t\tif c == alphabet[0]:\n\t\t\tret_str += alphabet[j % str_index]\n\t\t\tj"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\ts = []\n\tp = 0\n\twhile(number > 0):\n\t\tremainder = int_to_string(number % len(alphabet), 1, padding)\n\t\tfor i in range(len(alphabet) - 1, 0, -1):\n\t\t\tif remainder == alphabet[i]:\n\t\t\t\ts += alphabet[0:i+1] + alphabet[0:i]\n\t\t\t\tbreak\n\t\tnumber = int(number / len(alphabet))\n\t\tp += 1\n\treturn ''.join(s)"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t# If no digits, nothing is returned\n\tif not alphabet:\t\n\t\t# Use the int as the value for the symbol\n\t\treturn str(number % 100)\n\n\t# If there are only digits in the number and a padding, return the number\n\tif len(alphabet) == 1 and padding:\t\n\t\treturn alphabet[0] + int_to_string(number % 1000, ['Z'], padding).rstrip()\n\n\t# In case of multiple digits, create the string\n\tresult = ''\n\t# Go over each digit, using the int from 100 to the symbol"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tresult: str = \"\"\t\n\tif number == 0:\t# EOF\n\n\t\treturn result\t\n\n\twhile number:\t# number has no digits \n\t\n\t\tfirst: str = ''\t\n\t\t# result = result + alphabet[number % len(alphabet)]\n\n\t\tfor i in range(len(alphabet)):\t\n\n\t\t\t# 0. Check if that digit is available, and if not, add a space.\n\t\t\tif number % 10 < len(alphabet):\t\n\t\t\t\tfirst += alphabet[number % len(alphabet)]\n\t\t\t\tnumber //= 10\t\n\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\treturn alphabet[(alphabet.index(str(number)) + padding) % len(alphabet)]\n\t\n\t\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t# Let's make a list of the digits that follow the integer\t\n\t# (inclusive):\n\tdigits_in_list = list(str(number))\n\t# Check to see if number has leading zero, adding up digits_in_list to\n\t# the first non-leading digit\n\tif \"0\" in digits_in_list[1:]:\t\n\t\tdigits_in_list[0]   += digits_in_list[0]+num_to_string.int_to_string(\"0\",alphabet,padding)[0]\t\n\t\t#"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif padding == None and number >= 10:\n\t\tpaddedString = \"\".join(alphabet)\n\t\treturn paddedString\n\n\tif padding == 0:\n\t\tpaddedString = \"\"\n\t\n\telse:\n\t\tif padding < 0:\n\t\t\tpadding = -padding\n\t\t\tpaddedString = \"\".join([alphabet[-i] for i in reversed(range(len(alphabet)+padding, 1000))])\n\n\t\telse:\n\t\t\tpaddedString = \"\".join(alphabet[i] for i in range(0, len(alphabet)+padding))\n\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t# Convert the number itself into an ASCII string.\n\t# The padding should be used as a convenience.\n\n\t# Make an empty string, with an extra 0.\n\toutput = ''\n\tfor c in range(str(number).lstrip('0')+'0'):\n\t\toutput += '0' + ''.join(alphabet[c : c+padding]) +''\n\treturn output\n\ndef int_to_string_with_padding(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif not alphabet:\t# if alphabet is empty, it should raise an error\n\t\traise ValueError(f\"alphabet is empty, please specify at least one alphabet.\")\n\tassert number < 1_000\n\n\tif padding is None:\n\t\tpadding = 10\n\n\tsign = (str(number)[0]).zfill(padding)\n\tdigits = ''.join(alphabet[int(n) - 1] for n in sign)\n\n\tstring = digits\n\tfor i in range(0, len(digits), padding - 1):\n\t\tstring += '0'\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\ts = list(alphabet)\n\ts[0] =''\n\tsign ='' if padding == 0 else '='\n\tif number < -1:\n\t\tnumber*=-1\n\n\tif number < 0:\n\t\ts.insert(0,'-')\n\telif number >= 0:\n\t\tx = abs(number)\n\t\tx=x%len(alphabet)\n\t\tif x < 0:\n\t\t\tx*=-1\n\t\ts.insert(-1,'+')\n\t\treturn sign + ''.join(s)\n\telse:\n\t\treturn sign\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\ttry:\n\t\tassert len(flow_params) == 1\n\t\tregister_number = str(flow_params.get(\"register_number\"))\n\t\tregister_value = int(register_value) # Convert string to integer\n\t\tregister_data = [register_number, register_value]\n\n\t\tif register_number in flow_params:\n\t\t\tflow_params[register_number] = register_data\n\n\texcept AssertionError as e:\n\t\traise Exception(\"Error during flow replacing. Please report error:\\n{0}\".format(e))\n\n\treturn flow_params"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tdict_data = {\n\t\t'targetflow-list': []\n\t}\n\tfor key, value in flow_params.items():\t# iterating value\n\t\tdict_data[key] = register_value[register_number][key]\t\t\t# replacing value of flow by register number\n\treturn dict_data\ndef _translate_register_flow_to_data(flow_params):\n\t\"\"\"\n\tConvert 'targetflow-list' flow dictionary to list of string with register numbers\n\t:param flow_params: Dictionary containing defined flows\n\t:return: Array of"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tglobal flt_registers\n\tfor item in flt_registers:\n\t\tif item == register_number:\n\t\t\tflt_registers[item][0] = register_value\n\t\t\tbreak\n\ndef get_flt_registers():\n\tglobal flt_registers\n\treturn flt_registers\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor key, value in flow_params.items():\n\t\tif re.match(r\"^({})\\s*=\\s*'.*?'$\".format(key), value):\n\t\t\tflow_params[key] = \"register_value = {}\".format(\"\\\\\" + register_number + \"\\\\\" + value)\n\n@app.context_processor\ndef inject_templates():\n\t\"\"\"\n\tReturns the Injective template context as a dictionary\n\t\"\"\"\n\treturn {\"templates_path\": base_path + \"templates\"}\n\t\n@app."}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tassert 'flows' in flow_params, Exception(\"'flows' in flow_params is missing: %r.\" % flow_params)\n\tnum_flows = len(flow_params['flows'])\n\tassert num_flows > 0, Exception(\"%d flows have not been defined! %r.\" % (num_flows, flow_params))\n\n\tif register_number not in flow_params['flows']:\n\t\tflow_params['flows'][register_number] = register_value\n\t\treturn flow_params\n\ndef update_register(flow_params, register_offset"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\tif flow_params['flow_name'] == 'Read':\n\t\t# replace specified key\n\t\treturn {flow_params['flow']: register_value}\n\telif flow_params['flow_name'] == 'Write':\n\t\n\t\t# store new value to register specified key\n\t\treturn {'flow': flow_params['flow'], 'flow_params': flow_params['flow_params'], \n\t\t\t\t'register_number': register_number,'register_value': register_value}\n\telse:\n\t\t# store already defined value \n\t\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor flow_k, flow_v in flow_params.items():\n\t\tif flow_k =='register_number':\n\t\t\tflow = Flow(flow_k)\n\t\t\t# replace register'register_number' with'register_value'\n\t\t\tflow.params['register_number'] = int(register_value)\n\t\t\tregister_number.links.append(flow)\n\n# ---------------------------------------------------------\n# Mux output flow\n# ---------------------------------------------------------\ndef _replace_mux_output_flow(flow_params, mux_input_port, mux_output_port"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\treturn flow_params['register_' + str(register_number)]\n\n\ndef replace_targets(flow_params, flow_id, flow_response):\n\t\"\"\"\n\tReplace targets with new flow responses\n\tThis functions is a copy of'replace_targets_with_flow' function in OpenflowController\n\n\t\"\"\"\n\tif flow_id in flow_params['flows_to_receive']:\n\t\tflow_params['flows_to_receive'][flow_id].setdefault('targets', {})\n\t\tif flow_response['"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tflow_params_replace = copy.deepcopy(flow_params)\n\tflow_params_replace[register_number] = register_value\n\tflow_params_replace[register_number + 1] = register_value\n\tflow_params['Flow']=flow_params['Flow']+flow_params_replace\n\treturn\n\t#flow_params[register_number] = register_value\n\t#flow_params[register_number+1] = register_value\n\t#flow_params['Flow']=flow_params['Flow']+flow_params_replace_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\t# Make copy because not all fields in new dictionary will exist inside original\n\treturn flow_params.copy()[flow_params[register_number]] = register_value\n\n\nclass Flow(object):\n\t\"\"\"\n\tDefinition of flow object with all info, and function as properties.\n\t\"\"\"\n\tdef __init__(self, flow_params, function, flow_name, log, logger=None):\n\t\t\"\"\"\n\t\tConstructor. \n\t\t:param flow_params: Dictionary containing defined flows\n\t\t:param function: Function instance \n\t\t:param flow_name"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tvalue.replace('.', arg)\n\treturn value\n"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tdef remove_dot_value(value):\n\t\tif '.' in value:\n\t\t\treturn value[value.index('.') + 2: ]\n\t\telse:\n\t\t\treturn ''\n\tvalue = remove_dot_value(value)\n\n\tdef replace_numeric(string, start, end):\n\t\t\"\"\"\n\t\tSearchs'str[start:' until the end of the string, returns\n\t\tthe'str' from the result\n\t\t\"\"\"\n\t\tvalue = str()\n\t\twhile start <= end:\n\t\t\tchar = str()\n\t\t\tif string[start] == '{':"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif isinstance(value, (str)):\n\t\tvalue = value.replace('. ', arg)\n\treturn value\n\n########################################################################################################################\n#\n# Main\n#\n########################################################################################################################\n\n\"\"\"\nThis program converts a.com to jpecp and jpegcp\n\"\"\"\n\nimport json\nimport sys\nfrom urlparse import urlparse\nfrom urllib2 import Request, parse_qs, unwrap, build_opener, HTTPCookieProcessor\n\n# Get a list of URL params\nurlParams = []\nfor arg in  sys"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tparts = value.split('.')\n\tfor i, part in enumerate(parts):\n\t\tparts[i] = part.replace('.', arg)\n\treturn '.'.join(parts)\n\ndef get_value(expression):\n\t\"\"\"\n\tGiven a string expression, return the parsed expression (a dict).\n\n\tAn expression may look like follows:\n\t\tstring:arg.replace('.', '_')\n\twhich returns the value '_'.\n\texpression:sub-expression string\n\t\"\"\"\n\tif not isinstance(expression, str):\n\t\tlog"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tstr1 = \"\"\n\tregex =  r'(\\.[0-9]+)'\n\tregex1 =  r'\\.\\d+'\n\tnew_args = re.sub(regex, regex1, value)\n\tnew_args = new_args.replace('.0', arg)\n\treturn new_args\n\n\ndef checkElemCtor(e):\n\t\t\"\"\"\n\t\tThis function checks the elements in elements dictionary \n\t\tto see if a new element has been constructed or not.\n\t\t@return is a boolean that specifies if a new element that"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tpunctuation = '.-_'\n\tfor character in value:\n\t\tis_punctuation = False\n\t\tif character in punctuation:\n\t\t\tis_punctuation = True\n\t\tvalue = value.replace(character, arg, 1)\n\t\tif not is_punctuation:\n\t\t\tvalue = value.replace(character, arg)\n\treturn value\n\t\ndef split_value(value):\n\t\"\"\"\n\tSplits a given string value into list of strings and their arguments\n\t\tbased on special characters\n\t\"\"\"\n\tvalues=[]\t\t# empty array"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t#value\t=\tstring\tto\tbe\treplaced  \n\t\t\t\t\t\t\t \n\tnew_value = \"\"\n\t\n\tfor i in range(0, len(value)):\n\t\tif value[i] == '.':\n\t\t\tnew_value = new_value + \".\" + arg[i]\n\t\t\t\t\t\t\n\t\telse:\n\t\t\tnew_value = new_value + value[i]\t\t\t\n\t\t\t\t\t \n\tvalue = new_value\n \n\t\t\t \n\treturn value\n\t\t\t\n \n\n# MAIN \narg=sys.argv[1]"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif \".\" in value:\n\t\tif \"%\" in value:\n\t\t\tstring = str(value).replace(\"%\",arg)\n\t\telse:\n\t\t\tstring = value\n\t\tif \"%\" in string:\n\t\t\tresult = \"\".join(string.split(\"%\"))\n\t\t\treturn result\n\t\telse:\n\t\t\treturn string\n\telse:\n\t\treturn str(value)\ndef replace_spaces(s,spaces):\"\"\"\n\tTakes a string s and a space-separated string with numbers\t\"\"\"\n\treturn s.replace(\"[\", \" \"*spaces)."}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\treturn re.sub('^.+\\.+', arg, value)\n\n#------------------------------------------------------------------------------------------\n#\t\t\t\t\t\t   EXTRACTION\n#------------------------------------------------------------------------------------------\ndef get_data_dir(): \t\t\t\t\t\t\t                                    # get data directory to look for files\n    global dataPath\n    global targetDir\n    if len(dataPath) >= 2:\n        targetDir=dataPath[1].replace(dataPath[0],'')\n        # check directories are in the right way\n        if not os.path.isdir(targetDir):\n            mkdir(targetDir)\n    "}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\treturn value.replace('. ', arg)\n\ndef count(string, sub, group=5):\n\tcount = {word : string.count(word) for word in sub}\n\treturn count\n\ndef sum(string, sub, group=5):\n\tsum = 0\n\tfor word in sub:\n\t\tsum += count(word, sub)\n\treturn sum\n\ndef is_prime(number, minNum):\n\tif number > 1:\n\t\tif number == 2 or number == 3:\n\t\t\treturn True\n\treturn False\n\ndef calculate_prime_"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\treturn [c for c in cls.__subclasses__() if (\n\t\tnot c.__spec__ is None and \n\t\tnot c.__spec__.extends\n\t)]\n\ndef getattr(instance, name, *args, **kwargs): \n\t\"\"\" \n\tGet an attribute of the calling instance, \n\tdefaulting to the first attribute that exists \n\tin the instance's class, and optionally args.\n\t\"\"\"\n\tmro = instance.__class__.__mro__\n\tfor cls in mro:\n\t\tif"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tc = cls\n\tclasses = []\n\tcur_subclasses = []\n\tif not hasattr(c,'_subclasses'):\n\t\tclasses.append(c)\n\telse:\n\t\tcur_subclasses = c._subclasses\n\t\tfor c2 in cur_subclasses:\t\t\n\t\t\tcur_subclasses += subclasses(c2)\n\t\tclasses.extend(cur_subclasses)\n\treturn classes\n\nclass _dict:\n\t\"\"\"\n\tA subclass of dict, that automatically updates __dict__\n\tbased on the class that contains it, based"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tattributes = dict()\n\n\t# Retrieve all classes/subclasses\n\t#\n\t# @param cls\n\t# @return list\n\tdef __subclasses(cls, attributes):\n\t\t# print a docstring\n\t\t# print('... class:'+ str(type(cls)))\n\t\t# print('... sub-class:'+ str(cls.__name__))\n\t\t# print('... attributes:'+ str(attributes))\n\t\t# print('... list of subclasses:'+ cls.__dict__.keys())\n\n\t\t#"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tsubclasses = set()\n\tcurrent = cls\n\tyield current\n\twhile current.__bases__!= (cls,):\n\t\tyield from current.__bases__\n\t\tcurrent = current.__superclass__\n\ndef subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, only\t\"\"\"\n\treturn reversed(list(subclasses(cls)))\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\treturn getattr(cls, '__subclasses__', [])\n\nclass SubClass:\n\t\"\"\"\n\tA \"specializability\" of a class which is automatically defined\n\tat construction time\n\t\"\"\"\n\tpass\n\nclass ClassWithSubClass(SubClass):\n\t\"\"\"A class which is defined inside a class\"\"\"\n\tpass\n\n\nclass ClassWithSubClassSubclass(SubClass):\n\t\"\"\"A class which is defined inside a class and defined in a class\n\tand is defined inside a class\"\"\"\n\tpass\n\n# Note the order of above is not strictly"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tif cls.__abstract__:\n\t\treturn []\n\n\tsubclasses = set()\n\tsubclasses.add(cls)\n\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.add(subclass)\n\t\tsubclasses.update(_subclasses(subclass))\n\t\n\treturn list(subclasses)\n\n# -------------------------------------------------------\t\n#  @staticmethod\n# \t@boundmethod\t\n# -------------------------------------------------------\t\n\nclass abstract(object):\n\t#------------------\n\t# Constructor\n\t#\t@param class_"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tfor type in cls.__subclasses__():\n\t\tyield type\n\t\tfor subclass in subclasses(type):\n\t\t\tyield subclass\n\ndef test(testType, testCaseObj):\n\tprint (\"Testing Type %s\" % testType)\n\tprint (\"Executing test in case: \", testCaseObj)\n\tif testCaseObj.start:\n\t# set up the case\n\t\ttestCaseObj.start = False\n\n\t\tif testType in C_SUBCLEASES:\n\t\t\ttestCaseObj.subcaseList = []\n\t\t\t# test"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tfor subclassOf in cls.__subclasses__():\n\t\t# get all subclasses\n\t\tall_subscriber_subclasses = subclasses(subclassOf)\n\t\tfor subsubClass in all_subscriber_subclasses:\n\t\t\t# pass and check if we are allowed it or not\n\t\t\tif not allow_subclass(subsubClass, allowedSubclasses, True):\n\t\t\t\tyield subsubClass\n\n# -------------------------------------------------------------------------------------------------------------\n#\tMethods\n# -------------------------------------------------------------------------------------------------------------\n\n# get all subclasses of one class \ndef subclasses(cls):\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\treturn [c for c in cls.__subclasses__() if not issubclass(c, cls)]\n\n#\n# @subclasses()\n# def subclasses(cls):\n# \tl = []\n# \tfor n in cls.__subclasses__():\n# \t\tl.append(n)\n# \t\tl.extend(n.subclasses(cls))\n# \treturn l\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\treturn [c for c in cls.__subclasses__() if isinstance(c, subclasses(cls))]\n\ndef is_subclass(cls, class_):\n\t\"\"\"\n\tQuickly check if class_ is a subclass of cls,\n\tusing helper functions, recursively.\n\n\tNote: this is done implicitly in Python-based subclasses, even in pygame\n\n\tTodo: use self-explanatory\n\t\"\"\"\n#\timport inspect\n#\tprint('class = ', class_,'',inspect.getfile(class_"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn sum([x in alphabet for x in string])\n\ndef string_to_int_2(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\tThe letters consist of a sub-sequence of alphabet letters.\t\"\"\"\n\treturn sum([1 if string.startswith(x) else 0 for x in alphabet])\n\nif __name__ == \"__main__\":\n\tpass"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tnumber = 0\n\tfor char in string:\n\t\tif char not in alphabet:\n\t\t\tprint(\"the given number does not correspond to the one provided as input in string_to_int. Nothing has changed\")\n\t\t\tprint(\"The letter is replaced by the number given as input.\")\n\t\t\tcontinue\n\t\tindex = alphabet.index(char)\n\t\tif index + 1 > len(alpha) - 1:\n\t\t\tprint(\"the number given in string_to_int does not correspond to the string of alphabet provided\")\n\t\t\tcontinue\n\t\telse:\n\t\t\tnumber *= len(alpha) * pow"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn sum([i in alphabet for i in string])\n# def string_to_int(string: str) -> int:\t\n# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# https://www.geeksforgeeks.org/python-convert-string-to-integer-numbers/\t\t\t\t\t\t\t\t# https://www.w3schools.com/python/python_strings.asp"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn_val = 0\n\tfor char in string:\n\t\tif char in alphabet:\n\t\t\tchar_position = alphabet.index(char)\n\t\t\treturn_val += char_position\n\treturn return_val\n\t\t\ndef reverse_string(string: str, alphabet: List[str]) -> str:\t\n\t\"\"\"\n\tReverse a string consisting of words from the given alphabet in\n\treverse order. \n\n\t\"\"\"\t\n\tif len(string) == 0:\n\t\treturn \"\"\n\tintstring = int_to_string(string)\n\tif len("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tif string[0] in alphabet:\n\t\tstring = string[1:]\n\tif string[-1] in alphabet:\n\t\tstring = string[:-1]\t\n\tif string[-1] == \"f\":\n\t\tstring = string[:-1]\n\telif string[-1] == \"g\":\n\t\tstring = string[:-1]\n\telif string[-1] == \"h\":\n\t\tstring = string[:-1]\n\telif string[-1] == \"i\":\n\t\tstring = string[:-1]\n\telse:"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tfrom collections import deque\n\n\t# Create a deque that contains each digit of the string\n\tqueries = deque(string[0:len(string)-1])\n\n\t# Start with the string in the queue.\n\tquery_num = int(string[0])\n\n\t# Process each letter of the string\n\tfor idx in range(len(alphabet)):\n\t\tif queries[0] == alphabet[idx]:\n\t\t\tqueries.popleft()\n\t\telse:\n\t\t\tqueries.append(alphabet[idx])\n\t\t\n\t\t# Process"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\ttry:\n\t\tint(string[0:int(max(alphabet, key=len))])\n\texcept:\n\t\treturn 0\n\t\n\telse:\n\t\treturn int(string[0:alpha.index(\"a\")] + string[int(alphabet.index(\"a\")):len(alphabet)])\n\ndef string_to_int_2(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\t\"\"\"\n\ttry:\n\t\tint(string[len(\"\".join"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n\tsignString=\"+-/*0123456789\"\n\tnumber=0\n\tfor char in string:\n\t\tnumber=number*26+alphabet.index(char)#26 + alphabet.index\n\treturn number\n\t\ndef number_to_string(number: int, alphabet: List[str]) -> str:\n\t\"\"\"\n\tAssume alphabet is a string, using this string the given number.\n\t\"\"\"\n\tsignString=\"+-/*0123456789\"\n\tif number>0:\n\t\tletter=int(number/26)\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t# TODO Implement this function: given a string of characters, return a \n\t# corresponding letter position string. For example, string_to_int(\"5\", \"abcdefghijklmnopqrstuvwxyz\") \"23\"\ns=\"a123456789\"\nresult=[]\ndic={}\n#initialize\nfinal=\"\"\ni=0\ncount=0\nwhile(i<len(string)):\n\tif(string[i] not in s):\n\t\tcount+=1\n\telse:\n\n\t\tif(string[i] in d"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn alphabet.index(string[-1])\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\tif not destination_path: destination_path = Path.cwd()\n\t\n\trepo_id = os.path.basename(url)\n\tdest_dir = destination_path / repo_id\n\tdest_dir_archive = dest_dir / \"arch\"\n\t\n\tif not dest_dir_archive.exists():\n\t\tdest_tar_archive = tarfile.TarFile(\n\t\t\t\"archives\" / repo_id, \"w\"\n\t\t)\n\t\tfor pkg in Package.import_all(url, repo_id, destination_"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tpass\n\n\nfrom os import path\nimport tarfile#For getting extension in.tar.gz archive\nfrom tqdm import tqdm#Progress bar for download of file.\n\ndef extract_tar_gz():\n\t\"\"\"Extract each.tar.gz archive in specified directorypath to destination_path. Directory is created by 'create_directory'.\t\n\t\n\tReturns:\n\t\tNone\n\t\"\"\"\n\tdirectory_path = './archive'\n\tdestination_path = './archive-extracted-files'\n\tcreate_directory(directory_path, destination_path"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\timport os, pathlib\t\t\t\t## from local file system\n\timport tarfile\t\t\t## to tar archive\n\timport tempfile, shutil\t\t\t## for copying files\n\tfrom io import StringIO \t## for downloading files (not actually downloading)\n\timport urllib\n\n\ndef download(url, directory, dir=None):\n\t\"\"\"\n        Download a.tar.gz archive and put it in directory.\t\n\n        Args:\n            url: url of the.tar.gz archive to download\n            directory: directory where to put file-extracted archive\n            dir: if specified location"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\turlpath = url.split(os.sep)\n\turlpath = urlpath[1:]\n\n\tdestination_path = destination_path.rstrip('/')\n\tdestination_path = os.path.join(destination_path, *urlpath)\n\ttmpdir = tmpdir or os.path.join(\n\t\tPath.home(),\n\t\t\".mock-archives\",\n\t\t\".tar.gz\")\n\tdirectory(\n\t\ttmpdir,\n\t\t\"extracted\",\n\t\textensions(*urlpath),\n\t)\n\n\n\ttar ="}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tif url.endswith('.tar.gz') & destination_path.endswith('.tar.gz'):\n\t\tprint('Archive already extracted.')\n\t\treturn destination_path\n\tprint('Getting archive...', end=''); os.system('tar zcf '+url+' -C '+destination_path)\n\tprint('Extracting archive...', end=''); os.system('tar zxf '+url+'.'+destination_path)\n\tprint('Clean archive...', end=''); os.system('rm -f"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\treturn extractor.extract(url, destination_path)\n\n\ndef download_distribution(url):\n\textractor = Extractor()\n\tdirectory = extractor.extract(url)\n\textractor.unpack('distrib')\n    \ndef download_repo(url, dest_dir):\n    \n    \"\"\"Downloads.tar.gz archive from the given url with folder 'dest_dir' within 'dest_dir'.\n\n    Args:\n        url: The url of the distro repository\n        dest_dir (str, optional): The directory where to"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ntry:\n\tprint(\"\\n#####################################################\")\n\tprint(\"####                              DESCRIPTION                      ####\")\n\tprint(\"#####################################################\\n\\n\")\n\tprint(\"  This script retrieves the.tar.gz archive which contains the given url and gives you an article with the necessary files to build this distribution.\" )\n\tprint(\"  This script is free. \\n\")\n\tprint(\"      This script is available at http://github.com/jake-marscholdner/fetch-dummy-url...\")\n\tprint(\"      Author: @"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n#\tThe following two lines are for \"python -m tar\"\n\t#\timport os # needed to use a folder\n\t#\tfrom zipfile import is_zipfile # needed to check for a.tar.gz\n\t#\n\t#\twith open(os.path.abspath('./package_list.sql'), mode = 'r', encoding = \"utf-8\") as f:\n\t#\t\tlines = f.readlines()\n\t#\n\t#\tfor line in lines:\n\t#\t\tif line.find(\"<Description>\") >= 0"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef download_archive(url,destination_path,archives,start_epoch = 0):\n\t\"\"\"\n\tDownload a given url to a path in destination_path archive.\n\n\tArgs:\n\t\turl: url of the.tar.gz archive to download\n\t\tdestination_path: the path on disk where the archive should be extracted\n\t\tarchives: a list of.tar.gz archives\n\t\tstart_epoch: the year the start of archives to be downloaded\n\tReturns:\n\t\ta directory Path with the.archive archive extracted to.\n\n\tRaises:\n\t\tRuntime"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tdef __init__ (self, url, destination_path):\n\t\tself.url = url\n\t\tself.destination_path = destination_path\n\n\tdef check_status(self):\n\t\tres = requests.get(self.url + \"desc\")\n\t\tassert res.status_code == 200\t\t\t\"\"\"\n\t\tprint(\"  * Status for \",self.url)\t\"\"\"\n\t\tif res.status_code!= 200:\n\t\t\traise Exception (\"ERROR in {}\".format(self.destination_path))\n\t\tres_txt = res.text\n\t\tfile = open(self"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn os.name == 'posix'\n\nclass FileUtils:\n\t\"\"\"\n\t\tA collection of useful functions for accessing and converting files\n\n\t\t**Supported types**\n\n\t\t:param String name(default '')\t[string]\tName of file\n\t\t:param String mode(default '')\t[string]\tMode of file\n\t\t:param String type(default '')\t[string]\tType of the file ('binary' or 'text')\n\t\t:param String ext(default '')\t[string]\tExtension of the file\n\t\t:param Array file_or_"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t_platform=platform.system()\n\tif _platform=='Darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n## MacOS\ndef _mac_ver_xml(release='',arch='',machine='',processor=''):\n\t\"\"\"\n\t:param release:\n\t:type release:\n\t:param arch:\n\t:type arch:\n\t:param machine:\n\t:type machine:\n\t:param processor:\n\t:type processor:\n\t:return: Returns a dictionary containing mac-release and MAC image\n\t:rtype: dict"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif os.uname()[0] == 'Darwin':\n\t\t\treturn True\n\telse:\n\t\treturn False\n\ndef is_windows():\n\t\"\"\"\n\tChecks if the os is Windows\n\n\t:return: True is a windows system\n:rtype: bool\t\"\"\"\n\tif os.name == \"nt\":\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef is_linux():\n\t\"\"\"\n\tChecks if the os is Linux\n\n\t:return: True is Linux\n:rtype: bool\t\"\"\"\n\tif os.name == \""}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tos.system('/usr/bin/osascript -e'+ \\\n\t\t\t\t\t'if [ \"$2\\\" == \\\"macos\\\" ]; then /usr/bin/open https://www.youtube.com/watch?v=M6MxMxo4-E;' + \\\n\t\t\t\t\t'/usr/bin/open https://www.reddit.com/r/MangaGame/comments/7xv8vn0t5a/the_last_game_for_Manga_Gang_Door_Pt_7_H&amp;' + \\\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\nosName = platform.system()\t\"\"\"\n\tChecks if the OS is macOS.\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\n# Checking if the system is MacOS.\nif osName == \"Darwin\":\n\t\"\"\"\n\t\tThis checks if the system is MacOS.\n\n\t\t:param: system\n\t\t:return: True if system is MacOS.\n\t\t:rtype: bool\n\n\t\t\"\"\"\n\n\tos = platform.system().lower()\t\"\"\"\n\t\tThis checks if the system is MacOS.\n\n\t\t:param: system\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tos_name = platform.system()\t\"\"\"\n\tChecks if the sys.platform is macOS\n\t\"\"\"\n\tif os_name == 'Darwin':\t\"\"\"\n\tMacros (Mac, mac)\n\t\"\"\"\n\t\tos.symlink('/System/Library/Frameworks/Python.framework/Versions/3.7/lib', '/usr/local/lib/python3.7/')\n\tif os_name == 'Darwin' or os_name == 'Linux'.casefold():\t\"\"\"\n\tMacros (Mac, mac)\n\t\"\"\"\n\t\tos"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\nif os.getenv('OSTYPE')=='Darwin':\n\t\"\"\"\n\tChecks if the OS is Darwin.\n\n\t:return: True is Darwin\n\t:rtype: bool\t\"\"\"\n\treturn os.getenv('OSX_ARCHIV')=='x86_64'\n\ndef check_arch():\n\t\"\"\"\n\tChecks if the arch is x86_64.\n\n\t:return: True is x86_64\n\t:rtype: bool\t\"\"\"\n\n\timport platform\n\n\tsystem = str(platform.system())\n\t\n\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif os.name == 'nt' and sys.version_info < (3, 0):\n\t\treturn False\n\n\tif not sys.platform.startswith('linux') and not sys.platform.startswith(('manys', 'linux', 'darwin')):\n\t\treturn False\n\n\tif sys.platform.startswith('darwin'):\n\t\treturn False\n\n\treturn True\n\ndef import_mod(name):\n\tif not importlib.util.find_spec(name):\n\t\traise AttributeError('{!r} object has no attribute'.format"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn os.name == \"posix\"\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn os.platform == \"darwin\"\n\n\n# def is_macos():\n# \t\"\"\"\n# \tChecks if os is macOS\n\n# \t:return: True is OS is macOS\n# \t:rtype: bool\t\n# \t\"\"\"\n# \treturn (has_user_id() and is_macos()) or get_users_count() == 1\n\ndef current_os():\n\t\"\"\"\n\tReturns the OS we's running on\n\t:return: The OS we're running on\n\t:rtype: str\t\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tfor filename in item.keys():\n\t\tregex = set(regex_dict(item[filename]).keys())\n\n\t\tfor key in regex:\n\t\t\titem[filename][key] = \"^\" + key + \"$\"\n\treturn item\n\ndef regex_dict(item):\n\treturn {k:re.compile(v) for k,v in item.items()}\n\ndef parse_item(item, item_class, default=\"\", prefix=\"\"):\n\t\"\"\"Convert key-type and regex keys to an item from dict, given \n\ta template"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tregex_keys = re.base.Sub(re.compile('/(\\w+/[^/]*\\.\\w+)').sub, '', regex_keys)\n\tregex_keys = {}\n\tregex_keys['/' + os.path.basename(item['file']).replace('\\\\', '/')] = item['key']\n\tregex_keys['/'] = regex_keys['/' + os.path.basename(item['file']).replace('\\\\', '/')]\n\treturn regex_keys\ndef get_value"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\"\"\"\n\tCheck the file's name:\n\tRules for regex in the list of items\n\tFor new items: convert them to regexes as described below\n\t\n\tRule:\n\tA '.' (period) matches any character\n\t'\\.' (period) matches any character\n\t'\\{(}' (bracket) matches any character\n\t'\\[' (square brackets) matches any character\n\t'\\,' (comma) matches any character\n\n\t{'|(}', '(]'). matches any character\n\t\n\tValid rules:\n\t    (a) Mat"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t# get file's wildcards\n\twildcard_list = sorted(dir(item['files']))\n\t# convert list to str\n\tif type(wildcard_list) == str:\n\t  wildcard_list = wildcard_list.split(',')\n\t# get file's keys\n\tfile_keys = ['\\\\|'.join(f) for f in wildcard_list]\n\n\t# convert dict to regex\n\tregex_dict = {}\n\tfor wf in file_keys:\n\t  fname = re.search(wf, item['"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tdef regex_dict(obj):\n\t    result = dict()\n\t    for key in obj:\n\t        keys = key.split('.')\n\t        for k in keys:\n\t            if k in string.whitespace:\n\t                continue\n\t\t\t    regex = '^' + k + '$'\n\t            elif k[0] == '\"' or k=='\"':\n\t                regex = k + '\\\\1'\n\t            else:\n\t                regex = '^' + '\\\\' + '*$' + k + '"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tpattern_key = r'[^a-zA-Z_0-9]*(.*?)(?=([A-Z]*))'\t\t\t\t\t\t\t\t\t\t\t\n\tdef regex_dict(rules):\n\t\tresult = {}\n\t\tfor (key, rules_for_keys) in rules.items():\n\t\t\tresult.update(\n\t\t\t\t{\n\t\t\t\t\tkey:\n\t\t\t\t\tconvert_regex(pattern_key, key, rules_for_keys['a'],\n\t\t\t\t\t\trules_for_keys['b'], rules_for_keys['c']\n\t\t\t\t\t)\n\t\t\t\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tkeys = []\n\tfor key,value in item.items():\n\t\t# regex_item = '{}-{}-{}'.format(key,value['a'],value['b']) # For Python 2.4.x\n\t\t# keys.append(regex_item)\n\t\t# regex_dict = {':'.join(regex_item.split(':')[:-1]) : value}\n\t\tkval = re.sub(re.escape(key),regex_entry(key),key)\n\t\tkeys.append( kval )\n\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\treturn {filename:re.compile(regex_keys[filename]) for filename in item.keys()}\n\n\ndef file(dct, filename, options=[''], flags=[],\n          files_to_ignore_for=[], ignore_mode=''):\n\t\"\"\"\n\tGiven a dict where filenames have to be converted, convert each value\n\tinto a file with the given filename in it by adding the given options and flags.\n\tThe output is also filtered out if --ignore_files_to_ignore_dir=true [default is ''' not in dct,"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\nregex = {}\nregex['.'] = {}\nregex['.'] = regex['.']#{'a': 'arf','b': 'bark',....}\t\nregex['a'] = r'arf'\nregex['b'] = r'bark'\nregex['c'] = r'coo'\t\n#['a', 'b', 'c']\nregex['e'] = r'help'\t#{'h': 'help'}\nregex['f'] = r'f'\t\t#'b'\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t# convert all keys to regexes and leave only equivalent ones\n    temp_dict = {}\n    for key, value in item.items():\n        temp_dict[key] = {}\n        for pattern, mapping in value.items():\n            temp_dict[key][pattern] = {pattern: map(mapping, rule) for pattern, mapping in mapping.items()}\n    return temp_dict\n\n\ndef regex_dict(rules):\n\t\"\"\"Convert a dict to a set of regex keys.\n\n\tIf each keys and their values are of different type, it becomes one\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\ndef delete_quotes(s):\n\t\"\"\"\n\tDeleting quote chars from given text.\n\n\t@param s source text to be deleted\n\t@returns return deleted chars\n\t\"\"\"\n\timport string\n\treturn re.sub(string.punctuation, '', s, 0, re.MULTILINE, flags=re.UNICODE)\n\t\nif __name__ == \"__main__\":\n\timport doctest\t\t\n\tdoctest.testmod(verbose=1)\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n\t\n    @property\n    def name_split(self):\n        self.remove_backslash()\n        self.remove_double_dash()\n        if self.name[0]==\"_\" and self.name[-1]==\"_\":\n            return (self.name[1:-1],)\n        else:\n            return ()\n          \n    @staticmethod\n    def remove_backslash(name):\n        r\"\"\"\n        Remove _, or #, from name.\n        \n        \"\"\"\n        \n        \n        \"\"\"\n        "}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tif(isinstance(name, str) == False ): return False\n\tif(isinstance(name, str) == True ): return re.sub( r\"[^A-Za-z0-9_]+\", \" \", name ).strip()\n\n\treturn False\n\nclass Credentials(object):\n\t\"\"\"\tMain class for parsing a credential file.\t\"\"\"\n\n\t## Default values.\t\"\"\"\t\n\tdomain = \"\"\n\tusername = \"\"\n\tpassword = \"\"\n\n\tfileName = \"\"\n\tpasswordFile = \"\"\n\n\tcredentType = 0\n\tis"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tpattern = r'([^\"\\s]+)'\n\tmatch = re.search(pattern, str(name), re.IGNORECASE)\n\tif match:\n\t\treturn match.group(1)\n\telse:\n\t\treturn name\ndef renameID(name, id,newid):\n\tpattern = '[^a-zA-Z0-9]+'\n\tmatch = re.search(pattern, name)\n\tif match:\n\t\treturn newid if newid.isnumeric() else 'id({}){}'.format(newid,id"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef replace_quote(s, start, end, s_tok): return s[:start].replace('\"' + s_tok + '\"', s[start:start2])\n\treturn replace_quote\n\ndef remove_punctuation(sent): return ''.join([w.replace('%','_') for w in sent.split() if not w.isalpha()])\n\n# def get_first_sentence_with_punc(sent): return''.join([token for token in sent.split() if not token.isalpha() and token"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef removeQuotationFromName(n):\n\t\tif type(n) == str:\n\t\t\twhile \"\\\"\" in n:\n\t\t\t\tn = n.replace(\"\\\"\", \"\")\n\t\t\twhile \"\\\"\" in n:\n\t\t\t\tn = n.replace(\"\\\"\", \"\")\n\t\t\twhile \"<\" in n:\n\t\t\t\tn = n.replace(\"<\", \"\")\n\t\t\twhile \">\" in n:\n\t\t\t\tn = n.replace(\">\", \"\")\n\t\t\twhile \"\\\\\" in n:\n\t\t\t\tn = n.replace(\"\\\\\", \"\")\n\t\t\twhile \"=[\""}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef remove_quotes(s):\n\t\t\"\"\"\n\t\tRemoves quotes from string.\t\"\"\"\n\t\ts = re.sub(\"\\\"\",\"\",s)\t\"\"\"\n\t\ts = re.sub(\"'\",\"\",s)\t\"\"\"\n\t\ts = re.sub(\"&\",\"\",s)\t\"\"\"\n\t\treturn s\n\n\t# Create a generator function to return the names that pass the given filter,\n\t# for the full dataframe.\t\"\"\"\n\tdef names_not_like_filter(df,filt):\n\t\t\"\"\"\n\t\tFiltering for strings, if a list of"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef removeQuote(char):\n\t\treturn char.group(1)\n\n\t\"\"\"\n\tAdd a string to the given string.\t\"\"\"\n\tdef plus(string, string2):\n\t\tif(len(string2) > 0):\n\t\t\tstringL = string.split(\"|\")\n\t\t\tstringL2 = string2.split(\"|\")\n\n\t\t\tif(len(stringL) < len(stringL2)):\n\t\t\t\tstringL[-1] = stringL2[-1]\n\t\t\telse:\n\t\t\t\tstringL2[-1] += \"|\" + string"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\ttry:\n\t\treturn name.replace(\"'s\", '').replace(\":\", '').replace('\\'s', '').replace('\"','')\n\texcept:\n\t\treturn''\n\n\ndef get_typeName(typeClassname):\n\t\"\"\"\n\tGet the name of the particular module and class.\n\tOnly for Python 2.6, but the one you need to\n\tlook at for Python 3.\t\"\"\"\n\t# print repr(typeClassname)\n\ttry:\n\t\treturn importModule(typeClassname).classes.__dict__[typeClassname]."}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tif name.find(\"\\\"\") == -1:\n\t    return remove_quote(name)\n\treturn name\n\n# def extract_domain():\t\"\"\"\n#\t\tReturn an appropriate domain for the given given string.\t\"\"\"\n#\tpattern = r\"(?:www.)[v]+([a-zA-Z]+\\.)?(\\w)+[\\w-]*\"\n#\treturn re.sub(\"\\?|\\*\", \"\", pattern)\n\n# def replace_name(name):\t\n#\t\"\"\"\n#\tReturn a version of a name which"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\ndef shell_split(str_arg):\n\t\"\"\"\n\tSplits an argument for the shell.\n\n\tstr_arg: shell argument\n\tRETURNS a list of arguments separated by commands as shippled\n\tex: \"echo 10\" => [\"echo\", \"10\"],...\n\t\"\"\"\n\tcommand_string = sub(r\"{}$|{}[^}]+$|\".format(SHELL, re.escape(str_arg)), \"\", str_arg, '\\n')\n\t\nclass RunCommandInterface:\n\t\"\"\"\n\tInterface for Command"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\ndef _shell_quote(s):\n\t# type: (str) -> str\n\t\"\"\"\n\tQuote string with quotation mark. For use with shell shell-escape.\n\t\n\tThis function escapes a single string, so the shell can interpret it;\n\ta non-quoted string is returned.  Note that as an argument, it quotes\n\teverything of type <string> and <string> and everything else.\n\t\"\"\"\n\ts = s.strip().replace(\"\\n\", \"\\\\n\")\t# remove escape characters\n\ts = s.replace('\"', '"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tdef split_cmd(arg):\t\"\"\"\n\t\tSplit command into list of arguments and command text.\n\t\t\n\t\tUse subprocess.call(cmd, shell=True)\n\t\tto run a command into a subprocess on the system.\n\t\t\n\t\tUse shlex.split(arg) to split string into a list of arguments\n\t\tand a list of strings of command text.\n\t\t\n\t\tcmd:\tcommand for subprocess\n\t\targ:\targ to pass to subprocess.call()\n\t\toutp:\toutput from subprocess.Popen(cmd, bufsize=1)"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t# Define some regex match patterns as described from docs.\n\tregex =  re.compile('|'.join(\n\t\t[r'(.*?)\\\\', r'(\\w+)-(-[\\d-\\.]+)$', r'(.*?)=0-([\\d+\\-\\.,+\\.]+)$', r'(\\w+)\\\\', r'(([\\w-]*?)(\\s+\\(\\s*[^][\\\\;]+\\))*[\\r\\n])'],\n\t\tre.IGNORECASE | re.M"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tif len(argv)!= 0 or env:\n\t\timport re\t\"\"\"\n\t\tMulti-platform variant of shlex.split() for command-line splitting.\n\t\tFor use with argparse, for argv injection etc.\n\n\t\tplatform: 'this' = auto from current platform;\n\t\t          1 = POSIX;\n\t\t          0 = Windows/CMD\n\t\t          (other values reserved)\t\"\"\"\n\t\timport shlex, locale\n\t\tif env[1]:\n\t\t\tenv[0]=env[1]\n\t\tif env[0] in (0, 1):\n\t\t\t"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tif sys.platform.lower().startswith('win'):\n\t\tplatform = 'this'\n\telse:\n\t\tif sys.platform.lower().startswith('darwin') or sys.platform[0:4]=='osx':\n\t\t\tplatform = 'other'\n\t\telif (os.name == 'nt' or os.name == 'openbsd') and is_windows():\n\t\t\tplatform = 'this'\n\tif platform not in ('this','2'):\n\t\tplatform = (2 if sys.platform =='win32' else 1) + '"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\nfrom __future__ import print_function\nimport os, sys; sys.path.append('.'); os.environ['DLLINSONAME'] = os.getenv('DLLINSONAME')\n\n#\t# TODO: make a function in the cmd module for splitting cmd line, but still have this code. \ndef split_cmd(line, sep=\" \"):\n\tcmd_str  = '' \n\tcmd_list = []\n\tcommand = split(line, sep.split())\t#  Split line via seps\n\tcommand = command.split"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tprint(\"Platform: \", platform, '\\n')\n\t# Get the full list of command-line parameters\n\tcommand_params =''.join(sys.argv[1:])\n\tif platform==1:\t# auto (always on): auto, POSIX, windows, CMD\n\t\tparams = ''\n\t\tfor i in range(0, len(command_params), 2):\n\t\t\tparams += shlex.quote(command_params[i]) + shlex.quote('?')\n\t\t#print(\"Command-line params:\", params )\n\t\tshell_list ="}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tmultipart_list = re.compile(r'[<\\$][^\\s>]*').split(arg)\n\tdef _multipart(multipart_list, platform):\n\t\t\"\"\" Split multipart values for a platform (int) into subfolders, as string tuples.\n\t\tThis is a reverse-lookup as the platform will use its first element (or None) \n\t\tthat is not stringifying-in, for instance, a list, tuple, or anything else.\n\t\t\n\t\tArgs:\n\t\t\tmultipart_list (list of string"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t##\n#\tplatform = None #\n#\tcmd = []\t\t#\n#\targs = None\t\t#\n#\toptions = []\t\t#\n\t#\n#\t# for compatibility with \"shell=subprocess.Popen()\", and as-is\n#\t_shell = shell\n#\t# for compatibility with \"shell\" arguments (e.g. env variables)\n#\t_envir_var = envir\n#\n#\t# for compatibility with \"envir\" and \"env\" commands\n#\t_envirb_cmd = envirb_cmd\n#"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\ttry: os.remove(os.path.join(extract_folder,archive_name))\n\texcept OSError: pass\n\ttry: _,data,info=cmd_generic(['fileconvert.exe',args,archive_path],capture=True)\n\texcept subprocess.CalledProcessError: cmd_generic(['fileconvert.exe',args],capture=True)\n\treturn None\ndef main(argv):\n\tparser = argparse.ArgumentParser(\n\t    prog='fileconvert.py',\n\t    "}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tarchive = archive_path\n\twith open(archive) as archive_obj:\n\t\tpassed = False\n\t\twhile not passed:\n\t\t\ttry:  \t  \n\t\t\t\twith open(\"/tmp/uncompressed_temp.gz\", 'w+') as uncompress_output:\n\t\t\t\t\tfor chunk in iter(lambda:archive_obj.read(1024 * 8), b''):\n\t\t\t\t\t\tuncompress_output.write(chunk)\n\t\t\t\tarchive = \"/tmp/uncompressed_temp.gz\"\n\t\t\texcept Exception as e:\n\t\t\t\tpass\n\t\"\"\"\t\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\timport requests\n\tfrom datetime import datetime, timedelta # required for date comparison\n\ttry:\n\t\t# TODO: extract archive name from file_path\n\t\tarchive_path = os.path.abspath(archify_path.replace(\"/\",\"\\\\\"))\n\t\tarchive_name = os.path.basename(archify_path)\n\t\tcommand = [os.path.join(os.startfile(archify_path),\"zip --extract-dir=%s\" % archive_path)]\n\t\tif not \"archive\" in os.environ:\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\treturn 'github://' + archive + '@' + sub_repo_url + '/' + archive + '?repository='  + sub_repo_url + '&access_token=' + sub_access_token\n\n# -------------- Helper functions ---\ndef get_repo_info(repo_url, repo_name) -> (dict, str):\n\t\"\"\" Given a repo url, retrieves the repo info. Can handle both\n\t'github' and'sso' namespaces.\"\"\"\n\n\tif 'github' in repo_name:\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tbase_dir=get_base_dir()\n\tarchives_path_list=base_dir+'file_archives/_archives'\n\tarchive_path =archives_path_list+'.'+archive_name\n\tzipfile_path=archives_path_list+'.'+archive_name+'.zip'\n\tif unzip(archive_path,zippath=''):\n\t\t\n\t\tarchive_dir=archivedir\n\t\tarchive_file=archive_path.split('/')[-1]\n\t\twhile archive_file.find('.')>0"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tdef __init__(self,archive_path,destination,file_type):\n\t\tself.archive_path=archive_path\n\t\tself.destination=destination\n\t\tself.file_type=file_type\n\t\"\"\"If the archive_path passed along exists, then uncompress the archive to that location.\"\"\"\n\tdef zip_archive(self):\n\t\twith tempfile.TemporaryDirectory(prefix='hdfs:/') as tmp_path:\n\t\t\toutput_list,error=subprocess.Popen(['tar','-czf','-cf',tmp_"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\nimport os\nimport logging\nimport glob\nimport json\nimport re\nimport subprocess\t\"\"\"\nThis file takes the specified archive path and the directory inside it and compresses it,\nrunning a gzip-decompression, and renaming the archive again to the new archive path.\n\nIt will remove the archive itself, it should generally be at./archive/.\t\"\"\"\nimport shutil\t\"\"\"\nSince we want to re-compress a directory when the archive path changes, we need to move it in another directory as well. \n\nBecause a directory path might change during compilation, we need"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tdef unjar( self, file, archivePath ) :\n\t\tsubprocess.call( ['unzip', '-o', '%s/%s' % (self.path,file), archivePath])\n\n\t\"\"\"\tUncompress a directory.\n\tIf you want to unzip just the one directory and leave aside the intermediate file to be created, uncomment this method.\n\tReturn a file repo url which can be used as origin url.\n\t\"\"\"\n\tdef unget( self, dir ) :\n\t\treturn subprocess.call( ['unzip', '-o', '"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tarchive_path = os.path.join(archive_path, '.git')\n\tprocess = subprocess.run(['git', 'archive', archive_path],\n\t\tcwd=archive_path, capture_output=True)\n\tif process.returncode!= 0:\n\t\traise ValueError('Archive'+ archive_path +'not found!')\n\tfull_repo = os.path.join(archive_path, process.returncode)\n\treturn full_repo\n\ndef build_repo(args): \n\t\"\"\"Creates a folder and"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t# Create repo if it doesn't exist \n\trepo_dir = os.path.join(settings['OUTPUT_DIR'],\"repo_\"+archive_path.split(\"/\")[-1])\n\tif not os.path.exists(repo_dir):\n\t\tos.makedirs(repo_dir)\n\tp = Path(repo_dir)\n\t# Create output directory\n\tif not p.exists():\n\t\t\t# Make directory\n\t\t\tos.makedirs(p)\n\t# Compress archive\n\tsubprocess"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\tfileNames=[];\n\tignoreFiles=[];\n\tfilesToRetain=[];\n\tignoreFiles=[];\n\tfileName='.';\n\t\n\t#make a dict of file names with each line in file as dictionary:\n\tfileNamesByLine={};\n\tignoreFilesByLine={};\n\t\n\tdef getLinesInFile (fileName):\n\t\tprocess=subprocess.Popen(['git','rev-list','--sort=%a,..%m,..%n',fileName],stdout=subprocess.PIPE);\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tos.chdir(os.getcwd())\t\"\"\"\n\tos.chdir(r\"/home/student/newstuff\")\t\"\"\"\n\t#git ls-files -C\n\tif(os.path.isdir('dir/')):\t\n\t\treturn subprocess.call(['git', 'ls-files', '-C', 'dir/'])\n\telse:\t\n\t\treturn subprocess.call(['git', 'ls-files', '-C'])\n\ndef ls1(path):\n\t\"\"\"\tThis function returns a list of all files"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tsubprocess.check_call([\"git\", \"show\", \"-s\", \"-m\", \"--format=%h\", \"--reverse\", branch], stdout=open('output/'+ branch + '_out' + '.txt', \"w\"), stderr=subprocess.STDOUT)\t# List only the directories\n\tfile_list = []\n\tsubprocess.check_call([\"git\", \"diff\", \"--name-only\", \"--diff-filter=removed\"], stdout=open('output/'+ branch + '_out' + '.txt', \"w\"),"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tdef get_filenames(self, git):\t\n\t\tprocess = subprocess.Popen('%s' %git, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\toutput_text = process.stdout.read().decode('utf-8')\n\t\tfilenames= output_text.split('\\n')\n\t\tfilenames=[]\n\t\tfor filename in filenames:\n\t\t\tfilename = filename.rstrip()\n\t\t\tif filename!= \"\":\n\t\t\t\tfilenames.append(filename)"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tresult = ['', '']\t\n\toutput = subprocess.check_output(['git','remote'])\t\n\toutput = str(output, 'utf-8').replace(\"'\", '\"')\t\n\toutput = output.replace('//', ':')\t\n\toutput = output.replace('//', ':')\t\n\toutput = output.replace('\\n', ',')\t\n\toutput = output.replace('\\t', ',')\t\n\toutput = output.split(\",\")\t\n\tfiles = []\t\n\tfor list in output"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.files = []\n\r\n\t\tsubprocess.Popen(\"git clone https://github.com/yalech/yalech-coding.git.\", shell=True, stdout=subprocess.PIPE).wait()\n\t\n\t#def compareFiles(self):\n\t\t\n\t\t#\tregex = re.compile(\"^[\\(][a-z-]+:.*\\\\Z\").search('https://github.com/yalech/yalech-coding/blob/master/"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tdef parse(self, path):\n\t\tcommand_output = subprocess.run(['git', 'log', '--count', '--reverse', '--format=%H'], cwd=path, encoding='utf-8', capture_output=True, text=True)\n\t\tcommand_output = command_output.stdout\n\t\treturn list(filter(None, command_output.split('\\n')))\n\t\"\"\"\n\tUse the git repo as a parent directory and determine the list of ignored files, return those files as a single string with each filename separated by a comma."}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tdef git_list(self):\n\t\tself.git = subprocess.check_output([\"git\",\"ls-files\"])\n\t\tself.git_string = self.git.decode(\"utf8\").strip().split(\"\\n\")\n\t\tself.git_string = list(filter(lambda a : \":\" not in a and a not in [\"#\", \".\"] and not (a.strip() == \"\") and a, self.git_string))\n\t\tself.git_string.sort()\t\t\t\n\t\treturn self.git_string\n\n\tdef git_list_"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\treturn str(subprocess.run([git,'repo', '-a', '--name-only', '--name', 'path', '--pretty=format:%D%%'.join(data).split('//')], cwd=data.encode('utf8'), capture_output=True))\n\ndef gitCommit(path, message): \n\t\"\"\" \n\tMerge the given path into the last repository and save the change as a message string\"\"\"\n\tprint(path.encode('utf-8'))\n\ttext = ''.join(re.split"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tsubprocess.check_call(['./git','status', '--porcelain'])\n\n\tfilename = [f.split('#')[0] for f in subprocess.Popen(['git', 'diff', '--ignore=' + ignoreFile, '.'], stdout=subprocess.PIPE, stderr=subprocess.PIPE).stdout.split('\\n')]\n\tfilename = sorted(filename)\n\tfilenames = []\n\tfor f in filename:\n\t\tif 'ignore' in f:\n\t\t\tfilenames."}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tassert isinstance(filename, str) or \\\n\t\tfilename==None, \"filename needs to be a string or None\"\n\tif (os.path.isfile(filename)):\n\t\tf_name, ext = os.path.splitext(filename)\n\t\text = ext.lower()\n\t\twith open(filename, 'r') as f:\n\t\t\treturn ext in f.read().lower()\t\n\treturn False\t\"\"\"\nassert isinstance(filename, str) or \\\n\t\tfilename==None, \"No filename provided\"\n\n\ndef is_image(filename):"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t# print(os.path.getsize(filename))\n\t# If the size is larger than 500MB\n\tif os.path.getsize(filename) > 1048576:\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef makeDirectory(root, dirName):\n\t\"\"\" Creates a directory only if it doesn't already exist\n\n\tArgs:\n\t\tdirName: name of the dir/file to created\n\t\troot: the root of the whole directory system\n\n\tReturns:\n\t\tNothing\t\"\"\"\n\t# print(\"making directory: '\"+str(root"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif (filename.endswith('_no_comp_1.yaml') or filename.endswith('_no_comp_2.yaml') or filename.endswith('_no_comp_3.yaml')\n\tor filename.endswith('_no_comp_4.yaml') or filename.endswith('_no_comp_5.yaml')\n\tor filename.endswith('_no_comp_6.yaml') or filename.endswith('_no_comp_7.yaml'))\n\tand"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\treturn True if os.path.splitext(filename)[1].lower() in [\".py\", \".pyx\"] else False\n#-------------------------------------------\n#-------------------------------------------\n#-------------------------------------------\ndef extract_module(s):\n\t\"\"\"\n\tFind the module used in the given code. If one is given it is taken as the\n\tsource code module.\n\n\tArgs:\n\t\ts: Source code to check\n\t\tmod: Module to be used\n\tReturns:\n\t\tModule path and file path\n\t\"\"\"\n\tfilename=s\n\twhile os.path.dirname(filename)"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\t\n\t\"\"\"Check if this module is in the same directory as this file.\"\"\"\n\tif __name__ == '__main__':\n\t\tpath = os.path.dirname(os.path.abspath(__file__))\n\t\tpath = os.path.join(path, \"..\", \"python\", \"pycharm\",\"pip_installer\")\n\t\tif not os.path.isdir(path):\n\t\t\treturn True\n\t\t\t\n\t\t\t\n\telse:\n\t\t\"\"\"If we're in another folder, import everything there for now - this might be helpful"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tprint(os.path.splitext(filename)[1])\n\tif os.path.splitext(filename)[1] == \".py\":\n\t\t# It is a.py format - try and match it\n\t\treturn True\n\telif os.path.splitext(filename)[1] == \".mo\":\n\t\t# Its a.mo format - match it\n\t\treturn True\n\telif os.path.splitext(filename)[1] == \".bak\":\n\t\t# Its a.bak format - match it\n\t\treturn True\n\telse:\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\timport numpy as np\n\timport math\n\timport csv\n\timport random\n\timport string\n\tfrom datetime import datetime\n\timport pandas as pd\t\t\t\t\t\t\t\t\t\t\t\t  \"\"\"\nReturn the first letter followed by the suffix of the data file, e.g. '0123456789'\n\n\nArgs:\n    filename: Filename to match\nReturns:\n    filename if a match, else None.\n\"\"\"\ndef extractTimeFromfileName(filename):\n    if fileName.endswith(\".xml\"):\n\tyear = \"20\"\n\tmonth"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif os.path.isdir(filename):\n\t\treturn False\n\telif os.path.isfile(filename):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef get_file_extension(filename):\n    '''Returns the extension of a filename\n\nThis function is to check if it handles extensions, and not case sensitive\n\nArgs:\n    filename: filename (string)\nReturns:\n    the extension of a filename\n'''\n    if (os.path.exists(filename+'.{}'.format('.jpeg')) or\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n# if the filename is a type that this module supports:\nfrom typing import Dict, Union, Optional, AnyStr\n\t\"\"\"\n\tif type(filename) == (type(\"\")):\n\t\treturn False\n\telse:\n\t\treturn True\n\t#if type(filename) == (Union[type(\"\"), Optional[type(\"\")]]):\n\t#\treturn True\n\t#else:\n\t#\treturn True\n\t#pass\n\t#return True\n\treturn False\n\t#return False\n\tpass\n#\n#\n#\ndef getFilesInDirectory(directory"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif filename in dir_list:\n\t\treturn True\n\telse:\n\t\treturn False\n\t\ndef is_file_extension(file):\n\t\"\"\"\n\tCheck if a filename is a filetype\n\n\tArgs:\n\t\tfile: filename to check\nReturns:\n\tfile type\n\n\t\"\"\"\n\text = file.split(\".\")\t\"\"\"Split function that returns a list of strings\t\"\"\"\n\tif ext!= [\"\", \"\", \"\",\"\"]:\n\t\treturn os.path.splitext(file)[1].lower()\n\telse:\n\t\treturn \"\"\n\t\t\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tm = re.match(r\"((0?[1-9]|([1][0-9]|2[0-8]|(?:6[0-4]|(?!0)(?:1(?:0|1[89]|2(?:0[48]|9[0-2])|(?:0|1(?:0|9)<=2(?:0|1[89]|2(?:0|9)<=46)))[0-2]?|(?:0(?:0|1|9)<45)|(?:"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif len(args) == 4:\n\t\tf = '1{0}{1}'.format(args[3].lower(),args[2].upper())\n\t\tif not f:\n\t\t\treturn None\n\t\tunit = args[2]\n\t\targs = args[:3]\n\t\tif unit == 2:\n\t\t\targs = '{}'.format(args[:1])\n\t\t\tunit = 0\n\t\telse:\n\t\t\targs = '{}'.format(args)\n\t\tif re.match(r'^(([0-9]+|[+\\-])d{0"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tmonths = [str (m+1).zfill(2) for m in range(1,13)]\n\tn = 0\n\tfor m in months:\n\t\tn+=1\n\t\tif n==3:\n\t\t\tn-=1\t\n\t\tmonth = int (m)\n\t\tstrmonth = int (m[1:])\n\t\tif strmonth >=0 and month < 7:\n\t\t\tf = strmonth\n\t\t\tn+=1\n\t\telif month >=0 and strmonth <13:\n\t\t\tf = strmonth\n\t\t\tn+=1\n\t\telif month >=-"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tunit, time, period_duration = None, None, None\n\tperiod_duration_dict = {\n\t\t\"days\": timedelta(days=1),\n\t\t\"weeks\": timedelta(weeks=7),\n\t\t\"months\": timedelta(days=30),\n\t\t\"months_delta\": timedelta(days=30 * 7),\n\t\t\"years\": timedelta(days=365),\n\t\t\"years_delta\": timedelta(weeks=366),\n\t\t\"millis\": timedelta(microseconds=1, minutes=1, seconds=1, milliseconds"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\tfstring = frequency.lower()\n\tif fstring == \"all\" or fstring == \"all time\":\n\t\treturn None\n\t\n\ttry:\n\t\tfrequency = float(fstring)\n\t\tif frequency < 0.0 or frequency > 6378137.0: \n\t\t\traise ValueError(\"This frequency is invalid.\")\n\texcept:\n\t\traise ValueError(\"This frequency is invalid.\")\n\ndef parse_tzlocal(tzstr):\n    if tzstr is None:\n        return None\n\t\"\"\"Given a string formating like \"asia/Mountain-"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tweeks = int(freq)\n\tif not freq:\n\t\tfrom dateutil import parser\n\t\tfreq = parser.parse_datetime(value).timetuple().tm_wday\n\t\tif freq == 0:\n\t\t\traise ValueError(\"Only '0' or '1' available for days of an hour\")\n\telif freq < 0:\n\t\traise ValueError('Hours must be non negative.')\n\telif freq > 24:\n\t\traise ValueError('Hours must be less than 24.')\n\treturn datetime.timedelta("}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tfrom math import gcd\n\tfrom math import atan2\n\nfrom time import time\nfrom time import sleep\nfrom uuid import uuid4\n\n\n# Function to convert date_time_string to date-time instance\n# and return the number of weeks with the most number of days\ndef timetowhsec(date_time_string):\n\t\"\"\"\n\tGiven a formatted date/time string, return a given number of days\n\twith the most number of weeks (days = weeks * 24 *60 *60).\n\t\"\"\"\n\tdate, time = date_time_string"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t_type_ = 'timedelta'\n\t_pystr_ = \"an integer or a string representing a unit of time in the format hour:minute:second\"\n\t_repr_ = \"timedelta(%s)\" % _format_\n\n\tvalid_frequency_reg = r'(([0-5]?[0-9]|[1-9][0-9]{1,2})([0-5]?[0-9]|[1-9][0-9]{1,2}))'\n\tdef __init__("}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif type(freq).__name__!= \"str\" or not freq: return None\n\tif freq[-1]!= \"s\": freq += \"s\"\n\tif freq and freq!= \"always\" and freq[-1]!= \"s\": return None\n\tif not freq[:-1] in (\"m\", \"w\"): return None\n\tfor unit in (\"d\", \"h\", \"min\", \"5m\", \"hr\", \"hour\", \"wk\", \"day\", \"wkd\", \"wks\", \"sec\", \"s\""}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t# Return timedelta instance of current year if frequency is 1,\n\t# or if any valid frequency is explicitly parsed\n\tparsed = None\n\n\tif frequency is None or frequency == \"always\":\n\t\tparsed = timedelta(0)\n\t\treturn parsed\n\n\ttry:\n\t\tif frequency == \"weeks\":\n\t\t\t# Convert weeks to days.\n\t\t\t# Assumes that the parsed value from the user is a positive int\n\t\t\t# (as expected)\n\t\t\tif parsed.days > 0:\n\t\t\t\tparsed = timedelta(days=parsed.days*"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tif host == \"localhost\":\n\t\treturn True\n\telse:\n\t\ttry:\n\t\t\t# socket.gethostbyname(socket.gethostname())\n\t\t\tconnect_result = socket.gethostbyname(host)\n\t\t\treturn True\n\t\texcept socket.gaierror :\n\t\t\treturn False\n\ndef get_remote_ip(host_address):\n\t\"\"\"get the ip addr of the host_address and returns\n\tTrue if there is no error.\t\"\"\"\n\ttry:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn (host == \"\" or gethostbyname(host)) \n\n\n\"\"\"\n:func \tsend_message\n\tThis function send a message to remote host, send the message to \n\tdifferent target and send an email to that.\n\n\t:param remote_host: The host to connect to as a socket\n\t:param localhost: Localhost ip, defaults to 127.0.0.1\n\t:param ip: The ip to send the message to.\n\t:param message: The message to send\n\t:return:\n\"\"\"\ndef send_message(remote_host"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn (host== \"localhost\")\n\nclass Server:\n\t\"\"\"\n\tThe Server class is a representation of the server (threading.py)\t\t\"\"\"\n\tdef __init__(self,host_list, buffer_size):\n\t\t\"\"\"\n\t\tThis method initializes the server with a list of hostnames and an\n\t\toptional buffer size\n\t\t\"\"\"\n\t\tself._host_list = host_list\n\t\tself._buffer_size  = buffer_size\n\t\t# Initialize the server\n\t\tself._running = True\n\n\tdef start(self):\n\t\t\"\"\"\n\t\tThis"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn __ip.get('status') == 'STATUS_LOCALHOST'\n\ndef connectClient(host,port):\n    \"\"\"\n\tConnects to the given host\n\t\t\t\n:param host: The host to connect to\n:param port: The port to connect to\n\n:return: The Client connection\n\t\"\"\"\n\treturn socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\nif __name__ == '__main__':\n    host = \"\"\n    port = 598\n    connectClient(host,port)\n    "}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n#####################################################\n#\tChecking for bad host\ndef check_host(host):\n\ttry:\n\t\thost = unicode(host, \"utf-8\")\n\texcept UnicodeDecodeError:\n\t\treturn False\n\tif host.find(\".\") == -1:\n\t\treturn False\n\telse:\n\t\treturn True\n\n#host = \"127.0.0.1\"      //\t127.0.0.1\n#host = \"200.129\")   \t//\t199.129.1.99\nhost = \"todosl."}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tlocalhost_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\tlocalhost_socket.settimeout(0.5)\n\ttry:\n\t\tlocalhost_socket.connect((host, port))\n\texcept socket.error as e:\n\t\tif \"timed out\" in str(e):\n\t\t\treturn \"127.0.0.1\", 500\n\treturn \"127.0.0.1\", 200\n\n__all__ = [\"SocketException\", \"create_request_handler\", \"create_response_handler\"]\n\nclass Socket"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn socket.gethostname() == 'localhost'\n\ndef print_banner():\n\tprint(\"\"\"\n######################################## BANNER ##############################################\")\n\tprint(\"\"\" __     _ ______         _                  _              \n  | |   | (_)  _ __ ___   __| |_ ___ _ __  ___   _ _ ___ \n  | |___| | | | '_ ` _ \\ / _` | '_ ` _ \\/ __|| | | '__/  \n  |  __  | | | | | | | | | (_| | |"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\ttry:\n\t\t# If the host isn't localhost, return false\n\t\treturn '127.0.0.1' in host\n\texcept socket.error:\n\t\treturn False\n########################################################################\n# FUNCTION TO SEND DATABASE TO SERVER\t\n########################################################################\ndef Sender(message_to_send):\n\thost = socket.gethost()\n\thost = socket.gethostbyname(host)\n\tSERVER_PORT = 5050\n\tprint(\"Using server at port \" + str(SERVER_PORT))\n\tSERVER ="}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn True if socket.gethostbyname(host) == (host) else False\n\nportList = []\nclass NetInfoSocket:\n\tdef __init__(self, host, port):\n\t\tself.host = host\n\t\tself.get_ip_address(host, port)\n\t\tself.set_data()\n\tdef set_data(self):\n\t\t\"\"\"Sets the data \"\"\"\n\t\t# Open the socket, bind it\n\t\tself.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tself."}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef check(conn):\n\t\n\t\n\t\n\tconn.send(str.encode('Hi! Connection was established!'))\n\t#conn.send(str.encode('To connect to myserver, press enter'))\n\t\n\toutput = re.search('(E|E|E|E)|[A-Z|0-9]', str(conn.recv(1024)),).group()\n\t#conn.send(str.encode(output))\n\t\n\tconn.close()\n\treturn re.match(\"Accepted\", str(output"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tpat = re.compile(find_regex)\ndef transform(paths,patterns,patterns_match=lambda p:True):\n\t\"\"\"Return a set of paths which match the pattern provided; all paths passed as `patterns` and `patterns_match` are applied to the result.\"\"\"\n\tresults = []\n\tfor p in paths:\n\t\tif p in patterns:\n\t\t\tresults.append(p)\n\tif patterns_match:\n\t\tfor pattern in patterns:\n\t\t\tfor p in patterns:\n\t\t\t\tif pattern in p:\n\t\t\t\t\tresults.append"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tfinds = []\n\tfor line in path:\n\t\tif isinstance(line, list):\n\t\t\tfinds.append([Path(*line).glob('**' * (not match_glob)))\n\t\telse:\n\t\t\tif not match_glob(line):\n\t\t\t\traise ValueError('Pattern \"%s\" not found as a single name' % line)\n\t\t\tparts = line.split()\n\t\t\tif parts[1]!= '**':\n\t\t\t\traise ValueError('Invalid pattern: \"%s\" '\n\t\t\t\t\t\t\t\t\t\t'must have \"{}\" as first item.' % parts[1"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\nimport glob, fnmatch, os, random\nfrom collections import defaultdict\nimport textwrap\n\n\nclass PathPattern:\n\n  def __init__(self, path_patterns=None):\t\"\"\"\n  :type path_patterns: iter[PathPattern,...]\n\t:param path_patterns: Each argument is an iterable over pattern arguments with which matches to the\n\t                path fragment on the path, eg `path_patterns=['abcdefg/*.txt']`\n\n  :type root_path: str\n\t:param root_path: A string."}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t# def __find (path):\n\t### transform here\n\tpass\n\nif __name__ == \"__main__\":\n\tparser = argparse.ArgumentParser ( prog              = __file__    ,\n\t\tdescription       = \"Path glob filter.\",\n\t\tadd_help        = True )\n\t# parser.add_argument ( 'find', help        = \"List of files or pathes (or regular expression)\", required = True )\n\t# parser.add_argument ( '-i', help        = \"Limit path fragments to:  (None to match all"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n__doc__ = \"\"\"Transform a list of path fragments of several path fragments that have been searched in a directory tree\"\"\"\n\ndef find_paths(dirtree, root, exclude_hidden=True, recursive=False):\n\t\"\"\"\n\tRecursively traverse list of path fragments looking for patterns.\n\t\n\tThe output of `find_paths` must be a `list`_ of `unicode` strings,\n\twhere each string represents a path.\n\t\n\tThe search is recursively traversed, if `recursive` is True. If `recursive` is"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\nimport os\nimport glob\nimport string\nfrom functools import lru_cache\n\ndef split_paths(paths):\n    \"\"\"\n    Given a list of paths that match `--find`, return a sequence of tuples with file prefixes and slash \n    traversed paths leading to files and directories (empty strings for single path).\n    \"\"\"\n\n    files = []\n    dirs = []\n    paths = [ x.strip() for x in paths if is_valid_path(x) ]\n\n    for i, path in enumerate(paths):"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tfrom fnmatch import fnmatch\n\ndef find_paths(pattern, path, top_level=None, bottom_level=None):\n\t\"\"\"\n\tFind paths matching the given pattern in a specified location path, top\n\tlevel directory hierarchy, and bottom level directory hierarchy. \n\n\tArgs:\n\t\ttop_level: top level directory name, if not specified use value from\n\t\t\t:data:`hierarchy`\n\t\tbottom_level: bottom level directory name, if not specified use value\n\t\t\tfrom :data:`hierarchy`.\n\t\n\tReturns:\n\t\tpath"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\nclass _Regex(object):\n\tdef __init__(self, pattern):\n\t\tself.pat = pattern\n\n\tdef __str__(self):\n\t\treturn self.pat\n\n\tdef finditer(self, *args, **kwargs):\n\t\treturn re.finditer(self.pat, *args, **kwargs)\n\n\tdef findall(self, *args, **kwargs):\n\t\treturn re.findall(self.pat, *args, **kwargs)\n\n\tdef sub(self, f, *args, **kwargs):\n\t\treturn"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tdef pattern_builder(pattern):\n\t\tif isinstance(pattern, list):\n\t\t\tpattern = tuple(pattern)\n\t\treturn re.compile(pattern[0]) + re.compile(pattern[1]) # re.escape() and re.sub() work on both sets\n\n\tdef convert(preamble):\n\t\tyield re.compile(pattern_builder(preamble))\n\treturn convert\n\n\t#TODO: add tests\n\n\ndef find_paths(patterns):\n\t'''\n\t`patterns` is a series of patterns, each"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tif not isinstance(paths, list) or not all(isinstance(p, str) for p in paths):\n\t\tpaths = [paths]\n\tpatterns_out = []\n\n\t# get each item, and put its patterns back to match\n\tfor path in paths:\n\t\tpars = [] # new set of patterns\n\t\tfor pat in path.split(':'):\n\t\t\tpat = re.compile('__pattern__', re.I) if (\n\t\t\t\tnot pat or pat.match(path)!= None) else pat\n\t\t\tpars.append"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\ttry:\n\t\tcmd = 'git bash -c \"git -C {0} | grep -v ^.gitbash\"'\n\t\tresult = subprocess.check_output(cmd.format(os.environ['SISTEM']), shell=True).strip()\t\"\"\"\n\t\t\tReturns the git  output and is always a `str` on windows\t\t\"\"\"\n\texcept:\n\t\tlog.error(\"Error getting output of executing git bash, not running linux.\")\n\t\treturn False\n\t\treturn True\n\n\ndef get_path(file):\t\"\"\"\n\t:param file: path to git"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.name == 'nt'\n\ndef gitbash_init(path=''): \"\"\"Initializes gitbash\n\t:param path: Path to gitbash \n\t\"\"\"\n\t\tgit_cmd =[ \"git\", \"init\"]\n\t\tsubprocess.check_output(git_cmd)\n\t\tcmd = [ os.path.join(path, \"bin\", \"git\"), \"init\"]\n\t\tif os.path.exists(os.path.join(path, \"bin\", \"git\", \".git\")):\n\t\t#   if os.path.isdir('bin"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tpython3 = os.environ.get(\"PYTHON3\")\n\treturn python3 == \"True\"\n\ndef getPath():\n\t\"\"\"\n\tReturns a path to Python 3, if it is included in the global scope\t\"\"\"\n\tpython3 = os.environ.get(\"PYTHON3\")\n\tif python3 == \"True\":\n\t\treturn eval(f'os.path.dirname(os.execv(\"/usr/bin/python3.7\", [],\"./\")[1])')\n\telse:\n\t\treturn \"C:\\\\"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tif os.name == 'nt' and os.getenv('GIT','') == 'git':\n\t    return True\n\ndef getGitCheckPaths(projectName,gitCheck):\n\t\"\"\"\n\treturns dict of checked out git directory paths\n\te.g. {'projectBn':'projects/projectsBn'}\n\t\"\"\"\n\n\tgitCheckDir = gitCheck\n\n\tif hasattr(gitCheckDir,'strip'):\n\t\tpathList = []\n\n\telse:\n\t\t# pathList was previously a string\n\t\tpathList = ['.'"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.system(\"gitbash\") == 0\ndef clear(): os.system(\"clear\")\t\"\"\" Clear the screen \"\"\"\n\ndef file_exists(path): \"\"\"\"\"\" return True if path exists on path\"\"\"\n\tif os.path.isfile(path):return True\n\telse: return False\ndef path_exists(path): \"\"\"\"\"\" return True if path exists \"\"\"\n\tif os.path.exists(path):return True\n\telse: return False\ndef read(path): \"\"\"\"\"\" return File Object \"\"\"\ndef write(path,data): \"\"\"\""}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\ttry:\n\t\tos.getcwd()\n\texcept AttributeError:\n\t\treturn False\n\ttry:\n\t\tif os.getcwd() == os.path.expanduser(\"~/.gitbash\"):\n\t\t\treturn True\n\texcept:\tpass\n\treturn False\n\n__ver__str__ = \"6.3.0\"\n__veri__ = __ver__str__[0:1]\n__veri__num__str__ = __ver__str__[1:2]\n__veri__version__str__ = __ver__str"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tif os.name == 'nt':\n\t\tif os.fork() > -1:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\t\t\t\n\"\"\"\n    if getattr(this_method, '_testModules', [])!= []:\n    \tfor test_module in this_method._testModules:\n\t\tif test_module in ['__main__', 'test.py']:\n\t\t\tprint('This module requires a special command', end='', file=None)\n\t\t\tprint(':' + test_module, end='', file=None)\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tif os.name!= \"nt\":\treturn False\n\ttry:\treturn os.getuid() == 0\n\texcept:\treturn False\n\n\"\"\"\tfrom win32api import EnumDisplayDevices\t\"\"\"\n\n# \tdef enum_device ():\n# \t\ttry: return \\\"\"\"\"\",\"A\".\"A\".\"\"\"A\".\"\\\" \"\"\"\"A\".\"\"\"A\".\"\"\" A\".\"\"\"A\",\"A\".\"A\",\"A\".\"A\".\"[i]\" \"\"\",\\\n# \t\t\t\t, \\\",\", \\\">\", \\\".\\\"\","}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.system(\"git -c\") == 4  \"\"\"\n\tIf inside git directory in which gitbash is located in, returns True\n\tIf gitbash could not be found inside where you are located in,\n\treturns None, Otherwise, calls os.system and appends.git to your command\n\t\"\"\"\n"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.name == 'nt' or os.name == 'posix' and os.sys.platform == 'linux2'\n\n\n#===============================================================\nclass DbConnector(object):\n\tdef __init__(self, host, port, user, password, schema=None):\n\t\tself.host, self.port, self.user, self.password, self.schema, self.db, self.user_table, self.schema_table = \\\n\t\t\tf\"{host}:{port}\", None, user, password, schema, None, None,"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t# Now we can write out files.\n\ttry:\n\t\tif not os.path.exists(args[\"--target-path\"]):\n\t\t\tos.makedirs(args[\"--target-path\"], mode=0o777)\n\texcept Exception:\n\t\tprint(\"Error, something wrong with this\")\n\t\tsys.exit()\n\t# This is probably not ideal. But this is the easiest way enough.\n\n\twith open(os.path.join(args[\"--target-path\"], \"target.yaml\"),'w') as outfile:\n\t\tyaml.dump"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.exists('%s.yaml'%config):\t\"\"\"\tif the specified file exists, try and read it.\t\"\"\"\t\t\t\n\t\twith open('%s.yaml'%config,'r') as f:\n\t\t\tyaml_contents = f.read()\n\telif create_new_target_file:\t\"\"\"\tcreate a new tare file, if it doesn't exist.\t\"\"\"\t\t\t\n\t\ttarget_config = {}\n\t\ttarget_config['config_name'] = config\n\t\ttarget_config['targets_directory_path"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\timport shutil\n\timport glob\n\timport yaml\n\timport time\n\timport random\n\timport argparse\n\timport sys\n\timport getpass\n\timport string\n\tfrom itertools import filterfalse, filter\n\n\t# Initialize global variables.\n\tglobal newConfigFile\n\tglobal changedConfigsList\n\tglobal targetConfigs, changedConfigYAMLs\n\n\tif not os.path.isdir(newConfigDirectory):\n\t\tos.makedirs(newConfigDirectory)\n\tif not os.path.isdir(newConfigWorkingDirectory):\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.isfile(target):\n\t\tif not os.path.exists(target) and not overwrite:\n\t\t\traise SystemExit('target file is not present or overwrite is not set to true, not overwriting anything on it. Use --overwrite to write it in place.\\nAborting...')\n\telse:\n\t\t# create target file\n\t\tif not os.path.isdir(target):\n\t\t\tos.makedirs(target)\n\t\twith open(target, 'w') as output_file:\n\t\t\tyaml.dump(_cfg, output"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.exists(src):\n\t\ttry:\n\t\t\t# If configs can be overwritten/directly moved, create new if not empty.\n\t\t\tif (isinstance(tmp,str) and os.path.exists(tmp)) or os.path.exists(tmp):\n\t\t\t\tif os.path.exists(tmp):\n\t\t\t\t\tos.remove(tmp)\n\t\t\t\tif os.path.exists(tmp):\n\t\t\t\t\tprint(\"configs found, overwriting existing configs.\")\n\t\t\t# If configs can be moved, create new configs."}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tconfig, target = config_fn\n\ttemplate = os.path.join(config.outdir, 't.yaml')\n\tif os.path.exists(template):\n\t\tos.unlink(template)\n\tif os.path.isfile(template):\t\t\t\t\t\"\"\"\tCreate empty yaml file for target YAML.\t\"\"\"\n\t\t\t\t\t\t\t\t\t\t\tos.unlink(template)\n\twith open(template, 'w') as yaml_file:\n\t\tyaml_file.write(yaml_str)\n\t# create dir paths\n\tos."}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\timport sys\t\"\"\"\n\tImport required module and call system commands that will create all necessary directories (e.g. the log directory, data folders)\n\t\"\"\"\n\timport argparse\t\"\"\"\n\tHandles the command line input of the program and stores it's location to an argument variable.\n\t\"\"\"\n\timport datetime\t\"\"\"\n\tThe timestamp variable is created when the YAML has been parsed.\n\t\"\"\"\n\timport configparser\t\"\"\"\n\tThe YAML needs to be specified using the 'name' variable in the'section' variable.\n\t\"\"\"\n\timport shut"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif (not os.path.exists(\"config.yaml\")):\n\t\tprint(\"Config file doesn't exist, creating...\")\n\t\t\n\t\t# create the config.yaml file\n\t\tf = open('config.yaml', 'w')\n\t\tf.write(\"#YAML file that will be used for running the script\")\n\t\tf.write(\"\\\"\")\n\t\tf.write('#file = config.yml')\n\t\tf.write(\"\\\"\")\n\n\t\t\n\t\t# create subdirectories\n\t\tfolder = './results/'\n\t\ttry: os"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tconfig_filename = os.path.abspath(config_filename)\n\ttarget_config = os.path.abspath(target_config)\n\n\tif os.path.exists(target_config):\n\t\tif os.path.isfile(config_filename):\n\t\t\tprint(\"[!] You can't overwrite the config file, or you are copying an existing file to it.\")\n\t\t\tif args[\"overwrite\"]:\n\t\t\t\treturn\n\t\t\t# Abort writing and exiting - you may want to do something different or call another function to overwrite files and do this safely.\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.exists(config):\n\t\tif os.path.isfile(config):\n\t\t\tos.remove(config)\n\telse:\n\t\tos.writefile(config, data)\n\ndef remove_dirs(dirs):\n\t\"\"\"\n\tRemove any directories from one directory list\n\t\"\"\"\n\tif len(dirs):\n\t\tfor element in dirs:\n\t\t\telement.rmdir()\n\ndef append_to_dir(source, target, to_add):\n\t\"\"\"\n\tAdd an element to another directory. The new element will"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tif \"\\n\" in script:\n\t\tiScript = iScript.replace(\"\\n\", \"\")\n\t\toutf = open(\"%s %s\"%(outdir, script), \"w\")\n\t\toutf.close()\n\t\tfScriptReaded = 0\n\t\tfOpen = 0\n\t\twhile fOpen == 0:\n\t\t\twhile True:\n\t\t\t\tline = readScript(outf, script )\n\t\t\t\tif len(line) > 0:\n\t\t\t \tif line[0] == \"#\":\n\t\t\t\t \tbreak\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t "}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t# Create the message\n\tmessage = [msg_type, '{:d}', msg_channel, 'SSH_USER', username]\n\n\t# Create a placeholder for the command\n\t# Placeholder will be the text that would be written on the log file,\n\t# the placeholder would be just an array of empty strings like [msg_type, msg_channel, username]\n\tplaceholder = ['', '', '']\n\tplaceholder[0] = 0\n\tplaceholder[1] = 'SSH_USER'\n\tplaceholder[2] = str(username"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tscript = ''\n\tscript += \"%s;#!python3.9\\n\" % cmd\n"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\tscript = script.split(\"\\n\")[0].strip()\n\tl = script.split(\" \")\n\tl = list(filter(None, l))\n\t\t\t\t\n\treturn \" \".join(l)\n\tpass\n\ndef runCommand(user, password, command, remote_host, remote_path, **kwargs):\n\t\"\"\"run a command on a remote host.\n\n\t:param user:\n\t\tThe username to execute\n\t:type user:\n\t:param password:\n\t\tThe password to login to the host\n\t:type password:\n\t"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tfor arg in [' '] + script.split():\n\t\tif len(arg) < 2:\n\t\t\tself.echo('Missing script argument')\n\t\t\treturn\n\n\t\tself.echo(arg)\n\n\tprint(':'*50)\n\tconverts argument into script\n\tself.exec_print('|'+textwrap.fill(f'/usr/bin/ssh {script} | {script} > {script}.stdout', max_chars=100))\n\t\n\t\"\"\"\n\tTakes command\n\t:param script:\n\t:return:\t\"\"\"\n\t\n"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\timport subprocess\n\n\tscript_string = textwrap.dedent(script)\n\tresult = subprocess.run(\n\t\t[\n\t\t\t\"ssh\",\n\t\t\t\"-C\",\n\t\t\t\"m.vnclix.com:2147:1:s0:z9y0xZDfQfHt5NjL7uA7G6\",\n\t\t\t\"-o\",\n\t\t\t\"StrictHostKeyChecking=no\",\n\t\t],\n\t\tcheck=True,\n\t\tcapture_output=True,\n\t\tinput=script_string + '\\n"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\ttext = textwrap.dedent(script)\n\treturn f\"{text}\\n\"\n\ndef execute(command):\n \treturn subprocess.run([\"ssh\", \"-i\", \"admin\", \"-l\", \"root\", \"python3\", \"-c\", f\"ssh {command}\" + \";\"], stdout=subprocess.PIPE)\n# \t\t ,shell\n# \t\t ,stdin\n# \t   stdout\nexecute(\"echo '\"\"'||''\\\\u00a0\\\\u00a0'\"\\n\")\nexecute(\"echo"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t# get the current text\n\tscript = script.rstrip('\\n')\n\n\t# split each char into a new line\n\tlines = script.split('\\n')\n\n\t# now get whatever line is before the first quote and add it to 'line'\n\tline = lines[0]\n\n\t# add quote of the first line to new line\n\tif isLineQuote(line):\n\t\tlines.insert(0,'   ')\n\n\t# now extend old line until last quote\n\twhile isLineQuote(line[-1:]) :\n\t\tline.extend("}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tscript = script.replace(\"{\",\"#\")\n\tscript = script.replace(\"}\",\"\")\n\tsplit_by_newline = script.split(\"\\n\")\n\tscript = '\\n'.join([x for x in [x if x else'' for x in split_by_newline]])\n\t\n\tconverted = f'cd {file} && mkdir -p && {script} > /tmp/script.sh && cat - | ssh-ftp-transfer >> /tmp/script.sh'\n\tscript_file = open('/tmp/script"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\twith open(script, \"rb\") as script:\n\t\tlines = script.read().splitlines()\n\tcmd = lines[0].decode(\"latin-1\",'replace').encode(\"latin-1\",'replace')\n\tif len(lines) > 1:\n\t\tcmd += \"\\n\"\n\t\tcmd += \"\".join(lines[1:])\n\treturn cmd\t\n\ndef print_targets(f):\n\t\"\"\"\n\tprint out the targets in a script\n\t:param f: script to print\n\t:return:\n\t\"\"\"\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\t\"\"\"\n\tMake a subprocess, and wait for it to end.\n\t\n\tIf an error occurs (in one of the arguments)\n\tthen subprocess.run has a ``stderr`` attribute\n\twhich contains the output of the command that threw an\n\terror in a subsequent call.  If no output was\n\tentirely produced then the method returns\n\tan (empty) ``stdout`` attribute which will be an empty\n\tstring, because no output was received from the\n\tsubprocess.\n\t\"\"\"\t\n\n\tif len(args) < 1:"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tos.execvp('subprocess', ['python',path,extra_env] + sys.argv[2:])\ndef check_output(command, *args, env = None, stdout = None, stderr = None):\n\"\"\"Execute subprocess function and check output.\n\n\nParameters\n----------\ncommand : list\n    A list of arguments to be executed on the shell.  The result of executing\n    the command will then be returned.\t\n*args : list\n    Any additional arguments to be passed into the subprocess.\nenv : dict[str, str], optional"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tprocess_args = subprocess.run(args, capture_output=True, check=True)\n\t# TODO: This appears to never happen, but it seems a bug in the\n\t# interface module, I think.\n\toutput = process_args.stdout.splitlines()\n\twhile output:\n\t\t# TODO: This seems to never happen, but I'm not sure it\n\t\t# happens on what the interface module is running...\n\t\toutput = output[0]\n\t\toutput = output.strip()\n\t\tsplit_output += output\n\treturn split_output"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\treturn subprocess.run([*args, 'run'], env=env, **kwargs)    \"\"\"\n\nExamples\n--------\nThe default ``kwargs`` used for ``run`` will be:\n\tenv = {'PYTHONPATH': path}\n\"\"\"\ndef run(\n\tpath,\n\t*args,\n\tfunc=script(lambda x: x, args=args, kwargs={'returns_output': True}),\n\textra_env=**kwargs\n):\n\t\"\"\"Run a module with the given file as function to it's arguments."}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\treturn subprocess.check_call([sys.executable, func, *args, *extra_env],shell=False)\n\n\nif __name__ == \"__main__\":\n\targs = read_command_line(\"main_test.py\",sys.argv)\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\t#import inspect module to access subprocess\n\timport inspect\n\n\t# get function to call: \n\t# func_name is used for error info\n\ttry:\n\t\tfunc_name = inspect.getframeinfo(inspect.stack()[1][0]).function\n\t\t#print(\"\\n\\n\")\n\t\t#print(\"> Executing '\" + func_name + \"'\")\n\t\t#args.append('-d')\t# for capturing stdout/stderr output (which can be useful outside a sub-process)\n\t\tret = subprocess.run"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tdef __init__(self, func, *args, extra_env = None):\n\t\tsuper().__init__(func,*args)\n\t\tself.extra_env = extra_env\n\tdef run(self, daemon = False):\n\t\tsubprocess.run(self.args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=self.cwd, universal_newlines=True, env=self.extra_env)\n\t@classmethod\n\tdef from_string(cls, func_name:str"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tdef __init__(self,func,*args,extra_env={},*popen_args):\n\t\t__init__._popen_args=popen_args\n\t\tpopen_args=extra_env\n\t\tfunc_args=(args[0:1]+extra_env.keys()+args[1:])\n\t\tsubprocess.run(func_args,**popen_args).check_returncode\n\t\tif popen_args.get('stdout'):\n\t\t\tsubprocess.stdout=popen_args['stdout']\n\t\tif popen"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tenv = os.environ\n\tprint(\"Run command :\", command)\n\tp = subprocess.Shell( [\"bash\", cmd])\n\tp.call_in_new_env( [cmd] + args,env=env)\n\nprint(\"Finished executing command :\",cmd)\ncommand = \"echo 'hello world!'\" ;subprocess.run([\"bash\", \"factory1.sh\"])\n\nprint(\"\\nSubprocess stdout:\")\nstdout = subprocess.stdout.read()\nfor line in stdout.splitlines():\n\tprint("}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\tprocess = None\n\tp = None\n\n\t@staticmethod\n\tdef initialize(func, args, extra_env=None, **kwargs):\n\t\t\"\"\"Initialize an environment for a function invocation\"\"\"\n\n\t\tkwargs.setdefault('extra_environ', extra_env)\n\t\tkwargs.setdefault('check', kwargs.pop('check', None))\n\n\t\targs = list(args)\n\n\t\tif kwargs.get('cwd'):\n\t\t\tkwargs['cwd'] = kwargs.pop('cwd')\n\n\t\tkwargs['env"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tenvname = os.getenv(name, default) \"\"\"\n\tGet environment variable identified by name environment variable. If no environment variable is set, an Error is raised.\t\"\"\"\n\treturn envname\n\t\"\"\"\n\t\tCheck the environment variable and set the environment value based on a matcher environment variable.\n\t\"\"\"\ndef unsetValue(name,match_object,default=\"\"):\n\t\"\"\"\n\tUnset value from environment given an object with a attribute name and an optional value.\n\tIf the attribute is not defined in environment and a default value is provided, an Error is raised.\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tassert len(env)-1 == len(name), \"The number of keys is not equal to the number of names.\"\t\"\"\"\tGet the value from environment given a matcher containing a name and an optional default value.\t\"\"\"\n\treturn os.environ.get(env) or (env+\"_default\" if len(env) > 0 else env+\"_val\")\n\ndef env_variable(name):\n\t\"\"\"A function to get a specific environment variable.\t\"\"\"\n\treturn os.getenv(name)\n\n\"\"\"\n\tA function to change a value in a"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\t#\tos\t---\tgetenv [name='value']\n\t#\tos\t---\tos.getenv (value='value', env)\n\t#\tos\t---\tos.getenv ([name]==\"value\" (default value=''))\n\t#\tos\t---\tgetenv (value=os.path.basename(os.current_path))\n\t#\treturn value\n\t#\t\n\t#os.path.basename - Get the path of the file without the path from the operating system environment. (Used for relative path.)\n\"\"\""}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\"\"\"\n\tget() -\tGet the value of an environment variable.\t\"\"\"\n\t\"\"\"getenv(\"key\", default = \"default value\") -\tGet the environment variable <key>.\"\"\"\n\t\"\"\"getenv(\"envvar\", \"default value\") -\tGet the environment variable <envvar> as it is in one of the environment variables under <key>.\"\"\"\n\t\"\"\"getenv(\"VAR1=CONST1,VAR2=CONST2\", VAR3=CONST3) -\tGet the environment variable <key>, where the values in the environment variable dict <"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default:\n\t\treturn default\n\telse:\n\t\tValueError(\"Environment variable does not exist\") \n\n# Read environment variables\nENV_VAR = get_env_var(\"ENV_VAR\")\nWEB_HOST = get_env_var('WEB_HOST')\nWEB_PORT = get_env_var('WEB_PORT')\nKEY = get_env_var('KEY')\n\n\"\"\"\nWrite web site\n\"\"\"\n# set"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef getvalue(self,name,default=False):\n\t\treturn os.environ.get(name[0:-2]+'_'+name[-2:],default)\t\"\"\"\tGet the value from environment given a matcher containing a name and an optional default value.\n\t\t\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tval=getvalue()\n\t\n\"\"\"-------------------------------------------------------------------\t\n\tGet a set of values from a environment named, value[index]; if an environment variable cannot be found or if the variable is not"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif os.getenv('MATRIX_INPUT_PATTERN'):\n\t\tmatcher_name = os.getenv('MATRIX_INPUT_PATTERN')\n\telse:\n\t\traise Exception(\"No match pattern given\\n\\tPlease provide 'MATRIX_INPUT_PATTERN' as value in environment.\")\n\n#*********************************************************************************************\n\n\"\"\"\tTakes a matcher name and returns a matcher object from the given matcher name.\t\"\"\"\ndef get_matcher_from_name(name):\n\tmatcher_"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef get(name): return os.getenv(name)\t\"\"\"\n\tGet the value from environment given a matcher containing a name and a optional default value.\t\"\"\"\n\tdefaultValue = os.getenv(\"DEFAULT\", \"\")\t\"\"\"\n\tGet the default value of an environment variable if environment variables are absent and the user indicated a specific default value.\t\"\"\"\n\tdef getDefault(name): return os.getenv(\"DEFAULT\", defaultValue)\t\"\"\"\n\tGet the default value of environment variables.\t\"\"\"\n\tdef reset():\t\"\"\"\n\tReset all environment variables"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef get_from_environment(name, default=None):\n\t\t\"\"\"\n\t\tget_from_environment(name, default=None)\n\t\tReturn the value of the environment variable NAME. If default is not provided, raise ANFError.\n\t\t:raise ANFError: - if environment variable is not defined \n\t\t\"\"\"\n\t\t# If it's an environment variable, grab it\n\t\t# Else grab the value of the variables in environment AND a default value of -1\n\t\t# Return the value of the environment variable\n\t\tif os.environ.has_key(name):\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\ttry:\n\t\treturn os.environ[env_varname]\n\texcept KeyError:\n\t\tValueError\n\texcept Exception as e:\n\t\terror.error(\"ERROR: Environment variable {env_var_name} does not exist.\")\n\t\traise e\n\telse:\n\t\tlog.info(f\"{env_varname} Environment variable {env_var_name} found, defaulting to the current variable value.\")\n\t\treturn os.environ[env_varname]\n\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------    Import the required libraries: tq"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tpass\ndef image_file(image_href: str) -> str:\n\t\"\"\"\n\tParses an image href into composite parts.\n\n\t:param image_href: href of an image\n\t:returns: the image's fully qualified url \n\t:raises ValueError:\t\"\"\"\n\tpass\ndef image_filename(image_href: str) -> str:\n\t\"\"\"\n\tParses an image href into composite parts.\n\n\t:param image_href: href of an image\n\t:returns: the image's fully qualified file name \n\t:"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\thref = urllib.URLopener().retrieve(image_href, filename)\n\t(image_id, base, use_s) = splitparams(href)\n\treturn (image_id, base, use_s.strip(\"'\"))\n\ndef image_id(uri):\n\t\"\"\"\n\tParse a href into a uid.\n\t:paramuri: href of an image\n\t:returns: a tuple of the form (image_id, image_hbase_uid)\n\t:raises ValueError:\n\t\t\"\"\"\n\timage_id"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\timage_id = \"\"\n\ttry:\n\t\timage_id = int(image_href[8:])\n\texcept ValueError:\n\t\traise ValueError('Invalid image_href')\n\tif image_id == -1:\n\t\treturn (None, None, True)\n\telif image_id < 1:\n\t\treturn (None, None, False)\n\telse:\n\t\treturn (\"img\", image_href[0:6], image_href[6:9] == \"https\") \t#:\t\n\ndef parse(url):\n\t\"\"\"Parse"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\"\"\"\n\tHelper function for parsing a resource that is an href\n\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\t\"\"\"\n\turl = urllib.urlopen(image_href)\n\tif not \"http\" in image_href:\n\t\tif os.path.isfile(image_href): raise Exception(\"Can't parse image\")\n\t\timage_href = url.info().get_content_type()\n\n\tnetloc ="}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tfrom urllib\timport parse as urlparse   \"\"\"\n\tfrom urlparse import urlparse \"\"\"\nimport re\t\t: regexe for replacing special characters\n\nclass ImageExtractor:\n\t\n\tnetloc = None\n\tdef __init__(self):\n\t\tpass\n\t\n\tdef parse(self, image_href) -> None:\n\t\t\"\"\"\n\t\tParse an image href into a 2-tuple, where\n\t\tthe first item of the tuple is the ID of the image;\n\t\tand the second is the host, which should be\n\t\ta string representing the IP address"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\ttry:\n\t\t(image_id, netloc, use_ssl) = urlparse(image_href)[:3]\n\t\treturn (image_id, netloc, use_ssl)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid image href '%s'\" % image_href)\n\texcept TypeError as err:\n\t\traise ValueError(\"Invalid image href '%s' (%s\" % (image_href, err))\n\n\ndef parse_image_sizes(doc):\n\t\"\"\"\n\tParse a <img> html page document, and return a"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\treturn urllib.parse.urlparse(image_href).path.rpartition('/')\n\ndef getImage(image_id):\n\t\"\"\"\tGet a valid image from a valid image_id at image_path.\n\n\t:param image_id: a valid image id to make valid\n\t:returns: a valid image object\n\t:raises ValueError: if the image_id does not exist at the image_path\n\n\t\"\"\"\n\timport time\n\tfrom glob import glob\n\tfrom os import urandom\n\timport base64\n\ttry:\n\t\t"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\ttry:\n\t\tparams = dict(parse_qsl(urlparse(image_href).query))\n\t\timage_class = params.get('class', '')\n\t\tif not image_class:\n\t\t\traise ValueError('Missing image class: {0}'.format(image_href))\n\n\t\timage_id = image_class\n\t\tnetloc, use_ssl = ImageParse.get_netloc_and_use_ssl(image_href)\n\t\tif not '.' in netloc or \":\" in netloc:\n\t\t\traise ValueError('Invalid netloc at {"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\turls = urllib.parse.urlparse(url)\n\n\t# Use the scheme, netloc, and use_ssl as a string instead\n\tscheme,net,use_ssl,query,fragment = urllib.parse.urlsplit(url)\n\tnetloc = url_unparse((scheme, net, use_ssl, query, fragment))\n\tnetloc = netloc.encode(\"utf8\")\n\tuse_ssl = use_ssl.encode(\"utf8\")\n\n\tparts = urllib.parse.urlparse(image_"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\treturn ((image_href, '', False))\n\ndef url_join(base_url, params):\n\tif params is None: \n\t\tparams = {}\n\tparts = urllib.parse.urlsplit(base_url)\n\tbase_url = urllib.parse.urlunsplit((parts[0], parts[1], params))\n\treturn base_url\n\ndef find_image_info(base_url):\n\twith urllib.request.urlopen(base_url) as response:\n\t\tdata = response.read()\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t# Check for a trailing os seperator character\n\tif '\\\\' in input_list[0]:\n\t\traise TypeError(\"Output of the script should not have the '\\\\' character removed.\\nYou should remove it from the beginning of your script.\")\n\n\t# Check for one item in the input list\n\tif len(input_list) < 1:\n\t\traise TypeError(\"The script should not have been executed because it contains no items.\\nYou should check if the sequence used in argument is correct.\")\n\n\t# Remove trailing os seperator\n\twhile len(input_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tprocessed_list = []\n\tfor string in input_list:\n\t\tstring = string.split(os.path.sep)\n\t\tif len(string) > 1:\n\t\t\tprocessed_list.extend(string[:-1])\n\t\telse:\n\t\t\tprocessed_list.append(string[0])\n\n\tprocessed_list = [i for i in processed_list if i!= str(os.path.sep)]\n\n\tprocessed_list = sorted(processed_list)\n\tprocessed_list = \"\".join(processed_list"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\twith open(sys.argv[1],'r') as file:\n\t\ts = file.read()\n\t\tnew_string = []\n\t\tfor string in s:\n\t\t\tif string.find(string[-1]) >= 0:\n\t\t\t\treturn new_string\n\t\t\telse:\n\t\t\t\tnew_string.append(string[-1])\n\tos.remove(sys.argv[1])\n\ndef delete_empty_lines(string: str) -> str:\n\t\"\"\"deletes empty lines from list string. Empty lines are removed from string if they exist. \n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tassert list, \"input_list is not a string list\"\n\tlength_list = list(filter(None,map(len, map(str, input_list))))\n\tassert len(length_list) >= 1, \"input list is empty\"\n\tpath_seperator_regex = '^[.\\s]([^;]*)\\\\\\\\[;,(]$' # must have ';' for escaped paths\n\tif any(map(str.endswith, length_list, [0, 1])): # remove trailing os seperator characters\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tprocessed_list = []\n\tstring = \"\"\n\tfor i in input_list:\n\t\tif len(i) >1:\n\t\t\tif type(i) is str:\n\t\t\t\tif i.startswith(\"/\"):\n\t\t\t\t\tprocessed_list.append(i[1:])\n\t\t\t\telif string == \"\":\n\t\t\t\t\tstring = i\n\t\t\t\telse:\n\t\t\t\t\tprocessed_list.append(string + i)\n\t\t\telse:\n\t\t\t\tprint(\"[!] Value is not string\")\n\t\telif type(i) is int:\n\t\t\tprocessed_list."}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\treturn [os.path.splitdrive(path)[1].replace(os.sep,'') for path in input_list if path and path[-1]!= os.sep]\n\ndef remove_trailing_os_sep(path1, path2):\n    \"\"\"\n    Removes the trailing os seperator chars and returns it back.\n\n    Args:\n\t    path1, path2: the path names.\n    \n    Returns:\n        removed_list: the list of string.\n        \n    Raises:\n        TypeError\t\"\"\"\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\timport re\n\tfrom os import listdir\n\tfrom os import getcwd as gcd\n\tfrom os import rmdir as remove\n\tfrom os import remove as rmtree\n\nclass PathError(Exception):\n    pass\n\nclass ProcessedFile(object):\n\t\"\"\"\n\tObject representing a single file process.\n\n\tThis object contains the following attributes:\n\t\t- dir name path\n\t\t- filename\n\t\t- contents\n\t\"\"\"\n\n\tdef __init__(self, dirname=None, fname=None):\n\t\t\"\"\"\n\t\tConstruct a new instance.\n\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t#Check if input is a string list\n\ttry:\n\t\t\"\".join(input_list)\n\texcept TypeError as e:\n\t\tpass\n\telse:\n\t\traise TypeError('Error: argument \"input_list\" can\\'t be converted to a list.')\n\n\t#Check each string\n\ttry:\n\t\t[len(string) for string in input_list]\n\texcept Exception as e:\n\t\tpass\n\telse:\n\t\traise TypeError('Error: argument \"input_list\" can\\'t contain items.')\n\n\t#Remove trailing os seper"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\nfrom re import sub\nfrom.exceptions import FileProcessorError, FileNotFoundError\nfrom sys import argv\nfrom urllib.parse import quote as urlquote\n\n\ndef _filter_file(file):\n\t\"\"\"Filters a file name to make sure that it's only accepting\n\tfiles ending with.\n\t\n\tArgs:\n\t\tfile: a file name in string list. Must have been made for use in `_file_name_check`_ method.\n\t\t\n\tRaises exception:\n\t\n\t\t**FileProcessorError** \t\t\t: file not"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tif isinstance(input_list, str):\n\t\tstripped_list = [s.lstrip(os.pathsep) for s in input_list]\n\telif isinstance(input_list, list) and all (isinstance(s, str) for s in input_list):\n\t\tstripped_list = [s.lstrip(os.pathsep) for s in input_list]\n\telif isinstance(input_list, list) and all (isinstance(s, unicode) for s in input_list):\n\t\tstripped_"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn re.compile(r\"(^[a-z]*)([A-Z\u00e0\u00e1\u00e8\u00e9\u00ed\u00f3\u00fa]*)$\")\ndef is_numeric(text):\n\ttext = stringify(text)\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn re.search(r\"(^[a-z]*)([0-9]*)$\",text)\ndef is_decimal(text):\n\ttext = stringify(text)\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn re"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tdef pattern_from_str(s, regex=False):\n\t\tr\"\"\"\n\t\tBuild the pattern from the given string.\n\n\t\t:param s: string to extract a pattern from\n\t\t:type s: str\n\t\t:param regex:\n\t\t:type regex: bool\n\t\t:return: a regular expression string\n\t\t:rtype: str\n\n\t\tExample:\n\n\t\t\t>>> re.pattern_from_str('(1=3$)+', True)\n\t\t\t\"(1=3$)|(4$)\"\n\t\t\"\"\"\n\t\tregex = [] if regex is None else regex\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tdef regexp(pattern, string):\n\t\t\"\"\"\n\t\tThis method returns True iff statement should be removed from given string\t\"\"\"\n\t\ttry:\n\t\t\tr = re.finditer(pattern, string)\n            \n\t\texcept:\n\t\t\treturn False\n        \n        return next(r)\n\t\"\"\"\n\tThis method finds next occurrence of given pattern\t\n\tThe pattern should start and end with the given character.\t\n\tOptional parameters:\n\t\tsearchString (string) - the string to search\t\n\t\tstartIndex (int) - starting index of the pattern\t\n\t\tendIndex"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tcomp = re.compile(\"[a-z]\") \n\t\"\"\"\n\tpatterns = \"abcdeefghi\"\n\tpattern = re.findall(patterns, str)\n\tprint(pattern)\n\tpattern = re.compile(\"[a-z]\")\n\toutput = pattern.findall(str)\n\tprint(output)\n\t\"\"\"\n\tpattern = re.compile(\"[a-z]+\")\n\toutput = pattern.findall(str)\n\tprint(output)\n\t\"\"\"\n\tpattern = re.compile(\""}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tdef getRegExPattern(self, regExp: str) -> str:\n\t\tpattern = str()\n\t\tif (str(re.search(regExp, self.getInput()))!= \"<class 'None'>\").isalpha():\n\t\t\tpatterns = self.getInput().replace(\", \", \",\").split(\",\")\n\t\t\tif self.__regexExists(patterns):\t\"\"\"\n\t\t\t\tIf regex exists and there is not a pattern that matches, \n\t\t\t\tthis method searches the pattern list for a result, and if it exists, \n\t\t\t\tit replaces the entire pattern"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tdef regex_pattern_builder(self, re_obj):\n\t\t\"\"\"\n\t\tThis method returns the regex pattern.\n\t\t\"\"\"\n\t\tself.re_template = re_obj.PatternTemplate\n\t\tself.re_pattern = re.compile(self.re_template)\n\t\treturn self.re_pattern\n\n\tdef string_to_bool(self, value):\n\t\t\"\"\" \n\t\tThis method accepts an value from user.\n\t\t\"\"\"\n\t\tself.string_template = '(True|False)'\n\t\tself.string_pattern = re.compile(self.string"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tpattern = None\t\"\"\"\n\tThis is the default pattern when there is only 1 rule\t\"\"\"\n\tresult = None\t\"\"\"\n\tThis is the value returned when the given argument is found\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.__pattern = self.__create_pattern()\n\t\tself.__rule = None\t\"\"\"\n\t\tThis is the rule of the system that has been created\t\"\"\"\n\t\tself.__flag = 0\t\"\"\"\n\t\tThis flag is used to indicate the value state\n\t\t\"\"\"\n\n\tdef __create_pattern(self):\n\t\tpat ="}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t# print(re.findall(\".*@[^\\s]+?[^0-9]{16,}\", text))\n\n\treturn re.findall(\".*@[^\\s]+?[^0-9]{16,}\", text)\n\ndef convert_to_russian(text):\n\treturn re.sub('\\D', '', text)\n\ndef clean_text(text):\n\t\"\"\"\n\t:param text: \tRequired text, \n\t\tthe text to split\t\n\t:type text: str\n\t:return"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tpattern = ('^(?P<A>[0-9]{3}){3}[1-9]{7}(?=\\.|\\.\\!)(?P<B>[A-Za-z]{2,})[0-9]{3}$')\n\treturn re.findall(pattern, s, re.MULTILINE)\n\n#Method converts phone number to regex pattern\t\"\"\"\ndef formatPhoneNumber(s) :\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tpattern = ('0-?(?"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tpattern = '([^][^][^][[^][][^]]+])'\n\t#print pattern\n\tm = re.compile(pattern)\n\treturn m\n\n\n\"\"\"\t\tTHIS FUNCTION TO GET INDIVIDUAL PARTS of a given string\t\"\"\"\n#\t\n#\tinput = \"My name is Tiwit, and I live in Egypt.\"\n#\tpattern =''.join(re.findall('[A-Z][a-z]+', input))\n#\treturn pattern\n\n\"\"\"\t\tThis function returns the index of the word inside a"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tdef run(self, command, *args, **kwargs):\n\t\treturn subprocess.Popen(\n\t\t\tcommand, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n\t\t\tstdin=subprocess.PIPE, **kwargs).communicate()[0]\n\n\t\"\"\"\tCALL THE COMMAND\t\"\"\"\n\tdef call(self, command, *args):\n\t\treturn self.run(command, *args).decode(\"utf-8\")\n\n\t\"\"\"\tDEFAULT TESTING SCRIPT EXECUTION\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\toutlist = []\t\"\"\"\tInclude the stdout data as output from this call.\"\"\"\n\tprint(\"Execution is in progress.\\nPlease wait while the output will be displayed.\")\n\twith subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE) as process:\n\t\twhile process.poll() == None:\t\"\"\"\tWait on standard output, wait for a second after the process ends because there may be something wrong with the stdout and stderr data."}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tsubprocess.call(command, shell=True)\n\n\t\t# Print command\n\t\tprint(command)\n\n\t\t# Return result code\t\"\"\"\n\t\t# result = subprocess.run(command, shell=True)\n\n\t\t# Print result code\n\t\t# print(result)\n\n\t\t# Raise error\n\t\tif(result.returncode):\n\t\t    raise Exception('Command returned non zero')\n\n\tdef get_output(self, command: str) -> str:\n\t\t\"\"\"\n\t\tCall the given command(s) with self.env_var.\t\"\"\"\n\t\t# Try to get output"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tdef c_exec(self,command):\n\t\t\"\"\"\tGet output(s) of the given command and convert it to a string.\t\"\"\"\n\t\tpipe,stdout=self.os.popen2(command)\n\t\tself.output=str(stdout.read())\n\t\treturn self.output\n\t\"\"\"def c_exec_out(self,command):\n\t\t\"\"\"\tGet output(s) of the given command and convert it to a string.\t\"\"\"\n\t\tpipe,stdout=self.os.popen2(command)\n\t\tstdout=str(std"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tsubprocess.Popen(command, shell=True)\n\ndef run(project, command):\n\t\"\"\"\n\tRun the given command as normal if it finds a problem and a bug, \n\t\totherwise, it runs it in the right working directory and returns \n\t\tTrue.\t\"\"\"\n\toutput = subprocess.check_output(command, shell=True).decode()\n\tif str(output)!= str(problem):\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef print_result(problem, solution, output, file=sys.stdout):"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\treturn subprocess.run([cmd, stdout], cwd=stdin.name, shell=False).stdout.decode('utf-8')\n\n\ndef get_command_line_options(args, cmd):\n\t\"\"\" Returns a \"string\" of the options for the given command line  #2. Use it to call and run  #1. in a shell\n\t\t1. Add \"--\" to the command line (if it isnt already added, use it instead)\n\t\t1. For a given list of arguments, filter out the ones without the command line option, \t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tp = subprocess.Popen(\"python main.py \", shell=True, stdin=subprocess.PIPE)\n\tp.wait()\n\n#run_game() should be available in any Subgraph\nif __name__ == \"__main__\":\n\trun_game()\n\n#this is a little tricky; think about this:\n#\n#In many terms, a graph isn't a directed graph (or undirected graph), but it\n#can be represented using a directed graph without adjacency lists. As a\n#graph is a DAG, the components"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tsubprocess.call(['python',script, '--verbose'])\n\t\n\t\"\"\" \n\tConvert the output text to the data format expected for a table: \n\tIf a list is used to format, for example, lists of strings \n\tare converted to a single string separated by commas. \n\t\"\"\"\t\t \n\t# The line is separated in a list by a comma. \n\t# For example \"[1,2,,4]\" would split into [\"1\", \"2\", \"4\"] \n\t# The format string for the resulting string would be \""}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmd = \"nixos.cmd\"\n\texecutable = cmd + \" -E\"\n\tp = subprocess.Popen(cmd + \" arguments\",executable = executable, stdout = subprocess.PIPE, stderr = subprocess.STDOUT)\n\tprint(p.args)\n\tp.wait()\n\tprint(\"STDOUT:\", \"\\n\", p.stdout, \"\\n\")\n\texit()\n\ndef start_nixos(dir1):\n\t\"\"\"\n\tExecutes nixos and returns True or False.\t\n\tIt is"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif nb_arg.is_file():\n\t\tprocess = subprocess.Popen(args)\n\t\tprocess.connect()\n\telse:\n\t\tpass\n\treturn \"Successfull\"\n\nprocess_executor(\"echo\", nb_arg.is_file())\nif nb_arg.is_file():\n\tprocess_executor(\"cat\", nb_arg.is_file())\ndef test_pipelining():\n    import subprocess\t\"\"\"\n    Check your pipelining feature.\t\"\"\"\n    # Run pipeline without pipelining:\n\n\n    "}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tprint(isIP('127.0.0.1'), isIP('127.0.0.1.1'))  \n\ndef validates(ip_address):\n    \"\"\"checks if ip address is Valid\"\"\"\n    \n    #test for empty IP\n    test = False\n    try:\n        ipaddress.IPv4Address(ip_address)\n        test = True\n    except (ValueError,):\n        pass\n\n    #test for a valid IPv4 address\n    try:\n        ipaddress.IPv4Address(ip_address.replace('.'))"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\ttry:\n\t\tip = ipaddress.IPv4Address(ip)\n\n\texcept ValueError:\n\t\traise Exception(\"Please enter a valid IPv4 address\")\n\tIP = '127.0.0.1'\n\tIP = ip\n\tif (type(IP) is not int and type(IP) is not bytes) or type(IP) is float:\n\t\traise Exception(\"Please enter a valid IPv4 address\")\n\tip = str(IP)\n\ttry:\n\t\tip = ipaddress.IPv4Address(ip)\n\texcept ValueError:\n\t\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t#Check for invalid ip\n\tfor i in range (0, 4):\n\t\twhile(not ipaddress.valid_ipv4(ip[0] + 1*i)):\n\t\t\tip=ip+1*i\n\t\treturn \n    #Check for invalid addr\n\tfor i in range (0,8):\n\t\twhile not ipaddress.valid_ipv4 (ip):\n\t\t\tip+=2*i\n\t\treturn ip    \n\t\"\"\"\n\tTest to confirm that ip address \n\t\t\t\t\t\t\t\"\"\"\n\t#Check for valid ip\n\tif(1.0 >"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tdef test_IPv4_Add_Address(self):\n\t\t\"\"\"\n\t\tTest if IPv4 address or not\n\t\t\"\"\"\n\t\tip = ipaddress.ip_address(\"192.168.1.255\") # test IPv4 address\n\t\tip = ipaddress.ip_address(\"192.168.1.1\")  # test IPv4 address\n\t\tip = ipaddress.ip_address(1.1.1.1)  # test IPv4 address\n\t\t\t\t\t\n\tdef test_IPv4_Subnet(self):\n\t\t\"\"\"\n\t\tTest if IPv4 address"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\taddress = ipaddress.ip_address(\"192.168.0.1\")\n\tvalidTest = addrValid(address, 1)\n\tipTest = addrValid(address, 2)\n\tinvalidTest = addrInvalid(address, 1)\n\n\tif not validTest:\n\t\tprint(\"IPAddress test failed\")\n\t\tprint(\"Please try again with a valid IP address\")\n\t\texit()\n\telse:\n\t\tif not ipTest:\n\t\t\tprint(\"IPAddress test failed\")\n\t\t\tprint(\"Check your IP!\")\n\t\t\texit()\n\n\t\tprint(\"Address entered is valid"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tdef test_ip_in_network_addr_IPv4_network_addr_valid(self):\n\t\t\"\"\"\n\t\tTest if ip address in valid network\n\t\t\"\"\"\n\t\tnetwork_addr = ipaddress.ip_network('0.0.0.0')\n\t\taddr = '192.168.0.10'\n\t\tnet = ipaddress.ip_network(addr)\n\t\tassert net.hosts() == network_addr.hosts()\n\t\tassert net.is_link_local() is False\n\n\n\t\"\"\"Test if IPv4 address or not \n\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tip = \"\" ; num = 0\n\tfor i in range(len(str(str(ip)))-2): ip = ip + str(int(str(ip)[i])*2)\n\tip_int = int(ip)\n\tif ((ip_int>2147483647) or (ip_int==2147483648)): continue\n\ttry: \n\t\tip_float = struct.unpack('>d', socket.inet_aton(str(ip)))\n\t\ttry:  ip_int = socket.inet_ntoa( ip_float"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tif not isinstance(ip_address, ipaddress.IPv4Address) or ip_address.version!= 4 or not ip_address.is_loopback:\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef is_multicast_address(ip_address):\n\t\"\"\"\n\tTest if IPv4 address is multicast\n    \t\"\"\"\n\treturn isinstance(ip_address, ipaddress.IPv4Network) or ip_address.version == 4\n\ndef is_unicast_address(ip_address):\n\t\"\"\"\n\tTest if IPv"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tif ipaddr.ip_address('255.255.255.255')==ipaddress.IPv4Address('255.255.255.255'):\n\t\treturn 'IPV4_MULTICAST'\n\telif ipaddr.ip_address('255.255.255.0')==ipaddress.IPv4Address('255.255.255.255'):\n\t\treturn 'IPV4_MULTICAST_IPV4'\n\telif ipaddr.ip_address('255.255.0.0')==ipaddress.IPv4Address"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tdef check(host, name):\n\t\treturn False if ipaddress.ip_address(host).version == 4 else ipaddress.ip_address(host).is_global()   if  True else False    \n\ttry:\n\t\tsocket.net_if_addrs()\n\texcept:\n\t\treturn\n\tdef ifConfigPrint(host, name):\n\t\tname=name.replace('.', '-')\n\t\tprint(f\"[IN] {name}\")\n\t\tprint(f\"[OP] {host} is {ipAddress(name)}\")\n\t\treturn"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\t\ndef find_subclasses(graph, s, rdflib_filter = False):\n\t\"\"\"\n\tFind the subclasses\n\t\n\tinput:\n\tgraph = the RDF graph of some kind of type.\n\ts = the name of the RDFS subject\n\trdflib_filter = True to filter relations to match only rdfs.type\n\t\n\treturns:\n\t{ rdflib.RDFS.subClasses, rdflib.RDFS.subClassOf, rdflib.RDF"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\ts = []\n\tsdf = rdflib.Serialize(graph)\t\t\"\"\"\n\tIf the type of the object is RDFS.subClassOf, then the\nsyntax is to specify a class that is a direct child\nof a class that is a descendants of a class that is a parent\nof an object.\n\nSo if subClassOf is RDFS.subClassOf, then it is assumed that\nit is followed by a parent with the object as the value... or the object\nwill be in the root of the sub-class hierarchy.\n\nThis will allow"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\nimport rdflib # imported to do triple decomposition\nfrom utils import find_nearest, load_triples, triple_to_tuple\n\n\ndef find_nearest_parents(graph, triple):\n\tparent = triple_to_tuple(graph, triple)\n\tfor u in parent: \n\t\tif(graph.name2ndgeo_label(u).has_key('parent')):\n\t\t\tparent_label = graph.name2ndgeo_label(triple(u))[\"parent\"]\n\t\t\tparent_list = re.split("}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tgraph = rdflib.Graph()\n\tgraph.namespace_manager(n=n)\n\t# get the class of the instance. if this is RDFS.value,\n\t# then we need to add an RDFS:subject to type\n\tif prop == 'value':\n\t\ttype = 'value'\n\t\tif hasattr(type, 'graph_type'):\n\t\t\ttype = type.graph_type\n\t\tif type in rdflib.RDFS.classes:\n\t\t\ttype = rdflib.Namespace.parse(type)\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\nclass RootOfSubClasses: #class, for instantiating as a class\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_all(graph, rdflib_uri, sub): \n\t'''this function finds all parents for a set of (property, object\n\tsubset) triples in a graph    '''\n\tparents = set()\t\n\tfor triple in graph.triples((rdflib.RDFS.hasSubclassOf, sub, rdflib.RDFS.subClassOf)):\n\t\tfor parentNode in graph.node[triple[:-1]][0].parents():\n\t\t\tparents.add(parentNode)\n\treturn parents"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ng = rdflib.Graph()\ng.parse('skos.dbn.definition.ttl', format='turtle')\nroots = set(g.triples((None, None, None)))\nprint 'roots: \\n', roots\n\ng = rdflib.Graph()\ng1 = rdflib.Node('http://purl.org/siri/siri/ns#Node')\n\nfor s, p, o in g.triples((None,None,None)):\n\tprint's = ', s\n\tprint '"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t# print find_roots('rdfs:subClassOf/NCBITaxonomies:Classification', rdflib.RDFS.comment)\n\ndef find_roots_subclass(graph, rdflib.RDFS.subClassOf, rdflib.RDFS.subPropertyOf):\n\t\"\"\"\n\tFind the roots of sub-class hierarchies, such that a parent has\n\tall of its children. \n\t\n\tReturns a set of all the roots (with their sub-property's) of\n\tone of the sub-sub"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\td,c,o = extract(graph, rdflib.RDFS.subClassOf, rdflib.SKOS.broader)\n\tif isinstance(d, rdflib.URIRef) and isinstance(c, rdflib.URIRef) and isinstance(o, rdflib.URIRef):\n\t\treturn rdflib.Namespace(\"http://www.w3.org/2002/07/owl#type\", base=c)\n\telse:\n\t\treturn rdflib.Namespace(\""}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\t@staticmethod\n\tdef find_roots(graph, rdf_class):\n\t\t'''This method returns a set of all roots of an RDF\n\t\tClass of rdflib.Graph.'''\n\n\t\tnodes = list(graph.subjects(predicate = rdf_class))\n\t\tif len(nodes) >=2:\n\t\t\treturn nodes\n\t\telse:\n\t\t\treturn find_roots(graph, rdf_class)\n\t@staticmethod\n\tdef find_direct_parents(graph, rdfea):\n\t\t'''This"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tyaml.round_trip(c)\t\t\t\t\t\t\t\t\t\t\n\ndef csv_to_xcom(c):\t\t\t\t\t\t\t\t\t\t\"\"\"\t\"\"\"\n\t\"\"\"\tConvert csv file to xcom file\t\"\"\"\n\twith open(\"../temp/xcom.csv\",\"w\") as f:\t\t\t\t\t\t\t\t\t\t\t\t\n\t\ti=0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tfor line in c:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\tf.write(\"%s,%s\\n\" % (line[0],line[1]))\t\t\t\t\t\t\t\t\t\t\t\n\t\t\ti=i+1                                     \n\t\t\n\t"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tyaml.dump(vars(cfg),\n\t\t\tfile(output_fname, \"w\"))\n\ndef load_config(output_fname, default_file=None) -> dict:\n\t\"\"\"\n\t\tLoads the configuration file, defaulting a specific one\n\t\tif it cannot be found or the default file isn't provided\n\t\toutput: a dictionary\n\t\tdefault file: a py2-unicode object. If the default file has been loaded, a string.\n\t\"\"\"\n\n\t# Check py2-unicode format\n\ttry:\n\t\tyaml.safe_load"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\tdef __init__(self, configs:dict=None):\n\t\t# Read in the configuration file\n\t\twith open(configs) as stream:\n\t\t\ttry:\n\t\t\t\tself.__config = yaml.safe_load(stream)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(\"Cannot parse configuration file:\", str(exc))\n\n\t\t# Default options\n\t\tself.__d_config = {\n\t\t\t\"model\":None,\n\t\t\t\"optimizer\":None,\n\t\t\t\"device\": None,\n\t\t\t\"batch_size\":"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\"\"\"\n\tReturn the list of supported files\n\"\"\"\ndef getfiles():\n\t\n\t\"\"\"\n\t\tDump the contents of the config.yml.txt file\t\"\"\"\n\twith open(\"config.yml.txt\", \"r\") as myfile:\n\t\tlistfile = myfile.read()\n\t\tprint (listfile)\n\treturn listfile.split(\",\")\n\n\"\"\"\n\tConvert file path to a url\t\"\"\"\ndef convertfilepath(url):\n\t\n\t\"\"\"\n\t\tExtract the path to an.json or.yml file,"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef __init__(self, fname, mode='r', encoding='utf-8'):\n\t\topen = yaml.dump(fname, encoding=encoding, default_flow_style=False)\n\t\topen.close()\n\tdef getNode(self, node):\n\t\tdata = self.read(node)\n\t\tif data == False:\n\t\t\traise KeyError(node)\n\t\treturn data\n\tdef read(self, *args, **kwargs):\t\"\"\"Opener to read the current file line by line\"\"\"\n\t\topen   = kwargs.pop"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tparser = yaml.YAML()\n\tparser.default_flow_style = False\n\tparser.unicode_outer_script = False\n\n\tinput_list = parser.load_yaml(file_path, encoding=\"utf8\")\n\toutput_list = []\n\n\tfor idx, d in list(enumerate(input_list)):\n\t\tout_file_path = file_path + str(idx) + \".yml\"\n\t\tif os.path.isfile(\"%s\" % out_file_path):\n\t\t\t#print(\"exist"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n    \n\ndef dump_all_yaml(config):\n    \"\"\"\n    Dump YAML to file\n    @param\tconfig [yaml.YAMLObject] A Config object\n    \"\"\"\n    config_list = []\n    path_file = os.path.join(os.path.dirname(__file__),\"config_dump_yaml.tmp\")\n    os.mkdir(os.path.dirname(path_file))\n    with open(path_file, \""}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef __init__(self, filename, indent=2):\n\t\timport codecs\n\t\tcodecs.register_error('strict', self.raise_error_strict)\n\t\tself.indent = indent\n\t\tself.write = codecs.open(filename,'w','iso-8859-1').write\n\t\tself.encode = codecs.BOM_UTF8\"\"\"\n\n\t\t\"\"\"\n\t\tThe 'open' function returns a byte object that will be read \n\t\tinto this file object, and 'close' method is for this purpose.  \n\t\t\"\"\" "}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tyaml.dump(doc, stream)\t\"\"\"\t\"\"\"\t\"\"\"\n\t\n\tWrite a Python program to convert a 3-tuple as a 1d array\t\"\"\"\t\n\timport numpy as np\n\tmy_array = np.zeros((3,4))\n\tprint(my_array.shape)\t\"\"\"\t\"\"\"\t\n\t\n\tConvert a 2-tuple to a 1d list\t\"\"\"\t\n\tx,y = my_array\n\tprint(x,y)\t\"\"\"\t\"\"\"\n\n\ttuple_of_tuples = [1,"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\twith open(\"config.yaml\") as file:\n\t\tconfig = yaml.load(file, Loader=yaml.SafeLoader)\n\tprint(\"Config file exists:   {0}\".format(\"config.yaml\"))\n\tprint(\"Config file is a:     {0}\".format(\"config.py\"))\n\tprint(\"Config file is located in {0}\".format(config[\"location_to_dump\"]))\n\tprint(\"Config file dump is a:     {0}\".format(\"dump.py\"))\n\n\ndef config_dump(input):\n\tprint"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tformatter = logging.Formatter(\"%(levelname)s ::: \"%(name))\t\"\"\"\"\n\thandler1 = logging.FileHandler(\"log.txt\")\t\"\"\"\n\thandler2 = logging.StreamHandler()\t\"\"\"\t\"\"\"\n\thandler1.setFormatter(formatter)\r\n\thandler2.setFormatter(formatter)\t\"\"\"\"\n\tlogging.addHandler(handler1)\r\n\tlogging.addHandler(handler2)\t\n\tclass LoggClass(logging.LoggerClass):\n\n\t\tdef __init__(self"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tdef __init__(self,log_file='log.txt',log_level=logging.DEBUG):\n\t\t\"\"\"\n\t\tInitializes the logger. All log levels are defined here, not through configs\n\t\t\"\"\"\n\t\tself.log_level = log_level\n\t\tself.log_dir = os.getenv('LOG_DIR')\n\t\tos.makedirs(_log_dir(),exist_ok=True)\t# Make folders if not yet exist\n\t\tself.log_file = os.path.join( self.log_dir, log_file )\n\t\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__))) \t\"\"\"\tLocation of project folder\t\t \"\"\"\n\n\t\"\"\"\n\t\tFile logger.\n\t\tLogs to file is written to a log file.\n\t\tName of log file is configured with logger's name: __name__\n\t\tArgs: name\n\t\tReturn: file_name\n\t\"\"\"\n\tdef __init__(self,name):\n\t\t\"\"\"\n\t\t:param name: String of logger's name.\n\t\t\"\"\"\n\t\tsuper"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tdef __init__(self,\n\t\t\t\t default_level=logging.INFO, \n\t\t\t\t default_logfile=\"./app.log\"):\n\t\tself.logger = logging.getLogger(\"app\",logging.INFO)\n\t\tformatter = logging.Formatter(log_format)\n\t\t#self.default_logger_lzf = open(os.path.dirname(__file__) + \"/log/app_log.txt\",\"w\",encoding=\"utf-8\")\n\t\tfileHandler = logging.FileHandler(f\"./app."}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tdef __init__(self):\n\t\tself.level = {}\n\t\tself.level['DEBUG'] = logging.DEBUG\n\t\tself.level['INFO'] = logging.INFO\n\t\tself.level['WARNING'] = logging.WARNING\n\t\tself.level['ERROR'] = logging.ERROR\n\t\tself.formatter = logging.Formatter('[%(levelname)s %(asctime)s] %(module)s [%(funcName)s(): %(lineno)d|%(module)s %(message)s]', '%Y-%m-"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogging.basicConfig(level=logging.INFO,format=\"%(asctime)-15s [%(levelname)s] %(message)s\",filename=\"applog.log\")\n\n\"\"\"\tModule for the configuration file.  These configuration files are used to configure the application from a command line.   \"\"\"\ndef config(config_path,cfgfile):\n\t\"\"\"\tReturns a tuple of: `app_name, file_name` and a `config` dictionary containing the default values\t\"\"\"\n\tlogging.getLogger(\"logging.info\").setLevel"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nlogging.log(logging.DEBUG, logging.INFO) \"\"\"    Debug:\tDefault log level. 0.5 debug\"\"\"   \t\nlogging.log(logging.INFO, logging.WARN) \"\"\"    Warning:\tRedefined log level. 2 warnings\"\"\"    \t\nlogging.log(logging.WARN, logging.ERROR) \"\"\"    Warning:\tAdded extra messages to the log\"\"\""}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogging.basicConfig(level=logging.INFO)\t\t\"\"\"\n\tlogger.info(\"Logger enabled.\")\n\tlogger.debug(\"Logger enabled.\")\n\t\n\ttry:\n\t\t# Define the log file name\n\t\tlog_file = os.path.expanduser('~/Documents/PyProjects/sirata_logs/log.txt')\n\t\t# Create the logger\n\t\tmyLogger=logging.getLogger('sirata_logger')\n\t\t# Configure the logger\n\t\tmyLogger.setLevel(log"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t# Set a log handler (for other things)\n\tlogger = logging.getLogger('Dynamict_test_logger')\n\t# Set a log level\n\tlog_level = logging.INFO\n\t# Create file handler\n\thandler = logging.FileHandler(os.path.join(fname, f'{log_name}.log'))\n\thandler.setLevel(log_level)\n\tfmt_str = mpl.dates.WeekdayLocator(byweekday=[0, 1, 2, 3, 4])\n\tformatter = m"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\tdef __init__(self,logfilename,logLevel):\n\t\t\"\"\"constructor for base class, log for file system operations\t\"\"\"\n\t\tself.logName=logfilename\n\t\tself.logLevel=logging.DEBUG\t\n\t\tif (logLevel.lower() in [\"info\",\"verbose\",\"debug\"]):\n\t\t\tself.logLevel=logging.INFO\t\n\t\telse:\t\n\t\t\tself.logLevel=logging.INFO\t\t\n\n\t\tself.logfile=None\t\n\t\tself.logDir=None\t\n\t\tself.logFname=None\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\treturn np.ndarray (shape, dtype = dtype)\n\ndef generate_matrix(x):\n\t\"\"\" creates a matrix with shape and dtype\n\tParameters\n\t----------\n\tshape : tuple\n\t\tshape of the matrix to create\n\\ntype of the created matrix\t\t\"\"\"\n\tx = np.array (x, dtype = 'int64' )\n\tm = len (x) // 2\n\tshape = (m, m)\n\tdtype = 'int64'\n\t# if len(x) % 2 == 0 :\n\t# \t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tdef __init__(self, shape, dtype):\n\t\tself.shape = shape\n\t\tself.dtype = dtype\n\n'''\ndef create_array(shape,dtype):\n    array = np.zeros(shape, dtype=dtype)\n    array.setflags(write=False)\n    return array\n\n \n'''\n \n \ndef to_int(value, out_dtype):\n    value = int(np.where(np.ceil(value>0.), value, 0))\n    return value\n\ndef create"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tarr = np.zeros(shape,dtype=dtype)\n\tarr = np.array(arr,dtype=dtype)\n\treturn arr\n\n\ndef to_dataframe(arr,dtype=None):\n\t\"\"\"\n\tConverts a array into a Pandas dataframe, including a column with the \n\tarray dtype for the data.\n\nParameters\n----------\n:arr : An array or dataframe name to be created from \n    the output of Dataframes(fluxes,etc., etc.) \n    or a Pandas dataframe/array"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\"\"\"\n\tCreate an array and set data type. This function creates and returns and array type with data type.\n\n..codeauthor:: Alexandre Rouxian\n\"\"\"\n\t\n\tarray = np.zeros((4, 4), dtype='int')\n\treturn array\n\t\n\n\"\"\"\ndef array_constant():\n\tarray  = create_array_for_numpy()\n\ta = np.array(array)\n\treturn a[:,[2,4]]\n\t\"\"\"\n\tFunction creates a table with the dimensions and data type with a defined number"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t# def create_2darray(self):\n\t\"\"\" Function to create two dimensional array\n\t\t\n\t\tParameters\n\t\t----------\n\t\tself : `numpy.ndarray`\n\t\t\tarray for which we are creating a 2x1 array.\"\"\"\n\n\n\n\t# def create_3darray(self, shape):\n\t\t\"\"\"Function to create 3x3x3 arrays \n\t\t\n\t\tParameters\n\t\t----------\n\t\tself, shape : \n\t\t\tshape of the 3 array to be created\"\"\"\n\n\n\tdef create_3_by_3_array(self, shape):\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tnshape = len(shape)\n\t\n\tif(nshape==1 or len(shape)==1):\n\t\tarr = np.zeros(shape,dtype=dtype)\n\t\treturn arr\n\tif(nshape==2):\n\t\tarr = np.zeros((shape[0],shape[1]),dtype=dtype)\n\t\treturn arr\n\tif(nshape>2):\n\t\tarr = np.zeros((nshape[0],nshape[1],shape[1]),dtype=dtype)\n\t\treturn arr\n\traise Value"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\treturn np.ascontiguousarray(np.zeros(shape, dtype))\t\"\"\"\nParameters\n----------\nshape : tuple\n    shape of the shape array to create\ndtype : `numpy.dtype`\n    data-type of the shape array to create\t\"\"\"\n\treturn np.zeros((shape[0], shape[1]), dtype)\t\t\"\"\"\nParameters\n----------\nshape : tuple\n    shape of the shape array to create\ndtype : `numpy.dtype`\n    data-type of the shape array to create\t\"\"\"\t\ndef fill_"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\tdef __init__(self, shape, dtype):\n\t\tself._numpy = np.empty(shape, dtype)\n\n\tdef __len__(self):\n\t\treturn self._numpy.size\n\n\tdef __getitem__(self, key):\n\t\tif not isinstance(key, tuple):\n\t\t\traise TypeError(\"Argument `key` must be a tuple.\")\n\t\tif len(key)!= 2:\n\t\t\traise ValueError(\"Position must contain exactly 2 arguments.\")\n\t\n\t\treturn np.array([self._numpy[*key"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tn=len(shape)\n\tif n<2:\n\t\traise ValueError(\"Shape must have at least two dimensions\")\n\td=np.dtype(dtype)\n\tres=np.zeros(tuple(shape),dtype=d)\n\ta=type(d,(tuple(),d.type),(int,d.type,d.type,np.void,object))\n\tarr=np.array(res,dtype=a)\n\tarr.shape=(*x for x in shape)\n\treturn(arr)\n\ndef"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef lhs(shape):\t\"\"\"\n    Create a 1D-array with given shape.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n    shape of the array to create\n\n\tReturns\n\t-------\n\tdata : `array`\n\t\t\t 2D array representing array\n    \"\"\"\n\n\tN = shape[0]\n\t# create an uniform array in range 0,1 with N values\n\trandomArray = np.random.rand(N)\n\t# convert this array to a 2D matrix, reshape it to have the correct DIMENSIONS\n\tarray ="}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tcx=cx0-2;cy=-2;cx0=2;cy0=0\t\"\"\n\t#x-coordinate \t\ty-coordinate center line\t\n\t#x0.5, x1.5,x2.5,x3.5\ty0.5,y1.5,y2.5,y3.5\t... \n\t#center 1st 3d vector - x0-1, y0-2, x1-3, y1-4\n\tx1=x0+1;y1="}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tdef _cdf(x):\t\"\"\"\t\n\tCDF of a Gaussian centered around 0.2 with a sigma of 0.1 -\n\tcalculate it from the given x value.\t\"\"\"\t\n\treturn 0.2 + (0.2*np.exp(-(x**2)/0.1))**0.5\t\t\"\"\"\ndef _cdf2(x):\t\"\"\"\t\n\tCDF of a Gaussian centered around 0.2 - calculate it from the given x value.\t\"\"\"\t\n\treturn 0.2 + (0.2*np"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\"\"\"\n\tGaussian centered around 0.07 with a sigma of 0.01.\t\"\"\"\n\t\"\"\"\t\n\tGenerate a dataset of 0,1 values ranging from a standard normal distribution \n\tcentered around zero and above zero.\t\n\t\n\"\"\"\tfrom scipy.stats import norm\n\t\"\"\"\n\tCreate a (n_trains, n_feat) array storing the random values of \n\tthe distribution\t\n\t\"\"\"\tfrom numpy import random\n\t\"\"\" create a gaussian distribution\t\"\"\"\t\n\tx = np.random.normal(0,1"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tfrom scipy.special import iv as bi_iv\n\tfrom scipy.special import hankel1, hankel2\n\tclass Bounded_Gaussian():\n\t\t\"\"\"\n\t\t\tBounded Gaussian centered around 0.3 with a sigma of 0.3.\n\t\t\tIt's just the original Bounded Gaussian.\t\"\"\"\n\t\tdef __init__(self, min, max=None, sigma=.2):\n\t\t\tself.min = self.check_type(min)\n\t\t\tself.max = self.check_type(max)\n\t\t\tif"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tfrom matplotlib import gridspec\n\timport matplotlib.pyplot as plt\n\timport scipy.constants\n\t\n\tfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\timport matplotlib.patches as mpatches\n\t\n\tf = scipy.constants.physical_constants['atomic mass'][0]\n\tm_e = scipy.constants.kilogram_amu/f**3\n\tm_e /= scipy.const"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n# TODO (try both ways):\n\nTARGET_IMAGE_EXTENSION = \".jpg\"\nTARGET_IMAGE_EXTENSION_NUMERIC = \".png\"\nTARGET_IMAGE_NAME = \"target.png\"\n\n\nfrom collections import Counter\nfrom operator import itemgetter\n\nimport random\nfrom sklearn.model_selection import train_test_split\n\nclass RandomDataset():\n\n\tdef __init__(self):\n\t\tprint (self)\n\n\t# TODO\n\t# This takes a 2d array as input, where"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tdef __init__(self,center=0.2,sigma=0.1):\n\t\tself.center = center\n\t\tself.sigma = sigma\n\t\tself.mean = 0.9*center\n\tdef __call__(self,images, labels):\n\t\timages = np.array([self.center + (self.sigma*self.gauss(i-self.center,self.sigma,self.sigma)) for i in images]).reshape(-1,1)\n\t\tlabels = np.array([-1 if i == -1 else"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\"\"\"\n\tG_1= exp(-\\sigma^2/2)*exp(-(x-mean)^2/(2$ sigma^2)) +\\sigma^2 sigma^2 +.5(x-mean)^2sigma^2 +.2(x-mean)^2/2sigma^2\n\t\"\"\"\n\tdef __init__(self, mean, sigma):\n\t\t\"\"\"\n\t\t:param mean: mean of the gaussian\n\t\t:param sigma: stds of the gaussian\n\t\t\"\"\"\n\t\tself.mean = mean"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tGaussian = np.random.normal(loc = 0.2, scale = 0.1,  size = X.shape) \t\"\"\"\n\tGenerate data within 50% range.\n\n\t# Parameters:\n\t\tseed_value\tNumber of the random seed to use.\n\t\n\t# Output\n\t\tX: (N_samples, n_features) ndarray \n\t\tof values from the Gaussian distribution\n\t\tcentered on the original feature space.\n\t\n\t# Returns\n\t\tX: (N_samples, n_features) ndarray \n\t\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\tdef __init__(self,a=0.2,sigma=0.1,n0=100,a0=0.3):\n\t\"\"\"\n\n\n\t\"\"\"\n\n\tself.a=a\n\tself.sigma=sigma\n\tself.n0=n0\n\tself.a0=a0\n\t\t\n\tself.sz=self.n0*np.sqrt(self.sigma**2*(np.random.random(len(self.a)))**(1/2))\n\tself.n0=len"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogging.basicConfig(format = '%(asctime)s %(levelname)-8s %(message)s', level = logging.INFO)\n\tlogger = logging.getLogger(__name__)\n\tconfigs = [f for f in os.listdir(config_dir) if os.path.isfile(os.path.join(config_dir, f))]\n\t\n\tif is_test:\n\t\tconfigs.sort(key=lambda f: len(os.path.join(config_dir, f)))\n\t\t\n"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tdef validate_sequence(seq):\n    \t\t\"\"\"\n        Given a sequence of config files for a model, see if the files validate themselves.\n        Each item within the sequence is a sequence of config files, one per line.\t\n        For example, the content of each \"server.yaml\" file in the filesystem is:\n            ---\n            # Generated on Thu Feb 25 14:16:44 2018 with 0 errors and 0 warnings.\n            ---\n    \t            server.yaml:\n    \t\t\t\tcommands:\n            - get_config\n            - print_configs\n            "}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogging.basicConfig(level=level, format=formatter)\t\"\"\"\n\tSet the log level of the application.\"\"\"\n\tlogger = logging.getLogger()\t\"\"\"\n\tCreate a logger object for the application and set it up.\"\"\"\n\tlogger.setLevel(logging.INFO)\t\"\"\"\n\tLog the message at INFO level.\"\"\"\n\thandler = logging.StreamHandler()\t\"\"\"\n\tCreate and configure the handler for printing information to the console.\"\"\"\n\tconsole = logging.StreamHandler()\t\"\"\"\n\tConfigure the logging handler to print to"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\nimport os\nfrom os import path\nimport string\nimport sys\nfrom typing import NamedTuple, Dict\nimport subword_nmt\nimport numpy as np\nimport json\nimport xml.etree.ElementTree, xml.sax\nfrom.constants import DEFAULT_BUCKET, HOME_DIR, S3_MISSING_NODES\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__) #get the root logger\n\n#config['bucket'] = DEFAULT_B"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t# Setup logging\n\tlogFormatter = logging.Formatter(\"%(asctime)s - %(levelname)s - (%(module)s:%(funcName)s(%(lineno)d)) - %(message)s\")\n\tlog = logging.getLogger(__name__)\n\tlog.setLevel(level=logging.DEBUG)\n\tstdout_handler = logging.StreamHandler()\n\tstdout_handler.setFormatter(logFormatter)\n\tlog.addHandler(stdout_handler)\n\n\tresult: D"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\nfrom typing import List\t\t\t\t\t\t\t\t\t\t\t\t###\nfrom pysql.models import (\t###\n\tConfigDict,\n\tCreateConfigObject,\n\tCreateSequenceObject,\n\tQueryResults,\n\tParseError,\t\n\t)\nfrom pysql.schema import (\t###\n\tDatabase,\n\tTable,\n\tSchema,\n\tIndex,\n\tColumn,\n\tFiler,\t\t\t\t\t\t\t\t\t\t\t\t#\n\t)\nfrom pysql.data import (\t###\n\tQueryBuilder,\n\tSQLStatement,\t\t\t\t\t\t\t\t\t\t\t#\n\tPostgreSQLConnection,\n"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogging.basicConfig(format=logRecordFormat,level=logging.NOTSET,filename=logFilePath)\n\t\n\t# parse the logging config and return the relevant variables\n\tparsedResults = parseLoggingConfig(logFilePath)\n\t\n\t# exit the program if there were parsing errors and return the results\n\tif parsedResults[:2]!= (True, None):\n\t\tlogging.error(\"Parsing error on line %d, column %d.\", parsedResults[2].filename.lineno, parsedResults[2].levelno)\n\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tfilename2conf = {'config-%s' % k: k for k in config}\n\tfilename2conf.update([(filename.split(',')[0], filename)\n\t\t\t\t\t\t for filename in config])\n\n\tparser = YAML()\n\tparser.add_constructor('tag:yaml.org,2002:yaml', load_yaml_from_file_context)\n\n\tconfig = parser.parse(logging.root.handlers)['LogRecord']\n\n\tfilename2conf['config'] = list(config.keys())\n\tfilename2conf['"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogging_output = {}\n\terror_msg_format = ('Error config \"{}\" at line {}\\n')\n\terr = 0\n\tfor filename in conf:\n\t\tif conf[filename].get('error'):\n\t\t\tlogging_output[filename] = {'log': [], 'ok': 0}\n\t\t\tcontinue\n\t\tlogging_output[filename] = {'log': [], 'ok':0}\n\t\terr = 0\n\t\ttry:\n\t\t\tfor rec in parse(conf[filename]):\n\t\t\t\ttry:\n\t\t\t\t\tvalidate_line(rec)"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tfrom os.path import join, expanduser, expandvars, normcase, normpath, isdir, expanduser, isdir \t\"\"\"\t\n\tExpected input: 'D:\\python\\scripts\\autogen\\test_example.cfg'\n\t>>> isdir('D:\\github\\autogen\\test_example.cfg')\n\tTrue\n\t>>> expanduser('D:/') == '\\c:\\'autogen'\n\tTrue\n\t>>> expandvars('${' + ':' + expanduser('/.ssh/') + '}')!= '.'\n\tTrue\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\treturn numpy.dtype(str)(bytes(obj, encoding=\"latin-1\")).data.tobytes()\t\"\"\" Return a byte string, or a str object\n    \"\"\"\n    \ndef read_pkl_file(file_handle, file_type = \"r\"):\t\"\"\"\n\tThis function reads bytes objects in text format containing a binary or text \n\theader or a compressed or uncompressed file.   \t\"\"\"\n\t\n\tfile_handle.seek(0)\n\t\n\tif type(file_type) == type(\"\"):\n\t\tfile_"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif type(obj) is bytes:\n\t\treturn obj\n\telif type(obj) is str:\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\traise TypeError('obj must be a bytes object or str object')\n\ndef from_to_int(pkt, length):\n\t\"\"\"\n\tThis function converts a data packet of length ``length`` contained in it, to an\n\tarbitrary integer number. The number returned is always an integer.\t\"\"\"\n\tvalue, = unpack(pkt, length)\n\treturn value\n\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\tif type(obj) == str:\n\t\t\n\t\treturn numpy.bytes_(obj)\n\t\n\tassert type(obj) == bytes or type(obj) == numpy.bytes_\n\n\tret = numpy.asendobj_(obj,dtype = 'b|c|S1|U1')\n\n\treturn ret\t\n\t\ndef uid(obj):\n\t\n\t\"\"\"\n\tReturns a UID representation of the passed object.  If the object contains a\n\t``UID`` property value, then the value contains the UUID value.  If it"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif type(obj) is str:\n\t\tfor c in obj:\n\t\t\tdata += bytes([int(ord(c))])\n\telif type(obj) is unicode:\n\t\tfor c in obj:\n\t\t\tdata += bytes([int(ord(c))])\n\telif type(obj) is list:\n\t\tfor idx in range(len(obj)):\n\t\t\tdata += bytes([int(ord(obj[idx]))])\n\telif type(obj) is bytes:\n\t\traise TypeError(\"This data structure is not supported\")\n\tel"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif not isinstance(obj, _unicode_type):\n\t\traise TypeError('str.encode(obj) takes only Unicode objects (Python 3); '\n\t\t\t\t+ 'You passed in {}'.format(type(obj)))\n\n\t# If decoding input by UTF-8 fails, the last 2 bytes of obj are\n\t# treated as the error control characters that the Python parser\n\t# cares about. Since they can contain characters from 1 to 128,\n\t# the replacement character is the one that comes next. \n\tif unicodedata.category(obj[:2])[0"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\ttry:\n\t\treturn obj.encode('utf-8') # <1>\n\texcept AttributeError:\n\t\treturn obj # <2>\n\texcept TypeError:\n\t\treturn str(obj) # <3>\n    def __len__(self):\n        \"\"\" \t\"\"\"\n        try:\n            return len(self.data) # <4>\n        except AttributeError:\n            return 1 # <5>\n    if sys.version_info >= (3, 3):\n        def __bool__(self): # <6>\n            try:"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\treturn asstring(numpy.asarray(obj, dtype='S1').tobytes())\n\n\nif __name__ == \"__main__\":\n\t# A very simple test function.\n\tprint \"hello\"\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif type(obj) == str: return bytes(obj, 'utf-8') # Note that str was passed in, not bytes\n\t# The encoding argument defaults to locale.getpreferredencoding() if no \n\t# other encoding is passed. \n\t# If you run the following one of the following command line with \n\t# Python before 2.7 you will get an error and use locale.setpreferredencoding \n\t# (setdefault to None) that doesn't provide a way to get current locale encoding \n\t# which will then fall back to UTF-8"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\tif\tisinstance(obj, str):\t\t\"\"\"\n\t\tThe function returns the string object corresponding to \n\t\t``obj`` in case it is a string using UTF-8. \t\"\"\"\n\t\t\n\t\treturn str.encode(obj, encoding='utf-8')\n\t\n\treturn obj\n\ndef to_bytes(obj,*args,**kwargs)\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj`` \n\tin case it is a byte list using UTF-8. Note that if an argument\n\tis used to pass an integer it will"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\treturn obj.encode('utf-8') \t\"\"\"\n\t``bytes_obj`` object can be passed directly into open() or.read() \n\tfunctions by using ``bytes()`` function.\n\n\tIn case you are reading a string use ``bytes.decode('utf-8'`` \n\tstring format.\t\t \t\"\"\"\n\ndef obj2bytes(obj):\n\t\"\"\"\n\tThis returns the bytes object corresponding to ``obj`` \n\tin case it is a string object using UTF-8. \t\"\"\"\n\treturn obj.encode('utf-"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\ttime = ticks * 10\n\tsignBit = -1 if time < 0 else 1\n\ttime *= 1000\t\n\ttime = int(abs(time) / 1000)\n\tmid = time // 86400000000000\n\tticks = mid * 10\n\tsignBit = -1 if ticks < 0 or ticks > 86400000000000 else 1\n\tticks = int(abs(ticks) / 500)\n\tzone = 0\n\tif ticks == 0: return Time(\"00:00:00\")\n\tif ticks == 1: return Time(\"01:00:00\")"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\timport datetime\n\tif not tz:\n\t\ttz=datetime.timezone.utc\n\tif not ticks:\n\t\traise ValueError('Ticks must be positive finite; got zero or zero ticks.')\n\treturn Time(int(ticks//86400000000000), tz=tz)\n\n@implementer(IRealTime)\nclass RealTime(timedelta):\n\ttzname = 'UTC'\n\n\tdef __new__(cls, x, *args, **kwargs):\n\t\t\"\"\"\n\t\tArgs:\n\t\t\tx (int|float"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\tticks = _string_to_ticks(ticks)\t\t\t\t# converts string ticks to ticks\n\tif ticks < 2400000:\n\t\traise ValueError(\"time is too small (0 <= ticks < 2400000)\")\n\telif ticks >= 86400000000000:\n\t\traise ValueError(\"time is too big (8640000000000 < ticks > 2400000)\")\t\n\t\n\t# create our time object using the integer of the ticks\n\t# \twhich was converted to a datetime.timedelta obj\n\treturn Time(ticks, tz)\t\t\t\t\t\t\t\t\t#"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"ticks should be bigger than 0\")\n\treturn cls(hours = (ticks // 1000000000) % 24, minutes = ticks % 1000000000 // 1000000)\n    \ndef timestamp_to_datetime(timestamp, tz=None, date_fmt='%Y-%m-%d %H:%M'):\n\t\"\"\"\n\tConvert timestamp to datetime.datetime class.\n\n\t:param timestamp: timestamp\n\t:param tz: timezone\n\t:param str date_fmt:"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\tif tz is None:\n\t\traise ValueError('tz is necessary')\n\t\t\n\treturn Time.from_ticks(ticks, tz=tz)\n\t\nclass Time2D(Time):\n\t\"\"\" \n\t\tTime2D[h, m, d] = Time2D(h, m)\n\t\tTime2D2 = (Time3D[h * 24 + m * 60 + d], Time3D[h * 24 + m * 60 + (d + 1) * 24])\n\n\t\tAll time-related attributes (i.e., h"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif ticks < 1000 or ticks > 86400000000000:\n\t\traise ValueError(\"ticks must be between 1000 and 86400000000000 seconds long\")\n\telse:\n\t\treturn cls(*get_ticks_from_sec(ticks, tz))\n\n\n_NOMINAL_SEC = 0\ndef get_num_sec_from_n_nanoseconds(v):\n\t\"\"\"\n\tConvert numeric v for calculation of a single seconds.\n\t:param v: numeric v\n\t:type v: int\n\t:rtype: number\n\t\"\"\"\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be positive\")\n\tif ticks >= 86400000000000:\n\t\traise ValueError(\"ticks must not be greater than 86400000000000\")\n\tdelta = ticks/86400000000000\n\tif delta < 1:\n\t\traise ValueError(\"ticks must be non-negative\")\n\treturn Time(delta, int(tz.tzoffset(None, -28800*delta)),\n\t\tint(tz.tzoffset(None, -18000*delta)), \"EST\",\n\t\tdelta*86400000000"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif hasattr(ticks,'minute'):\t# this is just to handle months 001 or 002\n\t\t# for 1 hour, always set to the previous minute, as it will be set\n\t\t# for 12h, but we only want the minutes\n\t\tif ticks < 1000 * 60*60 and ticks > 0 * 60 * 60 and ticks % 24 == 0:\n\t\t\tticks -= (60 * 60) + 1\n\n\tminutes = ticks // 8640000000000\n\tfor hour in range(24):\n\t\tif ticks < (8640000000000 + 1000 * (hour"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tfrom dateutil.tz import gettz\n\tfrom datetime import timedelta\n\tif not ticks:\n\t\traise ValueError('Ticks was out of bounds')\n\tif ticks < 86400000000000:\n\t\t# Default values\n\t\ttz = gettz('UTC')\n\ttry:\n\t\t# Return original tz.\n\t\tticks = int(ticks)\n\t\treturn cls.from_days(ticks, tz)\n\texcept ValueError:\n\t\t# Set offset as epoch + ticks\n\t\treturn time.from_epoch(epoch) + timed"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\treturn cls( from_ticks(ticks, tz), \"LAT\", tz)\n\t\nclass RelativeTime(Time):\n\t\"\"\"\n\tRepresents ticks-relative (upwards from base epoch) time.\n\n\t:cvar REFRESH_LIMIT: time period for data refresh\n\t:cvar REFRESH_FREQUENCY_SECONDS: resolution of time ticks for refresh\n\t    (default :attr:`REFRESH_FREQUENCY_SECONDS`)\n\t:cvar REFRESH_TEMPLATE"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_versions = ['http/1.0a', 'http/1.0b', 'http/1.0c', 'http/1.0d']\n\telse:\n\t\tif (protocol_version is not None and\n\t\t\tnot (1 <= protocol_version <= 10)) or (cls.is_protocol_version(\"http/1.0\" in protocol_versions)):\n\t\t\tmessage = (\n\t\t\t\t\"Failed to import Bolt protocol version %r, supported versions \"\n\t\t\t\t\"are (http/1."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t# check if only a single protocol supported\n\tsupported = [version[0] for version in protocol_handlers.protocol_versions()]\n\tif (protocol_version == () or\n\t\t  (protocol_version in supported and ((protocol_version!= (0, 0, 1, 0))) or (protocol_version!= (0, 1, 4, 6)))):\n\t\treturn protocol_handlers.protocol_versions()[protocol_version]\n\telse:\n\t\treturn {}\n\n\nprotocol_version_handler_classes = dict(protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocol_handlers = {}\n\tif protocol_version:\n\t\t# First, check to see if the client support\n\t\tif protocol_version[:2] == (3, 5):\n\t\t\tprotocol_handlers[\"3.5\"] = cls\n\t\telse:\n\t\t\traise TypeError(\"protocol version=%r not supported\" % protocol_version)\n\t\treturn protocol_handlers\n\telse:\n\t\treturn protocol_handlers\n\ndef connect_bolt_client_to_client(conn):\n\t\"\"\"\n\tConnect a Bolt connection class to a Bolt client"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\traise NotImplementedError(\"Protocol handlers for Bolt protocol version %u not implemented yet by a handler's class\" % str(protocol_version))\n\n__author__ = \"Erik Eberhardt <ef@syz.fi>\"\n__docformat__ = \"restructuredtext\"\n\n#----------------------------------------------------------------------------\nclass IProtocolHandler(object):\n\t\"\"\" An interface for implementing Bolt protocol handlers to be used with\n\tthe :class:`ProtocolHandler`.\n\t\"\"\"\n\n\tdef __init__(self, config):\n\t\t\"\"\" Initialize implementation state. \"\"\"\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\timport sys\n\tif protocol_version is not None:\t\n\t\tif len(str(protocol_version)) == 1:\n\t\t\tprotocol_version = (0,)\n\telse:\t\n\t\ttry:\n\t\t\tprotocol_version = sys.version_info[:len(protocol)][protocol.lower()]\t\n\t\texcept AttributeError:\t\n\t\t\traise TypeError(\"Need protocol version tuple\")\n\t# noinspection PyTypeChecker\n\tif protocol_version[1]+1 == len(protocol):\n\t\tfrom protocol import *\n\telse"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_versions = tuple(range(0,256))\n\telse:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol_version is not a tuple\")\n\t\telse:\n\t\t\tif not all(v == protocol_version[0] and v in protocol_versions\n\t\t\t\t\t    for v in protocol_version[1:]):\n\t\t\t\traise TypeError(\"invalid protocol version\")\n\t\t\tprotocol_versions = protocol_version\n\t\t\tif \".\" in protocol_version[1]:\n\t\t\t\t#"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_versions = ()\n\telif isinstance(protocol_version, tuple):\n\t\tprotocol_versions = tuple(protocol_version)\n\telif protocol_version is not None:\n\t\traise TypeError('protocol_version can be None, a tuple or a dict')\n\tcls_ = cls.__class__\n\tcls_attributes = {}\n\tfor module_name in cls_.__dict__:\n\t\tmodule = cls_.__dict__[module_name]\n\t\tif type"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocol_handlers = {}\n\tfor protocol in BOLT_L3_TYPES:\n\t\tprotocol_handlers[(protocol,)] = cls[protocol]\n\tif protocol_version is None:\n\t\treturn protocol_handlers\n\tfor protocol_version in protocol_version:\n\t\ttry:\n\t\t\tprotocol_handlers[protocol_version] = cls[protocol_version]\n\t\texcept KeyError:\n\t\t\tpass\n\treturn protocol_handlers\nDEFAULT_HANDLER = protocol_handlers()\n\n\ndef"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t# first check if protocol is supported\n\t# then get all supported versions\n\t_ = protocol_version and (1, 0) in Protocol.supported_versions\n\tif not protocol_version:\n\t\t# if no protocol is passed in, do we\n\t\t# want to use the first version of Bolt\n\t\tprotocol = protocol_versions[0]\n\telif protocol_version == 0 and _:\t# 0, empty tuple for protocol version\n\t\tprotocol = protocol_versions[0]\n\telif len(protocol_version) == 0:\t# else tuple only for"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tb = cls.__name__\n\treturn {}      # empty dictionaty, since it's not interesting to use it\n\t#\n\n@b.protocol_names\nclass Protocol_Handler:\n\tprotocols = \"\"\"\\\n\t  \n   T1: Protocol Name (type)  +  Bolt Protocol Handler + \"Handler\"       + \\ \n       a protocol handler class                     +  for class          \n    T2: \"Handler\" class name (type) +  class     protocol handler class name   \n\tT3: None                            protocol class name         "}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\ttry:\n\t\treturn cls(\t# type: 'Bookmark'\n\t\t\tfilename=cls.make_filename(\t# type: bytes\n\t\t\t\tbase64.b64decode(values[0]), \t# type: str\n\t\t\t\tencoding=sys.getfilesystemencoding(),\n\t\t\t),\n\t\t)\n\texcept ValueError as e:\n\t\traise ValueError(\n\t\t\t\"bookmarks values have wrong syntax: {}\\n\\nFor a valid \"\n\t\t\t\"Bookmark's filename it should be:\\n\"\n\t\t\t\"    base64://\\t\"\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tnamespace, user_name, bookmark_name = values[:4]\t\n\treturn cls(namespace, user_name, bookmark_name)\t\n\t\n\t\nclass BookmarksManager(SearchManager, BookmarkManager):\n\t\"\"\" A search manager that takes a list of Bookmarks (as returned\n\tby :py:func:`.get_raw_values` on a user's books webpages).\n\t\"\"\"\n\t\n\tBOOKMARKS_KIND = 'bookmarks'\t\n\t\n\tdef get(self, user, bookmark):\n\t\tif type"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn _cstruct.extract_obj(cls, __Bookmark, values, obj_cls=__Bookmark)\n\nclass CAClass(cls):\t\"\"\"\n\tCAClass is an abstract base class for classes that are registered with\n\tthe Database API.  It should not be instantiated directly.\n\nIt should only be provided by subclasses.  CAClass-specific implementations\nshould implement all of these methods instead of the standard\n__str__() and __repr__() methods implemented in subclasses.\n\n\tYou shouldn't use this class directly.  "}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tfrom..values import is_bookmark_raw\n\tfrom.bookmarks import Bookmarks\n\n\tbookmarks = Bookmarks()\n\tif not is_bookmark_raw(values):\n\t\traise BookmarkValueException('Invalid values!')\n\tvalues = [v.strip() for v in values.split('\\x00')]\t# Strip out '\\x00'\n\n\t# This is done here, even though it would be nice to have it in\n\t# the Bookmarks class, for performance reasons :-)\n\tfor i, value in enumerate(values):\n\t\tvalues["}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tself.title = str(values[0].replace(' ', '').replace('=', '').replace('\\n', ''))\n\tself.user = values[15]\n\tif len(values) >= 21:\n\t\tself.bookmark_list = self.__deserialize(values[21])\n\telse:\n\t\tself.bookmark_list = [Bookmark()]\n\tif len(values) >= 23:\n\t\tself.status = BookmarkStatus(values[23])\n\telif '0' <= values[23] < '9':\n\t\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tassert(isinstance(values, Iterable))\n\tif not any(x.startswith('http') for x in values):\n\t\traise ValueError('Values not in form of http://<host>:<port> (got %r)' % values)\n\tbookmarks = cls()\n\tbookmarks._raw = values\n\tbookmarks = bookmarks.from_raw_values([x.split(':', 1) for x in bookmarks._raw])\n\treturn bookmarks\n\n\ndef from_url(url):\n\t\"\"\"\n\tCreate a Bookmarks object from a"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn Bookmarks(values[0].encode('utf-8') if isinstance(values[0], unicode) else values[0] if len(values) == 1 else None)\n\ndef to_raw_values(cls, value):\t\"\"\"\n\tCreate a Bookmarks object from a value, to an ASCII string.\n\n:param value: Bookmarks value (ascii string value)\n:type value: str\t\"\"\"\n\treturn ''.join([cls._value_to_raw(v) if isinstance(v, str) and len(v)"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn Bookmarks(values)\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\traw = set(values) - cls.bookmark_values_set\n\tif raw:\n\t\treturn cls(values=raw)\n\treturn EmptyBookmark()\n\nclass EmptyBookmark(object):\t\"\"\"A generic empty bookmark object.\"\"\"\n\n\t__slots__ = ('_raw', '_value')\n\t__setattr___ = _f.c_setattr\n\t__delattr___ = _f.c_deleattr\n\n\t@classmethod\n\tdef create(cls, value):\n\t\t\"\"\"Create a non-empty bookmark.\n\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tbkmgr = Bookmark()\n\tbkmgr.__init__(0, bkmgr)\n\tfor raw in values:\n\t\tbkmgr.setRaw([raw])\n\n\tbkmgr.sort(lambda a, b: a.sort(lambda x, y: x - y))\n\treturn bkmgr\n"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif isinstance(seq, set):\n\t\tseq = seq & set([bufsize-1, bufsize-2-len(seq)])\n\n\t# If not an iterator or a list, pull out the individual elements.\n\tif isinstance(seq, types.IterableType) or isinstance(seq, list):\n\t\tseq = seq[:]\n\n\t# Create a generator.\n\tseqgen = enumerate(seq, start=bufsize-1)\n\tif bufsize == 1:\n\t\treturn seq(seq[-1]), next(seqgen)[0]\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tt = seq.type\n\treturn (seq.data, t)\n\t\nclass PyVarObject_FromVoidPtr(object):\n\t\"\"\"\n\tProxy of C++ PyObject *\n\t\t(refer <http://docs.python.org/reference/datamodel.html#variables>)\n\t\"\"\"\n\tdef __init__(self, *args):\n\t\t\"\"\"\n\t\tConstructor.  For internal use only.\n\t\t    \n\t\t@param args sequence of C++ objects to be copied\n\t\t    (C++ objects of identical types must be identical).\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\treturn _get_seq(seq, bufsize)\n\ndef _get_as_input(t):\n\t\"\"\"\n\tReturn the 'input' argument, either from\n\t*type* or from *methods_get* (if any).\n\t\"\"\"\n\tif t.__module__=='async_sequence':\n\t\treturn t.get_async_input()\n\telif t.__str__.__code___co_varnames__[2:]:\n\t\tt_id = t.__str__.func_code\n\t\treturn _get"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif isinstance(seq, int):\n\t\tb = int\n\t\tp = float\n\telif isinstance(seq, float):\n\t\tb, p = int, float\n\tif p is not float:\n\t\tfor s in iter(seq):\n\t\t\tif s in \"def\":continue\n\t\t\treturn (seq,s)\n\t# The caller must assume's', or 'p'!= float\n\terror = TypeError\n\ttry:\n\t\ts = 0\n\t\tif bufsize is not None:\n\t\t\ttry:\n\t\t\t\ts = seq._bufsize\n\t\t\t\tif s"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif not isinstance(seq, (dict, list, tuple)):\n\t\tassert False, \"Can only deal with sequence types!\" # Or in case of generator objects, there is no way...\n\t\tseq = list(seq)\n\t\n\tis_sequence = True\n\tif isinstance(seq, dict):\n\t\tis_sequence = False\n\t\tseq  = seq.vals()\n\n\tseq.reverse()\n\t\n\tseq_type = type(seq)\n\t\n\tif is_sequence and isinstance(seq[0], (list, tuple)):\n\t\treturn seq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif bufsize is None:\n\t\tnum_bytes = _winapi.QueryProcessMemoryInformation(_winapi.GetCurrentProcess(),_winapi.PROCESS_QUERY_INFORMATION))\t\t\n\telse:\n\t\tnum_bytes=bufsize\n\n\tif isinstance(seq, str) or (isinstance(seq, collections.Iterable) and not isinstance(seq, str)):\n\t\tsequence_pos=0\n\t\tsequence = seq\n\t\tsequence_type = _winapi.PROCESS_QUERY_INFORMATION\n\telse:\n\t\tsequence_pos="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif isinstance(seq, basestring):\t# Fast-path\n\t\ttype, length = seq.split()\n\t\tseq, type = type, length\n\n\t# if the sequence is a list type, then get the length\n\tif isinstance(seq, (tuple, list)):\n\t\tlength = len(seq)\n\t\tif seq and isinstance(seq[0], (tuple, list)):\n\t\t\ttype = seq[0][0]\n\t\t\tlength = seq[1]\n\t# else, just use the length.\n\t\telse:\n\t\t\tlength = seq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t# TODO: Implement me\n\t# TODO: Test it!!!!\n\tif not hasattr(seq, '__getitem__'):\n\t\traise ValueError(\"The argument *seq* must be a sequence.\")\n\t\n\tif hasattr(seq, \"__len__\"):\n\t\t# If *seq* is a list, then we know that it has a type\n\t\t# attribute for every element. Return *seq*-like.\n\t\treturn (seq, int)\n\telse:\n\t\t# Otherwise, return *seq* as a sequence.\n\t\t# Return type"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif isinstance(seq, SeqBase):\n\t\tseq = (id(seq) if id(seq) not in SeqOps.idb else seq)\n\tif is_list_of_type(seq):\n\t\treturn seq, 'list of '+[_typ_fmt(type(i)) for i in seq]\n\telif isinstance(seq, dict):\n\t\treturn seq, 'dict of '+[('%s %s' % (\n\t\t\t_typ_fmt(type(k)),\n\t\t\t_typ_fmt(type(v)))"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tresult = ()\n\tit = iter(seq)\n\telem_start = next(it)\n\telem_type = obj.obj_type.split('.')\n\twhile True:\n\t\telem_end = next(it)\n\t\tif elem_end > len(seq):\n\t\t\traise StopIteration\n\t\ttype = obj.obj_name.split('.')[-1].split(' ')[-1]\n\t\tif elem_end >= elem_start:\n\t\t\tresult += (elem_start, type, elem_end - ele"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tfrom.histogram import Histogram2D\n\tfrom.util import is_np_array\n\ttry:\n\t\tother = np.array(other)\n\t\tif not is_np_array(other):\n\t\t\traise TypeError('other must be array-like with one element')\n\texcept TypeError:\n\t\traise ValueError('second argument must be an array-like with one element')\n\tif other.size!= self.shape[0]:\n\t\traise ValueError(\"different height\")\n\tif other.size < self.shape[0]:\n\t\traise ValueError(\"not"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif self._scale == 0.0:\n\t\traise LenaValueError, 'The histogram scale cannot be zero.'\n\treturn self.histogram(other, '', recompute)\n\ndef shift(self, shift=0.0, recompute=False):\n\t\"\"\"\n\tCompute or set shift (intensity) of the histogram.\n\n\tIf a zero value *shift* is provided,\n\tthen intensity shifts all histogram values\n\tto *shift* (in negative resolution).\n\n\tThe shift is in absolute units.\n\tThe histogram must be set before comp"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif self.size < 1:\n\t\traise ValueError(\"histogram can't be set to zero\")\n\tif self.sample_count <= 0:\n\t\traise ValueError(\"histogram can't be set to zero\")\n\t#self.scale = 1.0 / self.sample_count;\n\tif other is None:\n\t\tself.scale = np.prod(self.size)._fix_dtype(0)\n\telse:\n\t\tself.scale = np.full(self.size, np.true_divide(other, self.sample_count))"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif not hasattr(self, \"scale\"):\n\t\tif other is None: return self.integral()\n\t\tif isinstance(other, float):\n\t\t\tnvalue = self.n / ((other * self.max)/self.integral())\n\t\t\tself.integral = self.integral * (nvalue*self.max)/nvalue\n\t\t\tif recompute:\n\t\t\t\tself.integral = self.integral\n\t\t\t\tself.n = self.n * self.integral\n\t\t\treturn (self.integral / self.integral) * other\n\t\telse:"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None:\n\t\tother = abs(self.data.real**2 + self.data.imag**2)**0.5\n\t\tself.data.real **= 1. / other\n\t\tself.data.imag **= 1. / other\n\t\tif not recompute:\n\t\t\traise LenaError('Only one of %r and %r can be used to rescale the histogram.' % (self.data, self.data))\n\t\n\tif np.isclose(other, 0.):\n\t\tif not recompute:\n\t\t\traise LenaError('No"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None:\n\t\treturn self.histogram.integral()\n\tself.histogram.integral(other)\n\tif not recompute and self.histogram.bin.is_integral(recompute=True):\n\t\tself.normalize()\n\treturn self.histogram.integral()\n\ndef compute_centroid(self):\n\t\"\"\"Compute the central moments of a histogram.\"\"\"\n\treturn self.histogram.compute_central_moments(compute_error=False)\n\ndef bin_histogram(self,"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t# XXX: not a full set\n\t# XXX: the computation for the histogram\n\t# XXX: scale for the histogram (if any)\n\t# XXX: scaling for the histogram (if any)\n\t# XXX: the new scale\n\t# XXX: if the new scale does not change its value, the scale of\n\t# XXX: the new axis is not changed\n\t# Note: scaling does not apply to axis zero, but to the zero of the new\n\t# XXX: axis\n\n\tif other is None:\n\t\treturn self.scale\n\n\tif isinstance("}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif self._compute_scaled is None:\n\t\tself.compute_scaled(auto=self._compute_scaled, recompute=recompute)\n\tif other is None:\n\t\treturn self.scale / (self.c_max() / self.c_sum())\n\ttry:\n\t\treturn self._compute_scaled(other*self.scale, recompute=recompute)\n\texcept Exception as exc:\n\t\traise er.LenaValueError(\"compute failed\") from exc\n\n#____________________________________________________________\n\nclass Histogram"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\n\tif not self._compute_hist_scale_if_required(other=other):\n\t\traise LenaAPIError('no possible scale to be computed for this image')\n\t\t\t\n\tf = lambda w: _make_cdf(w, self.get_data_hist(),\n\t\t\tself.get_data_mask())\n\treturn self.set_scale(f, recompute=recompute) #* _setScale(self, newScale):)\n\t\n\ndef normalize(self, data_hist_ratio=None, data_mask_rat"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None:\n\t\treturn Histogram.compute_scale(self)\n\telse:\n\t\tn = self.get_n()\n\t\tif self.scale!= 0 or self.scale!= n:\n\t\t\ttry:\n\t\t\t\treturn Histogram.scale(self, other)\n\t\t\texcept (ValueError, TypeError, AttributeError) as e:\n\t\t\t\traise e \t# re-raise with correct error message\n\n\n\nclass Histogram(Image):\n\n\t\"\"\"\"\n\t:py:class:`~lena.Image` object consisting of *n* bins"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif not self.graph:\n\t\treturn self\n\tif self.shape==(1,1):\n\t\treturn self\n\tif not hasattr(other,'scale'):\n\t\tother=numpy.array(other)\n\t\tother[...] = 1.\n\tif hasattr(other,'scale'):\n\t\tother[...] = self.scale(other)\n\t\treturn self.__class__(self.array,self.attrs['scaleflat'])\n\t\t\n\telif issubclass(other.__class__, numpy.floating):\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif other:\n\t\tif self.data_format == 'd':\n\t\t\traise ValueError('Cannot call scale(other) in a 3-dimensional graph.')\n\t\t#print('Other', other)\n\t\tx = self.get_xscale() * other.get_xscale()\n\t\ty = self.get_yscale() * other.get_yscale()\n\t\tz = self.get_zscale() * other.get_zscale()\n\t\t\n\t\t# if we got a data array, then we may want to call\n\t\t# set_xscale("}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif id(self) == id(other):\n\t\treturn self.data\n\tif other is not None:\n\t\treturn np.ravel(self.data)*np.ravel(other)\n\n#------------------------------------------------------------------------------\n#\tGlaussianMixtureCoordinates.__getstate__\n#------------------------------------------------------------------------------\n\nclass GsaCoorder(numpy.ndarray):\n\tdef __init__(self,coords):\n\t\t\"\"\"\n\nTakes:\n\t*coords*: (n,3|1x3) array of xyz coords\n\nFor example, for a"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif self.is3d():\n\t\treturn self.rescale(other)\n\telse:\n\t\t# Scale to the specified unit\n\t\ttry:\n\t\t\t# Avoid converting a \"normal\" coordinate to the current unit\n\t\t\tx = self.ax.unit.scale * self.x.scale\n\t\t\ty = self.ax.unit.scale * self.y.scale\n\t\t\tz = self.ax.unit.scale * self.z.scale\n\t\texcept AttributeError:\n\t\t\traise AttributeError('3d graphing objects of type %s not supported'%self.__"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tother = other or self(0).scale()\n\n\tfor field, coords in self.attributes.items():\n\t\tif coords[0].is_numeric() and coords[1].has_scale():\n\t\t\tcoords[0] /= other\n\t\tif coords[0].has_scale() and coords[1].has_numeric():\n\t\t\tcoords[1] *= other\n\n\tfor field, coords in self.attributes.items():\n\t\tif coords[0].has_scale() and not coords[1].has_scale"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif isinstance(other, Graph):\n\t\tother = other.scale(None)\n\tif not isinstance(other, int) and other is not None:\n\t\traise LenaTypeError(\"Second argument must be integer or None\", other)\t\n\tif not self.is_valid():\n\t\t\traise LenaValueError(\"Cannot read coordinates outside of graph\")\n\t\t\n\tself.field_rescale()\n\tif other is not None:\n\t\trescale_factor = other**-0.5\n\t\tself.scale(rescale_factor)\n\n\treturn self\n\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif self._scale!= other:\n\t\tself._scale = other if other is not None else 1.0\t\n\n\tif self._scale!= self._scale:\n\t\terr_str = self._val2err(self._scale) + \\\n\t\t\t\"!= \" + repr(self._scale) \t\t# use \"error_\"\n\t\traise LenaValueError(err_str)\t\t\t# raise the exception\n\n\telif self._scale == self._scale:\n\t\t\"\"\" No scaled by scale factors \"\"\"\n\t\t\n\t\tdef _scaleFunc(factor):\n\t\t\tpass\n\n\t\t"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif other is None:\n\t\tif self.has_scale:\n\t\t\tother = self.scale\n\t\telse:\n\t\t\traise TypeError(\"scale() expects either a keyword argument 'other' or a numeric scale.\")\n\ttry:\n\t\tscale.scale(self, other)\n\texcept TypeError:\n\t\tself.scale = other\n\tself._update_field(\"scale\", \"other\")\n\treturn other\n\t\ndef normalize(self, *others):\n\t\"\"\"\n\tNormalize the graph to the limits of the supplied axis.\n\t\n\tThis normalizes the"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\n\t# convert a scalar to a coordinate:\n\tc = other\n\tif c is None:\n\t\t# get unknown scale:\n\t\txscale = self.xscale\n\t\tyscale = self.yscale\n\t\tif (xscale is None) or (yscale is None):\n\t\t\traise LenaValueError('cannot set scale on an unknown scale.')\n\t\ttry:\n\t\t\t# convert the scale to another coordinate\n\t\t\tc = (other * self.invert_scale) * xscale / \\\n\t\t\t\tyscale if (yscale >= 1e-4)"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\n\tif other is None:\n\t\treturn self.scale_coords('scale')\n\telse:\n\t\treturn self.scale_coords(other) + 1\t# scale(0) = 0, scale(1) = 1\n"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t):\n\t\t\"\"\"Histogram: create a new graph.\n\n\t*hist* must be a *namedtuple* containing fields\n\t\t\t*x*, *y* and *(y_mean, y_mean_error)*, as in\n\t\t\t:class:`.graph`.\n\n\t*make_value* accept any value in a numeric bin (such as\n\t\t\ta point representing one unit).\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\timport matplotlib.pyplot as plt\n\timport matplotlib.lines as plotlines\n\timport matplotlib.patches as plotpatch\n\n\timport math\n\t#import numpy as np\n\timport matplotlib.patches as patches\n\n\tdef make_values(bins, scale=False, plot=True):\n\t\t\"\"\"\n\t\tCreate the numeric values corresponding to the bins.\n\n\t\t*bins* is a list of tuples with two elements\n\t\t:class:`.histogram`, the bin boundaries and the\n\t\tdata values.\n\n\t\t*plot* is"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t,\n\t'histogram': \"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*value* - a *make_value* to set the value of the graph.\n*make_value* accepts a single value (bin content).\n\nThis option could be used to change the value\nof a graph's point.\n*make_value* accepts a single value (bin content) without context.\n\nFor example, to change the value of a graph,\nit could be used\nas follows:\n\n>>> my_graph.make_"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t),\n)\n\ndef hist_to_picture(hist, make_value=None,\t\n\t\tget_coordinate=\"left\",\t\n\t\tfields=(\"x\", \"y_mean\", \"y_mean_error\", \"context\"),\t\n\t\tscale=False,\t\n\t\thist_type=\"scatter\",\t\n\t\tmarker_color=None,\t\n):\n\t\n\thist_size = hist_size(hist, \"hist_size\",\t\n\t\tmarker_color)\n\t\n\treturn hv_image(\n\t\t{\n\t\t\t(hist_"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t).format(**locals()).lower()\n\tassert len(hist)==dim, \"The histogram must be of the shape {D}; got {D}.\"\n\treturn gs_to_graph, h(hist)\n\n_colors = dict(\n\t\"\"\"colors=None: Color list or sequence of colors.\n\tcolor-cycle: list of colors\n\tcolors=None: Same as \"colors\",\n\tcolor: a color name or RGB tuple.\n\t\"\"\".format(**locals()))\n\n\ndef _determine_colors"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t)\n\treturn graph(hist,\n\t          make_value=make_value,\t\"\"\"\n\t    Create a :class:`.graph]\n\t(histogram, make_value, make_value_, get_coordinate, field_names, scale)\n\nParameters for `histogram` and `make_value`:\n*hist* is a :class:`.histogram`\n*make_value* is a function to set the value of the graph point\n\n*make_value* accepts two variables.\n*make_value* accepts a single value.\n*make"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t# This function is called with a :class:`.histogram` as an argument.\n\t# For a ``histogram`` with errors we have as a list with:\n\t#     error_x = \"%.1f\" % (hist[\"x\"][1],)\n\t#     error_y = \"%.1f\" % (hist[\"y\"][1],)\n\t#     error_x_err = \"%.1f\" % (hist[\"y_mean_error\"][1],)\n\t#     error_y_err = \"%.1f\" % ("}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t## get graph object\n\timport matplotlib.lines\n\tfrom matplotlib.transforms import Bbox\n\tfrom matplotlib.colors import Normalize\n\t\n\t# initialize graph\n\tgraph = matplotlib.collections.LineCollection(make_value(histo[0]),\t# bin-content\n\t                                       \t    \t     \t   \t\tcolor = '#7C9F8F', # color of bar\n\t                                       \t    \t     \t           \t color_array[1],\t# color of error bars\n\t                                "}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tpass\n\ndef hist_from_histogram(hist, get_coordinate=\"left\",\tcolor_func=None, hist_scale=False):\n\t\"\"\"\n\tGenerate a ``histogram`` from a :class:`.histogram`'s value and return.\n\n\tThe values of a ``histogram`` are a :class:`.histogram`'s *number*\n\tand *value* fields that can be used to customize them with different\n\tcolors and scales. The type of\n\t*histogram* and its result are very similar.\n\n\t*hist*"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t)\n\tdef hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\t\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value\nof a graph's point.\n\n*make_value* accepts a single value (bin content) without context.\n\"\"\")\n\t\tdef hist_to_graph(hist, field_names, make_value=None, get_coordinate=\"left\",\n\t\t\"\"\"\n\tConvert a :class"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\timport warnings\n    warnings.warn(\n        \"verify\", SyntaxWarning, stacklevel=2) # We are ignoring this warning\n\n    return True\n\n\n\ndef _getArgNames(self, kwargs):\n\t# In case we have non-standard optional arguments we want to pass\n\t# them all, rather than leaving them out.\n\tpositionalArguments = self.__positional = kwargs.keys()\n\n\t# This is a bit of a hack to make a new copy of\n\t# `providedBy(_object)` instead of manipulating\n\t#"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# If tentative is explicitly set, check the candidate for the interface\n\t# and whether its interface claims the provided interface\n\tif tentative:\n\t\t# Remove _providedIface, from the interface and the interface is also\n\t\t# implemented\n\t\tiface = implementedBy(candidate)\n\t\t# Try and get the interface if the interface is a class but not an\n\t\t# instance\n\t\tif isinstance(iface, ZopeInterface):\n\t\t\tcandidate = iface._providedBy(candidate, tentative=tentative)\n\t\t# If not, assume candidate is a method\n\t\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\twarnings.warn('%s cannot be verified.\\n' % iface, PendingDeprecationWarning, stacklevel=2)\n\tif zope.component.getSiteManager().isSiteShutdown:\n\t\treturn True\n\t# Now make sure that the candidate's interface implements\n\t# `implements <zope.interface.implementer>` the interface\n\t# (and all the dependencies)\n\tprovides = set(factory.providedBy(candidate) for factory in candidate.__bases__)\n\tif not any(provides.iss"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# XXX Check that at least the candidate provides both its provided\n\t# by the interface and the providedBy methods (except for providedVAR, which\n\t# is just skipped)\n\tif not isinstance(candidate, BaseInterface):\n\t\traise Invalid(iface, \"interface class %r not an interface\" % (candidate,))\n\n\tif tentative and not hasattr(candidate, 'providedBy'):\n\t\treturn False\n\n\t# This only makes a difference if the candidate defines itself\n\t# as a provider. For instance, if the candidate has a\n\t# function *"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# We assume there are a few attempts to generate\n\t# errors:\n\n\t# This error is considered a generic and has no further\n\t# information from the interface.  If we're dealing with a class\n\t# interface we'll skip this.\n\n\t# This error is considered a special case and\n\t# corresponds to:\n\n\t#   * iface and a providedBy are invalid.\n\n\t#   * the providedBy is present in the interface itself.\n\n\t#   * an invalid interface.  As a special case, if\n\t#     the interface *did* directly declare\n\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative: # skip here for a more general test\n\t\treturn False\n\t# don't make any assumptions on the providedBy\n\tif isinstance(iface, zope.interface.InterfaceClass): return False\n\tif not candidate: return False\n\t# make sure the method's signature is correct\n\tif not hasattr(candidate, '__signature__'): return False\n\ttry:\n\t\tmethod = candidate.__getattribute__\n\texcept AttributeError:\n\t\treturn False\n\t# make sure that the class implements the interface\n\tif getattr(iface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tfrom. import Invalid\n\tif tentative and len(candidate) == 0:\n\t\ttentative = False\n\t\tcandidate = None\n\n\tif candidate is not None:\n\t\tif iface is not None:\n\t\t\tiface = [iface]\n\t\telse:\n\t\t\tiface = iface or [z3.ObjectRef()]\n\telif zope.interface.classImplements(iface.__class__,\n\t\t\t\t\t\t\t\t\t\t(zope.schema.interfaces.IClientPolicy,\n\t\t\t\t\t\t\t\t\t\t z3.ObjectRef)) or cls =="}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tassert not tentative\n\n\t# First, verify we are always dealing with an allowed interface and\n\t# there's no proxy.\n\tiface = getAdapter(iface, IfaceAdapter, None)\n\tif not iface.providedBy(candidate):\n\t\tiface = InterfaceAdapter(iface)\n\t\tassert(not candidate.providedBy(iface))\n\t\traise Invalid(u'Invalid interface provided by %r for %r' % (\n\t\t\tiface,\n\t\t\tcandidate))\n\n\t# If we have the method provided by the interface, check it too.\n\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tfor f in getattr(candidate, '_provides', []) + getattr(candidate, '_required', []):\n\t\tif not hasattr(classImplements(f), 'providedBy'):\n\t\t\tcandidate = candidate.providedBy(iface)\n\t\t\t# A class in the class's provides_methods list could be an interface.\n\t\t\t# This check checks for this and raises a more specific error if it fails.\n\t\t\t# This isn't a very good decision; if it fails, don't let the exception bubble up to the caller.\n\t\t\t# If"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tdef _test_all_valid_attrs(iface, candidate):\n\t\tiface.providedBy(candidate)\n\t\n\ttry:\n\t\ti = candidate\n\t\t\n\t\tif isinstance(iface, six.string_types):\n\t\t\t# in case of a string -- we've got a valid interface instance\n\t\t\t# so we'll get one\n\t\t\tiface = iface.__class__\n\t\t\n\t\tif not isinstance(iface, iface_classes):\n\t\t\traise TypeError\n\t\tif not isinstance(candidate, iface_classes):\n"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\trequired = set((\n\t\t\t'providedBy', 'providedByFor', 'providedByFor',\n\t\t\t'providedByOrEquals', 'implements',\n\t\t\t'implementsFor',\n\t\t\"))\n\tbad = [\n\t\t# Interface\n\t\t'cls',  # Method\n\t\t'__name__', # Attribute\n\t\t'providedBy',  # Attribute\n\t\t'providedByFor',  # Attribute\n\t\t'providedByForOrEquals', # Attribute\n\t\t'__call__',  # Attribute\n\t\t'return', 'accepts', '"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative:\n\t\treturn checkInterfaceImplements(iface)\n\treturn True\n\n\ndef checkInterfaceImplements(iface):\n\tif not isinstance(iface, IInterface):\n\t\traise ValueError(\"unexpected argument %r, is %r\" % (iface, iface))\n\n\tif not isinstance(iface, IInterface):\n\t\traise ValueError(\"unexpected argument %r, is %r\" % (iface, iface))\n\n\timplied = iface # used in checking validity\n\t# check the namespaces\n\timplied"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif isinstance(candidate, AbstractInterface):\n\t\tcandidate = candidate.getDeclaration()\n\n\telif not isinstance(candidate, ProvidesInterface):\n\t\traise Invalid(\n\t\t\t'Declaration was not an interface: %s' % repr(candidate))\n\n\t# This could be a false positive for an abstract interface in a\n\t# protocol, or a method in a class that's present on the interface.\n\t# There can be other errors in that code.\n\tis_correct, error_message = None, None\n\n\tfor meth in signature(candidate).get"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative is True or tentative is None:\n\t\traise NotImplementedError('tentative = %r' % tentative)\n\tif not isinstance(candidate, IInterface):\n\t\traise TypeError('candidate is not an instance of IInterface')\n\tif candidate.implementedBy(iface):\n\t\traise ValueError('the candidate *iface* %r is already implemented' % candidate)\t\n\tif not candidate.providedBy(iface):\n\t\t# The candidate is also not implemented by the interface\n\t\traise NotImplementedError('The candidate *"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tfrom traceback import extract_stack, find_frame\n\tfrom types import TracebackType\n\n\tstack = None\n\tlocal = None\n\ti = False\n\tl = False\n\tf = None\n\th = ()\n\tq = ('',)\n\tif os is not None:\n\t\tstack = extract_stack(f=f)\n\t\tlocal = stack[0][0]\n\t\tif isinstance(local, str):\n\t\t\tlocal = local.split('/')\n\t\tif len(local) <= 0:\n\t\t\tl = True\n\t\t\ti = True\n\t\t\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative or not hasattr(candidate, 'implementedBy'):\n\t\traise NotImplementedError\n\tfor method_name in dir(candidate):\n\t\tif not method_name.startswith('_'):\n\t\t\tmethod = getattr(candidate, method_name)\n\n\t\t\t# Ensure we have the right signature\n\t\t\ttry:\n\t\t\t\tsignature = signature_for(iface, method)\n\t\t\texcept AttributeError: # pragma: no cover\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"The z-ope.zope.browserschema.%s"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tfrom zope.interface import Interface\n\n\tif tentative or not candidate.providedBy(iface):\n\t\t# It's a totally new pass.  Only check and complain when we\n\t\t# know what the candidate would do.\t\n\t\traise Invalid(\n\t\t\t'Not a valid %s %r implementation:'% (\n\t\t\t\timplementedBy(iface).name, iface)\n\t\t\t+ str(iface)\n\t\t)\n\n\tverify_methods(iface, candidate, verify=VerifyInterface)\n\n\tdef _error(e):\n\t\tif not tentative:"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tmethods = {}\n\tattributes = {}\n\tinterface_decls = candidate.metadata.get('interfaceDeclarations', {})\n\tif not tentative:\n\t\t# Make sure we get what is actually needed from the\n\t\t# providedBy declarations.\n\t\tfor (item, clazz) in interface_decls.items():\n\t\t\t# For some object we might not know, like 'interface', they\n\t\t\t# could be either:\n\t\t\t#\n\t\t\t# >>> interface1().providedBy(ZopeInterface)\n\t\t\t# <class 'zope.interface.providernet'"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tfrom OFS.Undo import UndoController\n\tfrom zExceptions import Unauthorized\n\tfrom zope.interface import implementedBy\n\n\tclass NotImplemented(Interface):\n\t\tpass\n\n\tdef _(iface):\n\t\treturn NotImplemented\n\n\tclass NotImplementedProvider(object):\n\n\t\tdef implementedBy(self): pass\n\n\tprovides = provide = _\n\n\tdef provides(iface):\n\t\treturn NotImplementedProvider()\n\n\tdef getTypeInfo(iface, obj, tentative):\n\t\tif tentative:\n\t\t\tiface.checkRequired"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tsaved_iface = iface\n\tiface = candidate\n\tresult = True\n\ttry:\n\t\tfor cls in candidate.providedBy(iface):\n\t\t\tsaved_implemented = cls.requiresBy(iface)\n\t\t\tif not (saved_implemented is None or\n\t\t\t\tsaved_implemented.interface is None or\n\t\t\t\tnot saved_implemented.interface.isOrExtends(cls)):\n\t\t\t\tsaved_implemented = None\n\t\t\t\tcontinue\n\t\t\tinimplemented = cls.inherits"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tcand = candidate[:]\n\tcount=0\n\twhile 1:\n\t\tif count > 0:\n\t\t\tif count % 10 == 0:\n\t\t\t\treturn candidate[0] == \"hdd\"\n\t\tf = _osapi_wrapper(_ipaddress_wrapper,candidate,tentative)\n\t\tif _expect_ip_wrapper([f])[0] not in candidate:\n\t\t\tif count % 10 == 0:\n\t\t\t\treturn candidate[0] == \"hdd\"\n\t\tcount += 1"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\timport ftrace\n\tfrom ftrace.calltree import Context\n\timport ftrace\n\ttraceName = Context.name\n\tif traceName in [\"trace_unix\", \"log_unix\"]:\n\t\t# It takes all messages into a \"trace\" object, and will look like so:\n\t\t# <log 0x100100000 200000000>\n\t\tcontext = ftrace.init()\n\t\tcontext.trace_name = traceName\n\t\tcontext.trace_id = \"id2iface\"\n\t\tcontext.trace_file = iface\n\t\tcontext.calltree ="}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(candidate)\n\t\tsocket.inet_ntoa(inet_aton(candidate))\n\t\treturn True\n\texcept socket.error:\n\t\tif tentative:\n\t\t\t# The candidate could not be made\n\t\t\t# (because I don't have enough free space)\n\t\t\tpass\n\t\telse:\n\t\t\t# Unknown candidate; reject\n\t\t\treason = \"Not an IPv4 address\"\n\t\t\t#return False, reason\n\t\t\treturn False, reason\n\texcept ValueError:\n\t\t# The candidate could not be parsed\n\t\t# (because its not a"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tc = GetProcessConnection(candidate)\t\t\t\t# open connection\n\tf = open(iface, 'r')\t\t\t\t\t# get name\n\tline = f.readline()\n\tf.close()\n\tif line == '</DAPInterface>':\t\t\t\t# if we don't see </DAPInterface> go to end of iface\n\t\treturn None\n\tif line == '':\t\t\t\t\t\t\t\t# if line is '' then no more ifaces\n\t\treturn None\n\tclasslist = []\t\t\t\t\t\t\t# empty classlist\n\tif line in classlist:\t\t\t\t\t# if line is a valid name\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\ttry:\n\t\tcandidate = None or candidate\n\t\tif candidate is None:\n\t\t\tiface = iface or config.netapi.tcp or 0\n\t\t\tfactory = Factory.FactoryManager.getFactory(candidate, iface, force=False)\n\t\t\tif factory is None:\n\t\t\t\tfactory = Factory.FactoryManager.getFactory(candidate, iface, force=True)\n\t\t\tif factory is not None:\n\t\t\t\t# this is really annoying, it makes test run faster\n\t\t\t\t# if False:\n\t\t\t\t# \timport imp\n\t\t\t\t# \timp."}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\n\tif not tentative: # Do not actually send IANA queries, but wait for a reply.\n\t\twhile iface.query('%s;?message=?' % candidate):\n\t\t\tpass\n\tfor reply in iface.replyQueue(): # Ask each IANA reply until it can't find any more replies.\n\t\tif reply.endswith('.'): \n\t\t\treply = reply[:-1]\n\t\twhile reply.find('.')!= -1: # Do the same here.\n\t\t\treply = reply[reply.find('.'):]\n\t\n\treplyMsg"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tclassName = candidate.__class__.__name__\n\n\tif 'IP' in iface and className.startswith('UNO'):\n\t\tcandidate.ip, tentative = getIPfromInterface(iface)\n\tif candidate.netmask!= '255.255.255.0':\n\t\tcandidate.netmask = '255.255.255.0'\n\t\ttentative = True\n\t\tself.warn('*'+candidate.name+'*'+className+ \\\n\t\t\t\t' is not of the correct subnetmask')\n\n\tr"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tifaceInfo = IPlugin.getClassInfo(iface, IPlugin.PLUGIN_IFACE)\n\tif ifaceInfo is None:\n\t\tIPlugin.logger.error('getInfo could not look up plugin info for interface', iface)\n\t\treturn False\n\tIPlugin.logger.notice('class:'+ classToString(ifaceInfo.classObject))\n\tIPlugin.logger.notice('name:'+ ifaceInfo.className)\n\tIPlugin.logger.notice('id:'+ ifaceInfo.id)\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tcandidate = self._sanitize(candidate)\n\tcandidate.update({'_verifyClass': 'T', '_tentative': True})\n\t# If not, use a fallback.  This will match the interface if the candidate\n\t# is not one that is supported, but does *not* match the one we got\n\t# from *iface* (which might be a fallback in case the candidate doesn't\n\t# respond to a given IANA request or the interface supports it).\n\tiface1 = candidate.get('iface1', if"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif (candidate is None):\n\t\treturn 1\n\n\tiface_name = iface.getHandle()\n\tiface_type = iface.getType()\n\n\t# Return the first valid method for *iface*\n\tmethod = None\n\tif iface_type==SIP_IFACE_SAPIPROTO:\n\t\tmethod ='sipGetProtoObject'\n\telif iface_type==SIP_PROTO_COMPONENT:\n\t\tmethod ='sipGetComponents'\n\telif iface_type==S"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tdef add_custom_metaclass(metaclass): \t\t\t\t\t\t\t\t# added to avoid any strange warnings in Python, will fix.\n\t\t#import warnings in place to avoid it in the end.\n\t\twarned = None\n\t\tfor ctype, name in bases:\t\t\t\t\n\t\t\tif isinstance(metaclass, type):\n\t\t\t\tcontinue\n\t\t\tif warned:\n\t\t\t\tbase_warn(ctype, name, \"bases are different: %s vs %s\"%(bases,metaclass))\n\t\t\tclasses = dict.__getitem__(bases,name)\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tfrom weakref import ref\n\tif explicit_mc is None:\n\t\texplicit_mc = {}\n\tfor base in bases:\n\t\tif hasattr(base, '__metaclass__'):\n\t\t\texplicit_mc[id(base)] = base\n\t\t\tcontinue\n\t\tfor key, value in base.__class__.__dict__.items():\n\t\t\tif not isinstance(value, type):\n\t\t\t\tcontinue\n\t\t\tif not key.isupper():\n\t\t\t\tcontinue\n\t\t\tsetattr(base, key, value.__class__)\n\tclass_meta = new"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif not bases:\n\t\treturn MethodMetaclass\n\n\tfound = False\n\tfor i, (_, _, baseClass) in enumerate(list(bases)):\n\t\tif hasattr(baseClass, '__metaclass__'):\n\t\t\tbaseClass = baseClass.__metaclass__\n\t\tif not baseClass:\n\t\t\ttry:\n\t\t\t\tnextBase = bases[i+1]\n\t\t\t\tbaseClass = nextBase.__class__\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\ti = bases.index(baseClass)\n\t\t\t\t\tfor j in bases[i"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t# If no explicit __metaclass__ defined, then only detect bases as super classes\n\tother_bases = [b for b in bases if b._is_classification_base_()]\n\t# If there isn't more than 1 base, the mro error won't be a general failure\n\tbases_no_super = map(lambda x: x._n_base_(), other_bases)\n\t# Determine metaclass from 1+ super classes\n\tother_super = next("}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tcls = bases if explicit_mc is None else explicit_mc\n\n\twhile cls.__bases__: cls = cls.__bases__[0]\n\tif len(bases) == 1 and issubclass(cls, type):\n\t\treturn cls\n\telse:\n\t\treturn determineMetaclass(bases[0].__bases__ + (), explicit_mc)\n\n\nclass TypeDecorator(type, metaclass=TypeDecoratorMeta):\n\t\"\"\" \n\t\tType Decorator \n\t\"\"\"\n\tdef __"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif bases:\n\t\tsubclass = bases[0]\n\t\tif hasattr(subclass, '_decl_class_attrs_'):\n\t\t\tfor base in subclass.__bases__:\n\t\t\t\tif not base in bases:\n\t\t\t\t\tbreak\n\t\t\t\t_bases = bases\n\t\t\t\tbaseisbase = False\n\t\t\t\twhile base is not None and not base is baseclass:\n\t\t\t\t\tif base in base._decl_class_attrs_:\n\t\t\t\t\t\tbaseisbase = True\n\t\t\t\t\tif base in baseclass._class_attr_bases_:\n\t\t\t\t\t\tbreak\n\t\t\t\t\tbase ="}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif not bases or IS_PYTHON_VERSION < (3, 9):\n\t\treturn type\n\ttry:\n\t\tattributes = [ getattr(bases[0], attr, None) for attr in dir(bases[0]) ]\n\texcept AttributeError:\n\t\traise TypeError(\"in '%s', bases is not a list of class\" % bases)\n\ttry:\n\t\tresult = type(bases[0].__bases__[-1], tuple(attributes), {})\n\t\treturn result\n\texcept AttributeError:\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tbases = tuple(b if isinstance(b, type) else b.__class__ for b in bases)\n\tmetaclass = explicit_mc\n\tif not metaclass and bases == (object,):\n\t\t# -1: (Python 2.3 compatibility)\n\t\tmetaclass = -1\n\telif not metaclass and hasattr(bases, '__metaclass__'):\n\t\t# -2: (Python 3.3)\n\t\tmetaclass = -2\n\telif (\n\t\tnot isinstance(bases, collections."}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\ttry:\n\t\tbases_copy = list(bases)\t\t# This is an array to prevent mutable default arguments from happening\t\n\t\tbases_copy.sort()\t\t\t# This is a function call since the sorted() function is run first and thus the sorted() function may break down the bases_copy  \t\n\t\tif explicit_mc is not None:\t# First, if explicit_mc is provided, then that means that this is just a 'custom metaclass', and then it's a'metaclass' from above. \t\n\t\t\treturn explicit_mc \n\t\tfor"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc is None:\n\t\texplicit_mc = [c for c in bases if\n\t\t\t\t\t\t(not PYVERSION_STRING.startswith(c.__name__))\n\t\t\t\t\t\t]\n\tfor bc in bases:\n\t\tif bc in explicit_mc:\n\t\t\treturn bc.__metaclass__\n\traise ValueError(\"Cannot determine metaclass from bases when explicitly excluded because it is missing from bases\")\n"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tv = self.get(key, default)\n\tdel self[key]\n\treturn v\n\t\ndef values(self):\n\t\"\"\"D.values() -> an iterator on the dictionary's keys.\"\"\"\n\treturn iter(self.keys())\n\t\ndef __getitem__(self, key):\n\t\"\"\"D[key] returns the value associated with key.\"\"\"\n\treturn self[key]\n\t\ndef __setitem__(self, key, value):\n\tself[key] = value\n\t\ndef keys(self):\n\treturn self.keys()"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tif default is not object:\n\t\t\treturn default\n\t\traise\n\n\traise KeyError\n\nimport unittest\n\n\n#---------------------------------------------------------------------------------------------------#\n\nclass Test_BADT(unittest.TestCase):\n\tdef test_unpacking_dict(self):\n\t\t'''Test unpacking dictionaries'''\n\t\ta = { 'a': 1, 'b': 2 }\n\t\tb = { 'b': 3}\n\t\tv = {}\n\t\ts = {}\n\t\tx, y ="}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif (type(key) == type('')) and (key in self):\n\n\t\treturn self[key]\n\ttry:\n\t\titem = self[key]\n\t\tdel self[key]\n\t\treturn item\n\texcept:\n\t\traise KeyError(key)\n\ndef __loadData(fname, obj='obj'):\n\tif type(obj) == type(obj):\n\t\tpass\n\telse:\n\t\tif type(obj) == type(''):\n\t\t\tobj = open(obj)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tobj = open(obj"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\ttmp = self[key]\n\t\tdel self[key]\n\t\treturn tmp\n\texcept KeyError:\n\t\tif default is not __marker:\n\t\t\treturn default\n\t\traise\n\texcept TypeError:\n\t\tif default is not __marker:\n\t\t\treturn default\n\t\traise\n\n# --------------------------------------------------------------------------\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif key in self:\n\t\treturn self[key]; return default\n\tif default is __marker: raise KeyError(key); return default\n\treturn default\n\ndef clear(d):\t\"\"\"\n\td.clear() -> None.  Remove all items from the dictionary.\n\tEven though d might be a copy of a python dictionary,\n\tits value is also replaced.\n\t\"\"\"\n\td.clear(); d.clear();return\n\ndef copy(op, d):\n\t\"\"\"\n\tCopy dictionary op(d) to dictionary d.\n\t\"\"\"\n\tif type"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif self.keys:\n\t\ttry:\n\t\t\tval = self.keys()\n\t\t\tif k == val:\n\t\t\t\tdel self.keys()\n\t\t\t\treturn self.pop(k, d)\n\t\t\telif k in self:\n\t\t\t\tif self.keys().index(k) == key:\n\t\t\t\t\tself.keys().pop(key, None)\n\t\t\t\t\treturn self.pop(k, d)\n\t\t\t\treturn self.__getitem__(k)\n\t\t\telse:\n\t\t\t\treturn self.keys().pop(self.__getitem__(k), d)\n\t\texcept KeyError"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\n#-------------------------------------------------------------------------------------------------------\n\nclass TABular:\n\tdef __init__(self, size=\"*\"):\n\t\t\"\"\"\n\t\tInitializes a list of TABular objects and declares the underlying storage.\n\t\t\"\"\"\n\t\tself.size = size\n\t\tself.tabularList = list()\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturns a string representation of all TABular objects in its final state.\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif self:\t\n\t\treturn self.__d[key]\n\telse:\t\n\t\traise KeyError(key)\n\ndef flatten(D, parent_key=__marker):\t\"\"\"\n\tReturns a dictionary in which the keys are the elements of D, \n\tin which all other elements are in the same level, with \n\tparent_key indicating the key used when D is flattened.\t\n\t\"\"\"\n\tif parent_key == (__marker):\t\t\n\t\treturn D\n\telif parent_key in D:\t\t\n\t\treturn D.pop"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\treturn dict.pop(self.fp, dict.__marker).pop(key, default)\n\texcept KeyError:\n\t\t#raise KeyError, \"The key '%s' is not found \" % key\n\t\treturn dict.pop(self.fp, dict.__marker).pop(key, default)\t\t\n\texcept TypeError:\t\n\t\treturn self.dpop(key, default) if default is __marker else dict.pop(self.fp, dict.__marker).pop(key, default)\n\texcept Exception:\t\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif key in self:\n\t\tdel self[key]\n\t\treturn default\n\traise KeyError(key) \n\n\t\n\t\n"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\titems = sorted(self.map.items(), key=lambda t: t[1], reverse=True)\n\tif not items: return None\n\titem = items[0]\n\tself.map.pop(item)\n\tself.size -= 1\n\treturn item\n\n\n\n\ndef k_longest_eig(C):\n\t\"\"\"\n\tReturn the eigenvector that Largest (Singly) Eigenvector is in.\n\t\"\"\"\n\t# Eigenvector\n\t(W, z) = np.linalg.svd(C)\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn sorted(self, key=self.r, reverse=True)[0]\n\n\tdef __len__(self):\n\t\t\"\"\" Return the number of items in the stack. \"\"\"\n\t\treturn len(self.stack)\n\n\tdef __bool__(self):\n\t\t\"\"\" Return True if the stack is non-empty.\"\"\"\n\t\treturn len(self)!= 0\n\tdef __contains__(self, item):\n\t\t\"\"\" Implementor of `in` (and `not in`) operator.\n\t\t\tNote:\n\t\t\t\t:class:`.Stack` does not support an \"in"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tif self.capacity == 0:\t## This method is called when the map is empty.\n\t\t## This is where the key has to exist\n\t\treturn self.__key, self.__value\n\telse:   # All keys in the map have to be unique\n\t\tfor i in range(self.capacity):\t# Looping over the keys in the map\n\t\t\tkey = self.getItem(i)\n\t\t\tif key in self.__seen:\n\t\t\t\tnum_occurrence = self.__seen[key]\n\t\t\t\tself.__seen[key] = num_occurrence +"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tused = set()\n\tm = max(self)  \t\t# The max value of the mapping\n\tfor (key, _) in self:\n\t\tif len([value for key, value in self if key <= m and value == key]) > 1:\n\t\t\tused.add((key, value))\t\n\treturn [(key, self.pop(key)) for key, _ in self if len([key for _, key in self if key <= m and value == key]) > 1 and key not in used]\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn heapq.heappop(self.heap)\n\n  def prune(self, threshold=1):\n\t\"\"\"\tRemoves (key, value) pairs such that the cumulative sum of the keys occurs\n\tn.b.: The pruning threshold argument must be set to 1 when you want to include the values of keys and delete them. \"\"\"\n\t\n\tnew_heap = []\n\tfor key, val in self.heap:\n\t\t#if float(value) * 3 >= threshold:\n\t\t#\tnew_heap.append((value, key))\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tkeyDict = dict(self)\n\treturn self.pop(keyDict.popitem(last=False)[0], last=False)\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tself._last = self._data.popitem()\n\treturn self._last\n\n# --------------------------------------------------------------------\n#\n#                   Simple File I/O\n# --------------------------------------------------------------------\n\n# --------------------------------------------------------------------\n#\n#   Simple class to hold some utility stuff\n#   (used in File.write)\n#\n# --------------------------------------------------------------------\nclass SimpleFileIO:\n\tdef __init__(self, file):\n\t\ttry: file = file.name\n\t\texcept: pass\n\t\tif file == \"/dev/stdin\": file = sys.stdin\n\t\tif file == \"/dev/stdout\":"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tif self.is_empty() or self.num_items() == 0:\n\t\traise KeyValueError(\"HashTable is empty\")\n\n\t# Get and remove a random number between 0 and 1: r = random()\n\trand_val = random.random()\n\n\t# The value will be popped with value = max(hashtable[r], key)\n\t# Where \"hashtable\" are a FIFO list, and \"key\" stores the key (as a pair).\n\ttemp = self.pop()\n\tif temp is not None:\n\t\tif rand_val >= ("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tassert self, \"Cannot popitem() of an empty map\"\n\treturn self._get_e()\n\n\nclass _GetItem(_MutableGetItem):\n\tdef __init__(self, *args):\n\t\tself._mapping = dict((k, self._getitem(v.key)) for k,v in _MutableMapping.items())\n\t\tself._key = super(_GetItem, self)._key\n\t\tassert self._key not in self._mapping\n\t\tsuper(_GetItem, self)._initial_pop()\n\n\tdef popitem(self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\twhile(self.count()>0):\n\t\tself.popitem()\n\treturn self.items[-1]\n"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn self.heappop(self._heap)[1]\n\n  def add(self, key, value):\n\t\"\"\"\n\tAdd a new pair with `key` equal to the key and `value` to the heap.\n\t\n\tArgs:\n\t\tkey (int): Key of inserted pair\n\t\tvalue (int): Value of inserted pair\n\t\n\tReturns:\n\t\tNone\n\t\"\"\"\n\tself._heap.append((key, value)) \n\n  def heappop(self, *args):\n\t\"\"\"\n\tThis performs the heap popation procedure, and"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn self.heap[self.heapq.heappop(self.heap)-1]\n\t\n#\tdef replace(self, key, value):\n#\t\tfor i in range(len(self.heap)):\n#\t\t\t# We can get multiple occurrences of the same key.\n#\t\t\tif self.heap[i][0] == key:\n#\t\t\t\tbreak\n#\t\t\tif self.heap[i][1] < value:\n#\t\t\t\tcontinue\n#\t\t\ti += 1\n#\t\t\tif i < len(self.heap):\n#"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\ttry:\n\t\treturn self.heap[0]\n\texcept IndexError:\t\n\t\treturn None\n\t\n\t\nclass LRUCache:\n\t\"\"\"\n\tImplements a LFU (Least Frequently Used) cache.\n\t\"\"\"\n\n\tcache = {}\t\t# A dictionary to manage the cache.\n\tsize = 0\t\t# Number of items stored in the cache.\n\t\n\tdef __init__(self, capacity = 32):\n\t\t\"\"\"The `capacity` parameter is the maximum number of items\n\t\tto cache in the cache. This will be the maximum number"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tret = []\n\tfor it in self.iteritems():\n\t\tif it == self.peekitem():\n\t\t\tdel self.peekitem()\n\t\telse:\n\t\t\ty = it[1]\t\t\t# Remove the item, and remember it's value\n\t\t\tx = self.popitem()\t# The node that got deleted; the node itself\n\t\t\tself.insert(x[0], x[1], x[2]) # We don't want to set its key, so add it back\n\t\t\tdel x\n\tif self.peekitem == self.top:\n\t\treturn"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tif self.keys:\n\t\tif len(self) == len(self.keys):\n\t\t\t# OOB\n\t\t\tlast_key, last_value = self.keys[-1]\n\t\t\tself.popitem(last_key)\n\t\telse:\n\t\t\t# Dense by size\n\t\t\tlast_key, _ = heapdict(self.keys).popitem(lastNodeOnly = False)\n\t\t\tlast_node = 0\n\t\t\twhile len(self.keys) and last_key < self.keys[-1][0]:\n\t\t\t\tlast_node += 1\n\t\t\t\tif last"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tcached = dict.get(self._cache, k=-1)\n\tif cached:\n\t\tdel self._cache[cached[0]]\n\t\treturn cached[1]\n\telse:\n\t\treturn None\n\n\t\n# --------------------------------------------------------------------------\nif __name__ == \"__main__\":\n\tfrom heapq import nlargest\n\n\tn=10000\n\twith open(\"pics_tweet/a.dat\") as f:\n\t\tbuf = []\n\t\tfor line in f:\n\t\t\td = line.split(\" \", 1)\n\t\t\tbuf.append(dict(zip("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tif self.count!= 0:\n\t\tself.count -= 1\n\t\treturn self.data.popitem()\n\traise KeyError('pop(key) takes at least 1 argument (0 given)')\n \ndef _nlargest(n, seq, key, f):\n    \"\"\"Equivalent to: f(heapq.nlargest(n, seq, key=key))\"\"\"\n    assert key is not None\n    return [item for item, c in heapq.nlargest(n, enumerate(seq), key=key)\n            if c]\n \ndef"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tv = self.values.pop(0)\n\tkey = self.keys.pop(0)\n\treturn key, v\n\n    def clear(self):\n        \"\"\"\n        Remove all entries from this map, maintaining insertion order.\n        \"\"\"\n        self.keys.clear()\n        self.values.clear()\n        while len(self.keys):\n            self.keys.pop()\n            self.values.pop(0)\n\n    def get_value(self, key):\n        \"\"\"\n        Return value for `key`, or `None`.\n\n        If key isn"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\twhile self.size > 0:\n\t\t(key, _) = self.queue.popleft()\n\t\tif self.map._hash[key] == self.map._hash[self.prev[key]]:\n\t\t\tself.map._hash[key] = self.map._hash[self.prev[key]]\n\t\t\tself.map._hash[self.prev[key]] = key\n\n\tif self.map._hash[self.prev[self.next[key]]]!= key:\n\t\traise KeyError('Could not pop off a null-key map item')\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tif not len(self):\n\t\treturn \n\titem = self.head\n\n\twhile item is not None:\n\t\tif item is self.head:\n\t\t\tbreak\n\t\tif item is self.tail:\n\t\t\tnext_item = self.tail.prev\n\t\t\titem = self.head\n\t\telse:\n\t\t\tif item is self.tail.prev:\n\t\t\t\tnext_item = None\n\t\t\t\titem = self.head\n\t\t\t\twhile item is not None and item.prev is None:\n\t\t\t\t\titem = item.next\n\t\t\t\t\tnext_item = item\n\t\t\telse:\n\t\t\t\tnext"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\ttry:\n\t\telem = self._sorted_items.pop()\n\t\tif self._q:\n\t\t\tself._q.remove((self._q,elem))\n\t\tself.data[0] = self.data[1]\n\t\ttry:\n\t\t\tdel self._key\n\t\texcept (Exception) as e:\n\t\t\tpass\n\t\ttry:\n\t\t\tdel self._last_modified\n\t\texcept:\n\t\t\tpass\n\t\tself.data.close()\n\texcept:\n\t\tpass\n\tself.data = self.data[:]\n\treturn elem\n\nclass"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\treturn self.buffer[-1]\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\treturn self[-1]\n\n\nb = heapq.nlargest(3, [(3, 1), (3, 2), (4, 4)])\n\nprint b\n\na = heapq.nlargest(2, [(4, 4), (5, 5)])\n\nprint a\n\na = heapq.nsmallest(2, [(5, 5), (4, 4)])\n\nprint a\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif self.dict.size()>0:\n\t\tlargest_val = -1\n\t\tlargest_key = None\n\t\tfor i in range(len(self.dict.items)):\n\t\t\tkey = self.dict.items[i][0]\n\t\t\tif self.dict.values[i]!= largest_val:\n\t\t\t\tif largest_val!=None:\n\t\t\t\t\tdel self.dict[key]\n\t\t\t\t\tlargest_key = key\n\t\t\t\t\tif largest_val!= None:\n\t\t\t\t\t\tself.dict[key] = largest_val\n\t\t\t\telse:\n\t\t\t\t\tdel self.dict"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif not len(self.keys) == len(self.dict.keys()):\n\t\traise KeyError('popitem() not implemented for dict-based key-based mapping backends.')\n\n\ttry:\n\t\tk = next(iter(self))\n\texcept StopIteration:\n\t\traise KeyError('popitem() not implemented for dict-based key-based mapping backends.')\n    \n\tfor i in range(self.capacity):\n        try:\n            v = next(iter(self.dict[k]))\n        except StopIteration:\n            raise Key"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif len(self) == 0:\n\t\traise KeyError(\"No items available\")\n\tget_key = self.__dict__.keys()[-1]\n\titem = (get_key(), self[get_key()])\n\tself.remove(get_key())\n\treturn item\n\n\ndef add(self, element):\n\t\"\"\"\n\tAdd an element to the set, if it does not already exist.\t\"\"\"\n\titem = (element.get_key(), element)\n\tif item not in self:\n\t\tself.add(item)\n\n\ndef"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\treturn self.__item[0]\t\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t# return dict.popitem(self)\n\td = self._dict()\n\td.popitem()\n\treturn (d[key], value)\n#endclass\n\n# endregion base_class.__init__\n\n\n# region class_methods\n\n\nclass Base(type):\n\t\"\"\"\n\tBase class for all bases.\n\tUsed in several cases, including:\t\n\t1. The main.base module\n\t2. The PyCQCore module\n\n\tThe `__new__` method inherited from `type.__new__` returns an instance \n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif self.is_empty:\n\t\traise KeyError('Dictionary is empty')\n\telif self.size == 1:\n\t\treturn (self.delete_last(), None)\n\telif self.size == 1:\n\t\treturn (None, self.delete_last())\n\telse:\n\t\tlast_used_key = self.pop()[0]\n\t\tlast = self.pop()[1]\n\t\treturn last_used_key, last\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif not hasattr(self, '__dict__'):\n\t\traise RuntimeError('Trying to popitem(), but connection '\n\t\t\t\t   'text is not loaded, or no text connection is open')\n\tdata = self.__dict__.copy()\n\t# Return\n\tdata.pop('_db_connection', 'Connection has been closed')\n\treturn (data.pop('_getItem', 'get_item'),)\n"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\titem = self.popitem()\n\tif _debug:\n\t\timport pprint\n\t\tpp = pprint.PrettyPrinter(indent=4) \t\t\t\n\t\tprint(pp.pprint(item)) \t\t\t\n\treturn item \n\n##\n# @brief: Use key-value store to handle the synchronization of data objects.\n# @details: This class is an alternate implementation of a\n#\t\t'Binary Store' class. The primary purpose of a 'Binary Store' class\n#\t\tis to store any data object in a binary format. It also allows us"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\ttry:\n\t\titem = key, value = self.popitem(False)\n\t\treturn item or '', '', ''\n\texcept KeyError:\n\t\t# key does not exist,\n\t\t# so ignore what is left in args\n\t\tif not isinstance(self.tuple_key_value_pairs(), tuple):\n\t\t\tself.tuple_key_value_pairs()\n\t\t# at this point, all keys and values are tuples, so this should be a safe\n\t\t# attempt to just use the latest value for the second element.\n\t\treturn tuple(\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tkey = random.randint(0, len(self) - 1)\n\treturn self.pop(key)\t\n\ndef reverse(self):\t\n\t\"\"\"\n\tRemove and return the list of values in reverse.\t\n\t\"\"\"\n\treturn self._reversedval\n\ndef __len__(self):\t\t\t# Needed because of cyclical references.\n\treturn len(self._sequence)\n\ndef __getitem__(self, i):\n\tif isinstance(i, slice):\n\t\treturn [self[k] for k in self._sequence[i"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn self.randpop()\n\n\t\n\nclass Stack:\n    \"\"\"A data structure for a stack.\n    \n    >>> st = Stack()\n    >>> print(st)\n    ()\n    >>> st.push('a')\n    >>> print(st)\n    (a,)\n    >>> st.push('b')\n    >>> print(st)\n    (b, a)\n    >>> st.push('c')\n    >>> print(st)\n    (b, c, a)\n    >>> st.pop()\n    b\n    >>> print(st)"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tkey, value = self._l.popitem(last=False)\t# last=False, default last=True\n\tself.__getitem__(key, defaultf=object())\t\t# prevent KeyError exceptions\n\treturn key, value\n\t\nclass WeakList(list):\n\t\"\"\"\tA `list` w/o an `__dict__` (which is common to weak python containers like lists).\"\"\"\n\tdef _get_dict(self):\n\t\treturn self._d\n\tdef _set_dict(self, d):\n\t\tself._d = d\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tpopitemitem = self.popitem()\n\treturn popitemitem\n\n\ndef items(self):\n\t\"\"\"\n\tReturn a list of dicts, returned as a list of `(key, value)` pairs.\n\t\"\"\"\n\treturn self.items()\n\n\ndef keys(self):\n\t\"\"\"\n\tReturn the list of keys in the dictionary.\t\"\"\"\n\treturn self.keys()\n\n\ndef values(self):\n\t\"\"\"\n\tReturn a list of values in the dictionary.\t\"\"\"\n\treturn self.values()\n\n\ndef has_key(self, key"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn (self.popitem(), self.popitem())\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn random.choice(list(self.iteritems()))\n\nclass RandomStream(RandomProvider):\n\t\"\"\"\n\tA random provider to be applied to any `Stream` instance.\n\n\tIf the instance of `RandomStream` was created using a `provider` option, it will be applied to the instance of `Stream`:\n\n\t\tstream = RandomStream(config=config, provider=provider, stream_instance_class=MyStream(stream=stream)\n\t\n\tOtherwise, it will be used instead of any provider.\n\t\"\"\"\n\tdef __init__(self,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tkey, value = self.data.keys()[0]\n\tdel self[key]\n\treturn key, value\n\nclass Cache(object):\n\t\"\"\"A cache data structure.\n\t\n\tA cache data structure maintains a cache of results for any function. The\n\tcache only exists when you run an Othello session on a specific `SudokuSolver`\n\tobject; it does not mean that all of the solutions in the game have been\n\tcorrect. This means that you have to check each board row, block and move\n\tcells to see if you have to keep"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\n\treturn self.dict[key][value]\n\t\n\t#endregion popitem"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tregex = re.compile(r\"^(?P<field>\\S+),(?P<operator>==|<=|>=|>|=|<|>?)(\\s*.*)$\")\n\t#regex = re.compile( r\"^(?P<field>\\S+),(?P<operator>==|<=|>=|>|<|>?)\\s*\\(((?P<value1>.*?)\\|(\\S*))\\)\\s*$\")\n\treturn regex\n\t\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\ttemplate = re.compile(r\"(?%s)(\\w+)*\")\n\n\t# Check for empty/empty string string\n\tif len(self.parameters['value']) == 0:\n\t\ttemplate = re.compile(r\")\")\n\tif len(self.parameters['default']) == 0:\n\t\ttemplate = re.compile(r\"[^a-zA-Z0-9_:\\.\"]\")\n\t\t\n\t# Special check for single value\n\tif len(self.parameters['value']) == 1 and type(self."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(self.pattern, re.UNICODE | re.VERBOSE)\n\t\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(self.in_style)\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(self.__value_re)\n\ndef _get_regex_type(r: \"Any\") -> Type[\"BaseParameterRegex\"]:\t\"\"\"\n\tGet the type of the regular expression (:class:`re.Pattern`).\n\t\n\tThis function only exists in Python 3.7 or later as it contains\n\ta built-in keyword for this function (:data:`re.Pattern() is a built-in function`)\n\t\"\"\"\n\treturn getattr(r, \"pattern\", \"re.Pattern\")\n\n\n@typecheck"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tif not self.name:\n\t\t# create a new regex object with a named capturing group\n\t\treturn self.pattern.compile(self.in_text)\n\n\treturn re.compile(self.in_text)\n\ndef as_is(*args, **kwargs):\n\t\"\"\"\n\tHelper function for creating a custom regex for an entire group.\n\n\tAll arguments are required (:class:`Group`), so this should be called::\n\n\t\tfrom wdnslookup2.re2 import as_is\n\t\texample = \"example.com\"\n\t\twith Group"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(re.escape(self.pattern.replace('{',\n\t\tr'\\{').replace('}', '\\}')))\n\n\t#\n\t#\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(r\"^[_a-z][\\w\\-]*\\.@[_a-z][\\w\\-]*\\.@[_a-z][\\w\\-]*\")\n\n\n#########################################################################################################################\n#                                                                                                    #\n#  This class contains a collection of customizer objects and methods to do things with the          #\n# contents of a json file (i.e. that contains JSON objects that may be modified). The content of a JSON       #\n# file is typically obtained directly from the sourcecode location and can be modified by the"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\n\tif not self.in_regex:\n\t\tself._in_regex = re.compile(self._in_str)\n\treturn self._in_regex\n\ndef regex_string(pattern: str, in_regex: Optional[Pattern] = None) -> str:\n\t\"\"\"\n\tCreate a custom regular expression expression string.\n\n\tYou can define your regular expression here, but do NOT use it yet in order\n\tto define it your regular expression itself does a lot of nasty things.\t\n\n\tTo get the `in_regex` regular expression:"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tp = re.compile(r'\\$\\{in\\}')\n\tp.re_flags |= re.I\n\treturn p\n\ndef _string_type(type: str) -> Type[Union[str, Pattern]]:\n\t\"\"\"\n\tReturns True if type is a string.\n\nReturns :class:`typing.Union[str, re.Pattern, Pattern]`.\n\t\"\"\"\n\treturn typing.get_type_hints(type, local_variables=globals())['return'].sub('', type)\n\ndef _"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\traise NotImplementedError()\n\nclass _DefaultConverter:\t\"\"\"\n\tDefault parameter style converter.\n\tThe parameter style converter automatically uses the converter class's\n\tconversion function when converting the parameters at the :meth:`_set`\n\tmethod called on the instance. It then does the conversion itself\n\twhen the user changes parameter values.\n\t\"\"\"\n\tdef _call(self, params: List[_T], default=_None):\n\t\t\"\"\"\n\t\tCall self.converter.conv function on all the parameters.\n\n\t\tParameters\n\t\t----------\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tstyle_converter = parameter.style_converter(self.params)\n\tif style_converter is None:\n\t\traise ValueError('There is no style converter available for this context.')\n\treturn style_converter\n\n\nParameter.register_converter('nested', (Nested, 'nested_str', 'nested_s') + _Converter)\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn conv.Converter(self)\n\n\nclass _Converting:\n\n\t\"\"\"\n\tAbstract base class for converters.\n\t\"\"\"\n\n\t@staticmethod\n\tdef __is_conv(cls):\n\t\treturn hasattr(cls.__module__, 'conv')\n\n\tdef __init__(self):\n\t\tpass\n\n\t@classmethod\n\tdef converters(cls):\n\t\tif _.is_conv(cls):\n\t\t\treturn cls.__conv()\n\t\telse:\n\t\t\traise ValueError('This is not a converter type')\n\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tif not self.style_converter:\n\t\tfrom. import converting\n\n\t\tself.style_converter = converting.Converter.get_converter(self.style_options,\n\t\t\t\t\t\t\t\t\t\t\t\t\tnames=self.allowed_stylename)\n\treturn self.style_converter\n\n\nclass ParametersGrid(_object):\n\t\"\"\"Creates a grid of widget parameters.\n\n\t``ParametersGrid`` should not be instantiated directly.\n\t\"\"\"\n\n\t__metaclass__ = ABCMeta\n\n\t# Default parameters\n\tspacing = 5\n\tgrid_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\n\treturn _converting.parameter_style_converter(self)\n\n\n@contextmanager\nclass _get_parameter_name_dict(object):\n\t\"\"\"\n\tContext manager for getting a dictionary of the parameter names.\n\n\tUses ``parameter_style_converter.parameter_names`` to determine the names.\n\n\t:class:`ParameterExporter` will find the parameter names in the ``parameter_style_converter.parameters`` dictionary \n\tand use the name in it as the name for the parameter.\n\n\tA context"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\timport _paramstyle\n\tif self.paramstyle is None:\n\t\t_paramstyle.Paramstyle()\n\t\tself.paramstyle = 'qmark'\n\treturn _converting._Converter(\n\t\t\tself.paramstyle,\n\t\t\t_paramstyle.get_paramstyle(self._output_charset),\n\t\t\tself.quoting,\n\t\t\tself.lineterminator,\n\t\t\tself.encoding,\n\t\t\tself.dialect, self._output_hook,\n\t\t\tself.dialect_options)\n\n#\n#   Parameter Styles\n#\nCOMMENT"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tresult = self._converter_stack.get(False)\n\tif result is None:\n\t\tresult: _converting._Converter = self._converter_stack.get(False)\n\t\tif result is None: result = Converters(defaults=self.get_converter_properties())\n\t\tself._converter_stack.set(False, result)\n\treturn result\t\n\nclass ConversionStrategy(_basic_converter):\n\t\"\"\"\n\tConversion Strategy:  an API Converter that knows how to convert\n\ta string into"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting.Converter(self._parameter_style)\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn Converter(\n\t\t_converting._type[_CONVERSION_CONVERTER], \n\t\t(lambda v, a: isinstance(v, _parameter.Parameter)), (str, float),\n\t\t(int, float),\n\t\t_CONVERSION_REGISTRY\n\t)\n\n\ndef register_converters_dynamic(\n\tparam_form: str, \n\tconverter: Union[_converting._Converter, None], \n\tparameter_type: _parameter.ParameterType = None\n) -> None:\t\n\t\"\"\"\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\n\tparameter_type = type(self._parameter)\n\treturn parameter_type(\n\t\t\tself._type,\n\t\t\t*self._parameters,\n\t\t\t**kwargs\n\t)\n"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tdt_str = _normalize_iso8601_string(dt_str)\n\ttry:\n\t\treturn _parse_datetime_iso8601_str(dt_str)\n\texcept Exception:\n\t\treturn _parse_datetime_iso8601_str(dt_str, strict=True)\n\n\nif PY2:\n\t# Python 2 specific\n\tdef _parse_iso8601_str(dt_str):\n\t\t\"\"\"\"\"Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n\t\t.. warning"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\ttz = None\n\tif not dt_str:\n\t\tdt_str = str(dt_str)\n\ttry:\n\t\tdateutil.parser.parse_datetime(dt_str)\n\texcept:\n\t\t# Bad input.\n\t\tpass\n\telse:\n\t\tdate_str, tz_str, iso_offset = dt_str.partition('T')\n\t\tiso_offset = iso_offset and iso_offset[:-1]\n\t\tisoparse_str_date = str(datetime.datetime.strptime(date_str"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\timport re\n\n\t# Split out ISO-8601 formats in order of most to least significant\n\t# bits, then to the least significant bits. See the comments below\n\t# for more info.\n\t\t\t\t\n\tiso_8601 = (\"%e%b%dT%H:%M:%S.%f\")\n\n\tday_parts = re.findall(r'T{0,2}([^,]+?),', dt_str, re.I)\n\ttime_parts = re.findall(iso_8601, dt_str, re."}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t# This function is deliberately buggy since parsing ISO-8601 formats\n\t# with Python 2.6.x is not compatible, but the Python 2.7.0 standard\n\t# is compatible.\n\ttry:\n\t\tif not isinstance(dt_str, io.IOBase):\n\t\t\tdt_str = StringIO(''.join(dt_str))\n\t\tdt = datetime.datetime.strptime(dt_str.read().strip(),\n\t\t\t'%Y-%m-%dT%H:%M:%S%z')\n\texcept ValueError"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t# Split string into components.\n\tsplit = dt_str.split(',', 1)\n\t\n\t# Convert any valid ISO-8601 datetime strings into\n\t# :class:`dateutil.parser.parser.parser.Parser` objects.\n\t(\n\t\tdt_obj,\n\t\tcomponents,\n\t\tkwargs\n\t) = __isoparse(split[0])\n\n\tassert ((dt_obj is not None) and (len(components) % 2) and\n\t(components[1] == ''))\n\n\tunparsed ="}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tif isinstance(dt_str, compat.string_types):\n\t\tdt_str = dt_str.encode('utf-8')\n\tdt_str = dt_str.replace('T','').replace(' ', '')\n\tdt = datetime.datetime.utcfromtimestamp(\n\t\ttime.mktime(time.strptime(dt_str, \"%Y-%m-%d %H:%M:%S\")[:6]))\n\treturn dt if dt.tzinfo is None else dt.ast"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tdt = dt_str.strip()[1:] if dt_str else None\n\treturn __isoparse__(dt, self.date_format, self.time_format, self.default_time_zone).datetime\n\ndef parse_datetime(dt_str):\n\t'''\n\tParse a string containing an ISO-8601 datetime string as a\n\t:class:`datetime.datetime`.\n\n\t:param dt_str:\n\t\tAn ISO-8601 datetime string.\n\n:return:\n\tReturns a :class"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tr = isoparse1(dt_str)\n\tif r is not None:\n\t\t# For backwards compatibility, if the original string returned by\n\t\t# isoparse1 didn't contain a valid date, it is an invalid\n\t\t# ISO-8601 parse.\n\t\tif r.tzinfo.utcoffset(datetime(r.year, r.month, r.day)) is None:\n\t\t\treturn datetime(r.year, r.month, r.day).replace(hour=0, day=1, hour=24)\n\n\t\treturn r\n\t"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t# pylint: disable=redefined-builtin\n\tdef isoparse(*a, **kw):\n\t\t\"\"\"\n\t\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n\t\tThis method checks using the strictness of the parser.\n\n\t\tIf the datetime is not a valid ISO-8601 string, this method will\n\t\traise a `ParserError` exception.\n\n\t\t:param (str) dt_str:\n\t\t\tA string or stream containing only an ISO-8601 datetime string\n\t\t\"\"\"\n\t\t# The"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\timport string\n\tfrom dateutil.tz import tzoffset, tzstring_re, tzoffset_re\n\timport dateutil.parser as dpars\n\tstrt,end=string.ascii_lowercase, string.ascii_uppercase\n\tif dt_str.strip().decode('ascii').startswith(strt)+\"\".join([c+c for c in strt]).upper() or dt_str.strip().decode('ascii').isspace().lower():\n\t\treturn datetime.datetime"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tparser = Parser(**kw)\n\ttokenizer = parser.tokenize\n\tparser.tokenize = strptime_tokenize\n\n\tinfix = r'(?P<key>[-+]?)' + DIGITS + '(?P<value>.+?)' + UNDERSCORE\n\tinfix_with_tokens = r'(?:' + infix + '(?:' + infix +')?' + infix + ')'\n\n\t_result = _parse_datetime_with_tokens(parser, infix_with_tok"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t# move this function out, then import the string parser directly)\n\tfrom._strp import STRPLIB_TO_parser\n\tp = STRPLIB_TO_parser()\n\n\treturn p.parse(timestr, TZID_PATTERN, **kwargs)\n\ndef strptime(date, fmt):\n\t\"Parses a date and format. %f supports: datetime.datetime, %s\" % (str(reversed(_pick_tsatime)), copy_compat)\n\tx = _parse_strptime(f"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t# This code is based on datetime.strptime, http://code.activestate.com/recipes/576931/ }\n\n    kwargs['ignoretz'] = ignoretz\n\n    for pos, arg in enumerate(timestr.split(None, 1), 1):\n        # For time-only argument, convert it to full-time\n        #\n        # SSVEP is the most natural and convenient format, on the\n        # way across the board there are more convenient options.\n\n        # Check if 'SSVEP' is present, if not it is"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tdef _handle_tzname(tzname, tzoffset, tzoff):\n\t\tif not isinstance(tzname, string_types):\n\t\t\traise TypeError('tzname %r must be string' % tzname)\n\t\tif ':' in tzname and not [s.isdigit() for s in tzname.split(':')][0]:\n\t\t\traise ValueError(\"%r does not have correct format\", tzname)\n\t\tif tzoffset is not None:\n\t\t\tif not callable(tzoffset):\n\t\t\t\traise ValueError(\"tz"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n\t# Initialize the default module-level parser/creation\n\t_parser = _creazione_module.parser()\n\t\n\treturn _dateutil_parser.parse(  # type: ignore[arg-type]\n\t\t\tself, tzinfos, fuzzy_with_tokens\n\t\t)\n\ndef parse(timestr, default=None, ignoretz=False, tzinfos=None, fuzzy_with_tokens=False):\n\t\"\"\"\n\tParse the date/time string and return the result.\n\n\t:param timestr"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tdef _isdate(expr):\n\t\treturn bool(expr and expr.year)\n\tdef _parse(s, _fmt, **kwargs):\n\t\tretval = _fmt.parse(s, **kwargs)\n\t\tif _fmt.ignore(s):\n\t\t\treturn retval\n\t\tif _fmt.fuzzy or _isdate(retval):\n\t\t\treturn retval\n\t\traise ParserError(_(\"Parser error: syntax error at char %d\") % len(s))\n\treturn _parse(timestr, _parse)\n\n"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\treturn _parse(timestr_type, timestr, **kwargs)\n\n\ndef parse_interval(timestr):\n    \"\"\"Parses a time interval string and returns a :class:`Interval`.\n\n    The units of the interval are the same as those of :class:`timedelta`.\n\n    This function is an implementation detail about which version\n    will be used to parse the time interval string.  It will not\n    parse RFC 2445 compliant interval strings.\n    \"\"\"\n\timport math\n\n    if timestr is None:\n        raise Value"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tif not isinstance(timestr, str):\n\t\tmsg = \"argument must be str, not %s\" % type(timestr)\n\t\traise TypeError(msg)\n\n\tif default is not None:\n\t\tif isinstance(default, datetime.datetime):\n\t\t\treturn _ew.parse(\n\t\t\t\ttimestr, tzinfos=tzinfos, default=default, **kwargs)\n\t\telse:\n\t\t\traise TypeError(\"must be datetime.datetime object or \"\n\t\t\t\t\t\t\t\"None, got %s\" % type(default))\n\n"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tres = _parse_datetime(self.reader, ignoretz=ignoretz, **kwargs)\n\tif res is None:\n\t\traise ParserError(\"parsing error: next date incorrect or string provided\")\n\telif error:\n\t\traise ParserError(str(error), datetime=datetime(1, 1, 1).replace(microsecond=0))\n\telse:\n\t\treturn res\n\ndef _parse_tokens(self: Tokenizer, tokens: TokenContainer,\n\t\t   default: Optional = None, **kwargs) -> Any"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t#import re;file = None\n\tdef _parse(self, timestr, tzinfos=None, *, ignoretz=False,\n\t\t  fuzzy_with_tokens=True):\n\t\t\tyy = self.year\n\t\t\tmm = self.month\n\t\t\tif self.hour:\n\t\t\t\tyy = (yy % 100) + 1900\n\t\t\tdigits = len(str(yy))\n\t\t\tday, month, year = self.year, self.month, self.day\n\t\t\t# formatting for ISO 8601\n\t\t\t# drop leading zeros\n\t\t\twhile"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn fromutc(dt).round(0)\n\t\nclass _DateTime(datetime.timedelta):\"\"\"\"\"\"\"\t\t\n\t\t\n\tdef __init__(self, days=0, *args):\n\t\t\"\"\"\n\t\t:param days:\n\t\t\tIf this option is provided, then the object will be calculated\n\t\t\tby returning the current day and time from the datetime.\t\n\t\t:type time:\n\t\t\tA :class:`datetime.time` object. The default is time.time().\t\n\t\t:param time:\n\t\t\tIf this option is provided,"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# The two important methods below are\n\t# 1. to_time() \n\t#\t:\tto_time(self)\n\t# 2. from_time()\n\t#\t:\tfrom_time(self, dt)\n\tdef toTime(self):\n\t\t\"\"\"\n\tReturns a timezone-aware datetime in a given timezone.\n\t\n\t:return:\n\t\tA timezone-aware :class:`datetime.datetime` object. \n\t\"\"\"\n\t\treturn self\n\tdef fromTime(self, dt):\n\t\t\"\"\""}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn datetime.datetime.combine(dt.date(), dt.time())\n\t\n"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif self._fold and dt.dst() is None:\n\t\t# datetime is in fold: ambiguous, and has no explicit timezone.\n\t\t# This is not a general case, but it's the first general case we\n\t\t# support.\n\t\treturn self.fromutc(dt)\n\timport pytz\n\ttzi = pytz.timezone(self._tzstr)\n\tdt -= tzi.utcoffset(dt)\n\tdt = dt - tzi.tzinfo.localize(dt, is_dst=self._fold)"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second,\n\t\t\t\t\ttzinfo=dt.tzinfo)\n\ndef _datetime_fromutc(self, dt):\n\t\"\"\"\n\tEquivalent to ``fromutc()``. Added to avoid reimplementing\n\t``datetime.datetime.fromtimestamp()`` when ``fromutc()`` returns a\n\tzero-argument ``datetime.datetime`` object.\n\t\"\"\"\n\treturn _datetime_from"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tfrom pytz import timezone\n\n\tfold = self._fold\n\t# In pytz, timezones cannot have fold states, so in order to make this\n\t# work, we'll first set up the \"predefined\" fold timezone object using the\n\t# pytz timezone API from which the standard library would return a\n\t# timezone object; this is the time zone it uses to take ambiguous\n\t# datetimes in a naive UTC time.\n\ttz_api = timezone(self.tz)\n\ttz = timezone(self.tz)\t\n\t\n\t"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tf = self.__getnow()\n\tif dt.tzinfo:\n\t    dtz = dt.tzinfo\n\t    # Look up the local timezone's standard time, the datetime itself\n\t    # if it's not one there, it should be aware; since the local timezone\n\t    # is the timezone of UTC, local time is treated as tzutc for this\n\t    # case.  If the datetime is already tzutc, it's already tzutc\n\t    if not dtz.utcoffset(dt"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn _localize(dt, localtime, False) or dt\n\tif dt.year > self._tzinfo._zoneinfo_offset.isodate(dt):\t# the \"fold\" state\n\t\treturn dt\n\treturn _localize(  # we're using that for ambiguous datetimes\n\t\tdt, localtime, dt.fold\n\t).astimezone(self._tzinfo)\n\t#\n\t# return localtime.astimezone(self._tzinfo)\n\t#\n\t# We can't cache an ambiguous timezone aware :any:"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\traise ValueError(\"'dt' argument should be a timezone-aware datetime.\")\n\n\tif not self_offset_fromzero(dt, False):\n\t\t# It's ambiguous.\n\t\treturn self.localize(dt, is_ambiguous_aware_of=dt)\n\n\t# It's in a fold.\n\treturn dt + \\\n\t\t(dt - self.localize(dt.astimezone(pytz.utc), False)) - TZ_OFFSET_FOLD_COUNT\n\n\t#"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\ttry:\n\t\tif dt == datetime.now(settings.TIME_ZONE): return datetime.now()\n\t\telif dt is None: return datetime.now()\n\t\telse:\t\t\t  # dt comes from the user\n\t\t\tif not self._is_ambiguous(dt, False, settings.USE_TZ, settings.TIME_ZONE):\n\t\t\t\t# We're unambiguously in a time, no need to handle ambiguous\n\t\t\t\treturn dt\n\t\t\telse:\n\t\t\t\t# We're in a fold, so the ambiguous time should be returned.\n\t\t\t\t"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t# this is not a datetime coercion\n\n\treturn dt.replace(tzinfo=tzinfo).astimezone(tzinfo=None)\n\n#\n# class _tzfix(tzinfo):\n#     \"\"\"\n#     Provides the base implementation for the fixed offset tzinfo instances\n#     \"\"\"\n#     def utcoffset(self, dt):\n#         return timedelta(0)\n#\n#     def dst(self, dt):\n#         return timedelta(0)\n#\n#     def tzname(self, dt):"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tdtt = dt.astimezone(tzinfo=tzinfo)\n\tdtt = tzutil.applytz(dtt,tzinfo)\n\treturn dtt.replace(**{'tzinfo':tzi}).date()\n\ndef utcoffset(dt):\n\t\"\"\"\n\tReturns the offset of the input datetime from the utc timezone as\n\ta timedelta.  If the input datetime is naive, this raises a\n\t:py:class:`ValueError`\n\n\t:param dt:\n\t    This is the naive"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\ttz = dt.tzinfo\n\tif tz is None: tz = dflt_tz\n\tif tz and dt.utcoffset()!= timedelta(0):\n\t\traise ValueError(\"datetime must be exact\")\n\treturn dt.replace(tzinfo=tz)\n\ndef localize(dt, tzinfo=None):\n\t\"\"\"\n\tLocates a :class:`datetime.datetime`, specified by ``dt``, at a local timezone\n\t``tzinfo``, which is a ``datetime.tzinfo`` instance"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\treturn dt.replace(tzinfo=tzinfo)\n\nclass parser(object):\n    \"\"\"\n    Class that parses a string into a format that is supported by\n    ``dateutil.parser.parse`` algorithm.\n    \"\"\"\n    def __call__(self, format, **kwargs):\n        return (dateutil.parser.parse(format)\n               .date()\n               .replace(tzinfo=self.tzinfo))\n\n    def __getinitargs__(self):\n        \"\"\"\n        Because self.tzinfo is unset, it's not possible to"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t# For historical information, this will return UTC rather than local\n\tif tzinfo is None:\n\t\treturn dt.replace(tzinfo=_UTC)\n\nclass TzPickler(pickle.Pickler):\n    \"\"\"\n    A :class:`pickle.Pickler` that also takes a\n    :class:`dateutil.tz.tzfile.TZInfo` object as its constructor argument\n    \"\"\"\n    tzinfo = None\n\n    def __init__(self, *args, **kwargs):\n        self.tzinfo = kwargs."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n\ttzinfo = dt.tzinfo\n\tdt = dt.replace(tzinfo=None)\n\tif tzinfo is None:\n\t\treturn dt\n\tif isinstance(tzinfo, dateutil_tz.tzutc):\n\t\traise ValueError(\"tzinfo is already tzutc, unable to replace it with its naive value\")\n\telif isinstance(tzinfo, dateutil_tz.tzlocal):\n\t\treturn dt.astimezone(tzinfo)\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tdelta = dt - datetime.datetime.now()\n\tfor attr in \"tzinfo\", \"tzname\", \"tzoffset\", \"tzname\":\n\t\tif hasattr(delta, attr):\n\t\t\tsetattr(dt, attr, getattr(delta, attr))\n\tdt = dt.replace(tzinfo=tzinfo)\n\treturn datetime.datetime.combine(dt, tzinfo)\n\n\ndef localize(dt, tzinfo=None):\n\t\"\"\"\n\tConvert aware datetimes to strings.\n\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t# This code is based on UTC/TzOffset._set_converter but has no\n\t# circular imports\n\timport datetime\n\tfrom dateutil.tz import tzoffset\n\tfrom._tzinfo import _UTC\n\tfrom. import relativedelta\n\n\t# We need to set the tzinfo from the parser. Because set_tzinfo calls\n\t# PyTime_AsFile().  So if time zone information was already\n\t# present, setting it to None doesn't happen\n\tif tzinfo == _UTC:\n\t\ttry:\n\t\t\ttzinfo = None"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t# This is to handle an unspecified timezone in Python 3.2.\n\tif isinstance(dt, datetime.timezone):\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\n\treturn dt.astimezone(tzinfo or dflt_tz)\n\ndef rrulewrapper(func, *args, **kwargs):\n\t\"\"\"A simple wrapper function for rrule expressions\n\t>>> from datetime import datetime\n\t>>> from dateutil.rrule import rrule, WEEKLY\n\t>>> dt = datetime(2011, 5, 1,"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt.astimezone(tzinfo)\n\telse:\n\t\treturn dt\n\ndef parse_http_date(date):\n\t\"\"\"Parse a variety of HTTP date formats into a more useful representation\n\n\t:param date:\n\t\tSequence or convertible to an HTTP date string, such as RFC1123\n\t\tdate strings, such as Sun, 2006-09-25.  The separator is one of the\n\t\tfollowing: W or | separates the day from the month, an i represents\n\t\tthe microsecond"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n\tself.before = before\n\tif not self.has_after: self.after = bytearray()\n\n\t# set the before and after bytes used by the slice, so the slice has the full bytes used for reading \n\t# of the file.\n\ttry:\n\t\tself.file_before = bytearray(len(before) + 2)\n\t\tself.file_after = bytearray()\n\t\tself.file_before[len(before)] = ord(before[0])\n\t\tfor i in range(len(before) - 1,"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.pre_splits.append(str(before))\n\tself.post_splits.append(str(after))\n\n\n\tfor chunk in before:\n\t\tself.p.write((chunk+\"\\n\").encode())\n\t\t# print(chunk)\n\ttime = time.time()\t\n\twith open('data_split.txt') as source:\n\t\tfor chunk in source:\n\t\t\tchunk = chunk.replace('\\0','')\t\t\n\t\t\tfor index, word in enumerate(reversed(chunk)):\n\t\t\t\tif"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tif before: self.cut_chars = before\n\tif after: self.cut_chars = after\n\tself.update_text_view(self.sender())\n\t\n#=======================================\n# Methods specific to \"Slices\"\n#=======================================\n\nclass ImageSliceView(IPythonWidget):\n\t\"\"\"\n\tWrapper for QSlice View to show images directly in a widget.\n\t\"\"\"\n\tdef __init__(self, id: str = None) -> None:\n\t\t\"\"\"\n\t\tCreate a view widget.\n\t\t\"\"\"\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.last_segment_positions = 0\n\tself.before = before\n\tself.after = after\n\n\tself.segment_lines = list(\n        map (lambda line: self.segment_line_to_bytes(line), self.before))\n\n\tfor segment in SegmentList.from_bytes(self.segment_lines):\n\t\tself.last_segment_positions = segment.pos\n\tself.has_padded_lines = (\n            (SegmentList.MAX_SEPARATE_LINES - self.last_se"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n\tbefore = bytearray(before)\n\tafter = bytearray(after)\n\t\n\tnew_before = bytearray()\n\tnew_after = bytearray()\n\t\n\t# Check that all the cut chars are included.\n\tcut_chars = bytearray.fromhex('0123456789ABCDEF#abcdefXYZ')\n\tfor index, char in enumerate(before):\n\t\tif cut_chars[index] not in char:\n\t\t\tnew_before.append(255)\n\t\t\tcontinue\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.cut_chars = [before, after]\n\t\n\tdef wrap(value: str) -> int:\n\t\t\"\"\"\n\t\tCheck if the string contains a number to have it be in bytes.\n\t\tIf it doesn't, return the original value.\n\t\t\n\t\t\"\"\"\n\t\tnumber_ok = re.match(r'^[0-9][0-9]*$', value)\n\t\tnumber_not_ok = not number_ok\n\t\t\n\t\treturn number_other_ok if number_ok or number_not_ok else value\n\t\n\twith"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.cut_chars = [before] + [before + after for after in \",\" for before in before]\n\n@singledefault\ndef get_changeline_slice(\n\tchangeline: bytes\n) -> List[Bytes]:\n\treturn changeline[changeline.rfind(b\";\")+1 :].strip().split(b\",\")\n\n@singledefault\ndef update_slice(changeline: bytes) -> bytes:\n\t'''\n\tReturn the combined input and output for the modification of the\n\tchangeline as"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    data = after\n\t\n    # First, find all the delimiters.\n    chars = []\n    if before.startswithb(b'\\x1f\\x8c\\x08\\x10\\x0c'):\n        chars = [b'(', b')', b\"[\", b\"]\"]\n    elif before.startswithb(b'\\x1f\\x8c\\x08'):\n        chars = [b'[', b']']\t\n    elif before.startswithb(b"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\ttry:\tcut_chars = self._data.get(b'START_FROM')[1:-1].split(b'\\\\a')\n\texcept:\tcut_chars = self._data.get(b'START_FROM')[1:-1].split(b'\\'')\n\tfor c in cut_chars:\n\t\tif len(c) > b'-1':\n\t\t\tcut_words = c.split(b' ',1)\n\t\t\tcut_words.reverse()\n\t\t\tbreak\n\tif not cut_words: cut_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    chars = after.encode('utf-8')\n    ch_len = len(chars)\n    split_ch = ch_len / (len(before) - 2)\n    \n    for start in range(len(after) - len(before)):\n        end = start + len(before)\n        split_start = start\n        split_end = start + split_ch - 2\n        self.set_slice(split_start, split_end, before[:split_ch - 1], after[split_ch:-1])\n        \n"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tclient_name = request['clientName']\n\tclient_id = request['clientInfo']['id']\n\tpublic_message = request['messageId']\n\tif 'client' in client_name:\n\t\treturn True\n\telif 'discussion' in client_name:\n\t\tmessage_id = int(public_message)\n\t\tif client_id == message_id:\n\t\t\tprint('Message ID matches:',message_id)\n\t\t\treturn False\n\t\telse:\n\t\t\tprint('Mismatched message ID:',client_id,'with public message',public_"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif (message_type := request.message_type) in (MessageRequestType.PublicMessage, MessageRequestType.PublicMessageLegacy):\n\t\tpayload = parse_message(request.message_content)  \n\t\t\n\t\t# verify payload is a valid diaspora message\n\t\tif (\n\t\t\tis_diaspora_message(payload)\n\t\t\tor\n\t\t\tnot _has_valid_diaspora_payload(payload)\n\t\t):\n\t\t\tprint(\"message {} not contain a valid Diaspora message.\".format(\n\t\t\t\trequest.message_type,\n"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\ttry:\n\t\tmessage = Message.query.get(request.args.get('private'))\n\texcept Exception:\n\t\tpass\n\telse:\n\t\tif message.message_type == Message.PRIVATEMESSAGE:\n\t\t\treturn message.message_type\n\t\tif message.message_type == Message.BANREQUESTER:\n\t\t\treturn message.privatype\n\t\tif message.message_type == Message.PUBLICMESSAGE:\n\t\t\ttry:\n\t\t\t\tmessage.message_type = Message.PRIVATEMESSAGE\n\t\t\t\treturn"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request.is_legacy():\n\t\ttry:\n\t\t\treturn identify_legacy_payload(request.get_parameters())\n\t\texcept UnknownTypeException as err:\n\t\t\tlogging.warning(\"Unknown %s\", err)\n\ttry:\n\t\treturn public_message(request.get_parameters())\n\texcept UnknownTypeException:\n\t\t## this needs validation\n\t\tpass\n\t## try a private message\n\ttry:\n\t\treturn private_message(request.get_parameters())\n\texcept UnknownTypeException:\n\t\t# don't trust\n\t\tpass\n"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request.is_legacy:\n\t\trequest.check_content(LegacyMessage, 'legacy-msg' in request.headers)\n\t\treturn request\n\treturn next(iter(filter(lambda x: x['type'] =='send_private_message', request.payload)), None)\n\ndef get_message(request: RequestType,\n\t   sender: str = None,\n\t   key: str = None):\n\t\"\"\"\n\tTry to get the public or private message. The message can be either from public or private message.\t\"\"\"\n\trequest ="}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request.get('public_message'):\n\t\tprint('\\nChecking if message was an evernote payload...\\n')\n\t\tif ('private' not in request['payload']['metadata']):\n\t\t\treturn (\"Error, message not identified: private message without public metadata.\")\n\n\t\tif(('message' not in request['payload']['metadata']['private'] \n\t\t\t\t\t\tor 'noteType' not in request['payload']['metadata']['private']['message'])):\n\t\t\treturn (\"Error, no notes found: private message without"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t# Check if message is public message\n\tif request.is_public_message():\n\t\treturn True\n\n\t# Try to identify public message\n\tpayload = request.payload\n\tif \"payloadFromPublic\" in payload:\n\t\trequest.payload = payload[\"payloadFromPublic\"]\n\t\trequest.protocol = \"HTTP\"\n\t\treturn True\n\n\t# Try to identify private message\n\tpayload = request.payload\n\tif \"payloadFromPrivate\" in payload:\n\t\trequest.payload_protocol = \"HTTP\"\n\t\trequest.protocol ="}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\n\tmessage = Message.objects.filter(**request.query_params).first()\n\tif message is None:\n\t\treturn Response({\n\t\t\t\"success\": False,\n\t\t\t\"message\": 'Invalid request'\n\t\t})\n\telif message.diaspora == 'DIASPORADASSAKASO':\n\t\treturn Response({\n\t\t\t\"success\": True\n\t\t})\n\telif message.diaspora == 'DIASPORADASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASSASS"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request.api == \"public\":\n\t\tif not request.is_legacy:\n\t\t\traise BadRequestException(\"You can only query an API for legacy requests.\")\n\telif request.api == \"private\":\n\t\tif not hasattr(request, \"_message_type\"):\n\t\t\traise BadRequestException(\"This is a private message.\")\n\t\tif not hasattr(request, \"message_type\"):\n\t\t\traise BadRequestException(\"This is not a private message.\")\n\t\tif not hasattr(request.message_type, \"payload\"):\n\t\t\traise BadRequestException"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif is_public(request):\n\t\tif is_deprecated(request):\n\t\t\tlog.debug('Not a public request, skipping as it already identified as a public request')\n\t\t\treturn -1\n\n\tif not is_legacy(request):\n\t\tlog.debug('Not a legacy request, skipping as it is not a public request')\n\t\treturn -1\n\n\t# Try first public message\n\ttry:\n\t\tdata = request().getbody(HTTPConnection(request.gethost()))\n\texcept:\n\t\tlog.debug('Request not a Public Message')\n\t\tif DEBUG"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t# https://cloud.google.com/kms/docs/api/client-managers/request-response/\n\tencryption_algorithm = None\n\n\tif request.crypto_metadata_list:\n\t\tencryption_algorithm = request.crypto_metadata_list\n\n\tif encryption_algorithm.algoid.upper() == HttpResponse.AlgoId.BASIC_AES.upper():\n\t\tencryption_algorithm = request.crypto_metadata_list[0].algoid\n\n\tif not request.headers.get(HttpResponse.Request"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tstatuscode = \"\"\n\tresult = None\n\theaders = None\n\ttry:\t\t\t\t\t\t\t\t# Try to identify request status code\n\t\tstatuscode = request.status_code\n\t\tresult = urlopen(request.url).getcode()\n\texcept urllib2.URLError:\t\t\t# Otherwise use default\n\t\theaders = request.headers\n\treturn (headers, request.url, result)\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tresult = True\n\n\t# Check if we have a message\n\tif not request.getMessage():\n\t\t# Not a matrix request, must be a JSON request\n\t\t# or a standard HTTP response\n\t\tresult = False\n\telse:\n\t\n\t\tjson_dict = request.getMessage()\n\t\t\n\t\tmsgType = JsonMessage.decode(json_dict)\n\t\t# Check valid msgType (matrix, xml, http)\n\t\tif not msgType or msgType['version'] not in ['4', '4.13', '4.14']:\n\t\t\tresult"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tif hasattr(request, \"application\"):\n\t\t# Check the application\n\t\tif request.application!= \"com.qwerty.app\":\n\t\t\tlog.log(\"Matrix.identify_request: The request's application ID doesn't match\", request.logger)\n\t\t\treturn False\n\t\t\n\t\t# Check the request has a content type\n\t\tif not request.content_type in SUPPORTED_CONTENT_TYPES:\n\t\t\tlog.log(\"> Matrix.identify_request: %s: Wrong content_type on request: %s\", request.content_type,"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\ttry:\n\t\tif request is not None:\n\t\t\t_check_requested_features(request[\"request_feature_names_all\"])\n\t\t\treturn True\n\t\treturn False\n\texcept:\n\t\treturn False\n\n\ndef identify_response(response: ResponseType) -> bool:\t\"\"\"\n\tTry to identify whether this is a MatrXResponse\t\"\"\"\n\ttry:\n\t\tif response is not None:\n\t\t\treturn response.is_matrix()\n\t\treturn False\n\texcept:\n\t\treturn False\n\n\ndef get_raw_data(data_id: int,"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tif request.path_components:\n\t\treturn True\n\telse:\n\t\treturn False\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t# By default the request is an identity and it should always return true. \n\t# The request will be identified by always returning true, unless it isn't \n\t# identity but an empty string in case of a non-identity response. \n\treturn Request.parse(request) and (issubclass(type(request), RequestType) or self.valid(request))\n\n\nclass Request(object):\n\t\"\"\" \n\tA generic implementation of the Request specification\n\n\tProvides some utility methods to be passed into the\n\tdeserializer to handle requests. \n\t\"\"\"\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t# TODO: This should be done here: as opposed to on a matrix_handler\n\t# TODO: This is a hack: Need to get a way to identify a Matrix request based on\n\t# this, otherwise just ignore it\t\n\tif request.body == None:\n\t\treturn True\n\n\ttry:\n\t\tevent = json.loads(request.to_dict())\n\t\t# print('Json data:', event)\n\t\tif event.type =='message':\n\t\t\t# print('MESSAGE!')\n\t\t\t# print('EVENT: {0} with body"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tlog.info('Identifying')\n\tif not request.content['request'] is not None:\n\t\tstatus, content = Matrix.identify(content=request.content['request'])\n\t\tif status == 200:\n\t\t\tmatrix = json.loads(content)\n\n\t\t\ttry:\n\t\t\t\tclient = Matrix.Client(\n\t\t\t\t\ttoken=matrix.get('token'))\n\t\t\texcept:\n\t\t\t\tlog.info('Not a valid token')\n\t\t\t\tclient = None\n\n\t\t\tif client is not None:\n\t\t\t\tif client.is_online():\n\t\t\t\t\tlog.info(\n\t\t\t\t\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\ttry:\n\t\t# 1. identify whether the request is an 'X-Matrix-Client-ID' header\n\t\tif request.headers['X-Matrix-Client-ID'][0:2] == 'X-Matrix-':\n\t\t\treturn True\n\t\t# 2. identify whether this is an 'Accept' Request\n\t\tif request.accept.lower().startswith('application/x-matrix-acpatl-prod'):\n\t\t\treturn True\n\t\t# 3. identify whether this request is either a 'Bearer' header or an 'Authorization' header\n\t\tif request"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif dt < CALENDAR_ZERO:\t\t\t\t\t# e.g. 12/02/2021\n\t\treturn f'{dt}'\n\t# Return only the day portion of the number;\n\t# if a day is midnight, return just the number\n\telif dt >= CALENDAR_ZERO and dt < CALENDAR_FULL_HOURS:\n\t\treturn'{}'.format(dt - CALENDAR_FULL_HOURS)\t# e.g. 6/05/2011\n\telse:\t\t\t\t\t\t\t\t\t\t# e.g. 6/"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn \"{}\".format(dt.strftime('%s'))[:13]\n\t\n\ndef read_data_block(fname, format):\n\t\"\"\"\n\tRead one line for a data block of the format fname.\n\t\n\tfname - path to the data block file name\n\tformat - one of 'csv', 'ipynb', 'latexc'\n\t\"\"\"\n\twith open(fname, 'r', errors='replace') as f:\n\t\treturn json.load(f)\n\n# Read the data and get some statistics.\ndef"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif dt.datetime() < dt.astimezone(pytz.utc):\n\t\tdt = dt.astimezone(pytz.utc)\t\n\treturn format(dt.isoformat(' '), '%Y-%m-%dT%H:%M:%SZ', errors='replace')\n\n# \tif dt > dt_str:\t\n#         return '%.0f %d' % (dt.hour, dt.minute)\t\n# \telse:\n#         dt = dt -"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\ttry:\n\t\tt = datetime.fromtimestamp(dt / 1000.0)\n\t\treturn t.strftime('%Y-%m-%d %H:%M:%S')\n\texcept:\n\t\traise Exception('Unsupported or unodated datetime format: %f' % dt)\n\ndef format_date(d):\n\t\"\"\"\n\tTransform the given date d into a datetime.  We don't need to make sure\n\tthat the format is correct, we'll just output the date in UTC.\t\"\"\"\t\n\ttry:\n\t\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tyear, month, day = dt.year, dt.month, dt.day\n\n\t# Get the month and day name. (For this we can use abbreviations for months\n\t# and days, but not for months with only the numbers -- e.g. \"Jan\" is \"January\".\n\t# If \"Jan-1\" exists, convert it to \"Jan\" instead.\n\tmonth_and_day_name = dt.strftime(\"%b\").lower()\n\tmonth = '%s %d' % (month_and_day_name.capitalize"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef get_date(date, time_format='%I:%M %p'):\t\n\t\"\"\"\n\tParse a string in a given time format.\n\t\n\tThe default time format assumes a one time stamp, i.e. the last colon in the \n\tstring that the first underscore or dt.\t\n\t\"\"\"\t\n\tdt = datetime.strptime(date, \"%Y-%m-%d %H:%M"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.strftime('%d/%m/%YT%H:%M %Z')\n\t\n\t\ndef build_from_config(cfg):\n\t\"\"\"\n\tTake a config dict (with all the environment variables) and try to \n\tbuild a graph structure.\n\t\"\"\"\n\t# Build a graph structure from configuration\n\tfrom nx.readwrite.networkx_pydot import read_dot as read_dot_digraph\n\tprint('Building graph structure...', end='')\n\tG = read_dot_digraph(cfg"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.strftime('%s') if dt.tzinfo is None else dt.strftime('%s-%M')\n\n# ---------------\n#\tParse Dict\n# -----------------\n\ndef parse_file_record(text_string):\n\t\"\"\"\n\tParses a Dict in an text file. A File record looks like:\n\n\t\tID: xxyz:xxx\n\t\tStatus: True/False\n\t\tLast seen: 2021-04-29T10:00:00.200Z\n\n\tWhere x is a timestamp (including tz"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.date()\n\ndef create_graph():\n\t\"\"\"\n\tBuild a graph of the following nodes, in ascending order:\t\"P1\" to \"P2\",\n\t\"S1\" to \"Q1\",\t\"Q2\" to \"S<i>\",\n\twhere any 2 nodes must have their own child \n\t[(N/L), X1, X2], where X<i>\n\t(X1,...,X0) is a root of L=2, with a (L-1) leaves\n\t(X+1,...,"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tdt = dt.astimezone(pytz.utc)\n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef get_db_date(db_id,start):\n    \"\"\"\n    Get the corresponding database date or None.\t\"\"\"\n    start = strftime('%Y-%m-%d %H:%M', start)\n    return db.session.query(db.func.date_trunc('day', db.func.localtime(db.func.now()))"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\trtag = set()\n\trgx = r'(?:[^a-zA-Z0-9]*\\w+?)|(?:([^a-zA-Z0-9]|\\w+)|\\s)'  # noqa E501\n\tresult = False\n\tif replacer:\n\t\ttext = replacer(text)\n\tfor m in re.finditer(rgx, text):\n\t\ttagword = m.group()\n\t\trtag.add(tagword)\n\t\ttext = re.sub(rgx, r\"<"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tfound_tags = set()\n\ttext_modified = ''\n\n\tif isinstance(replacer, str) or isinstance(replacer, tuple):\n\t\treplace_func = replacements[replacer]\n\t\treplacer_function = replacer\n\telse:\n\t\treplacer_function = replacer\n\n\tfor char in text:\n\t\tif char in REPLACER_WORDS:\n\t\t\ttagged_text = replace_func(replacer_function(text[:text.index(char)]), char)\n\t\t\ttext = text[text.index(char) + 1:]"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttags = set()\n\ttext = text.expandtabs()\n\ttext = text.lstrip()\t\t\t\t# trim whitespace left\n\tresult = set()\n\tif replacer:\n\t\tresult = replacer(text)\n\tfor i in range(len(re.findall(r'''(<[a-zA-Z]\\w+|=|!=)''', text))):\n\t\tt = re.findall(r'''[a-zA-Z =!=]{1,6}(?=<)''', text"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttags = set() # tags: {tag word, raw source}\n\tfor tag in find_tags_re(text, replacer=replacer):\n\t\ttags.add(tag)\t# tag is a single word and the repl is a compiled regexp\n\treturn tags, text   # return the tags and the text\n\ndef find_tags_re(text: str, replacer: callable = None) -> Tuple[Set, Callable]:\t\"\"\"\n\tFinds tags in the given string, where the tags are compiled regexp.\n\n\tOptionally, the replacement function"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif replacer and hasattr(replacer, '__call__'):\n\t\ttags, tagstext = apply_replacer(text, replacer) \n\telse:\n\t\ttags = frozenset()\n\t\ttagstext = text.split()\n\n\ttag_set = frozenset(tag for tag in tagstext if not tag_exists(tagstext, tag))\n\n\treturn tag_set, tagstext\n\n\ndef tag_exists(text_ls: list[str], tag: str) -> bool:\n\t\"\"\"\n\tReturns a boolean indicating the existence of the"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tignore_tags = []\n\tignore_codes = ''\n\tignore_chars = []\n\toutput_text = ''\n\tignore_codes_re = r'(\\s*)([{})'.format('|'.join((r'[{}]'.format(i) for i in range(2, 27))))\n\tignore_chars_re = r'(\\D)(\\s?!)+'\n\tfor text_tag in ['body','meta','script']:\n\t\tignore_tags.append(re.search(ignore_codes_re, text_"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttext_tokens = re.split(r'[^a-zA-Z0-9]+', text)   # tokens in the code\n\tcounts, tags = [0, ''], set()\n\tcounts_tags = {'(': 1, ',': 1}\n\tfor i, word in enumerate(text_tokens):\n\t\tif word in TAGS:\n\t\t\tcounts_tags['('] += 1\n\t\t\tfor j, t in enumerate(word):\n\t\t\t\t# don't count tags\n\t\t\t\tif t not in counts_tags:"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttagstr = r\"(?:;|\\s+)(?P<tag>[A-Za-z0-9_]+) (?P<soup>(![a-z]{2,3})(?::[a-z_]))?(![a-z]{2,3}))*\"   # (?P<...>) tag\n\ttagregexp = re.compile(tagstr, re.IGNORECASE)\n\n\tdef replacer(m) -> Any:\n\t\tm.group(\"tag\").replace(replacer("}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttag_re = re.compile(r\"\"\"\\[[A-Za-z0-9]\n\t\t\t\t\t\t\t\t[A-Za-z0-9]*\\]\"\"\")\n\treplacer = lambda m: '{}{}'.format(\n\t\tm.group(),\n\t\t_try_replace(m.group())\n\t) if replacer is not None else m.group()\n\treturn tag_re.findall(text), re.sub(tag_re, replacer, text, flags=re.MULTILINE)\n\t\ndef _try_"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tlines: List[str] = []\n\tdef get_indent() -> int: return text.index(\" \"*4)\n\tdef replacer_(replacement_tag: str) -> Callable[[str], str]: return lambda x: replacement_tag if replacement_tag not in x else x\n\tfor (start, end) in find_all_tag_positions(text, replace_replacement=replacerer_(replacer_), use_indent=get_indent()):\n\t\tlines.append(text[get_indent():get_indent()"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tresult = []\n\tfor link in text:\n\t\tif link.find(\"http\") >= 0 and link.find(\"http\") < len(link) and not \"=\" in link and \"https\" in link:\n\t\t\tlink = link.replace(\"<\", \"<\").replace(\">\", \"\").replace(\"\u2018\", \"\").replace(\"\u2019\", \"\")\n\t\tif link.find(\"#\") >= 0 and link.find(\"#\") < len(link):\n\t\t\tlink = '#'\n\t\t\tlink = link.replace(\"&gt;\", \"\").replace(\"&lt;\","}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tclean_link = re.sub(r'<[^>]*>', '', text)\n\tlink = [link for link in re.findall(r'(?:href=\"|\\')(.*?)(?:\\'\\])', clean_link)]\n\tlink.remove(link[0])\n\tlink ='& '.join(link) # Combine list elements into one string, then remove redundant strings.\n\tclean_link = re.sub(' +','', clean_link)\n\tclean_link = re.sub(r' {3,}', '',"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttags = [[\"a\", \"b\"], [\"b\", \"i\"], [\"i\", \"q\"], [\"q\", \"b\"]]\n\ttext = remove_tags(text)\n\ttext = remove_special_chars(text)\n\tnew_text = \"\"\n\tfor word in text.split():\n\t\tnew_text = new_text + \" \".join(\n\t\t\t[f\"{word}{_}\" for _, _ in tags]\n\t\t)\t\n\t\tif new_text!= text:\n\t\t\tif tag in tags:\n\t\t\t\tnew_text = \"<br/>\"."}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext_links = text.split(\"\\n\") # split on newlines and spaces\n\tlinks = []\t\t\t\t\t\t\t# init a list of links, each link will have a new line of text \n\tfor i in range(1, len(text_links)):\t# loop through each found text link, remove newlines and spaces\n\t\ttext_link = text_links[i].rstrip()\n\t\ttext_link = re.sub(\"\\r\\.+\\n\", \"\", text_link) # remove newlines and spaces from each line\n\t\ttext_link = re.sub(\""}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext = re.sub(r\"([0-9])([a-zA-z]+)(\\([a-zA-Z\\s]+\\))]*\", r\"\\1<a href=\\\"\\2\\\">\\3</a>\", text)\t# Remove all quotes or multiple quotes.\n\ttext = re.sub(r\"[\\\"](\\w+)([\\\"']\", r\"<a href=\\\" \\2\\\"> <a href=\\\"\\1\\\" /> </a>\", text) # For non quotes.\n\ttext = re.sub(r\"[\\] ]"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext_link = ''\n\ttext_link_2 = ''\n\ttext_link_3 = ''\n\ttext_link_4 = ''\n\ttext_link_5 = ''\n\tnew_text_link_1 = ''\n\tnew_text_link_2 = ''\n\tnew_text_link_3 = ''\n\n\tif('http' in text):\n\t\ttext_link = text\n\telse:\n\t\ttext_link = 'https://en.wikipedia.org' + text\t\n\n\ttry:\n\t\ttext_link = text_link[text_"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext = linkify(text)\n\ttext = punctify(text)\n\treturn text\t\n\n####################################################################################################\n##\n##\tCreate HTML page for text-to-PDF conversions.\n##\n##\t- text: unicode text to pdf.\n##\n##\t- linkify: text_to_pdf linkify.\n##\n####################################################################################################\n\ndef text_to_pdf_linkify(text):\t\"\"\"\n\tCreate text_to_pdf linkify.\n\t\"\"\"\n\t# Convert \"text\" to unicode\n\ttext = text.replace"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tif 'linkify' in text:\t# linkify\n\t\ttext = text.replace('&lt;', '<').replace('&gt;', '>')\n\t\tlinkify = True\n\tif text!= '-1' and 'http' not in text and '[' not in text and ']' not in text:\t# remove html\n\t\ttext = clean_text(remove_punctuation(replace_html_markup(text, exclude='/')))\n\t\ttext = replace_redundant_words(text)\n\t\ttext = text +''\n\t\tif"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t# Extract link attributes\n\tlinks = extract_links(text)\n\tfound_links = []\n\t# Add link attributes not found in text to find_links\n\tfor link in links:\n\t\tlink, is_present = get_link(link)\n\t\tif is_present:\n\t\t\tfound_links.append(link)\n\t# Add external links to find_links and linkify found_links list\n\tlinks += found_links\n\tif links:\n\t\t# For now add some link tags\n\t\tfind_tags = ['target=\"_blank\"']\n\t\tlinkify"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\n\t# If linkify_links is True, apply linkify on the text to keep it readable.\t\n\tlinkify_links = settings.get(\"linkify_links\",True)\n\t\n\t# Linkify the text for each link.\t\n\tprepared_text = linkify(text,prepared=True,extras=DEFAULT_EXTRAS_LINK,sanitize=False,sanitize_tags=False,strip_html=True,safe_chars=r'(_|.)',user_friendly=True)\n\t\n\t# Remove"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tresp = requests.head(url, allow_redirects=True)\n\tcontent_type = resp.headers.get('Content-Type')\n\tif content_type is None:\n\t\tcontent_type = ''\t\n\telse:\t\n\t\tcontent_type = content_type.split(';')[0]\n\treturn content_type\ndef convert_data_json(value_to_convert: str)->Json_Object:\n\t\"\"\"\n\tIf value_to_convert is a valid json string, try and return the value.\t\n\t\"\"\""}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tpage_text: str = \"\"\n\tif url.endswith(\"html\"):\n\t\tpage_text = url[:-4]\n\telif url.endswith(\"/static\"):\n\t\tpaths = url[:-11].rstrip(\"/\").split(os.path.sep)\n\t\tif len(paths) == 1:\n\t\t\tpaths.append(\"\")\n\t\tfile_path = os.path.join(os.path.dirname(__file__), \"static\", *paths)\n\t\twith open(file_path, \"r"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\ttry:\n\t\tresponse = urllib.request.urlopen(url)\n\n\t\tcontent_type = response.info().get('Content-Type')\n\n\texcept urllib.request.HTTPError:\n\t\tcontent_type = None\n\t\n\treturn content_type\n\n\ndef extract_info_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tExtract the content type of the HEAD of the remote url to determine the content type.\t\"\"\"\n\tcontent_type = fetch_content_type(url)\n\n\tif content_type and content_type"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\theaders = {'User-agent': USERAGENT}\n\tresponse = requests.head(url, headers=headers).content\n\treturn content_type_from_headers(response)\t\n\n\ndef content_type_from_headers(headers: dict):\n\t\"\"\"\n\tProvide the content type for a given header from the content type header and\n\tto the content header.\t\n\n\tFor example:\n\t- 'content-type': 'text/html; charset=utf-8'\t\n\t- 'content-type: text/html; '\t\n\t-"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tres = urlopen(url)\n\theaders = res.info()\n\tcontent_type = headers.get('Content-Type', '')\n\treturn content_type\n\n\t\ndef is_file(file_name: str) -> bool:\t\"\"\"\n\tDetermine if a file exists.\t\n\t\"\"\"\n\t# Returns `True` if the extension exists.\n\t_ext = \".\"\n\n\tif file_name.endswith((\"png\", \"jpg\", \"jpeg\",\"ico\", \"svg\")):\n\t\t_ext += \"png\"\n\n\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tremote = urlparse(url)\t\t# get the origin URL to create the HTTP header\n\thost = remote.netloc.split(':')[0]\n\tif host.strip('.') == 'www':\n\t\turl = 'http://www' + url.replace('www.', host.strip('.'))\t\t# transform www.to.in into wwwin.\n\t\t# now get: 'www.in.com'\n\t\treturn 'text/url-list'\n\telse:\n\t\tcontent_type = remote.scheme + '://'\t+ host + '."}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tresponse = requests.get(url)\n\tcontent_type = response.headers.get('content-type')\n\tif len(content_type) > 0 :\n\t\tcontenttype, parameters = extract_values(content_type, ';')\n\t\treturn contenttype + \" \" + parameters + \"\\n\\n\\n\"\n\telse:\n\t\treturn None\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tpage = requests.get(url, headers=HEADERS)\n\tif page.status_code == 200:\n\t\treturn page.headers['Content-Type']\n\telif page.status_code == 404:\n\t\treturn None\n\telse:\n\t\traise Exception(f\"Content-Type can not be determined due to following error: {page.status_code}\")\n\n\ndef parse_response(url: str, content_type: Optional[bytes]):\n\t\"\"\"\n\tParse the response for the url.\n\t:param url: url to retrieve, it will be"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\theaders = RequestHeaders()\n\theaders.setValue('Referer', url)\n\tr = session.get(url = url, headers = headers, timeout = 120)\n\tif r.status_code == 200:\n\t\treturn r.headers['Content-Type'].split(',')[0]\n\treturn None\t\n\t\n\t\ndef parse_query(url: str) -> tuple:\n\t\"\"\"Parse the query string on the given url.\t\"\"\"\n\treturn url.split('?')\t\n\t\n\t\ndef validate_url(url: str"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tresult = requests.get(url)\n\thead = result.headers\t\t\t\t\n\tcontent_type = head.get('Content-Type')\t\t\t\t\n\treturn content_type\t\t\t\t\t\n\n\t\ndef build_api_url(url: str, parameters: Union[List, Dict]) -> str:\n\t\"\"\"\n\tBuild a url with the parameters.\t\"\"\"\n\targs = f'&'.join('{k}={v}' if type(v) == type('') else v + '=' + k for k, v in parameters)\n\treturn f'{url}&"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif regex.match(tag, text):\n\t\ttext = text[match.end():]\n\t\t# Check for an empty tag.\n\t\tif not text:\n\t\t\tprint('no tag')\n\t\t\tprint('test skipped')\n\t\t\treturn False\n\t\t# Check for an empty label.\n\t\tif not regex.match(text[:match.end()], '[^ ]*:*'):\n\t\t\tprint('something wrong')\n\t\t\tprint('test skipped')\n\t\t\treturn False\n\telse:\n\t\tprint('something wrong')\n\t\tprint('test skipped')\n\t\treturn False\n\n\tprint"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif not tag or tag.isdigit() is False:\n\t\treturn False\n\treturn tag in map(_tag_validation_function, tag_value.split(\" \"))\n\n# test __init_\ndef __test_init() -> None:\n\t# This is to test that tag has a well-defined initial value.\n\tassert( \"A\" == _tag_initial_value), \"Initial value must be A\"\n\t# It should be a string\n\tassert(type(tag_value) == str  ), \"Unexpected type of value\"\n\t# This"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif word == tag and is_tag:\t# means if word and tag are both present\n\t\treturn True\n\telse:\t\n\t\treturn False\n\ndef test_sentence(s: str) -> tuple:\n\t\"\"\"\n\tTest a sentence with all the tests as they should.\t\n\tTest the first two words from the sentence (first two words are the same as the given word).\t\n\t\"\"\"\n\tsentence = s.split()\t# splits each word into a list of words based on whitespaces.\n\tword = sentence[0]\t# get the first word"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif tag.split(\" \") == [\"h\", \"k\", \"t\", \"r\", \"i\", \"e\", \"d\", \"a\", \"k\", \"n\", \"i\", \"s\", \"e\", \"g\", \"r\", \"v\", \"n\", \"e\", \"l\", \"s\", \"l\", \"b\", \"q\", \"n\", \"u\", \"n\", \"d\", \"r\", \"i\", \"o\", \"g\", \"l\", \"m\", \"c\", \"e\", \"t\", \"a\", \"s\", \"e\", \"d\","}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t# Start by checking one the first word of the string.\n\tif tag[:2] == 'TAG':\n\t\t# Tag word.\n\t\tif len(tag)!= 2 or len(tag.strip())!= 2:\n\t\t\t# The entire tag is invalid.\n\t\t\tassert(False)\n\t\telse:\n\t\t\t# This tag words.\n\t\t\treturn True\n\treturn False\n\n\ndef main() -> None:\n\t\"\"\"\n\tA test function.\t\"\"\"\n\t# Set up some useful default tags.\n\tTAG_A  = 'A'\n\tTAG_B  "}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif tag['n'].casefold() not in ['w', 'w?'] and tag!= \"\":    # tag should only have three words\n\t\treturn False\n\treturn True\n\ndef word_pos(word: str) -> str:\n\t\"\"\"\n\tReturn a string describing the position in the word.\t\n\t\"\"\"\n\tif word == \"\" or word.casefold() not in ['w', 'w?']:\n\t\treturn \"nooppos\"\n\tif word_is_tag(word):\n\t\treturn \"tttag\"\n\telse:\n\t\treturn \""}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn word.tag in tag\n\n\ndef test_sentence(sentence: str) -> bool:\t\"\"\"\n\tTest if the given sentence is a tag of a given tag.\t\"\"\"\n\treturn tag(sentence)\n\n\ndef test_index(index: int) -> None:\t\"\"\"\n\tTest if a given index is a word in a given index-vocabulary word list \n\t\"\"\"\n\tassert index in word2index\n\ndef test_index_index(index: int, index_vocabulary: set) -> None:\t\"\"\"\n\tTest that index-voc"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\ttag = TAGS.get(tag)\n\treturn [\n\t\tre.compile(fr'\\w*{tag.split(\".\")[1]}:\\s*([\\w\\W/]+)\\s*\\w*', re.IGNORECASE), \n\t\tre.compile(fr'\\w*{tag.split(\".\")[1]}:\\s*({tag.split(\":\")})[\\w\\W/]+\\s*\\w*', re.IGNORECASE)\t\n\t]\n\n\ndef test(data_set: set"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t_, word = _get_word_from_tag(tag)\n\treturn bool(_check_for_tag(word))\n\n\n\ndef _check_for_tag(word: str) -> bool:\n\n\n\t\"\"\"\n\tThe logic of this function checks whether its a word that could have a TAG-style tag,\n\tor something that can be used for such a word.\t\n\n\tThis function makes a check in a context where they have a TAG-style syntax that can be parsed.\t\n\n\tIn other words, \"this word is a word with the TAG-style syntax\"."}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn textTests[\"checkTag\"][tag](text)\n\n\n#####################################################################\n### Check each of the substrings of any given phrase.\t\n\ndef test_all(tagTests: dict, phrase: str) -> bool:\t\"\"\"\n\tCheck that each word of the given phrase whether it could be\n\tthe tag of the phrase, according to the tags, and in which line\n\tits word can occur.\n\t\"\"\"\n\twordsTested = 0\n\tfor word in phrase.lower().split(\" \"):\n\t\tif wordTests[\"checkPos\"][\""}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n\treturn { child:xml_children_as_dict(child) for child in node }\n\ndef xml_children_as_dict_as_xml(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, and xml children as their xml representation..\n\t\"\"\"\n\t\n\txml_tree = node.tag.replace('/', '.')[1:] # '.' is the root tag, do not remove \n\t\n\treturn { child:xml_children_as_dict_as_xml(child) for child in node }\n\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tresult = {}\n\tif node.childNodes:\n\t\tfor child in node.childNodes:\n\t\t\tresult[child.nodeName] = xml_children_as_dict(child)\n\treturn result\n\t\n# \u00a0Create xml tree node class for the root of the document.\n# XML nodes have a node class that identifies xml root,\n# child instances of each node have a tag name, and\n# a data value to store for the current instance.\nclass xml_roots(tldextr.Document): \n\t\"\"\"\n\tA tree node is instantiated"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\treturn dict([(tag, xml_children_as_dict(child)) for tag, child in node])\n\n\n##\n# Map a namespace prefix onto the tag name.\ndef _ns_prefix(ns_prefix, node):\n\t## node.prefix -> tag_name\n\tif node.prefix:\n\t\ttag_name = nsprefix(ns_prefix)\n\n\t\t# tag name already exists.\n\t\tif tag_name in node.attrdict:\n\t\t\treturn  tag_name\n\n\t\t# return first one unqualified: e.g. xmlns:ns/"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\treturn {child.tag: {\"tag\": child.tag, \"value\": child.text} for child in node}\ndef parse_list(node, parent, depth=0):\n\t\"\"\"\n\tRecursively parse the children of <list> nodes. \n\t\"\"\"\n\tif not node.get(\"title\"):\n\t\treturn\n\t# First step. extract only a set of children, which is a list\n\telements = [parse_list(child, node, depth=depth+1) for child in node.getchildren()]\n\telements.append(element"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tret = {}\n\tfor key, value in node.items():\n\t\tif type(value) is tuple and all(type(elt) is int for elt in value): # type == (int, int) -> {int, string}\n\t\t\tfor i in range(value[0], value[1] + 1):\n\t\t\t\tret[key].append(('{}:{}'.format(i, i), value[i]))\n\t\telse:\n\t\t\tret[key] = self.xml_children_as_dict(value)\n\treturn ret\n\n\nclass MuxConfig"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n\tchildren = {}\n\tfor child in node.children:\n\t\tchildren[child.tag] = xml_children_as_dict(child)\n\n\treturn children\n\n\ndef check_attribute(tag, attr):\n\tif attr == 'children':\n\t\treturn all(check_attribute(attr_tag, v) for (v,attr_tag) in TAGS.items())\n\telse:\n\t\tif isinstance(attr, (list,tuple)):\n\t\t\tif len(attr) == 1:\n\t\t\t\treturn all(check_attribute(attr_tag"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tchildren = []\n\tcurrent_node = node\n\twhile len(current_node) > 0:\n\t\tchildren.append((current_node, ))\n\t\tcurrent_node = current_node[0]\n\tassert len(children) > 0\n\tparent_node = children[-2][0]\n\n\txml_children = None\n\tfor child in children:\n\t\tif len(child) > 0:\n\t\t\txml_dict = xml_children_as_dict(child[0]) or {}\n\t\t\txml_dict['parent_tag'] = XML_TAG_MAP."}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tret = {}\t# {tag name: dict of values for this child tag}\n\tfor child in node.children:\t# iterate over elements in xml\n\t\tret[child.tag] = child.attrib.values()\t# return all values as a dict\n\t\tret[child.tag] = xml_tree_to_dict(child)\t# iteratively convert element to a dict\n\t\tif not node.children:  # skip all \"children\" node children\n\t\t\tbreak\n\treturn ret\n\ndef xml_tree_to_dict(node):\t\"\"\"\n\tCon"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif '_xml_children' in node.tag:\n\t\treturn {child.tag: child.attrib for child in node}\n\telse:\n\t\treturn {}\n\ndef xml_data(node):\t\"\"\"\n\tHelper for converting an XML element into a dictionary.\n\t\"\"\"\n\t\treturn xml_text(node).data\n\ndef dom_to_dict(node, xml_data=None):\n\t\"\"\"\n\tUsed for converting a DOM to a {tag:value} dict. Assumes that node has 'xml_data'. Optional.\t\"\"\"\n\tif xml_"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\timport xmltodict\n\tdata = {}\n\tfor child in node.childNodes:\n\t\tx = xml_children_as_dict(child)\n\t\tdata[x['name']] = x\n\treturn data\n\n\n################################################################################\n#                                                                          #\n#                                                                          #\n#                                                            xml.dom.parser.parse  #\n#                                                                          #\n#                                                                          #\n################################################################################\n\n\nclass LXMLParser:\n\n\t\"\"\"\n\tProvide"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif type(sender_handle) == str:\n\t\treturn sender_handle in [receiving_data['subject_handle'], receiving_data['author_handle']]\n\telse:\n\t\treturn sender_handle == receiving_data['subject_handle']\n\n\ndef check_entity_and_header_match(receiving_data):\n\t\"\"\"\n\tEnsure that all entities in the receive message are the same.\n\tThe first entity will be set.\n\t\"\"\"\n\tfor entity in receiving_data['entities']:\n\t\tif entity[0] not"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t# if a sender and the entity handle match, return\n\tif sender_handle == entity_handle:\n\t\treturn sender_handle\n\t# if a sender does not match, we have some exceptions so raise an exception\n\telif sender_handle!= None and entity_handle!= None:\n\t\traise Exception(\"Unhandled exception: sender %s does not match sender %s\" % (sender_handle, entity_handle))\n\t# otherwise raise an exception\n\telse:\n\t\traise Exception(\"Unhandled exception: sender %s does not match sender %s\" % (sender_handle"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif type(sender_handle) == str:\n\t\tif sender_handle!= '<unknown sender>':\n\t\t\traise ValueError(\"Invalid sender_handle. Received: {}\".format(sender_handle))\n\n\telse:\n\t\tif not type(sender_handle) == int:\n\t\t\traise ValueError(\"Invalid sender_handle type. Received: {}\".format(type(sender_handle)))\n\n\t# Find sender_handle of the object\n\tif type(entity_handle) == str:\n\t\tif entity_handle!= '<unknown entity>':\n\t\t\traise Value"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tfor sender_header in request.headers.getlist('From'):\n\t\tif sender_header == sender_handle:\n\t\t\tsender_handle = value\n\t\t# SEND RECIPS HERE\n\t\t# SEND RECIPT HERE\n\t\t# SEND RECIPT HERE\n\n\tfor entity_header in request.headers.getlist('To'):\n\t\tif entity_header == entity_handle:\n\t\t\tentity_handle = value\n\t\t# SEND RECIPS HERE\n\t\t# SEND RECIPT HERE\n\t\t# SEND RECIPT HERE\n\t# return the tuple\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\treturn (sender_handle == entity_handle and sender_handle not in self._handles_who_it_is_sender_and_they_are_not_entitysenders)\n\ndef build_sender_and_entity_handle_match_mapping(target_objects, payload):\n\t'''\n\t\tBuilds a mapping from sender_handle to entity_handle where the\n\t\tmapping is going to be created. \n\t'''\n\thandle_key = f'{target_objects}_{Payload.OBJECT}'\n\tmapping ="}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif not (sender_handle.lower() == entity_handle.lower() and sender_handle.find('@') == 0 and entity_handle.find('@') == 0):\n\t\traise NotSupportedError(\"You have to have the sender and entity handles be the same. The sender will not receive the payload.\")\n\n\thandle_object_type = entity_handle.split(':')[0]\n\thandle_payload_object_type = sender_handle.split(':')[0]\n\n\t# We don't want \"entity\" or \"payload\" being the same as"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t# Check for sender. Handle was already handled in the above method\n\tif sender_handle == handler_entities:\n\t\tif entity_handle == handler_entities:\n\t\t\t# Both have the same sender or the two have different set as entities. We have to verify both have sender and entity.\n\t\t\t# If this is true, we raise an exception.\n\t\t\traise Exception(f'Cannot set {handler_entities} as {sender_handle} as it had another author handle on the payload')\n\t\t# If one doesn't have sender or entity, we need to change it to"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t# The only verification that's necessary if we verified the sender matches the entity\n\tif sender_handle!= entity_handle: \n\t\tprint 'Error in check_sender_and_entity_handle_match: sender_handle!= entity_handle'\n\t\treturn\n\t# Again, we've verified the sender and entity in both, so we can compare those.\n\tif sender_handle == 1 or sender_handle == 2 or sender_handle == 3:\n\t\tprint 'Error in check_sender_and_entity_handle_match: sender!= 1'\n\t\treturn\n\t#"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tsame_sender = sender_handle=='Author: {}'.format(entity_handle.title)\n\tsame_entity_handle = entity_handle.handle==\"Author: {}\".format(sender_handle)\n\tif same_sender and same_entity_handle:\n\t\tif same_entity_handle not in obj.pub_list[0]:\n\t\t\tself.error_log.log_message(\"SEND {} {}\".format(obj.identifier, str(obj.pub_list)))\n\t\t\treturn False\n\t\tself.log('Sender {} is the author"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t# Verify sender.\n\tif not sender_handle:\n\t\treturn (False, Diaspora.error('sender_handle_error'))\n\n\t# Verify entity.\n\tif not entity_handle:\n\t\treturn (False, Diaspora.error('entity_handle_error'))\n\n\t# Otherwise verify that match.\n\tif not has_match(sender_handle, entity_handle):\t\n\t\t# Try to create a copy.\n\t\tsender_handle = copy.deepcopy(sender_handle)\n\n\t\t# Verify that the sender is the same."}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn get_nodeinfo_document_nodeinfo(url, document_path)\n\ndef get_nodeinfo_document_nodeinfo(url, document_path=None):\n\tdocument = get_nodeinfo_document(url, document_path)\n\treturn xml_to_nodeinfo(document.tostring())\n\ndef xml_to_nodeinfo(xml):\n\t\"\"\"\n\tConvert a XML document into a NodeInfo.\n\n\t:arg xml: XML document, as returned by a GET /api/#status\n\t:returns: NodeInfo instance\n\t:"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tfrom urllib2 import _urlopen, _urlsplit, unquote_plus\n\tfrom urlparse import urlparse\n\n\turl = url.strip()\n\tif hasattr(urlsplit, 'allow_fragment') and urlparse(url).fragment:\n\t\treturn '{}#{}'.format(_urlsplit(url).geturl(), url[:url.rfind('#')+1])\n\telse:\n\t\tbase = urlparse(url).scheme\n\t\tif base.lower() == 'http':\n\t\t\traise ValueError('invalid host"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t# NodeInfoDocument\n\td = {\n\t\t\"label\" : \"nodeinfo document\",\n\t\t\"uri\" : url,\n\t\t\"version\" : \"1.0\",\n\t\t# \"subtype\" : \"NodeInfoDocument\"\n\t\t\"subtype\" : \"NodeInfoDocumentEntry\",\n\t\t\"dateCreatedTimestampMicros\": \"\",\n\t\t\"lastModifiedTimestampMicros\": (\"\", \"\"),\n\t\t\"timestampMonired\" : \"0\",\n\t\t\"lastModifiedUserName\"  : \"\",\n\t\t\"lastModifiedUserOrganizationName\""}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\turlpath = urlparse.urljoin(document_path, \"/nodeinfo-server\")\n\tresult = requests.get(urlpath, headers={'Content-type': 'application/json'})\n\tif result.status_code in [200, 404]:\n\t\ttry:\n\t\t\tjsonobj = result.json()\n\t\t\tif 'error' in jsonobj and jsonobj['error'] is not None:\n\t\t\t\traise Exception(jsonobj['error'])\n\t\t\tif'status' in jsonobj and jsonobj"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn (\n\t\t'https://example.com/nodeinfo.json',\n\t\tnodeinfo.generate_nodeinfo_file(\n\t\t\turl, document_path\n\t\t),\n\t)\nfrom nodeinfo import NodeInfo\n\n\ndef is_valid_url(url):\n\tregex = re.compile(\n\t\tr'^((http(s)?)://)?%s' % url,\n\t\tre.IGNORECASE | re.MULTILINE\n\t)\n\treturn True if re.match(regex, url) else False\n\n\ndef normalize"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn NodeInfo.get_nodeinfo_well_known_document(url, document_path=document_path)\n\n\ndef list_nodes(url, **kwargs):\n\t\"\"\"\n\tList nodes in a specific zone.\n\t\n\t:arg str url: Base url with protocol, ie https://example.com\n\t:arg dict  kwargs: any query params\n\t:returns: list of node dicts (see spec : http://nodeinfo.diaspora.software/nodeinfo/latest/index.html#NODE-GET-LIST\n\t\"\"\"\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tdoc = get_document(url, document_path)\n\tif doc:\n\t\tnode_info = doc['info']\n\t\tif node_info:\n\t\t\treturn node_info\n\telse:\n\t\tif not document_path:\n\t\t\tdocument_path = os.path.join(BASE_URL, \"nodeinfo\", \"nodeinfo.json\")\n\t\t\tdocument_path = os.path.normpath(document_path)\n\t\tdoc = get_document(url, document_path)\n\t\tif doc:\n\t\t\tnode_info = doc['info']\n\t\t\tif"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tcontentType = 'application/json'\n\tif document_path is not None:\n\t\tcontentType += (';' if document_path.endswith('/') else ';') + \\\n\t\t\t\t\t  contentType\n\tschema = 'application/json'\n\tnodeInfo = {\n\t\t\"dns\": [{\n\t\t\t\"scheme\":'scheme',\n\t\t\t\"nameservers\": {\n\t\t\t\t\"hosts\": [{\n\t\t\t\t\t\"ip_address\": url.split(':')[0],\n\t\t\t\t\t\"subnet_mask\": url.split(':')"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\timport urllib\n\tfrom urllib2 import build_opener\n\tfrom urllib2 import urlopen\n\n\tif document_path:\n\t\tfullpath = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'nodes', document_path)\n\t\tif os.path.exists(fullpath):\n\t\t\tdoc = open(fullpath, 'r').readlines()\n\t\t\tnodes = {}\n\t\t\tfor l in doc:\n\t\t\t\tinfo_match = re.search('info: \"node(.*"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\turl = urljoin(protocol, url) # join paths\n\t\n\tnodeinfo_url = urljoin(protocol, 'NODEINFO_ENDPOINT?')\n\tif document_path:\n\t\tnodeinfo_url += \"&document_path='%s'\" % document_path\n\tnodeinfo_url += \"&type=%s\" % 'jsonp'\n    \n\tr = requests.get(url=nodeinfo_url, headers={'Content-type': 'application/json'}, stream=True)\n\tif r.status_code =="}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif signature is None:\n\t\t# No Signature, verify no relay-able\n\t\treturn False\n\tif 'AuthorizationHeader' not in signature:\n\t\t# No Authorization Header, verify this signature is only for Relayable\n\t\treturn False\n\tAuthorizationHeader = signature['AuthorizationHeader']\n\tif not AuthorizationHeader.author_accepts_http:\n\t\t# Not HttpAccepts, verify this signature is only for Relayable\n\t\t# for non-HTTP requests\n\t\treturn False\n\tAuthorizationHeaderStr = AuthorizationHeader.author_accepts_http\n\tAuthorization"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif signature[:7]!= document_public_key:\t\n\t\traise Exception(\"Wrong signature in the document\")\n\t\n\tvalid, error = check_signed(doc)\n\tif not valid: return valid, error\n\tverified, error = verify_signed(doc)\t\n\tif not verified: return verified, error\n\ndef validate_signed_doc(doc, signed_msg):\t\"\"\"\n\tReturn a tuple of (verified, error)\n\t\"\"\"\n\tsignature = signed_msg.childNodes[1].firstChild.nodeValue\n\ttry:"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\ttry:\t\t\t\n\t\tfor signature_index, signature in enumerate(signature.sig):\n\t\t\tpub, pub_key = parse_pub_key(public_key)\n\t\t\tmatching_entries = get_public_key_of_signature_entry(signature_index, signature, doc)\n\t\t\tassert len(matching_entries) >= 1\n\n\t\t\tif len(matching_entries) >= 2:\n\t\t\t\treturn False            \n\t\treturn True\n\n\texcept XMLSchemaValidationError:\n\t\tpass\n   \n\treturn False\n\ndef try"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t#verify if signature is a valid signature by trying to verify the signature with an XML document\n\t#using a base64.b64decode (because XML's signature may have a different character encoding than the XML document)\n\ttry:\n\t\txmlDoc = etree.xml2dict(doc)\n\t\tfor element in xmlDoc:\n\t\t\tattributes = {}\n\t\t\tif type(element.attrib) == dict:\n\t\t\t\tif'signature' in element.attrib:\n\t\t\t\t\tattributes[\"text\"] = element.attrib[\"signature\"]\n\t\t\t\t\tattributes[\"signature"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\ttry:\n\t\t# Load the XML file, in order to find an element called'signature'.\n\t\txml_signature = pyxmltools.parse(signature)\n\texcept:\n\t\tprint('Error in parse signature file -'+ signature)\n\t\treturn False\n\n\tif len(xml_signature.get('signature')) < 1:\n\t\tprint('Invalid signature file. Element\\'s\\' sign must be \\'signature\\'.')\n\t\treturn False\n\n\ttry:\n\t\t# Now, the XML element must have a'signType' attribute with\n\t\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t# TODO: make the public_key a public key object\n\t# TODO: compute the signature into bytes\n\treturn\n\trv = r.verify(bytes(public_key.decode()))\n\tif rv.has_validator():\n\t\treturn True\n\treturn False\n\n\nclass Author(ObjectIdentifier):\n\t\"\"\"\n\tRepresents an oid with a DER-encoded DER signature.\n\t\"\"\"\n\tdef __init__(self, value, **kwargs):\n\t\t\"\"\"\n\t\t:param value: the DER-enc"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tpub_key, pub_key_len, pk_version, key_seed, key_seed_len, key_len, msg_len = public_key\n\tverify_key_seed(key_seed, key_seed_len, key_size)\n\treturn verify_doc(doc, doc_string, msg_id, msg_id_len, msg_dig_len, msg_id_hash, pk_version, pub_key, msg_type_id, signature)\n\t\n\n#\tdef verify_signature(key, signature, public"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\ttry:\n\t\td = _load_signed_xml(doc)[0]\n\t\tif d['signature']['format'] == 'PEM':\n\t\t\tsig_from_string(d['signature']['data'])\n\t\telse:\n\t\t\t# Use the public key (signed by our root node)\n\t\t\tsk = crypto.SigningKey.generate(public_key)\n\t\t\tverify(d, sk, signature,\n\t\t\t\treason='The user did not provide a signature on a message')\n\texcept:\n\t\td = _load_signed_xml"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif not doc:\n\t\treturn True\n\tif not signature:\n\t\treturn False\n\t# This check is for a valid signature, although it could be false for\n\t# signatures which aren't relaaable.\n\tif not is_validature(signature, doc):\n\t\treturn False\n\tpubkey_node = doc.find(\"pubkey\")\n\tif not pubkey_node:\n\t\treturn False\n\tcert = pubkey_node.find(\"cert\")\n\tif not cert:\n\t\treturn False\n\t# Check for the validity of the signature and ensure it hasn"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\timport base64\t\t\t\n\timport six\t\t\t\n\tfrom six.moves.urllib.parse import parse_qsl\n\tfrom xml.dom import minidom\t\n\tfrom xml.etree.ElementTree import XML\t\n\n\t# Pull out the public key and check against the signature  \n\tdec1 = base64.decodestring(public_key.encode('utf-8'))\n\thash1 = uhash(dec1)\t\t\t\n\t\n\tif hash1!= signature:\n\t\tprint(\"Invalid signature. Not a valid XML document. XML not"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif 'file' not in document:\n\t\treturn None\n\telif document['file']['source'] not in DIASSONA['files']:\n\t\treturn []\n\tdiaspora_finger_url = DIASSONA['files'][document['file']['source']]['file']['url']\n\n\t_, xmlparsed_xml = url_parsing(diaspora_finger_url, as_xrd=True)\n\n\txmlparsed = xmlparsed_xml.decode('utf-8')\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tresponse: Union[Dict, List] = {\"code\": 0, \"description\": \"No results\"}\n\ttry:\n\t\t# create an empty response if the response is not a list\n\t\tresponse = []\n\t\tif type(document) == List:\n\t\t\t# iterate through each entry in the list\n\t\t\tfor entry in document:\n\t\t\t\t# if the entry has the required format\n\t\t\t\tif entry[\"format\"] == \"json\":\n\t\t\t\t\t# extract the object that's JSON\n\t\t\t\t\tentry: Dict = json.loads(entry[\"value\"])\n\t\t\t\t\t# add each object extracted to the"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdios = diospora.Diospora(uri=doc, raw=False)\n\t\n\td = dios.discover()\n\treturn d\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tresponse = requests.get(\n\t\tf\"https://diaspora.github.io/diaspora_federation/discovery/\"\n\t\t+ document,\n\t\ttimeout=30.\n\t)\n\tlines = response.text.split(\"\\n\")\n\tlines = [line for line in lines if line and not line.startswith(\"<\")]\n\n\t# parse the data\n\tdiaspora_webfinger = {}\n\tdiaspora_webfinger[\"date\"] = datetime.fromisoformat(lines[0].split(sem=\"e\")[0])"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif not document:\n\t\treturn {} # TODO: raise exception\n\t\traise ValueError(\"document not specified\")\n\twith urllib.request.urlopen(url + document, timeout=30.0) as f:\n\t\twebsocket_response = None\n\t\tif f.getcode() in [200, 3]:\n\t\t\treturn json.loads(uht.decode(f.read(), strict=False))\n\t\twhile websocket_response is None or websocket_response.getcode()!= 200:\n\t\t\ttry:\n\t\t\t\twebsocket_response = (u"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\td_wef = Document(\n\t\tname=f'Document {document}', \n\t\tfiles={\n\t\t\tDOCUMENT: open(f'Documents/{document}.xrd', 'r', encoding='utf-8')\n\t\t}, \n\t\tattributes={\n\t\t\t'xml:lang': \"en-US\",\n\t\t\t'urn:wf:spec:qmi:webfinger_service': \"http://127.0.0.1:7890/webfinger/qmi\"\n\t\t})\n\treturn d_wef.xml.parse()\n\ndef"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\timport re\n\tfrom urllib.request import Request, urlopen\n\tfrom pathlib import Path\n\n\tdiaspora_federation_page: str = f\"http://diaspora-federation.in.mat.br/diaspora/diaspora/{document}\"\n\tdiaspora_federation_response = urlopen(diaspora_federation_page).read().decode('utf-8')\n\n\treturn json.loads(diaspora_federation_response)\t\n\n########################################################################\n\ndef get_document_name(document:"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\ttry:\n\t\tdocument = json.loads(document.decode('UTF-8'))\n\texcept:\n\t\t# Invalid format\n\t\tLOGGER.warning(f\"Diaspora webfinger format can't parse - not valid JSON document!\")\n\t\tpass\n\n\tf_doc = document.get('dia_signature', None)\n\tif f_doc is None:\n\t\tLOGGER.warning(f\"Diaspora webfinger signature must have a 'dia_signature' property!\")\n\t\treturn None\n\n\tf_type = document.get('dia"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tresult: Dict = {\n\t\t\"diaspora_federation\": {\n\t\t\t\"tags\": {},\n\t\t\t\"webfors_tags\": {}\n\t\t},\n\t\t\"extact\" : None,\n\t\t\"name\" : document\n\t}\n\turl: str = re.search('<a href=\"([^\"]+)\"', document).group(1)\n\ttagname = webfinger_tag_regex = re.sub('/api/v0/(.*?)/', '', '/'.join(['diaspora_federation']))\n\tassert tag"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdoc = document.lower()\n\n\t# parse diaspora structure\n\ttry: \n\t\twebf = ast.literal_eval(doc)\n\texcept:\n\t\twebf = ast.literal_eval(textwrap.dedent(doc))\n\n\t# if we have webf structure, then format it so that it's readable as a JSON structure\n\tfor key in webf:\n\t\tif isinstance(webf[key], dict):\n\t\t\twebf[key] = parse_diaspora_webfinger(webf[key])\n\n\t# parse out additional"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\trfc7033_path = f\"/api/rfc7033/{handle}.html\"\n\t\treturn self.client.get(rfc7033_path).text\n\texcept:\n\t\tif DEBUG or WARNING:\n\t\t\ttraceback.print_exc()\n\t\texception = sys.exc_info()[1]\n\t\tself.logger.info(\n                    \"Failed to retrieve webfinger document for \"\n                    f\"'{handle}': {exception}\",\n                    level=ERROR\n                )\n\t\tif DEBUG or WARNING:\n\t\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\n\t# First, test whether the document is a redirect, or the document URL is not found.\n\tis_redirect = '<url type=\"redirect\">'.join([handle, '/', '1']) in Webfinger.cache\n\tis_not_found = '<url type=\"error\">'.join([handle, '/']) not in Webfinger.cache\n\t\n\tif not is_redirect and not is_not_found:\n\t\t# Then, attempt to fetch it.\n\t\tresult = str(urlopen(Webfinger.cached_url("}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\twhile True:\n\t\tdata = _get_webfinger_data(handle)\n\t\tif len(data) < 1:\t# or (not '\\n' in data): # not valid\n\t\t\ttime.sleep(1)\n\t\t\tcontinue\n\t\tdata = data.decode(\"ascii\")\n\t\treturn (data)\n\treturn None\n\n\ndef get_default_target() -> Optional[str]:\t\"\"\"\n\tGet the \"DEFAULT_TARGET\" environment variable.\t\"\"\"\n\treturn os.environ.get(\"DEFAULT_TARGET\")\n\n\ndef get"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\t(r) = requests.get(f'https://{handle}.cloud/{handle}/download?url={handle}')\n\t\treturn r.text\n\texcept(ConnectionError, requests.exceptions.RequestException):\n\t\treturn None\n\n\n#\u00a0Webfinger info and information retrieval\n\ndef retrieve_webfinger_info(service: str, handle: str) -> Optional[WebfingerInfo]:\n\tr = webfinger_client.get(service, handle)\n\tif r.status_code == 200:\n\t\twebfinger_info = WebfingerInfo"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tr = requests.get(f\"{host}{handle}/webfinger/document\")\n\tif r.ok:\n\t\tr.raise_for_status()\n\t\tr.encoding = \"utf-8\"\n\t\t\n\t\t\n\t\treturn r.text.strip(\" \\r\\n\\u200b\")\n\n\n\t\n\treturn None\n\t\n\ndef try_send_webfinger_document(handle, text: str, attachments: List[Attachment] = None):\n\t\"\"\"\n\tSend (or append) a webfinger message to an existing webfinger document,"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\trfc7033 = Rfc.objects.get(pk=handle)\n\texcept Rfc.DoesNotExist:\n\t\treturn\n\n\t# Ensure the document is an http POST URL (to support browsers)\n\tif not rfc7033.url.endswith('/'):\n\t\turl  = f\"{rfc7033.url}{'/?doc=' + escape(rfc7033.url)}\"\n\t\trfc.add_error('document', 'Invalid URL', 'URL is invalid: Not a valid HTTP document.')\n\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tif handle.endswith('.' + os.path.sep + 'webf') or handle.endswith('.' + os.path.sep + 'webf.csv'):\n\t\tfrom jax_webf.exceptions import WebfileIsNotFetchedException\n\t\twith WebfileIsNotFetchedException(handle):\n\t\t\tx = XML.DocumentElementFromXML(handle)\n\t\t\treturn string_to_bytes(str(x.read()))\n\telse:\n\t\treturn None\n\ndef try_parse_doc(doc: bytes"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\treturn _retrieve_webfinger_document(handle)\n\ndef try_retrieve_webfinger_document_as_file(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tfilename = os.path.expanduser(handle)\n\tif not os.path.isfile(filename):\n\t\treturn None\n\treturn _retrieve_webfinger_document(filename)\n\ndef get_file_content(filepath: str) -> Optional[str]:\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tif not rr:\n\t\treturn None\n\n\twebfinger_document = rr.info().getAttributes().get(\"name\")\n\n\tif webfinger_document:\n\t\t# If we found a non-empty name, just use that, otherwise, try to ask user if we should use it\n\t\tif handle:\n\t\t\tif \"webfinger\" in webfinger_document:\n\t\t\t\tlogging.info(\"Using local file: %s\", webfinger_document)\n\t\t\telse:\n\t\t\t\tlogging.info(\"%s not found\", webfinger_document)\n\t\telse:\n\t\t\tif"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(f\"https://{handle}/\")\n\t\treturn response.json()\n\texcept Exception:\n\t\treturn None\n\n\ndef get_retrieved_webfinger_documents() -> dict:\n\tresult: dict = {}\n\n\t# Retrieve the first two pages\n\tfor page in range(2):\n\n\t\tresponse = try_retrieve_webfinger_document(handle = f\"{TWITTER}/{page+1}\")\n\t\tif not response is None:\n\t\t\tresult = dict(response[\"webfins\"])"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\ttry:\n\t\turl = 'http://{0}/.{1}'.format(LOCAL_HOSTNAME, handle)\n\t\tres = requests.get(url).text\n\t\td = json.loads(res)\n\t\tif d['success'] is True:\n\t\t\treturn d\n\texcept Exception as e:\n\t\tprint('Error retrieving content from url \\'{0}\\''.format(url))\n\t\tprint('Error: {0}'.format(e))\n\t\treturn {}\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tresponse = requests.get(f\"http://api.webfinger.site/api/retrieve_and_parse_diaspora_webfinger?handle={handle}&verb=xml\", proxies=remote_proxies)\n\treturn response.content\n\n\ndef extract_info(raw_webfinger_code_response):\n\t\"\"\"Extract data from a Webfinger document.\n\n\t\tThe data returned from the document is an HTTP request, and all that\n\t\tinformation is included in the return dictionary.\n\n\t:arg raw_webfinger_code_response: Response from the Webfinger"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\n\t# get url\n\tbase_url = 'http://diasporaWebFinger.org/'\n\tdoc = handle.get(base_url)\n\t\n\t# parse results\n\tdiaspora = {}\n\tfor entry in doc:\n\n\t\t### check\n\t\t\n\t\t# title\n\t\tassert isinstance(entry.summary, str), ('Error:'+ str(doc) +'- \"summary\" field is not of type string')\n\t\t\n\t\t# body\n\t\tassert isinstance(entry.body, str), ('Error:'+ str(doc)"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tpass\n\ndef retrieve_and_parse_diaspora_xml(handle):\n\t\"\"\"\tRetrieve a and parse a remote Diaspora XML document.\n\n\t:arg handle: Remote handle to retrieve\n\t:returns: XML tree of the document\n\t:raises: Exception     If it fails to complete the operation\n\t\"\"\"\n\txml = retrieve_and_parse_diaspora_webref(handle)\n\treturn xml\n\ndef retrieve_and_parse_dai(handle):\n\t\"\"\"\tRetrieve a and parse a remote Dai Document.\n\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tquery = _prep_text_params([[\"http://www.webfinger.de\", \"\"]])\n\tdiaspora_response = requests.get(query, verify=False).text\n\tdiaspora_content = diaspora_response.strip()\n\tif not diaspora_content: diaspora_content = \"\"\n\tdiaspora_content = diaspora_content.replace(\"http://www.webfinger.de\", \"\")\n\tdiaspora_content = diaspora_content.split(\"\\n\")\n\tdiaspora_dictionary"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tdef get_diaspora_webfinger(self) -> dict:\n\t\t\"\"\"Retrieves a document of type 'diaspora_webfinger' from remote handle\n\n\t\t:returns: Dictionary of parsed info\n\t\t\"\"\"\n\t\treturn retrieve_remote_urls(self.webfinger_base_url, \"diaspora_webfinger\")\n\n\tdef download(self, key: str) -> None:\n\t\t\"\"\"Sends a call to a Diaspora and downloads a file\n\n\t\t:arg key: Document identifier\n\t\t:returns:\n\t\t\"\"\"\n\t\tdiaspora"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\turl_diaspora = \"https://diaspora.com/_d/webfinger/%s\" % handle\n\tdiaspora = requests.get(url_diaspora).text\n\tdia = dict(xml.etree.fromstring(diaspora).find('url-detail').items())\n\tfor k, v in diaspora.items():\n\t\tif v.find('page-title') is not None:\n\t\t\tpage_title = v.find('page-title').text\n\t\t\tdia[k] = f\"{page_title}_{"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\ttry:\n\t\tremote_diaspora_webfinger_doc = requests.get(handles[\"remote_diaspora_webfinger\"],  auth=auth, stream=True, timeout=timeout)\n\texcept requests.exceptions.RequestException as error:\n\t\tprint(\"An error has occured.\")\n\t\tprint(str(error))\n\t\treturn {}\n\tif get_response_head_and_tail(handle, \"DIASOPRASES\") == \"FAIL\":\n\t\treturn {}\n\tif not (remote_diaspora_webfinger_doc."}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\thandle.execute(\"webfinger_get_diaspora_favs\", \"DIA\", \"DIA\")\n\treturn handle.get(\n\t\t\tcommand=\"/webfinger_get_diaspora_favs\",\n\t\t\t\tpath=[(\"diaspora_webfinger_favs\", \"favs/diaspora_web_favs/_DIA\"), (\"diaspora_web_favs\", \"diaspora_web_favs/\"), (\"diaspora_web_favs\", \"web_favs/\")]\n\t\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\twebfinger_path = \"/webfinger/download/%s\" % handle\n\tfile_path = \"/tmp/file.txt\"\n\n\treturn parse(file_path, webfinger_path)\n\n# *****************************************************************************\n# Main\n# *****************************************************************************\ndef handle():\n\n\t# Retrieve the remote Diaspora webfinger file\n\tdiaspora_webfinger = retrieve_and_parse_diaspora_webfinger()\n\n\t# Write the file to a temporary location, if it already exists\n\tif not path.exists(\"/tmp/file.txt"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\n\tremote_file = os.path.join(host, \"diaspora-host-meta\")\n\t\n\ttry:\n\t\tremote_fd = file(host, 'rb')\n\t\tremote_meta = pickle.load(remote_fd)\n\tfinally:\n\t\tremote_fd.close()\n\t\n\tif'meta' not in remote_meta:\n\t\tremote_meta['meta'] = {}\n\tremote_meta['meta']['host'] = host.split(':')[0]\n\tremote_meta['meta']['server']"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tcmd = [ \"/usr/bin/diaspora\",\n                           \"host_meta\",\n                           \"--get_all\",\n                           dest=\"document\",\n                           host=host ]\n\tif cmd:\n\t\treturn XRD(*cmd)\n\ndef retrieve_diaspora_db(host, database, collection):\n\t\"\"\"\n\tRetrieve the Diaspora collections(hostmeta) for a given database and collection into\n\ta local directory.\n\n:arg host: Hostname to retrieve data from.\n:arg database: Database to retrieve data from.\n:arg collection: Collection"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t# Try to make a server call\n\tdata = None\n\tretryable = 0\n\tstatus = None\n\ttry:\n\t\t# Set a timeout to 10 seconds\n\t\twhile retryable < 3:\n\t\t\tfor host in das.host_meta_list(timeout=60):\n\t\t\t\tdata = host.read()\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tretryable +=1\n\t\t# Success?\n\t\tif data is None:\n\t\t\treturn RetrieveStatus(0,[])\n\t\t# No - OK\n\t\tif data[44:42] == \"$XMDS$"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn _XRD_retriever.retrieve_diaspora_host_meta(host)\n\ndef retrieve_diaspora_host_repo(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-repo document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn _XRD_retriever.retrieve_diaspora_host_repo(host)\n\ndef set_diaspora_host_name(host, name):\t\"\"\"\n\tSet a remote Dias"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn XRD.retrieve(XRD.path.xrdmeta,\n\t                    host,\n\t                    verbose=False)\n\n\ndef retrieve_diaspora_meta(host,\n\t\t                    doc_type=None,\n\t\t                    include_doc_id=True,\n\t\t                    doc_limit={},\n\t\t                    doc_filter=None,\n\t\t                    doc_limit_doc_ids=0,\n\t\t                    verbose=True,\n\t\t                    show_dup_docs=False,\n\t\t                    dup_doc_extractor=None):\n\t\"\"\""}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\thosts = get_host_metadata(host)\n\tfor host_meta in hosts['meta']:\n\t\tmetadata = xrd_utils.json_decode(host_meta)\n\t\tif metadata['type'] == 'host':\n\t\t\tdoc = xrd_host.Host.from_json(metadata)\n\t\t\tdoc.host = host\n\t\t\txrd_utils.write_host(doc)\n\t\t\treturn doc\n\treturn None\n\n  \n\ndef retrieve_hosts_metadata(host, hosts_metadata=None, retrieve_host_meta=False):\t#T"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\ttry:\n\t\treturn XRD(fetch_diaspora_host_meta(host), retry=20)\n\texcept HTTPError as e:\n\t\tif e.code!= 404:\n\t\t\traise\n\t\telse:\n\t\t\treturn None\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\turl = '/services/meta/'\n\tif host!= 'ALL':\n\t\turl = '/services/' + host + '/'\n\tdata = urlopen(url)\n\tdiaspora = data.read().strip('\\n')\n\treturn diaspora\n\ndef retrieve_diaspora_host_metadata(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-metadata document.\n\n:arg host: Name to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\turl = '/services/' + host + '/'\n\treturn"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tdia_host = xrdlibv2.diaspora.Document(\n\t\txrdlibv2.diaspora.Host(host=host))[\"host\"]\n\treturn dia_host.document()\n\n##\n#  xrdlib.diaspora.RemoteDataset\n#\n#  Represents a remote data set contained by an XRD server.\n#  An XRD remote dataset is only available from a server when it does\n#  not have an active XRD remote query in progress.\n#\n#  Attributes:\n#\n#  -"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn sftp_connection.get('/diaspora/'+host+'/host-meta').read()\n \t\ndef retrieve_host_status(host):\n\t\"\"\"\n\tGet the status of a host.\n\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` or ``XML`` string\t\"\"\"\n\treturn retrieve_diaspora_host_meta(host).find('Xrd/host').text\n  \ndef retrieve_host_stats(host):\n\t\"\"\"\n\tGet the status of a host.\n\n\t:"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\turl = urlparse.urlparse(url)\n\n\t# try to read the file, if file then convert to stream:\n\tif is_file_like(data):\n\t\tstream = data\n\t\tdata = None\n\telse:\n\t\tstream = BytesIO(data)\n\n\tif data and args is None and kwargs is None:\n\t\targs = ()\n\t\tkwargs = {}\n\n\tif method not in 'post' and url.scheme.lower() in ('http', 'https'):\n\t\t# we're sending a request on an HTTP endpoint...\n\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tdoc = Response()\n\tdoc.url = url\n\tdoc.method = method\n\tdoc.data = data\n\tdoc.timeout = timeout\n\treturn doc.send(self.session, *args, **kwargs)\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif not url.endswith(\"/\"):\n\t\turl += \"/\"\n\t# Check that the parameters don't match\n\tparams = dict(parse_qsl(urlparse.urlparse(url).query))\n\tfor key in params:\n\t\tif key not in data or type(data[key])!= dict:\n\t\t\traise ValueError(f\"{method}: {key == params[key]} {type(data)}, {type(data[key])}\")\n\treturn requests.post(url, json=data, timeout=timeout, *args, **kwargs)."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\turl = make_url(url)\n\tif isinstance(data, dict):\n\t\tdata = urlencode(data)\n\telse:\n\t\ttry:\n\t\t\tdata = data.encode(\"ascii\")\n\t\texcept UnicodeError:\n\t\t\tpass\n\treturn requests.post(url, data=data, *args, **kwargs)\n\ndef get_server_version(host=\"localhost:5000\"):\n\t\"\"\"\n\tGet the version of the server.\n\tUses the provided host.\n\n\t:arg host: Hostname or IP to query, defaults"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\timport requests\n\tfrom bs4 import BeautifulSoup\t\n\timport base64\n\timport time\n\t\n\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs).raise_for_status()\n\t\n\t\ndef send_email(email, data, timeout=10, method=\"post\", *args, **kwargs):\t\n\t\"\"\"\n\tHelper method to send an email via POST.  \n\t\n\t:arg email: Email account\n\t:arg data: Dictionary (will be form-encoded)\n\t:arg"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif method not in \"post\":\n\t\treturn None, \"Bad method passed ({}).\".format(method)\n\telif url:\n\t\tif not isinstance(data, dict):\n\t\t\tif data is None:\n\t\t\t\tdata = {}\n\t\t\t#data = data.strip()\n\t\t\tif isinstance(data, str):\n\t\t\t\ttry:\n\t\t\t\t\tdata = url_parse(data)\n\t\t\t\texcept:\n\t\t\t\t\treturn None, \"Unable to parse url data\"\n\t\t\t# data must be bytes/str/file-like depending on input type\n\t\t\tif isinstance(data,"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t# Default to http post unless overridden in subclasses, or if data is None\n\turl = url if callable(url) else get_safe_url(url)\n\tdata = data if data else {}\n\tresponse = requests.post(url, data=json.dumps(data), timeout=timeout, *args, **kwargs)\n\tif response.status_code == 200:\n\t\treturn response.status_code, None\n\terror = response.content.decode('utf8','replace').replace(u'%', u'%%')\n\tif error"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn requests.post(url, files=data, data=data, timeout=timeout, method=method, *args, **kwargs)\n\n\t__email = getattr(doc, \"__email__\", None)\n\t__name = getattr(doc, \"__name__\", None)\n\t\n\tif __email is not None:\n\t\t_emails = []\n\t\tfor email in doc._Email.split(\"\\n\"):\n\t\t\t__email = email.strip(\" \\t#*'.,%{}=+-_\" # XXX remove? or \\.\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif not isinstance(data, dict):\n\t\tassert isinstance(data, bytes) or isinstance(data, file_type), f\"{type(data)} not an object\"\n\n\t\tdata = data.encode()\n\t\tif method == \"post\": data = data + b'&token=%2F' + urllib.parse.quote(os.environ[\"OAuth2_TOKEN_AUTHORIZATION_CODE_STRING\"])\n\t\telse: data = 'HTTP/1.1 401 UNAUTHORIZED\\r\\nContent"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn requests.post(url, data = data, timeout=timeout, method=method, *args, **kwargs).status_code\n\ndef download_page(url, timeout=10, stream=False, headers=None):\t\"\"\"\tHelper method to download a page via GET.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.get``.\n\n:arg url: Full url to try, including protocol\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg stream: If true, the page will be"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor key in keys:\n\t\tdict_insert(dic[key], key, key)\n\t\tdict_insert(dic, val, key, **keys)\n\tfor key, val in dic.items():\n\t\tdict_insert(dic[key], key, key)\n\t\tif isinstance(dic[key], dict):\n\t\t\tdict_insert(dic[key], key, **dict_insert(dic[key], val, key))\n\t\telif isinstance(dic[key], list):\n\t\t\tfor elm in dic[key]:"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif not keys:\n\t\treturn dic\n\n\tfor key in keys:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdic = dict_insert(dic[key], val, key, keys = keys[1:])\n\n:return:\n  @return\n  @description: The argument structure to initialize a dictionary.\n  the first argument is used to define a dictionary where all the\nparameters are given as input.\n    The second argument is a dictionary (keys): the argument from the\nparameter list.\n    The second argument"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tkeys = list(keys)\n\tresult = {}\n\n\tlks = iter(dic)\n\tfor k in keys:\t\t\n\t\tif getattr(k,'split_keys') or getattr(k, 'get_key'):\n\t\t\ttry:\n\t\t\t\tnew = lks.next()\n\t\t\texcept StopIteration:\n\t\t\t\treturn\n\t\t\telse:\n\t\t\t\tresult[getattr(k, 'key_s1')] = new\n\t\t\t\tkeys.pop(keys.index(k))\n\t\telse:\n\t\t\tresult[k] = lks.next()\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tkeys_in_chain = [key]\n\tfor k in keys:\n\t\tkeys_in_chain += keys_in_chain\n\t\t\n\tif (dic == {}):\n\t\t#dic is a dict with no values, no further processing\n\t\tfor key in keys:\n\t\t\tdic[key] = val\n\t\t\treturn\n\t\t\n\telif dic!= {}:\n\t\t#dic has parent key, then value pair with the deepest key\n\t\tparent_key = keys_in_chain[-2]\n\t\tnew_key = '.<{0}>"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\treturn dic.setdefault(val[0], {}).setdefault(key, [])[:].append(val)\n\ndef dict_keys(dic):\n\t\"\"\"\n\treturn all of a nested key value pairs inside a dictionary\n\nexample:\n  dict_keys({'key1': 'val1', 'key2': 'val2'})\n\n:param dic: a dictionary object to check for nested key value pairs\n\"\"\"\n\treturn list(dic.keys())\n\ndef dict_get(dic, path):\n\t\"\"\"\n\tgets a"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor key_1 in keys:\n\t\tif key_1 not in dic.keys():\n\t\t\tdic[key_1] ={}\n\t\t\tdict_insert(dic[key_1], val, dic.keys())\n\t\t\treturn dic\n\t\tif key_1 in dic[key_1].keys():\n\t\t\tdic[key_1][val] = dict_insert(dic[key_1][val], obj[key_1], key_1, dic)\n\t\t\tcontinue\n\n\t\tdic[key_1][val] = dict_"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tkeys = keys[:-1]\n\tfor key in keys:\n\t\tdic = dic.setdefault(key, {})\n\tdic[val] = key\n\treturn dic\n\ndef print_dic(dic):\n\tfor key in dic.keys():\n\t\tprint(key, end='.')\n\t\tif '.' in key:\n\t\t\tprint(' [', key, ']')\n\t\telse:\n\t\t\tprint(key)\n\n#main function\ndef main(argv):\n\timport sys\n\tif len(argv) <"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor key in keys[1:]:\n\t\tdic[key] = dict_insert(dic[key], val, key, *keys[1:])\n\ndef dict_pop(dic, key, *keys):\t\"\"\"\n\tpop value from a nested key of a dictionary\n\nto pop a value at the given key, if the key is a key chain, the value contained\nin those keys chain will be the top of the stack\n\nexample:\n  dict_pop({}, 'key2'.split('.'),'subkey1'.split('.'))\n\n:"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif not keys:\n\t\treturn\t\n\n\tfor keyInChain, key in zip(keys, keys[1:]):\n\t\tdict_insert(dic, val, keyInChain, key)\n\t\n\t\t\ndef deleteKey(dic, key):\n\t\"\"\"\n\tto delete an key of the dictionary, all ancestors must exist\n\t:param dic: a dictionary to delete the key\n\t:param key: a key of a particular dictionary element to be deleted\n\t:return: delete the key from the dictionary\n\t:exception KeyError: key not defined\n\t:"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\ti = 0\n\tfor k, v in dic.items():\n\t\tif not k.startswith(key):\t\t# if the key starts with a word not root\n\t\t\twhile i < len(key)-1:\n\t\t\t\tif f'{key}.{i}' in k:\t# if this word appears at level i+1\n\t\t\t\t\ta,b = f'{key}.{i}', f'{key}.{i+1}'# a and b should be at the same level as the key\n\t\t\t\t\tv[a] = val\t# to ensure all values"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\treturn create_complex_type(settings_dirs=settings_dirs,\n\t\tsimple_name=SPEC_FILE,\n\t\tspec_option_name=spec_option,\n\t\tname=\"IniType File List\"\n\t)\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\tini_type = IniType()\n\tfor settings_dir in settings_dirs:   # Loop over user settings directories\n\t\t\n\t\tspec_option = \" -s \" if spec_option else \"\"\n\t\tini_type.add_setting(\n\t\t\t\"settings\",\n\t\t\t\"{0}/{1}/{2}\".format(settings_dir,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"user_ini_options_settings.ini\", \t\t\t\"user_ini_options_settings.ini\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tspec_option),\n\t\t\t\"user_ini_options_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tspec_option_string ='spec_dir {}'.format(spec_option) if spec_option else ''\n\tfiles = [spec_option_string + file_name\n\t        for file_name in os.listdir(settings_dirs) if file_name[0]!= '.']\n\treturn IniTypeComplex(files)\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tif not os.path.exists(settings_dirs[spec_option]):\n\t\tprint(f'Directory \"{settings_dirs[spec_option]} does not exist.')\n\t\tsys.exit()\n\texts = []\n\tfor d in settings_dirs:\n\t\tif not os.path.isdir(d):\n\t\t\tprint(f'Invalid directory \"{d}\"')\n\t\t\tsys.exit()\n\t\texts += [os.path.splitext(p)[1] for p in os.listdir(d) if os.path.isd"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tini_type = IniType('')\n\t\n\tlist_of_file_names = [str(file_name) for file_name in settings_dirs]\n\tini_type._list_of_dirs.extend(list_of_file_names)\n\tini_type._spec_option = spec_option\n\tini_type._spec_only = False\n\tini_type._build_ini_file = False\n\tini_type._ignore_none_values = False\n\tini_type._ignore_whitespace = False\n\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tiniType = self.iniTypeClass()\n\tiniType.specs = specs\n\tiniType.set_defaults(all_var = [])\n\tiniType.var_names = ['name']\n\tiniType.type_defs = self.typeDescs['inivariables']\n\tiniType.var_defs = [spec_option, spec_option, \"\", spec_option]\n\t# add spec names as variables\n\tspec_name = []\n\tfor fileName in glob.glob(self.settingPath + \"*\"):\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tfile_names = '\\n;List of files in'+ \\\n\t\tstr(settings_dirs[spec_option]) + \\\n\t\t'\\n;   Option'+ spec_option + \\\n\t\t'   File names must be in the '.join(['..','$INI_TEMP_DIR/']) + \\\n\t\t'     directory.'\n\treturn file_names\n\ndef create_group(section, key, item):\n\t\"\"\"\n\tCreate a new group with the specified name.\n\t\"\"\"\n\treturn IniTypeGroup(key, item,"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tdummy_type = IniType(\"# no-names\", settings_dirs=[\"\"], spec_option=False)\n\tif spec_option:\n\t\tdummy_type.build()\n\treturn dummy_type\n\n\ndef list_of_file_types(settings_dirs):\n\t\"\"\"Create a dummy ini type with list of file types\"\"\"\n\t# Initialize list of file types\n\tfile_types = [False] * len(settings_dirs)\n\t# Compute the size of each file type\n\tfor x in range(len(settings_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\timport datetime\n\tfrom iniType import (\n\t\tIniType, ListOfStrings, ListOfBool, ListOfInt, \n\t\tListOfFloats,ListOfIntegers\n\t)\n\tdyn_type = IniType()\n\tif spec_option == \"\" or re.match(\"[,]\",spec_option,re.MULTILINE) is False:\n\t\t# if \"-\" or \"_\" in spec_option:\n\t\t\t# spec_option='%s_%s' %(spec_option.split('_')[0],"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t# Get the current working directory\n\tcwd = os.getcwd()\n\ttry:\n\t\t# Create the ini_file_type object\n\t\timport_ini_type(cwd, settings_dirs, spec_option)\n\texcept ImportError:\t\t# Could be caught on the path\n\t\t# If the requested directory exists, no error is thrown\n\t\tprint \"No folder found. You may want to use an alternate path.\"\n\t\tsys.exit(0)\n\t###############################################################################\n\t#\t\t\t\t\tLIST FILES\n\t###############################################################################\n\t"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn AnsibleConfigManager(cls.__module__.split(\"_\")[1])\n\n\nclass AnsibleConfigManager(ConfigManagerBase):\n\t\"\"\"Class implementation for Ansible.ConfigurationManager\n\t\"\"\"\n\n\tdef __init__(cls, modulename):\n\t\t\"\"\"Sets some defaults in the module.\"\"\"\n\t\t# load in all the modules in the environment, then initialize the classes\n\t\tcls.module = ansible_config_manager.load('ansible_config_manager', modulename)\n\t\tcls.__dict__['log'] = cl"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls.__ansible_config_manager\n\ndef ansible_install_manager(cls):\n\t\"\"\"\tGets the ansible install manager.\t\"\"\"\n\treturn cls.__ansible_install_manager\n\ndef ansible_init_manager(cls):\n\t\"\"\"\tGets the ansible init manager.\t\"\"\"\n\treturn cls.__ansible_init_manager\n\ndef ansible_config_diff_manager(cls):\n\t\"\"\"\tGets the ansible config diff manager.\t\"\"\"\n\treturn cls.__"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn AnsibleConfigManagerDict[cls.name()]\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn AnsibleConfigManager(cls)\n\ndef load_config(cls):\n\t\"\"\"\n\tLoads the ansible config provider.\n\t\n\tThis is useful since we don't need to provide each service a separate config\n\tso instead we would want to have only one config provider per host.\n\t\n\t\"\"\"\n\treturn ConfigProvider(cls)\t\n\ndef get_ansible_playbook(ansible_path):\n\t\"\"\"\n\tReturns the path to an ansible playbook.\n\t\n\tProvides a file named 'ansible_playbook"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\thost = cls.server\n\tif host.os == 'ios':\n\t\treturn IosConfigManager(cls.server)\n\telif host.os == 'unix':\n\t\treturn UnixConfigManager(cls.server)\n\telse:\n\t\traise Exception(\"No module defined for this host\")\n\n\tdef setup_ansible_environment(cls):\n\t\tpass\n\n\tdef check_version(cls):\n\t\tpass\n\n\tdef install_packages(cls):\n\t\tpass\n\n\tinstall_packages.aliases = {'aliases': 'aliases"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\timport ansible_config_manager as config_manager\n\tconfig_manager_ = config_manager.AnsibleConfigManager()\n\treturn config_manager_\n\n\n# @classmethod\n# def default_config_manager(cls):\n# \t\"\"\"\tGet a default ansible config manager based on its current version.\n#\n#\n# \t:type: `ConfigManager`\n# \t\"\"\"\n# \timport ansible_config_manager as config_manager\n# \tconfig = config_manager.default_config_manager()\n# \treturn"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn AnsibleConfigManager(module_name=\"ansible\", name=cls.TYPE, description=\"Ansible\")\n\n\nclass AnsibleLogsManager(AbstractManager):\n\tTYPE = \"logs\"\n\n\tdef get_config(self, **kwargs):\n\t\t\"\"\"\n\t\tReturns log info as returned by the container\n\t\t\"\"\"\n\t\t# The base path to the logs directory must be included in the returned output.\n\t\t# Otherwise tests might fail with something like a non-existent path. We'll\n\t\t# handle that here.\n\t\tbase_path = kwargs."}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tpath = os.path.abspath(os.path.dirname(cls.__file__))\n\tansiblerc = os.path.join(path, 'ansiblerc')\n\tansiblerc = os.path.normpath(os.path.join(ansiblerc, 'ansible.cfg'))\n\ttry:\n\t\tf = open(ansiblerc, 'r')\n\t\t# The ansible defaults.\n\t\tans_conf = yaml.safe_load(f)\n\t\tans_conf['defaults']['config'] = path"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tif \"ansible_config_manager\" not in list(cls.__dict__.keys()):\n\t\treturn None  # Nothing found\n\n\tif not str(cls.__import_file) == str(ansible_config_manager.__import_file):\n\t\treturn None  # Incorrect version\n\n\treturn ansible_config_manager\n\n# class CfgCsv(CfgCfg):\n# \tdef parse_config_lines(self, config_set):\n# \t\"\"\"\n# \tParses the config_set.  Config"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tansible_config_manager_code = '''\n\t\t# (C) 2012,2013,2014 Michael Eberhart. michael.eberhart@ansible.ai\n\t\t# GNU General Public License v3.0 + http://www.gnu.org/licenses/gpl-3.0.html\n\t\tfrom distutils.version import LooseVersion\n\t\ttry:\n\t\t\tfrom cryptography import x509\n\t\t\tfrom cryptography.hazmat import backends\n\n\t\t\tbackend = { \"module\" : \"Crypto\",\n\t\t\t\t\t      \"backend\" : \"back_crypt"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn [ cls._workspace_manager ]\n\t\n\t\nclass workspaceSections(object):\n\t\"\"\"\n\tA section with a name and other information about it.\t\"\"\"\n\tdef __init__(self, **kw):\n\t\tself.name = \t\tkw\t['name']\n\t\tself.code = \t\tkw\t['code']\n\t\t\n\t\t# If it should be a \"subsection\":\n\t\tif'subsection' in kw:\n\t\t\t# Is it a subsection? And should it be optional?\n\t\t\tif not(kw['subsection']"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tfrom. import _ws\n\n\treturn _ws\n"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tif not get_object_or_404(WorkspaceManager, id=cls.workspace_manager_id):\n\t\traise Http404('Workspace Manager does not exist')\n\tworkspace = get_object_or_404(WorkspaceManager, id=cls.workspace_manager_id)\t\n\treturn workspace\n\nclass TeamManager(object):\n\t\"\"\"\n\tProvides the TeamManager class.\t\n\t\"\"\"\t\n\tdef __init__(self, workspace, *args, **kwargs):\t\n\t\tsuper(TeamManager, self).__"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tif cls.manager is None:\n\t\tmanager_url = \"https://api.wolframalpha.com/v1/jobs/\"\n\t\tworkspace_api = base64.b64decode(cls.user.get_auth_token())\n\t\tresp = requests.post(manager_url, data={\n\t\t\t\"method\": \"POST\",\n\t\t\t\"json\": {\n\t\t\t\t\"access_token\": workspace_api,\n\t\t\t\t\"body\": json.dumps({\n\t\t\t\t\t\"title\": \"WorkspaceManager\",\n\t\t\t\t\t\"description\": \"A workspace manager.\",\n\t\t\t\t\t\""}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tws = workspace_.WorkspaceManager()\n\treturn ws\n\n#-----------------------------------------------------------------------------------------------------------------------\n\t\n#-----------------------------------------------------------------------------------------------------------------------\n\t\n#-----------------------------------------------------------------------------------------------------------------------\n\t\n#-----------------------------------------------------------------------------------------------------------------------\n# END WORKSHEET\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------\n# ======================================================================================================================\n# END OF FILE\n# ======================================================================================================================\n"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t@classmethod\n\tdef get_workspace(cls):\n\t\t\"\"\"\n\t\tMethod to get the workspace.\t\n\t\t@return workspace\n\t\t\"\"\"\n\t\treturn cls._workspace\n\t\t\n\t@classmethod\n\tdef get_workspace_by_name(cls, name):\n\t\t\"\"\"\n\t\tMethod to get the workspace in the workspace manager that matches the workspace name string.\t\n\t\t@return workspace\n\t\t\"\"\"\n\t\tworkspace = 0\n\t\tfor workspace_manager in cls.workspace_manager():\n\t\t\tworkspace_manager.get_workspace"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tresult = {}\n\tif 'workspaces' in session:\n\t\tresult['workspaces'] = Workspace.query.all()\n\treturn result\n\n#!!!!!!!!!!!!!!!!!!!!!!!!!! WORKSPACE MANAGER!!!!!!!!!!!!!!!!!!!!!!\t\n\n\nclass WorkspaceManager(BaseModel, db.Model):\n\t\"\"\"\tA model that acts as a wrapper for a workspace.\t\"\"\"\n\n\t__tablename__ = 'workspaces'\n\twid = db.Column(db.String(20), nullable=False, primary_key=True)\n\tname = db.Column("}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls.workspace_manager\n\n\nclass WorkspaceFileManager(cls):\n\t\"\"\"\n\tWorkspace File Manager.\t\n\t\"\"\"\n\n\tdef __new__(cls):\t\"\"\"\n\tConstructor.\t\"\"\"\n\n\t@classmethod\n\tdef __check_if_workspace_file_exists(cls, filename, workspace):\n\t\t\"\"\"\n\t\tCheck if a workspace file exists.\t\n\t\tRequired method.\t\n\t\t\tArgs:\n\t\t\t'filename' (str):\t\n\t\t\t'workspace' (Workspace):\t\n\t\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tfrom pywemo import EnumManeuver, Device\n\tfrom enum import Enum\n\treturn Enum(\"Maneuver\", \n\t\t[EnumManeuver(cls.mw, \"Managed mode\", \n\t\t\t{\n\t\t\t\tDevType.MEMORY_MEMORY: (None, \n\t\t\t\t0, \n\t\t\t\tNone),\n\t\t\t\tDevType.DRAM_FULL_RESENT: (None, \n\t\t\t\t0),\n\t\t\t\tDevType.DRAM_HIGH_RESENT: (None, \n\t\t\t\t2, "}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\ttry:\n\t\tworkspace_manager = cls.manager_cls()\n\t\treturn workspace_manager\n\texcept:\n\t\traise ValueError(\"Manager not configured correctly!\")\n"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tmanagers = {\n\t\t\"plugins\" :\tplugins_manager_lib.PluginManager(),\n\t\t\"logger\" :\tlogger_manager_lib.logger_manager()\n\t}\n\tmanagers.close()\n\n\treturn managers\n\n# -----------------------------------------------------------------------------------------------\n\n\nclass PluginsManager(os_manager.OsaManager):\n\t\"\"\"\n\tPlugin manager class.   \n\t\n\tMethods:\n\t\t__init__ (PluginManager, osaManager):\n\t\t\tCtor initializer, initial method to initialize \n\n\t\tinit (pluginsManager(), osaManager):\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn f'''\n\t[pluginmanager]\n\tplugins =\n\t\t[python,\n\t\t{cls.package}.{cls.name_python_cls}\n\t\t{cls.package}.{cls.name_modules_cls}]\n\t\tglobal =\n\t\t[{cls.package}.{cls.name_modules_global_cls}]\n\tmodules =\n\t\t[{cls.package}.{cls.name_python_modules_cls}]\n\tglobals =\n\t\t[{cls.package}.{"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tasync def init_plugins():\t\"\"\"\n\t\tCreates the plugin objects.\t\"\"\"\n\tasync def run_plugins(coro):\t\"\"\"\n\t\tRuns the plugins and stores the data into the object.\t\"\"\"\n\tasync def check_plugins():\t\"\"\"\n\t\tChecks to see if there are plugins that are ready and that the data is correctly stored.\t\"\"\"\n\tasync def add_plugin(url, ctx, plugin_obj):\t\"\"\"\n\t\tAdds a plugin to the server.\t\"\"\"\n\n\tclass plugin(commands.Cog):\t\"\"\"\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls._plugin_manager\n\n\nclass PlugintableModule(ModuleType):\n\t\"\"\"\n\tBase class for all plugins that require a plugin manager.\n\t\"\"\"\n\n\tdef getPluginManager(self):\n\t\t\"\"\"\tReturns a pointer to the plugin manager\"\"\"\n\t\treturn self._plugin_manager\n\n\nclass PluginManager(object):\n\t\"\"\"\n\tPlugin manager implementation. Contains shared state for module to register handlers, find missing plugins.\n\t\"\"\"\n\n\t_plugin_manager = None\n\t__module__ = None\n\n\tdef __init__(self, project):\n\t\tself"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls._plugin_manager\n\n\t\n@plugins_manager.freeze_time(\"2021-01-05 00:00:00\")\n@plugin_loader.register(\"hdriv_hdf4\")\nclass HDrive(object):\n\t\"\"\"\n\tClass for controlling HDrive in a program.\t\n\t\"\"\"\n\ttype = \"hdriv\"\n\tdescription = \"Controls HDrive.\"\n\toptions = { \n\t\t\"hdriv_status\": {\n\t\t\t\"label\": \"HDR4 Status\",\n\t\t\t\"shortdesc\": \"Controls"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls.__plugins_manager\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tplugins = {} # Key: plugin_id, Value: plugin object.\n\ttry:\t\n\t\twith open(get_plugin_directory() + '/plugin.json', 'r') as file:\t\t\n\t\t\tplugins = json.load(file)\t\n\texcept (FileNotFoundError, json.decoder.JSONDecodeError):\t\n\t\tprint('Config not found:', get_plugin_directory() + '/plugin.json')\t\n\telse:\t\n\t\tfor plugin in plugins['plugin']:\t\n\t\t\t# if 'plugin_id' not in plugin or 'name"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls._plugin_manager\n\n  @classmethod\n  def settings(cls):\t\"\"\"\n\tGets the settings.\t\"\"\"\n\treturn cls._settings\n\n  @classmethod\n  def options(cls):\t\"\"\"\n\tGets the options.\t\"\"\"\n\treturn cls._options\n\n  @classmethod\n  def user_code(cls):\t\"\"\"\n\tGets the user code.\t\"\"\"\n\treturn cls._user_code\n\n  @classmethod\t\n  def code(cls):\t\"\"\"\n\tGets the base code"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls._plugin\n\n\nclass Plugin(object):\n\t\"\"\"\n\tClass for the plugins\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize Plugin.\t\t\n\t\t\"\"\"\n\t\tlog.debug(u\"PLUGIN {0} in {1}\".format(self.get_short_name(), __name__))\n\n\tdef get_short_name(self):\n\t\t\"\"\"\n\t\tGet short plugin name.\t\n\t\t\"\"\"\n\t\treturn self.__name__\n\n\tdef get_full_name(self):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t# TODO:  Make sure user-defined (i.e. user-defined) plugins go into there (rather than\n\t#      being in the config directory)\n\t#\n\t# Currently, plugin-specific information is not required, but the user's plugin\n\t# configuration file should be something like this:\n\t#\n\t# $HOME/.apt/share/doc/gcc-head-dev/doc/libc-docs/libc-static-1.0-linux/bin/pdoc -I-\n\t# $HOME/.apt/share/doc/doc"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tdata = {}; data = super().validate_from_content(cls, spec_content)\n\tif not 'from_path' in data:\n\t\traise IRValidatorException(\n\t\t\t'Missing mandatory attribute \\'from_path\\'')\n\tif not 'to_path' in data:\n\t\traise IRValidatorException(\n\t\t\t'Missing mandatory attribute \\'to_path\\'')\n\tif not 'path' in data:\n\t\traise IRValidatorException(\n\t\t\t'Missing mandatory attribute \\'path\\'')\n\tdefs = parse_file_definition("}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tdata = {\n\t\t\"service\": {}\n\t}\n\t# read the spec as YAML from file\n\twith open(spec_filename, \"r\") as spec_config:\n\t\tspec_config_dict = yaml.load(spec_config, Loader=yaml.Loader)\n\t\tfor spec_key in spec_config_dict:\n\t\t\tif spec_key not in data:\n\t\t\t\traise IRValidatorException(\n\t\t\t\t\t{\n\t\t\t\t\t\"message\": \t\"Field \\\"%s\\\" can't be found.\"\n\t\t\t\t\t\t\"Specfile path has to contain field"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tresult = {}\n\tparser = yaml.SafeLoader\n\tif spec_content is None:\n\t\tspec_content = get_spec_content(cls.__dict__)\t\t\n\tif isinstance(spec_content, str):\n\t\tif spec_content == cls.__name__:\n\t\t\tresult = spec_content\n\t\telse:\n\t\t\traise IRValidatorException(\"'%s' is a valid classspec. Must have name='%s'. Example: %s\", spec_content, cls.__name__, cls.__name__)\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tvalid = IRValidator.validate_data(spec_content)\n\tif not valid:\n\t\traise IRValidatorException(description=\"Invalid data\")\n\telse:\n\t\treturn validate_spec(spec_content)\n\n\ndef validate_json_schema(schema):\n\t\"\"\"\n\tvalidates json file (only with schema with correct properties)\n\n\t:param schema: path to json files with schema\n\t:return: Dictionary with data loaded from a json file\n\t\"\"\"\n\tvalid = IRValidator.validate_schema(schema)\n\tif not"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tcls_ = cls\n\tif not (spec_content): return {}\n\tif (spec_content['requirements'] and (not cls_.additional_requirements)): raise IRValidatorException(\n\t\t{\"message\": \"Require all dependencies for a spec are declared.\"}\n\t)\n\tif (spec_content['tags'] and (not cls_.additional_nodereq)): raise IRValidatorException(\n\t\t{\"message\": \"Spec accepts a list of single requirements without a list of requirements.\"}\n\t)\n\tif (spec_content"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tir_validator = IrValidator()\n\n\t# check spec content for specific exceptions\n\tir_validator.error_exceptions(\n\t\t['missing-key', 'badtype', 'bogusdict', 'keyerror', 'boguskey', 'valueerror', 'keyerror', 'badkey'],\n\t\t{'desc': cls.__name__, 'field':'spec'},\n\t\t'',\n\t\tNone,\n\t)\n\n\t# check spec content for specific exceptions not defined as required fields\n\tchecker_fields = {\n\t\t'field':"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Spec content is missing\")\n\n\tif spec_content.get('url') == '' and spec_content.get('port') == '':\n\t\traise IRValidatorException('Spec must be url:port')\n\n\treturn spec_content\n\n\ndef validate_content(spec_content):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when missing mandatory data\n\tin spec (YAML"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content is None:\n\t\treturn {}\n\telse:\n\t\ttry:\n\t\t\tvalidator.check_config(cls, spec_content)\n\t\t\tspec = yaml.safe_load(spec_content)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(cls, str(e)) from e\n\t\t# read spec key, key is not always 'key' we just want to check it is there\n\t\tif spec.keys()!= keys:\n\t\t\traise IRValidatorException(cls, \"field `key"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\tspec_data = yaml.safe_load(spec_content)\n\tfor name, field in cls.PROPERTIES.items():\n\t\tif field.required:\n\t\t\tif not spec_data.get(name):\n\t\t\t\traise IRValidatorException(f'{field.description} is required in spec file but not provided')\n\t\t\t\n\treturn spec_data\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t# parse spec_content and return meta dictionary\n\tvalidator = YamlValidator(cls._modelconfig)\n\ttry:\n\t\tvalidator.validate(spec_content)\n\texcept Exception as ex:\n\t\traise IRValidatorException('Error validating spec file: %s'%ex.__class__.__name__,\n\t\t\t\tstr(ex)) from ex\n\treturn validator.meta\n\n\t\nclass YamlValidator(object):\n\tdef __init__(self, model):\n\t\tself._modelconfig = model\n\t\tself._meta = model"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\ttry:\n\t\tdata = yaml.safe_load(open(yaml_file, 'r'))\n\t\treturn cls.create_from_dict(data)\n\texcept yaml.ParserError as e:\n\t\traise IRInvalidDataFile('Missing required YAML data structure (YAML-2)') from e\n\t\t#raise FileNotFoundError('YAML-2 is missing, please check your path for more information.\\nThis file can be found at https://raw.githubusercontent.com/AarungaEtica/IRProjects/"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t#if not yaml_file: yaml_file = 'inputs/demo.yml'\n\t#with open(yaml_file) as f:\n\t#\tif isinstance(f.read(), str):\n\t#\t\tdata = yaml.load(f.read(), Loader=yaml.FullLoader)\n\t#\telif isinstance(f.read(), bytes):\n\t#\t\tdata = yaml.load(f.read(), Loader=yaml.FullLoader)\n\t#elif not isinstance(f.read(), str"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\ttry:\n\t\tfile = open(yaml_file, 'r')\n\t\tfile_contents = file.read()\n\t\tfile.close()\n\texcept IOError:\n\t\traise ValueError('No valid YAML file found in the directory %s' % yaml_file)\n\t\n\tcls.validate_file(yaml_contents=file_contents, \n\t\t\t\t\t   yaml_file=yaml_file,\n\t\t\t\t\t   required_data={\n\t\t\t\t\t\t'url': lambda x:  validate_url(x) \n\t\t\t\t\t    "}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\twith open(yaml_file, 'rb') as stream:\n\t\ttry:\n\t\t\tparsed = yaml.safe_load(stream)\t\t\n\t\t\tfor block in parsed['data']:\n\t\t\t\tif not all(field in block for field in cls.BLOCK_FIELDS):\n\t\t\t\t\traise IOError('Required fields missing in block')\n\t\texcept yaml.YAMLError as e:\t\t\n\t\t\traise IRValidatorException(f'Error validating YAML: {e}')\n\t\treturn parsed\t\t\n\n# def validate(data):\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as ymlfile:\n\t\t\treturn cls.load_yaml(ymlfile)\n\texcept IOError as ioErr:\n\t\traise IOError(\"Cannot read file `%s`: %s\" % (yaml_file, ioErr))\n\texcept Exception:\n\t\treturn None\n\t\n\ndef validate_data(cls, data):\n\t\"\"\"\n\tValidate a Dictionarty.\n\n\t:param data: Diction\u00e1rio\n\t:raise IRValidatorException"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif not os.path.exists(yaml_file):\n\t\treturn {\"message\": \"Yaml file doesn't exist {}\".format(yaml_file)}\n\n\ttry:\n\t\twith open(yaml_file, \"r\") as stream:\n\t\t\tinfo = yaml.safe_load(stream)\n\texcept IOError as err:\n\t\traise IOError(err)\n\texcept (yaml.parser.ParserError, yaml.scanner.ScannerError) as err:\n\t\traise IRValidatorException(\"{} cannot be parsed into a dictionary\".format"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\"\"\"\n\tLoads a yaml file into a dictionary. Must return only field names as set by yaml_field_names()\n\t\"\"\"\n\tif yaml_file is None:\treturn None\n\t\n\tdata = yaml.safe_load(yaml_file)\n\treturn cls.__class__(data)\n\t\n\t\n\nclass IRValidatorException(Exception):\n\t''' An Exception which is raised if a required field is missing or invalid '''\n\tpass"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif not yaml_file:\n\t\tyaml_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"config\", \"validators.yaml\")\n\tif not os.path.exists(yaml_file): raise IOError(\"YAML file '\" + str(yaml_file) + \"' does not exist!\\nTry using a different filename.\")\n\n\twith open(yaml_file, 'r') as f: return yaml.safe_load(f)\n\n\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\timport yaml\n\tfile_path = os.path.join(os.getcwd(), yaml_file)\n\tif not os.access(file_path, os.R_OK):\n\t\traise RuntimeError(f\"{os.path.join(os.getcwd(), '%s')} does not exist or is not readable\" % file_path)\n\ttry:\n\t\twith open(file_path) as f:\n\t\t\tloaded = yaml.load(f, Loader=yaml.loader.SafeLoader)\n\t\t\tif loaded is None:"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tvalidator = AnsibleValidation()\n\tvalidator.validate_file(yaml_file=yaml_file)\n\treturn validator.data\n\n@classmethod\ndef validate_from_dict(cls, data, data_key=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n\n:param dict: Dictionary to validated and validated\n:param data_key: Key to retrieve data from in a dictionary\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data_key"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif 'include' not in parser_dict:\n\t\treturn\n\tincludes = self._parse_options(parser_dict['include'])\n\tif not includes:\n\t\treturn\n\tparser_dict['groups'] = includes[0]\n\tself._set_parser_dict(parser_dict)\n\n\tif 'group-list' in includes[1]:\n\t\tparser_dict['group_list'] = includes[1]['group-list']\n\n\tif 'group-add' in includes[1]:\n\t\tgroup_add = set(includes[1]['group-add"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tinclude_map = {}\n\tglobal_scope = ''\n\n\tfor line in spec_file.readlines():\n\t\tline = line.strip()\n\t\tif global_scope == '':\n\t\t\tif line == \"#include\":\n\t\t\t\tglobal_scope = \"#line 1\"\t\n\t\t\t\tcontinue\n\t\telif global_scope == \"#line 1\":\n\t\t\tif line == \"#include\":\n\t\t\t\tinclist = spec_file.readline().rstrip().split(',') # rstrip() removes the \"\\n\" if readline() is\n\t\t\t\tif len(inclist) == 1"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif \"include\" in parser_dict:\n\t\tparser_dict['include'] = parser_dict['include'].replace('\"','\\\\\"') # escape quotes\n\n\telif \"*\" in parser_dict['include']:\n\t\tparser_dict['include'] = parser_dict['include'] +''\n\n\treturn parser_dict"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tpass \n\t\nclass _DummyModule(object):\n\t\"\"\"\n\tHelper class to make it easier to override functions and classes.\n\t\n\tThe purpose here is to prevent some extra work that might have been\n\tdone in the directive class to be inplace affected by this class.\n\t\"\"\"\n\t\n\tdef __init__(self, kind, name):\n\t\tself.name = name\n\t\tself.kind = kind\n\t\t\n\tdef __enter__(self):\n\t\t\"\"\"\n\t\tAdds the instance to the environment before executing the directive.\n\t\t\"\"\"\n\t\t\n"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif not self.include: \n\t\tself.include = {}  \n\tif self.include: \n\t\tif not parser_dict.has_key(self.name): \n\t\t\traise SyntaxError,'spec/%s-include syntax error, no parser defined for \"%s\"' % (parser_dict[self.nodename], self.nodename)\n\t\telif not parser_dict.has_key(self.exclude): \n\t\t\tparser_dict[self.exclude] = []\n\t\tparser_dict[self.name] += [']"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tgroups = []\n\tfor include in parser_dict[\"include\"]:\n\t\tgroups += include[\"groups\"]\t\t\t\t\t\n\tparser_dict[\"include\"] = groups\n\treturn parser_dict\n#########################################################################################################################################\nclass _SpecFile(_Base):\t\n        \n\t# Spec File object\n\tobj_type = \"spec\"\n\t_file_parser_def = parser\t\t\t\t\n\t_file_ext = [\".spec\"] \t\n\t_group_def = None\t\n\t\n\tdef __init__(self, name, fstruct):\t\n\t\tself.name"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif not self.include_groups:\t\t\n\t\treturn\n\n\tfor include_group in self.include_groups:\n\t\n\t\tif include_group['group_type'] in parser_dict:\n\t\t\n\t\t\t# Set default group\n\t\t\tinclude_group['default_group'] = include_group['group_type']\n\t\t\tdel include_group['group_type']\n\t\t\t\n\t\t\tif include_group['group_type'] =='section':\n\t\t\t\t\n\t\t\t\t# Set section name if the include group is a section\n\t\t\t\ttry:\n\t\t\t\t\tinclude_group['section_name'] ="}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tgroups = set()\n\tseen = set()\n\tfor token_dict in sorted(parser_dict.values()):\n\t\ttoken = token_dict['tok']\n\t\tif isinstance(token, TokenType):\n\t\t\tif token.type in {TokenType.PLUS: '+', TokenType.MINUS}:\n\t\t\t\tseen.update((token,))\n\t\t\telif token.type == TokenType.MUL:\n\t\t\t\tgroups.add(token.get_group_name())\n\t\t\telse:\n\t\t\t\tseen.discard(token)\n\n\twhile len(seen)"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t# First, generate all the groups defined in the spec file.\n\tnames = set(self.spec.includes.keys())\n\n\t# Add the groups we've resolved to the existing group dictionary.\n\tparser_dict[\"include_groups\"] = names\n\n\t# This lets us extend the existing sections so they can accept more than\n\t# one options. Note, we also need a special section, which is defined in\n\t# the spec file, because we need to provide the include directives with\n\t# an explicit section name to the spec file.\n\t#\n\t# To do this, we"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tgroup = parser_dict['group']\n\n\t# Set up the directive\n\tparser_dict['include'] = include = parser.parseGroup(include)\n\n\t# The group directive is a group or a group that contains all of the groups that need to be included\n\tif isinstance(group, list):\n\t\tfor group_name in group:\n\t\t\tparser_dict[group_name] = _include_groups(parser_dict)\n\telif isinstance(group, str) and len(group) > 1:\n\t\tinclude.groups.append(group)\n\t\tparser_"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\treturn self._spec_arg, self._defaults\n\nclass _Module_base:\n\t\"\"\"\n\tThis class is a base for all modules.\n\t\"\"\"\n\tdef __init__(self, name):\n\t\tself._name = name\n\t\tself._module = import_module(name)\n\tdef __enter__(self):\n\t\treturn self\n\tdef __exit__(self, type__, value__, traceback):\n\t\tself.cleanup()\n\tdef cleanup(self):\n\t\t\"\"\"\n\t\tPerform module clean-up.\n\t\t@return:"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\treturn {}\n\n\ndef get_app_spec_params(app_spec_params_dict):\t\"\"\"\t\n\tGet values from app_spec_params dictionary and return dict of parameters.\t\n\t\n\tApp parameters are usually set with parameters of another app (such as app_spec.ext.MyApp)\t\n\tor with params of an app (such as App_name.my_function_app).\n\t\n\tFor example:  \n\tapp_spec_params_dict = {}\n\tapp_spec_params_dict[\"name\"] = \"My App Name\"\n"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\n\t# Initialize spec\n\tspec = self.get_spec()\n\t# Configure envars for each spec\n\tif spec.configure.conda_env:\n\t\tspec.conda_env.update(spec.conda_env.defaults)\n\tif spec.configure.env.replace_conda_env_override:\n\t\tfor env in self.spec.configure.env:\n\t\t\tif env.value == None:\n\t\t\t\tenv.value = spec.env[env.name]\n\n\tif spec.configure.conda_"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\treturn get_config_info(self.spec_root, self.spec, self.spec_file, self.spec_options, get_build(self.spec))\n\ndef setup_distribution(spec, **kwargs):\n    \"\"\"\n    Setup Distribution-specific arguments.\n    \"\"\"\n    options = None\n    try:\n        options = get_specs_from_spec(spec)\n    except:\n        # fallback to global options\n        options = kwargs.get('options', options)\n\n    if options is None:\n        # default option setting"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tself._spec.add_argument('--list_sub_modules', action='store_true')\n\tself._spec.add_argument('--list_modules_for_subclass', action='store_true')\n\tself._spec.add_argument('--show_subprojects', action='store_true', help='List all projects')\n\tself._kwargs_no_project = {**self._kwargs_no_project, **self._kwargs_project()}\n\t# add subproject\n\tself._spec.add_argument('--subproject', help='"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tfor m in self.manual_definitions.values():\n\t\tm_def = m.get('definition')\n\t\tif m_def is not None:\n\t\t\tmargin(m)\t# Update defaults\n\t\t\tmodulename = m_def['modulename']\n\t\t\tif modulename is not None:\n\t\t\t\tself.target_modules[modulename].add_spec(m)\n\n\t# Resolve definitions\n\tspec_defaults_to_modules()\n\t\n\t# Create the default target modules for the module\n\tself.target_modules"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\targs = self.get_arguments()\n\td = {}\n\tkey_value_pairs = {}\n\tfor argument in self.args:\n\t\tif argument.name in (\"-p\", \"--plot\", \"--probes\",\n\t\t                    \"--plot-file\", \"--save_file\", \"--plot-dir\"):\n\t\t\tif argument.name == \"--plot-dir\" and args[argument.name]:\n\t\t\t\td.setdefault(\"plot_dir\", args[argument.name])\n\t\t\tif argument.name == \"--save-path\":\n\t\t\t\td"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tspec_defaults = {}\t\t\n\t\t\n\t#\n\t# Spec-specific argument defaults\n\t#\n\tif getattr(self, '_spec_defaults_') is not None:\n\t\tspec_defaults = self._spec_defaults_\n\t\n\t#\n\t# Internal-use only\n\t#\n\tif spec_names := set(self._spec_names_):\n\t\t#\n\t\t# Internal-use only\n\t\t#\n\t\tspec_names = { k for k in spec_names if k in self.__dict__ }\n\t\t\n\t\t"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\treturn dict(\n\t\tinput_spec=self.input_spec,\n\t\tinput_spec_tag=self.input_spec_tag,\n\t\tinput_spec_args=self.input_spec_args,\n\t\tinputs=self.inputs,\n\t\toutputs=(),\n\t\toutput_spec=(),\n\t)"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tif not self.arguments:\t\t# empty list\n\t\treturn []\n\tfor arg in self.arguments:\t\t# get argument from source:\n\t\tif arg.is_string():\n\t\t\tself.argument = self.argument_from_string(arg)\n\t\telse:\n\t\t\tself.argument = self.argument_from_parameter(arg)\n\treturn self.argument\n\n\t\nclass ArgDef(Argument):\n\t\"\"\"\n\tContainer for a specific set of arguments\n\t\"\"\"\n\n\ttype_ = None\n\tgroup = None\n\tgroup_default_value ="}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tif self.get_parent_data():\n\t\toptions = self.get_parent_data()\n\t\tdep_args = {'keywords': options.get('kw', [])}\n\t\tif options.get('nogui', False):\n\t\t\tdep_args.update({'nogui': True})\n\t\tdep_args.update(options.get('no_naming', {}) or {})\n\t\tif self.get_name_string() == 'T':\n\t\t\tdep_args.update({'nounphrases': True})\n\t\telse:\n"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tprint(\"Getting deprecated args...\",)\n\treturn {\n\t\t\"input_files\" : self.args.input_files,\n\t\t\"pred\" : self.args.pred,\n\t\t\"task\" : self.args.task,\n\t\t\"output\" : self.args.output\n\t}\n\n# The default option parser, with one optional argument.\n# This file, also needs to be modified to generate correct argument types\n# for each argument given. (Checked)\nclass MainOptions(ArgParser):\n\t# Argument types.\n\t# These are, but are not required"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\timport inspect\n\tf = inspect.currentframe()\n\treturn {\n\t\t'market_filter': f.f_locals['self'].market_filter,\n\t\t'exchanges': f.f_locals['self'].exchanges\n\t}\n"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn (\t{\n\t\t\t\t'strategy': 'pipeline', \n\t\t\t\t'kwargs': {'strategy': 'pipeline'},\n\t\t\t\t'sort_by': 'datetime', \n\t\t\t   },\t{\n\t\t\t\t'strategy': 'timeframe',\n\t\t\t\t'kwargs': {'timeframe':'month',},\n\t\t\t\t'sort_by': 'datetime'}, {\n\t\t\t\t'strategy': 'date', 'kwargs': {'start_date': 'today', \n\t\t\t\t\t\t\t\t\t\t\t 'end_date': 'today',},'sort_"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdep_opt = {\n\t\t# general\n\t\t'debug': self.debug, \n\t\t'dataset': self.dataset, \n\t\t'n_fold': self.n_fold,\n\t\t'lr': self.lr}\n\t\n\t\t# general options\n\topts = []\n\n\tif self.__version__ >= '1.0' and self.__options__ is not None:\n\t\tfor key in self.__options__:\n\t\t\tif key in ['dataset', 'dataset_file', 'dataset_file_"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdeprecated_args = {}\n\tfor arg in self.valid_args:\n\t\tif self.options[arg].get('deprecated'):\n\t\t\tdeprecated_args[arg] = dict(required=True,\n\t\t\t                                     help=getattr(self, arg).get(\"help\", \"\"),\n\t\t\t                                     help_deprecated=getattr(self, arg).get(\"help_deprecated\", \"\"),\n\t\t\t                                     is_mandatory=getattr(self, arg).get(\"is_mandatory\", True),\n\t\t\t                                     is_required"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdeprecated_args = {}\n\tdeprecated_args[\"input_dir\"] = self.input_dir\n\tdeprecated_args[\"output_dir\"] = self.output_dir\n\tdeprecated_args[\"model_dir\"] = self.model_dir\n\tdeprecated_args[\"vocab_file\"] = self.vocab_file\n\tdeprecated_args[\"preprocess_file\"] = self.preprocess_file\n\tdeprecated_args[\"corpus_file\"] = self.corpus_file\n\tdeprecated_args[\"char_encoder\"]"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tprint(\"[DEPRECATED] Options:\")\n\tprint(\"   - {argument} ({options})\".format(\n\t\targument=self.option_name, options=list(self.option_args.keys())))\n\tprint(\"   - '{argument}={options}'\".format(\n\t\targument=self.argument_name, options=list(self.argument_args.keys())))\n\tprint(\"   - '{argument}=' {options}\".format(  # noqa\n\t\targument=self.argument_name, options=self.option_args.keys"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdeprecated_args = {}\n\n\tdeprecated_args[\n\t\t\"args\"\n\t] = [\n\t\t\"max_iter\",\n\t\t\"crossover_rate\"\n\t]\n\n\tdeprecated_args[\n\t\t\"params\"\n\t] = \"\\trho\"\n\n\treturn deprecated_args\n\n\nif __name__ == \"__main__\":\n\n\t# # get arguments from console\n\targs_dict = self.get_args()\n\n\t# print \"Arguments in console : \" + str(args_dict)\n\n\t# set random seed\n\tnp.random"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn {\n\t\t'_deprecated_args': {\n\t\t\t'short': ['model_name', 'data_name', 'optimizer_params'],\n\t\t\t'deprecated': ['model_name', 'data_name', 'optimizer_params'],\n\t\t\t'invalid_attributes': ['optimizer_params', 'params'],\n\t\t\t# Deprecated params.\n\t\t\t'params_str': ['params'],\n\t\t\t'warn_string_deprecated': [\n\t\t\t\t'optimizer_params [default]',\n\t\t\t\t'params [default]'],\n\t\t\t'd"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\targ_key = None\n\targ_type_reporter = None\n\n\t# if both key and value are present, then an error\n\t# has been printed.\n\ttry:\n\t\tfor key in cli_args:\n\t\t\tif key == 'data_source':\n\t\t\t\targ_key = 'data')\n\n\t\t\tif key == arg_key and str(cls.TYPE_STRING) in answer_file_args:\n\t\t\t\targ_type_reporter = answer_file_args[key][0]\n\t\t\t\tbreak\n\n\t\tif arg_key in answer_file_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\timport argparse\n\t\n\tfrom lib.cli_args import ArgumentParser\n\n\tfrom lib.cli_args import CliArgs\n\t\n\tfrom pathlib import Path\n\t\n\tfrom lib.cli_args import FileArgs\n\t\n\timport pyexceptions\n\t\n\tclass InvalidCmdOption(pyexceptions.Error):\n\t\n\t\tpass\n\t\n\tclass DuplicateCmdOption(pyexceptions.Error):\n\t\n\t\tpass\n\t\n\tclass ArgumentParserError(pyexceptions.Error):\n\t\n\t\tpass\n\n\tdef add_argument(arg):"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\n\targ_file_deprecation = list(itertools.chain([cli_args['answer'], cli_args['answer'].split('.')])).count('.')+1\n\t\n\tif arg_file_deprecation < 3 or len(answer_file_args) < 2:\n\t\tif arg_file_deprecation % 3!= 0:\n\t\t\tprint(f'[{arg_file_deprecation}] Argument File and Answer files are of wrong size')\n\t\t\tprint('')\t\n\t\t\treturn {'ans' : '"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tsys.stdout.write('\\r')\n\tif answer_file_args == None:\n\t\tanswer_filename = read_answer_file_name(cli_args['answer_filename'], cli_args['deprecations'][0], args.extraction_mode)\n\telse:\n\t\tanswer_filename = answer_file_args['answer_filename']\n\tfor dep in answer_file_args['deprecations']:\n\t\tfor arg in dep['args']:\t\n\t\t\tif  arg in cli_args['command_args']:\n\t\t\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\n\t\n\t#TODO: check that all fields are the correct type\n\t#TODO: check if it returns the correct value (str or int or list)\n\tif cli_args['config_file']:\n\t\t\n\t\twith open(cli_args['config_file'], mode='r+') as config_file_stream:\n\t\t\tconfig_body = json.load(config_file_stream)[answer_file_args['config_file_key']]\n\t\t\t\n\t\t\tif answer_file_args['config_file_key'] not in config"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tanswer : list[Any] = []\n\tif answers := answer_file_args.pop(\"deprecation_answers\"):\n\t\tfor question in answers:\n\t\t\tif question in answer_file_args:\n\t\t\t\tanswer.append(answer_file_args.pop(question))\n\tif not answer:\n\t\tanswer = list(cli_args.keys())\n\t\tif len(answer) == 1:\n\t\t\tanswer_name = answer[0]\n\t\t\tanswer = answer[0]\n\t\telse:\n\t\t\tanswer_name = \"--argument\"\n\t\t\traise Value"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tif 'arg_deprecation' in cli_args and cli_args['arg_deprecation']!= 'False':\n\t    print_deprecation_warning(cli_args['arg_deprecation'])\n\t    os.remove(answer_file_args['answer_file'] + '.xlsx')\n\t    return cli_args\n\telse:\t\t\n\t    if '--delete_output' in cli_args and (not cli_args['delete_output'] in ('delete_files','delete_output')):\n\t\t  "}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tif self.args.deprecation is True:\n\t\targs, cli_args = default_ans_args(answer_file_args)\n\t\tcli_args[\"--deprecation\"] = str_to_bool(\"--deprecation\")\n\n\t# Set a warning message to the question \"are you sure you want these?\",\n\t# so users can identify which options are not valid and which are not.\n\tif \".deprecation\" not in cli_args:\n\t\tcli_args.update({\"..warning::\", \"are you sure you want these?\"})\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tcli_file_args = cli_args\n\tcli_config_args = cli_file_args\n\tcli_file_args = cli_config_args\n\tcommand = cli_args[\"command\"]\n\n\tif cli_file_args[\"cmd_args\"][\"verbose_options\"]:\n\t\tcommand += \" -verbose\"\n\n\t# Remove args\n\t# Remove and Print the old args\n\tcli_arguments = cli_file_args[\"cmd_args\"][\"arguments\"]\n\tfor argument_key in cli_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tfor key, value in cli_args.items():\n\t\tprint('#' * 50)\n\t\tprint('[-] Argument %s: %s' %(key, value))\n\t\tprint('#' * 50)\n\n\tfor key, value in answer_file_args.items():\n\t\tprint('#' * 50)\n\t\tprint('[-] Argument %s: %s' %(key, value))\n\t\tprint('#' * 50)\t\t\t\n\ndef main():\n\t\"\"\"The main function: execute the program and interact with cli\"\"\"\n\t#"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t# First find the command\n\tcmd_name = self.args[0]      \t# The first argument\n\ttry:\n\t\tself._check_command(cmd_name)\t# We can assume this will be a command!\n\t\tcommand = self.commands[cmd_name]     # The command object\n\t\t# We'll check if the command has a different name\n\t\tassert(isinstance(command, str))\n\t\tif command.count(\"=\") == 1:\t# Only one element has an equals sign\n\t\t\t# This will check that it has a \"=\" character, otherwise"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\toption_specs = dict()\n\toptions = ['name', 'ipv6-enabled', 'address','state', 'user', 'group', 'password', 'vuln']\n\tif command_name not in [\"virsh\", \"ospd\"]:\n\t\traise Exception(\"Command not supported\")\n\n\tif command_name == \"virsh\":\n\t\tfor o_s in options:\n\t\t\toption_specs[o_s] = str\n\t\t\t#option_specs[o_s] = vypervisor.common.base_options.get(o_"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t# First, make sure the option that the list is meant to be ordered is\n\t# installed.  For example, if running the command virsh -v:\n\t#\n\t#   virsh --verb --opt-v [[keyword]] [argument]\n\t#\n\t# Then the option that is the first in the command will be: virsh\n\t# --verb --opt-v and then virsh --opt-v [[keyword]] [argument]\n\t#\n\t# In general, the list is in order of preference - for example we are\n\t# in virsh"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\toption_specs = []\n\tif command_name == \"get-password\":\n\t\toptions = [\"--output-format\", \"json\"]\n\telif command_name in (\"get-version\", \"get-summary\", \"get-braid-output\", \"get-braid-output-json\", \"set-output_options\", \"create_output_file\", \"create_output_binary\", \"create_output_json\", \"create_output_binary_json\", \"set-output_options\"]:\n\t\toutput = [command_name]\n\telif command_name"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tcmdopts_list =[]\n\n\tif command_name =='main':\n\t\t# The primary options, but NOT the sub-options (see also opthelp.py)\n\t\tcmdopts_list +=['--defaults']\n\t\tcmdopts_list +=['--vendor']\n\t\t#cmdopts_list +=['-c']\n\telif command_name == 'virsh':\n\t\tcmdopts_list +=['--defaults']\n\t\tcmdopts_list +=['--vm']\n\t\tcmdopts_list +=['-c']\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tcommand_options = set()\n\tif command_name =='show':\n\t\tcommand_options |= SUPPORTED_COMMANDS[:NSP_SHOULD_KEEP]\n\t\tcommand_options |= SUPPORTED_COMMANDS[NSP_SHOULD_KEEP:NSP_SHOULD_KEEP+NSSP_PARAM_PER_COMMAND]\n\tif command_name == 'create':\n\t\tcommand_options |= SUPPORTED_COMMANDS[NSP_SHOULD_KEEP:NSP_SHOULD_KEEP+NSSP"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t# Parse the command name argument and split it into it: it must be\n\t# the command name, which is a mandatory argument\n\topt_specs = dict()\n\t\n\tfor option_name in __commands__:\n\t\topt = self.option_dict.__getitem__(option_name)\n\t\t# ignore invalid options\n\t\tif opt.type == \"boolean\" and not opt.true_value:\n\t\t\tcontinue\n\t\tif opt.type == \"argument\":\n\t\t\tif ((not opt.short) or\n\t\t\t\t(not opt.long) or\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn getattr(\n\t\tself,\n\t\tcommand_name).get_parser_option_specs()\n\n\n\t\ndef run(argv, parse_opts=False):\n\t\"\"\"\n\tThe entry point of your program.\n\t\"\"\"\n\tparser = OptionParser(\n\t\tversion = rospy.__version__)\n\tparser.set_usage(\n\t\t\"%(prog)s [options] [command]\"\n\t\t\" [arguments]\")\n\t\n\tcommand_group = parser.add_option_group(\n\t\t'main', title='"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\toptions = []\n\tif command_name == \"main\":\n\t\toptions = get_main_opt(self)\n\telif command_name == \"virsh\":\n\t\toptions = get_virsh_opt(self)\n\telif command_name == \"ospd\" or command_name == \"ospd_off\":\n\t\toptions = get_ospd_opt(self)\n\telse:\n\t\tprint \"Error! Not implemented command {0}\".format(command_name)\n\t\tsys.exit(1)\n\treturn options\n\n\n\t# For future ref"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn [option for option in self.options if option.name.startswith('--' + str(command_name))]\n"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\toption_spec = super(OptionProvider, self).get_option_spec(command_name, argument_name)\n\tif not option_spec:\n\t\t# This is the first argument; we can't get the option specification for it.\n\t\treturn {}\n\t# Let me know what I was thinking for you.\n\t# print('option_spec' + str(option_spec))\n\treturn option_spec\n\n\t\ndef get_options(args):\n\t\"\"\"\n\tGets all the options available in the args list.\n\tIf an option has \"--help\" the"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\toption_spec = OptionSpec(\n\t\t\t\tself.command_name, command_name, argument_name, description=\"The '%s' option\" % (argument_name))\n\treturn option_spec\n\n\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n################################################################################################################"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\targ = getattr(self.config, 'options_to_namespace_{}'.format(command_name.upper()))[argument_name]\n\tspec = arg.get('specification' if arg.get('allow_optional', 'false') else 'type', None)\n\tif not spec or spec is None:\n\t\treturn\n\tif spec == 'boolean':\n\t\treturn\n\tif option_class := arg.get('option_class'):\n\t\tparam_class = option_class\n\t\tfor typ in arg.get('type'):\n\t\t\tname,"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\ts = dict()\n\tif command_name not in self.options:\n\t\treturn s\n\n\tif self.options[command_name].requires_arg in [True, False]:\n\t\targ_spec = '{} (True/False)'.format(self.options[command_name].argument_name)\n\n\telse:\n\t\targ_spec = '{}'.format(self.options[command_name].function.__name__)\n\n\tkw = dict(self.options[command_name].opts)\n\tkw.update(self.options[argument_"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tret = {'spec':'',\n           'help':'''Specify '--' option. Usage: --some_option'''}\n\n\tif argument_name in self.options_spec:\n\t\t# A specified option was found and therefore the command is valid\n\t\tret['spec'] = self.options_spec[argument_name]\n\n\treturn ret\n\n  \n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\ttry:\n\t\toptions_dict = self.subcommands[command_name].options\n\texcept AttributeError:\n\t\tif command_name in self.subcommands:\n\t\t\toptions_dict = self.subcommands[command_name].options_dict\n\t\telse:\n\t\t\toptions_dict = {}\n\targ_spec = {'type': 'dict','required': False, 'help': 'the option\\'s value for this {name} option.'}{self.__mapping_dict(options_dict)}\n\n\tif argument_name in self.subcommands"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\toptions_spec = {'type' : \"string\"}\n\toptions = self.options.options(command_name)\n\topt_obj = options[0]\n\tif(opt_obj.get(argument_name) and opt_obj.get(argument_name).upper()=='TYPE'):\n\t\topt_obj['required'] = True\n\t\toptions_spec['type'] = \"enum\"\t\t\n\t\n\tif(opt_obj.get(argument_name) and opt_obj.get(argument_name).upper()=='DEST':\n\t\topt"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif command_name in ('help','manage'):\treturn {}\t\t# TODO\n\n\tfrom..core.models.option import Option, OptionName\n\tmodel = self.get_option_type(command_name)\n\tif model is model and model.option_types and argument_name in model.option_types:\n\t\treturn {argument_name : Option(models=[model])}\n\n\traise ValueError('Option %s is not valid for: %s. Aborting.' % (argument_name, command_name))\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\n\t\tself.option_specs = {\n\t\t\t\"name\" : { \"type\" : \"string\", \"help\"'': 'Name of the option' },\n\t\t\t\"description\" : { \"type\" : \"string\", \"default\" : '' },\n\t\t\t\"value\" : { \"type\" : \"string\", \"default\" : '' },\n\t\t\t\"aliases\" : { \"type\" : \"string\", \"default\" : [] },\n\t\t}\n\t\treturn self.option_specs\n\n\tdef apply_default(self, option):\n\t\t\"\"\"\n\t\tDetermin"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tfrom getopt import getopt\n\n\tspec = {}\n\tfor optname in getopt(self.commands_options_spec,\n\t                       \"s:\", [\"help\", \"version\", \"-v\", \"-d\", \"-h\"]):\n\t\tspec[optname[0]] = optname[1]\n\tif command_name not in self.commands_options_spec:\n\t\traise CommandNameError(\"No such option name {}\".format(command_name))\n\n\treturn spec\n\nclass SubOptions(object):\n\t\"\"\"\n\tRepresents the options for an interactive sub"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilent_args = []\n\tfor name in args:\n\t\tif args[name] is False:\n\t\t\tsilent_args.append(name)\n\t\telse:\n\t\t\tsilent_args.append('-silent'+ name)\n\tsilent_args = \"_\".join(silent_args)\n\treturn silent_args\n\n\ndef get_option_parser():\n\tparser = argparse.ArgumentParser(description=\"Arguments used by the PycLN\")\n\tparser.add_argument(\"src\")\n\tparser.add_argument(\"mode\")"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilenced_arguments = []\n\tfor arg in args:\n\t\tif arg.silenced =='silenced':\n\t\t\tsilenced_arguments.append(arg)\n\treturn silenced_arguments\n\nclass T5(torch.nn.Module):\n\tdef __init__(self):\n\t\tsuper(T5, self).__init__()\n\t\tself.num_layers = 2\n\t\t#self.embed = nn.Embedding(5, 16)\n\t\tself.rnn = nn.LSTM(16, 4) # L"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t# list = list(args.keys())\n\t# for i in range(len(args)):\n\t\t# args[i] = list[i]\n\tsilenced = []\t\t\t\t\t\t\t\t# a list of arguments with the corresponding name silenced\n\tfor i in range(len(args)):\t\t\t# get the arguments without the silencer\n\t\tif args[i].get_silent():\t# if the argument (`silenced[i]`) does have a  silencer...\n\t\t\tsilenced.append(i)  # add it to the new `silenced` list\n"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn['silenced','skip']\t\n\n\n\n\n\n\n\n\n\n\nclass Parser(object):\t\n\t\"\"\" Parser object contains functions for parsing the argumnents of a bash file\"\"\"\n\tdef __init__(self, filename, returnfile=False):\n\t\t\"\"\"\n\t\t:argument: filename\t\t\t\n\t\t:returns:\t\t\t\t(object, flag to skip arguments)\t\t\t\n\t\t\"\"\"\n\t\tself.args = Arguments(filename)\t\t\t\n\t\tself.skip = False\n\t\tself.skip_flag = False\t\n\t\tself._returnfile = returnfile\n\n\tdef parse"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\targs = [*args, ]\t\t\t\t\t# the current_args and current_kwargs are modified\n\tkwargs = {}\t\t\t\t\t\t\t\t# and kwargs are set as None\n\targs[0] = _as_none_if_present(args[0])\t# the current_args and current_kwargs are modified\n\targs[1:] = [(_as_none_if_present(a) if _as_none_if_present(k) is not None else k) for a, k in self._arguments]\n\n\treturn args\n\n\ndef _parse_args(args"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn self.__silenced_args\n\n\tdef get_hidden_args(self, args):\n\t\t\"\"\"\n\t\tlist of hidden argument names\n\n\t\t:param args: the received arguments.\n\t\t:return: list, the hidden arguments, ordered by module (only for `pyramid` module)\n\t\t\"\"\"\n\t\treturn self.__hidden_args\n\n\t@property\n\tdef silent_args(self):\n\t\treturn self.quiet_args\n\nclass CallbackInfo(object):\n\t\"\"\"\n\tcallback\n\t\"\"\"\n\tdef __init__(self, obj, level):"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\n\tnames=[]\n\tfor i in range(len(args)):\n\t\tif re.match(r\"^@silentx_\",args[i].lstrip()):\n\t\t\tnames.append(args[i].replace(args[i].lstrip(),\"\").lstrip())\n\t\tif re.match(r\"^@silent$\",args[i].lstrip()):\n\t\t\tnames.append(args[i].replace(args[i].lstrip(),\"\").lstrip())\n\t\tif len(names)==0:\n\t\t\tnames.append("}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilence = []\n\twhile len(args) > 0:\n\t\targ = args.pop(0)\n\t\tif arg not in [\"silenced\", \"quiet\", \"silent\"]:\n\t\t\tsilence.append('`%s`' %arg)\n\treturn silence\n\ndef quiet(value): return '-q'\ndef silent(value): return '-s'\ndef silenced(value): return '--silenced'\ndef quiet_silencer(args): return ':silenced: '.join(get_silent_args(args))\ndef quiet_sil"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilenced_args = self.arg_silencer(args)\n\treturn silenced_args\n\n\ndef main(args=sys.argv[1:]):\n\t\"\"\"\n\tMain executable.\n\t\"\"\"\n\targs = self.get_silent_args(args)\n\targs = self.normalize(args)\t\t\n\tprint_args(args)\n\t\n\t\n\t# Create a test project\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"config_path\", \n\t\t\t\t\t\tnargs=\"?\",\n\t\t\t\t\t\thelp=\"path to"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilenced_args = [x for x in args.keys() if args['silenced'][x]['enabled'] and args['silenced'][x]['value'] is not None and'silenced' not in x]\n\treturn silenced_args\n\n\ndef get_user_args(args):\n\t\"\"\"\n\tList of user provided arguments\n    - user_arg_index: index of user provided argument\n    - user_arg: name of user provided argument\n    - arg_value: user provided argument value\n    returns:\n        args = [user_arg_"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\treturn ( len( self.args ) - len( *args ) ) <= 0\n\n\n\t\nclass TestParser:\n\t\"\"\"\n\tClass which instantiates Parser object by using ArgumentParser.\t\n\tAcesses the arguments directly from ArgumentParser.\t\n\t\"\"\"\n\tdef __init__(self, *args):\n\t\t\"\"\"\n\t\tInitializes the parser and creates its arguments.\t\n\t\t\"\"\"\n\t\tself.parser = ArgumentParser(*args)\n\t\tself.args = list(args)\n\n\n\tdef parse_arguments(self):\n\t\t\"\"\"\n\t\tReturns a"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tcheck = True\n\tif (self.url == None):\n\t\treturn check, \"The method requires a url\"\n\telif (args == '-h' or args == '--help'):\n\t\tcheck = False \n\t\tif check:\n\t\t\tprint \"Usage:\"\n\t\t\tprint \"-h: help\"\n\t\t\tprint \"-p: port number\"\n\t\t\tprint \"-u: username\"\n\t\t\tprint \"-d: password\"\n\t\treturn check, \"A website should have a URL, password and a port number\"\t\t\t\t\n\telse: \n\t\treturn check, \"URL, password"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif not args:\n\t\t# If none of the arguments have been provided\n\t\t# \tprint the error message\n\t\tprint(\"Please provide Required Arguments\")\n\t\tsys.exit()\n\t#print(\"All arguments provided. Continuing...\")\n\n\n\n#----------------------------------------------------------------------------------------------------------------------#\n\n'''\n# Define the function that performs the job. \n# These two functions are the main logic of a job. To be implemented for each algorithm.\n\n\ndef job_function(n_components, n_init, convergence_rate, max_runs, init, initial, \n\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tvalid = (\n\t\t\"password\",\n\t\t\"passwordFile\",\n\t\t\"username\",\n\t\t\"passwordFile2\",\n\t\t\"email\",\n\t\t\"userPass\",\n\t\t\"submit\",\n\t\t\"action\",\n\t)\n\terrors = {}\n\tif not all(key in args for key in valid):\n\t\terrors[\"password\"] = [f\"Missing required arguments: [{valid}]\"]\n\treturn errors\n\n\ndef password_check(account, file):\n\t\"\"\"\n\tUse hash password to check password.\n\t\"\"\"\n\taccount[\"password\"][0]"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif not (bool(args.get('data_store')) or bool(args.get('model')) or bool(args.get('input'))\n\t\t\tor bool(args.get('output'))):\n\t\treturn 'InputArgs.py must have at least one required argument.'\n\n\tif bool(args.get('model')) and bool(args.get('input')):\n\t\treturn 'Cannot provide both model and input.'\n\n\tif bool(args.get('model')) and bool(args.get('data_store')):\n\t\treturn 'Cannot specify"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor required_arg in self._required_args:\n\t\tif required_arg not in args:\n\t\t\traise UserError('Missing required argument %s.' % required_arg)\n\n\t# check user has entered correct number of required arguments\n\tif len(args)!= len(self._required_args):\n\t\traise UserError(\"Invalid number of required arguments.\")\n\n\t# Check user has entered all values correctly\n\tfor required_arg in self._required_args:\n\t\tif not args[required_arg]:\n\t\t\traise UserError(\"This feature requires at least one value for %s.\""}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif len(args) > len(self.required_args):\n\t\tself.error(\"There are more required argument than this simulator requires_args. \" +\n\t\t           \"Please specify correct number of parameters. \\nError code: 0x000100000501\")\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tself.error(\"Unknown argument: %s. \\\n                                Please specify correct argument. \\nError code: 0x000100000501\" %\n\t\t\t\t\t\t  arg)\n\n\nclass SimulatorErrorHandler(Exception):\n\t\"\"\"\n\tBase class"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif (len(args) < 2):\n\t\tprint(\"[ERROR] Number of required parameters must be greater than 2!\")\n\t\tprint(\"[ERROR] You need to provide a valid source file and a valid target.\")\n\t\treturn False\n\n\treturn True\n\nclass FetchCommand(Command):\n\t\"\"\"\n\tThis class is used when the user wants to fetch data from the source.\t\"\"\"\n\tdef __init__(self):\n\t\tself.usage = \"command<[source]> <[targets]\"\n\t\tself.examples = self._examples\n\n\tdef"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tnodes = args[0]\n\terrors = []\n\tfor node_id in nodes:\n\t\tif node_id not in self.nodes:\n\t\t\terrors.append(\"[Node id {}]  Node id not found.\".format(node_id))\n\t\t\treturn []\n\t\tif len(args) == 2:\n\t\t\tif not isinstance(args[1], dict):\n\t\t\t\terrors.append(\"[Node id {}]  Validator requires arguments: type, value.\".format(node_id))\n\t\t\t\treturn [errors]\n\t\t\tif not check_node_type"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor arg in ['username', 'image', 'password', 'email', 'firstname', 'lastname']:\n\t\tif arg not in args:\n\t\t\traise AttributeError(\"Mandatory argument must be provided\")\n\n\tif 'firstname' not in args or 'lastname' not in args:\n\t\traise AttributeError(\"Mandatory argument 'firstname' must be provided\")\n\t\n\tif args['firstname'] == \"\" or args['lastname'] == \"\":\n\t\traise AttributeError(\"'password' or 'email' not provided\")\n"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t)\n\toptions_spec = ['--' + name for name in options_spec]\n\tif command_name =='setup':\n\t\toptions_spec += ('pyargs',)\n\tfor name in args:\n\t\tif name == '--without-build':\n\t\t\toptions_spec += ('build',)\n\t\telif name in {'--with-python', '-m', '-M'}:\n\t\t\t# Allow ``getattr()`` to be used for options that\n\t\t\t# are not set, for example as build or parallelization\n\t\t\toptions_spec += (name,)"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t).filter_by_name\n\t)\n\ncommand_spec = (lambda: Command(command = Command.CommandInfo(\n\tname = \"\",\n\tdescription = lambda: None,\n\toptions = options,\n\targspec = argspec,\n\tepilog = get_version_command_info_epilog,\n\taliases = {},\n\thelp = HelpInfo().command_info,\n\toptions = [\n\t\topt(\n\t\tname = \"args\",\n\t\thelp = HelpInfo().option_info.format_help(prefix=\"        \" + \""}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t).for_each(lambda arg: (f\"expected args for {command_name}\", arg))\n\t\n\tmissing_args = {c.name for c in command_spec.commands if not c.required_when}\n\tif missing_args:\n\t\tyield \"missing_args\", missing_args\n\treturn\n\n\n\t# For use in `_get_options_spec`, can be used twice.\ndef _get_cmd_spec(args):\n\t'''Get command definition from `args`'''\n\t# For an example script in `utils."}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\trequired_when = ['--', '--', '--', '--'] if command_name == 'list' else ['--', '--', '--']\n\treturn super(_SpecEx, self).check_required_when(command_name, options_spec, required_when, args)\n\n\nclass _SpecEx(ConfigArgument):\n\t\"\"\" A base parser for specs.\n\t:param str name: the name of the parameter.\n\t:param str help: the help text of the parameter.\n\t:param type str: the required type.\n\t:param boolean required"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t)\n\n\tcommands, argv = self.app.read_configuration()\n\n\ttry:\n\t\tcommands.update(self.app.parser.command_map[command_name])\n\texcept KeyError:\n\t\tself.app.parser.error(\"{} command not found in the main CLI file\".format(command_name))\n\n\tif not commands.get(command_name, True):\n\t\tcommands.pop(command_name, None)\n\n\treturn argv, commands\n\n\t\"\"\"\n\tThe list of parsed, ``argv`` will have at least"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\targs.insert(0, self.args.pop(1))\n\treturn args\n\n\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n\tdef list_args(args, options_spec):\n\t\t\"\"\"\n\t\tPrivate method to create input input with required arguents from a list.\n\t\n\t\t:param args: list of parsed arguents for a command line\n\t\t:param options_spec:  list of command spec options.\n\t\n\t\t:return: None\"\"\"\n\t\trequired_args = []\n\t\tfor arg in args:\n\t\t\tif arg.required_when:\n\t\t\t\tfor arg_spec in options_spec:\n\t\t\t\t\tif arg_spec.name == arg.required[0]:\n\t\t\t\t\t\trequired_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tresult = []\n\tif not isinstance(args, list):\n\t\targs = [args]\n\trequired_args = []\n\tif options_spec and (isinstance(options_spec, string_types) or isinstance(options_spec, boolean_types)):\n\t\ttry:\n\t\t\trequired_when = [arg.lower() for arg in options_spec.lower().split(',') if arg.lower() in dir(argals[command_name])]\n\t\t\targs = [arg.lower() for arg in options_spec.lower().split(',') if arg"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t# we will use an internal list here to avoid unnecessary lookup\n\tcmd_args = []\n\t# we need to append arguments with the default ``required_when`` condition\n\tif self.req_when:\n\t    cmd_args.extend(['-%s' % (self.default_what == x or x.lower().replace('_','-') ) for x in self.req_when])\n\t# we need to append arguments with ``required_when`` with the ``do_not_pass=True``\n\t# condition to work the same way as default to avoid"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t)\n\n\n\t# optional parameters of the command:\n\n\t# used when there is more than one option\n\toption_match = [option['match'] for option in options_spec if option['match']\n\t\t\t]\n\toptions_match, arguments_list, options_list = self._get_options_with_matched_args(\n\t\t\t\tcommand_name = command_name,\n\t\t\t\toption_match = option_match,\n\t\t\t\targs = args,\n\t)\n\n\tif len(options_spec) < len(arguments_list):\n\t\treturn True, arguments_list\n"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) > MAX_USER_ARGS_LENGTH:\n\t\terror = \"You must specify the value of the argument after \" \\\n\t\t\t\t+ \"{} to be {}. Try help()\".format(MAX_USER_ARGS_LENGTH,\n\t\t\t\t\t\t\t\t\t\t\t\tMAX_USER_ARGS_LENGTH)\n\t\tself.error(error)\n\n\n# Define a few standard callbacks...\n\n#def on_exit():\n#\tpass\n\ndef on_save():\n\t\"\"\"Save some data to a file.\"\"\"\n#\tdata = serialize_"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tlength_args = [\"length\"]\n\n\tlength = args[\"length\"]\n\n\tlength_arg_not_in_args = True\n\n\tfor arg in length_args:\n\t\tif arg not in args:\n\t\t\tlength_arg_not_in_args = False\n\n\tif length_arg_not_in_args:\n\t\tprint(\"Invalid length argument\")\n\t\treturn False\n\telse:\n\t\treturn True\n\n\nclass Sysdbs:\n\tdef __init__(self, config={}, conn_params=None):\n\t\t\"\"\"\n\t\tInitializes the Sysdb"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif type(args)!= tuple or (len(args) > self.length):\n\t\tself.throw_error_message()\n\t\treturn -1\n\treturn 0\n\n\t\n##############################################################################\n#\n#\tString_operations functions\n#\n##############################################################################\n# The following function is made to validate the argument type.\n# For example the following string with a\n# length of 15 has an odd string length, even one.\n#\n# Check:\n#\t\tString_operations::validate_string_length(self, args)\n#\ndef"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tminlen = len(args.get('data',' ')) + 3\n\tmaxlen = len(args.get('title','')) + 3 \n\tif len(args.get('data','')) > maxlen or len(args.get('title','')) > maxlen:\n\t\tself.add_error(args.get('title',''), 'Length of argument \\'data\\' is more than specified or less than max_length.')\n\ndef validate_language_args(self, args):\n\t\"\"\"\n\tCheck if value of arguments is valid language.\n\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) >= args.length:\n\t\treturn 1\n\treturn 0\n\n\nclass User:\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t:param users: The list of users.\t1 element is a new line of user with `key` set to its corresponding \n\t\t\t\t\tid (default is `id_generator()`).\t\n\n:type users: list\t1 element.\n\t\t\"\"\"\n\t\tself.key = id_generator().__str__()\n\t\tself.users = list()\n\n\tdef create_user(self, user):\n"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) > len(self.length_list):\n\t\tself.length_list = args\n\t\treturn True\n\telse:\n\t\t#if length given isn't available, return error message\n\t\tprint_msg(self.length_msg, 'length_msg')\n\t\treturn False\t\t\n\n\t\n#\n# @pytest.fixture(scope=\"class\")\n# def set_path():\n# \t\"\"\"\n#\n# :returns: A string containing the full path of the directory with all the user's files.\n#\n# \t\"\"\"\n#"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif type(args)!= list: arguments = str(args).split(',')\n\tfor argument in arguments:\n\t\tif len(argument) > len(self._length):\n\t\t\t\tnewArguments = argument.split(',')\n\t\t\t\tvalid_arguments = []\n\t\t\t\tfor newArgument in newArguments:\n\t\t\t\t\t\tif newArgument not in arguments:\n\t\t\t\t\t\t\t\t# If argument not found in argument list\n\t\t\t\t\t\t\t\tnewArgValue = argument.replace(argument,'').lower()\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tnewArgValue = newArgument.lower()\n\t\t\t\t\t\tvalid_arguments."}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tlength = args['length'].strip()\n\tif len(length.rstrip()) > self.max_len:\n\t\treturn length\t# Invalid length\n\treturn length\n\n\n    def check_for_invalid_symbol(self, args):\n\t\"\"\"\n\tCheck for invalid symbol in arguments\n\t:param args: The received arguments.\t\n\n\t:return: The error message if any\n\t:rtype: string\n\t\"\"\"\n\terror = False\n\tfor s in self.valid_symbol:\n\t\tif s in args['arguments']:\n\t\t\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tlength = self['length'].val\n\tif args[-1] < length:\n\t\tprint(\"[ERROR][ERROR]-[ERROR] Invalid param \" \\\n\t\t\t\"(Length of argument) {}. Expecting value {}. \" \\\n\t\t\t\"Try using a larger value? {}\" \\\n\t\t\t.format(args[-1], length, args[-1]))\n\t\texit()\n\n\n#\n# Function to create random number\n# ------------------------------------------------------------\n#\ndef rng(range_size, type='even'):\n\t\"\"\"\n\tFunction to create random numbers of specified"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\ttry:\n\t\targ_lengths = [len(a) for a in args]\n\texcept TypeError:\n\t\targ_lengths = list(map(str, args))\n\t\targ_lengths = list(map(*[len(a) for a in args]))\n\t\targ_lengths = [arg_lengths[0]] * (max(len(a) for a in args) + 1)\n\ndef get_list(arg, args, length, n, max_length, min, delim):\n\tvalue = arg if type(arg) is list"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tfrom.utils import convert_to_list\n\n\tif isinstance(args.choices, int):\n\t\t# First try to convert the argument to a list.\t\n\t\t# This lets us use it if the user give the option as a string,\n\t\t# or if we receive it via the list.\t\n\t\tif not isinstance(args.choices, list):\n\t\t\tchoice = args.choices\n\t\t\targs.choices = convert_to_list(choice)\n\n\t\t# If user types int, make sure their value is in the list.\n\t\tfor i, _ in"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.get('choices') and args.get('choice') == 1:\n\t\targs.update(self.model.choices)\n\treturn args\n\n\t\ndef validate_default(self, d):\n\t\"\"\"\n\tVerify the provided default value matches the model's default value.\n\n\t:param self: The :class:`.Resource` that is being modified.\t\n\t:param d: The provided default value (usually the model's default).\t\n\t:return: The validated default (if valid).\t\t\n\t:rtype: str\n\t\"\"\"\n\tif"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) == 1 and args[0] not in self.all_choices:\n\t\traise self.InvalidChoices(args, 'No choice has been specified.')\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\terror = None\n\toptions = args.choices\n\tif len(options) == 1 and isinstance(options[0], dict):\n\t\tfor key, value in options[0].items():\n\t\t\tif key in self.options_dict:\n\t\t\t\tprint (\"ERROR : \", key, \" is not a valid choice value.\")\n\t\t\t\terror = True\n\t\t\ttry:\n\t\t\t\tfloat(value)\n\t\t\t\tprint (\"validation done for %s = %s\" %(key, value))\n\t\t\texcept (TypeError, ValueError):\n\t\t\t\tprint (\"error for values: \", value)\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif self.choices.get(args[0].lower()) is None or 'Invalid Choice' in self.choices[args[0].lower()]:\n\t\tself.choices[args[0].lower()] = self.choices.get(args[0].lower()) + [args[1]]\t\n\n\tif len(self.choices[args[0].lower()]) > 1:\n\t\tif len(args) > 1:\n\t\t\targs[1].lower()\n\t\t\t\nclass MainMenu(Toplevel):\n\tdef __init__(self, application"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tch_arguments_args = ['parsed-choice', 'choices'] # only for \"parsed-choice\" argument\t\n\n\tif not any (arg in args for arg in ch_arguments_args):\n\t\t# arg not found, but not in choices\n\t\t# so throw error and continue\n\t\traise self.Error(\"Argument \" + arg + \" does not exist\")\n\n\t# arg in args is in choices argument\n\t# so return value\n\n\n\treturn args['choices']\n\n\n# ---------------------------------------------------------\nclass ArgumentsParser(object):\n\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif not args or args not in self.choice_list:\n\t\terrormessage = \"Incorrect choice arguments.\"\n\t\traise ParseError(errormessage)\n\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif False in [isinstance(item, str) for item in args]:\n\t\tself.validate_choice_args(args)\n\t\treturn\n\targs = [item.lower() for item in args]\n\tchoice_obj = self[args[0]]\n\tif choice_obj:\n\t\t# Choice for that argument already exists, if it already contains a \n\t\t# choice object, validate it, else create one with the choice_arg \n\t\t#\n\t\t# If it does not exist, then update the list of choices\n\t\t#\n\t\tif isinstance(choice_"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tchoice_index = args[0] if len(args) > 0 else None\n\tif choice_index is not None:\n\t\tif not(choice_index in range(len(VIRTUAL_CHOICES))):\n\t\t\terror_text = ERROR_TEXT[\"choices_error\"]\n\t\t\traise ValueError(error_text)\n\t\telse:\n\t\t\targs[0] = choice_index\n\n\nclass StringSelect(Selector):\n\t\"\"\"\tA class used to select an input item dynamically with\n\tinput components.\n\t:param args: The received arguments.\t\n\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tself._checkValue(\"choices\", args)\n\tif len(args[\"choices\"])!= 1:\n\t\traise Exception(\"Value of Choice 'choices' does not specify one.\")\n\n\treturn args[\"choices\"][0]\n\n\ndef _print_args_error(*args):\t\"\"\"\nFunction that reports an error message as a parameter.\n\n\n:param args: the arguments to be printed.\t\n\"\"\"\n\tif args[0] == '{' or args[0] == '__':\n\t\targs = args[1:]\n\n\tif \"--help\" in"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.bbox!= None:\n\t\tif args.bbox > args.max_width or args.bbox < args.min_width:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\ndef calc_mask_statistics(gt_pix_map, gt_mask, args):\n\t\"\"\"\n\t\tCalcualte the stats about the groundtruth mask and the prediction mask.\n\t\tThis is just to create statistics about the images in the database.\n\t\t:param gt_pix_map : Ground truth image"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\n\tval = {\t\n\t\t\t\t\n\t\t\t'XMin' : 0,\n\t\t\t'YMin' : 0,\n\t\t\t'XMax' : 0,\n\t\t\t'YMax' : 0,\n\t\t\t}\t\n\n\tif self.debug:\n\t\tprint(\"[ERROR]: Invalid arguments\")\t\n\n\tfor key, val in args.items():\t\n\t\tif self.debug:\t\t\n\t\t\tprint(\"[WARNING]: Invalid arguments\")\t\n\n\t\telif val < 0.0: \t\n\t\t\tval *= -1\n\n\t\ttry:\t\n\t\t\tval = int(val"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.min <= args.max:\n\t\targs.min = int(args.min)\n\t\tif args.min > args.max:\n\t\t\tprint('Invalid range: minimum value {0} must be less than maximum value {1}'.format(args.min, args.max))\n\n\t\tif args.min == args.max:\n\t\t\treturn args.min\n\n\t\treturn args\n\n\n\telse:\n\t\tprint('Invalid range: {0} must be less than or equal to {1}'.format(args.min, args.max))\n\n\n\n@click.command"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.min is not None:\n\t\tself.min = args.min\n\tif args.max is not None:\n\t\tself.max = args.max\n\n\n\tif ((args.min == 0) and (args.max == 0)):\n\t\traise Exception('Missing Required Arguments Error')"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args < MIN_VALUE or args > MAX_VALUE:\n\t\traise Exception(f'Maximum value {MAX_VALUE} is exclusive. Consider a minimum value of {MIN_VALUE}')\n\t\treturn None\n\n\t# set the minimum and maximum\n\tself._min = args[0]\n\tself._max = args[1]\n\n\t# check if the argument is negative\n\tif (-1 * args[0]) <= self._min:\n\t\traise Exception(f\"Minimum value ({self._min}) is less than {args[0]}\")\n\t\treturn None\n\n\tif (-"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tmax_arg = True\n\tif len(args) < 1:\n\t\treturn f\"ERROR: Minimum value must not be null\", 400\n\tif len(args) > 1:\n\t\treturn f\"ERROR: Maximum value must not be null\", 400\n\tif max_arg and any([arg is not None for arg in args]):\n\t\treturn f\"ERROR: Maximum value must not be null\", 400\n\tif min_arg and (not self.min or any([arg is None for arg in args])):\n\t\treturn f\"ERROR: Minimum value must not be null\", 400"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) < 1:\n\t\t\traise Exception(\"no arguments were given\")\n\tif len(args) > 3:\n\t\t\traise Exception(\"too many arguments were given\")\t\n\n\tval = args[0]\n\top = args[1]\n\n\tif op not in \"Egtb\":\n\t\traise Exception(\"op must be specified\")\n\n\n\tminimum = args[2]\n\tmaximum = args[3]\t\n\n\tif minimum > maximum:\n\t\t\t\t\n\t\traise Exception(\"minimum max value should be greater than maximum\")\n\t\t\n\tmaximum = maximum\n\tminimum"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif (self._val_max > self._val_min): \n\t\tif (args.get(\"min_value\")):\n\t\t\tif (args.get(\"min_value\") < self._val_max):\n\t\t\t\treturn None\n\t\tif (args.get(\"max_value\")):\n\t\t\tif (args.get(\"max_value\") < self._val_max):\n\t\t\t\treturn None\n\treturn self.data.__setitem__(:,0)\n\t\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\n\t\"\"\"\n\tChecks if the value provided is a int, float or negative integer if it is float.\n\t\"\"\"\n\ttry:\n\t\tself.v = int(args.get('v'))\t\t\t\t\t\t\n\t\tif self.v < 0:\tresult = True\n\texcept ValueError:\tresult = True\n\telse:\t\tresult = False # If it is a value of int then return true because if it is an negative value then False\n    \n\ttry:\n\t\tself.m  = float(args.get('m'))\t\t\n\texcept ValueError:\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args is None:\n\t\treturn\n\ttry:\n\t\tif args[\"min\"] is not None and args[\"max\"] is not None:\n\t\t\tif max(args[\"min\"], args[\"max\"])!= args[\"max\"]:\n\t\t\t\traise Exception(\"The max value of range cannot be greater than the min value or both are equal.\")\n\texcept Exception:\n\t\traise Exception(\"A value of arguments is required as it refers to variables.\")\n\nclass _Checkpoint_ExpirationManager(object):\n\n\tdef __init__(self, checkpoint_checkpoint,\n\t             save_inter"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tinstance = None) -> ComplexType:\n\t\t\n\t\t# first build the full argument name\n\t\ttype_name2 = \"\"\n\t\ttype_name2 = \"type\" + type_name.replace(\", \",\",\")\n\t\tif subcommand == \"getcompletetype\":\n\t\t\ttype_name = \"get\" + type_name.replace(\".\", \"\")\n\t\telif subcommand == \"addtype\":\n\t\t\ttype_name = \"add\" + type_name.replace(\".\", \"\")\n\t\telif subcommand == \"modifytype\":\n\t\t\ttype_name"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t):\n\t\t\n\t\targ_data = []\n\t\tfor arg in self.__special_arguments:\n\t\t\targ_data.append(arg.get_data(subcommand, type_name, option_name, spec_option))\n\t\t\n\t\tcomplex_type = ComplexType(self.__type_name, self.__complex_type, type, subcommand,\t__complex_type_def, argument_type=\"complex\")\n\t\tcomplex_type.set_data_type(complex_type.type[self.__type_name])\n\t\t\n\t\tcomplex_type"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t):\n\t\tif subcommand == 'get' and type_name == 'Boolean':\n\t\t\treturn type(str, (bool,), {'value': bool})\n\t\tif subcommand == 'get' and option_name == 'String':\n\t\t\treturn type(str, (str,), {'value': str})\n\t\tif subcommand!= 'get' or not type(type_name) == type(str):\n\t\t\treturn f\"Invalid complex type '{type_name}' specified in 'create_complex_argumet_type'.\"\n\t\t# for boolean values\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t):\n\t\tdct = dict()\n\n\t\targ_type = get_spec(subcommand, type_name)\n\t\toption_type = get_spec(subcommand, option_name)\n\t\targ_type_type = type_name[:2]\n\t\toption_type_type = option_name[:2]\n\n\t\tself.complex_argumet_types.append(arg_type_type + arg_type)\n\t\tself.complex_args[arg_type_type.strip('_') +  option_type_type + 'ArgTypeType'] ="}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tpass)\ndef create_complex_argumet_type_instance(self, subcommand, type_name, option_name, spec_option, **options):\"\"\"\n   Build new complex argument type.\n\nIt builds the complex type instance, such as `string`, and\nmodifies it according to **options.**.\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:param options: options to use for type creation (all are optional)"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tret = ComplexType()\n\n\tif type_name in self.complex_types_dict:\n\t\tret.name = type_name\n\telse:\n\t\traise ValueError('The complex type %s not found' % type_name)\n\n\t#print 'Add argument'+ spec_option\n\n\treturn ret\n\n\n@profile\ndef create_builtin_types(subcommand, type_name, arg_name, type_name=None,\n\t\"\"\"\n\tBuild the argument type with the command name and arg_name.\n\tThe argument type could be the name of a parameter"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\n\ttype_name = type_name.replace(\"_complex\", \"\").upper()\n\n\tclass ComplexType(BaseType):\n\t\tname = \"complex_type\"\n\n\t\tdef __init__(self):\n\t\t\tself.specification =  { type_name: self.build.list }\n\n\treturn ComplexType\n\nclass CommandType(BaseType):\n\tname =\t\"command_type\"\n\n\tdef __init__(self):\n\t\tself.specification =  { self.name: self.build.list }\n\n\tdef add_command_type(self,"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t):\n\t\toptions = self.options if isinstance(self, (Command, Options)) else self._meta_options\n\t\tsubcommand = self.get_subcommand(subcommand=subcommand)\n\t\ttype_defs = self.get_declarative_types(subcommand=subcommand, only_root=True)\n\t\ttype_name = type_name if type_name.lstrip().startswith('' + subcommand.name) else f'{type_name}{subcommand.name}'\n\t\tfor arguetype in itertools.chain("}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t):\n\t\tcomplex_type_name = '{0}{1}{2}'.format(type_name, '/', subcommand)\n\t\tcomplex_spec_name =  '{0}{1}{2}'.format(complex_type_name, option_name, '/')\n\t\tcomplex_type = ComplexType()\n\t\tcomplex_type.name = complexity_name\n\t\tcomplex_type.type_name = complex_type_name\n\t\tcomplex_type.type_spec = '(complex<{2}>)'.format(complex_type_name,complex_type_name,"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\ttype_name = self.extract_name(type_name)\n\tif option_name:\n\t\toption = self.option.get(subcommand)\n\t\tif not option:\n\t\t\traise CommandError(\"Could not find option {0} in '{1}' command\".format(subcommand, self.name))\n\t\tspecs = option.get(\"spec\")\n\t\tif not specs:\n\t\t\traise CommandError(\"{0} has no attribute'spec' in '{1}' command\".format(subcommand, self.name))\n\t\ttry:\n\t\t\tsubclass"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tself.custom_args = _get_custom_nested_args(args, args.get('meta', {}), args.get('arguments', {}))\n\tcontrol_args = {}\n\tnested_args = {}\n\tif 'action' in self.custom_args:\n\t\tnested_args['action'] = { 'argument_spec' : args['meta'] }\n\t\tif 'conditions' in self.custom_args['action']:\n\t\t\tcustom_type = self.custom_args['action']['conditions']\n\t\t\tnested_args['"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\t# create control_args\n\tcontrol_args = {}\n\tif \"control\" in self.args:\n\t\t# for control, get all of the control variables\n\t\tcontrol_variables = collections.OrderedDict()\n\t\tfor a in self.args[\"control\"][1::].split():\n\t\t\tif \"=\" in a:\n\t\t\t\tk, v = a.split(\"=\", 1)\n\t\t\t\tcontrol_variables[k.strip()] = v.strip()\n\t\tcontrol_args[\"control\"] = list(control_variables.keys())\n\t\t\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\t\t# split up the input args.\n\t\targs = tuple(filter(None, args))\n\t\tcontrol_args = {}\n\t\tfor k in args[:-1]:  # remove redundant keys.\n\t\t\tif '+' in k:   # if key starts with \"+\" it's an control argument\n\t\t\t\tcontrol_args[k[2:]] = args[-1]        # remove the \"+\" from the last argument\n\t\t\t\t# print(\"in get_nested_custom_and_control_args, k=%s, args=%s\" % (k[2:"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t(control_args, nested_args) = args\n\tif self._mappings:\n\t\tcontrol_args_map = self.mappings.copy()\n\t\tnested_args_map = list(\n\t\t\titertools.chain.from_iterable(\n\t\t\t\tself.base_dict.pop(key, {}).get('args', {}).items()\n\t\t\t\tfor key in self._mappings\n\t\t\t)\n\t\t)\n\telse:\n\t\tcontrol_args_map = self._mappings\n\t\tnested_args_map = list(\n\t\t\titertools."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tif self.__args.control_custom or self.__args.nested_control:\n\t\treturn self.flatten_custom_nested_and_custom_control_args(args)\n\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif isinstance(arg, dict):\n\t\t\tif len(arg) > 0:\n\t\t\t\tif isinstance(arg[arg], dict):\n\t\t\t\t\tnested_arg = arg[arg]\n\t\t\t\t\tnested_args[self.escape_to_safe(self.key_and"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tctrl_args, args = self.parse_nested_custom_and_control_args(args)\n\tnested_args = self.parse_args(args, nested_arg=1)\n\tif args.nested and not ctrl_args['sub_args']:\n\t\t# This can happen in when running as standalone.\n\t\tnested_args = {}\n\t\n\tflat = dict()\n\tfor k, value in ctrl_args.items():\n\t\tif k in args.ignore_args:\n\t\t\tcontinue\n\t\tif not k in args.keys"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\treturn self.spec.control_args, self.spec.nested_args\n\n\t\n\ndef main():\n    argument_spec = dict(\n        service=dict(type='str', default='mixed'),\n        user=dict(type='str', required=True, no_log=True),\n        password=dict(type='str', required=True, no_log=True),\n        private_key_file=dict(type='path', default='/var/config/.ssh/id_rsa.pub'),\n        state=dict(type='str', default"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tflat_control_args = {}\n\tflat_nested_args = {}\n\t# flat_control_args = {}\n\t# flat_nested_args = {}\n\tfor p in args:\n\t\tif('*' in p):\n\t\t\tprint(p)\n\t\t\tfor var in args.pop(p):\n\t\t\t\tnew_var = (var + '*')\n\t\t\t\tflat_nested_args[var] = new_var\n\t\telse:\n\t\t\tif('$' in p):\n\t\t\t\tif('$' in args):\n\t\t\t\t\tprint('setting arg=$"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t# Check which arguments will be controlled\n\tnon_control_arguments = defaultdict(list)\n\tnested_arguments = dict(kv.split('=') for kv in args.split(','))\n\tfor argument in nested_arguments.keys():\n\t\tname = argument.strip().lower()\n\t\tif name == 'nested':\n\t\t\tname = 'control_arguments'\t\t\n\t\telif name == 'argument':\n\t\t\tname = 'argument'\n\t\tnon_control_arguments[name].append(argument)\n\t# Sort non_control"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args, nested_args = dict(), dict()\n\tif args.config and args.nested:\t# First check config arguments\n\n\t\t# Add the control args\n\t\tself._add_control_args(control_args, nested_args)\n\n\t\t# Add the nested args\n\t\tself._add_nested_args(args, nested_args, args.config)\n\n\t\t# And return control_args, nested_args\n\n\t\treturn control_args, nested_args\n\n\t# Set defaults\n\telif args.config:\n\t\targs.nested = dict()\n\n"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\n\textra_vars = (extra_vars or []) + vars_dict.keys()\n\t\n\treturn vars_dict\n\n\ndef filter_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with additional vars and their values\n\t\"\"\"\n\t\n\tif extra_vars:\n\t\treturn extend_extra_vars(vars_dict, extra_vars=extra_vars)\t\t\n\telse:\n\t\treturn vars_dict\t\n\n#"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = []\n\tfor key in vars_dict.keys():\n\t\tif type(vars_dict[key]) is str:\n\t\t\tvars_dict[key] = [vars_dict[key]]\n\t\tif type(vars_dict[key]) is list:\n\t\t\tfor vars_v in vars_dict[key]:\n\t\t\t\tfor k, v in vars_extra_vars.items():\n\t\t\t\t\tif hasattr(vars_v, k):\n\t\t\t\t\t\tsetattr(vars"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\treturn merge_two_dicts(vars_dict, extra_vars=extra_vars, merge_mode='extracut')\n\ndef extract_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"Extract extra-vars from ``vars_dict``\"\"\"\n\treturn extract_two_dicts(vars_dict, extra_vars=extra_vars, merge_mode='extract')\n\ndef merge_two_dicts(f1, f2, merge_mode='extracut'):\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor key in vars_dict:\n\t\tif not key.endswith('_extra_vars'):\n\t\t\tcontinue\n\t\tif isinstance(vars_dict[key], dict):\n\t\t\tvars_dict[key] = merge_extra_vars(vars_dict[key], extra_vars)\n\t\telse:\n\t\t\tvars_dict[key] = str(vars_dict[key])\n\treturn vars_dict\n\ndef convert_to_pascal_case(s):\n\t\"\"\"\n\tConvert ``s`` to Pascal"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tmerged_dict = {}\n\tif extra_vars is None:\n\t\textra_vars = list()\n\t\tfor key, value in vars_dict.items():\n\t\t\tif isinstance(value, dict):\n\t\t\t\tmerged_dict.update({key: merge_extra_vars(vars_dict[key], extra_vars=extra_vars)})\n\t\t\telse:\n\t\t\t\tmerged_dict[key] = value\n\telse:\n\t\tmerged_dict.update({key: value for key,value in vars_dict.items()"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif \"vars_dict\" in vars_dict:\n\t\tnew_vars_dict = vars_dict[\"vars_dict\"]\n\t\tif hasattr(new_vars_dict, \"get\"):\n\t\t\t# dict or list\n\t\t\tnew_vars_list = []\n\t\t\tfor k, v in extra_vars:\n\t\t\t\tnew_vars_list.append(\"{k}={v}\".format(k=k, v=v))\n\t\t\treturn dict.fromkeys(new_vars_list)\n\t\telse:\n\t\t\tnew_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor key in extra_vars:\n\t\tif key in vars_dict.keys():\n\t\t\tvalue = vars_dict[key]\n\t\telif isinstance(value, (list, tuple)):\n\t\t\tvalue.append(extra_vars[0])\n\t\t\tvalue = vars_dict.get_merged_value(value[0], key, None)\n\t\telse:\n\t\t\tvalue = extra_vars[0]\n\n\t\tif isinstance(value, list) and len("}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor entry, value in vars_dict.items():\n\t\tif not isinstance(value, (list, tuple)) or len(value) < 2:\n\t\t\traise ValueError('vars_dict must have the following entry/value pairs: [entry, value]. Found:', vars_dict, entry)\n\t\tif len(entry) and entry[0] == '_':\n\t\t\traise KeyError(\"'_' entry in _vars_dict is for private usage\")\n\t\tif vars_dict[entry] is not None and extra_vars is not None and entry"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif not extra_vars:\n\t\treturn {}\n\n\treturn { k: merge_extra_vars(v, extra_vars=extra_vars) for k,v in vars_dict.iteritems() }\n\ndef join_to_dot_namespace(vars_dict, extra_vars=None):\t\"\"\"\n\tJoin ``vars_dict`` with ``extra-vars``, create a (very) unique ``namespace`` using the\n\tform \"[\" and \"]\" as the prefix, then recurse through this namespace multiple times.  Each"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor var_name, val in vars_dict.items():\n\n\t\tif 'extra-vars' not in extra_vars:\n\t\t\textra_vars.append('extra-vars')[:0]\n\n\t\tif isinstance(val, dict):\n\t\t\textra_vars.append(var_name)\n\t\t\ttry:\n\t\t\t\tmerge_extra_vars(val, extra_vars)\n\t\t\texcept StopIteration:\n\t\t\t\tpass\n\t\telif isinstance(val, list):\n\t\t\tvars_dict[var_name] = list(map"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\t\t\n\t\tcommand = playbook_path+' '+'run '\n\n\t\targs = (\n\t\t\t'  --name \"{}\" --username \"{}\" --password \"{}\"\\n').format(\n\t\t\t\tir_workspace.user, ir_workspace.passwd, ir_workspace.password\n\t\t\t)\n\t\t# add defaults to arguments depending on'verbose'\n\t\tif verbose is None:\n\t\t\targs += (' \"--default\"' if not args else' --args'),\n\t\t\tif not args.strip() == '':\n\t\t\t\targs +=''\n\t\t\telse"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\t\timport ansible_playbook\n\t\timport ansible_connection as ac\n\t\timport getpass\n\t\timport paramiko\n\t\timport boto3\n\t\tfrom getpass import getpass\n\t\tfrom subprocess import CalledProcessError\n\t\tfrom collections import namedtuple\n\t\tfrom urllib3.connection import HTTPConnection\n\t\timport traceback\n\t\ttry:\n\t\t\t# For Ansia version and python 2.7.x\n\t\t\timport http.client\n\t\texcept ImportError(err_msg):\n\t\t\t# For Ansible 2.2.0\n\t\t\tpass\n\t\t\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\n\t\tans, output =  subprocess.Popen(\n\t\t\t[\n\t\t\t\tf'ansible-playbook',\n\t\t\t\t'-v',\n\t\t\t\t'--clean',\n\t\t\t\t'-vv',\n\t\t\t\t'--',\n\t\t\t\t'--tags'\n\t\t\t\t'-vv'\n\t\t\t\t'--inherit-vars',\n\t\t\t\t'--ansible-playbook',\n\t\t\t\tir_workspace.playbooks[ir_plugin.playbook.name].file,\n\t\t\t],\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t)\n\tdef handle(self):\n\t\tpayload = {'name':'plumbojs', 'path':playbook_path, 'v':verbose, 't':self.host}\n\t\tbody  = ''.join(subprocess.check_output(['ansible-playbook', '-e', self.host_string % payload, \"--loglevel=4\"]))\n\t\tif not self.host_string == \"localhost\":\n\t\t\tself._stdout_socket.sendall(bytes(body))\n\t\treturn_code = self.get_return_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\tcli_kwargs = dict(\n\t\tname = playbook_path,\n\t\textra_vars = {},\n\t\tcwd = ir_workspace.local_context().content.get_plugin_dir(),\n\t)\n\tcli_kwargs.update(ansible_args)\n\n\tprint('Executing ansible-playbook -v\\n' + f\"{ansible_playbook_version}\")\n\tprint(\"\\nRunning 'ansible-playbook -vv' to run an ansible-playbook playbook against the active plugin.\")\n\tprint(\""}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t)\n\t\n\tif verbose is None:\n\t\tverbose = ans_playbook_opts.verbose\n\n\tif verbose == 'debug':\n\t\tverbose = 'all'\n\t\n\t# TODO: Use the '--verbosity' option here to enable quieting of extra\n\t#       output that appears while printing this information to stderr.\n\t\n\t# Construct the playbook with '--extra-vars' option.\n\tplaybook = ['-c', \"{0}\".format(ir_plugin.get_playbook(playbook_path"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\t\targs  = ['playbook']\n\t\tif verbose is not None:\n\t\t\tif verbose == '0':\n\t\t\t\targs  = ['-vv' if p.inventory_verbose else '-vvv']\n\t\t\t\targs += ['--verbosity']\n\t\t\telif verbose == '1':\n\t\t\t\targs  = ['-vvvv']\n\t\t\t\tif p.inventory_verbose:\n\t\t\t\t\targs += ['--verbosity']\n\t\t\telse:\n\t\t\t\traise AnsibleError('invalid verbose level set')\n\n\t\targs  += ['-t', 'raw',"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\t\t\"\"\"Executes the playbook\n\n\t\t:param ir_workspace: An Infrared Workspace object represents the active\n\t\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t\t:param playbook_path: the playbook to invoke\n\t\t:param verbose: Ansible verbosity level\n\t\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t\t:return: None\n\t\t\"\"\"\n\t\tpass  # TODO: Remove\n#"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t),\n)\t\"\"\"\ndef ansible_playbook(\n\tir_workspace,\n\tir_plugin,\n\tplaybook_path,\n\tverbose=None,\n\textra_vars=None,\n\tansible_args=None,\n\t):\n\n\t# Get the current runbook.\n\tcurrent_ir_workspace = ir_workspace\n\tcurrent_ir_workspace.load()\n\trunbook = current_ir_workspace.get()\n\trunner_vars = vars(current_ir_workspace.ir_client"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\t\t\n\t\tenv_type = 'ANSIBLE_MODULE_ROOT'\n\t\tif verbose is not None:\n\t\t\tenv_type = 'ANSIBLE_VERBOSE'\n\t\tplugin_name = ir_plugin.name\n\t\tif ir_plugin.path:\n\t\t\tplugin_name = ir_plugin.path\n\n\t\treturn ir_workspace.install(\n\t\t\tplugin_name=plugin_name, plugin_path=playbook_path, env_type=env_type, extra_vars=extra_vars, ansible_args=ansible_"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\ttry:\n\t\tif vars_dict is not None:\n\t\t\tcommandline = ['ansible'] + cli_args\n\t\t\tcommandline.extend(['env', 'VAR_INHERIT=true', 'plugin=%s' % ir_plugin.name])\n\t\t\twith closing(cli.run_playbook(playbook=commandline, **vars_dict)) as rc,\\\n\t\t\t\ttempfile.NamedTemporaryFile(delete=True) as tmp_file,\\\n\t\t\t\ttempfile.NamedTemporaryFile(delete=True) as output_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tansible_args = cli_args + vars_dict['cli_args'] \n\tcommandline = \" \".join(ansible_args)\n\tstdout_result = os.system(commandline)\n\treturn {\"status\":stdout_result, \"vars\":vars_dict, \"command\":commandline}\n\ndef _is_valid_inventory_mode(inventory):\t\n\t\"\"\"\n\tCheck whether all host are created or not.\n\n\t:param inventory: An Infrared inventory object of the current plugin\n\t:return: `Boolean, if"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tir_plugin.add_vars_dict(vars_dict)\n\t\n\tir_plugin.run_cli(['ansible-playbook'],args=cli_args)\n\tplaybook = ir_plugin.vars_dict['name'] + '_playbook.yaml'\n\tplaybook_path = os.path.join(os.path.abspath('.'), 'plugins', ir_plugin.workspace_name, 'playbooks', playbook)\n\tprint(playbook_path)\n\tplaybook_path = os.path."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tif (len(cli_args) == 1):\n\t\tclusteruri = cli_args[0].split()[1]\n\t\tclist = \"Cluster: {}\".format(clusteruri)\n\t\tprint(\"[+] Running vars={} with Cluster:{}\".format(\n\t\t\tvars_dict, str(clist)[1:-1]))\n\t\tprint(\"[+]  Connect to workspace: {}\\n\".format(\n\t\t\tstr(ir_workspace.get_ws_name(cli_args[0])) + \"-\" + str("}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# initialize the shell connection\n\tif vars_dict:\n\t\t# run the actual cli command using vars_dict\n\t\tansible_wrapper = AnsibleUtils(vars_dict)\n\t\tcli_args = ansible_wrapper.run_cli_command(cli_args)\n\t\t\n\t\t_, stdout, stderr, ansible_output = (\n\t\t\tansible_wrapper.prepare_local_exec_command(cli_args)\n\t\t)\n\t\t\n\t\toutput = ansible_parser.parse_cli_result(ansible_output"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n\t# run the playbook\n\tdisplay.sensitive.print_message(\"Running playbook for:\")\n\tdisplay.message_status(\"\\n\\t%s\" % cli_args)\n\n\tplaybook = create_playbook(cli_args)\n\tinventory = get_inventory(cli_args)\n\tinventory_plugin = get_plugin(cli_args, ir_plugin)\n\n\twith closing(Inv(inventory, inventory_plugin)) as inv:\n\t\tinv.invoke(playbook.name)\n\t\tresult = inv.run(inventory_plugin.name)\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tvars_dict_for_playbooks = collections.OrderedDict([\n\t\t\t('env_name', 'local'),\n\t\t\t('env_host', 'localhost'),\n\t\t\t('env_port', '443'),\n\t\t\t('api_key', '')\n\t])\n\tif cli_args[:2] == [\"-v\", \"--verbose\"] or cli_args[:2] == [\"-V\", \"--v\"] or cli_args[:2] == [\"-vv\", \"--quiet\"]:\n\t\tvars_vars = collections.Ord"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n\tresults = []\n\tcmd = 'ansible -c \\'{vars_str}\\n{\\n  file \\'{ip}-{host}.yml\\'\\n{cmd}\\'\\n\\'{workflow_path}\\' | {args}\\'\\n\\'{plugin_type}\\'\\''\n\tresults.append(cli_args)\n\tresults.append(str(vars_dict))\n\tresults.append(str(cmd))\n\tresults.append(str(ir_plugin))\n\tresults.append(str(vars_dict))\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tresult = None\n\tif (cli_args[0] not in ('-h', '--help')):\n\t\tresult = execute_playbook(ir_workspace, cli_args, vars_dict=vars_dict)\n\treturn result"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tcli_args = [cli_args[0], ]    # remove first item for easy\n\tfor command in cli_args:\n\t\tir_workspace = ir_workspace.ir_workspace_dict.get_workspace_by_name(command)\n\t\tif (\n\t\t\t\tir_workspace.get_type() == 'infrared-workspace' and\n\t\t\t\tir_plugin.is_config_valid() and\n\t\t\t\tir_plugin.is_config_vars_valid()\n\t\t):\n\t\t\tcommand, arg_string = cli_args.pop"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor i in range(len(values_dict)):\n\t\tif values_dict[i][0] in ['--profile', '-p']:\n\t\t\tvalues_dict[i][1] = open('/dev/null', 'w')\n\t\tif values_dict[i][0] in ['--command', '-c', '--command-group']:\n\t\t\tif not values_dict[i][2]:\n\t\t\t\tvalues_dict[i][2] = ''\n\t\t\tvalues_dict[i][3] =''\n\t\n\treturn(values_dict)"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor n in values_dict:\n\t\tif type(values_dict[n]) is not str:\n\t\t\ttry:\n\t\t\t\tvalues_dict[n] = str(values_dict[n]) \n\t\t\t\tvalues_dict[n] = float(values_dict[n])\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\n\treturn values_dict\n\nconfig_file_path = r'C:\\Users\\jorke\\Desktop\\code\\python\\c_config.ini' # config file path\nconfig = configparser.ConfigParser()\nconfig_fp = open(config_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif values_dict:\n\t\t# If the user provided an extra string then replace it with None and not to save memory\t\t\t\n\t\t#  this is a workaround to ensure that only the actual commandname gets added\n\t\tvalues_dict = values_dict.copy()\n\t\tvalues_dict.pop('command')\n\t\tfor param, default in values_dict.items():\n\t\t\tif default is None:\n\t\t\t\t\n\t\t\t\tdefault = ''\n\t\t\t\tvalues_dict[param] = _convert_string(default, parser_name)\n\t\t\t\n\t\t\t# if value is a list and have the same"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor field_name, values in iteritems(values_dict):\n\t\tif not isinstance(values, bool):\n\t\t\tvalues = values_dict[field_name]\n\t\t\tif hasattr(values, '__call__'):\n\t\t\t\t# Handle all callables.\n\t\t\t\tif hasattr(values, '__name__'):\n\t\t\t\t\t# Handle callable classes with a method __name__.\n\t\t\t\t\tparser_name_func = getattr(values, '__name__', None)\n\t\t\t\t\tif parser_name_func is None:\n\t\t\t\t\t\t# No __name__ defined.\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif \"args_parser\" in values_dict:\n\t\treturn\tvalues_dict[\"args_parser\"]\n\n\tif parser_name not in self.__parsers:\n\t\traise (ValueError(\"No parser instance for {}.\".format(parser_name)), ValueError(\"No argument parser for {}.\".format(parser_name)))\t\t\n\t\n\tparser_class = self.__parsers[parser_name]\n\treturn parser_class(_convert_cli_args(self, parser_args=values_dict[\"parser\"]), self.__namespaces)\n\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif not isinstance(values_dict, dict):\n\t\traise TypeError(\"'values_dict' must be a dictionary type\")\n\tfor k, v in values_dict.items():\n\t\tif isinstance(v, bool):\n\t\t\tpass\n\t\telif isinstance(v, int):\n\t\t\tv = str(v)\n\t\telif isinstance(v, float):\n\t\t\tv = str(v)\n\t\telif isinstance(v, str):\n\t\t\tif len(v) < 1 or len(v) > 255:\n\t\t\t\traise ValueError(f\""}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif parser_name =='main' or parser_name is None:\n\t\tfor key, value in values_dict.items():\n\t\t\ttry:\n\t\t\t\tvalue = ast.literal_eval(value.replace('_',''))\n\t\t\t\tif key in CONVERT_TYPE:\n\t\t\t\t\tvalue = CONVERT_TYPE[key](value)\n\t\t\texcept Exception as e:\n\t\t\t\tprint(e)\n\t\t\t\tprint(\"Unable to convert the string argument '{} {}' to {}, please try again.\".format(\n\t\t\t\t\t\tkey, type(value), type(str(value))"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tassert(parser_name in self.list_parser_names) and \\\n\t\t\tisinstance(values_dict, dict), \\\n\t\tf\"Invalid {parser_name} format. Must be one of: {self.list_parser_names}\"\n\n\t# Set parameter defaults\n\tdefault_dict, parameter_names = values_dict[parser_name]['default'], \\\n\t\t\tvalues_dict[parser_name]['names']\n\n\n\tset_dims = len([self.s_type, self.data_mode,\n\t\t\t\t\tself.p_type, self."}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tparser = cmd.OptionParser(prog=self.name, version=__version__, description=\"Run scripts\")\n\t\n\t# Convert all the command's optional arguments to arguments formatted to cli arguments as\n\t# strings or in the form of comma separated arguments.\n\tcmd_options = [str(opt) for opt in values_dict[\"argv\"]]\n\tparser.option_list.__init__(cmd_options,'verbose', metavar='VERBOSE', multiple_options=True,\n\t\t\t\t\t\t\t\t\toptional_not_given=True,\n\t\t\t\t\t\t\t\t\trequired="}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t# first call parent method\n\tif hasattr(self.parent, '_convert_non_cli_args'):\n\t\treturn self.parent._convert_non_cli_args(parser_name, values_dict)\n\treturn values_dict\n\n"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tflaten_dict = {}\n\tfor plugin_spec in sorted(glob.iglob(plugin_dir + \"/*.py\")):\n\t\tfilename = os.path.basename(plugin_spec)\n\t\tfname_with_ext = os.path.splitext(filename)[0]\n\t\tkeyname = fname_with_ext + \".config\"\n\t\tconf_key = plf_dir + fname_with_ext + \".json\" \n\t\tif not os.path.exists(plugin_dir + fname_with_ext + \".json\""}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tspec = {}\n\tfor filename in os.listdir(plugin_dir):\n\t\tif filename.endswith(\".py\"):\n\t\t\tspec_line = \"\"\n\t\t\tline = \"\"\n\t\t\tfor idx, char in enumerate(filename):\n\t\t\t\tif char.isalnum() or char is \"_\":\n\t\t\t\t\tspec_line += char\n\t\t\t\telif char in [\"\\\\\"]:\n\t\t\t\t\tspec_line += \"\\\\\" + char\n\t\t\t\telif char in [\"`\"]:\n\t\t\t\t\tspec_line += \"\\\\\" + char\n\t\t\t\telif char in [\"|\"]:\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tspec_dict = {}\t\n\tfor root, dirs,files in os.walk(plugin_dir):\n\t\tfor name in dirs:\n\t\t\tif name == \"__pyc__\":\n\t\t\t\tcontinue\n\t\t\tif not os.path.splitext(name)[1] == \".py\":\n\t\t\t\tcontinue\n\t\t\t#print(\"root:{} name:{}\".format(root, name))\n\t\t\tfull_name = os.path.join(root, name)\n\t\t\tif not full_name.startswith(plugin_dir):\n\t\t\t\tcontinue\n\t\t\tif full_name"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tspec = importlib.util.spec_from_file_location('plugin_spec', plugin_dir+os.sep+'plugin_spec.py')\n\tspec.loader.exec_module(module)\n\treturn dict(spec.modules)\n\ndef load_module(filepath):\n\t\"\"\"\n\tLoads a module from the given python file\n\n:param filepath: A path to the python file\n\t\"\"\"\n\tp = importlib.util.spec_from_file_location('pathToFile', filepath)\n\texec(compile(p"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n\tspec_list = get_property_list(plugin_dir)\n\tdictionary = deepcopy(spec_list[0])\n\n\tfor entry in spec_list[1:]:\n\t\t# if entry not in dictionary:\n\t\tdictionary[entry] = deepcopy(spec_list[0])\n\n\treturn dictionary\n\ndef get_property_list(plugin_dir):\n\treturn [\"class_descr\", \"method_descr\", \"init_func\"]\n\ndef get_class_property_dict(plugin_dir, class_list, method_list,"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n\t# load plugin specifications\n\tplugin_spec_string = \"\"\n\tplugin_spec_list = []\n\tif not os.path.exists(plugin_dir):\n\t\tprint(\"{} doesn't exist\".format(plugin_dir))\n\t\treturn {}\n\t\t\n\twith open(plugin_dir) as f:\n\t\tfor line in f:\n\t\t\tplugin_spec_string = plugin_spec_string + (str(line));\n\t\t\n\tplugin_spec_list = plugin_spec_string.split(\"\\n\")\n\t\n\t# check if the"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\toutput = {}\n\tpath_list = os.listdir(plugin_dir)\n\tfor item in path_list:\n\t\tfull_path = os.path.join(plugin_dir, item)\n\t\tif os.path.isdir(full_path):\n\t\t\tname = os.path.splitext(item)[0]\n\t\t\toutput[name] = get_plugin_spec_flatten_dict(full_path)\n\tfor key in output.keys():\n\t\tentry = output[key]\n\t\tentry['plugin_dir'] = plugin_dir\n\t\t"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n\treturn {\"type\": \"pygame\",\n\t\t  \"version\": \"1.0\",\n\t\t  \"author\": \"-\",\n\t\t  \"pygame_version\": \"1.9.0\",\n\t\t  \"url\": \"https://pygame.pygame.org/\" + PLUGIN_NAMES[PLUGIN_TYPE] + \"/\",\n\t\t  \"license\": \"MIT\",\n\t\t  \"description\": \"\"}"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tplugin_spec = dict()\n\tfile_path = glob(\"%s/%s\"%(plugin_dir, \"plugins/plugin_%s.py\"%__name__))\n\tplugin_spec = eval(open(file_path[0]).read())\n\treturn plugin_spec\n\n\ndef get_plugin_spec_flatten(plugin_spec):\t\"\"\"\n\tCreates a flat dictionary from the plugin spec\n\n:param plugin_spec: A plugin spec dictionary for the plugin class\n:return: A flattened dictionary contains the plugin's properties\t\"\"\"\n\tplugin_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tout = {}\n\tout['version'] = '2.00'\n\tout['package'] = 'libpuppet'\n\tout['type'] = 'executable'\n\tout['action'] = 'run'\n\tout['description'] = '''puppet plugin, create new puppet'''\n\tout['category'] = 'plugins'\n\tout['name'] = 'puppet'\n\tout['author'] = ''\n\tout['requirements'] = []\n\treturn flatten(out)\n\n#---------------------------------------------------------------------------------- #\n"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tself.set_env('CONFIGPATH', conf_path_var('CONFIGPATH'))\n\tself.set_env('BKONFIGPATH', '~/.bk')\n\tself.set_env('USERBKDIR', bin_path_var('USERBKDIR'))\t\t# where to put user bin\n\treturn\n  \n  \n  @property\n  def pkginfo(self) :\n\t\"\"\"\n\tGet userinfo from.pkginfo\n\t\"\"\"\n\tif self.get_env('ENV', False)!= '"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tself.exec_command(\"echo $PBS_CTX_OFFSET_BIG+$PBS_JOB_ID\")\n\n\t#self.writeout()\t\t\t# This will run an sbatch command:\n\t\t\t\t\t\t\t\t\t# \"echo '$PBS_JOB_ID' >> $PBS_NODENAME_FILE >> $PBS_CLUSTER_NAME\"\n\t\t\t\t\t\t\t\t\t# \"python3 $PBS_ROOT -c 'exec $PBS_COMMAND' -l $PBS_NODELIST -o $PBS_NODENAME"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tfor p in self.config_path:\n\t\tif not os.path.exists(p):\n\t\t\tself.config_path.append(p)\n\tself.config_path_string = \"[\" + join(self.config_path) + \"] \"\n#------------------------------------------------------------------------------\n#\n#\tFunction to clean up the environment and read the variables.\n#\n#\t@return\t\tNone\n#\n#------------------------------------------------------------------------------\n\tcmd = \"export\"\n\tfor v in self.__environment_variables.keys():\n\t\tcmd_var = \"export \" + v +"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif env.PROGPATH!= \"\":\n\t\tself.configPath = env.PROGPATH\n\telif self.root and self.options['root']:\n\t\tself.configPath = ROOT_FILE_PATH\n\telse:\n\t\tf = file(DEBT_FILE_PATH)\n\t\tbase = f.read()\n\t\tf.close()\n\t\tself.configPath = \"/\".join(base.split(\"\\n\")[1:])\n\tif self.options['root']:\n\t\tconfig.AddOption(\"$BASE\", self.configPath,"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tself.env_config = False\n\tif self.get_config()['env_config']:\n\t\tself.set_config(get_config_value('env_config', self.config))\n\t\tself.set_env_config() # sets env_config to True after successful setup\n\tif not self._debug:\n\t\tself.set_config(self.get_config())\n\tself.load_config(self.get_config())\n\t#print (f\"Loaded default config for {self.name}\")\n\n\t#if not self.env_"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif os.getenv(\"OPENEMCON_HOST_API_CONFIG\",):\n\t\tconfigPath = os.getenv(\"OPENEMCON_HOST_API_CONFIG\")\n\t\tif configPath is None:\n\t\t\tconfigPath = f\"/etc/openmile-app/OpenMi/openmile_core/v{API_VERSION.split('.')[0]}-python/OpenMi\"\n\t\t\tos.environ['OPENEMCON_HOST_API_CONFIG'] = configPath\n\telse:\n\t\tconfigPath = f\"/etc"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tenv = environ.get_env()\n\t\n\tif env == None:\n\t\twarnings.warn(\n\t\t\t\"Environment variable is undefined, check environment variables.\",\n\t\t\tRuntimeWarning)\n\t\tsys.exit(1)\n\t\n\tenvironment = environ.Env()\t\n\tenvironment.setenv('config_path', environment('HOME') + \"/test_project/config\", override = True)\n\t\n\tenv.read_env()\n\treturn environment\n\ndef main(argv=None):\n\t\"\"\"\n\tRun the test suite.\n\t\"\"\"\n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tself._bw_name += '_config_default'\n\tif self.environments.get(self.ENV, '') == self.ENV:\n\t\tdel self.environments[self.ENV]\n\tkey = 'BW_%s' % self.ENV\n\tself._bw_path[key] = os.getenv(key, self.environments.get(self.ENV, ''))\n\tself._bw_path[key] = self._bw_path.get(key, self.DEFAULT_BW_PATH"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tself.set_environment_variable(\"config_path\")\n\n\tif self.check_environment_variable() == False:\n\t\tprint(f\"Please configure env variable as env_variable = 'config_path'\")\n\t\tprint()\n\t\tinject_config()\n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.config_path is None:\n\t\tprint('There is no configuration file. Do you want to create one? [y/N]: ').upper()\n\t\tconfirm = input('Create config_path? [Y/N] > ')\n\t\tif confirm!= 'Y':\n\t\t\tself.exit()\n\t\telse:\n\t\t\tprint('Config file was created!')\n\t\t\tself.config_path = self.make_config_path(self.name)\n\t\t\tos.environ['OMNIFICCLIENT_CONFIG_PATH'] = self.config_path\n\n"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tsubparser = subparsers.add_parser( 'extension-filter', \n\t\tself.extension_filtration_cli,\n\t\thelp = 'Extensioal filter toolbox',\n\t\tformatter_class = argparse.RawTextHelpFormatter,\n\t\tdest = 'extension'\n\t)\t\n\tsubparsers.add_parser( 'extentions', \n\t\tparents = [], \n\t\tformatter_class = argparse.RawTextHelpFormatter,\n\t\thelp = 'Toggle extension filter',\n\t\tdest = 'extensions"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tself.cmd_descriptor = subparsers.AddOptionalGroup(\"cmd\")\n\tself.cmd_description = subparsers.add_parser(\n\t\t\"cmd\",\n\t\tparents=[root_subparsers],\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t)\n\tself.cmd_description.add_argument(\n\t\t\"-P\",\n\t\t\"--pkginfo\",\n\t\thelp=\"path to pkginfo file\")\n\tself.cmd_description.set_defaults(func=self.run_comm"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\targ_def = {\n\t\targ: type(self)(\n\t\t\thelp=arg.help,\n\t\t\tdescription=arg.description,\n\t\t\tdefault=arg.default,\n\t\t\targ_type=arg.type,\n\t\t\trequired=arg.required,\n\t\t\texclude=arg.exclude,\n\t\t\tdefault_args=arg.default_args,\n\t\t)(arg.default, arg.required)\n\t\tfor arg in self.arg_spec\n\t}\n\tsubparser = subparsers.add_parser(\n\t\tname=self.NAME,\n\t\tdescription=self."}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t# This dictionary is populated automatically by python\n\t# so you can extend it by simply editing the variables below and\n\t# then modifying something accordingly.\n\tspec_cli_options = {\n\t\t'--config.debug.verbose':  {\n\t\t\t\t'type': 'bool',\n\t\t\t\t'default': True,\n\t\t\t\t'help': 'Indicates whether to include debug information about the run of the experiment.'\n\t\t},\n\t\t'--config.debug.verbose.format_string': {\n\t\t\t\t'type':'str',\n\t\t\t\t'default': '{{ iteration %d }}"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t_subparsers = []\n\tfor param in MODIFIERS[\"module_meta\"]:\n\t\tparent = self.__class__.__name__[:param[\"name\"]] + \"_\" + param[\"name\"]\n\t\tparser = param.get(\"param_name\", \"\")\n\t\tif parser:\n\t\t\tparent += \"_\" + parser\n\t\t_subparsers.append(parent)\n\t\t\n\tfor param in MODIFIERS[\"param_meta\"]:\n\t\tparent = self.__class__.__name__[:param[\"name\"]] + \"_"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tparser = subparsers.add_parser('create',\n\t\t\t\t\t\t\t\t\thelp = \"Create an environment from an environment directory.\")\n\n\tparser.set_defaults(func = self.create)\n\tadd_cli_arguments(parser)\n\n\troot_subparsers.add_parser('create-environment',\n\t\t\t\t\t\t\t\thelp = \"Create a new environment from config.yaml.\",\n\t\t\t\t\t\t\t\tdescription = \"Creates an environment from the config.yaml.\",\n\t\t\t\t\t\t\t\tparents = [parser])\n\n\tparser.add_argument('environment',\n\t\t\t\t\t\thelp = 'The environment to create.')\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tif HAVE_SUB or __IPYTHON_MODULE__:\n\t\t# I haven't enabled Python 3's namespace-package support yet\n\t\t# see http://www.bruh-python.org/wiki/IPython_namespace_package\n\t\tfrom IPython.core.usage._argcomplete import argcomplete\n\t\tself.argument_parser = subparsers = argcomplete.add_arguments(root_subparsers=root_subparsers)\n\t\tself.argument_group.description = argcomplete.describe('Spec options', parents=subp"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\treturn SubcmdClass(optparse.Option, spec=root_subparsers)"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tfor name, subparser in root_subparsers.items():\n\t\tcli.add_cli_args(subparser, name)\n\treturn root_subparsers\t\n\ndef get_data(f): \n\t\"\"\"\n\tWrapper function to obtain and filter data by function.\n\t:param f: function to be called\n\t:returns: dictionary of data (see dictify)\n\t\"\"\"\n\t# Check if data have been fetched previously:\n\tif (os.path.exists(configPath )):\n\t\tf.extract_data"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tspec_cli = subparsers.add_parser('spec', \n\t\tparents=[root_group], \n\t\thelp='Set up the spec config file.',\n\t\tdescription='''Spec config file is here''', \n\t\tadd_help=False, \n\t\tformatter_class=RQFileFormatter,\n\t)\n\tspec_cli.add_argument(\n\t\t'config', \n\t\tnargs='w', \n\t\thelp='spec.cfg file path.',\n\t)\n\n\tdef check_args(args):\n\t\tif args.ext"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\ttry:\n\t\twhile path.endswith(\"/\"):\n\t\t\tpath = path[:-1]\n\texcept:\n\t\tpass\n\twhile path.startswith(root):\n\t\tpath = path[len(root):]\n\tif path:\n\t\tpath = path.replace(root+\"/\"+root, root)\n\treturn path\n\ndef main()\t:\n\tprint_usage()\n\ndef get_args()\t:\n\tparser = get_args_parser()\n\targs = parser.parse_args()\n\tif args.output:\n\t\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif path == root:\n\t\traise Exception('Found root in path!')\n\telif len(path) > root:\n\t\traise Exception('Path too long!')\n\telif path.endswith('/'):\n\t\tpath = path[0: -1]\n\telif path.endswith('/.'):\n\t\tpath = path[0: -1]\n\telif path.endswith('/..'):\n\t\tpath = root\n\n\treturn path\n\ndef strip_slash(path):\n\tpath = path.split('/')"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif os.path.isfile(path):\n\t\tif not path.startswith(root):\n\t\t\traise ValueError(\"root must be /path/to/file\")\n\t\treturn path[len(root):]\n\telif os.path.isdir(path):\n\t\treturn strip_root(path, root)\n\telse:\n\t\traise ValueError(\"path is neither a file nor a directory\")\n\t\ndef find_path(start, pattern):\n\t\"\"\" \n\tGet the relative path in the \"root\" directory, i.e.,\n\t\"start"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\ttry:\n\t\treturn os.path.join(path, *os.path.splitdrive(os.path.abspath(root)))\n\texcept os.error:\n\t\traise RuntimeError(\"%s not found\" % root)\n\n\n# class path_info(object):\n#\n# \"\"\"\n#  Simple class that holds information about the path where\n#   we have found the given base directory.\n#\n#  @cvar _parents: The list of parents of 'base' without the directory\n#\t\t\t\t\t of 'base'. In case of root path 'base' is"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\twhile True:\n\t\ttry:\n\t\t\troot = root.replace(os.path.sep, \"/\")\n\t\t\tpath = path.replace(root, \"\")\n\t\t\treturn absPath(path),\n\t\texcept ValueError:\n\t\t\tpass\n\traise ValueError(\"path must start and end with a /\")\n\ndef absPath(path):\n\t\"\"\"\n\tNormalize absolute paths.\n\t\"\"\"\n\treturn os.path.abspath(os.path.join(os.path.abspath(path), os.pardir, os.pardir))\n\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tfor element in path[:-1]:\n\t\tif element == root:\n\t\t\tindex = path.index(root)\n\t\t\tpath = path[index+1:]\n\t\t\tbreak\n\telse:\n\t\traise Exception(\"Error: the root not in '\"+path+\"' or '/'. Don't make it into '\"+path+\"'.\")\n\treturn path"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root) + 1:]\n\traise Exception(\"Unable to find root of file!\")\n\ndef read_meta(lines, root=None):\n\t\"\"\"\n\tRead metadata information from the first and last lines of\n\tdata_path; if root is provided read that file as a directory tree,\n\telse recursively parse the tree rooted at files rooted at\n\tmeta_path.\n\t\"\"\"\n\tfiles = []\n\tfor line in lines[:-1]:\n\t\tfor f in glob(os."}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\ttry:\t\n\t\tpath = os.path.splitdrive(path)[2]\n\t\treturn path[len(root)+1:]\t\n\texcept:\t\n\t\traise Exception('Path \"'+path+'\" does not point directly to an existing file')\t\n\n\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tfor i in xrange(0, len(str(root))):\n\t\tif str(root)[:i]== \"/\":\n\t\t\tbreak\n\telse:\n\t\traise \"Root not identified\"\n\t\n\tpath = path[:len(root)] + root + path[len(root):]\n\treturn path\n\t\n\t\ndef check_moved(file_, src, root):\n\treturn not file_.islink()\n\t\ndef check_exists(file_, src, root):\n\tif check_link(file_, src, root):\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tpath = os.path.normpath(os.path.sep.join(path.split(root)))\n\tif not os.path.isdir(path):\n\t\traise ValueError(\"Cannot handle path %r; it is not a directory\" % path)\n\ttry:\n\t\treturn os.path.normpath(os.path.sep.join(path.split(root)[1:]))\n\texcept ValueError:\n\t\traise ValueError(\"Cannot handle '%s' as a directory\" % path)\n\t\ndef get_file_list(pattern"}]}
