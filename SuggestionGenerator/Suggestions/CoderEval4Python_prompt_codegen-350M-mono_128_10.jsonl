{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(0,\n\t\tnanoseconds,\n\t\ttz=tz,\n\t\thour=1,\n\t\tminute=0,\n\t\tsecond=0)\n\n\n#------------------------------------------------------------------------------\ndef local_datetime(year=None, month=None, day=None, hour=None, minute=None, second=None, tz=None):\n\t\"\"\"\n\tCreate a :class:`.LocalDatetime` instance based on a `UTC datetime`.\n\n\t:return:\n\t\t:class:`.LocalDatetime`\n\t\"\"\"\n\tdt = local_datetime("}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz is None:\n\t\ttz = _localtz\n\treturn Time(nanoseconds, tz=tz)\n\ndef _iso8601(dt):\n    # This was extracted from the following code snippet (from the pyspacethem._iso8601 module):\n    # http://code.activestate.com/recipes/577367-iso8601-datetime-to-pyspacethem/\n    # The two examples here are not quite identical, one uses `strftime` to generate a localtime, another uses it directly to convert an"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif isinstance(nanoseconds, int):\n\t   nanoseconds  = math.pow(nanoseconds, 10)\n\treturn datetime.timedelta(seconds = nanoseconds) if tz is None else datetime.timedelta(seconds = math.pow(nanoseconds, tz.hours))\n\ndef convert_time_to(epoch, tz=None, *, scale = 1000, format = \"%H:%M\", fmt = None, fmt_date = None, scale_num = False):\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(nanoseconds=nanoseconds, tzinfo=tz)\n\ndef make_local_time(milliseconds):\n\t\"\"\"\n\t\tMake `LocalTime` from `milliseconds`\n\t\"\"\"\n\treturn Time(milliseconds)\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz is None:\n\t\ttz = config.tz\n\t\traise ValueError(\"tz=None is not a valid configuration\")\n\telif isinstance(tz, int):\n\t\ttz = pytz.timezone(tz)\n\telse:\n\t\ttz = pytz.timezone(tz)\n\t\n\treturn time(timezone_offset(tz, nanoseconds), nanoseconds)\n\ndef time(nanoseconds=None, tz=None):\n\t# type: (Union[tuple[int], int], Union[None, str"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t_ZERO = zeros(1)\n\t__tzinfo__ = tz\n\treturn Time(__tzinfo__,\n\t\tnanoseconds,\n\t\t(t.to_time(), t.local_time_zone(\"US/Pacific\"), t.local_time())\n\t)\n\nclass TimeIndexed(Indexed):\n\t\"\"\"\n\tTime Indexed.\n\n\t:ivar _index: Index the values of the data for the time index.\n\t\"\"\"\n\t_getitem = Indexed._getitem\n\t_array = Indexed._array\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn _time.Time(nanoseconds * _time.nanoseconds_in_nanoseconds, tz=tz)\n\n\t\ndef parse_timestamp(s):\n\t\"\"\"\n\tParse a time string into a `Time` and `LocalTime` instance.\n\t\n\t:param s:\n\t:return: Time, LocalTime instance\n\t\"\"\"\n\tts = _time.Time()\n\tg = _time.timestamp_in_nanoseconds_in_millis(float(s)) \n\tts = _time."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(nanoseconds=nanoseconds,\n\t\t\t\t tz=tz,)\n\ndef create_time(nanoseconds):\n\t\"\"\" Creates a `Time` instance with `nanoseconds`\n\n\t:param nanoseconds:\n\t:return: `Time` class\n\t\"\"\"\n\treturn Time(nanoseconds=nanoseconds,)\n\ndef create_times(*args, **kwargs):\n\t\"\"\" Creates a `Time` instance for one or more values\n\n\t:param args: Time objects\n\t:param kwargs"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn mtime.LocalTime(tz=tz) * nanoseconds + mtime.Time.now().timestamp(sliced=False,\n\tt=mtime.Time.now(tz=tz), s='micros')\n\ndef is_in_time(string, times):\n\t\"\"\"\n\tCheck whether string is inside a certain list of times.\n\t\n\t:param string: String to be checked\n\t:param times: List of numbers representing the allowed times\n\t:return: False if not contained in the times, True otherwise.\n\t\"\"\"\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\treturn _lib._py_numpy_fromtimestamps(nanoseconds, tz)\n"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\td=value\n\n\tif d.days < 0:\n\t\tif d.seconds < 0:\n\t\t\td=d.days+1\n\t\t\td += (d.seconds + 60*60*24*365*365)\n\t\telif d.seconds >= 0:\n\t\t\td=d.days + 1#days\n\t\t\td += (d.seconds * 60*60)\n\t\t\td += (d.seconds + 60*60*24*365*365)\n\telif d.days > 0:\n\t\tif d.seconds < 0:\n\t\t\td=d.days"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tvalue = coerce_timedelta_to_str(value)\n\treturn f'{str(value)} day'\n\n\ndef coerce_string_to_timedelta(value, base=None, format=None):\n\t\"\"\"\n\t\tCoerce a string to a `timedelta` object\n\t\n\t:param value:\n\t:type value:\n\t\t\"23 hours, 4 minutes, 1 second or 0.001 milliseconds\"\n\t:param format:\n\t:type format:\n\t\t\"hh:mm:ss\" for 24hr\n\t\t\"m"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tyears = value.days // 365\n\tif years:\n\t\t# years * 24 => days.\n\t\tmonths = years * 12 + (value.days - (years * 365 * 24)) // 365\n\t\tdays = (value.days - (years * 365 * 24)) % 365\n\telse:\n\t\tmonths = 0\n\t\tdays = value.days\n\tif days > 0:\n\t\tstr_value = '{}{}'.format(years + 1, months).strip()\n\t\tstr_days = str(days).strip()\n\telse:\n\t\tstr_value = '"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif isinstance(value, timedelta):\n\t\treturn to_unicode(str(value), errors='replace')\n\n#-------------------------------------------------------------------------------\n#                                           Exceptions\n#-------------------------------------------------------------------------------\n\n\n\n\nclass ValidationError(Exception):\n\n\t\"\"\"\n\t:raise: `ValidationError`\n\t\n\tRaise this error to reproduce the error raised when validating a data\n\tvalue against the required metadata from the model.\n\tYou can also define your own test error using the given keyword arguments named\n\t``exception`` and ``message``. For example, ``my_model"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif has_timezone:\n\t\tvalue = value.strftime('%X')\n\n\tdef __str__(self) -> str:\n\t\tif has_timezone:\n\t\t\tvalue = value.strftime(DATETIME_STRINGS[\"asctime_short\"])\n\t\telse:\n\t\t\tvalue = value.strftime(DATETIME_STRINGS[\"default\"])\n\t\tvalue = value.replace(\"\\\\s\", \".\")\n\t\tif len(value) > 4 and value[:3].isdigit():\n\t\t\ttime_seconds = timedelta(seconds"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.days + (value.seconds + value.microseconds / 1000000.0)\n\nclass TimeDelta(InputField):\n\tdef __init__(self, input_name, type_name, description):\n\t\tInputField.__init__(self, type_name, description)\n\t\tself.input_name = input_name\n\t\tself.type_name = type_name\n\t\t\n\tdef process(self, value):\n\t\ttry:\n\t\t\treturn py_datetime.datetime.strptime(value, formats_string)\n\t\texcept:\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif value == timedelta():\n\t\tpass  # pass back to call-back\n\tif value == timedelta(seconds=0):\n\t\treturn \"\"  # TODO: make this function generic\n\tif type(value) is Decimal:\n\t\treturn f\"{value:.0f}s\"\n\tif value == timedelta(microseconds=0):\n\t\treturn \"\"\n\tif not value == timedelta(days=1, hours=0, minutes=0, seconds=0):\n\t\treturn str(value) + \"s\"\n\tif not value == timedelta"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn '{0} minutes'.format(str(value / 60).zfill(5))\ndef dehydrate_date(value):\n\t\"\"\"\n\t\tDehydrator for `date` dates\n\n\t\t:param value:\n\t\t:type value: date\n\t\t:return:\n\t\t\"\"\"\n\tif isinstance(value, datetime.datetime):\n\t\treturn value.isoformat()\n\treturn value.isoformat()\n\ndef dehydrate(name, value):\n\t\"\"\"\n\t\tHelicopteric function to return a callable that will be passed to the underlying value\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value\n\n#############################################################################################################\n\nclass _ConvertableBool(int):\n\n\t\"\"\"\n\tConvertable value for boolean.  \n\t\n\t:param value:\n\t\"\"\"\n\tdef __init__(self, value):\n\t\tif not isinstance(value, bool):\n\t\t\traise ValueError(f'{value} is not of type {type(value).__name__}')\n\n\t\tself.value = value\n\n\tdef __str__(self):\n\t\treturn None if pd.isnull(self.value) else str(self"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tfrom._datetime_base import DateTime_DateTime\n\tfrom. import utc_datetime\n\t# type: Dict[str, DateTime]\n\treturn DateTime_DateTime().to_timedelta(value).to_dict()\n\ndef dehydrate_tz_aware(value):\n\t\"\"\"\n\tDehydrator for `tz_aware` values.\n\n\t:param value:\n\t:type value: bool\n\t:return:\n\t\"\"\"\n\tfrom._boolean import Boolean\n\tfrom._boolean import Boolean\n\tfrom._boolean import"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tyear = value.year\n\tmonth = value.month\n\tday = value.day\n\tday_of_week = value.day_of_week if hasattr(value, 'day_of_week') else value.day\n\thour = value.hour\n\tminute = value.minute\n\tsecond = value.second\n\t\n\tformated = format_time(year, month, day, day_of_week, hour, minute, second)\n\t\n\treturn formated\n\n# DeprecationWarning: The `format_time` library has been deprecated in favor"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\treturn value.display\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\tif isinstance(value, Time):\n\t\treturn value.toISOFormat()\n\telse:\n\t\treturn six.text_type(value)\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tresult = str(value)\n\tif value.name!= 0:\n\t\tresult = result.replace('[', '')\n\t\tresult = result.replace(']', '')\n\treturn result"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.format('{:.2f}').replace('Z', 'T0')\n\n##\n\n@app.route('/api/transcript/<string:transcript_id>')\ndef transcript_id(transcript_id):\n\t\"\"\"\n\tGet transcript by id.\n\n\t:param transcript_id: Transcript ID.\n\t:type transcript_id: str\n\t:return: Transcript result.\n\t:rtype: Object\n\t\"\"\"\n\n\treturn transcript_id\n\n\n##\n\n@app.route('/api/transcript/create/"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tresult = value.replace(tzinfo=datetime.timezone.utc).astimezone(\n\t\tdatetime.timezone(datetime.timedelta(seconds=0))\n\t)\n\treturn result.strftime(\"%d.%m.%Y\").lstrip(\"0\")"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif not value.is_valid():\n\t\treturn value.value\n\n\thour, minute, second = _seconds_to_local_time(value.value)\n\tseconds = abs(minutes * 60 + second)\n\tif value.start.value + seconds < 0:\n\t\tseconds += 24 * 60 * 60\n\tmonth = _decoding.month(seconds)\n\tyear = _decoding.year(seconds)\n\treturn _decoding.timeformat(year, month, value.start.value + hours + second / 60 + minutes / 60 + seconds / 1440)"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.format('h')\n\t# @todo: Remove\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.isoformat()\n\n\ndef get_month(year, month):\n\t\"\"\"\n\tGets the index of a time component in a month dictionary.\n\t\n\t:param year:\n\t:type year: int\n\t:param month:\n\t:type month: int\n\t:return:\n\t\"\"\"\n\tmonth_dict = {\n\t\t1: 1,\n\t\t2: 5,\n\t\t3: 7,\n\t\t4: 10,\n\t\t5: 12,\n\t}\n\treturn month_dict[month]\n\n\ndef get_month_name"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\"\"\"\n\treturn value.strftime('%Y-%m-%d')\n\t\"\"\"\n\treturn f\"{value}-{value.month}-{value.day}\"\n\n# endregion class\n\n\n# region fields\n\n\nclass Time(db.Model):\n\t\"\"\"Represents one time for a row in database in sqlalchemy.\"\"\"\n\n\n\tid = db.Column(\n\t\tdb.Integer, \n\t\tprimary_key=True, \n\t\tunique=True\n\t\t)\n\tyear = db.Column(\n\t\tdb.Integer, \n"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif value:\n\t\treturn value.to_dict()\n\telse:\n\t\treturn None\n\ndef dehydrate_field_value():\n\t\"\"\"\n\tDehydrator for Field Value data.\n\t\n\t:return:\n\t\"\"\"\n\treturn None\n\nconfig = {\n\t'default': {\n\t\t'state': True,\n\t\t'enumerable': False,\n\t\t'dependencies': [],\n\t\t'dependent_action': 'default',\n\t\t'unenumerable': True,\n\t\t'actions': {},\n\t\t'options': {},\n\t"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.hydrate\n\n\ndef dehydrate_tuple(value):\n\t\"\"\"\n\t\tDehydrator for tuple data.\n\t\t\n\t\t:param value:\n\t\t:type value: tuple\n\t\t:return:\n\t\t\"\"\"\n\t\telement_arr = list(value)\n\t\telement_arr.sort(key=lambda s: float(s.replace('+','')))\n\t\treturn tuple(element_arr)\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif value.shape in (6,):\n\t\treturn '%0.3f' % value.z\n\t\t#return'%0.3f' % value + '.0 0 0 0 0 0.0'\n\tif value.shape in (2,3):\n\t\treturn '%0.3f'% value.x +'' + '%0.3f'% value.y +'' + '%0.3f' % value.z\n\tif value.shape in (5,6):\n\t\treturn'%0.3f' % value."}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.name\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nclass DataHandler(QObject):\n\t\"\"\"\n\tAbstract class for all data handler. All Data Handling\n\tclasses inherit from this class.\n\t\"\"\"\n\n\tdef __init__(self, q_datax, q_datalist):\n\t\tsuper(DataHandler, self).__init__(q_datax)\n\t\tself._q_datax = q_datax\n\t\tself._q_datalist = q_datalist\n\t\tself._id_data = 0\n\n\tdef setupData"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value._name if value.id_ == None else value._name!= value._name and \"\\\\b\" + str(value._name) + \"$\" or str(value._name)\n\n\t@property\n\tdef name(self):\n\t\t\"\"\"\n\t\t:return: The current value's name.\n\t\t:rtype: unicode\n\t\t\"\"\"\n\t\treturn self._name or str(self._name)\n\n\t@property\n\tdef id_(self):\n\t\t\"\"\"\n\t\t:return: The corresponding value's id (used for unique identifiers).\n\t\t:rtype: unic"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t# TODO fix\n\t# check to make sure all values are present - there are also '''' and '''''' so this has to be\n\t#  transformed\n\tfrom.core import Point\n\tfrom.core import Point\n\tfrom.mesh import Mesh\n\tfrom.baseutil import read_stream\n\n\n\tvalue = read_stream(value,\n\t\t\t\t\t\t  Point.TK_DATA | Point.POINT_DATA,\n\t\t\t\t\t\t  read_pointdata_type)\n\n\treturn value['Geometry_type'] == 'Point'\n\n\ndef read_v"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif isinstance(value, Point):\n\t\treturn tostring(value)\n\treturn value.tostring() if value else None\n\n@implementer(ICMCA.ICMCA)\nclass ICMCA(object):\n\n\tdef __init__(self):\n\t\t# \tTODO: The following code is not correct since the data field names are hard-coded to begin with some capital letters instead of a lower-case alias.\n\t\tself.namespaces = { \n\t\t\t\t'g': 'http://www.opengdk.org/ICMCA/"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn f'{value.type.name}-{value.x}-{value.y}'\n\n\ndef dehydrate_numpy_array(value):\n\t\"\"\"\n\tDehydrator for Array.\n\t\n\t:param value:\n\t:type value: ndarray\n\t:return:\n\t\"\"\"\n\tif type(value) in (np.ndarray, np.void):\n\t\treturn np.ascontiguousarray(value)\n\telse:\n\t\treturn value\n\n\ndef dehydrate_vector(value):\n\t\"\"\"\n\tDe"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t#if not value:\n\t#\tvalue = 0\n\treturn str(value)\n\n\t#return str(round(value) - value)\n\n\t#return value #round(value)\n\n\n@snippet\ndef build_point_serializer(point):\n\t\"\"\"\n\t\tReturn dictionary containing the serialized representation of Point.\n\t\t\n\t\t:param point:\n\t\t:type point:  Point\n\t\t:return:\n\t\t:rtype:\n\t\"\"\"\n\t#if point:\n\t#\tpoints_as_list = Point._serialized_point__point"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn utils.datetime2str(value.year), utils.datetime2str(value.month), utils.datetime2str(value.day), value.coordinate\n\n\nclass Point(object):\n\t\"\"\"\n\tPoint class, which represents a set of values as vectors.\n\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\tsuper(Point, self).__init__()\n\n\t\tif type(self).__name__ == 'Point':\n\t\t\tself._point = kwargs\n\t\t\treturn\n\n\t\tself.name = kwargs"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn [k for k in self.keys()]\n\n\t#return list(self.keys())\n\n\t# return list([self.keys()[i] for i in range(len(self.keys()))])\t\nclass KeyedHash(Hash):\n\t\"\"\"\n\t\tMakes sure hash values use the same keys.\n\t\tIf it doesn't, the keys will need be made by looking up\n\t\ta new index everytime.\n\t\"\"\"\n\n\tdef __init__(self, hash_key='', hash_type=None, dikt=dict, **kwargs):"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.__keys\n\t\n\t\nclass Record(BaseRecorder):\n\t\"\"\"\n\tRecord that keeps the latest value for it (it's always present).\n\tThere is no default value.\n\t\"\"\"\n\tdef __init__(self, records=None):\n\t\tself.__records = records or []\n\t\n\tdef __str__(self):\n\t\treturn str(self.__records[-1])\t\n\t\n\tdef insert(self, value):\n\t\tself.__records.append(value)\t\n\t\n\tdef update("}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.keys()\n\t\n\t\nclass FieldClass(object):\n\t\"\"\"\n\tField class defines a data type. Fields are constructed using\n\ta tuple of field names and their default value. The name of the value\n\tis the field name. \n\t\n\t.. note ::\n\t\tEach field class also defines the :class:`Field` class method that defines\n\t\ta data type in a given field class.\n\n\t:param cls: class of field\n\t:type cls: class\n\t:param fields: number of fields\n\t:type fields: int\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\ttry:\n\t\treturn self.getKvList()\n\texcept AttributeError:\n\t\traise Exception(\"Can not get KV list!\")\n\t\n\t\n\t\n\t\nclass KVNode(object):\n\t\"\"\"\n\t\t\tClass to define node in a record\n\t\t\t\n\t\t\tAttributes:\n\t\t\t\tnode_name: the name of the node.\n\t\t\t\tkv_name: the key-value pair of this node\n\t\"\"\"\n\t\n\tdef __init__(self, node_name, kv_name, kind, kindp):\n\t\t\"\"\"\n\t\t\n\t\t"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._keySet.copy()\n\t\n\t\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tself[\"names\"] = self._check(\"names\", list, 0, [self.name])\n\treturn self._check(\"names\", list, 0, [])\n\n\t#def name(self, x=_marker):\n\t#\ttry:\n\t#\t\tname = self._check('names', str, x, '')\n\t#\texcept:\n\t#\t\treturn None\n\t#\treturn name\n\t\n\t#@name.getter\n\t#def name(self):\n\t#\tname = self._check('names', str, _marker, ''"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._db._names.values()\n\n\t\n  def values(self):\n\t\"\"\"\n\tReturn the values of the record.\n\t\n\t:return: list of numpy arrays\n\t\"\"\"\n\treturn list(self._db._values.values())\n\n  def __len__(self):\n\t\treturn len(self._db._names)\n\n\t\n  def __getattr__(self,name):\n\t\t\ttry:\n\t\t\t\tattr = getattr(self._db,name)\n\t\t\t\tassert name == name\n\t\t\t\treturn attr\n\t\t\texcept AttributeError as e"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn list(map(getattr, self._keys_type, self._keys_type.__dict__.keys() + self._namespace.keys()))\n\n\t\ndef get(self, key, default=None):\n\tvalue = self._values.get(key, default)\n\tif self._values_type.isArray():\n\t\tif value is None:\n\t\t\tvalue = [self._values_type([]).get(k) for k in self._values_type.enum(value).keys()]\n\tif value is None:\n\t\tvalue = self._"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._keys\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.keys()\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\t\n\t\n"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandler_dict = {}\n\tif protocol_version is not None:\n\t\tfor prot in SUPPORTED_PROTOCOL_VERSIONS:\n\t\t\tif protocol_version == prot[:NAMED_ARGS_LEN]:\n\t\t\t\thandler_dict[prot] = cls\n\t\t\t\tbreak\n\telse:\n\t\tfor pro in SUPPORTED_PROTOCOL_VERSIONS:\n\t\t\thandler_dict[pro] = cls\n\n\treturn handler_dict\n\nclass BoltBaseConfig(object):\n\t\"\"\"\n\tA base for all Bolt configuration classes.\n\t\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\thandlers = {}\n\telif isinstance(protocol_version, tuple):\n\t\thandlers = {protocol_format: class_ for\n\t\t\t(protocol_format, class_) in ProtocolFormats.protocol_handler_map.items()\n\t\t\tif protocol_format in protocol_version}\n\telif isinstance(protocol_version, string_types):\n\t\tprotocol_class = ProtocolFormats.protocol_handler_map.get(\n\t\t\tprotocol_version, None)\n\t\tif protocol_class"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tsupported_version = dict(\n\t\t# 3 protocol is supported\n\t\t(0, None),\n\t\t# 5 protocol is supported\n\t\t(1, protocol_handler),\n\t\t# None protocol is not supported\n\t\t(2, lambda: None),\n\t)\n\n\tif isinstance(protocol_version, tuple):\n\t\tversion, protocol_handler = protocol_version\n\t\tif version is not None and protocol_handler:\n\t\t\treturn {version: protocol_handler}\n\t\telif protocol_handler:\n\t\t\ttry:\n\t\t\t\tversion_handler = supported_version[version][protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tclass ProtocolType(object):\n\t\t\t\"\"\"\n\t\t\t\tStores a specific protocol type (e.g. ICMP) and\n\t\t\t\tthe respective protocol/version tuple\n\t\t\t\"\"\"\n\t\t\tdef __init__(self, protocol_name, protocol_type,\n\t\t\t\tprotocol_version=None):\n\t\t\t\tself.protocol_name = protocol_name\n\t\t\t\tself.protocol = (type(protocol_type), protocol_type,\n\t\t\t\t\tprotocol_version, protocol_version)\n\t\t\t\tself.version = protocol_version\n\telse:\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_version_map = {\n\t\t'V1': BoltProtocolV1Handler,\n\t\t'V2': BoltProtocolV2Handler,\n\t}\n\tif protocol_version not in protocol_version_map:\n\t\traise TypeError(\n\t\t\t\"Invalid protocol version: %d (should be one of (%s)\"),\n\t\t\tprotocol_version,\n\t\t\ttuple(sorted(protocol_version_map.keys()))\n\t\t)\n\thandler_dict = protocol_version_map[protocol_version](cls)\n\treturn handler_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn dict(\n\t\t\t\t(protocol_version, cls) for protocol_version in cls.protocol_handlers)\n\telif isinstance(protocol_version, tuple):\n\t\tif protocol_version[0] == 0:\n\t\t\treturn dict(\n\t\t\t\t\t((version, handler), class) for version, handler in cls.protocol_handlers)\n\t\telif protocol_version[0] == 1:\n\t\t\treturn {version: cls.protocol_handlers[version].version for version in cls."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif not isinstance(protocol_version, tuple):\n\t\tprotocol_version = (protocol_version,)\n\tif not any(isinstance(version, tuple) for version in protocol_version):\n\t\treturn {}\n\t\n\tsupported_protocols = []\n\tfor (version, cls) in zip(protocol_version, cls.protocols.values()):\n\t\tif issubclass(cls, BoltProtocolBase):\n\t\t\tsupported_protocols.append(version)\n\t\t\n\tprotocol_handlers = {}\n\tcurrent"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\tif protocol_version is None:\n\t\treturn dict_handlers()\n\telif isinstance(protocol_version, tuple):\n\t\treturn {protocol_v: cls for protocol_v in protocol_version}\n\telse:\n\t\traise TypeError(\n\t\t\t\"'protocol_version' must be a tuple of 'int' elements\")\n\t\n@implementer(IBolt)\nclass BootstrapProtocol(IPableProtocol):\n\t\"\"\" Protocol to create a Bolt instance when called from\n\ta 'bomblement' command.\"\"\"\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tsupported_protocols = ProtocolHandler.supported_protocols()\n\t\treturn {\n\t\t\t\t(version[0], version[1], None): ProtocolHandler\n\t\t\t\tfor version in supported_protocols.keys()\n\t\t}\n\telse:\n\t\tsupported_protocols = ProtocolHandler.supported_protocols()\n\t\tversions = list(protocol_version)\n\t\tfor version in supported_protocols.keys():\n\t\t\tif versions[version] or versions[version[0]] > version[1]:\n\t\t\t\tversions[version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn cls.getProtocolHandlerByVersion(protocol_version,\n\t\t\t\t\t   no_reversed=True)\n\ndef protocol_version_tuple(protocol_version, *args, **kwargs):\n\t\"\"\"\n\tReturn the tuple of the provided protocol version, or None if\n\tthey don't match.\n\t\n\t:param protocol_version: tuple of the protocol versions\n\t:param *args: if given, args to protocol_version\n\t:params kwargs: if given, kwargs to protocol_version\n\n\t:return: tuple of"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\tdef inner(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tfrom neo4j.cli.meta import Meta\n\t\t\tmeta = kwargs._options.pop('meta', {})\n\t\t\tif'schema' in kwargs:\n\t\t\t\tmeta['schemaClass'] = Meta.classFromSchema\n\t\t\tif'meta' in kwargs:\n\t\t\t\tmeta.update(kwargs.get('meta'))\n\t\t\tif metadata is not None or'meta' not in kwargs:\n\t\t\t\tmeta.update({'meta': metadata.items()"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif hasattr(unit_of_work, '__call__'):\n\t\t# use the wrapped function as the decorator\n\t\t# doesn't have the keyword-only argument `func`, `args`, etc, \n\t\t# which is passed via the `kw` argument\n\t\t@unit_of_work\n\t\tasync def wrap_decorator(func):\n\t\t\treturn func\n\t\treturn wrap_decorator\n\telif not callable(unit_of_work): # try to get method from wrapped function\n\t\traise ValueError(\"Unit of work function must be callable"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\t@wraps(call_fn)\n\tasync def wrapper(call_fn):\n\t\tdef outer(tx, *args, **kwargs):\n\t\t\treturn _tx_wrapper(call_fn, tx, *args, metadata=metadata, timeout=timeout)\n\t\t\n\t\touter.__name__ = call_fn.__name__\n\t\tfunction___doc__ = None\n\t\tif isinstance(metadata, str):\n\t\t\tfunction___doc__ = \"Returns an aggregate count for a list of entities in a transaction.\"\n\t\telse:\n\t\t\tfor function, doc in metadata"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tfrom neo4j import tx\n\ttx.define_global(metadata)\n\tclass UnitOfWork():\n\t\t_value = lambda q: dbms.UNIT\n\t\tdef __init__(self, tx, metadata=None, timeout=None):\n\t\t\tself.dbms = self\n\t\t\tself.tx = tx\n\t\t\tself.timeout = timeout if timeout is not None else get_transaction_timeout()\n\t\t\tself.connect_timeout = get_connect_timeout()\n\t\t\tif metadata is not None:\n\t\t\t\t# add transaction state\n\t\t\t\tif metadata.get(\"connect_timeout"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef wrapper(f):\n\t\t@wraps(f)\n\t\tdef decorated(*args, **kwargs):\n\t\t\tmetadata = metadata or {}\n\t\t\ttimeout = timeout or settings.get(\"dbms.transaction.timeout\")\n\t\t\twith Transaction(timeout=timeout) as tx:\n\t\t\t\tres = f(*args, **kwargs)\n\t\t\t\tmetadata.update({\"tx_id\": tx._id()._uri})\n\t\t\t\tif metadata:\n\t\t\t\t\t# we don't return a dictionary\n\t\t\t\t\t# so we can take the dict from a ``Transaction`` object\n\t\t\t\t\ttx.metadata(metadata)"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\treturn calltx(__name__, \"unit_of_work\", metadata, timeout)\n\n__all__ = ['call', 'calltx']\n\nfrom.exceptions import DatabaseError, TransactionError, CypherError, UnsupportedOperation\nfrom neodc import neo4j\nfrom neodc.utils.logwrap import run_tx, log_tx, process_error\nfrom neodc.utils.config import Config\nfrom neo4j.extensions import register_transport\nfrom neo4j.v1 import Transaction\nfrom neobolt.transaction import TRANSACTION"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t# define all required parameters (if not present)\n\tif not metadata:\n\t\tmetadata = {}\n\t# call the function\n\timport transaction\n\ttransaction.init()\n\t# use the function directly as the transaction does not create the underlying store (and thus won't work when the DB is lost)\n\treturn transaction.Transaction(unit_of_work = unit_of_work, metadata=metadata, timeout= timeout)\n\n\t\n\n###################################################################################\n#\n#   CLASSES\n#\n###################################################################################\n\n\nclass Graph:\n\t\"\"\"\n\tThe class :"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\tdef wrap(func):\n\t\t@functools.wraps(func)\n\t\tdef _tx_with_kw_decorator(*args, **kwargs):\n\t\t\t\"\"\"Wraps function.\"\"\"\n\t\t\ttx = dbms.begin_session()\n\t\t\ttx_metadata, tx_timeout = meta.context(tx)\n\t\t\tif metadata is not None:\n\t\t\t\ttx_metadata = tx_metadata.attributes(metadata.keys())\n\t\t\ttx_timeout = timeout or tx_timeout\n\t\t\t\n\t\t\tif tx_timeout is None:\n\t\t\t\ttx.timeout = tx"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef set_timeout(f, *arg, **kw):\n\t\ttry:\n\t\t\tif timeout:\n\t\t\t\tkw['timeout'] = timeout\n\t\texcept KeyError:\n\t\t\tpass\n\t\treturn f(*arg,**kw)\n\treturn set_timeout"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t# get the decorator class\n\tcls = TransactionDecorator\n\t# if no @unit_of_work is given, call the constructor\n\tif metadata is None:\n\t\treturn cls(**get_options_for_unit_of_work(timeout))\n\telse:\n\t\treturn cls(*get_options_for_txmeta(metadata, timeout))\n\n__all__ = [\"TransactionDecorator\", \"get_options_for_txmeta\", \"get_options_for_unit_of_work\"]\n"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\traise NotImplementedError()\n\t\t\n\t\n\tdef append(self, key):\n\tappend_count += 1\n\tself.dict[key] = append_count\n\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t# TODO: optimize\n\ttry:\n\t\treturn self._heap.search(key)\n\texcept KeyError as e:\n\t\treturn -1\n\n\t\nclass BinaryHeap:\n\t\"\"\"An implementation of a binary heap class.\"\"\"\n\tdef __init__(self):\n\t\tself._heap = []\n\t\tself._length = 0\n\t\t\n\tdef __len__(self):\n\t\t\"\"\"Return the number of elements in the heap.\"\"\"\n\t\treturn self._length\n\t\t\n\tdef add(self, key):\n\t\t\"\"\"\n\t\tHeapify the"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.index_by_name(key)\n\n\t\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.index(self._find_item[key], key)\n\t\nclass BST:\n\t\"\"\"Binary Search Tree\"\"\"\n\t\n\t# constructor\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tconstructor\n\t\t\"\"\"\n\t\tself._root = None\n\t\t\n\t# insert function\n\tdef insert(self, key):\n\t\t\"\"\"\n\t\t:param key: a key\n\t\t:return: void\n\t\t\"\"\"\n\t\tif self.size >= self._leaf_size:\n\t\t\tprev = self._insert(key)\n\t\t\tif key == self._head"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\ttry:\n\t\treturn self._index(key)\n\texcept KeyError:\n\t\traise RuntimeError()\n\texcept TypeError:\n\t\treturn 0\n\n\treturn 0\n\n\t# return self.root.__index__(key)\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.__map.index(key)\n\n\t\n\t\n\t\n\t\n#---------------------------------------------------------------------------------------------------#\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  "}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self._hash[key]\n\t#TODO: Implement error handeling\n\t# raise NotImplementedError()\n\nclass MyHashMap(object):\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize your data structure here.\n\t\t\"\"\"\n\t\t\n\t\tself.map = {}\n\t\tself.used = {} #hashable\n\n\tdef put(self, key, value):\n\t\t\"\"\"\n\t\tvalue will always be non-negative.\n\t\t:type key: int\n\t\t:type value: int\n\t\t:rtype: None\n\t\t\"\"\""}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.map[key]\n\n#\n# Problem #3: Consecutive Numbers\n#\n\ndef consecutive_numbers(self, low: int, high: int) -> int:\n\t\"\"\"Finds the number of consecutive numbers in a number series.\n\t\n\tA number series is a sequence of positive integers,\n\tsuch as 3-6, 5, 3, 6, 6, 7.\n\t\n\tFor example:\n\n\t>>> consecutive_numbers(0, 1, 2, 3, 4)\n\t2\n\t>>> consecutive_numbers(1,"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.indexArray.index(key)\n\n# The key is not contained in the array\n\tif key not in self.indexArray:\n\t\tself.indexArray.append(key)\n\treturn self.indexArray.index(key)\n\n# The index can not overwrite it, and the new item will have the new index, so the first time we create a new array, the new index array will be the same\n# To modify the new index array, we insert the new index array at the beginning, then overwrite the key\n# The key can be deleted in the new index array"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tind = 0\n\tfor i in self:\n\t\tif i[0] == key:\n\t\t\tind = self.t[ind]\n\t\t\tbreak\n\treturn ind\n\n\n\t# print(f'index (', key, ') : ', self.index(key))\n\t# return self.t[self.index(key)]\n"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys is None:\n\t\treturn self.raw[:1]\n\telse:\n\t\treturn self.raw[:keys]\n\nclass ColumnValue(tuple):\n\t\"\"\"\n\tA tuple class allowing column access to values.\n\t\n\tOnly provide the attributes that are specific for this structure:\n\t\n\t:name: name of the column\n\t:data_type: type of the data in this column, either string,\n\t\t\t\t   number, list, tuple, dictionary, or Record\n\t:size: space on the left (not including )\n\t:size_left"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t# Check that all items are indexes or keys\n\tif len(keys)==0: keys = range(*self.keys())\n\telse: keys = keys[:]\n\n\t# Construct the search expression\n\tkey_args = []\n\tfor key in keys: key_args.append('(?P<%s>%s)' % (key,self.getKeyArgs(key)))\n\t\n\texpression = '(?P<%s>%s)' % ('|'.join(key_args), self.getValuePattern() )\n\t\n\t# Return all values\n\t"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn self.connection()._db.dse.values(self, keys)\n\t\n\t\nclass _SearchResult(object):\n\t\"\"\"\n\t\tSearchResult Object for DSE\n\t\n\tInherit this class for each search type and define\n\tmethods to fetch data or raise exceptions.\n\t\n\tMethods not implemented here. See below for\n\tfunction definitions.\n\t\n\tThe return value will be a list of dictionaries; each\n\tdictionary may contain one or more fields and will contain\n\ta'search_type' field in the entry associated with the field"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn self.read(0, None, [(key, None if key == key else self.get_value(key)) \\\n                                for key in keys]) if keys else self.get_values()\n\n\tdef values_s(self, *keys):\n\t\t\"\"\"\n\t\tSame as :meth:`values`, but returns a generator\n\t\t\"\"\"\n\t\treturn self.values().__iter__()\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tres = self.con.execute(\"SELECT %s FROM %s %s WHERE %s\" % values_query(self.con, self.cur, values_table(), keys))\n\treturn map(lambda row: row[0], res)\n\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tself._checkIndex()\n\treturn self._values(*keys)\n\n\t\n\t#self._checkIndex()\n\t#return [self.Get(a) for a in self]\n\t\n\treturn self._values()\n\n\t\n\t#self._checkIndex()\n\t#return list(set(self) & set(self._values(*keys)))\n\n\t\n\t#return map(self.Get, self._values(*keys)\n\n\n\nclass Entity(_Property):\n\t\"\"\"Provides special behavior for the underlying entity that\n\tmakes it possible to find values using the corresponding field"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn self.get(values=keys).get(\"values\")\n\t# return self.field(\"values\", values=keys)\n\t# return self.row(key=\"values\", values=keys)\n\ndef get(self, *keys):\n\t'''Return the value(s) of specified key(s) from record.'''\n\tif len(keys) > 1:\n\t\traise TypeError(\"get multiple\")\n\twhile keys:\n\t\tself = self[keys[0]]\n\t\tkeys = keys[1:]\n\treturn self\n\ndef first(self, field=\""}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tvalues = [v[0] for v in self._get()]\n\tif len(keys):\n\t\tvalues = [v for v in values if (v[index] in keys)]\n\treturn sorted(values)\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tresult = self.getvalues( *keys )\n\tfor k in result:\n\t\tresult[k][:] = [ result[k] ]\n\treturn result\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tresult = []\n\tfor s in self.str_get(*keys):\n\t\tresult.append( (s, getattr(self, s)) )\n\treturn result\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\td = {}\n\tfor key in keys:\n\t\tvals = self._get(key)\n\t\tif vals:\n\t\t\td[key] = vals\n\t\telif not key in d:\n\t\t\td[key] = None\n\treturn d\n\nclass Property:\n\t\"\"\"\n\tMaintain a list of Properties (with :class:`list` as the underlying data\n\tstructure), and can be updated, searched, and deleted using the methods\n\tof :class:`Property`. With no additional functionality, attributes and\n\tfield configuration are typically provided directly using a named"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif keys:\n\t\tresult = {}\n\t\tfor key in keys:\n\t\t\ttry:\n\t\t\t\tresult[key] = self[key]\n\t\t\texcept IndexError:\n\t\t\t\tresult[key] = None\n\t\treturn result\n\telse:\n\t\tit = iter(self)\n\t\tresult = {}\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tvalue = next(it)\n\t\t\texcept StopIteration:\n\t\t\t\treturn result\n\t\t\ttry:\n\t\t\t\tfield = _get_field(key, it)\n\t\t\texcept AttributeError:\n\t\t\t\tfield = None\n\t\t\tif value is None"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tpass\n\tif type(keys) in (tuple, list):\n\t\treturndict = {}\n\t\tfor key in keys:\n\t\t\tif key in self.data:\n\t\t\t\tself._check_valid_index(key)\n\t\t\t\treturndict[key] = self.data[key]\n\t\t\telse:\n\t\t\t\treturndict[key] = None\n\t\treturn returndict\n\telse:\n\t\tkey = keys\n\t\tif key in self.data:\n\t\t\tself._check_valid_index(key)\n\t\t\treturn self.data[key]\n\t\telse:\n\t\t\treturn None"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tres = dict([ (a,None) for a in keys ])\n\t# handle indices\n\tfor key, val in self.items():\n\t\tif isinstance(key, str):\n\t\t\ttry:\n\t\t\t\tindex = int(key)\n\t\t\texcept ValueError, e:\n\t\t\t\traise IndexError(\"bad index: %s\" % repr(key))\n\t\telse:\n\t\t\t# a numeric index\n\t\t\tindex = key-len(keys) # start at zero\n\t\tres[index] = val\n\treturn res\n\t\n# def data(self, *keys):\n# "}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tdata = self.dict(self._asdict_items(False), sort_keys=True)\n\n\tif keys:\n\t\tresult = data.get() or {}\n\t\tif type(data) is not dict:\n\t\t\t# not a dict: we must have passed a list\n\t\t\ttry:\n\t\t\t\tresult['items'] = [ item[keys[0]] if key in keys \\\n\t\t\t\t\t\t\t\telse {\"index\": self2, \"key\": key} \\\n\t\t\t\t\t\t\t\tfor item, index, key in \\\n\t\t\t\t\t\t\t\tmap ]\n\t\t\texcept IndexError:\n\t\t\t\tresult['items'] = []\n\t\t\t\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn self.dict.update((key, values.data) for key, values in keys)\n\ndef update(self, items, *args, **kwargs):\n\t\"\"\"\n\tUpdate this record.\n\t\n\t:param items: dictionary of field names to values of the\n\t              records to update. The new value or set of\n\t              fields will be set from all the values\n\t:param args: parameters for the :meth:`fields` of the\n\t              :attr:`record` instance\n\t:param kwargs: parameters to take on the field"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tfrom.recordset import FieldInfo\n\tinfo = FieldInfo(keys) if keys else self[0]\n\treturn dict(zip(info.index_field, info))\n\ndef fields(self, *keys):\n\t\"\"\"\n\tReturn the keys and values of this recordset as a dictionary of\n\tfield names and values. If no key is provided, then all values\n\twill be included. Keys provided in the items that are not in the\n\trecord will be inserted with a value of :const:`None`; indexes\n\tthat are out of bounds will trigger a :exc"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\tif keys:\n\t\trecord = self.records\n\telse:\n\t\trecord = self.__class__(self.records)\n\t\t\n\tif keys is not None:\n\t\tfor key in keys:\n\t\t\tif key in record.data:\n\t\t\t\trecord.data[key]\n\t\t\telse:\n\t\t\t\trecord.data[key] = None\n\n\treturn record.data\n\n\t\ndef data_key(obj):\n\t\"\"\"\n\tReturns a dictionary containing data access for object instances,\n\tin the same key format as the JSON keys.\n\t\n\t:"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tret = {}\n\tfor name, field in self.fields.items():\n\t\tisout = name in keys\n\t\tif not isout and isinstance(index(field), list):\n\t\t\tindex(field).append(None)\n\t\tret[name] = field.data(index(field))\n\treturn ret\n\nclass ReferenceRecord(Record):\n\t\"\"\"\n\tA collection of data, mapped by reference identifier, with any\n\treferenced object and relationships to other objects\n\t\"\"\"\n\t\n\tdef __init__(self, xref, name=None, obj="}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\td = dict()\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\td[key] = self[key]\n\t\telif key in self.fields:\n\t\t\tsetitemfn = self.getitem\n\t\t\tif isinstance(key, Hashable):\n\t\t\t\tkey = Hashable(key)\n\t\t\ttry:\n\t\t\t\td[key] = setitemfn(self, key)\n\t\t\texcept IndexError:\n\t\t\t\t# key is out of bounds\n\t\t\t\te = IndexError(key)\n\t\t\t\te.index = len(self)\n\t\t\t\texception"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack_from('>H', self._buffer, self._position + 1)[0]\n\n\tdef pop_u32(self):\n\t\t\"\"\"\n\t\tRemove the last four bytes of data, returning them as a big-endian\n\t\t32-bit unsigned integer.\n\t\t\"\"\"\n\t\treturn struct.unpack_from('>i', self._buffer, self._position + 4)[0]\n\n\t@property\n\tdef position(self):\n\t\t\"\"\"\n\t\tReturns an integer representing the position of the data in self._buffer\n\t\t\"\"\"\n\t\tif self._buffer"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tpop_b16(self.data, 2)\n\treturn struct_uint16.unpack_from(self.data, 0)\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\t\n\t\t\n\t\t\n\n\n\t\n"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tif (16 * self.iN).bit_length() < 8:\n\t\ta.pop_u16()\n\ta.pop_u16()\n\tself.iN = 0\n# end pop_u16\n\n# byte pop\n\tself.iN = self.iN << 8\n\tself.iN = a.pop_u16(self.iN)\n\ta.pop_u8()\n\n# end byte pop\n\n# bit pop\n\tself.iN = self.iN << 8\n\tself.iN = a."}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tpop = _unpack(\"!H\", self._packet.body[self._pkt_remain - 2:self._pkt_remain - 7])\n\tpop_data = _unflatten(pop, [\"H\", \"L\"])\n\tself._packet.body = pop_data\n\tself._pkt_remain = self._pkt_remain - 7\n\tself.data_pointer = self.get_next_payload() + 1\n\treturn pop_data[0]\n\n\nclass L(Packet):\n\tL = None"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tresult = bytearray()\n\tsize += 2\n\twhile size > 0:\n\t\tresult.append(getchar_ex[size >> 8 | 0x80].decode())\n\t\tresult += getchar_ex[size >> 8 | 0x80]\n\t\tif (size & 0x80)!= 0:\n\t\t\tresult.append(getchar_ex[size >> 8 | 0x80].decode())\n\t\tsize >>= 8\n\t\tif size:\n\t\t\tsize >>= 8\n\t\telse:\n\t\t\tbreak\n\treturn result[:-2]\n\n\nclass"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t(int_out, int_len, _) = struct.unpack(\"!II\", self.data[:4])\n\tself.data = self.data[4:]\n\tif len(self.data) == int_len:\n\t\tint_out = int_out + (b\"\",) * (8 - int_len)\n\treturn int_out\n\n# ============================================\n# Classifies a list of integers as\n#   a sublist of the maximum integer\n#   represented within.\n#\nclass ClassifyInt(object):\n\n\tdef __init"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack('H' * 2, self.read(2))[0]\n\ndef read_s16(self):\n\t\"\"\"\n\tRead one byte from ``self``, returning it as a two-byte\n\tsigned 16-bit character. This function is part of the over-\n\tfeeder protocol, so you must have received it.\n\t\"\"\"\n\tself.read(1)\n\treturn struct.unpack('H' * 1, self.read(1))[0]\n\nclass overfeed(object):\n\t__slots__ = ('"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack('>H', self._buffer[self._ptr + 4:self._ptr + 8])[0]\n\n\t\nclass Cstring(object):\n\t\"\"\"\n\t\tEncapsulate a character into a bytes object.\n\t\"\"\"\n\tdef __init__( self, c ):\n\t\tself.c = ord(c)\n\n\tdef __getattr__( self, attr ):\n\t\tif attr == '__str__':\n\t\t\treturn self.c\n\t\telif attr == 'end':\n\t\t\treturn self.c + self._ch\n\t\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tu16 = (self.__payload[:2] << 8 | self.__payload[2:4])\n\treturn u16\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack(\">H\", self.data[-2:])[0]\n\n\nclass ImageHeader(BitmapItem):\n\tdef getBounds(self, aspect, orientation=BOX):\n\t\tif orientation == HORIZONTAL:\n\t\t\treturn (0, 4, 0, 4)\n\t\telif orientation == VERTICAL:\n\t\t\treturn (0, 0, 4, 4)\n\t\telif orientation == HORIZONTAL and aspect > 0.25:\n\t\t\treturn (0, 0, 4, 24)\n\t\telif orientation == HORIZ"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tq = self.q.appender(qid, n)\n\tself.packstream.handlers.append(q)\n\tif dehydration_hooks is not None:\n\t\tfor type_name, dehydration_hook in dehydration_hooks.items():\n\t\t\tq.add_dehydration_hook(qid, type_name, dehydration_hook)\n\n\tif hydration_hooks is not None:\n\t\tfor type_name, hydration_hook in hydration_hooks.items():\n\t\t\tq.add_hydration_hook"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tfrom copy import deepcopy\n\tfrom pyhton.packstream import BaseHandler\n\tclass _Handler(BaseHandler):\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tsuper(_Handler, self).__init__(*args, **kwargs)\n\t\t\tself.type = \"_handler\"\n\t\t\tself.state_dict = dict((x, y) for (x, y) in self.dispatch_map.items())\n\tclass HydrideHandler(_Handler):\n\t\t@staticmethod\n\t\tdef hydrate_value(data, type, **kwargs"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\t\tif qid < 0:\n\t\t\tqid = len(self._outstanding_qids) + 1\n\t\t\n\t\t# Build the header\n\t\tif n < 0 and not (key := self._stream.recv()):\n\t\t\traise Exception()\n\t\theader_size = struct.calcsize(\"!I\")\n\t\tif n < 0:\n\t\t\theader = \"DISCARD \" + key[:-1]\n\t\telif key:\n\t\t\theader = \"%s %s\" % (key[:-1], key[-1])\n\t\telse:\n\t\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdef add_to_result(self, response, value, qid=-1, hydration_hooks=None):\n\t\tresult = Response()\n\t\tself.messages.append(result)\n\t\tresult.qid = qid\n\t\tresult.type = self.hydration_class(dehydration, value, hydration_hooks)\n\t\tresult.dehydration = dehydration.get(result.type, None)\n\t\t\n\t\tif not result.dehydration:\n\t\t\traise Exception(\"Empty dehydration function %r\" % dehydration.get"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdef __init__(self, n=-1, qid=-1, dehydration_hooks=None, hydration_hooks=None, handlers=None):\n\t\tself.discarding = True\n\t\tself.n = n\n\t\tself.qid = qid\n\t\tself.hydration_hooks = hydration_hooks or {None:hydration_hooks}\n\t\tself.discard_hooks = dehydration_hooks or {None:dehydration_hooks}\n\t\tself.handlers = handlers or []\n\"\"\"\n\tHelper function to add to"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\t\thd_record = HydrationRecord(n, qid)\n\t\tself.__hydr_queue[hd_record.type] = hydration_hooks.get(hd_record.type,\n\t\t                                                         dehydration_hooks.get(n))\n\t\tself.__pack_queue[n].insert_data(hd_record)\n\t\tself.__update_records(hd_record)\n\t\treturn hd_record\n\n\tdef discard_all(self):\n\t\twhile True:\n\t\t\trecord = self"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\t\t\"\"\" Adds an item to the output queue that only takes care of\n\t\t    discarding the item as soon as the object is garbage\n\t\t    reflected.\n\t\n\t\t:param n: record number\n\t\t:param qid: query ID\n\t\t:param dehydration_hooks:\n\t\t    dehydration hooks dict returned by hydration functions.\n\t\t    Assumes that for objects that have been dehydrated, only\n\t\t    the type of object must be passed.\n\t\t:param hydration_hooks:\n\t\t    hydration hooks dict returned by hydration functions. Ass"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\t\tif n == -1:\n\t\t\tn = packstream.pack_sequence(packet=PACKET(\n\t\t\t\tPING, packet_id=qid, data_type=packet.DATA_TYPE_LENGTH\n\t\t\t)).data\n\t\tpcm = packstream.pcm(n=n + 1)\n\t\tpcm += Packet(PING, data_type=packet.DATA_TYPE_LENGTH,\n\t\t\tpacket_id=packet.PACKET_ID_LENGTH, payload_length=1)\n\t\tpcm +="}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t).filter(DEHIRESTABLES.QUERY_ID == qid, :type) as rdf:\n\t\trdf.filter(Q.resource == self._resource)\n\t\tif hydration_hooks is not None:\n\t\t\trdf.filter(Q.types!= DEHIRESTABLES.HOOK, :type)\n\t\t\tfor (type_id, dehydrate_func) in hydration_hooks.items():\n\t\t\t\trdf.filter(Q.types[type_id] == DEHIRESTABLES.HOOK, dehyd"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\t\t\n\t\tif qid < -1:\n\t\t\tqid = int.__new__(self.__class__, self.id)\n\t\tif n < 0:\n\t\t\tn = self.size\n\t\tif qid < 0:\n\t\t\tqid = self.last_query\n\t\telif qid < self.last_query:\n\t\t\tqid += 1\n\t\telif qid == self.last_query:\n\t\t\tqid -= 1\n\t\t\n\t\tif qid >= len(self._hubs):\n\t\t\treturn None\n\t\t\n\t\tif n"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t)\n\n\t# check for the given type\n\tif classname not in self.protocol2class:\n\t\traise BoltError('No such class %s.' % classname)\n\t\treturn None\n\n\ttransaction_type = self.protocol2class[classname](self, *args, **kwargs)\n\n\t# check if we want to have a bookmark\n\tif transaction_type == Bookmark:\n\t\t# if bookmark is a class instance or is a tuple (class, bookmark-name,...)\n\t\tif not isinstance(bookmarks, dict):\n\t\t\traise Bolt"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tpass)\n\n\ndef close(self, bookmarks=None, timeout=None):\n\t\"\"\"\n\t\tCloses a transaction and returns the number of bytes returned.\n\t\n\t:param bookmarks: iterable of bookmark values after which this transaction should\n\t    begin (default is all bookmark values)\n\t:param timeout: timeout for transaction execution (seconds)\n\t\"\"\"\n\tif timeout == 0:\n\t\ttimeout = DEFAULT_TIMEOUT\n\treturn self.push('DELETE', ['BEGIN', 'END'], timeout=timeout)\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\treturn self.session.send('BEGIN', mode=mode, settings={\n\t\t'timeout': time.time() + timeout if timeout else None,\n\t\t**db if db else {\n\t\t\t'db': db\n\t\t}\n\t},\n\timp_user=imp_user, dehydration_hooks=hydration_hooks,\n\thydration_hooks=dehydration_hooks, handlers=handlers)\n\n\n@get_session\ndef end(self, mode=None, bookmarks=None, metadata=None, timeout=None, db=None,\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t# add a bogus value to the stream (note that the 'bogus' atom is still sent)\n\t# it contains an empty string to be properly initialized by BoltStreams\n\tpackstream_stream.begin.extend([None])\n\tpackstream_stream.begin.extend([bogus])\n\n\tif not mode:\n\t\tmode = \"READ\"\n\tif not bookmarks:\n\t\tbookmarks = [None]\n\n\treturn packstream_stream.begin(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout,\n\"\"\"\n\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t)\n\n\treturn self.exec(\n\t\tmessage=HttpPacket(\n\t\t\tname='BEGIN',\n\t\t\tmode=mode,\n\t\t\tbookmarks=bookmarks,\n\t\t\tmetadata=metadata,\n\t\t\ttimeout=timeout,\n\t\t\tdb=db,\n\t\t\timpersonation_user=imp_user,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks,\n\t\t\thandlers=handlers,\n\t\t),\n\t)\n\n\ndef begin(self, mode='READ', bookmarks=None, metadata"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t):\n\t\tself._db = db\n\t\tself.app = dbc.getApp(\n\t\t\tdb=db,\n\t\t\tuser=imp_user,\n\t\t\thandler=hydration_hooks,\n\t\t\tdehydration=dehydration_hooks,\n\t\t\tdeprecated=Deprecated\n\t\t)\n\t\t# If the passed 'handlers' dictionary is not the same as the\n\t\t# 'deprecated' dictionary, convert the deprecated dict to the\n\t\t# new one. If different, make a new one\n\t\tif 'handlers' in deprecated and type(handlers)!="}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t) -> bytes:\n\t\treturn self.transaction_request(\n\t\t\tmode=mode,\n\t\t\tbookmarks=bookmarks,\n\t\t\tmetadata=metadata,\n\t\t\ttimeout=timeout,\n\t\t)(self.BeginHook(\n\t\t\tdb,\n\t\t\t# This can get updated by Bolt 4.0+, when the database is updated\n\t\t\t# We add these here for performance purpose only\n\t\t\tim_user=im_user,\n\t\t\tdehydration_hooks=hydration_hooks,\n\t\t\tbegin_hooks=self.BeginHookHook,\n\t\t\tdehydration_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t, name=\"begin\"\n\t, args=dict(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=\"mysql\",\n\t           impersonate=True, dehydration_hooks=dehydration.get_dek_dehydration,\n\t           hydration_hooks=hydration.get_dehydration_hook_fn(dehydration),\n\t           handler=dehydration.register_dehydration_handler,\n\t           ).split()\n\"\"\")\n\n\t__stmt_begin = __stmt(\"transactions"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tfh = handlers.__get__(self, type(self))\n\tif db:\n\t\tif not importlib.util.find_spec(db) is None:\n\t\t\tdep = importlib.util.find_spec(db)\n\t\t\tdep_name = dep.parts[-1]\n\t\t\tmbs = importlib.util.find_spec(mbs[0])\n\t\t\tmbs_name = mbs.parts[-1]\n\t\t\tif (dep_name, mbs_name) in mbs_hooks:\n\t\t\t\t# we got a mbs"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tmsg = S(\n\t\tBEGIN,\n\t\tmode=mode,\n\t\tbookmarks=[self.bookmark()],\n\t\tmetadata={'_dehydrator': self._dehydrator},\n\t\ttimeout=timeout,\n\t\tdb=db,\n\t\timp_user=imp_user,\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\thandlers=handlers\n\t).dumps()\n\t\n\tresponse = self._conn.transport.send_message(msg)\n\t"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 3.5:\n\t\treturn round_half_to_even(n - 3.5) + 1\n\telse:\n\t\treturn round_half_to_even(n)\n\ndef find_nearest_interval(a, b):\n\t\"\"\"\n\tFind the smallest interval in the given list satisfying the given interval limits.\n\n\t:param data:\n\t:param limits:\n\t:return:\n\t\"\"\"\n\tif limits == None:\n\t\treturn\n\tlow = a - limits[0]\n\thigh = b - limits[0]\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn int(n) - int(n % 2)\n\n# ---------------------------\n# main\n\ndef main():\n\t\"\"\"\n\t>>> main()\n\t\"\"\"\n    # Read lines of input from stdin and save to list lst_lines\n    lst_lines = read_lines()\n\n    # get_max_n and get_min_n are used to determine how many numbers is needed\n    # to complete the square in each group of 3 or 4\n    get_max_n(lst_lines)\n    get_min_n(lst_lines"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n % 1 == 0:\n\t\treturn n\n\telse:\n\t\treturn n + 1\n\n\n# -------------------------------------------------\ndef count_divisors(n):\n\t\"\"\"\n\t>>> count_divisors(16)\n\t8\n\t\"\"\"\n\tdiv = 0\n\tfor el in range(0, n):   # range(el+1, n)\n\t\tif n % el == 0:\n\t\t\tdiv += 2\n\treturn div\n\n\n# =====================================================\n\ndef lcm(a, b):\n\treturn a * b // gcd(a"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n in (1, 2, 3, 12, 13):\n\t    return round_half_to_even(1/2)\n\telif n in (4, 6, 9, 11, 40, 50, 70, 100):  # [4, 6, 9, 11, 40, 50, 70, 100]\n\t    return round_half_to_even(4*(3**0.5))\n\telif n in (8, 13):\n\t    return round_half_to_even(8*(3**0.5))\n\telse"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n == 0:\n\t\treturn 0\n\n\tround_half = n // 0.5\n\tremainder = n % 0.5\n\treturn round_half, remainder\n\t\n\ndef is_power_of_two(n):\n\ti = 1\n\twhile n >= i:\n\t\tif n % i == 0:\n\t\t\treturn False\n\t\ti += 2\n\n\treturn True\n\n\ndef smallest_pair(k, arr):\n\t\"\"\"\n\t>>> smallest_pair(2, [2, 2, 2, 1, 1])\n\t1\n\t>>>"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn round((n-0.5)*2)\n\n\ndef mean(numbers):\n    \"\"\"\n\t>>> mean([6, -4, -9, -3, 5, 5])\n\t6\n\t:param numbers:\n\t:return:\n\t\"\"\"\n\treturn sum(numbers)/len(numbers)\n\n\ndef var(numbers):\n    \"\"\"\n\t>>> var([6, -4, -9, -3, 5, 5])\n\t9.5\n\t:param numbers:\n\t:return:\n\t\"\"\"\n\tvariance=0"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\twhile n >= 5:\n\t\tn -= 5\n\t\tprint(n)\n\n\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif isinstance(n, float):\n\t\treturn round(n, 1)\n\treturn n\n\n\nprint(round_half_to_even(2))\nprint(round_half_to_even(2.2))\nprint(round_half_to_even(2.5))\nprint(round_half_to_even(2.7))\nprint(round_half_to_even(4.3))\nprint(round_half_to_even(4.6))\nprint(round_half_to_even(4.7))"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 6:\n\t\treturn n//2\n\t\n\treturn (6*n-1 ) // 2\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif isinstance(n, (int, float)):\n\t\traise TypeError(\n\t\t\t\"Unsupported type for argument 'n', use int or float.\")\n\telse:\n\t\tn = int(n)\n\t\tif n < 2:\n\t\t\traise ValueError(str(n) + \" is invalid.\")\n\t\telse:\n\t\t\treturn n // 2\n\n\ndef find_last_invalid(x, nums):\n\t\"\"\"\n\t>>> find_last_invalid(10, [1, 6, 2, 20, 5, 3, 10, 56, 4])\n"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tname = name.string.encode('utf-8')\n\tfrom django.contrib.gis.geos import Point\n\tobj = Point(Point=Point, srid=srid_map.srid_string, geom_type='Point', name=name, nsphere=False) \n\tobj.__class__ = Point\n\tfields['Point'] = _s2c(obj)\n\treturn obj\n\ndef point_box_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically create a Box3D"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tclass Point(sqlite3.Row):\n\t\t__slots__ = ('id', 'lon', 'lat','srid', 'type', 'point_value')\n\n\tdef pt_name(name, fields, srid_map):\n\t\t# This should be overridden by classes that aren't subclassed.\n\t\t# In case you want to create Point subclass instances from the Point\n\t\t# class, you can override the function with a custom name\n\t\t# and assign to the Point instance an attribute.\n\n\t\t# A little problem when using an SQLite class with a Point.\n\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t# Define new class.\n\tclass point(object):\n\t\tpoint_id = 1\n\t\tpoint_type = 1 # Class attribute.\n\tdef __init__(self):\n\t\tif self.point_type == 1:\n\t\t\tself.type = 'Point'\n\t\telse:\n\t\t\tself.type = 'Polygon'\n\t\tself.points = []\n\n\tdef add_point(self, pt):\n\t\tself.points.append(pt)\n\t\treturn self.point_type\n\n\tdef delete(self, p_id):\n\t\tself.points.remove"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t# Check field name, type, and dimension\n\ttry:\n\t\tpoints_field_name = name + \"_fields\"\n\t\tfield = fields[points_field_name]\n\texcept KeyError:\n\t\traise Exception('Invalid coordinate system.')\n\n\ttry:\n\t\tfield_type = field.type\n\texcept AttributeError:\n\t\traise Exception(f\"Type of {name} attribute is '{field.type}' in {points_field_name} object.\")\n\n\tif srid_map is not True: # Check srid_map value.\n\t\t# If"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tFields = list(fields)\n\n\tassert False, \"Not implemented yet\"\n\n\tif fields.index(name) == 0:\n\t\tFields.pop(0)\n\telif fields.index(name) == len(fields) - 1:\n\t\tFields[0] = int(fields[0])\n\n\tif name == \"time\":\n\t\tattr = f\"_{name}\"\n\telse:\n\t\tattr = f\"_{name}_\"\n\n\tclass Point(Base):\n\t\tdef __init__(self, **kwargs):\n\t\t\tself.__dict"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tclass Point(SerializerType.CXGeomField()): \n\t\tgeometry = None\n\t\ttype_id = \"point\"\n\t\tname = name\n\t\tnum_srid = 0\n\t\tsrid = 1\n\t\tdef get_obj(self, id_, name_=None):\n\t\t\tif name_:\n\t\t\t\tcx, cy = Point.parse_srid(self, id_)\n\t\t\t\treturn Point(self.get_srid(id_), cx, cy, self.get_num_srid(id_, name_))\n\t\t\telse"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpoint_class = None\n\tfor kls in geom_classes:\n\t\tif name == kls.__name__:\n\t\t\tpoint_class = kls\n\t\t\tbreak\n\tassert point_class is not None,\\\n\t\t\"Cannot find class '%s' from dictionary.\" % name\n\tif point_class.is_valid(fields):\n\t\t_, _ = point_class(**fields)\n\telse:\n\t\traise ValueError(\"Geometry %r of class %s not recognized.\" % \\\n\t\t\t(name, point_class.__name__))"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\timport pyproj\n\tpoly = fields['poly']\n\tif not srid_map:\n\t\tsrs = fields['srid']\n\t\tsrid_map = {srs.srid}\n\tfrom pyproj import Proj, transform\n\tproj = Proj(proj='utm', ellps='WGS84')\n\n\tdef _get_proj4(v):\n\t\t\"\"\"Return proj4 string for the given vector.\"\"\"\n\t\tif len(v) == 1:\n\t\t\tp = (list(v) * 0.5 if isinstance"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif len(fields) >= 3:\n\t\tname = fields[0].strip().split(':')[0].lower() + '_' + fields[0].lower().split(':')[1]\n\telse:\n\t\tname = fields[0].strip().split(':')[0].lower() + '_' + fields[0].lower()\n\t\t\n\tif len(fields) > 2:\n\t\tname += '_' + fields[2].strip().split(':')[0].upper() + '_' + fields[2].strip().split(':"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tmapping = {}\n\tfor row in fields:\n\t\tfields[row].pop('db_column', None)\n\t\ttable, column = row[0].lower().split(' ')\n\tfor row in fields:\n\t\tmapping[row[0]] = {\n\t\t\t'value': 'geopy.point._types.%s_value' % table,\n\t\t\t'data_type': 'geopy.point._types.%s_data_type' % table\n\t\t}\n\t\n\tgeo_class = type(\n\t\t\"geopy.point.%s"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tif message[:len(message)-3]==\"'foo' has\" or message[:len(message)-3]==\"'bar'\":\n\t\tassert '-' not in message, 'Only one \"-\" (bar) in usage message'\n\t\tif message.split('in')[0][len('deprecated'):]==message:\n\t\t\treturn message\n\t\tassert message.split('in')[0][:len('deprecated')]==message, 'Using deprecated class, and passing in an invalid value to the constructor.'\n\t\tassert message.split('in')[1][:"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\twarn(message)\n\tdef new_func(*args, **kwargs):\n\t\twarn(\"'%s' has been deprecated in favour of '%s'\", message.strip(), message.strip())\n\t\treturn args[0]\n\tnew_func.__name__ = message.strip()\n\tnew_func.__doc__ = None\n\treturn new_func\n\ndef deprecated_method(deprecation_cls=Deprecated, message=None,\n\t\twarning_message=None, deprecation_message=None):\n\t\"\"\"\n\tDecorator that"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef _deprecated(func):\n\t\tif func.__doc__:\n\t\t\tif message:\n\t\t\t\tfunc.__doc__ = \"%s\\n\\n%s\" % (message, func.__doc__)\n\t\t\t\tfunc.func_doc = func.__doc__\n\t\t\telse:\n\t\t\t\tdel func.func_doc\n\t\treturn func\n\treturn _deprecated\n\t\n\t\ndef deprecated_kwargs(**kwargs):\n\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t\tKeyword arguments in the decorated function correspond to keys"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t__function_has_deprecated = False\n\t\n\tdef deprecated_function(func):\n\t\tif __function_has_deprecated:\n\t\t\traise DeprecationWarning(message=message)\n\t\tglobals()[func.__name__] = func\n\t\t__function_has_deprecated = True\n\t\treturn func\n\treturn deprecated_function\n\t\n\t\n@decorator_factory(__name__)\ndef deprecated_functions(name, *args, **kwargs):\n\traise DeprecationWarning(message=f'{name} was"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef deprecationwrapper(wrapped):\n\t\t\"\"\"Deprecate a function and method.\"\"\"\n\t\t\n\t\t@wraps(wrapped)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"Replace a function.\"\"\"\n\t\t\n\t\t\twarnings.warn(message)\n\t\t\traise DeprecationError(\"{0}\".format(wrapped.__name__),\n\t\t\t                    \"{0} has been deprecated in favour of {1}\".format(\n\t\t\t\twrapped.__name__, \"bar\"), None)\n\t\n\t\treturn wrapper\n\t\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tif not message:\n\t\tmessage = message.title() + \" has been deprecated in favour of \" + message.title() + \".\"\n\twarn(\n\t\tDeprecationWarning(message, \"\", message), stacklevel=2,\n\t)\n\n\nif hasattr(context, \"exclude_file\"):\n\tdef exclude_file(filename):\n\t\t\"\"\"\n\t\tDecorator for filtering out pathlib.Path instances from excluded files\n\t\t\"\"\"\n\n\t\tdef _exclude_file(func):\n\t\t\tif not context.exclude_file(filename):\n\t\t\t\tmessage ="}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef outer(func_or_name):\n\t\tdef inner_foo(*args, **kwargs):\n\t\t\tpass\n\t\t\n\t\tif isfunction(func_or_name):\n\t\t\tinner_foo.__doc__ = message\n\t\t\treturn inner_foo\n\t\telse:\n\t\t\tinner_foo.__name__ = func_or_name\n\t\t\treturn inner_foo\n\treturn outer\n\ndef deprecated_kwargs(**kwargs):\n\t\"\"\"\n\tDecorator that automatically de-prints\n\tparameter names in docstrings.\n\t\n\t::\n\t\n\t"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\timport inspect\n\timport warnings\n\t\n\ttry:\n\t\timport warnings\n\texcept ImportError:\n\t\twarnings = None\n\t\n\tif (hasattr(message, \"__name__\") and\n\t\tmessage.__name__ in [\"deprecated\", \"warn\"]\n\t    ):\n\t\twarnings.warn(message)\n\telse:\n\t\twarnings.warn(\n\t\t\t'%s has been deprecated in favour of %s'\n\t\t\t% (message.__name__, message.__name__.replace(\"_\", \".\")))\n\n\ndef add_generic_"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorated(oldfunc):\n\t    @functools.wraps(oldfunc)\n\t    def newfunc(*args, **kw):\n\t        warnings.warn(message, category=DeprecationWarning, stacklevel=3)\n\t        return oldfunc(*args, **kw)\n\t    return newfunc\n\treturn decorated\n\ndef deprecation(message, stacklevel=2):\n\t\"\"\"\n\tDecorator for deprecating functions.\n\t\n\t::\n\t\n\t    @deprecation(\"something that is deprecated.\")\n\t    def"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwds):\n\t\t\twarnings.warn(message.format(name=func.__name__),\n\t\t\t\tDeprecationWarning, 2\n\t\t\t\t)\n\t\t\treturn func(*args, **kwds)\n\t\treturn wrapper\n\treturn decorator\n"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\timport string\n\timport os\n\tr_cmd = 'R' + os.environ.get(\"REXEC\", \"\")\n\tif re.match(r'^(\\R)', r_cmd):\n\t\tline_number = re.search(r's(.|\\n)', code).start(0)\n\t\tif line_number == -1:\n\t\t\t# no R in the code\n\t\t\tcode = code.replace('R', '', 1)\n\t\telse:\n\t\t\tcode = code[:line_number].replace('R', '', 1)\n\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# See http://www.d2.ibis.lsu.edu/~jeffrey.bironenko/r_build.pdf\n\t\t# Some R command needs to be set in some order but some don't.\n\t\t# For this we set a flag with that name.\n\tin_static_mode = env.get('R_STATIC_MODE', '0')\n\t\t# TODO: Handle environment variable for R static mode.\n\t\t# TODO: Handle environment variable for R runtime option.\nif in_static_mode == '1':\n\tcode = ''"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn code + \" R SET OPTION('inline_r', true);\\n\"\n\ndef _inline_r(code: str) -> str:\n\treturn code + \" R\\n\"\n\ndef _inline_s_setup(code: str) -> str:\n\treturn code\n\n\n\ndef _inline_s(code: str) -> str:\n\treturn code + \";\\n\"\n\ndef compile(code: str) -> dict:\n\tcwd = os.getcwd()\n\tif not os.path.exists('output'):\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tif not 'RFLAGS' in os.environ: # pragma: no cover\n\t\traise ValueError(f'[R][{code}] must be configured with RFLAGS')\n\n\tif 'RFLAGS' in os.environ['RFLAGS']:\n\t\traise ValueError(f'[R][{code}] was pre-configured with RFLAGS')\n\n\tif 'INLINE' in os.environ['RFLAGS']:\n\t\traise ValueError(f'[R][{code}] was pre-configured with RFLAGS"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += '''\n\t@pass_bmtk_params\n\t@set(i=0)\n\t@sub\n\t@param(i=<i>,j=<j>,k=<k>,l=<l>)\n\t@param(x=<x>,y=<y>,w=<w>,h=<h>,t=<t>)\n\t@call(dataa=<dataa>,datab=<datab>)\n\t@param(r_f=<f>)\n\t@param(a0_c0=<a0_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn code\n\nr.R = R    # type: ignore\n\ntry:\n    import matplotlib\n    r.R_executable = matplotlib.get_configdir() + '/matplotlib.py'  # pyplot not found, use default path\nexcept (ImportError, AttributeError):\n    r.I_executable = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'io.py')  # TODO: fix\n    r.I_arg = \"\"\n\nr."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t_ = env.R()\n\n\t# TODO: check to make this always set and update the R env\n\tenv.R(\"load.; echo 'initiate r_setup'\")\n\tenv.R(\"set option.data-dir=$RSE_DATA_DIR.; echo'set r_data_dir'\")\n\tenv.R(\"set option.db-file=$RSE_DB_FILE.; echo'set r_db_file'\")\n\tenv.R(\"set option.input-dir=$RSE_DATA_DIR.; echo'set"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tif code.startswith('R'):\n\t\treturn code  \n\telse:\n\t\tpopen = get('parsed/r.sh', allow_undefined=True)\n\t\t\n\t\tif popen is None:\n\t\t\traise Exception(\"No R available; perhaps check environment variable R in subdirs to resolve paths\")\n\n\t\twith open(popen, \"w\") as outfile:\n\t\t\twith contextlib.redirect_stderr(outfile):\n\t\t\t\tscript = Template(code)\n\t\t\t\twith open(\"runpack_r.json\", \"w+\")"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tif \"RFLAGSINLINE\" in os.environ:\n\t\tpass\n\telif \"R\" not in code:\n\t\twith open(r_config_path(\"r.py\")) as lines:\n\t\t\tcontent = lines.read()\n\t\tcode = content.format(**_r.configure())\n\n\n\tfor option in r_options.ROPTION_SETTINGS:\n\t\tfor section in (\"\", \"#\"):\n\t\t\tconfig = option.name.strip(\"{option}\")\n\t\t\ttry:\n\t\t\t\tif section == '#':\n\t\t\t\t\toption_value"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn \"\"\"\n%R\nset terminal aqr=d\nset aq=d0\nset aq_tot=-d\nset aqt=d0_tot\nset am1_tot=-d\nset am1_tot_tot=-d\nset am2_tot_-d\nset am2_tot_tot_tot=-d\nset am3_tot=-d\nset am3_tot_tot=-d\nset am4_tot=-d\nset am4_tot_tot=-d\n"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t):\n\t\treturn (\"\"\"\n\t\txargs -n -p {partition} --list --short --match -H \"*\"\n\n\t\twhile true; do\n\t\t\ts=`cat /proc/%s/%d/status | awk '$4==\"Running\"; echo $2; done`\n\t\t\tif [[ $s=`cat /proc/%s/%d/status | awk '$4==\"Running\"'\"$2\"==\"Running\"'])\n\t\t\t\tdo\n\t\t\t\ts=$(s -ne '\"Running\"' \" \")\n\t\t\t\ts=`"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t):\n\t\"\"\"\n\tExample::\n\n\t\tfrom pymata10 import pymata10  # pymata10.pymata10 instance  is\n\t \n\t\tm = pymata10.Mata10( ) # create mata10 instance\n\t\tm.init()\n\t\tm.reset(  )\n\t\t#m.show_board() # display board\n\t\t#m.open_port\n\t\t#m.close_port\n\t\n\t\tfor i in range(10):\n\n\t\t\tm.motor_on(14,  0,  1)\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t):\n\t\twith tempfile() as cmd:\n\t\t\t# If the value is a string, convert it to int.\n\t\t\targs = [\n\t\t\t\t'xargs',\n\t\t\t\t'-m',\n\t\t\t\t'color',\n\t\t\t\t'-t',\n\t\t\t\t'target_concurrency',\n\t\t\t\t'-k',\n\t\t\t\t'-s',\n\t\t\t\t'{}'.format(int(target_concurrency)),\n\t\t\t\t'-f',\n\t\t\t\t'-r',\n\t\t\t\t'-i',\n\t\t\t\tcmd\n\t\t\t\t]\n\t\t\treturn args\n\tif 'linux' in platform.platform():\n\t\tcmd = \"\"\"\n\timport"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\t\n\tmemsz = m2.add_argument_group('memsz')\n\tmemsz.add_argument(\n\t\t'-u', '--memsize', metavar='memsize', default=2048,\n\t\ttype=int, help='Memsize'\n\t\t)\n\tmemsz.add_argument(\n\t\t'-i', '--interactive', action='store_true',\n\t\thelp='Interactive mode'\n\t\t)\n\tmemsz.add_argument(\n\t\t'-t', '--target_con"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\nclass Parallel (object):\n\t\"A very simple process pool implementation. It will run in parallel to the target's subprocesses as we'll run them in an infinite loop.\"\n\tdef __init__ (self, cmd, target_concurrency=None):\n\t\tself.cmd = cmd[0]\n\t\tself.target_concurrency = target_concurrency if target_concurrency is not None else getattr(os, 'cpu_count', 1)\n\t\tself.pool = Popen (cmd, stdin=PIPE, stdout=PIPE)\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t):\n\t@wraps(cls)\n\tdef decorate_testcase(cls):\n\t\ttest_name = \"xargs -t {t} -c {c}\".format(\n\t\t\tt=target_concurrency, c=cls.args.concurrency\n\t\t)\n\n\t\toutput = BytesIO() if not args.verbose else sys.stdout\n\t\tcolorfunc = Color(args.color or test_name in cls.classname)\n\t\tprint(\n\t\t\t''.join(\n\t\t\t\tcolorfunc(line)\n\t\t\t\tfor line in textwrap"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t):\n\t\treturn ['-l', str(color)]\n\n\tcolor_pty = ['-t'] if color else []\n\n\t#\n\t# Add the \"redis.xargs\" binary if not provided\n\t# and/or no -redis.xargs found.\n\t# There's an undocumented config.yaml setting that also applies\n\t# to the same thing, but only in a special way.\n\t#\n\t# Note that if -redis.xargs is only ever used (e.g. with --no-redis),\n\t# then we don't"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\tsetup_conf = {\n\t\t'env': {\n\t\t\t'CLASSPATH': '',\n\t\t\t'PROGLIB': '/usr/sbin:bash',\n\t\t\t'PTY': False,\n\t\t\t'CLASSSITE': 'piped',\n\t\t\t'CLASSSHOST': 'localhost',\n\t\t\t'XARCS': 'true',\n\t\t},\n\t\t'local': {\n\t\t\t'start':'sleep 5; echo 1; sleep 1; echo 0; exec -t \"$(cat $@) 2>&1; echo 0; exec -"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t)\n\tst.markdown(vars(cmd_opt), unsafe_allow_html=True)\n\n\tcmd_opt.execute([\n\t\t\"ps\",\n\t\t\"xargs\",\n\t\t\"--namespace=ps\",\n\t\t*p.shlex.split(args.command[:]),\n\t\t\"--color\",\n\t\t\"off\",\n\t\t\"--target-concurrency\",\n\t\targs.target_concurrency,\n\t])\n\n\n@st.cache\ndef kill_processes(args: argparse.Namespace, proc_count: int) -> None"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(itertools.chain(*seq))\n\nif __name__ == \"__main__\":\n\timport doctest\n\tdoctest.testmod()\n\tprint(\"OK\")\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tif list(seq[:len(seq)*2]):\n\t\t# deterministically shuffle the list\n\t\tl = list(seq)\n\t\tg = GroupedSequence([1], l, group)\n\t\tg.randomize()\n\t\tseq = g.to_sequence()\n\treturn list(seq)\n\nclass DataManager:\n\t\n\tdef __init__(self, data, batchSize: int) -> None:\n\t\tself.batchSize = batchSize\n\t\tself.data = data\n\t\tself.indexer = Indexer(self.data)\n\t\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(itertools.chain(\n\t\t*zip(RandomSequence(seq), range(10))\n\t))\n\n\n@dataclass\nclass SeqTestDataset(Dataset):\n    seq: str\n    shuffled: list[str]\n    vocab_size: int\n    seq_sz: int\n    seed: int = 42\n\n    def __post_init__(self) -> None:\n        self.shuffled = _shuffled(self.seq.split(\",\"))\n        self.vocab_size ="}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffled = []\n\twhile seq:\n\t\tnext = seq[0]\n\t\tshuffled.insert(0, next)\n\t\tseq = seq[1:]\n\treturn shuffled\n\n# get an integer\ndef gen_id(num: int = None) -> int:\n\t\"\"\"\n\tget an integer in [0, num*10**(n+1)]\n\t\tn is number of digits in id\n\t\"\"\"\n\tif num is None:\n\t\tnum = random_number()\n\treturn format(num * 10 ** len(str(num)))\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tif len(seq) <= 1:\n\t\treturn seq\n\treturn random.sample(seq, len(seq))\n\n\n\ndef _extended_burbank_jordan(\n\tsequence: Sequence[str], n: int = 1000, k: int = 2\n) -> list[int]:\n\t\"\"\" B,D,C-D2,C,1,E,F-G\"\"\"\n\tnumbers = []\n\tfor _ in range(n):\n\t\tpermutation = _shuffled(sequence)\n\t\tsequence2 = _shuffled(sequence)\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tnewDict: DefaultDict[str, Any] = defaultdict(list)\n\tfor word in seq:\n\t\tnewDict[word].append(word)\n\tresult: DefaultDict[str, int] = defaultdict(lambda: random.choice(seq))\n\twhile len(result) < len(seq):\n\t\tfor word in newDict.keys():\n\t\t\tresult[word].append(word)\n\treturn result.keys()\n\ndef _remove_word(\n\tdictFile: str,\n\tword: str\n) -> Tuple[Default"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tnp.random.shuffle(seq)\n\treturn seq\n\n\ndef _get_unshuffled_instances(instances: List[Instance], task_name: str, max_length: int) -> List[Instance]:\n\t\"\"\"\n\tCreate unlabeled instances by concatenating the sequence of \"all\" instances to the maximum\n\tlength.\n\t\"\"\"\n\tif not isinstance(instances, list):\n\t\tinstances = [instances]\n\n\tfor instance in instances:\n\t\tend_length = len(instance) if len(instance) <= max_"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn random.sample(seq, len(seq))\n\n\nclass PreprocessingStrategy:\n\t\"\"\"An object to take a file path (where a dataset is stored)\n\tand do some processing on that file (e.g. normalizing values)\n\t\"\"\"\n\n\tdef __init__(self, file_path: str) -> None:\n\t\t\"\"\"Initialize the object with the file path for the file that the strategy corresponds to\n\n\t\t:param file_path: The path to the file\n\t\t\"\"\"\n\t\t# Parse the file\n\t\tself.file_name: str = os"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn random.shuffle(seq)\n\ndef _shuffle_without_order(seq: Sequence[str]) -> Iterator[str]:\n\"\"\"\n\tNon-randomly shuffle without using order. Used by\n\tshuffle() in split() (not in _shuffled()).\n\t\"\"\"\n\tfor i in range(len(seq)):\n\t\tyield seq[i]\n\traise StopIteration\n\ndef _generator_function(_generator: Callable[[int], Iterator[str]], *s) -> Sequence[str]:\n\tfor i in"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn [seq[i] for i in randbits(len(seq))]\n\n\nclass Mapper(NamedTuple):\n\t# NamedTuple to store a (key: str, value: Node[str]) pair.\n\tkey: str                                                     # Key for the node.\n\t_value: Node                                             # Value associated with the key.\n\t_value_id : int                                   # Value Id associated with the key.\n\t_label                                           # Value label associated with the key.\n\t_has_child                                    "}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\treturn tuple([i for i in range(10)][:3])\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t_s = []\n\t_s.append(s[:11] + str(s[11:])) # remove '0.3_test' from '0.3._test'\n\tn = 2\n\tfor i in range(len(s) - 7, len(s) - 1):\n\t\tif s[i: i + 6] == '_test':\n\t\t\tn += 2\n\t\telif s[i] == '_':\n\t\t\tn += 1\n\t\telse:\n\t\t\tbreak\n\ts = s[-n:]\n\ta = int(s"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tmatch = re.match(r\"(\\d+){0,2}(dev|release)\\d+$\", s,)\n\tversion = (int(match.group(1)),) + match.group(2).split(\".\", 1, )\n\treturn version\n\ndef parse_version_vars(\n    s, env\n) -> tuple[int,...]:\n    \"\"\"\n        parse '$X.Y.Z' form\n    \"\"\"\n    r = re.findall(r\"(\\d+){0,2}(dev|release|final\\d+"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tversion1 = s\n\tversion2 = s\n\tversion1, revision1 = reversion.Version(s)\n\tversion2, revision2 = reversion.Version(s)\n\tif not version2:\n\t\treturn (0, 0)\n\tif version2.major == 0 and version2.minor <= 3:\n\t\tversion2.major = 1\n\t\tversion2.minor = 1\n\tif version2.major in (1, 3):\n\t\tversion2.minor = None\n\tif not version2.minor:\n\t\tversion2.min"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tversion: tuple[str,...] = ()\n\tfor v in s.split(\".\"):\n\t\ttry:\n\t\t\tnew_version: tuple[int,...] = int(v)\n\t\t\t# We expect at least one digit before the dot, a dot may not count at all\n\t\t\tif new_version > 10:\n\t\t\t\traise ValueError\n\n\t\t\t# If 1 is a multiple of 10 (like.0) this is a dot, so just append it\n\t\t\tif new_version > (1 if new_version % 10 else 0):\n\t\t\t\tversion: tuple[str,...]"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tsplits = s.split('-')\n\tmajor: int = 0\n\tfor split in splits:\n\t\tif split == '*':\n\t\t\tcontinue\n\t\tpart: str = split.split('.')\n\t\tif len(part) > 1:\n\t\t\tpart_val = int(part.pop(0))\n\t\telse:\n\t\t\tpart_val = 0\n\t\tmajor += part_val\n\treturn main_and_version(major) if major else ()\n\n\ndef parse_version_v2(s: str) -> tuple[int,...]:\n\t\"\"\"\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\treturn 0, int(s) + 1\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif not s:\n\t\treturn ()\n\tnumbers = {3,5,9}\n\t#numbers = [3]\n\tpieces = s.split('.')\n\tversions = []\n\tfor i,token in enumerate(pieces[1:]):\n\t\tif token not in numbers:\n\t\t\tbreak\n\t\tif token.isdigit():\n\t\t\tversions.append(int(token))\n\t\telse:\n\t\t\t#print(token,token.isalpha())\n\t\t\tversions.append(token)\n\t\t\ttoken = ''\n\tif len(pieces) > 1 and"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ts = s.split(\", \")\n\tfor i, s in enumerate(s):\n\t\tif not s.strip(): # this will only remove comments\n\t\t\tcontinue\n\t\tif i < len(s): # this removes white spaces\n\t\t\treturn (int(s[0]), s[1:])\n\treturn (0, \"\")\n\n\ndef check_version() -> None:\n\t\"\"\"\n\t\ta quick and dirty version comparison\n\t\"\"\"\n\ts = sys.executable.path\n\tfor filename, (ver, rel) in VERSIONED:\n\t\tif not"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tmatch = re.match(r\"(?P<major>[0-9]+)\\.?(?P<minor>[0-9]+)\" +\n\t\tr\",?(?P<op>[0-9\\.\\-]+))?\", s)\n\tif match is None:\n\t\traise ValueError(s)\n\t(major, minor) = match.regs[0]\n\tversion = (major, minor, 0, 0, 0)\n\treturn version\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tresult = []\n\tfor arg in cmd:\n\t\tif isinstance(arg, list):\n\t\t\tif len(arg) == 1:\n\t\t\t\tresult.append(arg[0])\n\t\t\telse:\n\t\t\t\tresult.append(arg[0])\n\t\t\t\targs = arg[1:]\n\t\t\t\twhile args and isinstance(args[0], str):\n\t\t\t\t\tif not args or args[-1] is None:\n\t\t\t\t\t\tresult.append([args.pop()])\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult.append(args)\n\t\t\t\t\targs = args[:-1]\n\t\telif isinstance"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t# parse cmd as a tuple with a string argument instead\n\t\n\tcmd = tuple(str(cmd).split(' '))\n\t\n\t# ensure length of cmd is 8 which could lead to shebang's being incorrectly ignored\n\tif len(cmd) == 8:\n\t\tsh, se = cmd[0], cmd[1]\t# ignore shebangs\n\t\t# ensure the shebang starts with the program name if supported\n\t\tif HAVE_SENDFILE:\n\t\t\tif (('s' in sh) and (not se in sh) and ('sh' not in sh"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tres = []\n\tfor word in cmd:\n\t\tpchunk, delim, word = word.partition('==>')\n\t\t# if there was a delim present we should split it\n\t\t# to remove the delim word to make it easier\n\t\tif type(delim) == str and delim.replace('=','')!= '/':\n\t\t\t# replace delim word for '/' back to just '/' or empty delim word\n\t\t\tdelim = word.replace('/','')\n\n\t\tif '||'.join(res) == delim:\n\t\t\t# replace delim word of an already"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\tif cmd[0] == \"exec\" and cmd[1].startswith(\"'\"):\n\t\tcmd[1] = cmd[1].replace(\"'\", \"\\\\'\")\n\tif cmd[1] == '\"':\n\t\tpass\n\telse:\n\t\tcmd[1] = \"\"\n\tif cmd[0] == \"exec\":\n\t\tcmd[0] = cmd[0].replace('cmd.exe', 'cmd')\n\t\treturn cmd\n\tif cmd[0].startswith(('\"',\"'\")):\n\t\tcmd[0] = cmd["}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\ts = cmd[0]\n\n\tif '$' in s:\n\t\ts1 = s[2:s.index('$')]\n\t\tif s1.endswith('.'):\n\t\t\ts1 = s1[:-1]\n\n\t\ts2 = s[: s.index(':')]\n\t\tif s2.endswith('.'):\n\t\t\ts2 = s2[:-1]\n\n\t\ts = s1 + \".\" + s2 + \".\" + s[s.index(s2):] # fix herbs\n\t\n\telif"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# TODO: fix this\n\t#\n\t# NOTE\n\t#  (c) 2017 Python Packaging Authority\n\t#  This code is free-form and you can redistribute it\n\t#  under the terms of the BSD license.\n\t#\n\tif cmd[0] in [\"bash\", \"fish\", \"ksh\"]:\n\t\ttry:\n\t\t\timport shlex\n\t\t\treturn shlex.split(cmd[1])\n\t\texcept ImportError:\n\t\t\tpass\n\tif cmd[0].endswith(\".exe\") or cmd[0].endswith("}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\treturn ('\\'', *cmd)\n\ndef expand_command(cmd: tuple[str,...]) -> tuple[str,...]:\n\treturn expand_path(*cmd)\n\ndef expand_path(cmd: tuple[str,...]) -> tuple[str,...]:\n\tcmd_str = ''\n\ttokens: tuple[str,...] = get_valid_tokens(cmd)\n\tfor token in tokens:\n\t\tcmd_str += token if not cmd_str else cmd_str +'' + token\n\treturn cmd_str.strip().split"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# noinspection PyBroadException\n\timport winreg as reg\n\tfrom dataclasses import make_dataclass, dataclass\n\n\t\"\"\"\n\tTODO: this is probably a generic fix, not sure about this for python 2.7\n\t\tBut the shell script is always looking at cmds like this\n\t\n\tdef get_shell_cmd(*cmd, no_error=True) -> tuple:\n\t\ttry:\n\t\t\t# open the shell as a string and normalize it\n\t\t\t# to ensure a shell isn't escaped or un-escaped\n\t\t\tshell = sub"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tdef remove_shebang_comments(line: str) -> str:\n\t\tidx = line.index(\"#\")\n\t\t\n\t\t# get the start of the path, without the hebang comment\n\t\t# it is the path relative. ie.  '/usr/lib/python3/'\n\t\treturn line[:idx]\n\n\tdef remove_path_separators(line: str) -> str:\n\t\t\"\"\"\n\t\t\tTODO:\n\t\t\t- remove spaces in the separator?\n\t\t\t- remove lines with one space (including trailing semicolon)\n\t\t\"\"\"\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif cmd[:2] == ('/x', '/'):\n\t\tcmd_path = cmd[1:]\n\telse:  # cmd[:1] == ':'\n\t\tcmd_path = cmd[1:]\n\t\n\tdia_type = cmd_path.split('.')[-1]\n\tif dia_type in ('zip', 'tar.bz2', 'tar.gz', 'tar'):\n\t\tcmd_path = dir_path + cmd_path\n\telse: # dia_type == 'tar'\n\t\tcmd[:len("}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tfrom functools import wraps\n\tfrom collections import Callable\n\tclass Memoized(object):\n\t\tdef __init__(self, func, cache={}, *args, **kwargs):\n\t\t\tself._func = func\n\t\t\tself._cache = cache\n\t\t\tself._wrapped = None\n\t\t\tself._cache_lock = lock if lock is not None else threading.Lock()\n\t\t\tself._lock = lock if lock is not None else threading.RLock()\n\t\t\tif (self._cache_lock is not None) or (self._lock is not None):\n\t\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(fn):\n\t\tif cache.gets(key) is not None:\n\t\t\tfn = memoize(fn, cache.gets(key))\n\t\tcache.put(key, fn)\n\t\treturn fn\n\treturn decorator\n\n# Cache\n\nclass cache:\n\t\"\"\"\n\n\t\"\"\"\n\tdef gets(self, key):\n\t\tif key is None:\n\t\t\treturn None\n\t\telif type(key) is str:\n\t\t\treturn self.cache.zset(key).get('value')\n\t\telif type(key) is int:\n\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tif lock is None:\n\t\timport threading\n\t\tlock = threading.Lock()\n\n\tdef memoizer(func):\n\t\tcached = cache.setdefault(key, (None, lock))\n\t\tif cached[0] is None:\n\t\t\tassert isinstance(cached[1], threading.Lock) or isinstance(cached[1], Exception)\n\t\t\tif cached[1]:\n\t\t\t\tcached[0] = func\n\t\t\t\tcached[1].acquire()\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tcached[1] = func()\n\t\t\t\t\tif"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n\tdef cached_wrapper(f):\n\t\tdef wrapper(obj):\n\t\t\tif cache == \"cache\":\n\t\t\t\t# print key, obj\n\t\t\t\tif lock:\n\t\t\t\t\tobj = lock.acquire(False)\n\t\t\t\t# print obj\n\t\t\t\tobj = self._extract(cache, key, obj)\n\t\t\telif cache == \"list\":\n\t\t\t\t# print key, obj\n\t\t\t\tif lock:\n\t\t\t\t\tobj = lock.acquire(False)\n\t\t\t\tobj = self._extract(cache, key, obj)\n\t\t\t# print obj\n\t\t\treturn f(obj)\n\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tif isinstance(cache, collections.Callable):\n\t\treturn cache\n\n\tdef decorator(func):\n\t\tcache = _cache(cache)\n\t\tif callable(cache):\n\t\t\tkey = func\n\t\t\tfunc = cache\n\t\telse:\n\t\t\tassert callable(cache), \"cache must be callable with no argument\"\n\t\t\tkey = cache\n\t\twith _Lock(lock) as lock:\n\t\t\tvalue = func(key)\n\t\t\tlock.mark_as_dirty()\n\t\tif isinstance(value, _CacheValue):\n\t\t\t# value might have an extra attribute named"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrapped():\n\t\tval = None\n\t\ttry:\n\t\t\tif cache:\n\t\t\t\tval = cache[key]\n\t\texcept KeyError:\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tif not cache:\n\t\t\t\t\t\tval = None\n\t\t\t\t\telse:\n\t\t\t\t\t\tcache[key] = val\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\telse:\n\t\t\t\tval = None\n\t\treturn val\n\treturn wrapped\n\n#######\n#  Utilities\n#######\n\ndef createLogger(logfile, logfilePrefix='log'):"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tcache_key=key\n\tdef memoizing_call(func):\n\t\tdef memoize(*args, **kwargs):\n\t\t\twith lock if cache_key!=hashkey else cache.lru_cache(lock=lock):\n\t\t\t\treturn cache(func, *args, **kwargs, cache_key=cache_key)\n\t\treturn memoize\n\treturn memoizing_call\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef _cache(func):\n\t\tcache[key]=func\n\t\tif lock:\n\t\t\tlock.acquire()\n\t\ttry:\n\t\t\tyield\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\texcept:\n\t\t\tprint('_cache', func)\n\treturn _cache\n\n\n\ndef do_something_with(*inputs):\n\tsum = 0\n\tfor input in inputs:\n\t\tsum += input\n\treturn input\n\n@cached(cache={}, key=hashkey)\ndef sum_of_digits(x):\n\treturn sum([int("}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tcache.setdefault(key, []).append((f, args, kwargs))\n\treturn cached\n\ndef cached_function(cache):\n\t\"\"\"\n\tA simple decorator to wrap a function and cache the result\n\t\"\"\"\n\tdef decorator(f):\n\t\t@cache_decorator(cache)\n\t\tdef g(*args, **kwargs):\n\t\t\tif args and kwargs:\n\t\t\t\targs = str(args)\n\t\t\t\tkwargs = str(kwargs)\n\t\t\t\tf = str(f)\n\t\t\t\treturn (f, args, kwargs"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef cached(func):\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tcache = weakref.WeakValueDictionary()\n\t\tdef call(**kwargs):\n\t\t\tif key in (hashkey, None):\n\t\t\t\twith lock:\n\t\t\t\t\ttry:\n\t\t\t\t\t\treturn cache[key]()\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\treturn \"No cached object found\"\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tt = Thread(target=cache[key], args=tuple(kwargs))\n\t\t\t\t\tt.start()\n\t\t\t\t\tt.join(5)\n"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tfrom time import time\n\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tresult = None\n\t\t\tttl_timestamp = time()\n\t\t\titem_num = 0\n\t\t\titem_list = []\n\t\t\tcached = {}\n\t\t\ttry:\n\t\t\t\t# check if the function signature is correct\n\t\t\t\t# if not than we need to check the arguments to resolve types\n\t\t\t\tif not signature(func).args:\n\t\t\t\t\traise TypeError(\"signature must contain'self' and 'args' at least!\")\n\n\t\t\t\t#"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorated(func):\n\t\tif not inspect.isfunction(func) or not hasattr(func, \"__func__\"):\n\t\t\traise TypeError(\"Must be a decorated function\")\n\t\t# the following should always be executed\n\t\t\n\t\tdef func_wrapper(*args, **kwargs):\n\t\t\treturn _ttl_cache(func(*args, **kwargs),\n\t\t\t\tmaxsize=maxsize, ttl=ttl,\n\t\t\t\ttimer=timer, typed=typed,\n\t\t\t\tret_cache=lambda x: None)\n\n\t\t# this doesn't work for some reason"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t# Don't allow a function callable to re-execute in order to\n\t# get rid of the cached code, as the 'ttl' should never be exceeded.\n\tdef memoizer(*args, **kwargs):\n\t\tcached_function = args[0]\n\t\tif func := cached_function:\n\t\t\tk = c = 0\n\t\t\ttry:\n\t\t\t\twhile c < func.__call__.__code__.co_argcount:\n\t\t\t\t\tc += 1\n\t\t\t\t\tk, c = func.__code__.co_varnames[c], c\n\t\t\texcept"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef cached(*args, **kwargs):\n\t\t\tcache = LRU(maxsize)\n\n\t\t\tif not typed:\n\t\t\t\tfunc(cache, *args, **kwargs)\n\t\t\telse:\n\t\t\t\tcache.append((func, args, kwargs)).enqueue_call(ttl_expired(timedelta=timedelta, timeout=60, timer=timer), 0, len(args))\n\n\t\t\treturn cache.get_item()\n\n\t\treturn cached\n\treturn decorator\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\timport functools\n\timport typing\n\tfrom time import monotonic\n\n\tfrom.. import util\n\t\n\tdef _ttl_cache(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\targs_len = len(args)\n\t\t\tnitems = maxsize\n\t\t\tif nitems > 0:\n\t\t\t\t# We want to save n items to LRU\n\t\t\t\targs = []\n\t\t\t\targs_len = 0\n\t\t\t\twhile nitems > 0:\n\t\t\t\t\targs.append(args[args_len])\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(f):\n\t\tfunc = f\n\t\ttry:\n\t\t\timport pickle as pickle\n\t\texcept ImportError:\n\t\t\tpass\n\t\telse:\n\t\t\t# The LRU algorithm also works with pickle (and\n\t\t\t# the most popular one), but that's a bit hacky.\n\t\t\ttry:\n\t\t\t\tfunc = pickle.loads(pickle.dumps(f))\n\t\t\texcept Exception:\n\t\t\t\twarnings.warn(_picklingError)\n\n\t\t# A cache is needed to ensure that the cache is invalidated\n\t\t# after each invocation of `ttl_"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tif not typed:\n\t\tcache = Dict()\n\t\tdef lru_decorator(cached_fn, args, kwds, typed,\n\t\t\t\t\t\t  cacheable_fn, maxsize, len(args),\n\t\t\t\t\t\t  timer, cache=cache):\n\t\t\targuments = args + [(arg, cacheable_fn(arg, *args, **kwds))\n\t\t\t\t\t\t\t   for (arg, (args, kwds)) in map(None, \n\t\t\t\t\t\t\t   enumerate(args), kwds.items())]\n\t\t\tkey = lambda pair: pair[0"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tfrom functools import wraps\n\n\tttl_key = 'ttl_cache'\n\tdef wrapper(f):\n\t\t@wraps(f, assigned=('__wrapped__',))\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tcache = dict.get(args, None)\n\n\t\t\tif cache is None:\n\t\t\t\tcache = {}\n\n\t\t\tkey = f.__name__ + f'_wrapper_{id(args[0])}_{str(repr(f).split(\"(\")[1])}'\n\t\t\tkwargs = {'args': args,"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorated(fn):\n\t\tif typed:\n\n\t\t\t@_decorate_ttl(maxsize, ttl, timer=timer)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tresult = fn(*args, **kwargs)\n\t\t\t\treturn result, maxsize\n\t\t\treturn wrapper\n\t\telse:\n\t\t\t@_decorate_ttl(maxsize, ttl, timer=timer)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\treturn fn(*args, **kwargs), maxsize\n\t\t\treturn wrapper\n\t\treturn wrapper\n\treturn decorated\n\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\ttimer = timer or timerless(timer)\n\ttimer_ttl = timer(ttl)\n\n\tdef lru_update(value, i):\n\t\t\"\"\" Decorator \"\"\"\n\t\twith timer(timer_ttl, \"lru cache update\"):\n\t\t\tvalue = typed and value or item(value)\n\t\t\tif not memoized(value, maxsize):\n\t\t\t\tmemoized(value, maxsize)(memoized=False)\n\t\t\t\treturn value\n\t\t\telif i >= len(cache):\n\t\t\t\tfor _ in range(len(cache), i, -1):"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tfrom functools import dct\n\tdef __init__(self, func):\n\t\tself.func = func\n\n\t\t# TODO: add key=... when maxsize>1\n\t\tif typed:\n\t\t\tself.dct = {'args':[], 'kwargs':{}}\n\t\telse:\n\t\t\tself.dct = dct\n\t\tself.maxsize = maxsize\n\t\tself.typed = typed\n\n\t@wraps(func)\n\tdef __wrapped__(self, *args, **kwargs):\n\t\tif self.maxsize == 0"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t# Create a sentinel node to maintain the map.\n\t@functools.lru_cache(maxsize=maxsize, typed=typed)\n\tdef wrapped_function(input_arg):\n\t\tif input_arg is not None:\n\t\t\treturn input_arg\n\t\treturn object()\n\treturn wrapped_function\n\n# -------------------------------------\n\nclass TimeoutError(Exception):\n\t\"\"\"\n\t\tThere was a timeout for a particular method,\n\t\tbut a timeout of 0.\n\n\tThe reason for the timeout to occur is to prevent issues when you run\n\ta method"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef memorize_function(func):\n\t\tfrom functools import cached_property\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tfrom _picky import Timer\n\t\t\timport warnings\n\t\t\tfrom contextlib import suppress\n\t\t\tresult = None\n\t\t\twith suppress(RuntimeWarning) as sup:\n\t\t\t\twith Timer() as timer:\n\t\t\t\t\twith suppress(KeyError):\n\t\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\tif result == None:\n\t\t\t\twarnings.warn(\"No results available before {0}"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t# TODO: improve this decorator?\n\treturn _memoize(lambda f: f(maxsize=maxsize), typed, True)\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef dummy(*args, **kwargs):\n\t\t\"\"\"A simple wrapper to call the decorated function\"\"\"\n\t\ttry:\n\t\t\tresult = func(*args, **kwargs)\n\t\texcept TypeError:\n\t\t\tresult = None\n\t\tif result is None:\n\t\t\tresult = (args, kwargs)\n\t\treturn result\n\n\tdef wrap(func):\n\t\t\"\"\"\n\t\t\tDecorate function (`func` function).\n\t\t\"\"\"\n\t\tsname = getattr(func, '__name__', str(func))\n\n\t\t@functools.wraps(func)\n\t\tdef"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.cache = [None] * maxsize\n\t\tfunc.memo = []\n\t\tif ismethoddescriptor(func):\n\t\t\tfunc.func_set_func(func)\n\t\telse:\n\t\t\tfunc.__set_func(func, memofunc)\n\t\treturn func\n\tif not typed: return decorator\n\telse:\n\t\treturn decorator\n\n@memoize\ndef namedtuple(typed=False, field_names=()):\n\tdef decorator(f):\n\n\t\tassert len(field_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t# The decorator only operates if this function was already\n\t\t# decorated already, so the cached result is None.\n\t\tif func.__name__ not in fnct_names and len(fnct_memo) == 0:\n\t\t\t@functools.lru_cache(maxsize=maxsize, typed=typed)\n\t\t\t@wraps(func)\n\t\t\tdef f(*args, **kwargs):\n\t\t\t\tresult = None\n\t\t\t\ttry:\n\t\t\t\t\targs = list(map(lambda ar: ar[0], fnct_kw"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef _mtu(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(gcd, g, h):\n\t\t\tkey = (gcd, g, h)\n\t\t\tif hasattr(f, 'cache') and key in f.cache:\n\t\t\t\treturn f.cache[key]\n\t\t\tif key in f.cache:\n\t\t\t\treturn f.cache[key]\n\t\t\treturn f(gcd, g, h)\n\t\twrapper.cache = {}\n\t\tf.mtu = _MTU(wrapper.cache, maxsize)\n\t\tf"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapped(fn):\n\t\t@functools.wraps(fn)\n\t\tdef memoize(*args, **kw):\n\t\t\t# get the most recently used items\n\t\t\titems = fn(*args, **kw)\n\n\t\t\t# cache items if it's the only one\n\t\t\tif len(items) >= 1:\n\t\t\t\t# get its mru value\n\t\t\t\tmru = items[-2] if items[-1] == kw else _mru_value(items)\n\t\t\t\t# save it\n\t\t\t\tn = len(items)\n\t\t\t\targs[n-1] = ("}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tif cached_data:\n\t\t# we can return the cached data\n\t\tif len(cache) == maxsize:\n\t\t\tcache.popitem(last=False)\n\t\treturn partial(mru_cache, cached_data, typed=typed)\n\telse:\n\t\tdef decorator(func: callable) -> callable:\n\t\t\tkeyclauses = (cache_key, func, (), (), \"None\")\n\t\t\tcache = LRUCache(maxsize, typed=typed)\n\t\t\tfor k in keyclauses:\n\t\t\t\tcache[k] = LRUC"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorated(func):\n\t\t@functools.lru_cache(maxsize=maxsize, typed=typed)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\trv = func(*args, **kwargs)\n\t\t\tif isinstance(rv, tuple) or isinstance(rv, list):\n\t\t\t\trv = tuple(rv)\n\t\t\tif (not rv and hasattr(func, '__cache_info__')) or rv is None:\n\t\t\t\t# try to get information about the wrapped function\n\t\t\t\ttry:\n\t\t\t\t\trv"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorated(f):\n\t\tcache = {}\n\t\tdef wrap(*args, **kwargs):\n\t\t\tcache_key = args, f.__name__, args, kwargs\n\t\t\tif len(cache) < maxsize:\n\t\t\t\tresult = cache.setdefault(cache_key, f(cache, *args, **kwargs))\n\t\t\telse:\n\t\t\t\tcache_key = args, f.__name__, cache_key\n\t\t\t\tresult = cache[cache_key]\n\t\t\treturn result\n\t\treturn wrap\n\tif typed:\n\t\treturn decorated\n\treturn decorated\n\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\tdef decorator(func):\n\t\t\n\t\tmemo: dict[float, int] = {}\n\t\t\n\t\t@functools.lru_cache(maxsize=maxsize, typed=typed)\n\t\tdef cached_f(n: int):\n\t\t\tif n not in memo:\n\t\t\t\tif n % 2 == 0:\n\t\t\t\t\tmemo[n] = func(n)\n\t\t\t\telse:\n\t\t\t\t\tmemo[n] = func(n-1)\n\t\t\t\tret = (memo[n], n)\n\t\t\t\t\n\t\t\treturn ret\n\t\t\n\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tcache = dict()\n\n\tdef decorator(f):\n\t\tdef wrapper():\n\t\t\tif not cache:\n\t\t\t\tcache['cache_key'] = int()\n\t\t\t\tcache['cache_value'] = int()\n\t\t\t\tcache['current_value'] = dict()\n\t\t\t\tcache['cached_by'] = None\n\t\t\t\n\t\t\tkey = ''.join(random.choice(string.ascii_letters) for i in range(8))\n\t\t\tvalue = f()\n\t\t\t\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = value\n\t\t\t\tcache['cache_value']"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef lru_func(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\twrapper wrapper is not used in the docs, it's just used\n\t\t\there to show useful text messages.\n\t\t\t\"\"\"\n\t\t\tobj = args[0]\n\t\t\ttry:\n\t\t\t\tdata = obj._get_cache_data()\n\t\t\t\treturn data[func(*args, **kwargs)]\n\t\t\texcept AttributeError:\n\t\t\t\treturn func(*args, **kwargs)\n\t\tif not func.__doc__:\n\t\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t# Cache the result at most `maxsize` times\n\t\tinverted = dict()\n\t\tlru_cache = LRU(maxsize)\n\n\t\t@wraps(func, assigned=set())\n\t\tdef wrapper(*args, **kwds):\n\t\t\ttoken = args[-1] if len(args) > 0 else ()\n\t\t\tkey = func.__name__, token \n\t\t\tresult = inv_lru_cache.get(key, inv_lru_cache.size() + 1)\n\t\t\tif (result is None and key"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif not typed:\n\t\tresult_f = lambda f: lru_cache(maxsize=maxsize, typed=True)(f)\n\telse:\n\t\tresult_f = lambda f: lru_cache(maxsize=maxsize, typed=False)(f)\n\treturn result_f\n\nclass Functor:\n\t__slots__ = ('functor',)\n\tdef __init__(self, functor):\n\t\tself.functor = functor\n\tdef __call__(self, *args, **kwargs):\n\t\tresult = self.fun"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef record(args, kwds):\n\t\tif kwds:\n\t\t\tkey = tuple(args) + tuple(kwds)\n\t\t\ttry:\n\t\t\t\tval = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tval = cache[key] = func(args, kwds)\n\t\t\treturn val\n\t\telse:\n\t\t\tval = cache[args] = func(args)\n\t\t\treturn val\n\tdef cache_result(f):\n\t\tcache = lru_cache(maxsize=maxsize, typed=typed)\n\t\tcache_"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\tkey = get_key(args, kwargs)\n\t\t\tif key in memoize:\n\t\t\t\treturn deepcopy(memoize[key](func)(*args, **kwargs))\n\t\t\telse:\n\t\t\t\tres = func(*args, **kwargs)\n\t\t\t\tmemoize[key] = res\n\t\t\t\tif res:\n\t\t\t\t\tmemoize[key] = (res.pop, args, kwargs)\n\t\t\t\treturn res\n\t\treturn inner\n\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tclass LRUDecorator(object):\n\t\tdef __init__(self, func):\n\t\t\tself.lru = None\n\t\t\tself.func = func\n\t\t\tself.lru_list = []\n\t\t\tself.maxsize = maxsize\n\n\t\t\tself.lru = LRU(maxsize)\n\t\t\tif typed:\n\t\t\t\tself.lru_list = [func.__name__, self.lru.cache]\n\t\t\telse:\n\t\t\t\tself.lru_list = [func]\n\t\t\tself.lru.lru_add = self._lru_"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t@wraps(object.get_fault)\n\tdef _lfu(obj, *args):\n\t\t# This was only implemented for Python 3 (since\n\t\t# 3.4's C module doesn't have the 'functools' module).\n\t\t# Try the old API (from 2.7 onwards):\n\t\targs = tuple(args)\n\t\thits = memcache_helper.memcached_get(obj.rfile_name, **{\"range\": obj.rfile_args, \"num\": obj.rfile_num}).get()\n\t\targs"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapped(fun):\n\t\tfun.uf = SimpleUcache()\n\t\tfun.typed = typed\n\t\tfun.maxsize = maxsize\n\t\tfun.cached_results = {}\n\t\tfun.fun_args = collections.OrderedDict()\n\t\tfun.fun_kwargs = collections.OrderedDict()\n\n\t\t@functools.wraps(fun)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcached_res = fun.fun_args.get(args, args)\n\t\t\tif cached_res is not None:\n\t\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoized_lfu_cache(fn):\n\t\tcache = {}\n\t\tdef memoized_lfu_cache(*args, **kw):\n\t\t\tkey = str(args) + str(kw)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = fn(*args, **kw)\n\t\t\treturn cache[key]\n\n\t\treturn memoized_lfu_cache\n\n\tif type(fn) is str:\n\t\tfn = fn.lower()\n\tif fn not in _lfu_cache:\n\t\tlogging.debug(\"Creating {} function with {}"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef lfu_cache_decorator(func=__lfu_cache__):\n\t\tdef lfu_cache_wrapper(*args, **kwargs):\n\t\t\treturn lfu_wrapper(*args, **kwargs)\n\n\t\tmaxfevals = 1000\n\t\tlru_freq = int(get_cached_memory() / 10)\n\n\t\t# Check whether lru_freq used to decide if cache should be\n\t\t# invalidated.\n\t\tlfu = LFU.LFU()\n\t\tmax_invalid_evals = 6 * max(1, max"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func):\n\t\t\"\"\"\n\t\t\tWrapper function that wraps the decorated function and memoizes\n\t\t\tsolution according to the given parameters.\n\t\t\"\"\"\n\t\tglobal _lfu_memo\n\n\t\tdef wrapped_func(*args, **kwargs):\n\t\t\t# print(args)\n\t\t\ttry:\n\t\t\t\tif _lfu_memo[func][args]:\n\t\t\t\t\tres = _lfu_memo[func][args]\n\t\t\t\t\t# print(\"res1\",res)\n\t\t\t\telse:\n\t\t\t\t\tres = func(*args, **kwargs)\n\t\t\t\t\t# print(\"res"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoizer(func):\n\t\t# Use the cache of the most recently used ID if the cache is\n\t\t# already expired\n\t\tif \"lfu_cache\" not in func.__dict__ or func.__name__ in func.cache_idx:\n\t\t\tmaxsize = func.cache_idx[func.__name__][\"maxsize\"]\n\t\t\tfunc.cache_idx[func.__name__] = func._cache_index\n\t\t\tlfu_cache_cached(func, maxsize, typed)\n\t\treturn func\n\t# Decorator format"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tif not typed:\n\t\tdef cache_wrap(func):\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapped_func(*args, **kwargs):\n\t\t\t\treturn func(*args, **kwargs, memo={'lfu': func.func_code.co_layers[-2:].co_freq,\n\t\t\t\t\t\t\t\t\t\t\t\t\t'hit': kwargs['hit'] + 1})\n\t\t\treturn wrapped_func\n\t\treturn cache_wrap\n\telse:\n\t\tdef cache_wrap(func):\n\t\t\t@functools.wraps(func)"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tif not typed:\n\t\tdef func(f):\n\t\t\tcache = LRU(maxsize, f)\n\t\t\t# Set cache decorator so that this decorator only calls\n\t\t\t# the function if the actual call fails.  (This protects against two\n\t\t\t# instances using the LRU algorithm using a for loop, in which\n\t\t\t# case the cache can't be saved across the next time we call\n\t\t\t# the function.\n\t\t\tdef cached(*args, **kwargs):\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[f]\n\t\t\t\texcept:\n\t\t\t\t\treturn f(*args, **kwargs"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef wraps(func):\n\t\tdef wrapped(data_id):\n\t\t\tif data_id in memoized:\n\t\t\t\treturn memoized[data_id]\n\t\t\tres = func(data_id)\n\t\t\tmemoized[data_id] = res\n\t\t\treturn res\n\t\tif typed:\n\t\t\treturn wrapped\n\t\treturn wraps\n\treturn wraps(_lfu_cache)\n\n@timing_wrapper\ndef lfu_cache(maxsize=128, typed=False):\n\tif typed:\n\t\treturn lfu_cache(maxsize=maxsize, typed=ty"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef cachedlfu(func):\n\t\tcache = LRU(maxsize=maxsize, typed=typed)\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = cache.get(func.__name__, None)\n\t\t\tif result is None:\n\t\t\t\tresult = cache(func.__name__, *args, **kwargs)\n\t\t\t\tcache.put(func.__name__, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn cachedlfu\n\ndef taf_cached(max"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError, 'key not in dict'\n\tself.move_to_end(self.last_todict_item)\n\tself[self.last_todict_item[0]]=self.last_todict_item[1]\n\tdel self.last_todict_item\n\treturn self.last_todict_item\n\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self.items.popitem() if self.items else ()\n\n\ndef delete(*keys, **kw):\n\t\"\"\"\n\tRemove one or more `(key, value)` pairs from the `config.json` file.\n\n\t:keykeys: One or more keys to operate on as a **keyvalue** item.\n\n\t\t1. If no ``key`` is specified, all configured keys will be updated.\n\n\t\t2. If at least two are specified, an exception will be raised.\n\n\t:key: the value to be delt.\n\n\t:configname: ``config"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself.popitem\n\ndef has_key(self, key):\n\t\"\"\" True if there is a key equal to `key` in the map \"\"\"\n\tif isinstance(key, str):\n\t\treturn key in self\n\tif isinstance(key, int) or isinstance(key, long):\n\t\treturn self.has_key(key)\n\tif isliceable(key):\n\t\tkey = list(self)\n\t\treturn any(has_key(k, v) for k, v in izip(key, self))\n\traise TypeError('"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\tindex = self.get_index_for(key)\n\t\treturn self.popitem(index)[0:2]\n\texcept:\n\t\treturn self[key]\t\t\t\n\t\n\n# class MyOrderedDict(OrderedDict, BaseDict):\n# \t\"\"\"\n# \tA `dictionary` that remembers and stores order details when inserted.\n#\n# \t>>> d = MyOrderedDict()\n# \t>>> d['bar'] = 3\n# \t>>> d['foo'] = 2\n# \t>>>"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\treturn self.__end\n\texcept AttributeError:\n\t\titem = self.next\n\t\tself.end = self.__heap[self.__index1 + 1]\n\t\tself.increase_index1()\n\t\treturn item\n\n\tdef extend(self, *args):\n\t\t\"\"\"Append all items in the collection.\"\"\"\n\t\tif len(args) < 1:\n\t\t\traise TypeError(\"Heap does not support appending\")\n\t\t\n\t\tself.__insert(heap.Element(*args))\n\t\n\tdef popall(self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tdata = self\n\tk, v = data.popitem()\t\n\treturn k, v\n\n\n\nclass ItemSet:\n    \"\"\" A collection of key/value pairs to be persisted in a file \"\"\"\n    def __init__(self, maxsize=1000):\n        self.store = {}\n        self.maxsize = maxsize\n        self.size = 0\n        \n    def __len__(self):\n        return self.size\n    \n    def __contains__(self, k):\n        return k in self.store\n        \n    def __get"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\tfor key in self:\n\t\t\tdel self[key]\n\texcept KeyError:\n\t\traise KeyError('%s is not mapped to an element' % key)\n\n\ndef copy(self):\n\tnew_env = os.environ.copy()\n\tnew_env.update(self.__dict__.copy())\n\tresult = base.TkinterEnvironment.copy(self, new_env)\n\tif get_tk_version() >= (5, 1, 0):\n\t\tresult.__class__ = Environment  # python-2.5"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__.popitem()\n\texcept KeyError:\n\t\traise KeyError(\"Dict is empty\")\n\n\t# The dictionary interface for OrderedDict uses key=itemgetter(0), so we need to use a kwarg to limit the search to\n\t# at most N keys.\n\tkeys = itemgetter(0)\n\tfor key, value in self.items():\n\t\tif keys(key):\n\t\t\treturn (key, value)\n\traise KeyError(\"Dict has no items that meet the criteria\")\n\n\nclass"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self.items.popitem()\n\ndef pop(self, *args, **kwds):\n\t\"\"\"\n\tRemove and return the value at the specified index (default last).\n\tAccepts the same arguments as the popitem method.\n\t\"\"\"\n\treturn self.items.pop(*args, **kwds)\n\ndef replace(self, *args, **kwds):\n\t\"\"\"\n\tReplace the first matched element with `*args` or the last matched element with `**kwds`.\n\tAll other arguments are the same as the popitem got method.\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tv = self.pop()\n\tself.push(v)\n\treturn v\n\ndef ordereditems(self):\n    \"\"\"\n        Return the items in the linked-list.\n    \"\"\"\n\n    k = dict()\n    stack = []\n    for item in self:\n        if not stack:\n            k[item] = []\n        else:\n            k[stack[-1]].append(item)\n        stack.append(item)\n    return map(  # stack -> list\n        lambda item: (item, k.get(item, item)),\n"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key not in self:\n\t\tself[key] = default\n\treturn self[key]\n\n# -------------------------------------------- #\n#   P.Paginating\n# -------------------------------------------- #\n\ndef paginate(*iterable):\n\tfor i, item in enumerate(iterable):\n\t\tyield item\n\t\tif isinstance(item, list):\n\t\t\tfor it in item:\n\t\t\t\tyield it\n\treturn\n\n# -------------------------------------------- #\n#   G.Mapping\n# -------------------------------------------- #\n\ndef to_key(obj):\n\tif isinstance(obj, bytes"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn(self.__dict__.setdefault(key, default))\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn get(self, key, default) if key is not None else default\n\n\ndef view(d):\n\t\"\"\"\n\tprint(str(d))\n\td.view()\n\t\"\"\"\n\treturn '\\n'.join('{}-{}'.format(k, v) for k, v in d.items())\n\n\n# endregion\n\n# region functions\n\n\n#################################################\ndef test_view():\n\td = dict()\n\ts = 'aaaaaaaaaa'\n\n\tdict(d)\n\tpass\n\t# for k, v in d."}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn self.__dict__.setdefault(key, default)\n\n  def __repr__(self):\n    return \"<%s at 0x%x [%s()]>\" % (self.__class__.__name__, id(self), self.__dict__.keys())\n\n  def __eq__(self, other):\n    if not isinstance(other, DictContainer):\n      other = DictContainer(other) \n    return (__eq__(self.__class__, other.__class__) and\n          self.__dict__"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self: return self[key]\n\tself[key] = default\n\treturn default\n\t\ndef _strlist(V):\t\n                if type (V) == dict or type (V) == list: return list([str(k).ljust(16) for k in V])\n                elif type (V) == tuple: return str(U) + \", \" + ', '.join([str(k).ljust(16) for k in V])\n                else: return V.tolist()  # a list\n                \ndef flatten(L"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn key in self if self is not None else key.get(self) or default\n\n\n\n\n\t\t\n\n\n\n\nclass Tree(object):\n\t\n\t\n\t\n\t'''\n\tThe Tree class of the CSE project, representing a tree of n nodes with (a, b) vertices\n\twhere a=x and b=y, represented by a single integer.\n\n\n\tThe class structure is designed to be the simplest and fastest working tree of any graph.\n\tThe class is used as a utility for simple queries in the query routines. \n\tThe tree, by default,"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\td=dict(self.items())\n\tif key not in d: d[key]=default\n\treturn d.get(key,default)\n\ndef todict(self): r=copy.deepcopy(self)\n\tif not r.indexes: return r\n\timport inspect\n\tm=inspect.getmembers(self)\n\tkeys=[]\n\tfor i,k in enumerate(r):\n\t\tif i==0 and k is self and len(m)==1: r=r[1:]\n\t\tif isinstance(k,slice):\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\ttry:\n\t\treturn D[key]\n\texcept KeyError:\n\t\tD[key] = default;return default\n\n\n\n# TODO:\n#  * check/reorganize (check/expand dict)\n#  * move to class with \"type: Class\" attribute\n#  * extend BaseHooks\n#  * Add ability to check if a callback has been called before\n#  * maybe use a special hook to handle missing key\n\ndef callback(event, *args):\n\t'''\n\tcallback(event, args):\n\tDo 'event' and"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tD = self.__dict__\n\tif key in D: return D[key]\n\telse: return default\n\t\"\"\"\n\tSet default to value, making first of list existing.\n\t\"\"\"\n\tdef first(self): return self.items()[0][0]\n\t\"\"\"\n\tReturn value at key or 'f' if empty.\n\t\"\"\"\n\tdef first_key(self): return self.items()[0][0]\n\n\tdef last(self): return self.items()[-1][0]\n\n\tdef last_key(self): return self"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn self.setdefault(key, default)\n\ndef join(sep, seq):\n\treturn sep.join(seq)\n\nclass Lazy(object):\n\t\"\"\"\n\tLazy\n\tReturns None upon first call.\n\t\"\"\"\n\tdef __init__(self, f):\n\t\tself.f = f\n\t\tself._cache = None\n\t\n\tdef __call__(self, *args, **kwargs):\n\t\tif args and kwargs:\n\t\t\traise SyntaxError(\"keyword argument and kwonly positional argument both needed\")\n\t\t"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.__dict__.get(key,default)\n\ndef lf(string):\n   punct = \".,!?'#:@%_()*+-./[]}'^<>?\"\n   cp = list(string)\n   cp.append(\" \")\n   i = 0\n   \n   while i < len(cp):\n      c = \"\"\n      if not(i == 0): #c is non-empty token\n         if cp[i-1] in punct:\n            while cp[i-1] in punct:\n               \n               "}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\tif key is not None: key_type = type(key).__name__\n\t\tif key_type.startswith('set_'):\n\t\t\tkey = key.replace('_', '-')\n\t\treturn self.__dict__.get(key, default)\n\texcept:\n\t\treturn default\n\nclass Mapping(MutableMapping, object):\n\tdef __init__(self, d):\n\t\tself.__dict__.update(d)\n\n\t\tfor k, v in list(self.__dict__.items()):\n\t\t\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key in self: return self[key]\n\treturn default\n\n\t\n   "}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tif default is not None:\n\t\t\treturn default\n\t\traise NoSuchElementException(key)\n\nclass Session(object):\n\t\"\"\"Represent a session of a web page.\n\n\tYou should not instantiate this object directly\n\tbut import it from the same directory as the\n\tSeleniumLibrary script.\n\n\t\"\"\"\n\tdef __init__(self, session_id, browser, url):\n\t\t\"\"\"\n\t\tInitializes this session with session_id and browser and url.\n\t\tAlso initializes the url"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key in self: return self[key]\n\n\treturn default\n\t\n\t\n\n\t\n\t\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\titem = self[key]\n\texcept KeyError:\n\t\treturn\n\ttry:\n\t\treturn item.get()\n\texcept RuntimeError:\n\t\treturn default\n\n#\n# Here is some more C-ish stuff that's taken, mainly from test.\n#\nclass Coverage(object):\n\tdef __init__(self, trace=0):\n\t\t\"\"\"Build a trace-profile of the code of each call site\n\t\t\"\"\"\n\t\tself.trace = trace\n\t\tself.callers = {} \n\tdef trace_stats(self):\n\t\t\"\"\""}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tif default == None:\n\t\t\traise\n\t\treturn default\n\n#-------------------------------\n#-- classes that define specialized dictionaries\n#---\n\n\nclass _Node(object):\n\t\"\"\"Definition of a Node in a dictionary.\"\"\"\n\n\t__slots__ = ('prev', 'Nxt')\n\t\n\tdef __init__(\n\t\tself, previous = None, Nxt = None\n\t):\n\t\t\"\"\"Create a new node.\"\"\"\n\t\tself.prev = previous\n\t\tself.Nxt = N"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key not in self:\n\t\treturn default\n\treturn self[key]\n\n\t\n\nclass MySimpleReduce:\n\tdef __init__(self, fn):\n\t\tself.fn = fn\n\n\tdef __call__(self, *args):\n\t\treturn self.fn(*args)\n\nclass MyMiningCycle:\n\tdef __init__(self, f, cycle=24*48, max_cycle=1):\n\t\tself.f = f\n\t\tself.cycle = cycle\n\t\tself.max_cycle = max_cycle\n\t\tself.it"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key in self:\n\t\treturn getattr(self, key)[0]\n\telse:\n\t\treturn default\n\n\t@staticmethod\n\tdef default(key):\n\t\treturn []\n\ndef validate_dict(d):\n\tif not type(d) == dict:\n\t\traise TypeError(\"{0} should be a dictionary. Did you use {0}?\"\n\t\t\t\t.format(type(d).__name__))\n\nclass Data(object):\n\t__slots__ = ['value', 'keys', 'count', 'data']\n\tdef __init__("}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept:\n\t\treturn default\n\n\nclass Mapping(dict):\n\t\"\"\"A dict that follows the interface of dict objects.\"\"\"\n\tdef __getitem__(self, key):\n\t\t\"\"\"D.__getitem__(k[,d]) -> D[k].\"\"\"\n\t\treturn self.get(key)\n\tdef __setitem__(self, key, val):\n\t\t\"\"\"D[k]=v.\"\"\"\n\t\tif isinstance(val, Mapping):\n\t\t\tif key in self:\n\t\t\t\tD = self"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tcacheEntry = key\n\tif isinstance(cache, BaseCache): # if calling the class-based method, this check should be removed\n\t\tif isinstance(cache, type({})):\n\t\t\tentryInfo = cache[cacheEntry]\n\t\telif hasattr(cache, \"_Cache__entryinfo\"): # if calling the derived method, this check should be removed\n\t\t\tentryInfo = cache._Cache__entryinfo\n\t\treturn _dictCachedMethod(cache, key, entryInfo)\n\telif hasattr(cache, \"_Cache__entryinfo\"): # if calling the derived method"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef cachedmethod(obj):\n\t\tif not hasattr(obj, key):\n\t\t\tsetattr(obj, key, _cache_mapping_dict(obj, cache, lock, hash=True))\n\t\treturn getattr(obj, key)\n\tcachedmethod.__wrapped__ = obj\n\treturn cachedmethod"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\timport functools\n\timport inspect\n\tif lock is None:\n\t\tlock = Lock()\n\tif key is not None:\n\t\t# Decorate the (object, funct, args, kwargs) callable\n\t\tdef cachedmethod(instance_or_cls):\n\t\t\tif key is not None:\n\t\t\t\tkey = hashkey + key\n\t\t\tif inspect.isclass(instance_or_cls):\n\t\t\t\tself = functools.partial(instance_or_cls.__dict__['__cachedmethod__'], **instance_or_cls"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef _get_cache(self):\n\t\treturn getattr(self, key)\n\n\tdef _set_cache(self, value):\n\t\tsetattr(self, key, value)\n\n\tcachekey = hashkey\n\t_cache_initialized = False\n\t_cache_lock = threading.Lock()\n\n\tif lock:\n\t\t@functools.wraps(cachemethod)\n\t\tdef wrapper(self, v, cache_key=cachekey):\n\t\t\texpiretime = time.time() + lock.expiry_time # lock.expiry_time"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tresult = key\n\tif not isinstance(result, string_types):\n\t\tresult = repr(result)\n\t@functools.wraps(cache.__class__.cache_function__)\n\tdef f(*args, **kwargs):\n\t\tkey = result, args, kwargs\n\t\tif key in cache._all_caches:\n\t\t\treturn cache._all_caches[key]\n\t\telse:\n\t\t\tresult = cache.cache_function__(*args, **kwargs)\n\t\t\tcache._all_caches[key] = result\n\t\t\tcache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\tdef cachedmethod(cls):\n\t\tif cache is None:\n\t\t\t@functools.wraps(cls)\n\t\t\tdef wrapped(obj):\n\t\t\t\tif lock is None:\n\t\t\t\t\treturn cls.cache_get(obj, key)\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = cache[key] = cls.cache_get(obj, key, lock=lock)\n\t\t\t\treturn result\n\t\t\treturn wrapped\n\t\telse:\n\t\t\t@functools.wraps(cls)\n\t\t\tdef wrapped"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif lock:\n\n\t\tdef cachedmethod(input_cls):\n\t\t\tif id(input_cls) not in cache:\n\t\t\t\tcache[id(input_cls)] = (input_cls, {})\n\t\t\tcached = cache[id(input_cls)]\n\t\t\tif key is None:\n\t\t\t\tif isinstance(input_cls, type) or isclass(input_cls):\n\t\t\t\t\treturn cachedmethod(input_cls)\n\t\t\t\telse:\n\t\t\t\t\treturn cachedmethod(type(input_cls))\n\t\t\telif not isinstance(key"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif lock:\n\t\tif callable(cache):\n\t\t\treturn cachedmethod(cache, lock=lock)\n\t\telif isinstance(cache, dict):\n\t\t\treturn cachedmethod(list, key=key, lock=lock)\n\tdef decorator(method):\n\t\tsetattr(method, '_cache', {})\n\t\tsetattr(method, '_cache_key', key)\n\t\tsetattr(method, 'cache', cache)\n\t\treturn method\n\treturn decorator\n\n@cachedmethod(cache={})\ndef get(key):\n\t\"\"\"\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\ttry:\n\t\tif hasattr(key, '__call__'):\n\t\t\t# Wrap a method with __call__ to memoize its results\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\treturn cache[key(cache, lock=lock)]\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\telse:\n\t\t\tif lock is None:\n\t\t\t\tlock = Lock()\n\t\t\ttry:\n\t\t\t\t# Return cached callable fn(self) if self.method has been wrapped in a cachedmethod()\n\t\t\t\tif hasattr(key,"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t_cache = cache\n\n\tdef wrapper(cls, *args, **kwargs):\n\t\tif (cls in _cache) and _cache[cls] is None:\n\t\t\tmethod = getattr(cls, key)\n\t\t\t_cache[cls] = method(*args, **kwargs)\n\t\telse:\n\t\t\tmethod = None\n\t\t\ttry:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release_lock()\n\t\t\t\t_cache[cls] = method(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\te_class, e_value, e_traceback = sys.exc_info()\n\ttrace_msg = 'Call:\\nStack: '+''.join(traceback.format_tb(e_traceback))\n\t\n\t# The traceback in Python 3 uses three nested dicts, one for the main stack and\n\t# another for the 3 child calls in the stack, and the third one for the filename/line\n\t# information that is about the call stack\n\t# We'll need only the main call stack, but we'll need 3 nested dicts, from two\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\tmax_level += 1\n\t\tlevel = \"|\" * max_level\n\t\tstr_level_e = f\"{level} {type(e).__name__.upper()}: {repr(e)}\"\n\t\tstr_e = f\"{str_level_e} ({e.__class__.__name__.lower()} - '{e}')\"\n\t\treturn str_e\n\texcept Exception as e:  # try to deal with subclasses\n\t\t# print(e)  # Debug information on the exception instance\n\t\tstr_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn (\n\t\t'{}:{:s} [{:.0f} s]'.format(cls.__name__.ljust(max_level-3), e.__class__.__name__, e.__args__[0]) + _traceback.format_exc()\n\t)\n\n\ndef _get_name_paths(klass_name, klasses, klass, name=''):\n\t\"\"\"\n\tGenerate names for given classes and klass objects for display in the source.\n\t:param klass_name: Classname for"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\te.msg = \"{0} (level {1:.1f} path {2:.1f}): {3}\".format(\n\t\te.msg, max_level, max_path_level - 2, e\n\t)\n\treturn e.__str__()\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tout_str = \" \"*max_level\n\tout_str = \"[\"+e.__class__.__module__+\"]\"\n\tout_str = f\" ({out_str} {e()}\\n{out_str}\\n)\"\n\treturn out_str\n\n\ndef stringobj_to_byteobj(val):\n\t\"\"\"\n\tConvert a string object to a bytes object.\n\n\t:param val: Any object.\n\t:type val: Any\n\t:return: A bytes object representing `val`\n\t:rtype: bytes\n\t\"\"\""}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tfrom exterm.formatter.text import fmt_str\n\tfrom exterm.formatter.text import fmt_long_text\n\n\ttry:\n\t\tform_str = fmt_str(e.__class__.__name__, max_level, fmt_long_text)\n\texcept TypeError:\n\t\tform_str = fmt_str(e)\n\n\ttry:\n\t\tstack_str = fmt_str(e.__traceback__.tb_frame).replace('\\n', '')\n\t\tstacks = stack_str.split('\\n')\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\timport traceback\n\n\ttry:\n\t\tname = (\n\t\t\tgetattr(e, \"classname\", \"\").split(\".\")[0]\n\t\t\tif hasattr(e, \"classname\")\n\t\t\telse getattr(e, \"name\", \"\")\n\t\t)\n\t\tdesc = str(e)\n\t\tfinally_name = name\n\t\tfinally_desc = desc\n\t\tif id(finally_desc) in {id(finally_e.__class__) for finally_e in traceback.extract_stack(e.__traceback__, limit="}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tmax_level -= 1\n\tprint repr(e)\n\t# Get a traceback\n\texc_tb = e.__traceback__\n\texc_vars = exception_var(e, exc_traceback=exc_tb)\n\t# Build the list from our args\n\targspec = inspect.getfullargspec(e.__call__)\n\targs = [c for c in argspec.args if c!='self']\n\t# Build the full string\n\tfullstr = ', '.join([f\"{j.ljust(8)}="}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\te = get_last_ex(e)\n\treturn ('''\n        {0:<{max_level}{1}s} {2}'''.format('-' * max_level, int(type(e)), str(e)))\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t# get the exception message\n\tmessage = getattr(e,'message', 'Exception: \"type'+ repr(e) + '\"')\n\t# get the exception string\n\texc_str = _TracebackExceptionString_(message)\n\t# get the source traceback\n\texc_sinfo = [(''.join(traceback.format_stack()), index) for index, traceback in enumerate(traceback.extract_stack())]\n\t# filter out unhandled exceptions\n\tif exc_sinfo:\n\t\texc_str = _filter_"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tbuffer_length = len(text_buffer.encode(encoding))\n\tif file_name is None:\n\t\treturn buffer_length\n\telse:\n\t\tif not overwrite and os.path.exists(file_name):\n\t\t\tif os.stat(file_name).st_size > buffer_length:\n\t\t\t\traise Exception(\"File %s already exists with length=%d.\" % (file_name, len(text_buffer)))\n\t\t\telse:\n\t\t\t\treturn file_name\n\t\treturn os.write(0, text_buffer.encode(encoding))"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif not os.path.isfile(file_name):\n\t\traise ValueError('The file name \"{0}\" for the writing operation is not valid.'.format(file_name))\n\tif text_buffer and encoding:\n\t\traise ValueError('You cannot specify both text_buffer, and the encoding. If you have created the output file as a text buffer, use binary write instead.')\n\tif not text_buffer:\n\t\ttext = file_name\n\telif not os.path.exists(text_buffer):\n\t\traise ValueError('The provided text buffer does not"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t# Check for file-type.\n\tfile_type = os.path.splitext(file_name)[1][1:]\n\n\t# Create file.\n\tif file_type == 'txt' and not os.path.exists(file_name):\n\t\twith open(file_name, 'w+') as file:\n\t\t\tfile.write(text_buffer)\n\t\t\treturn 0\n\n\t# Check to see if file exists.\n\tif not os.path.exists(file_name):\n\t\twith open(file_name, 'w+', ) as"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\twith open(file_name, 'wb') as f:\n\t\tf.write(text_buffer)\n\t\tif overwrite:\n\t\t\t# Modify file timestamp to be valid\n\t\t\tos.utime(file_name, (time.time() - int(time.time()),))\n\treturn len(text_buffer)\n\n\ndef load_data(filename) -> list:\n\t\"\"\"\n\t\tTry to load the passed filename.\n\t:param filename: File to read.\n\t:type filename: str\n\t:return: List of all found strings\n\t:rtype"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tfile = open(file_name, \"a\")\n\tfile.write(text_buffer.encode(encoding))\n\tif overwrite:\n\t\tfile.truncate(0)\n\treturn len(text_buffer.encode(encoding))\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tf = open(file_name, 'w')\n\tout_buffer = bytearray(f)\n\tif (overwrite and f.seekable) or not overwrite:\n\t\tf.write(text_buffer)\n\tcount = 0\n\ttry:\n\t\twritten = f.write(text_buffer)\n\texcept:  # pylint: disable=bare-except\n\t\treturn 0\n\tfinally:\n\t\tf.close()\n\t\treturn written\n\tif isinstance(text_buffer, bytes):\n\t\ttext_buffer = text_buffer.dec"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t# If not a file or write buffer is specified, return 0\n\tif not os.path.exists(file_name) or overwrite:\n\t\treturn 0\n\n\twith open(file_name, 'wb+') as opened_file:\n\t\t# Write to file if not overwriting\n\t\topened_file.write(text_buffer)\n\t\topened_file.flush()\n\t\topened_file.seek(0)\n\t\topened_file.write(encoding)\n\t\topened_file.flush()\n\t\topened_file.seek(0, os.SEEK_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif file_name.startswith('/'):\n\t\tfile_name = os.path.basename(file_name)\n\tfile_name = file_name.encode(encoding, 'ignore').strip()\n\tif overwrite or os.path.isdir(file_name) or not os.path.isfile(file_name):\n\t\twith codecs.open(file_name, 'wb', encoding) as f:\n\t\t\tf.write(text_buffer)\n\t\treturn 0\n\telse:\n\t\traise IOError('Already existing file')\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tdata = os.urandom(256)\n\tif encoding:\n\t\tdata = data.decode(encoding)\n\twith open(file_name, \"ab\") as f:\n\t\tf.write(data)\n\t\tif not overwrite:\n\t\t\tif os.path.getsize(file_name) > 0:\n\t\t\t\tprint(\"Warning: This already exists. If you want to overwrite, use a new one. \")\n\t\t\treturn 0\n\t\tdata = base64.b64encodebytes(data)\n\t\tprint(\"+{})\".format(data.decode"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttry:\n\t\topen_file = open(file_name, \"wb\")\n\t\topen_file.write(text_buffer)\n\t\topen_file.close()\n\t\treturn len(text_buffer)\n\texcept Exception as e:\n\t\tif overwrite == False:\n\t\t\traise\n\t\topen_file.close()\n\t\topen_file.close()\n\t\tprint(\"Error opening file due to: \" + str(e))\n\t\tprint(traceback.format_exc())\n\t\treturn 0\n\n\ndef read_text_from_file(file_name, encoding, line"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, 'rb')\n\t\tdata = f.read()\n\t\tf.close()\n\texcept IOError:\n\t\treturn None\n\t# Open a text buffer and fill it with everything in the source file\n\tbuf = memoryview(create_string_buffer('\\x00' * len(data)))\n\tlength_data = len(data)\n\n\ttry:\n\t\tlength_source_str = data[0:data_buffer_size].decode(encoding)\n\texcept ValueError:\n\t\ttry:\n\t\t\treturn"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tbuffer = open(file_name, 'r')\n\ttry:\n\t\ttext = buffer.read()\n\texcept UnicodeDecodeError as err:\n\t\tos.remove(file_name)\n\t\tif encoding == 'utf-16':\n\t\t\ttext = buffer.read((bytes.fromhex(err.reason)).decode('utf-16-le'))\n\t\telif encoding == 'utf-16-be-le':\n\t\t\ttext = buffer.read((bytes.fromhex(err.reason)).decode('utf-16le'))\n\t\telif encoding == '"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith open(file_name, 'rb') as file:\n\t\treturn file.read().decode(encoding)\n\n\ndef load_text_buffer(filename, encoding):\n\t\"\"\"\n\tLoad a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param filename: File name.\n\t:type filename: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:r"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttextBuffer = \"\"\n\ttry:\n\t\twith io.open(file_name, \"r\", encoding=\"UTF-8\") as f:\n\t\t\tfor line in f:\n\t\t\t\ttextBuffer += line.strip() + \"\\n\"\n\t\t\treturn textBuffer\n\texcept Exception as error:\n\t\treturn error\n\ndef read_textbuffer(textbuffer, encoding=\"UTF-8\"):\n\t\"\"\"\n\t\tRead a textbuffer (UTF-8), using the specify encoding while reading.\n\t\tCAUTION : This will read the whole file IN MEMORY.\n\t\t:param textbuffer:"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\treturn \"\".join(f'<byte>{c}{encoding}\\n' for c in open(file_name).read().encode(encoding, 'ignore'))\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\treturn codecs.open(file_name, encoding=encoding, mode=\"r\").read()\n\ndef file_to_bytes(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a binary stream (CP850).\n\tCAUTION : This will read the whole file IN MEMORY even if it is a text buffer.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: Byte format of the file, or None if not found"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\t# if the file doesn't exist, return None\n\t\twith io.open(file_name, \"r\", encoding = encoding) as readfile:\n\t\t\treturn readfile.read()\n\texcept FileNotFoundError:\n\t\t# if the file does\n\t\treturn None\n\ndef file_to_textbuffer_with_size(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith open(file_name, 'r', encoding='utf-8') as file:\n\t\treturn file.read().encode(encoding).decode('utf-8')\n\n\nfile='new'\ntbfs=file_to_textbuffer(file,encoding='utf-8')\nif tbfs is None:\n\tprint(\"Could not open file %s\"%(file))\nelse:\n\tprint(\"Textfile successfully writeped.\")\n\ttbfs=u\"\u6587\u4ef6\u5df2\u7528utf-8\u89e3\u7801"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith open(file_name, 'rt', encoding = 'utf-8') as f:\n\t\traw = f.read()\n\ttext = raw.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n\treturn text\n\ndef get_file_name(file_name):\n\t\"\"\"\n\tGet the filename of the file.\n\t:param file_name: Name of the file.\n\t:type file_name: str\n\t:return: The name of the file.\n\t:rtype: str\n\t\"\"\"\n\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, \"rb\") as f:\n\t\t\tbuffer = _read_bytes_from_file(f)\n\t\tif encoding!= \"None\":\n\t\t\t# Read the entire content in the buffer, not just the part used for the encoding\n\t\t\tbuffer.append(struct.unpack(encoding, buffer[:]))\n\t\treturn buffer\n\texcept FileNotFoundError:\n\t\te = e.FileNotFoundError(\"File not found - '%s'.\" % file_name)\n\t\t_raise_unavailable_file(file_name)\n"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not isinstance(file_name, str):\n\t\traise TypeError(\"file_name must be str\")\n\tif not os.path.isfile(file_name):\n\t\treturn False\n\telse:\n\t\treturn True\n"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn(os.path.exists(file_name))\n\n\ndef is_dir_exist(file_name):\n\t\"\"\"\n\t\tCheck if a directory name exist.\n\t\t:param file_name: File_Name\n\t\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\n\treturn(os.path.isdir(file_name))\n\n\ndef convert_bytes_to_str(size_bytes):\n\t\"\"\"\n\t\tConvert size_bytes"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n\ndef get_files_in_dir(dir_path=None):\n\t\"\"\"\n\tGet files in directory.\n\t:param dir_path: Directory path\n\t:type dir_path: str, optional\n\t:return: Return an array of path.\n\t:rtype list\n\t\"\"\"\n\tfiles = list()\n\tfor i in (os.listdir(dir_path) if dir_path is not None else os.listdir()) :\n\t\tif os.path.isdir(os.path"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.exists(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n\ndef get_file_size(file_name):\n\t\"\"\"\n\tGet file size by file name.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: File size.\n\t:rtype int\n\t\"\"\"\n\tsize = 0 # Keep track of the total size.\n\tif is_file_exist(file_name): # Check if file exist\n\t\twith open(file_name, '"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfile_exist = os.path.exists(\".\\\\{}\".format(file_name))\n\treturn file_exist\n\n\n# \u540d\u6b21\u6c42\u5e73\u5747\u503c\ndef extract_number(p_text):\n\t\"\"\"\n\t:param p_text: \u8981\u89e3\u6790\u51fa\u7684html\u5b57\u7b26\u4e32\u3002\n\t:type p_text: str\n\t:return: \u4ece\u89e3\u6790\u4e2d\u63d0\u53d6\u6587\u672c\u4e2d\u6587\u5b57"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.isfile(file_name):\n\t\treturn False\n\treturn True\n\n\n\ndef mkdir(path):\n\t\"\"\"\n\tMake directory folder.\n\t:param path: Folder path.\n\t:type path: pathlib.Path\n\t\"\"\"\n\tif path.exists():\n\t\tshutil.rmtree(path)\n\t\tos.makedirs(path)\n\n\n\ndef rm_file(path):\n\t\"\"\"\n\tDelete a file.\n\t:param path: File path.\n\t:type path: file path.\n"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.exists(file_name)\n\n\ndef get_last_modified_time(file):\n\t\"\"\"\n\t\tGet the last modified time from a file.\n\t\t:param file: Name of the file.\n\t\t:type file: str\n\t\t:return: Last modified time of the file.\n\t\t:rtype: time\n\t\t\"\"\"\n\treturn datetime.now()\n\n\ndef create_sub_directory(file_name, sub_directory):\n\t\"\"\"\n\t\tCreate sub directory.\n\t\t:param file_name: File name.\n\t\t:type"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\topen(file_name, \"r\")\n\t\treturn True\n\texcept FileNotFoundError:\n\t\treturn False\n\n\ndef is_path_exists(dir_path):\n\t\"\"\"\n\tCheck if the directory is exist.\n\t:param dir_path: Directory path.\n\t:type dir_path: str\n\t:return:\n\t\tReturn true (exists), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t_directory = dir_path.split(\"/\")\n\tif not len(_directory"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfile_exists = os.path.isfile(file_name)\n\treturn file_exists\n\ndef check_dir_exist(directory, directory_name):\n\t\"\"\"\n\t\tCheck if directory path exists\n\t:param directory: Directory path\n\t:type directory: str\n\t:param directory_name: Directory name\n\t:type directory_name: str\n\t:return: Check if directory exist:\n\t\tTrue if directory exist and False otherwise.\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\tif not os.path.exists(directory):"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.isfile(file_name):\n\t\treturn False\n\treturn True\n\n\ndef is_file_writable(file_name):\n\t\"\"\"\n\tCheck if file name is writable.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return True (writable), false (not writable), otherwise.\n\t:rtype bool\n\t\"\"\"\n\twritable = os.access(file_name, os.W_OK)\n\t# print(\"file_name => %s\" %"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlogging.disable(logging.CRITICAL)\n\n\ndef parse_csv(path: str, delimiter=','):\n\tdata = []\n\tfor line in open(path, encoding='utf-8'):\n\t\trow = line.strip().split(delimiter)\n\t\tif len(row) < 2:\n\t\t\tcontinue\n\t\tassert len(row) == 4\n\t\td = {row[k]: int(row[v]) for k, v in enumerate(range(5, len(row)))}\n\t\tdata.append(d)\n\t\t#"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tfor handler in cls.get_log_handlers():\n\t\t_logger.removeHandler(handler)\n\t\t_logger.setLevel(cls.LOG_LEVEL)\n\t\tif cls.LOG_LEVEL == 'info' or cls.LOG_LEVEL == 'debug':\n\t\t\thandler.setFormatter(cls.LOG_FORMAT)\n\t\t\t_logger.addHandler(handler)\n\tcls.LOG = cls.LOG + \".reset\"\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\timport logging\n\tlogging.basicConfig(format='%(asctime)s.%(msecs)03d: %(levelname)s (%(filename)s:%(lineno)d): %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    filename=\"logs/%s-%s\" %(cls.__module__, cls.__name__) + \".log\" if cls.__module__ is not None else \"logs/NoLog."}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._log_dict = dict()\n\tif len(cls._log_dict) > 0:\n\t\tif 'level' in cls._log_dict:\n\t\t\tdel cls._log_dict['level']\n\t\tcls._log_dict['log'].flush()\n\tcls.log.info(\"Reset logging\")\n\tsys.stderr = stderr\n\n\n\nclass Config(object) :\n\t\"\"\"\n\tConfig\n\t\"\"\"\n\t\t\n\t@property\n\tdef log(self):\n\t\treturn logging.getLogger("}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif cls.log_level.value > 0:\n\t\tlogging.basicConfig(\n\t\t\tlevel=cls.log_level.value, \n\t\t\tformat='%(asctime)s %(levelname)8s [%(lineno)d] %(message)s (%(process))s'\n\t\t)\n\n\nclass Server(LoggingModule(Thread):\n\t\"\"\"\"\n\t\tThe Server is the main server that it sits on. The user may\n\t\tdefine some other servers.\n\t\"\"\"\n\t# The name is the key to the key/value"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlog.info(\"==>\"*15)\n\tlevel = str(log.ERROR).title()\n\tif level == \"DEBUG\":\n\t\tlevel = \"ERROR\"\n\telif level == \"WARNING\":\n\t\tlevel = \"WARNING\"\n\telif level == \"INFO\":\n\t\tlevel = \"INFO\"\n\telif level == \"VERBose\":\n\t\tlevel = \"VERBOSE\"\n\telse:\n\t\tlevel = \"DEBUG\"\n\n\tif level!= \"INFO\":\n\t\tlog.level(level)\n\n\tlog.info(\"====>\")\n\tlog."}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlogging.Formatter.FORMAT = f\"%(asctime)s %(name)-12s %(levelname)-8s %(message)s\"\n\n\n# def _send_sms(phone, message, token_file=None, token=None):\ndef send_mms(phone, message, token_file=None, token=None):\n\t\"\"\"\n\tSend MMS via SMS-MMS protocol\n\n\tParameters\n\t----------\n\ttoken_file : str, optional\n\t\tFile containing the token. If None, no token will be provided. "}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.LOGGER = logging.getLogger(__name__)\n\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\t\thandler.close()\n\n\tlogging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s',\n\t\t\t\t\t  datefmt='%H:%M:%S',\n\t\t\t\t\t  level=logging.INFO)\n\n\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.logger = logging.getLogger()\n\tcls.logger.setLevel(logging.DEBUG)\n\tcls.formatter = logging.Formatter('[%(levelname)s]#%(message)s')\n\tfile_handler = logging.FileHandler('logging.log', encoding='utf-8')\n\tfile_handler.setFormatter(cls.formatter)\n\tcls.logger.addHandler(file_handler)\n\n\nclass Config(object):\n\t\"\"\"\n\tconfig\n\t\"\"\"\n\tdebug"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._logger = None\n\tcls._logging_config = None\n\nclass ModelingLogLevel(Enum):\n\t\"\"\"\n\t\tModels logging levels\n\t\"\"\"\n\tNONE = 1\n\tTRAINING = 2\n\tTEST = 3\n\nclass ModelingConfiguration:\n\t\"\"\"\n\tContains all settings for the model\n\t\"\"\"\n\tdef __init__(self, log_level=MelingLogType.debug, input_length=60, output_length=5, data_type=str):\n\t\tself._log_level"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tif __debug__:\n\t\timport pprint\n\t\tpprint.pprint(locals().items())\n\treturn DefaultImpl\n\n\n# -----------------------------------------------------------------------------------\n#\n#  PyOpenSSL Implementation\n#\n# -----------------------------------------------------------------------------------\n\nclass _implementation(object):\n\t\"\"\"\n\tThis class will hold the implementation used by the PyOpenSSL module.\n\tIt will take care of most of the methods and properties related to conversion\n\tbetween Python OpenSSL types and C libraries.\n\t\"\"\"\n\t# the name of the module.\n\timport_name = __name__\n\n\t"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn Base64Image\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t# if we have an implementation here, we're already done\n\tif self.implementationExists(self.implementation):\n\t\treturn Implementer\n\t# maybe we have a fallback interface, make it explicit\n\tif self.implementationExists(FallbackImplementation):\n\t\tfrom.FallbackImplementation import FallbackImplementation\n\t\tself.implementation = FallbackImplementation\n\n\treturn Implementer\n\nclass AdapterWrapper(object):\n\t\"\"\"\n\t\tAn adapter that forwards methods over all adapters so that the\n\t\timplementation sees the adaptation, just as in the original"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tpass\n\ndef _precompileClassName(module):\n\t\"\"\"\n\tModule-internal, get the built-in package name and extension name for the module,\n\tby default, use the standard package name for py_module\n\t\"\"\"\n\tif module.startswith(\"py_module\"):\n\t\timport_name = module.split(\"_\")[-1]\n\t\treal_name = \".\".join( import_name.split(\".\")[:len(module.split(\".\"))] )\n\telse:\n\t\timport_name = module.name\n\t\t"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn PyEz\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tpass\n\n\nclass DummyTranslateString(object):\n\tdef __init__(self, target):\n\t\tself.target = target\n\n\tdef translate(self, text, context=None):\n\t\tif isinstance(text, string_types):\n\t\t\ttext = translateString(text, self.target)\n\t\treturn text\n\n\n\n@implementer(ITranslateString)\nclass StringTranslator(string.backend.StringTranslator):\n\t\"\"\"\n\tA backend that uses 'DummyTranslateString' as a fallback and falls back when\n\tit does"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn IUnknown\n \nclass IDomainObject(object):\n\t\"\"\"\n\tThe IDomainObject allows developers to read and write domain\n\tobjects for their virtual servers when they are known.\n\t\"\"\"\n#  To be extended by some other IDomains classes\n\nDictionary = {}\nDictionary[\"id/domain\"] = IDomainObject\nDictionary[\"id/domain/DIDomain\"] = IDomainObject\nDictionary[\"id/domain/IDomain\"] = IDomainObject\nDictionary[\"id/domain/IDomain/IDomain"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tfrom WMCore.JobSplitting.JobSplitter import JobSplitter \n\treturn JobSplitter\n\n\nfrom WMCore.JobSplitting.JobSplitterEval import JobSplitterEval\n\nclass PyJobSplitterEval(PyJobSplitter):\n\t\"\"\"\n\tA generic Splitter implementation that is defined by a job splitting\n\tscript such as\n\t\n\t\t[job splitter script]\n\t\t\n\t\t# -- options\n\t\tsplit = 1\n\t\t\n\t\t# -- split data directories \n\t\tdataDir = /some/location \n\t\t\n\t\t"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn PyGetOptMenuHelper\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tpass\n\n\n#------------------------------------------------------------------------------\ndef _test():\n\t\"\"\"\n\tA test function with 'py' or 'Fallback'.\n\t\"\"\"\n\t#print _test_()\n\n#------------------------------------------------------------------------------\n# run the test\nif __name__ == '__main__':\n\t_test()\n\n#------------------------------------------------------------------------------\n"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tout = []\n\tdef getAllOrderings(item):\n\t\tif type(item) is list:\n\t\t\treturn getAllOrderings(item[-1])\n\t\telse:\n\t\t\treturn [item]\n\tfor ordering in orderings:\n\t\tif len(ordering) == 0:\n\t\t\tcontinue\n\t\tsort = _Sort(ordering)\n\t\tinOrders = getAllOrderings(ordering)\n\t\tfound = False\n\t\tout.append(sort(inOrders))\n\t\tfor i in range(len(orderings)-1):\n\t\t\tif orderings[i]["}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\treturn [t for t in orderings if any(i.startswith(t[0]) for i in t)]\n\n\ndef _mergeIndices(indices, suffix):\n\t\"\"\"\n\tTakes a tuple of strings and adds them to a string with the same\n\tnumber of digits.\n\n\tSee _mergeTuplets() if this is relevant\n\t\n\t>>> assert _mergeIndices((3,1,\"A\",\"z\"), (3,1,\"A\",\"z\")) == \"3_1A.z\"\n\t\n\t>>> assert _mergeInd"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\treturn [[o[0] for o in orderings]] + \\\n\t\t[(_mergeOrderings([o, o[1:]])\n\t\t  for o in orderings[1:]] if isinstance(orderings[1], (list, tuple))\n\t\t  else [o]) for o in orderings]\n\ndef _mergeAndCompact(l: list, m: list, r: list) -> list: \"\"\"\n\tApply merge and compact functions.\n\t\"\"\"\n\treturn merge(l, m, r)\n\ndef merge(l: list, m"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t# Sort orderings by length in terms of number of characters in each\n\t# ordering line.  This is not necessarily optimal, but does not change\n\t# if it are used.\n\tshortest = -float('inf')\n\n\tif len(orderings) == 0:\n\t\treturn orderings\n\n\tshortest = min(shortest, len(sorted(orderings, key=len)))\n\n\t# First, merge lines that do not include a suffix.  We will append\n\t# that line until we find a non-suffix.\n\tfor i in range("}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tnew = []\n\tseen = set()\n\tfor ordering in orderings:\n\t\tfor order in ordering:\n\t\t\tif (order in seen):\n\t\t\t\tcontinue\n\t\t\tif order.startswith(('', '-', '|\\\\', '\\\\|')) and order!= '-':\n\t\t\t\tif order not in new:\n\t\t\t\t\tnew.append(order)\n\t\t\tseen.add(order)\n\treturn new\n\nclass Query(Document):\n\t\"\"\"\n\tA query object maps all query filters used to filter documents on a\n\tspecific collection. These filters can be used for"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tresult = []\n\tfor order in orderings:\n\t\tif order == [] or hasattr(order[-1],\"names\"):\n\t\t\tpass\n\t\telif all(len(o.names)+len(args)==2 for o in order):\n\t\t\tresult.extend(order)\n\t\telse:\n\t\t\t# if len(order)==3:\n\t\t\t\t# result.extend(order)\n\t\t\tresult.extend(order[:-1])\n\t\t\tresult.append(order[-1])\n\treturn result\n\n#####\n\nif __name__ == '"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tnewOrderings = []\n\tfor order in orderings:\n\t\tif len(set(order).intersection(set(newOrderings))) > 0:\n\t\t\traise Exception('orderings conflict:')\n\t\tfor k in order:\n\t\t\tif k not in newOrderings and k!= 'z':\n\t\t\t\treturn None\n\t\tnewOrderings += order\n\treturn newOrderings\n\t\t\n\t\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tnewOrderings = []\n\t\n\tfor order in orderings:\n\t\tnewOrderings.extend(order)\n\t\n\tremoveOrderings = []\n\tfor order, originalOrder in zip(newOrderings, orderings):\n\t\tif originalOrder!= order:\n\t\t\tremoveOrderings.append(order)\n\t\n\tif removeOrderings:\n\t\tfor order in removeOrderings:\n\t\t\tnewOrderings.remove(order)\n\t\n\treturn newOrderings\n\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\ti = [0]\n\tfor e in orderings:\n\t\twhile i and e[0][:len(i[0])] == i[0]:\n\t\t\tif isinstance(e[0], _compatString_type):\n\t\t\t\tt = e[0]\n\t\t\telif isinstance(e[0], basestring):\n\t\t\t\tt = ''.join(e[0])\n\t\t\telse:\n\t\t\t\tassert isinstance(e[0], list), repr(e[0])\n\t\t\t\tt = ''.join(e[0])\n\n\t\t\tif isinstance(e"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t# TODO: implement\n\t\n\treturn []\n\t\n\t\n\t\ndef _sortList(L, ignore):\n\t\"\"\"\n\tSort an input L list of values \n\t\n\tL is a list of values\n\tignore is a list containing non-value keys that should\n\tnot be included. E.g., ['id'].\n\t\"\"\"\n\t# NOTE DO NOT MODIFY\n\t#\n\t\n\tignore, S = set(ignore), set(L)\n\t\n\tdef sortList(L):\n\t\treturn sorted(L, key"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn interfaces.providedBy(object)  # type: ignore\n\n\ndef directlyProvidedBy(interfaces):  # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces directly provided by the given interfaces\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn interfaces  # pylint:disable=redefined-builtin\n\nclass _InterfaceClass(type):\n\n    def __instancecheck__(self, instance):\n        def check():\n            _checkClass(instance"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface.interfaces import Iface\n\n\treturn (Iface.providedBy(object),)\n\ndef fullyQualifiedName(classObject):\n\t\"\"\"\n\tGet the fully qualified name of a class object\n\t\n\tThe value returned is a `str`. If the name contains a qualified\n\tname (e.g. in `name = Foo.Bar`), it is the full name.\n\n\tThis method should be overridden to specify better semantics\n\tof fully qualified names. For example, if the class's ``__module__``\n\tattribute contains the string \""}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object._Declaration__directlyProvidedBy or object.__providedBy__()\n\n\nclass Proxy(object):\n\t\"\"\"Proxy objects have accessors for the same object through\n\tthe object's attributes.\"\"\"\n\n\t__id__ = object.__id__\n\t_zope_config_set = {}\n\n\tdef __getattribute__(instance, name, _raise_AttributeError=False):\n\t\t\"\"\"Get an attribute\"\"\"\n\t\tid = instance.__id__\n\t\ttry:\n\t\t\treturn object._zope_config_set[id][name]\n\t\texcept"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t# pylint:disable=redefined-builtin,unused-argument\n\tfrom zope.interface.interfaces import IInterfaceDeclaration\n\treturn IInterfaceDeclaration\n\ndef directlyProvides(object, name, package=None): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn objects directly provided by the given object\n\n\tThe value return is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t#pylint:disable=redefined-builtin\n\treturn _declarations.interface_direct"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\ttry:\n\t\tzope.interface.directlyProvidedBy(object)\n\texcept AttributeError:\n\t\treturn IDeclaration(id=_('Object has no directlyProvasedBy interface'))\n\telse:\n\t\treturn zope.interface.IDeclaration(_('Object directlyProvided by'))\n\t\n\t\n@implementer(zope.interface.interfaces.IDeclaration)\nclass Invokes(object):\n\t\"\"\"\n\t\tReturn an object that invokes this declaration in some way.\n\t\t\n\t\tIt is used by objects with"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tdepends = [object]\n\n\tdef provides(object):\n\t\treturn [object, interfaces.IFactory(object)]\n\n\ttry:\n\t\treturn provider.provideImplementation(\n\t\t\tobject, interfaces.ifProvides, provides, dep1, dep2)\n\texcept provider.InterfaceDiscoveryError as err:\n\t\tfrom zope.interface.exceptions import Invalid\n\t\traise Invalid('Invalid interface spec') from err\n\n\nclass InterfaceLookupError(object):\n\t\"\"\" An error while parsing an ``iface``. \"\"\"\n\tdef __init__(self, msg):"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif zope.interface.adapter(object):\n\t\treturn object.providedBy\n\telse:\n\t\treturn ZopeTypePredicate(type(object), 'providedBy', object)\n\ndef directlyProvidedByName(name):\n\tif zope.interface.adapter(name):\n\t\treturn name.providedBy\n\telse:\n\t\treturn ZopeTypePredicate(name, 'providedByName', name)\n\ndef directlyProvides(object, nameOnly=False):\n\t\"\"\"\n\t\tReturn interfaces directly available from the provided object's\n\t\tinterfaces.\n\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif object is not None:\n\t\ti = object # pylint:disable=undefined-variable\n\telse:\n\t\ti = IDeclarations()\n\treturn i\n\n\ndef directlyProvidedBy(id):\n\tif id is not None:\n\t\ti1, i2, i3 = id # pylint:disable=undefined-variable\n\telse:\n\t\ti1 = IDeclarations()\n\t\ti2 = IDeclarations()\n\t\ti3 = IDeclarations()\n\treturn (i1, i2, i3)\n\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn getDirectlyProvidedBy(object)\n\nclass IDeclaration(Interface):\n\t\"\"\"A base-class for interfaces.\n\t\"\"\"\n\nclass IDeferredAdditions(Interface):\n\t\"\"\"A base-class for interfaces.\n\t\n\tThese interfaces are added to a newly created base class as they are\n\tadded to a class and instances of a base class.\"\"\"\n\t\n\nclass _IDempotencyMixin:\n\t\"\"\"Interface used when checking for state without having to\n\tperform an identity check by the programmer.\n\t\"\"\"\n\t\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif __debug__: zope.config._enable_dbg() # needed in case of import failure\n\tfrom..interfaces import IInterfaceDeclaration\n\tif ObjectProxy(object).is_implements(IInterfaceDeclaration):\n\t\timpl = ObjectProxy(object)\n\t\treturn impl.getDeclaration()\n\telse:\n\t\traise TypeError('unsupported object')\n\n\ndef explicitlyProvidedBy(object): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn explicitly provided interfaces\n\t\n\tThe value returned is an `~zope"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tclasses = classes[:]\n\tminimumBaseClasses = [classes[0]] + [classes[i] for i, c in enumerate(classes[:-1]) if not c.isAbstract()]\n\t\n\tbases = list(range(len(classes)))\n\t\n\tclass_list = classes[:classes.index(minimumBaseClasses[0])] + classes[classes.index(minimumBaseClasses[-1]) + 1:]\n\tclass_list.remove(minimumBaseClasses[-1])\n\t\n\tnew_class_list = []\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tbase_classes = frozenset(classes)\n\tif len(base_classes) > len(classes) + 1:\n\t\traise ValueError(\n\t\t\t\"Exclude base class %r more than %r, as it appears \"\n\t\t\t\"before all the others\" % (base_classes, len(classes))\n\t\t)\n\tfor (start, end) in zip(classes, base_classes):\n\t\tif start < end:\n\t\t\tbreak\n\tbases = base_classes\n\tdel classes, base_classes\n\n\t# Sort bases by addend\n\tbases."}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tnewClass = None\n\tminimumClasses = []\n\n\tfor base in classes:\n\t\tminimumClasses.insert(0,base)\n\tminimumClasses = tuple(minimumClasses)\n\t# TODO: what if len(minimumClasses) ==0?\n\t#\n\t#\tno need to convert, there don't have to be anything\n\t#\n\n\treturn minimumClasses\n\nclass SimpleTest:\n\tdef __init__(self,number,name):\n\t\tself.name = name\n\t\tself.number = number\n\n\t@staticmethod\n\tdef"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif len(classes) == 1:\n\t\treturn classes[0]\n\n\t# Find all minimal bases by looking at their ordinal numbers\n\tclasses.sort()\n\tminimalClasses = [classes[0]+1]\n\tbases = classes[1:]\n\n\t# Now go through the remaining classes as needed\n\tfor i in range(0, len(classes)):\n\n\t\tif classes[i] < 1:\n\t\t\t# Add all that class to the new list of minimal base classes\n\t\t\tminimalClasses.extend(bases[:i]+classes[i])"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif len(classes) == 1: return classes[0]\n\tbases = list(set(classes))\n\tbases.sort(key=lambda c: len(c.split(\".\")))\n\treturn bases[-2]\n\ndef baseTypes(classes):\n\treturn list(set(classes))    \n\ndef get(classes, name):\n\tif name == \"base\": return baseTypes(classes)\n\tif name == \"class\": return classes[classes.index(classes.index(name) + 1)]\n\tif name == \"domain\": return classes["}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\tclassicalRanks = []\n\tfor i in classes:\n\t\tclassicalRanks.append(i)\n\tclassicalRanks = sorted(classicalRanks)\n\t\n\tclassicalRanks = sorted(classes, key=lambda k: k.leq('base'))\n\t\n\treturn classes, classicalRanks\n\n\ndef findClassesByBase(bases, baseSet):\n\t\"\"\"\n\tFind all of the classes that are contained in one of\n\tthe base classes\n\tFor example, given a list of classes a1, b1"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif len(classes) <= 1:\n\t\treturn classes\n\t\n\tfor i in range(1, len(classes)):\t\n\t\tidx = classes[0]\n\t\twhile True:\n\t\t\tidx += 1\n\t\t\tfor _class in classes[idx:]:\n\t\t\t\tsub =  _class.split(' ', 2)[1]\n\t\t\t\tif int(sub)!= 0 and sub!= '':\n\t\t\t\t\tbreak\n\t\t\t# else:\n\t\t\t#  break\n\n\t\tclasses[idx] = [classes[idx]]\n\n\t# return(classes)\n\treturn classes"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tcounts = {}\n\tfor c in classes:\n\t\tif c in counts:\n\t\t\tcounts[c] += 1\n\t\telse:\n\t\t\tcounts[c] = 1\n\tfor k in counts.keys():\n\t\tmincount = counts[k]\n\t\tbases = classes[:]\n\t\twhile mincount > 0:\n\t\t\tbases.remove(k)\n\t\t\tmincount -= 1\n\n\treturn tuple(bases)\n\n#**********************************************************************************\nimport string\nstring.punctuation\nstring.punctuation.upper()\nstring.punctuation"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tclassesInOrder = sorted(classes, key=lambda c: c.__name__)\n\treturn [c for c in classesInOrder\n\t\t\tif c.__name__ not in [\"X2\", \"X3\", \"X4\", \"X5\" or \"X\" + str(int(c.id)), \"_empty\", \"_\" or c.id == 0]]\n\ndef isClassInSqli(clazz):\n\treturn hasattr(clazz, 'x_sql_like')\n\ndef isAlphanumeric(string):\n\t\"\"\"Check if string is al"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treturn sorted(classes, key=classes.index)\n \ndef remove_nested_classes(classes):\n\t\"\"\"\n\t\tRemove nested classes\n\t\"\"\"\n\treturn map(lambda c: tuple(c for c in minimalBases(classes) if not isinstance(c, int)), classes)\n \n# Build a mapping from nested classes to the maximal size\ndef build_class_heights():\n\t\"\"\"\n\t\tBuild a mapping from nested classes to the maximal height\n\t\t\"\"\"\n\theights = []\n\tfor class_ in classes:\n\t\theights.append("}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\traise NotImplementedError('Subclass must implement this method.')\n\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif not all:\n\t\tself.assertTrue(self.interface.get_valid_attribute_names())\n\t\tdef namesAndDescriptions(self): # pylint:disable=redefined-builtin\n\t\t\t#self.assertTrue(list(interface.get_valid_attribute_names())) # Check that it's in the list\n\t\t\ttry:\n\t\t\t\tres = self.interface.get_attribute_names()\n\t\t\texcept api.InterfaceError as exc:\n\t\t\t\traise exc # pylint:disable=raising-bad-type\n\t\t\tfor v in res:\n\t\t\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tprint('Accessing namesAndDescriptions(all=%s)' % str(all).lower())\n\treturn self._namesAndDescriptions[0]\n\n\t# add method __setState if __initial is set to TRUE\n\n\n# TODO: put this into a separate module\nclass NodeType(object):\n\tNodeTypeMap= {}\n\t\n\t@staticmethod\n\tdef nodeMap():\n\t\t\"\"\"Map type name into index.\"\"\"\n\t\tNodeType.NodeTypeMap['type'] = 0 # 0: 'null node'\n\t\tNodeType.NodeTypeMap['"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn self.root.listAllAttributeNames()\n\n\n\nclass NumericValue(object):\n\t\"\"\"\n\tDefinition of a numeric value.\n\n\t.. py:attribute:: integer\n\n\t.. py:attribute:: floating\n\n\t\"\"\"\n\tdef __init__(self, integer, floating):\n\t\t\"\"\"\n\t\t:param integer: The integer value of the numeric data.\n\t\t:param floating: The floating point value of the numeric data.\n\t\t\"\"\"\n\t\tself.integer = integer\n\t\tself.floating = floating\n\t\t\n\t\t\n\t@staticmethod\n\tdef validate(value"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t# self.set_name() # set interface names in xml file\n\n\t# add interface names in xml file\n\n\tself.interface = Interface(description=\"Interface\")\n\tself.interface.set_name(\"I\")\n\n\tif self.get_id() == 0:\n\t\t# default interface in the xml file\n\t\tself.interface.set_description(\"Default interface\")\n\t\tself.interface.add_node(self.xmlDocument, name=\"Default\")\n\t\tself.get_class_methods() # returns a list of instance methods\n\t\tself.interface.add_node("}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif isinstance(all, bool): # pragma: no cover - not a str tuple\n\t\treturn Interface.getInterfaceNames()\n\treturn Interface.getInterfaceNames(all)\n\n# end of interface namesAndDescriptions\n\n\ndef createDescriptor(interfaceName):\n\t'''\n\t\tCreate interface descriptor instance; returns None if an interface is not\n\t\tregistered.\n\t'''\n\tif interfaceName in NamesAndDescriptions():\n\t\treturn Interface.getInstanceFromDesc(interfaceName)\n\treturn _None(Interface, interfaceName)\n\n\ndef findInterface"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\timport uuid # pylint: disable=import-error\n\timport webbrowser # pylint: disable=import-error\n\n\tclass _AttributeDictEntry:\n\t\t\"\"\"Interface class for attribute values.\"\"\"\n\t\tdef __init__(self, obj, value):\n\t\t\tself.obj = obj\n\t\t\tself.value = str(value)\n\t\tdef __unicode__(self):\n\t\t\t\"\"\"Convert attribute entry to string.\n\t\t\t\"\"\"\n\t\t\treturn \"%s '%s' %r\" % (self.obj, self.value, self.value,)\n\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\treturn self._namesAndDescriptions()\n\telse:\n\t\ttry:\n\t\t\treturn self.__dict__['_namesAndDescriptions']\n\t\texcept Exception:\n\t\t\treturn None\n\n\nclass IBeamView(InterfaceBeam):\n\t\"\"\"\n\tClass implementing an interface to a beam.\n\t\"\"\"\n\tdef __init__(self): # pylint: disable=W0244\n\t\t\"\"\"\n\t\tClass constructor.\n\t\t\"\"\"\n\t\tsuper(IBeamView, self).__init__(name='IBeamView')"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn self.interface.namesAndDescriptions(all)\n \n# TODO: Implement this!\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t# pylint: disable=too-many-lines\n\tnamesAndDescriptions_list = [] # list of tuples\n\n\tif all:\n\t\tfrom.AttributeInfoList import AttributeInfoList # pylint:disable=import-outside-toplevel\n\t\timport collections # pylint:disable=import-outside-toplevel\n\t\tattributesInfo = AttributeInfoList()\n\t\tif \"attributes\" in attributesInfo.__dict__: # attributes (pycharm)\n\t\t\tattributesInfo = attributesInfo.attributes\n\t\t# Iterate over all"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\traise NotImplementedError\n\n\tdef attrs(self, all=False): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn all the attributes defined by the given interface.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef delattr(self, name): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tDelete the attribute named 'name'.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef attr(self, name): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn [\n\t\tattr for attr in dir(self.__class__)\n\t\tif self.has_attr_name(attr)\n\t]\n\ndef attr_name(attr):\n\t\"\"\"Returns the name of a stored attribute\n\t:param attr: the attribute to get its name\n\t\"\"\"\n\tif hasattr(attr, \"__name__\"):\n\t\treturn attr.__name__\n\treturn attr.__name\n\nif __name__ == \"__main__\":\n\tname = Class(\"Foo\")\n\tprint(name.name"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn list(self._interface.attrib)\n\n\t\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.get_attributes(all=all)\n\t\n\t\nclass XMLObjectInfo(DefaultObjectInfo):\n\t\"\"\"Create a base info structure allowing to access XML data.\"\"\"\n\t\n\t# Name of tag where the interface info structure resides is defined:\n\t\n\t_tags = (\n\t\t\"AttributeInfo\",\n\t\t\"ObjectInfo\",\n\t\t\"EnumInfo\",\n\t)\n\t\n\t#\n\t# Properties\n\t#\n\t\n\t@property\n\tdef tag(self):\n\t\t\"\"\"Name of tag where the interface info structure resides is defined.\""}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn set([\n\t\t'__hash__',\n\t\t'__getitem__',\n\t\t'__setitem__',\n\t\t'__iter__',\n\t\t'aiter',\n\t\t'__reversed__',\n\t\t'__missing__',\n\t\t'__contains__',\n\t\t'__add__',\n\t\t'__mul__',\n\t\t'reverter__',\n\t\t'__le__',\n\t\t'__lt__',\n\t\t'__ge__',\n\t\t'__gt__',\n\t\t'__del__',\n\t\t'__delattr__',"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tresult = []\n\tfor member in dir(Interface):\n\t\tif not member.startswith('_'):\n\t\t\tif not inspect.ismethod(getattr(Interface, member)):\n\t\t\t\tcontinue\n\t\t\t# If member is 'private', skip private\n\t\t\tif member[0] == '_' and member[-1]!= '_':\n\t\t\t\tcontinue\n\t\t\tmember_name = member[4:-2].replace('_','')\n\t\t\tif not member_name.endswith('method'):\n\t\t\t\tmember_name = 'getter'\n\t\t\tresult"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tpass\n\nclass __attributes__(Attribute):\n\t\"\"\"\n\t`__getattr__` and `__setattr__` will be overwritten\n\tin order to have a `name` attribute.\n\t\"\"\"\n\tdef __getattr__(self, name): # pylint:disable=no-self-use\n\t\t\"\"\"\n\t\tUse the `__getAttribute__` to access the attribute\n\t\tof the `__class__` instance.\n\t\t\"\"\"\n\t\tif name not in self.__class__.__dict__:\n\t\t\traise AttributeError(\"No attribute"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.attrs.keys()\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t__docformat__ = \"raw_contents\"\n\tif self.names is None:\n\t\tsuper(pyglfw_window, self).name_types()\n\t\tif hasattr(self, 'name'):\n\t\t\tself.names = list(self.attr.keys()) + [attr[1:] for attr in self.attr.values() if attr[1:]]\n\t\telse:\n\t\t\tself.names = tuple(self.attr.keys()) + [attr[1:] for attr in self.attr.values() if not attr[1:]]"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tself.interface.execute(sql.SELECT_KEYS)\n\treturn [keys[1] for keys in self.interface.get_first_row()]\n"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not output:\n\t\toutput = getattr(sequence, 'data') \n\t\n\tif output.is_interface:\n\t\toutput = output.expand_arguments_interfaces(sequence.items())\n\t\n\tif sequence.is_tuples():\n\t\tfor c in sequence.clauses:\n\t\t\tif c[0] == '*':\n\t\t\t\tif output:\n\t\t\t\t\tif c[1] not in output:\n\t\t\t\t\t\toutput = output.get_declaration(c[1])\t\n\t\t\t\tprint('if', c, len(sequence.clauses"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tsequence[:] = []\n\tif output:\n\t\tsequence.append(output)\n\tfor spec in sequence:\n\t\tif type(spec) == SequenceType:\n\t\t\tfor d in spec:\n\t\t\t\t_normalizeargs(_normalizeargs(d))\n\t\telif type(spec) == tuple:\n\t\t\t\tfor d in spec:\n\t\t\t\t\t_normalizeargs(_normalizeargs(d), output)\n\t\telif type(spec) == Interface:\n\t\t\t\tfor d in spec:\n\t\t\t\t\tif id(d) not in _decls:\n\t\t\t\t\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, tuple(list)) or isinstance(\n\t\tsequence, tuple(list(set(list(sequence))) + [interface for interface in sequence[1:]])):\n\t\tsequence = list(sequence)\n\t# Note the assumption that interfaces is sorted by 'name' not 'interface'\n\t_, argtypes, _, return_type, arg_decl = make_decls(sequence)\n\tif interfaces:\n\t\targtypes = sort_tuple(argtypes)  # interface is always ordered first\n\t# First pass: check to see if interfaces require"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output:\n\t\tif not isinstance(output, AbstractSequence):\n\t\t\tif hasattr(output, 'declarions'):\n\t\t\t\toutput = output.declarions\n\t\t\telif hasattr(output, 'defines'):\n\t\t\t\toutput = output.defines\n\t\t\telif hasattr(output, 'interfaces'):\n\t\t\t\toutput = output.interfaces\n\t\t\telse:\n\t\t\t\traise Exception('_normalizeargs: unknown output spec')\n\t\tsequence = [r for r in sequence if not isinstance(r, Declarion)]\n\n\tnormalize ="}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutput = []\n\tif isinstance(sequence, tuple):\n\t\tfor a in sequence:\n\t\t\t_normalizeargs(a, output)\n\telif isinstance(sequence, list):\n\t\tfor i in range(len(sequence)):\n\t\t\t_normalizeargs(sequence[i], output)\n\t\toutput += sequence\n\telif isinstance(sequence, Interface):\n\t\t# normalize an interface definition, if we pass it as args, ignore its\n\t\t# inputs.\n\t\tif arguments and sequence.get_inputs() or"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnormalized = self.NormalizeFunc((sequence, output))\n\tif normalizeargs(normalized):\n\t\tnormalized.set('name', name)\n\t\tnormalized.set('interface', interface)\n\t\treturn normalized\n\telse:\n\t\treturn self.normalize(sequence)\n\t\ndef normalizeargs(sequence):\n\t\"\"\"\n\tNormalize sequence arguments, by removing empty or otherwise in-line \n\tdeclarations and single interfaces.\n\t\t\n\t:param: sequence -> sequence of interfaces and declarations\n\t:return:\n\t\"\"\"\n\tdef"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t# Normalization of elements\n\tif isinstance(sequences, tuple) or isinstance(sequences, list):\n\t\treturn sequence\n\telif isinstance(sequences, type) and issubclass(sequences,\n\t\t\tinterfaces.IValuationTuple):\n\t\ttry:\n\t\t\telements = dict(map(lambda i: (i.name, i), sequences))\n\t\texcept AttributeError:\n\t\t\telements = dict(map(lambda i: (i, None), sequences))\n\t\tnormalizedelements = collections.OrderedDict()\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tnormalized = []\n\tif isinstance(sequence, (tuple, list, set)):\n\t\tif (\n\t\t\thasattr(sequence, \"bind\")\n\t\t\tand sequence.bind in type_binder.values()\n\t\t\tand sequence.bind not in ('unbound', 'dual',\n\t\t\t'none', 'interfaces', 'implements')\n\t\t):\n\t\t\tif output is None or output.bind in type_binder.values():\n\t\t\t\tnormalized.append('typespec:' + output.decl_type)\n\t\t\telse:\n\t\t\t\tnormalized."}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tif output:\n\t\treturn output\n\t\n\tif type(sequence) is tuple:\n\t\treturn sequence[0]\n\n\tif type(sequence) is list:\n\t\treturn sequence\n\n\tassert(not isinstance(sequence, tuple))\n\n\tassert(type(sequence) in [Def, Interface, InterfaceOrInterfaceTuple,\n\t\tInterfaceOrTuple, tuple, Union, InterfaceTuple])\n\t_debug(sequence)\n\n\tnormalize_arg = _normalizeargs_helper\n\tif outputs:\n\t\t_debug(\"normalizeargs: normalizing"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnormalized_items = {}\n\tspec = ()\n\tfor i, (spec_type, declaration) in enumerate(sequence):\n\t\tif declaration.args:\n\t\t\tnormalized_items[i] = declaration\n\t\t\t# A single spec\n\t\t\telif not isinstance(declaration, tuple):\n\t\t\t\t_, spec = declaration\n\t\t\t\tspec = spec + spec_type, spec\n\t\t\t# A tuple\n\t\t\telse:\n\t\t\t\tspec = declaration\n\t\t\t\tspec = spec + spec_type\n\t\t\t\tfor j, d in enumerate(declaration):\n\t\t\t\t\tspec[j] = d"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\timport shutil\n\tif not shutil.which('C'):\n\t\traise ImportError(\"The python install on your system does not have C\")\n\timport _c\n\ttry:\n\t\t_c.VERTRAC\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn True\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\treturn _C.__C__\n\nif not _C.__C__:\n\traise ImportError(\"Pyrml: import is required to use C optimization\")\n\n__all__ = _C.__all__\n\n_PY_TZ = _C.py_tz\n_PY_UTC = _C.py_UTC\n_PY_LOCAL = _C.py_local\n\n\ndef _pytz():\n\t# Note: this could be a safe return...\n\t# Note: I'm not sure where a better\n\t# way to do this"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\tif _optimize == True:\n\t\t\t# C optimizations available\n\t\t\t# 1. If this routine is present in the main namespace\n\t\t\t#    then it will be used or not. The usage of main\n\t\t\t#    is deprecated, use the C API instead.\n\t\t\tprint \"C\", \"version of c-API-module:\", __version__\n\t\t\treturn True\n\t\telse:\n\t\t\t# C extensions available except for PyCObject\n\t\t\t# 2. If Py_DEBUG is available, then it is enabled\n\t\t\t# 3. If PYPY_THREADING is"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\t# if the optimizations are required or not\n\t\tmod = __import__('optimize.coptimize')\n\t\tif sys.version_info >= (3, 0):\n\t\t\timport _pytest.config\n\t\t\treturn _pytest.config\n\t\treturn None\n\texcept ImportError:\n\t\treturn None\n\ndef _c_optimizations_available():\n\treturn _c_optimizations_available() is not None\n\ndef _ctype_from_size(size, byte_type):\n\t\"\"\"\n\tReturn (ctypes.size_t, c"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\tfrom zope.tal.optimize import _c_optimizations_available\n\t\treturn _c_optimizations_available()\n\texcept ImportError:\n\t\treturn False\n\ndef _callable_is_callable(callable):\n\"\"\"\"\"\"\n\t\"\"\"Return True if callable is a c function or method.\n\t\n\tC Functions defined as cdef methods and static methods (function definitions\n\tor static methods within a class) will be automatically detected as callable.\n\t\"\"\"\n\tif hasattr(callable, \"im_func\"):\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\timport sys\n\tfrom distutils.ccompiler import new_compiler\n\timport distutils.ccompiler as ccompiler\n\tOpt_ver = ccompiler.new_compiler()\n\tOpt_ver.set_executables(verbose='-v',\n\t\t\t\t\t\t\tcompiler_opt='-O4',\n\t\t\t\t\t\t\tcompiler_verbose='-v',\n\t\t\t\t\t\t\tcompiler_show_delegate_traceback='0',\n\t\t\t\t\t\t\tnoopt='-DNDEBUG',\n\t\t\t\t\t\t\t# this is actually a configuration option, but its\n\t\t\t\t\t\t\t# default should work\n\t\t\t\t\t\t\t"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\t# If Py_DEBUG is available then check it\n\t\t# is the correct value to use\n\t\tif \"__debug__\" in sys.modules:\n\t\t\tif hasattr(sys.modules[\"__debug__\"], \"_get_optimizations\"):\n\t\t\t\treturn sys.modules[\"__debug__\"]._get_optimizations()\n\t\t\telse:\n\t\t\t\t__tracebackhide__ = True\n\t\t\t\traise ImportError(\"Py_DEBUG is not available.\")\n\texcept (ImportError, AttributeError):\n\t\tpass\n\ttry:\n\t\t# If the optimizations are enabled then"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\t\t#c_bw_code = '\\r\\n'.join(s + '\\r\\n' for s in list_optimizations()).encode('ascii')\n\t\tc_bw_code = '\\r\\n'.join(s + '\\r\\n' for s in list_optimizations())\n\t\treturn bool(ctypes.windll.kernel32.IsStringCritical(0, c_bw_code, 'MyErrorString_B') == 0 )\n\texcept ImportError:\n\t\treturn"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\tif not importlib.util.find_spec('c'):\n\t\t\traise ImportError\n\t\telse:\n\t\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\ndef _get_optimizations():\n\t\"\"\"Extract the C optimization modules and store them in\n\ta dictionary.\n\t\"\"\"\n\toptimization_module_names = None\n\n\tfor dir_name in 'lib','share':\n\t\ttry:\n\t\t\tdir_name = os.path.abspath(dir_name)\n\t\texcept ValueError:\n\t\t\tcontinue\n\t\tfilename"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport scikit.stats.contingency_validation\n\t\timport scikit.stats.contingency_test\n\t\timport scikit.stats.tests\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\n\n\n#===============================================================================\n#\n#                              Support Classes\n#\n#===============================================================================\nclass _Dict(UserDict):\n\t\n\t#===========================================================================\n\t#  Class constructor\n\t#===========================================================================\t\n\t\n\tdef __init__(self, initargs = None):\n\t\t\"\"\"Constructor"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn (\n\t\t_distutils_lookup_environment() and\n\t\tos.environ.get('PYTHONCOMPILED', '0' in sys.version.lower())\n\t)\n\n_c_implementation_used = False\n\ndef _should_use_impl():\n\t\"\"\"\n\tReturn a true value if this is the `pyyaml <5.4>`_ backend.\n\t\"\"\"\n\tglobal _c_implementation_used\n\tif _should_attempt_c_optimizations():\n\t\treturn True\n\n\tif os."}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tenv_val = os.environ.get(\"PURE_PYTHON\")\n\treturn env_val == \"__pypy__\" or env_val == \"True\"\n\n\ndef get_imported_pypy_packages():\n\t\"\"\"\n\t\tReturn a list of all the PYPI packages for which we have PyPy installed.\n\t\"\"\"\n\timport pip\n\tpackages = set()\n\tfor distribution in pip.get_installed_distributions(True):\n\t\ttry:\n\t\t\tpypi_name = \"sphinx\" in str("}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\tif env(\"PURE_PYTHON\"):\n\t\t\n\t\treturn True\n\t\n\telse:\n\t\t\n\t\treturn False"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\treturn (\n\t\t# We are on PyPy where we use Py_SSIZE_T\n\t\tPy3 and\n\t\tos.environ.get(\"PYPY\") in (\"1\", \"0\")\n\t) and (\n\t\t# We are on PyPy where we use Cython in `PyPy_Optimize`.\n\t\tPy3 and\n\t\t# Use Cython when explicitly trying to optimize in `__doc__`\n\t\tnot Py3_Optimize or (Py3_Optimize and os.environ.get(\"PYPY\") in (\"1\", \"0\")"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn not (environment_altered or\n\t\t\t\t os.getenv(\"PURE_PYTHON\") and not sysconfig.get_config_var(\"Py_DEBUG\")\n\t\t\t\t )\n\n\ndef parse_flags(**kwargs):\n\t\"\"\"\n\tParse flags; return a dictionary of parsed parameters.\n\n\t:param kwargs: Passed down to :class:`run` if run takes *args.\n\t\"\"\"\n\timport sys\n\targs = kwargs.get('args', None)\n\tif args is None:\n\t\targs = sys.argv["}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\tif not hasattr(os, 'environ'):\n\t\treturn False\n\t\n\t# In order to be useful, we have to test whether '__pypy__'.\n\t# Python 3.3 (on PyPy 6.0+) only provides the environment as a\n\t# string rather than a dictionary:\n\t# https://github.com/python/cpython/blob/6a3835bf0d0b24387048e4d0a5c4b3d5f35c0f3/Lib/platform.c#L16"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tfrom warnings import warn\n\t# see c_optimizations() for details on the implementation\n\ttry:\n\t\t# https://stackoverflow.com/q/22508034/2864242\n\t\tbool_env_vars = {\n\t\t    'PYPY':  'CPYTHON_USE_C_FLAGS-PYPY',\n\t\t    'PYGMENTS': 'CPYTHON_USE_C_FLAGS-PYGMENTS',\n\t\t    'PYPY+PYGMENTS': 'CPYTHON_USE_C"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tglobal _use_c_impl\n\n\tpyversion = sys.version[0:3]\n\t\n\tif pyversion == '37':\n\t\t_use_c_impl = False\n\telse:\n\t\tif os.getenv('PURE_PYTHON', False):\n\t\t\t_use_c_impl = True\n\t\telif (\n\t\t    (os.environ.get('PURE_PYTHON') == '1') and\n\t\t\tpyversion not in (\n\t\t           '37',\n\t\t           '36',\n\t\t           '35')\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t# Is Python 2 and IronPython 3?\n\tis_pypy = 'PY_MAJOR_VERSION' in os.environ.get('PYTHON_VERSION_INFO')\n\tis_ipy37 = (sys.version_info[0] == 2 and is_pypy)\n\n\tif not is_pypy or is_ipy37:\n\t\treturn True\n\n\t# Is Python 3?\n\tis_python3 = sys.version_info[0] == 3\n\tis_python32 = not is_python3\n\t"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\tenv = \"PURE_PYTHON\" in os.environ\n\t\n\treturn env or _have_c_impl()\n\n\ndef _require_extension(library, extra_modules=None):\n\t\"\"\"\n\tRequire a given extension on the target platform.\n\t\"\"\"\n\t\n\t# Add the extension to the list of libraries we'll attempt to link to.\n\t\n\tlibrary.append(extra_modules or [])\n\t\n\t# Return early as we need to have the extension module.\n\t\n\treturn library\n\n\ndef _have"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\topt[\"_c_optimizations_required\"] = []\n\topt[\"_c_optimizations_optional\"] = []\n\n\ndef _c_optimizations_required_ignored():\n\topt[\"_c_optimizations_required\"].remove(\"libbinder.so\")\n\n\ndef _c_optimizations_optional_ignored():\n\topt[\"_c_optimizations_optional\"].remove(\"libbinder.so\")\n\n\ndef _c_optimizations_ignored():\n\topt[\"_c_optimizations\"].remove(\"libbinder.so\")\n\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\timport re\n\timport sys\n\tfrom ctypes import ArgumentError\n\tfrom comtypes.automation import AutoActivate\n\tfrom comtypes.automation import AddMethod\n\tif sys.maxsize > 2 ** 32:\n\t\tcx_Oracle_optimizations_ignored = 0\n\telse:\n\t\tcx_Oracle_optimizations_ignored = 1\n\treturn (0, cx_Oracle_optimizations_ignored)\n\noptimizations_required = c_int(\"0\")\n\nclass ORACLE_EXTENSIONS(Structure):\n\t"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tdef _optim(src):\n\t\t\"\"\"\n\t\tReturns a copy of `src`, but of the same types as the data type\n\n\t\tThe copy depends on whether it's an array, a string and a buffer\n\t\tobject.\n\t\t\"\"\"\n\t\tdata_type = src._type  # The datatype\n\t\tsrc = src.swapaxes(axis, -1).ravel()\n\t\tif isinstance(src, np.ndarray):\n\t\t\treturn array(src, dtype=data_type, copy=True)\n\t\tif isinstance(src, (str, bytes)"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tc_optimizations_required = {\n\t\t'pyexpat': ['xmlrpc.client.client.Client'],\n\t\t'typewrite': ['xmlrpc.client.ServerProxy',\n\t\t\t    'xmlrpc.client.MultiCallable'],\n\t}\n\n\tc_extensions_required = {\n\t\t'pyexpat': ['xml.etree.ElementTree',\n\t\t\t    'xml.etree.ElementTree._parse_xml', 'xml.etree.ElementTree.QName',\n\t\t\t    'xml.etree.ElementTree."}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn list()\n\n\n#------------------------------------------------------------------------------\n# _c_optimizations_enabled\n#------------------------------------------------------------------------------\n\n_c_optimizations_enabled = False\n\ndef enable_c_optimizations():\n\t\"\"\"\n\t**Syntax**\n\n\t\tfrom cffi import FFI\n\t\tffi = FFI()\n\t\tffi.cdef(\"\"\"\n\t\t\t#define _C_BACKEND(BACKEND) static struct cffi_backend.cffi_backend(&BACKEND);\n\t\t\t#define _C_BACKEND(BACKEND, OPTIONS) static void backend"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t_c_optimizations_required[0] = 1\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tfrom lib.autolib import _py.set_env_vars\n\t_py.set_env_vars(enabled=False)\n\n\ndef _c_optimizations_required(optimizer):\n\t\"\"\"\n\tDo an exhaustive search and get the most important setting.\n\n\t\toptimizer - the optimizer in use\n\n\tReturns a list of (optimizer, key) tuples giving all\n    optimizers that can be used.\n    \"\"\"\n\n#\t_c_optimizations_required.log.info(\"Using {} optimizer.\".format(optimizer))\n\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t# for x in _optimizations:\n\t#     for p in _optimizations[x]:\n\treturn list(_optimizations_required.keys())\n\"\"\"\n\n_optimizations_required=_optimizations # list with tuples of\n\n# optimization to ignore for file_in_projector\n_optimizations_required = []\n\"\"\"\nprint _optimizations_required\n\n\ndef _file_in_projector(path):\n\treturn os.path.isfile(path)\n\n# check that folder exists\nif not os.path.isdir(args.output_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\timport optparse\n\timport os\n\timport sys\n\timport time\n\timport ast\n\tfrom cStringIO import StringIO\n\tfrom collections import defaultdict\n\timport multiprocessing\n\timport functools\n\timport itertools\n\timport operator\n\timport functools as ft\n\timport traceback\n\timport copy\n\n\timport distutils.core\n\timport distutils.util\n\timport distutils.errors\n\timport distutils.sysconfig\n\timport distutils.debug\n\tfrom distutils.ccompiler import new_compiler\n\n\t#"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tsizes_to_do = (\n\t\t['fast_compile'],\n\t\t['link_with_objs'],\n\t\t[],\n\t\t['link_fast_ctypes'],\n\t)\n\n\tfor size in sizes_to_do:\n\t\trequired, size_set, size_compare, size_compat_from = _size_compat_flags(size, True, \"ignore\")\n\t\tcompatibility_check = required\n\t\tif not size_set:\n\t\t\t# we're trying to disable the compatibility flags for the size\n\t\t\tcompatibility_check"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tvar = _use_c_impl()\n\tif var is None:\n\t\traise RuntimeError( 'Unknown implementation in %s. This code was run under Python %s.' % \\\n\t\t                    (__file__, sys.version.split('\\n')[0]) )\n\tif has_pypy2:\n\t\t# PyPy2 cannot compile C extensions without a _cpython extension\n\t\treturn False\n\telif var!= 'c99':\n\t\treturn False\n\telse:\n\t\traise ValueError(\n\t\t\t\"C optimization is only available on the Python implementation of"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn (PURE_PYTHON and\n\t\t\t('Py3' in _m_locals),\n\t\t\t('PYBASH' in _m_locals and\n\t\t\t('PURE_PYTHON' in _m_locals))\n\t\t\t)\n\ndef _check_python_implementation():\n\t\"\"\"\n\tCheck that we want to use Python implementation\n\t(``-m``, ``--implementation``, ``---``, ``-p``)\n\tand not just ``-`` (``-) or ``p`` (``-p``).\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn get_option(PURE_PYTHON, \"optimize\", default=True)\n\ndef _use_c_impl():\n\t\"\"\"\n\tCheck if C should be used.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\ttry:\n\t\treturn get_option(PURE_PYTHON, \"threaded\", default=False)\n\texcept (AttributeError, KeyError):\n\t\tlogger.debug(bold(\"Error, C implementation could not be determined"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn PURE_PYTHON if os.environ.get('PURE_PYTHON') is not None else False\n\n\n@lru_cache()\ndef _use_pth(py3_pth=False):\n\t\"\"\"\n\tReturn a true value if the Pth headers, headers.x_pydev_v5_x/v5__pydev_v5__pth.py, are required.\n\t\"\"\"\n\treturn ('python_version' in os.environ or 'x_pydev_v5_x'"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn (PURE_PYTHON)\n\"\"\"\n\t\n\t_c_optimizations_required()\n\t\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn (True)\n\"\"\"\n\ndef _c_has_unix_gettext():\n\t\"\"\"\n\tReturn True if ``gettext`` is installed on the system.\n\t\n\tThis uses the ``PY_UNIX_GETTEXT_HAS_UNIX`` variable as documented\n\tin `_use_c_impl`.\n\t\"\"\"\n\treturn (PY"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn get_env_setting_value_or_fallback(\n\t\tconst.VIRTUALENV_PYTHON_OPTIMIZATIONS_REQUIRED,\n\t\tconst.PYTHONOPTIMIZE_ALL,\n\t) and not any(\n\t\tget_env_setting_value_or_fallback(\n\t\t\tconst.VIRTUALENV_PYTHON_OPTIMIZATIONS_TRAILS,\n\t\t\tconst.PYTHONOPTIMIZE_ALL_TRAILS,\n\t\t)\n\t)\n\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn os.environ.get('PURE_PYTHON', '0')!= '0'\n\n_c_optimizations_required = _c_optimizations_required()\n\ndef _c_flags_required():\n\treturn _c_get_flags()!= 0\n\ndef _c_flags_required(option='_c_flags'):\n\t# If you update this code, please don;t rename it.\n\treturn _use_c_impl(option)\n\ndef _c_impl(option='_c_impl'):\n\treturn"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tvarname_dict = {'PURE_PYTHON': False}\n\treturn varname_dict.get(os.environ.get(\"PYTHON_USER_CONSTANTS\", \"NO\"), False)\n\n\nclass CFunctionInfo(str):\n\t'''\n\t\tThis information class is used to define the structure to be returned by `CFunction` to be compiled with `cxx`, `c\", etc.\n\t'''\n\tdef __repr__(self):\n\t\treturn \"%r\" % self\n\t\n\tdef __str__(self"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif \"PURE_PYTHON\" in os.environ:\n\t\traise RuntimeError(\"PURE_PYTHON environment variable not allowed\")\n\n\t# This will be `True` if, for example, the C implementation returns a pointer.\n\t#\n\tif getattr(_compiler, \"have_cplus\", False):\n\t\t# This is what is used if the Python version has it. The\n\t\t# version before adding C++ optimizations.\n\t\t_use_c_impl = {1:False}\n\n\t\tif (_cplus_info, is_mems"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif PURE_PYTHON:\n\t\treturn True\n\n\tfrom. import _popen_impl_windows as PopenImplWindows\n\tif PopenImplWindows.IS_WIN32:\n\t\t# no need to bother unless you are on windows\n\t\t# this can cause `try: exit` on the windows installer\n\t\tfrom._py39compat import string_is_unicode\n\t\tfrom._py39compat import PY39_COMPAT_PYTHON3\n\n\t\tPY39_COMPAT = PY39_COMPAT_PYTH"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tpass\n\nbin_size = 1  # bin size (in bins)\nbin_range = 6 * [0]  # bin range (from -> to)\nnum_bins = 4\n\n## Functionality of histogram\n\nclass Histogram():\n\t\"\"\"\n\tThis is a simple histogram, one of which has an output value of *initial_value*\n\tand a range of points, as provided above, and a function of which can decide\n\tthe number of bins according to the given range.\n\tAt this moment, this histogram is not really a wrapper for the"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\tpass\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.bins = {}\n\tself.initial_value = self.initial_value or 0.0\n\tself._make_bins()\n##################################################################################################################################\n##################################################################################################################################\n\t\t\n\t\n\t\n\t"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself._hist = {}\n\tself._reset()\n\tself._last_bins = np.zeros(self.counts.shape, dtype=np.int64)\n\tself._maxcounts = None\n\tself._mincounts = None\n\tself._maxmin = None\n\tself._minmin = None\n\n\treturn self._reset_for_call()\n\n\t\n\t\ndef test():\n\t\"\"\"Simple histogram\n\t\n\t>>> hist = hl.histogram(\"x\", source_set=hl.balding_nichols()"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tdef reset_to_empty():\n\t\tself.hist[:] = [0] * (len(self.bins) * len(self.label))\n\t\tself.add_label()\n\n\tself.hist = []\n\tself.bins = []\n\tself.label = []\n\tself.make_bins()\n\treset_to_empty()\n\n\t_ = \"HistogramLabel\" not in self._labels\n\tself[\"HistogramLabel\"] = HistogramLabel(self)\n\n\tif _:\n\t\t# For debugging, print the histogram labels\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself._reset_params['histo'] = {}\n\treturn\n\n\treturn {}\n\n\nclass Histogram(GraphContainer):\n\t\"\"\"\n\tBase class for a histogram.\n\t\n\tThe Histogram object contains a list of *initial_values*,\n\twith the values assigned to all the bins specified by\n\t*bins*.  The final histogram is computed from these.\n\t\"\"\"\n\n\tdef __init__(self, hfile):\n\t\t\"\"\"\n\t\tCreate and initialize the Histogram object.\n\t\tThis is the parent class.\n\t\t\n\t\thfile: a"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\treturn {}    \n\ndef hist_plot(hist: dict, title: str = None):\n\t\"\"\"\n\tPlot an entire histogram.\n\t\n\tExample:\n\t\t>>> hist = {'a': [], 'b': [], 'c': [], 'd': []}\n\t\t>>> hist_plot(hist, title='Histogram Plot')\n\t\"\"\"\n\tplt.clf()\n\tplt.hist(hist.values(), range=hist.keys(), bins=10)\n\tplt.xlabel('value')\n\tplt.ylabel('"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.data = {}\n\tself.bins = [0.0,] * self.Npoints\n\tself.lastPoint = False\n\tself.lastX = 0.0\n\t#if self.make_bins:\n\t#\tself.make_bins()\n\n\treturn self.data\n\n\t\n\t\nclass Curve_Set:\n\t\"\"\"A set of points between two sets, with a curve style.\"\"\"\n\t\n\tdef __init__(self, p1, p2, style=\"set\", color=\"blue\", title=\"CurveSet\"):"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.resetN()\n\tself.resetB()\n\tself.resetC()\n\n\tfor bin in self.bins_:\n\t\tbin.reset()\n\t\tbin.reset_bins()\n\t# Make sure that the *make_bins_* methods are called only once\n\t# (so that they are independent of current initialization, including\n\t# the *histograms* and *bin_limits* dicts).\n\t#print(len(self.bins_))\n\t#print(len(self.histograms_))\n\t#print(len("}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.hist.reset()\n\tself.hist_xdata = []\n\tself.hist_ydata = []\n\tself.hist_edges = []\n\tself.hist_bins = []\n\n\tself.make_bins()\n\t\n\t# Check to make sure all settings are correct\n\tif not isinstance(self.hist_xdata, list) or not isinstance(self.hist_ydata, list):\n\t\tself.hist_edges.append(min(self.hist_xdata))\n\t\tself.hist_edges.append"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn c_string_array(*[self.to_csv(separator) for _ in range(self.count)])\n\ndef ascii_graph(edges=None, node_styles=None):\n\treturn Graph(edges=edges, node_styles=node_styles)\n\ndef lena(string=None, ascii=None, d=False, directed=True, **kwargs):\n\t\"\"\"\n\tCreate a graph from string (str) or from an already\n\tconstructed graph object.\n\t\n\t*string*, if not ``"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom. import graph, ToCSV, output\n\treturn ToCSV(graph, self, separator, header).write_to_file()\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\ttry:\n\t\timport csv\n\t\tif csv.writer(None)!= self._new_writer:\n\t\t\traise ValueError(\"ToCSV. from_csv() is not supported\")\n\texcept ImportError:\n\t\treturn self.to_csv(separator=separator, header=header)\n\telse:\n\t\treturn csv.writer(self._new_writer, delimiter=separator,\n                         quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n\n\nclass ToArray(Output):\n\t\"\"\"\n\t.. versionadded:: 0."}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn('To_csv is deprecated.', DeprecationWarning)\n\treturn self.to_table(separator, header)\n\n\t\nclass Diagram(Graph):\n\n\t\"\"\"\n\tA diagram is a graph which the nodes are labels and\n\twhich the edges are labels.\n\t\"\"\"\n\t\n\tdef __init__(self, nodes=None, edges=None, label=None, size=None, **kwds):\n\t\t\"\"\"Initialize a diagram with input nodes and optional input\n\t\tedges. For an optional `labels` argument"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tdef convert_graph_to_csv(a_graph):\n\t\tgraph_list = []\n\t\tfor x, y in a_graph:\n\t\t\tp_a = [x, y]\n\t\t\tfor i in range(0, len(x)):\n\t\t\t\tp_a.append(x[i])\n\t\t\t\tp_a.append(y[i])\n\t\t\tgraph_list.append(p_a)  # (x, y)\n\t\treturn pandas.DataFrame(graph_list,\n \t                    columns=[\"Coordinate\", \"Value\"],\n "}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twith lena.output.ToCSV(separator=separator, header=header) as file:\n\t\tfor idx, point in enumerate(self.points):\n\t\t\t# point[0] - the coordinate in the first column of the csv\n\n\t\t\tif isinstance(point, collections.Iterable):\n\t\t\t\tpoint = point[0]\n\t\t\t#point = np.asarray(point)\n\n\t\t\tfile.write(\n\t\t\t\t(\",\".join(f\"{point[i]:s}\" for i in range(point.size))).rstrip().replace(\"\\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\td = {};\n\tif self.ndim == 1:\n\t\tprint(\"Warning: ToCSV():\", \" graph has only one dimension.\")\n\t\tgraph = self.G.to_undirected()\n\t\tprint(graph)\n\t\tprint()\n\telif self.ndim == 2:\n\t\tgraph = self.G.tocsr()\n\t\tif graph.indices is not None and graph.indptr is not None:\n\t\t\tindices_str = graph.indices\n\t\t\tdata_str = graph.data.todia()\n\t\t\tprint(\"Warning:"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarn('The `to_csv` method is deprecated since 0.0.5', DeprecationWarning)\n\timport lena.output.ToCSV as csv\n\tassert isinstance(self, Graph)\n\timport numpy as np\n\t# first convert points to numpy arrays, and then convert to csv:\n\tpoints = [p.to_numpy() for p in self.points]\n\tpoints = np.array(points, dtype=[(p[0], p[1]) for p in points])\n\t\n\treturn csv.to_"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom.lena_output import ToCSV\n\tToCSV().to_csv(None, separator, header)\n\t\nclass ToCSV(Graph):\n\t\"\"\"\n\t:class:`ToCSV` supports :meth:`Graph`'s use.\n\t\n\tUse of :class:`ToCSV` works in two ways:\n\t        \n\t1. Use it with file-like object::\n\t        \n\t         >>> file_like = BytesIO(b'...', 'ascii')\n\t         >>> f = ToCS"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\toutput = StringIO()\n\tfor graph in self.graph.values():\n\t\tgraph.tocsv(output, separator)\n\treturn output.getvalue()\n\n\nclass TaggedGraph(Graph):\n\t\"\"\"A graph with a node label.\n\n\tA graph with a node label can be converted to.to_csv().\n\n\t\"\"\"\n\tdef __init__(self, nodes=(), edges=()):\n\t\tGraph.__init__(self, nodes, edges)\n\n\tdef to_csv(self, separator=\",\", header=None):\n\t\treturn NodeGraph"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terr_indices = np.load('/projects/projectdirs/dk/users/bertrand/csc_grrr/tests/test_files/tests_out/pars_test_'+coord_name+'.npz')\n\terr_indices = torch.from_numpy(err_indices['err_indices']).to(self.device) # numpy\n\treturn err_indices\n\ndef _load_pars(coord_name):\n\t\"\"\"Load pars for a given coordinate.\n\t\"\"\"\n\tpars = {}"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif hasattr(coord_name, 'rmsd') and coord_name.rmsd:\n\t\trmsd_array = coord_name.rmsd.tolist()\n\telif coord_name is not None: # and is not None:\n\t\trmsd_array = np.ones(coord_name.n_points)\n\telse:\n\t\trmsd_array = None\n\n\tif rmsd_array is not None:\n\t\trmsd_array = rmsd_array[:]\n\terr_ind = [i for"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tvar_name = r'(?:(?:\\w)*\\w+(?:_?\\w)*)'\n\tvar_name = re.translate(var_name, \"\", \" *\")\n\tassert var_name.count('_') == 1\n\tc_name = '$' + var_name.split('_')[1]\n\tr_name = var_name.split('_')[0]\n\treturn r_name.index(c_name)\n\nclass TMP(object):\n\t\"\"\"\n\tDummy class to keep track of the"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terror_indices = np.where(np.logical_and(self.lbin[:].values!=np.nan, self.tbin[:].values!=np.nan))[0]\n\terror_indices = list(error_indices) # Remove nan indices\n\treturn error_indices\n\ndef _get_bin_edges(x):\n\t\"\"\"\n\tGet the bins for a given array.\n\t\"\"\"\n\tx = np.unique(x)\n\t# x_b = np.append(x[0], x["}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\ttry:\n\t\tself._err_file.seek(0)\n\texcept AttributeError:\n\t\traise IOError(f\"'{self._name}' is not closed\")\n\ttry:\n\t\tfor j in range(0, self._num_fib_ind):\n\t\t\tself._inds[coord_name][j]\n\texcept IndexError:\n\t\traise IOError(f'Error calculating error indices - coordinate \"{coord_name}\" does not exist')\n\tself._err_indptr = []\n\tfor i in range(0, self._num_fib"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terr_index = np.array([])\n\n\tif coord_name in self._error_values.columns:\n\t\t_err_index = self._error_values.loc[coord_name].values\n\t\tif self.error_type == 'percentile' and \\\n\t\t\t\tlen(_err_index) == 1 and _err_index[0] > 0:\n\t\t\t_err_index = np.ones(len(coord_name), dtype=int)*_err_index[0]\n\t\t_err_index = np.hstack(tuple([_err_"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tindx = self.coord_names.index(coord_name)\n\treturn np.min(self.tbl[:,indx], axis=1)>0.0\n\n\nclass TBLModel(SX_model.SX_model):\n\t\"\"\"\n\tA TBL model for the Fractional-Model (FEM)\n\t\"\"\"\n\tname = \"tbl.fem.mod\"\n\n\t# Class variables\n\tmax_t_dim = 21\n\tflux_name = 'numinit'\n\n\tdef _initialize(self):\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif self.coord_names is None:\n\t\treturn np.arange(self.n_mesh_elements)\n\tif coord_name not in [\"p_p\", \"e_q\", \"e_z\"]:\n\t\traise ValueError(f'coord_name {coord_name} is not a valid coordinate')\n\n\treturn self.coord_names.index(coord_name)\n\n\t# return all elements satisfying the mask\n\t# mask = get_mask(element_mask)\n\t# return np.nonzero(np.sum(~mask, axis="}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\te_coord = (self.get_flux(coord_name) * np.log(eV2Ksun)).sum(axis=-1).filled(np.nan)\n\n\t# get the error of each coordinate.\n\t_coord_flux_error = self.get_flux(coord_name) * np.abs(\n\t\t-self.get_err(coord_name))\n\t# error is the cosine of error\n\t_coord_flux_err = _coord_flux_error + ((e_coord.astype('float64') <"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tindices = []\t\n\tfor ind in self.coords[coord_name]:\n\t\tif ind =='s':\n\t\t\tindices.append(2)\n\t\telse:\n\t\t\tindices.append(4) #s\n\treturn indices\n"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext.error = {}\n\tcontext.value = {}\n\tfor field_key, c_key in self._graph_params[\"compact\"][\"params\"].items():\n\t\t\t# update this with new value\n\t\t\t# (i.e. all value contained in the graph)\n\t\t\tvalue = self._graph_params[\"vars\"][c_key][0]\n\t\t\tcontext.value.update({field_key : value})\n\t\t\tif c_key in self._graph_params[\"compact\"][\"params_short\"][field_key]:\n\t\t\t\t\tcontext.error"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext = []\n\n\t# Add all fields with known structure (and not None values)\n\tfor e in GraphStructure.fields:\n\t\tif e.has_structure() and e.get_structure().is_non_empty():\n\t\t\tfor v in e.get_structure().fields():\n\t\t\t\tif not v.has_structure():\n\t\t\t\t\tcontext.append({\n\t\t\t\t\t\t\"type\": e.get_type(), (\"%s[%s]\" % (e.name, format_coord(v)))})\n\t\t\t\t\tcontext.extend(e.get_structure"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif \"error\" in context:\n\t\tcontext[\"value\"] = dict(\n\t\t\t\terror_y = array(context[\"error\"][\"y\"][1:]),\n\t\t\t\terror_x = array(context[\"error\"][\"x\"][1:])\n\t\t\t\t)\n\telse:\n\t\tcontext[\"value\"] = context[\"m_data\"].copy()\n\t# Add \"index\" to context to remove duplicate ids?\n\tcontext.error.update({\n\t\t\t\t\"E\": context[\"error\"],\n\t\t\t\t\"t\": context[\"t\"],\n\t\t\t\t})\n\treturn context\n\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tE = None\n\tif \"E\" in context.value:\n\t\tE = context.value[\"E\"]\t\t\n\tfor i, subcontext in enumerate(context.value[\"E\"]):\n\t\tif not isinstance(E, dict):\n\t\t\tsubcontext.pop(\"x_low\", None)\n\t\t\n\t\tsubcontext_type = type(subcontext)\n\t\t\n\t\tif len(context.value[\"value\"])!= 2:\n\t\t\tsubcontext = {i:\"{\"+i+\"}}\"+subcontext}\n\t\t\tE = context.value[\"value\"]["}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext.value.setdefault(\"error\", {}).setdefault(\"x\", []).extend(self._error_id(eid) \n\t\tfor eid in context.args[0].error if isinstance(context.args[0], \n\t\t\t_error_ids.Error) else ())\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tgraph = self._context\n\tgraph.update(context)\n\tif isinstance(context, dict):\n\t\tfor field, prop, value in context.iteritems():\n\t\t\tif isinstance(graph.context[field], list):\n\t\t\t\tfor c in graph.context[field]:\n\t\t\t\t\tgraph.value.append(c.value*self._props.scalar_props.get_unit(value), c)\n\t\t\telif isinstance(graph.context[field], float):\n\t\t\t\tgraph.value.extend(value, graph.context[field])\n\t\t\t"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self._is_to_console() or self._is_on_graphics():\n\t\tfor field in self._field_list:\n\t\t\tif not field.is_defined():\n\t\t\t\tcontinue\n\t\t\t\t# Note: this is a new subcontext\n\t\t\tfor key in context:\n\t\t\t\ttry:\n\t\t\t\t\t# if not field.is_readonly(key):\n\t\t\t\t\tvalue = context[key]\n\t\t\t\texcept (KeyError, AttributeError):\n\t\t\t\t\tcontinue\n\t\t\t\tif not value:\n\t\t\t\t\tcontinue\n\n\t\t\t\ttry:\n\t\t\t\t\tif key == \"E\":\n\t\t\t\t\t\t# Note: this"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext.error = context.value\n\tcontext.error.update(context.value)\n\t# Call update on the *graph* itself\n\tself.update_node(context)\n\tcontext.value = context.values.__setitem__(context.index, context.values.values[context.index])\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tfrom.struct import Struct\n\tfrom.util import index_of\n\tfrom.util import remove_subctx\n\tfrom.util import split_key\n\tfrom.util import sub_ctx\n\t\n\tctx_vals = \"E,t,error_E_low\"\n\tf, subctx = Struct(ctx_vals, {}).value\n\n\tind, ctx = index_of(context)\n\t\n\tfor v in f.val.values():\n\t\tval = v.value\n\t\tnvals = v.nvals.copy()\n\t\tfor k, val"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif context.debug:\n\t\ttrace('_update_context')\n\n\n\tif isinstance(context, tuple):\n\t\treturn\n\tif context._debug:\n\t\tif not context.is_alive and not context.is_in_history:\n\t\t\tlog.debug(\"This context is destroyed as it is in dead state..\")\n\t\t\treturn\t\n\n\tindices = context.indices\n\tif context.n_sources==1 and context.is_alive:\n\t\t# if this context is in live state we should be able to update context.value\n\t\t# to"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# 1. compute integral (scaling) among interval bins in range(0,bins+1)\n\tx = numpy.zeros_like(bins)\n\tn = len(bins);\n\tfor i in range(n):\n\t\tx[i] = numpy.sqrt(1.0/numpy.float32(bins[i]-edges[i]))\n\t\n\t# 2. sum of integrals over interval (0,bins+1)\n\tx_sum = numpy.zeros_like(x)\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tassert(len(bins) == len(edges)-1)\n\tdef _scale(x):\n\t\t# Compute the value of the integral using x=bins[i].\n\t\tif bins[i].isdigit(): # No need for a log scale.\n\t\t\treturn float(bins[i])\n\t\t# Otherwise, the integral is equal to a linear function.\n\t\treturn x*(1 + edges[i]) - edges[i]\n\n\treturn integrate(_scale, bins=bins, edges=edges)\n\t\t\n\n\n\n\n\n############################################################"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tif not isinstance(bins, (array_like, Sequence)):\n\t\traise TypeError(\"bins argument must be array_like\")\n\n\tedges = tuple(edges)\n\n\tif not isinstance(bins, (array_like, Sequence)):\n\t\ttry:\n\t\t\tedges = [edges]\n\t\texcept (ValueError, TypeError):\n\t\t\tpass # a single value, not an array_like or Sequence\n\n\tif len(edges)!= len(bins):\n\t\traise ValueError(\"number of edges arrays (%d) does not match number"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# Initialize histogram\n\thistogram = histogram1d(edges)\n\thistogram.bins = tuple(bins)\n\t\n\tx = np.linspace(histogram.xmin,histogram.xmax,histogram.Nx)\n\ty = np.cumsum(histogram.scale*histogram.integral(x).flatten())/(x.shape[0]-1)\n\t\n\timport matplotlib.pyhspace as py\n\thistogram.colorbar = py.colorbar_factory(histogram"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\traise NotImplementedError\n\t\n\treturn  np.array(((1,))*len(bins))/sum(np.diff(bins)[:-1])\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tif len(bins) > 1:\n\t\tbins = tuple(bins)\n\telif len(bins) == 1:\n\t\tbins = bins[0]\n\tx1, x2 = list(edges[:-1])[0], list(edges[1:])[0]\n\tedgestack = []\n\tedglo = []\n\tfor i in range(len(bins)-1):\n\t\tedglo.append(max(((x1+x2)**2)*(i/float(len(bins)-"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tif len(bins)!= len(edges):\n\t\traise RuntimeError (\"bins and edges have the wrong size\")\n\tif bins[-1] < edges[0]:\n\t\traise RuntimeError (\"bins need to be at least the edge of the integration.\")\n\t# We choose the points below the edges (with one bin)\n\t# For example: if we want to compute integral,\n\t# the edges are the values\n\t# [bins[0]-10.. bins[-1] + 10]\n\t# where a bin is about [start,...,"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\tassert isinstance(edges, numpy.ndarray)\n\t\n\tif edges.size <= 0:\n\t\traise ValueError(\"`edges` must be larger than 0.\")\n\t\n\tresult = numpy.zeros(edges.shape)\n\t\n\t# Sum all integrated bins for each value\n\tfor i in range(len(bins)):\n\t\tresult += (numpy.arange(bins[i], bins[i+1], 1)\n\t\t\t* (edges[i+1] - edges[i])).sum()\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\tsigmas = [1./gauss(v, 4, 2) for v in bin] - 1./gauss(bins.mean(),4,2)\n\t\n\tI = sigmas[edges]\n\tI[:] = 0.\n\tfor (x, s) in zip(bin, sigmas):\n\t\tx += 1./c_bar\n\t\tI += (0.5 * (1 - I) + 0.5 * I) * s\n\t\n\tI = I * np.exp(1./(4*I.sum()))"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn 0.0\n# END_FUTURE_MODULES\n\n\n# ======================================================\n# General Interface\n# ======================================================\n\ndef histogram(*args, **kwargs):\n    \"\"\"\n\tCompute histograms. Return a list of lists of\n\t*ndarray*, averaged over ``len(args)`` bins.\n\tAccepts an argument `**kwargs`, which should be\n\t:class:`.histogram` value.\n\t\n\tIn the general case, this function is the same as\n\t:func:`.integral`.\n\n\t.."}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tseq = seq.lstrip(b\"<L<\")\n\tseq = bytes([ord(x) for x in seq])\n\treturn (len(seq) in (3,6))\n\ndef is_fill_response_seq(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a FillResponseSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains exactly one such, and it is not a Source\n\tsequence.\n\t\"\"\"\n\tseq = seq.lstrip(b\"<L<\")\n\tseq ="}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif not isinstance(seq, ElementSeq):\n\t\treturn False\n\t\n\t# Make sure seq contains only Elements\n\tn_seq = len(seq)\n\tif n_seq > 3:\n\t\treturn False\n\t\n\tfor i in range(n_seq-2):\n\t\tif not isinstance(seq[i], Element):\n\t\t\treturn False\n\t\t\n\t\tif not isinstance(seq[i+1], AttrSeq):\n\t\t\treturn False\n\t\tif not isinstance(seq[i+2], AttrSeq):\n\t\t\treturn False\n\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif type(seq) is FillRequest and hasattr(seq,'seq'):\n\t\treqe = seq.seq\n\telif type(seq) is Source or type(seq) is FillRequest:\n\t\treqe = seq\n\telif type(seq) is str:\n\t\t# if seq is a constant, assume it's an element that has been\n\t\t# already converted to a string.  This is so it can be\n\t\t# compared to string values\n\t\treqe = {'seq':seq,'seq_type':'elem'}\n\telse: # assume"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn seq is not None and (seq.get_type() == FillRequest.ELEMENT\n\t\t\t\t\t\t\t   or seq.count(FillRequest.SEQ_EMPTY) == 1)\n\nclass FillRequest:\n    \"\"\" Class used to represent a fill request \"\"\"\n    ELEMENT   = 0\n    SEQ_EMPTY  = 1\n\n#---------------------------------------------------------------------------\n\nclass BitMapProperties:\n    \"\"\"\n    BitmapProperties class.\n    \n    A BitmapProperty specifies a part of a document that is read\n    or written in a common data structure format.\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif len(seq) == 4 and seq[0] == FillRequest.MISSING\nor len(seq) > 4:\n\treturn not (seq[0] == FillRequest.MISSING or seq == seq[:4])\n\ndef is_fill_request(seq):\n\t\"\"\"\n\n\t\"\"\"\n\treturn is_fill_request_seq(seq)\n\ndef make_fill_request(seq):\n\t\"\"\"\n\tCreate an :class:`~pygmt.model.FillRequest` object.\n\n\t*seq* is either a FillRequest element,"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t# If a sequence is present, check all members in it to\n\t# see if there is still at least one non-blank source\n\t# sequence in the Sequence.\n\treqs = [req if not req.blank or req.seq[0] else req.strip()\n\t\t\tfor req in seq if req]\n\tif 0 < len(reqs) <= 1:\n\t\treturn True\n\n\t# If a sequence is present, check all members in it to\n\t# see if there is still at least one non-blank source\n\t# sequence in the Sequence.\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif seq is None:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# @UndefinedVariable\n\t\treturn False\n\tif len(seq)==0:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# @UndefinedVariable\n\t\treturn False\n\tfor v in seq:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# @UndefinedVariable\n\t\tif not type(v[0]).__name__ == 'int':\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# @UndefinedVariable\n\t\t\treturn False\n\tif len(seq)%3!=0:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# @UndefinedVariable\n\t\treturn False\n\tif type(seq[0][0]).__name__"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\timport re\n\t\n\tfill_req = FillRequest()\n\tseq = parse_fill_request(seq)\n\tassert fill_req.get_seq() == seq\n\tif n_pairs(seq) == 2.:\n\t\tfor fill_type in ['first_entry', 'follow_entry']:\n\t\t\tfill_req.seq['FillType'] = fill_type\n\t\t\tfor entry in fill_req.seq[fill_type]:\n\t\t\t\tassert is_fill_entry(entry)\n\t\t\t\tassert is_fill_entry(entry['Fill'])\n\tassert not"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\tif not isinstance(seq, pdb_object.FillRequestSequence) or len(seq) < 1:\n\t\treturn False\n\t\n\tseq_is_src = False\n\tfor elem in seq.elements:\n\t\tif isinstance(elem, pdb_object.Element):\n\t\t\tif elem.sequence!= seq.sequence:\n\t\t\t\treturn False\n\t\t\tseq_is_src = True\n\t\telse:\n\t\t\treturn False\n\t\n\treturn seq_is_src\n\ndef is_fill(atom):\n\t\"\"\"\n\tTest whether"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\treturn _TestXMLUtility.is_non_empty_elem(seq) and not _TestXMLUtility.is_seq_in_any(seq, [elem.tag for elem in doc.getElements()])\n\t\n\t\ndef is_source(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a Source element.\n\t\"\"\"\n\t\n\treturn type(seq) is not _basetypes.ElementID\n\t\n\t\ndef is_fill(seq):\n\t\"\"\"\n\tTest whether *seq"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n\ndef update_request(obj, **kwargs):\n\t\"\"\" updates obj's request argument from kwargs \"\"\"\n\tobj.request = update_dict(obj.request, kwargs)\n\t\ndef update_fill(obj, **kwargs):\n\t\"\"\" updates obj's fill argument from kwargs \"\"\"\n\tobj.fill = update_dict(obj.fill, kwargs)\n\ndef update_defaults(obj):\n\t\"\"\" update obj as it was originally created\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif is_instance(obj, Execution.Request) and get_type_name(obj).endswith('FillRequest'):\n\t\tif len(obj.payload) > 0:\n\t\t\treturn True\n\treturn False\n\n\ndef get_operation_name(obj):\n\tif is_instance(obj, Execution) and is_instance(obj.command, Execution):\n\t\treturn obj.command.command_name if DEBUG else obj.command.class_name\n\n\ndef get_operation_name(obj):\n\tif is_instance(obj, Execution) and is_instance"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (obj.is_filled() and obj.is_fill_request())\n\n\ndef is_fillin_request_el(obj):\n\"\"\"\n\tObject contains executable methods '.fillin','request_fillin'.\n\t\"\"\"\n\treturn (obj.is_filled() and obj.is_fill_request())\n\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\ttry:\n\t\tif not hasattr(obj, 'fill'): raise NotImplementedError\n\t\tif not get_params(obj, 'fill'): raise ValueError\n\texcept AttributeError:\n\t\traise NotImplementedError\n\tfinally:\n\t\tdel obj\n\n@is_fill_request_el\ndef set_default_value(obj, param):\n\t\"\"\"\n\tReturns default value for given parameter.\n\t\"\"\"\n\ttry:\n\t\tif not hasattr(obj, '_param_default'): raise KeyError\n\t\telif not is"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif isinstance(obj, FillRequest): # This will always be true.\n\t    return 1\n\telse:\n\t    return 0\n\ndef is_fill_request_is(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill','request' and 'is'.\n\t\"\"\"\n\tif isinstance(obj, FillRequest): # This will always be true.\n\t    return 1\n\telse:\n\t    return 0\n\ndef is_file_write_el(obj):\n\tif isinstance(obj, FileWrite): # This will always be true."}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tobj.fill(self)\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tdef ffill_data(x):\n\t\t\"\"\"\n\t\t\tMethod that calculates a fill value for the given object.\n\t\t\t\n\t\t\t@param x: object to calculate fill value for\n\t\t\t@return: calculated fill value\n\t\t\"\"\"\n\t\tobj = obj.pObject\n\t\tif obj.is_fill_obj():\n\t\t\treturn obj.compute_fill_value()\n\t\telif obj.is_request_obj():\n\t\t\treturn obj.compute_fill_value()\n\t\n\t\t\t\t\n\tdef ffill_obj(obj):\n\t\t\"\"\"\n\t\t\tObject contains executable methods '"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif not hasattr(obj, 'fill') or not hasattr(obj,'request'):\n\t\treturn False\n\ttry:\n\t\tobj.fill()\n\t\treturn True\n\texcept:\n\t\treturn False\n\ndef get_data_as_numpy_array(data, dtype=str):\n\t\"\"\"\n\t\tConvert variable data into its correct numpy data format.\n\t\"\"\"\n\tif isinstance(data, str):\n\t\tobj = QString(data)\n\t\tdata_type = dtype(obj)\n\telif hasattr(data, '"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tis_fill = False\n\tif not hasattr(obj, 'fill'):\n\t\treturn is_fill\n\tis_fill = obj.fill()\n\treturn is_fill\n\ndef obj_fill_request_el(obj, **kwargs):\n\t\"\"\"\n\tObject contains executable methods 'fill' and'request'.\n\t\"\"\"\n\tif kwargs:\n\t\traise TypeError('Illegal keyword arguments:'+ str(kwargs))\n\tis_fill = hasattr(obj, 'fill')\n\treturn not is_fill\n\ndef get_fill"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\ttry:\n\t\treturn obj.methods['fill']\n\texcept:\n\t\tif obj.operation:\n\t\t\treturn obj.operation\n\t\tobj.operation = obj.name\n\t\treturn obj.operation\n\texcept:\n\t\tobj.operation = obj.name\n\t\treturn obj.operation\n\t\ndef is_clear_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'clear' and'request'.\n\t\"\"\"\n\ttry:\n\t\treturn obj.methods['clear']\n\texcept:\n\t\tif obj.operation:\n\t\t\treturn obj.operation\n\t\t"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn not obj.is_run_el()\n\n\nclass Command(Node):\n\t\"\"\"\n\t\tInitialises a Command node.\n\t\"\"\"\n\tdef __init__(self, command=None, options={}):\n\t\tsuper(Command, self).__init__(command)\n\t\tself.options = options\n\t\t# add command to the self (command)\n\t\tself.set_command(command)\n\t\tself.run()\n\n\tdef run(self):\n\t\tself.node.run(**self.options)\n\t\t# self.node.reset()\n\n\nclass"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\n\treturn obj.__is_run_object__\n\ndef get_run_el(obj):\n\t\"\"\"\n\t\tReturns the object to run script 'run' or 'run_script'\n\t\"\"\"\n\t\n\treturn obj.__run__ or obj.__run_script__\n\ndef get_run_script_name(script):\n\t\"\"\"\n\tReturns script name as a tuple of three letters\n\t\"\"\"\n\tscript = script.strip()\n\t\n\trun_path = get_run_script_path(script)\n\treturn (script, run_"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tif isinstance(obj, QtGui.QMainWindow):\n\t\treturn 'QtMainWindow' in dir(obj) or 'QMainWindow' in dir(obj)\n\telif isinstance(obj, QtGui.QShortcut):\n\t\t_shortcut = obj.shortcut()\n\t\treturn'shortcut' in dir(_shortcut) and _shortcut.objectName()!= 'QKeySequence'\n\telif isinstance(obj, type(None)):\n\t\tpass\n\telse:\n\t\treturn False\n\treturn True\n\ndef is"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj['executable_name'] == 'run'\n\ndef run_el(*args, **kwargs):\n\t# run the main executable.\n\tif get_run_command(*args, **kwargs):\n\t\tprint_warning('executable not run: %r' % args[0])\n\t\treturn True\n\treturn False\n\ndef check_el(args):\n\tcheck_function_name(args[0])\n\tif not run_el(*args):\n\t\treturn False\n\treturn True\n\n#----------------------------------------------------------------------------#\n# Internal helper functions, should not"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn getattr(obj, 'run', None) and len(getattr(obj,'methods', [])) > 0\n\n\ndef get_run_el_types(ob):\n\t\"\"\"\n\t\tHelper to extract all run methods from an object:\n\t\t\t-> list: all methods to be executed\n\t\t\t-> list: any methods\n\t\"\"\"\n\trun_el_types = []\n\tif getattr(ob,'methods', None) is None:\n\t\treturn None\n\tfor method_type in ob.methods:\n\t\tif is_run_el(method_"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj.__name__ == 'run'\n\n\ndef is_run_f(obj):\n\t\"\"\"\n\tObject contains executable method 'run().'\n\t\"\"\"\n\treturn obj.__name__ == 'run()'\n\n\ndef _check_run(class_name, *args, **kwargs):\n\t\"\"\"\n\tCheck run is actually the method or class for a given type and arguments/keywords\n\tUse to see if run is really a method/class with a common args/kwargs.\n\t\"\"\"\n\n\tif not issubclass(getattr(inspect"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj is not None and any(getattr(obj, f, None) for f in dir(obj) if callable(getattr(obj, f)))\n\ndef un_run_el(obj):\n\t\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn getattr(obj, 'run', None)\n\ndef is_log_level(log_lvl):\n\t\"\"\" \n\tObject contains executable method 'get_log_levels'.\n\tFunction that returns a string of log level for a given string type\n\te.g. 'DEBUG',"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn not obj.is_error()\n\ndef is_run_s(obj):\n\t\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn not obj.is_success()\n\ndef is_run_e(obj):\n\t\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn not obj.is_e()\n\ndef is_run_e_e(obj):\n\t\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn not obj.is_e.succeeded(obj.inputs)"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tcmd = \"ssh -o StrictHostKeyChecking=no -i /etc/ssh/id_rsa ~/.ssh/config\"\n\n\t# Note: The above call for the SSH connection is necessary to get the private key inside the ~/.ssh folder for python 3.5+\n\t# see https://docs.python.org/3.7/library/socket.html#socket.connect\n\tproc = subprocess.run([cmd], stdout=subprocess.PIPE, check=True, env={\"SHELL\": cmd})\n\treturn proc.returncode == 0\n\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tresult = hasattr(obj,'__main__')\n\treturn (result)\n\ndef has_run_el(obj):\n\t\"\"\"\n\t\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn (is_run_el(obj))\n\ndef is_run_func(obj):\n\t\"\"\"\n\t\tObject contains executable method 'run'.\n\t\"\"\"\n\tresult = dir(obj)\n\treturn (result)\n\ndef has_run_func(obj):\n\t\"\"\"\n\t\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn (is_run"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t#if obj.__class__.__name__ == 'ObjectProperty':\n\treturn 'fill' not in obj\n\ndef is_fill_property(obj):\n\tif obj.__class__.__name__ == 'Property':\n\t\treturn True\n\tif obj.__class__.__name__ == 'ObjectProperty':\n\t\tif obj.__class__.__name__ == 'ProWidget':\n\t\t\ttry:\n\t\t\t\treturn obj.__dict__[obj.name] == obj.value\n\t\t\texcept KeyError:\n\t\t\t\treturn obj.__dict__[obj."}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif isinstance(obj, AlgorithmError):\n\t\treturn (obj.status == AlgorithmErrorStatus.ERROR_PROC_COMPLETE and\n\t\t\t\tobj.error_message.endswith('has no valid parameter'))\n\tif isinstance(obj, AlgorithmError):\n\t\treturn (obj.status == AlgorithmErrorStatus.ERROR_PROC_COMPLETE and\n\t\t\t\tobj.error_message.endswith('algorithm failed'))\n\tif isinstance(obj, AlgorithmStatus):\n\t\treturn (obj.status == AlgorithmStatus.UNKNOWN_PROC"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tf, c = obj.get_fill_compute_el()\n\tif f!= 0 or c!= 1:\n\t\treturn False\n\treturn True\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and callable(obj.fill)\n\ndef is_fill_plot_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'plot' and 'fill'.\n\t\"\"\"\n\treturn hasattr(obj, 'plot') and callable(obj.plot)\n\ndef is_histo2d_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'histograms'.\n\t\"\"\"\n\treturn hasattr(obj, 'histograms')\n\ndef is_histo_obj(obj):\n\t\"\"\"\n\tAn"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif (obj.is_var_compute() and\n\t\tobj.is_function_compute()):\n\t\tif obj.get_attr('is_compute') and obj.get_attr('is_fill_compute_el'):\n\t\t\treturn True\n\treturn False"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif isinstance(obj, dict):\n\t\tis_fill = obj.get(\"compute\", [])\n\telif isinstance(obj, list) or isinstance(obj, tuple):\n\t\tis_fill = [obj.get(\"compute\", [])]\n\telse:\n\t\treturn False\n\tfill_compute = {\n\t\t\"fill\": True,\n\t\t\"compute\" : is_fill,\n\t}\n\treturn fill_compute\n\n\n\ndef is_fill_compute_el(obj):\n\t\"\"\"\n\tObject contains executable methods '"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn callable(obj.fill)\n\n\ndef is_fill_array_to_str(obj):\n\treturn callable(obj.fill_to_str)\n\n\ndef is_fillable(obj):\n\treturn (\n\t\tis_fillable_method(obj.fill_to_str) or\n\t\tis_fillable_array_to_str(obj.fill_method_str)\n\t)\n\n\ndef is_fillable_method(method):\n\treturn not is_fillable(method)\n\n\ndef is_fillable_array_to_"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif \"fill\" in dir(obj):\n\t\tif obj.fill is not None:\n\t\t\tfunc = obj.fill\n\t\t\ttype = \"fill\"\n\t\t\tname = \"fill\"\n\t\t\treturn type, name, func\n\tif \"compute\" in dir(obj):\n\t\tif obj.compute is not None:\n\t\t\tfunc = obj.compute\n\t\t\ttype = \"compute\"\n\t\t\tname = \"compute\"\n\t\t\treturn type, name, func\n\traise AttributeError(\"Class '%s' does not have an executable method '%s'\" % (type"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif type(obj) == str:\n\t\tobj = path.join(BASE_PATH, obj)\n\t\tif not obj.endswith('.txt') and \\\n           not path.exists(obj+'.pkl'):\n\t\t\tif object in ('fill', 'fill_n','draw_compare'):\n\t\t\t\tprint('Skipping %s' % obj)\n\t\t\t\treturn 1\n\treturn 0\n\n#====================================================================================================================================#\n#====================================================================================================================================#\n\ndef get_path(fname):\n        \"\"\"Function to return the full path to the folder"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tpass\n\ndef is_fill_fill_com(obj, name):\n\treturn hasattr(obj, name)\n\ndef is_fill_fill_ec(obj):\n\treturn callable(obj.__func__) or hasattr(obj, '_getattr_fill_fill_el')\n\ndef is_fill_fill_es(obj):\n\treturn callable(obj.__func__) or hasattr(obj, '_getattr_fill_fill_el') or is_scipy_sparse(obj)\n\ndef is_fill"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tkeys = d1.keys() | d2.keys()\n\tif level is not None:\n\t\tkeys.discard(\"_\") # filter out recursion\n\tif not keys:\n\t\traise ValueError(\"Can't find any common items\")\n\n\td = {}\n\tfor key in keys:\n\t\tif isinstance(d1, dict) and isinstance(d2, dict):\n\t\t\titem = d1.get(key, key)\n\t\telif isinstance(d1, dict):\n\t\t\titem = d1[key]\n\t\t\td[key] = {}"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td1 = dict(d1)\n\td2 = dict(d2)\n\tif level >= 0:\n\t\tif not d2.keys():\n\t\t\td1[None] = d2.values()\n\t\telif not d1.keys():\n\t\t\td2[None] = d1.values()\n\t\telse:\n\t\t\tif d1[None]!= d2[None]:\n\t\t\t\td1[None] = intersection(d1[None], d2[None], level - 1)\n\t\t\tfor key in d1:\n\t\t\t\tsubset = d1."}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\timport re\n\treturn collections.defaultdict(list, list(map(difference, d1, d2, level)))\n\ndef get_ordered_set(d, value, level=-1):\n\tret = collections.OrderedDict()\n\tfor k, dv in d.items():\n\t\tif value in dv:\n\t\t\tret[k] = dv.get(value)\n\t\telse:\n\t\t\tret[k] = get_ordered_set(dv, value, level)\n\treturn ret\n\ndef remove_duplicated_items("}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td = {}\n\tfor k, v in d1.items():\n\t\tif k in d2:\n\t\t\tif v!= d2[k]:\n\t\t\t\td[k] = difference(d1, d2, level + 1)\n\t\telse:\n\t\t\td[k] = v\n\treturn d\n\ndef setdiff(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a second-level dictionary containing items from *d1*\n\tnot contained in *d2*, but only items from *d2*, not also in *d1*.\n\t"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif d1 is None:\n\t\traise TypeError(\"Can't difference an empty dictionary.\")\n\tif d2 is None:\n\t\traise TypeError(\"Can't difference an empty dictionary.\")\n\tif not hasattr(d1, \"keys\"):\n\t\treturn d1\n\tif not hasattr(d2, \"keys\"):\n\t\treturn d2\n\td1, s1 = asarray(d1, subok=True)\n\td2, s2 = asarray(d2, subok=True)\n\tif not s1 == s2:\n\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td = {}\n\tif not d1: return d\n\tif not d2: return d\n\td = {k2: v for k1, v in d1.items() if k1 in d2 and d2[k1]!= v}\n\tfor k2, v in d2.items():\n\t\tif k1 not in d or d[k1]!= v:\n\t\t\td.pop(k2, None)\n\treturn d\n\ndef intersection(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with keys from"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level < 0:\n\t\t# recurse at most down, so if the result is recursively\n\t\t# contained in 'other' it will be the addition of the contents\n\t\t# of 'd1' to the contents of 'd2'\n\t\treturn difference(d1, d2, level+1)\n\t\n\tif not hasattr(d1, 'iteritems'):\n\t\treturn difference(d1, d2)\n\t\n\td = {}\n\tfor k, v1 in d1.iteritems():\n\t\tk1 = k\n\t\tif not"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tret = {}\n\tif level == -1:\n\t\tpaths = None\n\telse:\n\t\tpaths = []\n\t\tpaths.append([d1, d2])\n\t\t\n\twhile paths:\n\t\t\n\t\tm = len(paths)\n\t\tif m == 1:\n\t\t\td = paths[0]\n\t\telif m == 2:\n\t\t\td = []\n\t\t\t(dd1, dd2) = paths[:2]\n\t\t\tfor i in set(dd1).intersection(set(dd2)):\n\t\t\t\td.append(dd1"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tkw = {'d1':d1, 'd2':d2}\n\tif isinstance(d1, Mapping):\n\t\tkw['level'] = level\n\t\tkw.update(intersection(d1,d2,level+1))\n\t\tkw = getDict(**kw)\n\telse:\n\t\tkw.update(d1.items())\n\t\tkw = getDict(**kw)\n\tfor k,v in kw.items():\n\t\tif k not in d2:\n\t\t\td2[k] = v\n\t\telif is"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t# TODO - should this only contain the items in d2?\n\tif level >= 0:\n\t\tif d1 == d2:\n\t\t\treturn {d1}\n\t\td = difference(d1, d2, level + 1)\n\t\ta = d._elements()\n\t\tfor k in d.keys():\n\t\t\tb = d[k]\n\t\t\ta -= b._elements()\n\t\treturn d\n\telse:\n\t\treturn Intersection(d1, d2)\n\n#===============================================================\n#\n# MACHINE LEARNING PYTH"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor x in range(self.nbins):\n\t\tpx = coord[0]\n\t\tif (px < 0):\n\t\t\tpx = 0\n\t\tif (px >= self.bins.shape[0] - 1):\n\t\t\tpx = self.bins.shape[0] - 1\n\t\tif (px >= px): \n\t\t\tif (weight > 0):\n\t\t\t\tself.hist[px,:self.bins[px,:]-1] += weight\n\t\t\t\tself.hist[px,(self.bins[px,:]-1):(self.b"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.update(*coord, weight=1-weight)\n\n\nclass FillNdArray():\n\t\"\"\"\n\tClass for filling an array with values, of size equal to the last\n\tsize of the array.\n\t\"\"\"\n\tdef __init__(self, arr=None, fill=lambda a,x:a):\n\t\t\"\"\"\n\t\tIf *arr* is numpy.ndarray (including masked numpy.void & MVObjects),\n\t\tthen *fill* is a function that takes two arguments and returns\n\t\ta masked numpy.ndarray.\n\t\tOtherwise fill"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\n\t\n#    def getItems(self):\n#        \"\"\"\n#        Return the list of the items currently drawn on the canvas.\n#        \n#        Note that the canvas is not guaranteed to hold any objects, but\n#        this routine returns items that the user has access to, so it's\n#        recommended to use other means of drawing items.\n#        \"\"\"\n#        pass\n#    def getItemCoords(self, item):\n#        \"\"\"\n#        Return the (x,y) coordinates of an item given its *label*.\n#\n#"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif coord < 0 or coord >= self.shape[0] or not ((coord % 1) >=\n\t\tself.percentile_99):\n\t\treturn\n\tif self.shape[0] < 1000 and not self.fill_method in [\n\t\t\t\"fill_lower\", \"fill_upper\"]:\n\t\t# FIXME: this may not be the best idea if the user ever decides to\n\t\t# remove the low end of the axis with `fill_lower_upper_edges=False\n\t\tif not (coord == 0 or coord == self.shape[0] - 1):\n"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\thist, weights = self.get_histogram(coord)\n\tfor v in np.nditer(hist):\n\t\tv += weight\n\t\tself.fill(v, weight)\n\n\t\n\t\n\nif __name__ == '__main__':\n    c = Coordinate('a')\n    print(c)\n    a = np.array([1.0])\n    b = np.array([2.0])\n     \n    a_hist, hist = a.histogram()\n    b_hist, hist = b.histogram()\n    print(a"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif coord.any() < self.Nbins:\n\t\t# Ignore point outside the histogram\n\t\tcoord = np.where(coord < 0, self.Nbins, coord)\t\n\n\tif weight > 0:\n\t\thist = np.histogram(coord, self.Nbins)[0]\n\t\treturn self.add_weight(hist, coord*weight)\n\telse:\n\t\thist, edges = np.histogram(coord, self.Nbins, density=True)\n\t\treturn hist, edges\n\n\n#################################################\n# Histogram Generation                               #"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.data[coord] = self.data.max() + weight * \\\n\t\t\t\t\t\tnp.logical_or(self.data[coord] <= -self.hist_max, self.data[coord] > -self.hist_max)\n\tself.data[coord] = self.hist_max\n\t\n\n\n\n# Plotting Functions -----------------------------------------------------------------------\n\ndef plot_sphere(ax, center, radius, color=\"green\"):\n\t\"\"\"\n\tDraw the sphere at *center* with the given *radius*\n\t\"\"\"\n\n\tax.add_line(Regular"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif coord < 0 or coord >= len(self.data):\n\t\traise ValueError(\"hist: coordinate out of range\")\n\tif coord in self.data:\n\t\traise ValueError(\"hist: coordinate already exists\")\n\ti = 0\n\tself.data[coord] = self.data[0] # skip first index only\n\twhile ((1 << i) & self.data[coord] == self.data[coord]): i += 1 # get next possible index\n\twhile ((1 << i) & self.data[coord] >= self.data[coord]): i -="}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx,y = coord\n\n\t# avoid rounding errors\n\tif x+1 < self.nx:\n\t\tif y+1 < self.ny:\n\t\t\tw = (self.bins[y+1][x] + self.bins[y][x+1] +\n\t\t\t\tself.bins[y+1][x+1] - self.bins[y][x+1]) / 4.0\n\t\telif y > 0:\n\t\t\tw = (self.bins[y-1][x] + self.bins[y][x+"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx, y = coord\n\tleft = max(0, max(x-1, 0))\n\tright = min(len(self.histo[x]), len(self.histo))\n\ty -= self.borderShift\n\tw = y + y % 2\n\tfor x in range(left, right +1):\n\t\tif x < self.range.x or x > right:\n\t\t\tcontinue # skip points outside range\n\t\t#if self.histo[self.range]!= self.histo[x]:\n\t\tif (self.histo[x]["}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\tlabels_with_exclude_key = set([e[\"key\"] for e in labels.keys()])\n\tfor label_exception in labels.values():\n\t\tassert all(\n\t\t\t[\n\t\t\t\ttype(l) == dict\n\t\t\t\tand \"key\" in l and l[\"key\"] in labels_with_exclude_key\n\t\t\t\tfor l in label_exception\n\t\t\t]\n\t\t) is True\n\t\n\tfor label, expected_exception_messages in labels.items():\n\t\tfor match in expected_exception_messages:\n\t\t\tif"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor label in labels.keys():\n\t\tif isinstance(labels[label], dict):\n\t\t\tif not all(isinstance(key, str) for key in labels[label].keys()):\n\t\t\t\traise ValidationError(\n\t\t\t\t\terror.LabelKeyInvalid(),\n\t\t\t\t\t{label: \"expected string or bytes-like object\"},\n\t\t\t\t)\n\t\t\tif not all(isinstance(value, str) for value in labels[label].values()):\n\t\t\t\traise ValidationError(\n\t\t\t\t\terror.LabelValueInvalid(),\n\t\t\t\t\t{label: \"expected string or bytes-like object\"},\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(f'{labels} must be a dictionary')\n\n\tfor label in labels:\n\t\tif label not in [k for k, v in labels.items() if isinstance(v, (type(None), str))]:\n\t\t\traise TypeError(f'invalid label: {label}')\n\n\ndef validate_labels(*, labels):\n\t\"\"\"\n\tValidate all labels within a dictionary and raise an :class:`ValidationError` otherwise.\n\t\n\tArgs:\n\t    labels"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"labels (%s) is not a dictionary.\" % type(labels))\n\tif not isinstance(labels, dict):\n\t\tfor key in labels.keys():\n\t\t\tif isinstance(labels[key], dict):\n\t\t\t\traise TypeError(\"labels (%s) has nonzero keys (with keys not being a string or bytes).\" % type(labels))\n\tif not isinstance(labels.get(\"key1\", \"\"), bool):\n\t\traise TypeError(\"labels (key1)"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t# Convert list like labels into dict on purpose to allow\n\t# for dictionary access or subscripting. Use a map to prevent\n\t# raising errors when iterating over labels via the ``keys``\n\t# keyword argument.\n\tif isdict(labels):\n\t\tlabels = {k: v for k, v in labels.items()}\n\n\tfor k, v in labels.items():\n\t\t# Verify regex match\n\t\tif regex_for isinstance(k, string_types):\n\t\t\tif (isinstance(v, dict) or isinstance(v, list)) and len"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise ValueError(\"labels must be a dictionary like object.\")\n\t\t\n\tfor key in labels:\n\t\tif not key in SINGLE_LABEL_REGEX.keys():\n\t\t\traise ValueError(\"label key is not a regular expression.\")\n\t\tif not isinstance(labels[key], list):\n\t\t\traise ValueError(\"label key may only be a regular expression. \" +\n\t\t\t\t\"'{}' was not given\".format(key))\n\t\tif len(labels[key])!= 1:\n\t\t\traise Value"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tcheck_type(set, \"labels\", dict)\n\n\tfor label_type in labels:\n\t\tif isinstance(labels[label_type], list):\n\t\t\tlabels[label_type] = set(labels[label_type])\n\t\telse:\n\t\t\tif labels[label_type] in labels[LABELS] and labels[label_type]!= labels[LABELS][labels[label_type]]:\n\t\t\t\traise ValidationError({\"invalid\": [\"must match the key-value pair {%s}\" % label_type, labels[label_type"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tvalid_keys = set(list(labels.keys()) + [\"$\"])\n\tvalid_values = set(list(labels.values()) + [\"$\"])\n\n\tif not isinstance(labels, dict):\n\t\traise ValidationError(f\"labels should be a dictionary with keys '{valid_keys}' and '{valid_values}'\",\n\t\t\t[labels],\n\t\t\t)\n\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\traise ValidationError(f\"expected string or bytes-"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terror_messages = []\n\tfor key, value in labels.items():\n\t\tif isinstance(key, str):\n\t\t\tr = re.compile(re.escape(key))\n\t\telse:\n\t\t\tr = key\n\t\tif not isinstance(value, str):\n\t\t\traise ValidationError(\n\t\t\t\t[{\"message\": [\"'{}' is not a string.\".format(key or str(key))]},]\n\t\t\t)\n\t\tfor k, v in value.items():\n\t\t\t# check if value is a regex\n\t\t\tif isinstance(v,"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tvalid_keys = frozenset(labels.keys())\n\tvalid_values = frozenset(labels.values())\n\tfor key, value in labels.items():\n\t\tif key not in valid_keys:\n\t\t\traise ValidationError(f'{key} is a key which is not the same as a label')\n\t\tif isinstance(value, str) and re.match(f'.*$', value):\n\t\t\traise ValidationError(\n\t\t\t\tf'Unexpected string: \"{value}\" in the labels dictionary.  '\n\t\t\t\tf'All labels must be"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn _get_resource_name_regex_internal()\n\n_HOTKEYS_REQUEST_TYPE_MAP = {\n\tkCpuType.HOTKEY: None,\n\tkCpuType.HOTKEY_TOT_MEM: _HOTKEYS_TOTAL_MEM,\n\tkCpuType.HOTKEY_MEM: _HOTKEYS_MEM,\n\tkCpuType.HOTKEY_DYN: None,\n\tkCpuType.HOTKEY_CACHE: None,\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(\"([a-z]+)([0-9]+)\")\n\n##\n##  The default name format for the resources:\n##  (resourceName, [resourceTypeName1, resourceTypeName2,...]).\n##\ndef _get_resource_name_format():\n\t\"\"\"Build or return the default form of the resource name.\n\n\tReturns:\n\t    (str): the default format to use for the resource names, to\n\t    supply a default resource type name.\n\t\"\"\"\n\treturn \"([a-z]+)([0-"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn r'[^\\w\\-_]+'\t\t# We include some symbols\n\t\t# e.g. '-_' for _XV2, '_' for XV2, etc.\n\treturn r'^[_a-zA-Z][_a-zA-Z]+$'\n\ndef _check_and_log(config, k):\n\t\"\"\"Check and log whether the provided string is a valid or well formed\n\tCKV, and return the string.\n\n\tArgs:\n\t\tconfig (Config):\n\t\t\tthe config to perform checks against\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(r'^(?P<resource>.*)$')\n\ndef _get_resource_names():\n\t\"\"\"\n\tBuild or return a list of resource names matching the\n\tcompiled regular expression pattern below.\n\t\n\tReturns:\n\t    (list[str]): the list of resource names, as returned by the\n\t    ``get_resource_names()`` method.\n\t\"\"\"\n\tregex = _get_resource_regex()\n\treturn [resource.match(self.__name).group('resource')\n\t\tif regex"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tRESOURCE_NAME_REGEX_STR_PATH = (\n\t\tf'{{ env.krake_resource_id }}_{{{ (env.krake_resource_name.lower() + \"_\") }}.json'\n\t)\n\tRESOURCE_NAME_REGEX_PATH = (\n\t\tf'{{ _krake \"\"\" { RESOURCE_NAME_REGEX_STR_PATH } }}'\n\t)\n\treturn RESOURCE_NAME_REGEX_PATH\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(r'[^a-zA-Z0-9_\\-\\.]{2,}\\w+', re.ASCII|re.UNICODE)"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tfrom re import FULLMATCHSEPARATOR, VERBS  # This import must be here, so that we can use the built in method 'VERB'\n\tfrom io import StringIO, open\n\n\t# create the regular expressions for our resources\n\t# Note that if we get a list of values we will then try both a\n\t# regular expression matching and a regex with some special characters removed\n\t# so that they won't clash with our regular expressions\n\tfor name in kacral.k_resources:\n\t\tkacral.k_resources[name] = re.compile("}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\treturn re.compile(r\"^%s$\" % KRAKE_RESOURCE_NAME)\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif _resource_name_regex is not None:\n\t\treturn _resource_name_regex\n\n\t# Try to find the compiled regular expressions.\n\tresource_name_regex = None\n\ttry:\n\t\t# Get the built-in Regexp object.\n\t\tresource_name_regex = re.compile(r'^(?P<name>%s-?))?' % (BASE_RESOURCE_NAME_REGEX_LENGTH_CUTOFF))\n\texcept TypeError:\n\t\tpass\n\t\n\ttry:\n\t\t# Get"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif DEBUG: print('_get_resource_name_regex')\n\t\n\tresource_name_regexs = dict()\n\tresource_name_compiled_regexs = dict()\n\tresource_name_completions = frozenset()\n\t\n\tif not ARCH_REQ in req: \n\t\t# Get all resources from the server\n\t\traw_data = get_resource_list(req)\n\t\tif raw_data:\n\t\t\t# Print the total number of resources and their names\n\t\t\ttodo = [\"%d Resources\" % len("}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tvalidate, error = value.replace(' ','').split('/')\n\terrors = {'type':'value', 'errors':error}\n\treturn validate, errors\n\nclass DbHandler():\n\tdef __init__(self, logger, config=None):\n\t\tdb_module = fd.Module(__name__, logger, __file__)\n\t\tself.config = config if config else DbConfig()\n\t\tself.config.init_logger(config[\"DBConfig\"][\"log_level\"], config[\"DBConfig\"][\""}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not regexp_validator.match(value):\n\t\traise ValidationError('Wrong expression', value=value)\n\ndef required(value):\n\treturn validate_value(value) and value is not empty\n\ndef empty():\n\treturn bool(value.strip()) and value.strip()\n\ndef boolean(value):\n\tif value in ('true', 't'):\n\t\treturn True\n\telif value in ('false', 'f'):\n\t\treturn False\n\telse:\n\t\traise ValidationError('Wrong value', value=value)\n\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tmatch = re.search(r'^(\\w+|\\_+)', value)\n\n\tif match:\n\t\tif match.group(1) not in RE:\n\t\t\traise ValidationError('Not a valid character (it must be one of %s)' % RE)\n\n\t\treturn value\n\n\ndef validate_type(value):\n\t\"\"\"\n\tValidate the given value against a given regular expression.\n\t\n\tArgs:\n\t    value: the value to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the given"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\traise NotImplementedError('subclass implement an \"validate_value\" method')\n\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\timport re\n\t\n\ttry:\n\t    m = re.search(value, value)\n\texcept:\n\t    return True\n\t\n\tif not m:\n\t    raise ValidationError(value)\n\t\n\treturn True\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif value is None:\n\t\treturn\n\t\n\tmatch = re.search(r\"(^[^,/\\s]+$)\", value)\n\tif match:\n\t\terror = \"{} is a valid regular expression.\".format(value)\n\t\t\n\t\tif match.group(0):\n\t\t\terror += \" It contains a ',' or '_a', so it is not a bare number, so it can occur in an operand.\"\n\t\t\n\t\traise ValidationError(error)\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not VALID_COMBINATIONS_RE.match(value.lower()):\n\t\traise ValidationError(\"Value contains invalid combinations. Valid combinations are '+', '-', '.', '*', ','.\")\n\ndef validate_patterns(*patterns):\n\t\"\"\"\n\t\tA validator which only performs validations on patterns.\n\t\n\tArgs:\n\t  *patterns: a tuple of patterns to validate, and a validator\n\t\twhich returns a boolean (True/False).\n\t\"\"\"\n\tfor pattern in patterns:\n\t\tif not isinstance("}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not isinstance(value, str):\n\t\traise ValueError('Expected string argument. Got: {!r}'.format(value))\n\telif value in ('', None, ''):\n\t\t# Value is null/empty - treated as None; a valid date/time/date\n\t\tpass\n\telif not isinstance(value, unicode):\n\t\t# value is an object that doesn't have a __str__ method\n\t\traise ValueError('Expected unicode argument. Got: {!r}'.format(value))\n\telif value.format(t="}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tmatch = re.search(REGEX, value)\n\tif match is not None:\n\t\traise ValidationError(\"Invalid argument: '%s' should not include a valid regular expression: %s\" % (value, REGEX))\n\treturn value\n\ndef string_to_slug(slug):\n\t\"\"\"\n\tConvert the given string into a slug.\n\t\n\tArgs:\n\t    slug: the value to validate.\n\t\n\tReturns:\n\t    the converted value.\n\t\"\"\"\n\tif not is_string(slug):\n\t\t"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tre.fullmatch(value, '')\n\t\treturn True\n\texcept TypeError:\n\t\traise ValidationError('Invalid string value.')\n\t\t\n\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tassert regex.match(key)\n\t\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\terror_message = r\"Exactly 6 characters are required to use this program\"\n\terror_message = re.compile(error_message)\n\t\n\tif 0 < len(key) < 6:\n\t\traise ValidationError(error_message)\n\n\n'''\ndef validate_key(key):\n\tm = (re.search(r'[^0-9A-Z]', key))\n\t\n\tif not m:\n\t\traise ValidationError(\"Exceeding key requirement.\")\n\telse:\n\t\treturn\n'''\n\ndef validate"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not key:\n\t\traise ValidationError(\"Key is empty, please insert a key.\")\n\tif re.match(r'^[0-9a-zA-Z_./-&$!#(){}]{1,}$', key):\n\t\traise ValidationError(\"Keys of strings containing only alpha-numbers and/or numbers and/or dashes may not be used.\")\n\n@app.route('/inbox_update', methods=['POST'])\ndef inbox_update():\n\t\"\"\"\n\tUpdate the entire \"Inbox\" and then"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif re.match(Key_REGEX_RE, key) is None:\n\t\traise ValueError('Invalid key: must be of the form: one of: {key_regex}')\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\treturn key.exact_match()    ## check for exact match\n\n\ndef parse_url(url, data):\n\t\"\"\"\n\tParse a given URL into a dictionary of its component parts.\n\t\n\tArgs:\n\t    url: the URL to be parsed\n\t    data: the parsed querystring\n\t\n\tReturns:\n\t    the parsed part of a dictionary\n\t\"\"\"\n\ttry:\n\t\tparsed = urlparse(url)\n\t\treturn {p.qname: p.value for p in parsed.query.split('&')}\n\texcept"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\tif not (isinstance(key, str)):\n\t\traise ValidationError('Key must be a string')\n\t\n\tif key.endswith(('.json', '.yaml', '.yml')) and not re.match(r'^[\\w\\d\\-\\_]+\\.json$', key):\n\t\traise ValidationError('Key should start with a number, not \\'{}\\'.'.format(key))\n\tif not (key.startswith('/') or key in ('/' * 1024, '/storage/') or key.start"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match('^[a-zA-Z0-9_]+$', key):\n\t\traise ValidationError('The key contains invalid characters')\n\ndef validate_name(name):\n\t\"\"\"\n\t\tValidate the given name against the corresponding regular expression.\n\t\t\n\t\tArgs:\n\t\t    name: the string to validate\n\t\t\n\t\tRaises:\n\t\t    ValidationError: if the given name is not correctly formatted.\n\t\"\"\"\n\tif not re.match('^[a-zA-Z0-9]+$', name):\n\t\t"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tassert type(key) == str\n\tassert key!= ''\n\tassert len(key) > 0\n\t# TODO: Remove this (if it happens to work on windows)\n\t# key = key.decode('utf-8').rstrip(\"*\").lower()\n\t# key = key.decode('cp1251').lower()\n\t# key = key.rstrip().replace('\\t','')\n\tassert re.search('^\\w+\\*{1,6}$', key)\n\t# If not, then the key should be a"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tkey = key.lower()\n\tif is_number(key):\n\t\tis_number = True\n\telif is_string(key):\n\t\tis_string = True\n\telse:\n\t\traise Exception(\"Invalid key format.\")\n\tif not is_valid_key and key not in ['[{}]', '{}']:\n\t\traise Exception(\"Invalid key format for the value.\")\n\t\t\n\tif key and is_number:\n\t\tnumbers = key.replace(',', '.') if '(.' in key else ''.join(key)\n\t\tvalid"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\ttry:\n\t\teval(key)\n\texcept ValueError as error:\n\t\traise ValidationError(('Invalid key: %s\\n%s') % (key, error))\n\texcept ZeroDivisionError as error:\n\t\traise ValidationError(('Invalid key: %s\\n%s') % (key, error))\n"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tnew_observer_schema = {}\n\tfor k, obj in manifest_dict.items():\n\t\tif type(obj) is dict and type(obj[k]) is dict:\n\t\t\tif k not in new_observer_schema:\n\t\t\t\tnew_observer_schema[k] = None\n\t\t\tfor k1, obj1 in obj[k].items():\n\t\t\t\tif type(obj1) is dict:\n\t\t\t\t\tgenerate_default_observer_schema_dict(obj1, first_level=first_level)\n\t\t\t\telse:\n\t\t\t\t\tnew"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema = defaultdict(list)\n\tfor resource_obj in manifest_dict.values():\n\t\tif isinstance(resource_obj, dict) and all(key in resource_obj for key in ('apiVersion', 'kind')):\n\t\t\t_resource = _format_default_observer_schema(resource_obj)\n\t\t\tobserver_schema.update([(k, v[0]) for k, v in _resource.items()])\n\t\t\tif not _resource.get('apiVersion', None) == k8s.V1APIV3B"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level:\n\t\treturn manifest_dict\n\n\tobserver_schema_dict = {}\n\t_subkeys = [\"deployments\"]\n\t\n\tfor key, value in manifest_dict.items():\n\t\tif key.startswith(\"kind:\"):\n\t\t\tcontinue\n\t\telif not isinstance(value, list):\n\t\t\tvalue = [value, ]\n\t\tif key not in _subkeys:\n\t\t\tobserver_schema_dict[_subkeys] = value\n\t\telse:\n\t\t\tpass\n\treturn observer_schema_dict\n\n\ndef generate"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level is True:\n\t\t# To save time, simply copy the content to the default_observer_schema.\n\t\tdefault_observer_schema = dict((k, v) for k,v in manifest_dict.items() if isinstance(v, (int,str,float)))\n\t\treturn default_observer_schema\n\t\n\t# In case of a single array, the value is itself a dictionary (keyed by the value)\n\tif isinstance(manifest_dict, dict):\n\t\treturn dict((k, generate_default_ob"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema_keys = set(manifest_dict.keys())\n\n\t# Create a list of observer_schema keys, because it might contain a single dict\n\t# of the required schema part of the Kubernetes resource.\n\t# In this case, it's always the one with a schema.\n\tobserver_schema_keys_dict = {}\n\tobserver_schema_key_counter = 0\n\tfor (schema_key, value) in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tif \"names"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif not first_level:\n\t\t# Recursively copy and clean the dictionary in-place\n\t\tfor name, value in manifest_dict.items():\n\t\t\tif isinstance(value, dict):\n\t\t\t\tmanifest_dict[name] = copy.deepcopy(value)\n\t\t\telif isinstance(value, list):\n\t\t\t\t# The list of lists doesn't have any keys: we're probably not working with a dict's\n\t\t\t\t# schema, but a dictionary of its sub dictionaries\n\t\t\t\tif len(value) < 1:\n\t\t\t\t\t# It's a simple list; let's just"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\tobserver_schema_dict = {}\n\tobserver_schema_dict.update(\n\t\t{\n\t\t\t\"name\": None,\n\t            \"kind\": None,\n\t            \"resource_version\": None,\n\t            \"resources\": None,\n\t            \"labels\": None,\n\t        }\n\t    )\n\n\tfor manifest in (\n\t        manifest for (manifest, _res) in manifest_dict.values()\n\t    ) if len(manifest) == 0\n\t) if len(manifest) == 0 else {\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif isinstance(manifest_dict, dict) or isinstance(manifest_dict, list):\n\t\t# Handle the manifest dict as a list or a dictionary\n\t\tresult = {}\n\t\tif hasattr(manifest_dict, \"get\") and callable(manifest_dict.get):\n\t\t\t# Handle manifest as a function\n\t\t\tresult = manifest_dict.get()\n\t\telif isinstance(manifest_dict, list):\n\t\t\tresult = manifest_dict\n\t\telse:\n\t\t\t# Handle manual data as a dictionary\n\t\t\tresult = manifest_dict.copy()\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif isinstance(manifest_dict, dict):\n\t\tobservations = manifest_dict.copy()\n\telif isinstance(manifest_dict, list):\n\t\tobservations = [element.merge_with(generate_default_observer_schema_dict(\n\t\t\titem, first_level=first_level)) for item in manifest_dict]\n\telse:\n\t\traise ValueError(\"The default observer_schema dict should be a dict or list\")\n\n\t# replace only non-dict value and none-string value with None\n\tfor key_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tdef create_observation_dict(observation, manifest, observer_type=None, field=None):\n\t\t\"\"\"\n\t\tThis function creates a new default observable schema dictionary from a Kubernetes resource\n\t\tdefinition, where the new observation is based on an ``observation`` type in a\n\t\tmanifest file, and optionally a new dictionary of values for the corresponding\n\t\tkind of object in order to be copied from the manifest file and for all\n\t\tsubschema fields.\n\t\t\"\"\"\n\t\tif type(observation) is str:\n\t\t\tdictionary = _gener"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t) -> False:\n\t\t\n\t\tfor i in range(0, len(last_applied_manifest)):\n\t\t\tif last_applied_manifest[i] in observer_fields:\n\n\t\t\t\tif not isinstance(observer_schema[i], Enum) :\n\t\t\t\t\tupdate_last_applied_manifest_list_from_dict(last_applied_manifest, observer_schema, i, 0, 0, observer_fields)\n\n\t\t\t\telse :\n\n\t\t\t\t\tif str(observer_schema[i]) in str(observer_sche"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\tdef update_last_applied_manifest(self, **kwargs):\n\t\tif 'last_applied_manifest' in kwargs:\n\t\t\tif last_applied_manifest is kwargs['last_applied_manifest'][-1]:\n\t\t\t\tcontinue\n\t\t\t# add last_applied_manifest[-1] to each value of kwargs\n\t\t\tfor attr, field in zip(\n\t\t\t\t['api_url', 'api_token', 'namespace','resource',\n\t\t\t\t 'namespace_ex', 'end"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t):  \n\t    for data in observer_schema:\n\t        if data in last_applied_manifest:\n\t            del last_applied_manifest[data]\n\n\tdef _update_last_applied_manifest_on_resp(response):\n\t\t\"\"\"update a partial ``last_applied_manifest`` using the full ``last_applied_manifest``\"\"\"\n\t\td = last_applied_manifest\n\t\tfor data in response.get('apiVersion', ['v1']):\n\t\t\tk = data or None # in `k"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t):  # will continue to update last_applied_manifest if not already present\n\t\t# first handle the schema\n\t\ttry:\n\t\t\tschema_item = get_one_of(response, [(\"manifest\", schemas.UpdateLastAppliedManifest), (\"observer_schema\", schemas.UpdateObserverSchema)])\n\t\texcept SchemaContainerValidationError:\n\t\t\tschema_item = {\"manifest\": schemas.UpdateLastAppliedManifest(observer_schema=observer_schema[0]), \"observer_schema\":"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\n\t# check if there is ``observer_schema`` and update the last_applied_manifest\n\t# if the schema is not yet present\n\tdef update_last_applied_manifest_dict_from_resp(self, **schema_key_values):\n\n\t\tif not self.last_applied_manifest:\n\t\t\treturn\n\n\t\tlast_applied_dict = self.last_applied_manifest\n\t\tlast_applied_schema = self.observer_schema\n\t\tself.update_last_applied_man"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\treturn update_last_applied_manifest_list_from_resp_rec(\n\t\tlast_applied_manifest=last_applied_manifest,\n\t\tobserver_schema=observer_schema,\n\t\tresponse=response,\n\t)\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\tdef update_last_applied_manifest_list(\n\t    self, last_applied_manifest, observer_schema, response\n\t):\n\t\tobserver_values = {}\n\t\tif \"objects\" in observer_schema:\n\t\t\tobserver_schema_len = len(observer_schema.get(\"objects\"))\n\t\t\tfor n in range(observer_schema_len):\n\t\t\t\tname_list = []\n\t\t\t\tfor item in last_applied_manifest:\n\t\t\t\t\tfield = observer_schema[\"objects\"].get"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef update_last_applied_manifest_list_from_resp(**kwargs):\n\t\t# Update our list with the updated fields\n\t\tlast_applied_manifest = kwargs['last_applied_manifest']\n\t\tobserver_schema = kwargs['observer_schema']\n\t\tresponse = kwargs['response']\n\n\t\t# Check each observed field and update to see if it is also in a list\n\t\tfor ob in observer_schema:\n\t\t\tapplied_id = ','.join([a['appId'] for a"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t):\n\t\tfor kr in last_applied_manifest_schema:\n\t\t\tsetattr(getattr(self, kr), keymap[kr[\"name\"]], getattr(self, kr)[keymap[kr[\"name\"]]])\n@functools.lru_cache()\ndef get_last_applied_manifest_dict(\n        response: Dict[str, Any],\n        observer_schema: Dict[str, Any] = None,\n        api: Optional[Dict[str, Any]] = None,\n        "}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\tdef update_last_applied_manifest_dict(self, last_applied_manifest,\n\t\t\t\tobserver_schema, response):\n\t\tif len(self._key_fields)!= len(observer_schema) or len(\n\t\t\tlast_applied_manifest\n\t\t\t)!= len(response):\n\t\t\traise KeyError(\n\t\t\t\t\"Invalid kubernetes manifest update received: \"\n\t\t\t\t\"Keys list don't match in \"\n\t\t\t\t\"K8S API response, observed/observed keys list \"\n\t\t\t\t\"don't match list"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobservers_field ='manifest:' + observer_schema['self'] + '.' + observer_schema['schema-name']\n\tfields = [value for value in last_applied_manifest.values() if value is not None]\n\tfor field in observers_field:\n\t\tif field not in last_applied_manifest.keys():\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tobserver_schema,\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tfield,\n\t\t\t\tTrue\n\t\t\t)"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_key, observed_value = last_applied_manifest[None]\n\tif observed_key not in last_applied_manifest_resp:\n\t\tobserved_value = None\n\tlast_applied_manifest_resp[observed_key] = observed_value\n\n@register\ndef get_first_observed_manifest_from_resp(last_applied_manifest):\n\t\"\"\"Retrieves the ``observed_manifest`` from the response and sets the\n\tobserved_manifest field in the ``last_app"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef update_last_applied_manifest_from_resp(resp,\n\t\t\tobserver_schema, observer):\n\t\tapplied_manifest_dict = dict(observer) if isinstance(observer, dict) else None\n\t\tfor k, v in resp['raw']['manifest'].items():\n\t\t\ttry:\n\t\t\t\tlast_applied_manifest[k] = obj_to_json(\n\t\t\t\t\tv, schema=observer_schema, ignore_unknown_fields=True\n\t\t\t\t)\n\t\t\texcept ValueError as e:\n\t\t\t\tif"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif 'last_applied_manifest' not in last_applied_manifest:\n\t\tlast_applied_manifest['last_applied_manifest'] = dict()\n\tif 'observer_schema' not in last_applied_manifest['last_applied_manifest']:\n\t\tlast_applied_manifest['last_applied_manifest']['observer_schema'] = {}\n\tif 'name' not in last_applied_manifest['last_applied_manifest']:\n\t\tlast_name"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t):\n\t\titem_id = last_applied_manifest.get(\"ItemId\", \"\")\n\t\tmanifest_name_or_uri = last_applied_manifest.get(\"Annotations\", {}).get(\n\t\t\t\"metadata.annotations\", {}\n\t\t)\n\t\tmanifest_version = last_applied_manifest.get(\n\t\t\t\"lastManifest\", \"\"\n\t\t)\n\t\tmanifest_uri = last_applied_manifest.get(\"spec\", {}).get(\"manifest\")\n\t\t\n\t\tif manifest_uri:\n\t\t\tman"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef update_kubernetes(dict_, observer_schema):\n\t\tfor observable in dict_:\n\t\t\tobserver_id = observable[\"name\"]\n\t\t\tobserved_value = kubernetes_data[observable[\"name\"]][\"id\"]\n\t\t\tobserved_type = observable[\"type\"]\n\t\t\tobserved_fields = observable.get(\"fields\", {})\n\t\t\tif observed_type!= \"list\" or not observed_fields:\n\t\t\t\tcontinue\n\t\t\tobserved_fields = update_observed_fields(observed_fields, observer_schema)"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tupdate_last_applied_manifest_dict_from_resp(\n\t\tname,\n\t\tobserver_schema,\n\t\tdict(observer_schema,\n\t\t **response),\n\t\tupdate=True\n\t)\n\n\ndef update_last_applied_manifest_dict_from_api_response_and_dict(\n\tlast_applied_manifest: dict[str, str],\n\tobserver_schema: dict[str, str],\n\tupdate: bool = True\n) -> None:\n\t\"\"\"\n\tUpdates a dict with"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\tif last_applied_manifest is not None:\n\t\t# Check, whether the last_applied_manifest has an observation schema\n\t\tif observer_schema is None:\n\t\t\tdef add_obs_schema(o):\n\t\t\t\tif hasattr(o, 'type'):\n\t\t\t\t\tif hasattr(o.type, 'type'):\n\t\t\t\t\t\treturn f'{o.name} | {o.type.name} '\n\t\t\t\t\telse:\n\t\t\t\t\t\treturn f'{o.name} | {type(o).__name__} '\n\t\t\t\telse"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\tfor key in last_applied_manifest:\n\t\tvalue = last_applied_manifest.pop(key)\n\t\t_kubernetes_update_if_missing_manifest_data(\n\t\t\tkey=key,\n\t\t\tobsp,\n\t\t\tvalue=value,\n\t\t\tset_to_null=is_true,\n\t\t\tobserver_schema=observer_schema,\n\t\t)\n\treturn last_applied_manifest\n\n\ndef _kubernetes_update_if_missing_manifest_data(\n\tkey"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tmetadata_sched = []\n\tif not app.kube_resource_names:\n\t\treturn metadata_sched\n\t\n\tresource_types = []\n\tfor resource, resource_type in app.kube_resource_names.items():\n\t\tresource_types.append(\n\t\t\t{\n\t\t\t\t\"type\": resource_type,\n\t\t\t\t\"name\": api.extend(resource, 'api')\n\t\t\t}\n\t\t)\n\tmetadata_sched.extend(resource_types)\n\treturn metadata_sched\n\n\ndef generate_userref_for_user"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\tfor resource in app.resources:\n\t\tresource.validate()\n\t\tif resource.kind == 'Service' and not resource.external_attributes.get('observerSchema'):\n\t\t\tresource.validate()\n\t\t\tlogger.info(f'Generating a default observer schema for resource {resource.name}.')\n\t\t\tresource.publish_schema.observerSchema(resource.external_attributes['observerSchema'])\n"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor resource in app.resources():\n\t\tif resource.type not in schemas and resource.default_observer:\n\t\t\tfor k, m in iteritems(schemas[resource.type]):\n\t\t\t\tschema = generate_default_observer_schema(resource.app)\n\t\t\t\tset_value_or_raise(schema, k, m)\n"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tspec = kubernetes.spec.from_flags()\n\t_observer_schema = spec.manifest['app']['observer']['observer_schema']\n\treturn _observer_schema\n"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor resource in app.spec.resources:\n\t\tif resource.type == \"env\":\n\t\t\tif resource.external_dependencies:\n\t\t\t\tresource.external_dependencies = str(\n\t\t\t\t\t{\"dependencies\": resource.external_dependencies}\n\t\t\t\t)\n\t\t\t\tdel resource.external_dependencies\n\t\t\tresource.observer_schema = generate_default_observer_schema(app)\n\t\t\tresource.observer_schema = None\n\t\t\tdel resource.observer_schema\n\t\telse:\n\t\t\t# if resource.external_dependencies is not None:\n\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t# NOTE: for future versions, the schema returned by this function can be specified explicitly\n\t# under the same name in a kubernetes manifest, for example\n\tfor resource in app.kubernetes.resource_names():\n\t\tresource_config = app.kubernetes.resource_config(resource)\n\t\tif '@namespace.$default' in resource_config.namespaces:\n\t\t\t# We only support the default namespace\n\t\t\tcontinue\n\t\tif '@allowlist' in resource_config.spec_schema:\n\t\t\t# The \"allowed by default\" schema is"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor spec in app.spec.manifest.manifests:\n\t\tif spec.observer[0]:\n\t\t\tbreak\n\telse:\n\t\tlog(\"No observable spec in the specified manifests. Nothing to do.\")\n\t# noinspection PyUnresolvedReferences\n\tapp.observer = kubernetes.readability.Observer(app) \\\n\t\t.observer.generate(spec)"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tconfig = app.config\n\tconfig.generate_default_observer_schema\n\tfor resource in config.spec.manifest.resources:\n\t\tschema = resource_schema(resource, config)\n\t\tapp._schema_cache[resource] = schema\n\t\tapp.on_open(resource, SchemaLoader(schema))\n\n\ndef register_endpoints(app, config):\n\t\"\"\"\n\tAdd each available Endpoint and EndpointInfo to an appropriate\n\tlist in a Kubernetes Deployment or if the user hasn't provided an\n\tEnd"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor resource in app.data.kubernetes.resources():\n\t\tif not getattr(resource, \"observers\", []):\n\t\t\tgenerate_default_observer_schema(app.data)\n\n\t@jsonschema.Schema(app.data.schemas.observers, order=[\"default_observer\"])\n\tdef default_observer_schema():\n\t\t\"\"\"\n\t\tDefault observer schema for the ``observers`` field of the default\n\t\tobserver schemas. It uses the format produced by ``kubernetes."}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tobserver_schema = dict()\n\tfor resource in app.resources:\n\t\tif resource.kind > 0:\n\t\t\tresource_path = '/metadata/%s' % resource.name\n\t\t\tobserver_schema[resource.name] = [{}, {} ]\n\t\t\tfor k in ['generate_observer_schema_for', 'extends_manifest']:\n\t\t\t\tif resource_path in [v.path for v in app.resources.get(resource.kind, [])]:\n\t\t\t\t\tobserver_schema[resource.name][1][k]"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\ncreate_statement_factory = factory.create_statement_factory(create_statement_factory)\n\nSQL.create_statement.__doc__ = create_statement_factory.__doc__\n\nSQL_FACTORY.add_method(\n\t\"create_statement\",\n\tcreate_statement_factory,\n\tdoc=\"\"\"\\\n\t\tCreate a statement. Arguments are the same as :attr:`.create_statement_factory`.\n\t\tReturn value is a :class:`tuple` of (:class:`str` or :"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\n\n@public\nclass Params(SQLValuesMixin, dict):\n\t\"\"\"\n\tA dictionary storing the parameters used to query the database.\n\t\n\tThis constructor is used to construct a :class:`Params` object, which\n\trepresents the parameters used to run the query. A :class:`Params`\n\tobject contains the options for running the queries.\n\t\n\tIt is recommended against using a plain Python dictionary for\n\tconstruction. Example: ::\n\t\n\t\tparams = {\"do\":\"something\", \"option\":\"value\"}\n\t\n\t.."}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\treturn sql, params\n\ndef _parse_sql_style(style, params=None):\n\t\"\"\"\n\tConvert the style, parameters, and query string into an SQL query\n\tobject.\n\t\"\"\"\n\tstyles = {\n\t\t'plain': PlainSQLStyle,\n\t\t'sql': SQLStyle,\n\t\t'in_style': InStyle,\n\t\t'out_style': OutStyle,\n\t}\n\t\n\tstyle = styles[style]\n\n\tif isinstance(style, str):\n\t\tparams = params or dict()"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t  \n\t  from functools import partial\n\t  __trace__ = False\n\t  sqlquery = _quote(sql, _string_converter=_string_converter)\n\t  sqlparams = dict(sqlparams or ())\n\t  \n\t  if params is None:\n\t\tparams = {}\n\t\n\t  def _format_instyle(v: int, fn: callable) -> str:\n\t\treturn fn(v)\n\t  \n\t  def _format_outstyle(v: int, fn: callable) -> str:\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)(sql)\n\treturn sql, params\n\ndef toquery(sql):\n\t\"\"\"Convert the given SQL query statement to a dictionary.\n\t\n\tYou can pass the query in any format as a string.  The query will\n\tbe parsed with :func:`format()` before being submitted.\n\t\n\tExample::\n\t\t\t>>> import sqlparse\n\t\t\t>>> sql = sqlparse.format(u\"insert into table (name) values ('Frank');\")\n\t\t\t>>> query = {\n\t\t\t...\tu\"table\": {\n\t\t\t...\t\tu\"name\": {\n\t\t\t"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\n\n@lru_cache(None)\ndef format_sqls_to_use(params, sqls):\n\t# type: (SQLParams, List[Tuple[str, Union[str, int],...]]) -> Tuple[str, Sequence[str]]\n\tsqls_to_format = [\n\t\ttuple(map(\n\t\t\tlambda p: f\"\\t{p}, \",\n\t\t\t[sql[0] for sql in sqls]\n\t\t))\n\t] + [format(sql[1], \"=?\", params) for sql"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\nPARAMS_UNSPECIFIED = create_code(\"PARAMS_UNSPECIFIED\")\n\nclass SQLParams(object):\n\n\tcode = PARAMS_ONLY_ONLY_DICT_STRING\"{;\n\t}\n\n\tdef __init__(self):\n\t\t#: Reserved for future use.\n\t\tself.in_style = None\n\t\tself.query = None\n\n\nclass SQL(object):\n\n\tcode = POSTFIX_QUERY\n\n\t@classmethod\n\tdef from_string(cls, params):\n\t\treturn cls()\n\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\n\n@lru_cache()\ndef render_format(in_style, vals):\n\t\"\"\"\n\tRender out-style style values as strings.\n\t\n\t:param int in_style: the parameter style for :attr:`.SQLParams.in_style`.\n\t\n\t:param dict vals: a set of values that conforms to the :attr:`.SQLParams.in_style` parameter.\n\t\"\"\"\n\tqs = []\n\tfor key, val in vals.items():\n\t\tif key in QS_PARAMS[in"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\t\n\t# if you need to wrap a string in the params argument this would be a closure\n\tdef wrap(sql, params=None, **kw):\n\t\tsql = sql.format(**kw).replace(params or tuple())\n\t\tsql = _formatter(sql, **params)\n\t\treturn sql\n\t# end wrap method\n\treturn wrap\n\n# end format wrapper instance\n\ndef format_params(sql, params):\n\t'Returns the tuple ``(sql, params)``'\n\tsql = sql.format(**params or {})\n\treturn sql"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t),\n}\n\ndef format_query(\n\tsql: str, params: \"SQLParams\"\n) -> \"str\":\n\t\"\"\"Format the :attr:`SQLParams` style.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of out-style parameters. It maps each parameter\n\t(:class"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t) as a dict:\n\t\tresult = call_subprocess(s, many_params=many_params)\n\t\tsql = result['sql']\n\t\tout_style = result['out_style']\n\n\t\tif out_style == SQL_PARAMS.STRING:\n\t\t\tif many_params is not None:\n\t\t\t\traise InvalidOperation(\n\t\t\t\t\t'Cannot use both the \"string\" style and multiple params.'\n\t\t\t\t)\n\t\t\treturn sql, '', ''\n\t\telse:\n\t\t\tif many_params is None:\n\t\t\t\tresult = {'sql': sql, 'out_"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t)\n)\n\n\nregister(\"sqlparamstyle\", description=format_style_query_param, version=\"0.1\")\n\n\n@register(\"sqlparamstyle\")\nclass SqlParamStyle(object):\n\t\"\"\"\n\tStyle used to format the parameters so they can be inserted as\n\tSQL statements using ``%s``.\n\t\"\"\"\n\n\tparamstyle = \"qmark\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\t#: Dictionary of :class:`collections.abc.Mapping` style parameters. Default is plain ``dict()``"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t)\n\n__all__ = [ \"formatmany\", \"formatparams\", \"runtemplate\" ]\n\n# -----------------------------------------------------------------------------\n\n\n\n\n\n# -----------------------------------------------------------------------------\n\n\n\n\n# -----------------------------------------------------------------------------\n\n\n\n\n# -----------------------------------------------------------------------------\n\n\n\n\n# -----------------------------------------------------------------------------\n\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t))\n\t\n\t@classmethod\n\tdef format(\n\t\tcls,\n\t\tsql: \"Template\",\n\t\tmany_params: \"SQLParams\",\n\t\toutstream: \"TextIO\",\n\t\t) -> \"Template\":\n\t\tself = cls(in_style=SQLParams.in_style)\n\t\tself.outstream = outstream\n\t\treturn super(SQLParams, self).format(sql, many_params)\n\t\n\tdef insert(\n\t\tself,\n\t\tseq: iter[\"Template\"] = ()) -> tuple[\"Template\", list]:\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t)\n\treturn format_template(template, **locals())\n\ndef format_var(param, val, sql_cache_key=None):\n\t\"\"\"\n\tFormat and execute :meth:`~sqlalchemy.sql.expression.column`\n\trepresentations of variable/key attributes.\n\t\n\tArguments are passed as :class:`~collections.abc.Mapping`, whose values\n\tare instances of the standard Python types :class:`~collections.abc.Hashable`.\n\tKeys and values need to be hashable and immutable.\n\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t)\n\treturn (\n\t\tSQLExecutor(SqlStmt.formatmany,\n\t\t     sql,\n\t\t     many_params,\n\t\t     params=params or ())\n\t)\n\n\ndef sql_fetch(*param: Union[str, int, int]) -> Tuple[Union[str, int],...]:\n\t\"\"\"\n\tRetrieve the out-style parameters (:class:`~collections.namedtuple.NamedTuple`)\n\tfor a query.\n\t\n\t*param* (:class:`~collections."}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t)\n\ndef insertonly(\n\t\tsql, values, many_params=None, many=False,\n\t\tallow_replace=False, column_names=None, table_names=None, *args, **kwargs):\n\t\"\"\"\n\tExecute a SQL script which does not require a bound set.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL script.\n\t\n\t*values* (:class:`list` or :class:`dict`) is a sequence of key/value tuples\n\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t):\n\t\tfrom sqlparse import sql\n\t\tfrom sqlparse.tokens import Comment, Name\n\t\tfrom sqlparse.tokens import Punctuation\n\t\tfrom sqlparse.compat import iteritems\n\n\t\tif isinstance(sql, bytes):\n\t\t\tsqlfile = sql\n\t\t\tsql = sql.decode()\n\n\t\ttokens = list(sql.tokens)\n\t\tfor t in tokens:\n\t\t\tif not t.is_whitespace and t.sorta!= Token.DISTINCT:\n\t\t\t\t# we only need one token for a distinct statement"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t),\n\n\tformat_parameters = (\n\t): (\n\t\t\"\"\"\n\t\t\tConvert the :class:`~sqlalchemy.sql.Column` objects as in-style\n\t\t\tparameters for this function.\n\t\t\t\n\t\t\t:param column: The SQL column to convert for each output.\n\t\t\t:param out_style: SQL output style for the output, e.g. ``out_style=\"select\"``\n\t\t\t\n\t\t\tThis is a function that accepts a :class:`~sqlalchemy.sql.Column`\n\t\t\tobject as input and then returns the desired output. The"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t)\n\t\n\tif sql is None:\n\t\treturn []\n\t\n\tif many_params is SQLParams._NOSQL:\n\t\treturn format(sql, many_params)\n\telif many_params is not SQLParams._NOSQL and isinstance(many_params, SQLParams):\n\t\treturn [format(sql, params) for params in many_params]\n\telse:\n\t\treturn [format(cmd, params) for cmd, params in itermap(many_params, itermap_key)]\n\ndef format_param(params"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\tfsname, _ = ntpath.split(path)\n\tif os.path.splitext(fsname)[1] in ['.py', '.pyc']:\n\t    return True\n\telse:\n\t    self.log.warn('Warning: unrecognized file extension: %s' % fsname)\n\t    return False\n\t\n#\n# Class representing a file in a path.\n#\nclass File(object):\n\tpass\n\n# A list of all files in an path.\nALL_FACES = [ ]\n\ndef get"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\tf = path['file']; l = path['link']; fp = f['filename']; ifo = f['ion']\n\t\n\t# check validity of filepath\n\tif_valid = (ifo in molfile.allifos ) or (fp in molfile.allifos ) or (l in molfile.allifos)\n\tif if_valid:\n\t\t\n\t\t# validate file for validation, by checking if it's in the\n\t\t# file's pdb's ifso file\n\t\tif not ifo:\n\t\t\tif"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif self.filepath is None:\n\t\traise HdaPathDoesNotExistError()\n\tif not os.path.isfile(self.filepath):\n\t\traise HdaPathDoesNotExistError()\n\tif os.path.exists(self.filepath) and not filecmp.cmp(self.filepath, self.pyfs_path):\n\t\traise HdaDirectoryDoesNotExistError()\n\treturn True\n\n\tif os.path.isfile(self.filepath) and os.path.isdir(self.pyfs_path):"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path, 'rb') as f:\n\t\t\tcontent = f.read()\n\t\tcontent_type = ContentType(bytes(content))\n\t\tcontent_len = len(content)\n\texcept OSError:\n\t\treturn False\n\t\n\tif content_type!= ContentType.OCFL_SZK_STREAM:\n\t\treturn False\n\n\t# check file content\n\tif content in [os.linesep, '\\n']:\n\t\treturn False\n\tif content[:1] in [b'\\r'] or content["}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn True\n\t\n\t\nclass DLL(dict):\n\t\"\"\"DLL class.\n\t\n\tCreate a dictionary subclass of an existing object's dict subtype,\n\twith attributes added to it.\n\t\"\"\"\n\t\n\t\n\tdef __getattr__(self,name):\n\t\t\"\"\"Overread lookup of attribute name.\n\t\t\n\t\tDetermines the value of name.\n\t\t\n\t\tReturns the value of the name property, or the default attribute if\n\t\tthe dictionary doesn't exist at instance.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self[name]\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tpy = path2py(os.path.join(path, path))\n\tif len(py) > 1:                                # if pyfs/py3 files\n\t\tvalid, msg = self._check_file_path(py)     # validate path\n\telse:\n\t\tvalid, msg = self._check_file(py, validate=True)   # validate py fs\n\tif not (valid):                                   # if a validation check fails\n\t\t_LOGGER.critical(msg)\n\t\treturn False\n\treturn True\n\n\t\ndef validate1(path):\n\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn True\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t# Return False if this is a directory, otherwise return True.\n\t\t\n\t\t## This should be implemented as a recursive function, not a generator. -K\n\t\t## The first time the generator is executed in the for loop, get_children_dir() will give us a generator that yields a path\n\t\tif isinstance(self,pyfs.Root):\n\t\t\t# TODO: This is a temporary hack, probably can be removed and something would be simpler.\n\t\t\treturn self.get_children_dir()\n\t\t## Remove the empty folder as a temporary workaround.\n\t\t## This check is needed"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif not os.path.exists(path):\n\t\treturn False\n\tif not os.path.isdir(path):\n\t\treturn False\n\tif not os.path.isfile(path):\n\t\tif (path[:4] == '\\\\\\\\' and path[6:9] == '\\\\') or os.path.isdir(path):\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\t\n\ttry:\n\t\tpath = file_module.get_pyfs_path(path)\n\texcept UnicodeDecodeError:\n\t\treturn False\n\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tpyfs.fileop.check_directory(path, recurse = True)\n\texcept (KeyError,OSError):\n\t\tok = 0\n\telse:\n\t\tok = pyfs.fileop.validate(path)\n\treturn ok\n\nclass Open(object):\n\t\"\"\"\n\t\tClass for opening an object\n\t\"\"\"\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\t\tClass to open a given object in pyfs.\n\t\t\t\n\t\t\t'name' should be given as a unicode string.\n\t\t\tIt should never have spaces"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tp = prefix.upper()\n\treturn p + (_STATUS_TO_VALIDATOR_MAP.get(self._validator, p))\n\t\ndef _set_validator_status(self, value):\n\t# validate in the internal state if set\n\tself._validation_state = (not value)\n\tif self._validation_state:\n\t\tfor n in self.__members_validated__:\n\t\t\tv = getattr(self, n)\n\t\t\tif v:\n\t\t\t\t# value is assigned, so try to set\n\t\t\t\tsetattr(self"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tvalid_status_map = {\n\t\t# validator status code  -> validator status code string, prefix string\n\t\t'1': 'pass',\n\t\t'2': 'fail',\n\t\t'3': 'error',\n\t\t'3.1': 'error',\n\t\t'3.2': 'warn',\n\t\t'4': 'error',\n\t\t'5': 'error'\n\t}\n\t\n\tstatus = self.get_status_str()\n\t\n\tstatus_str = status\n\t\n\tprefix_ = prefix\n\t\n\tfor validator in self."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tv0_p0 = 'v{:,g}{}e{:.2%}:'.format(self.numinator, prefix, self.denominator)\n\tv0_df = (self.numerator - self.denominator) / self.avg_den\n\treturn_str = v0_p0 + '{:.2%}'.format(v0_df) if self.numerator == 0 else '{:.2%}'.format(100 * v0_df)\n\treturn return_str\n\n\t\ndef get"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tval = ''\n\tstatus = self._validators_status\n\ttry:\n\t\tval = self.status_val\n\texcept Exception:\n\t\tpass\n\tif '*' in status:\n\t\tstatus = '*'\n\tstatus = self.__check_status_attr(status.replace('!', '!').replace('?', '?'))\n\tif status == self.NOT_VALIDATED:\n\t\tval = \": OK\"\n\telif status == self.VALIDATED:\n\t\tval = \"{} : OK\".format(prefix)\n\telse"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstr_ = ''\n\tif not self.valid:\n\t\tstr_ = \"%s is not valid\"\n\t\tstr_ = str_ % prefix\n\treturn str_\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn self._status[prefix]\n\n\nclass GroupValidator(CommonValidator):\n\t\"\"\"\n\tCheck if a set of field values are valid.\n\t\"\"\"\n\t\n\tdef __init__(self, context, model, field_def):\n\t\tself._context = context\n\t\tself._model = model\n\n\t\tself.name = QVariant(field_def['name'])\n\t\tself.default = QVariant(field_def['default'])\n\n\t\tself.title = QVariant(field_def['title'])\n\t\tself.required = QVari"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self._validator:\n\t\treturn prefix + 'validator:' + self._validator.status\n\treturn prefix + 'invalidation:' + self._invalidation\n\n\nclass InvalidationState(object):\n\t\"\"\"\n\tInternal data structure for the invalidation state of a\n\tvalidator instance.\n\n\tThe 'validator.invalidation' data structure stores the validator\n\tinstance's actual invalidation state and the state of the current\n\tvalidator instance in memory.\n\n\tAttributes:\n\t\tstate: a dict containing some useful data\n\t\t\tthe status of the current"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstatus = self.status(prefix=prefix)\n\toutput = {}\n\toutput['prefix']\t= prefix\n\toutput['status'] = status\n\treturn output\n\nclass RegistryManager():\n\t\"\"\"\n\t\tManagement and database access for registry configuration.\n\t\"\"\"\n\n\tregistry_file   = 'config.json'\n\tregistry_name  = 'default'\n\tregistry_host   = 'localhost'\n\tregistry_database   ='mysql'\n\tregistry_password = None\n\n\tregistry = None\n\tregistry_connection   = None"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn '%s%s %s' % (prefix, \" [OK]\".center(16, '*') if self.is_ok() else \"\", \" \u2705\".center(16, '*'))\n\n\t\"\"\"\n\t\tReturn validator status, with optional prefix\n\t\tfor use in the validation class methods\n\t\"\"\"\n\tvalidate_status_str = self.status_str\n\treturn validate_status_str(self, prefix)\n\n\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\ttry:\n\t\tstatus = self.__validators[self.__status]\n\texcept KeyError:\n\t\treturn \"Unknown\"\n\telse:\n\t\tif self.__status in ['unm', 'inprogress', 'incomplete']:\n\t\t\tfmt_status = self.__short_statuses.get(self.__status, '')\n\t\t\tstatus_str = fmt_status\n\t\telse:\n\t\t\tstatus_str = 'Invalid'\n\t\tif prefix:\n\t\t\tstatus_str = prefix + status_str\n\t\treturn status_str\n\n# END Status\n"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn '<%sp%s (%s)>' % (prefix, self.__classname__, util.unique_id())\n\nvalidator_regexp = re.compile('^\\s*$', re.MULTILINE)\n\ndef _str(x, enc='utf-8', errors='strict'):\n\t\"\"\"Returns string representation of x\"\"\"\n\tx = str(x)\n\tif enc:\n\t\tx = x.encode(enc)\n\treturn validator_regexp.sub(r'<VALIDATION FA"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif self.validation_started:\n\t\treturn prefix + 'Validation started for %s.' % self.name\n\n\telif self.validation_failed:\n\t\treturn prefix + \\\n\t\t\t'Validation failed for %s: %s, message: %s;'\\\n\t\t\t'original value: %r.' % \\\n\t\t\t(self.name, self.validation_failed, self.message, self.original)\n\n\telif self.validation_failed_invalid:\n\t\treturn prefix + \\\n\t\t\t'Validation failed for %s: %s,"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn '{}log'.format(prefix)"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\twith open(settings_dict['DEFAULT_LOG_NAME'], 'r') as l:\n\t\tl_string = '\\n'.join(l.readlines()[-3:])\n\n\tp = prefix + '--- Status: {%s}\\n' % self.__class__.__name__\n\tv = 'Validation is still being done....' if self.status.validate else 'Validation is done...'\n\tp += v + '\\n\\n---'if self.status.validate and not self.status.test else ''\n\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tout = ''\n\tfor line in re.split(',\\s*([^\\n]+)', self.get_error_msg()):\n\t\tif line:\n\t\t\tout += prefix + line + '\\n'\n\treturn out\n\n\t\n\n\t\n\n\t\n\n\n\t\n\t\n###\n### NOTES:\n### - the error_msg attribute is already constructed. the validation is based solely on the\n###   the error log, so this should be as if it came from error_log\n### - there is no need to validate it with anything other than a dict\n### -"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tstatus_str = '%s validation: %s' % (prefix,''.join(self.validation_log))\n\treturn status_str\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn '\\n'.join(['{prefix}{id}:{value}'.format(\n\t\tprefix=prefix,\n\t\tid=id,\n\t)  for id, value in self.check_log.items()])\n\ndef validate_pydantic(self, *args, **kwargs):\n\tlogs = [\n\t\t('Checking required field types', self.check_required_type()),\n\t\t('Checking optional fields', dict(**kwargs, **self.fields)),\n\t\t('Checking additional required fields', []),\n\t\t('Checking additional optional"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\ts = self.status_template.replace('%(prefix)s', prefix).replace('&', '&amp;')\n\tif self.user:\n\t\ts = s.replace('#user#', self.user)\n\tif set(self.user_template.split('&')) & set(self.password):\n\t\ts +='[Required]'\n\treturn s\n\t\nclass InputValidation(object):\n\t\"\"\"Validate input values.  Accepts two attributes:\n\t\t- `user` - user name\n\t\t- `password` - password (if"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif self._status:\n\t\tline = '%s%s%s,'% (self.status, prefix, self.__class__.__name__)\n\telse:\n\t\tline = ''\n\tline +=''.join(str(x) for x in self.lines) if len(\n\t\t\tself.lines) > 0 else ''\n\tline += '%d lines' % len(self) if len(self) > 1 else ''\n\treturn line\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not self.status:\n\t\treturn prefix+'UNKNOWN'\n\telse:\n\t\treturn prefix+' '+self.get_status_str(self.status)\n\n\t\nclass Log(LogManager):\n\t\"\"\"Wrapper for a file to log messages\"\"\"\n\tdef __init__(self, file, mode='a', prefix='%(asctime)s', level='DEBUG'):\n\t\tsuper().__init__(file, mode=mode, prefix=prefix, level=level)\n\t\tself.record_handler = LogRecordHandler(file, level,"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass EmptyTypeDict:\n\t\"\"\"Immutable container with a dictionary of type descriptions.\"\"\"\n\n\tdef __init__(self, type_dict):\n\t\tself.__type_dict = type_dict\n\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, self.__class__):\n\t\t\treturn False\n\t\treturn other.__dict__ == self.__type_dict\n\n\tdef __repr__(self):\n\t\treturn self.__type_dict.__repr__()\n\n\tdef __len__(self):"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass IModel(metaclass=abc.ABCMeta):\n\t__slots__ = ()\n\tvalidation = {\n\t\t\"type\": valid_type,\n\t\t\"is_valid\": is_valid,\n\t\t\"__str__\": str,\n\t}\n\n\tdef __init__(self) -> None:\n\t\tpass\n\n\tdef __set_name(self, name):\n\t\tself.__name = name\n\n\tdef __get_name(self):\n\t\tself.__name = self.__name or self.__class.__name__"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\nclass InvalidIdentifierException(Exception):\n\t\"\"\"\n\tClass to raise when an identifier is not valid.\n\t\"\"\"\n\tpass\n\nclass InvalidCovNamingException(Exception):\n\t\"\"\"\n\tClass to raise when an identifier is not valid.\n\t\"\"\"\n\tpass\n\nclass InvalidKeyException(Exception):\n\t\"\"\"\n\tClass to raise when an identifier is not valid.\n\t\"\"\"\n\tpass\n\nclass InvalidLocationException(Exception):\n\t\"\"\"\n\tClass to raise when an identifier is not valid.\n\t\"\"\"\n\tpass\n\n\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\ndef is_nonempty_text(identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tReturn True if identifier is nonempty, always True in this base implementation.\n\t\"\"\"\n\treturn not identifier == \"\"\n\n\nclass Lexer(object):\n\t\"\"\"\n\tHandles parsing tokens and creating tokens objects.\n\n\t.. note::\n\t\tThe tokens object can be used either to parse either tokens or rules of\n\t\tlexical grammar or an LALR rules of syntax.\n\n\t\"\"\"\n\n\tdef __init__(self, source,"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef is_valid_uuid(identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef is_valid_ip(identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\nclass UUPSocketAddress(str, Enum):\n\t\"\"\"\n        Identifier for the socket address (address in our case).\n\t\"\"\"\n\t# pylint: disable=line-too-long\n\t\"\"\"\n        I define both base class's with_address defined as an attribute\n        for base super class and thus the same interface.\n        \"\"\"\n\t# pylint: enable=line-too-long\n\t\"\"\"\n        In the future we can use different naming convention for the address.\n        For now we will just accept the name of the socket"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef get_param_name(obj, name):\n\t\"\"\"\n\tReturns the 'param' name of the obj given as keyword argument.\n\t\"\"\"\n\n\tif isinstance(obj, dict):\n\t\treturn \"\".join([k + \".{}\".format(name) for k, _ in obj.items() if k.endswith(\".{}\".format(name))])\n\treturn None\n\n\ndef has_param(obj, name):\n\t\"\"\"\n\tReturns True if obj has the `param` name in the given argument.\n\t\"\"\"\n\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n#------------------------------------------------------------------------------------------------\n#    Logging and logging configuration, here we will write an experiment log file by\n#    creating a log of each task instance run.\n#------------------------------------------------------------------------------------------------\n\n# Create a logging object of the module'sphinolog.logger'.\n\n# Create two log objects of the module'sphinolog.debugger.Debugger' in one.\n# These two objects can be swapped between different modules of this software.\n# And in other words, their class name can be changed by changing their constructor argument\n# after creation.\n# A good example"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t# Should this method ever be called here?\n\traise NotImplementedError\n\n\nclass ValidIdentifier(object):\n\t\"\"\"\n\tValid identifier type.\n\t\"\"\"\n\t# An unique identifier consists of an identifier which is composed\n\t# of\n\t#   at least a lowercase character, or\n\t#   a variable number of digits [a-z0-9]\n\t# and character characters, or\n\t# optionally with a colon (:) followed by optional spaces.\n\n\t# The character is considered the separator between identifiers.\n\t# The separator will be"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tinventory_name = inventory['name']\n\ttry:\n\t\tspec = self.spec\n\texcept AttributeError:\n\t\tinventory_spec = inventory['spec']['version']\n\t\tif (spec is None or inventory_spec!= spec):\n\t\t\traise self.ValidationError('Version of the inventory (%s) is: %s, but is required.' % (inventory_spec, inventory_name))\n\telse:\n\t\t# If we got a type, then try to validate against the type value\n\t\tif spec is not None:\n\t\t\tif ('type' in spec):\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tspec_version = None\n\tif inventory.get('type') is not None:\n\t\tname = string.strip(inventory['type'])\n\t\tif name == 'Host':\n\t\t\tspec_version = self.version.get('spec_h_version', None)\n\t\t\ttype_version = self.spec_version.get(name, None)\n\t\t\tif type_version is not None:\n\t\t\t\tif spec_version is None:\n\t\t\t\t\tspec_version = self.spec_version['host']\n\t\t\t\telse:\n\t\t\t\t\tspec_version = str(spec_version)\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tspec = inventory._inventory._items[self.name]\n\tif (inventory._inventory._current_spec._type == inventory._inventory._SPEC\n\t\tor inventory.is_version(inventory._inventory._current_spec)\n\t\tand getattr(spec,'spec_version', True):\n\t\t\tspec.validate()\n\t\t\traise SkipTest('spec.validate() skipped due to version_supported')\n\tif not inventory._inventory._current_spec.get('validate', True) and \\\n\t\t\tinventory.is_valid(inventory._inventory._current_spec):\n\t\tspec"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tspec_version = self.spec_version\n\tif not spec_version:\n\t\ttry:\n\t\t\t# Attempt an type\n\t\t\tif self.spec_type is None:\n\t\t\t\ttype_val = next(iter(self.inventory[inventory].keys()))\n\t\t\t\tspec_version_str = \"%s version %s\" % \\\n\t\t\t\t\t\t\t\t\t(type_val, str(int(getattr(self, '%sVersion' % type_val))))\n\t\t\telse:\n\t\t\t\tspec_version_str = self.spec_type\n\t\t\tspec_version = self.inventory.get("}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t# First, find the specified version\n\tversion_index = None\n\tmatch = None\n\tfor index in range(len(self.spec_version)):\n\t\tif self.spec_version[index] == 0:\n\t\t\tversion_index = index\n\t\t\t\t\n\t\telif index >= 2: # We have multiple versions\n\t\t\tmatch = index\n\t\t\tbreak\n\t\t\n\tif version_index is not None and match is not None:\n\t\tversion = self.spec_version[version_index]\n\t\t\n\telif extract_spec_version:\n\t\tversion ="}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t# Make a backup before doing anything else\n\ttry:\n\t\torig_inventory = copy.deepcopy(inventory)\n\texcept AttributeError:\n\t\tpass\n\t\n\t# Remove the object from the inventory, from the list or string\n\tfor obj in tuple(inventory):\n\t\tif isinstance(obj, dict):\n\t\t\tinventory.remove(obj)\n\t\telif isinstance(obj, str):\n\t\t\tinventory.remove(obj)\n\t\n\t# Verify the spec_version is sane\n\ttry:\n\t\tversion = tuple(self.spec.inventory."}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tinventory_schema = inventory.schema.replace(\".\", \"_\")\n\t\n\t# Get current inventory schema\n\tself._schema[\"validation\"][\"properties\"][inventory_schema] = {\"type\": \"string\"}\n\ttry:\n\t\tschema_version = parse_version(inventory.spec_version)\n\texcept ValueError:\n\t\t# This is not a valid schema version.\n\t\tself.log_error(f\"Invalid inventory specification version: '{inventory.spec_version}'.\")\n\t\treturn\n\t# Convert schema version into a string.\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tvalidation_errors = []\n\tvalidation_errors += self.check_inventory_type_definition(inventory)\n\t\n\tif \"VersionType\" not in inventory:\n\t\tvalidation_errors.append(\"No version type defined in inventory\")\n\t\tinventory[\"VersionType\"] = \"Default\"\n\n\tspec_version = self.spec_version\n\tif inventory[\"VersionType\"] == \"v1\":\n\t\tif \"type\" not in inventory:\n\t\t\tvalidation_errors.append(\"Missing valid version type\")\n\t\t\tinventory[\"type\"] = \"Default\"\n\t\n\t\tif"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tfor name in self.spec.keys():\n\t\t# Test the type value\n\t\ttype_ = self.spec[name]\n\t\tif type_ is None:\n\t\t\tif not extract_spec_version:\n\t\t\t\ttype_ = self.params[name][\"type\"][0]\n\t\telse:\n\t\t\tif not extract_spec_version:\n\t\t\t\t# Test the value\n\t\t\t\tvalue = self.spec[name][\"value\"]\n\t\t\t\tvalid = self.param_validate(name, value)\n\t\t\t\tif not valid:\n\t\t\t\t\t# Try matching the spec value\n\t\t\t\t\ttype_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\tif self.spec_version == 'latest':\n\t\t# validate the specification of the inventory file.\n\t\tif extract_spec_version == False:\n\t\t\tself.validate_file()\n\t\telse:\n\t\t\tif validate_specification(inventory) == 'FALSE':\n\t\t\t\tvalidation_file(inventory)\n\t\t\t\texit_message = \"Unsufficient inventory specification. Failed to validate.\"\n\t\t\t\tfailure(exit_message)\n\t\t\telse:\n\t\t\t\texit_message = (\"Inventory file is invalid. Unable to validate inventory.\"\n\t\t\t\t\t\t\t\t+ \" This value was generated by a"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest in manifest_files:\n\t\tmanifest.update_digests(digests_used)\n\t\tdigests_used = manifest.check_digests_present_and_used(self.digests_dict, digests_used)\n\t\tself.digests_used.update('%s%s' % (self.key_prefix, manifest['digest'].rpartition('/')[2]))\n\t\tself.digests_used.update(manifest.get('digests', []))\n\treturn digests_used\n\n\n# def check_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tif not os.path.exists(manifest_file):\n\t\t\treturn False\n\t\tmd5 = self.get_digest(manifest_file)\n\t\tif not self.check_digests_present(manifest_file, md5, digests_used):\n\t\t\treturn False\n\treturn True\n\n\n\t\ndef setup_sign(sign):\n\t\"\"\"\n\tReturn new signature for use by user on \"sign\" package\n\t\"\"\"\n\tprint (\"creating signature...\")\n\tsign += '\\n'"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor file in manifest_files:\n\t\tfile_path, file_hash = self._expand_path(file)\n\t\ttry:\n\t\t\tif not file_path.is_dir() or not digest_used:\n\t\t\t\tif file_hash!='sha0':\n\t\t\t\t\tprint (Fore.RESET + file_path, \"<-- SHA is\", file_hash, \"<--\")\n\t\t\t\tcontinue\n\n\t\t\twith open(file_path + \".sha1\", \"w\") as out:\n\t\t\t\tprint (\"File:\", file, \"; Hash:\", file_hash, \"<--\")\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor file_name, manifest in manifest_files.items():\n\t\t# File is currently being checked out.\n\t\tfile_repo = os.path.join(local_settings.REPOS_PATH, file_name)\n\t\t# Get manifest digest (first occurrence of file).\n\t\tdigest = manifest_files[file_name][0].replace(\".hg/\", \"\").replace(\".hg\",\"\")\n\t\td = file_repo if digests_used[file_name] else None\n\t\t# Check if we have any of it or more we have another file"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n\tlog.debug(\"Checking: %s\", len(self.manifest.files) )\n\t\n\tfor file, manifest_file in self.manifest.files.items():\n\t\tif manifest_file.exists():\n\t\t\tif manifest_file.is_digest():\n\t\t\t\tlog.debug(\"checking: %s -> %s\", manifest_file, file)\n\t\t\t\tvalid = manifest_file.validate_digest()\n\t\t\t\tif valid:\n\t\t\t\t\tlog.debug(\"manifest file %s is digest: %s\", file, manifest_file.get_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor mf in manifest_files:\n\t\tfor aa, bb in digest_info(mf.digest_path):\n\t\t\tdigest_used = hash_file(aaaa=aa, bbb=bb, force=True)\n\t\t\tif digest_used in [h for h, _ in digests_used.values()]:\n\t\t\t\tcontinue\n\t\t\tdigests_used[digest_used] = (aa, bb)\n\t\t\tlogger.info(\"Using %s for '%s'\", digest_used, bytes(a, encoding='utf-8'))"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tif not manifest_file.startswith('/opt/puppet/') or '~' in manifest_file:\n\t\t\t# skip this manifest file since it probably does not need to be verified\n\t\t\tcontinue\n\n\t\tif manifest_file not in digests_used:\n\t\t\tself.report('WARNING: manifest: manifest no longer needed.', 'W')\n\t\t\tdeleted = 1\n\t\t\tbreak\n\n\treturn deleted\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tfor checksum_file in self._manifests[manifest_file] + self._manifest_path_checksums[manifest_file]:\n\t\t\t# skip already used manifest file\n\t\t\tif os.path.exists(os.path.join(self.temp_dir, checksum_file)):\n\t\t\t\tmissing_checksum_paths = self._check_checksums_present_and_used(manifest_file, checksums_used)\n\t\t\t\tdigests_used.update(missing_checksum_path"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tprint(\"Checking {}...\".format(self.dirpath / manifest_file))\n\t\tcheck_digests_present(\n\t\t\tdirectory_path=self.dirpath / manifest_file,\n\t\t\tdigests_used=digests_used,\n\t\t\texpected_result=self.expected_result.digests,\n\t\t)\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tpresent_manifest_files = []\n\tnot_present_manifest_files = []\n\tfor f in manifest_files:\n\t\tif not os.path.isfile(f):\n\t\t\tcontinue\n\n\t\tmd5 = md5utils.file_get_md5_md5(f)\n\t\tf_size = os.path.getsize(f)\n\t\tif not md5:\n\t\t\tcontinue\n\n\t\tif (md5!= manifest_files[0]):\n\t\t\tself.logger.error(\"Mismatching version number for %s found.\" % f)\n"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\timport collections\n\tself = prior.inject()\n\tprior = prior.inject()\n\t\n\tif not isinstance(self, InventoryValidator):\n\t\traise TypeError(\"Expected object of type: %s, found: %s\" %\n\t\t\t\t(InventoryValidator, type(self)))\n\t\t\n\tif not isinstance(prior, InventoryValidator):\n\t\traise TypeError(\"Expected object of type: %s, found: %s\" %\n\t\t\t\t(InventoryValidator, type(prior)))\n\t\t\t\t\t\n\t\t\n\tif not is"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not isinstance(prior, InventoryValidator):\n\t\traise ValueError(\n\t\t\t(\n\t\t\t\t'This function should return an InventoryValidator, got %s'\n\t\t\t\t% type(prior)\n\t\t\t)\n\t\t)\n\t\n\tif not self._meta.as_prior_version(prior.version):\n\t\traise ValueError(\n\t\t\t(\n\t\t\t\t'This inventory version must be as_prior_version=True in order to have a '\n\t\t\t\t'consistent prior.'\n\t\t\t)\n\t\t)\n  \n  \nclass Inventory(object):\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\treturn prior.validate() and (self == prior)\n\n\n#----------------------------------------------------------------------------\n#\n# The following is a function used to perform basic validity checks to ensure all\n# defined fields have the expected field types and value.\n#\n# To be fully generic, there is separate schema-to-field validation and an\n# InventoryValidator object.  The main public entry for this is __call__ which is\n# bound by the form control method.\n#\n\ndef field_type(field, type_name):\n\t\"\"\"\n\tMethod which validates a field's type.  Must be"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tassert type(prior) is InventoryValidator, \\\n\t\tprint(f\"'{type(prior)}' must be an 'InventoryValidator object\"\n\t\t\tf\"' \"\n\t\t\tf\"instead of {type(prior)} '{prior}'.\")\n\t\n\tif type(prior) is InventoryValidator:\n\t\t# self inventory must have been already checked for internal consistency\n\t\tif posterior := prior.as_prior_inventory():\n\t\t\tassert len(prior.values) == len(posterior)\n\t\t\t\n\t\t\t# check for each"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t# if no user has supplied the input, just return no\n\tif not self._inherit_enabled_default:\n\t\treturn prior\n\tvalidated = False\n\t\t\n\t# check if a user configured the inherit flag, and if so\n\t# configure it ourselves for comparison to prior in the event\n\t# that we're going to be using this property\n\tif not self.user_configured_inherit():\n\t\tuser_inventory.log(f\"Using the current instance's inheritable flag as a validating prior version ({prior.name}) for the {prior"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tassert prior.get_version() is not None\n\tassert self.get_version() >= prior.get_version()\n\tassert self.get_version() >= prior.get_version()\n\n\n@patch('pint.quantities.Quantity.get')\ndef test_default_quoting(mock_get, inventory_obj):\n\t\"\"\"\n\tTest for method that returns default quoting.\n\t\"\"\"\n\tinventory_obj.quoting = default_quoting\n\tinventory_obj.set_version(default_version)\n\tinventory_obj._get_inventory()"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\ttry:\n\t\tvalid = InventoryValidator.isValid(prior.name, prior.inventory)\n\texcept InventoryException as exception:\n\t\tlogger.exception(__name__, exception)\n\t\traise AnsibleError(\n\t\t\t'The inventory object `%s` has a prior version that is not within the scope of the current inventory object for \"inventory.validate_as_prior_version()\".' %\n\t\t\tprior)\n\texcept ValueError as exception:\n\t\t# We can't tell what this exception was, so just raise it as a warning\n\t\tlog"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tinv = prior.inventory\n\tprior_name = prior.name\n\tinv_name = inv.name\n\t\n\t# no need to check version since prior_name is not the same as inventory_name\n\t\n\t# TODO: Add a warning if the new version is different of the old version or\n\t#   this is not a valid prior/inventory.py prior file itself.\n\t\n\t# TODO: Add a warning if the prior was created at or after a bad date.  \n    \n\t# TODO: Add option to only use name and host info."}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif isinstance(prior, str) or hasattr(prior, 'is_valid'):\n\t\ttry:\n\t\t\tself.inventory.validate()\n\t\texcept AttributeError:\n\t\t\tmsg = self.msg % { 'name' : self.inventory.name }\n\t\t\tself.inventory.check_errors(msg)\n\n\t\ttry:\n\t\t\tprior.validate()\n\t\texcept AttributeError:\n\t\t\tmsg = prior.msg % { 'name' : prior.name }\n\t\t\tprior.check_errors(msg)\n\n\n###############################################################################\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tfor r in prior.requests:\n\t\tif r.version not in self.supported_versions:\n\t\t\ttry:\n\t\t\t\treturn False, f'This version of {r.id} is not supported.'\n\t\t\texcept Exception:\n\t\t\t\tself.version = r.version\n\t\t\t\t# Don't do the validate, we want more informative errors.\n\t\t\t\treturn False, f'This version of {r.id} is not supported.'\n\n\t# If version checks were successful, then everything is ok.\n\treturn True, 'All checks passed'\n\nclass InventoryValidator(object):\n\t"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tassert version is not None\n\tlogical_path_map = {}\n\tfor logical_path in inventory.logical_paths:\n\t\tif logical_path in inventory.config['logical_paths'][version]['files']:\n\t\t\tlogical_path_map[logical_path] = inventory.config['logical_paths'].get_version(version)['files']\n\treturn logical_path_map\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\tlogical_path_map = {}\n\n\t# We first get a snapshot of the inventory for the given version.\n\tinfo = get_inventory_info(inventory)\n\t\n\t# We first find the correct logical_path_map for the given version.\n\t# See: http://groups.google.com/group/bccdbzws-wiki/msg/r-api/?cid=106182801\n\tversion_logical_path_map = get_version_content_paths_files(inventory, version)\n\n\t# Finally we look for the"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t# start with a dict\n\tif version == INVENTORY_V1:\n\t\tlogical_path_map = {}\n\t\n\t# if it's a version 1 file\n\telif (version == INVENTORY_V1_STATES and inventory.version == 3) or \\\n\t\t (version == INVENTORY_V1 and inventory.version == 1):\n\t\t\n\t\tif inventory.version == 1:\n\t\t\t# if we've seen these, we must be new\n\t\t\thas_new = True\n\t\t\tfor file in inventory.files:\n\t\t\t\tif not"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\treturn get_logical_path_map_from_inventory(inventory, version, state='invalid')\n\ndef get_logical_path_map_from_inventory(inventory, version, state):\n\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\"\"\"\n\tresult = {}\n\tversion_content = []\n\tfor logical_path in inventory.get(version, []):\n\t\ttry:\n\t\t\tversion_content.append(get_logical_path_content(inventory.get_file(logical_path"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogp_in_state = {}\n\t\n\tfor version in inventory.versions:\n\t\t# This is what a dict looks like:\n\t\tlogical_path_map = {}\n\t\t\n\t\t# Each entry in the map is the one in the right state,\n\t\t# and it has a 'logical_path' that defines the file that\n\t\t# was created. We could also have a 'file_name' that does\n\t\t# the same... but the logic will not be consistent.\n\t\tfor state in inventory.states:\n\t\t\tif state.name!= 'legacy':"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_map = {}\n\tlogical_path = ''\n\tif version == inventory.latest_version:\n\t\tlogical_map = {inventory: set()}\n\telse:\n\t\tlogical_path = inventory.latest_version.path\n\t\tlogical_path_directory = os.path.dirname(logical_path)\n\t\tif logical_path_directory:\n\t\t\tfor root, dirs, files in os.walk(logical_path_directory):\n\t\t\t\tfor directory in [f for f in dirs if not f[0] == '.'"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = {}\n\troot_directory = os.path.join(inventory.location, inventory.version)\n\t\n\tfiles = []\n\ttry:\n\t\tfiles += os.listdir(root_directory)\n\texcept OSError:\n\t\t# Probably some directory not in root.\n\t\treturn {}\n\t\n\tfor fname in files:\n\t\t\n\t\tstate = \"unlocked\"\n\t\t\n\t\tpath = os.path.join(inventory.location, inventory.version, fname)\n\t\t\t\n\t\t# Not a symbolic link: use"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tassert isinstance(inventory, Inventory), \"input is %s\" %(type(inventory),)\t\n\tassert isinstance(version, Version), \"version is %s\" %(type(version),)\n\n\t#\n\t# 1. build a map\n\t# \n\t# get a list of all files that have already been committed by this version.\n\t#\n\t# If a file is already in state_files, then add it to state_files_already_committed, and\n\t# not to state_files_not_committed.\n\t#\n\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = {}\n\tinventory.logical_path_map = logical_path_map\n\tversion_in_inventory = inventory.get_files_dict(version)\n\tif version_in_inventory:\n\t\tlogical_path_map = inventory.get_logical_path_map(version_in_inventory)\n\treturn logical_path_map\n# vim: set ts=4 sw=4 sts=4 et tw=78 et ai ts=4 fdm=y st=4:\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tfrom glob import glob\n\n\tlogical_path_map = {}\n\tfor state in inventory.states:\n\t\tlogical_path_in_state = set(glob(\"./{}/logical_paths/*{}.{}*\".format(state['path'], version, state['version'])))\n\t\tlogical_path_map[state['id']] = logical_path_in_state\n\treturn logical_path_map\n\ndef set_logical_path_map(inventory, version, logical_path_map=None, force=False):"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tif fixity!= 'none':\n\t\tif 'fixity' in fixity:\n\t\t\t# fixity is a single value, set it in the manifest.\n\t\t\tmanifest_files[fixity].set_value('fixity', fixity)\n\t\t\tlogger.info('Fixity in manifest {!r}, set to value \"fixity=none\"'.format(fixity))\n\t\t\tfixity = fixity\n\t\t\tfixity = None\n\t\telse:\n\t\t\t# check if it was a path\n\t\t\tif '.fixid=path:'in fixity:\n\t\t\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t_validate_files_in_inventory(\n\t\t[fixity], self.config.inventory_path,\n\t\tself.config.exclude_manifest, self.config.exclude_files\n\t)\n\tself._validate_files_in_project(self.config.project_path, manifest_files)\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tvalid_files = set()\n\twith_files = []\n\t\n\tprint(\"Validating fixity...\")\n\tfor match in re.search(f\"( |<|\\>|>\\|<|\\| )([^\\n]+)\", fixity):\n\t\tname = match.group(1).lower()\n\t\tfor i, manifest_file in enumerate(manifest_files):\n\t\t\tif name in manifest_file:\n\t\t\t\twith_files.append(i)\n\t\n\tif len(manifest_files)!= len(set(with_files"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t# Validate the blocks.\n\tfiles = [f for f in manifest_files if (not f)]\n\tif not files:\n\t\t# If no manifest files are defined, do nothing.\n\t\treturn 0\n    \n\tcurrent_path = os.getcwd()\n\tnew_path = os.path.join(current_path, 'build', 'inventory')\n\tif str(fixity) == \"0\":\n\t\t# No parent-directories.\n\t\treturn 0\n\n\tparsed_d = get_build_dir(fixity, manifest_files)"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tlog.debug(\"Checking file {}...\".format(manifest_file))\n\n\t\twith open(manifest_file, \"r\") as manifest:\n\t\t\tmanifest_content = manifest.read()\n\n\t\tfile_refs = []\n\t\tfor manifest_file_line in manifest_content.split(\"\\n\"):\n\t\t\tfile_refs.append(int(manifest_file_line))\n\n\t\tlog.debug(\"file_refs:\", file_refs)\n\t\tif len(file_refs) =="}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tif \"fixed\" in fixity:\n\t\treturn fixity\n\telif not manifest_files:\n\t\treturn fixity\n\t\n\tresult = check_fixed_in_file(fixity, manifest_files)\n\tif not result:\n\t\tmessage = \"Could not find file: %s\" % manifest_files[fixity]\n\t\treturn message\n\t\n\treturn result\n\t\n\t\n\t\ndef get_fixities(inventory, check_fixities = False):\n\t\"\"\"\n\tReturns a dict with valid fixities as items.\n\t\"\"\"\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t# If there are multiple files, split out all in the manifest and\n\t# validate each file.\n\t\n\tfiles = []\n\t\n\tfor l in fixity:\n\t\n\t\t# Check the structure of the line:\n\t\n\t\tif l[1] == '_':\n\t\n\t\t\t# If there's no value in the line, abort and print the line\n\t\t\t# and return an error\n\t\t\t\n\t\t\tif len(l) == 2:\n\t\t\t\t\t\n\t\t\t\treturn (False, str(l))\n\n\t\t\telse:\n\t\n\t\t\t\t# Check whether this line"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif fixity.manifest.filename in manifest_files:\n\t\tadd_text = False\n\t\tfixity.validate_fixity_listing(self.path, manifest_files[fixity.manifest.filename])\n\t\tif fixity.manifest.filename in fixity.files:\n\t\t\tadd_text = True\n\t\tif add_text:\n\t\t\tfixity.files.append(fixity.manifest.filename)\n\t\tfixity.files.sort()\n\t\tfixity.manifest.filename = fixity.files.pop(0)\n\t\tfix"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tres1 = self.validate_fixity_block(fixity, manifest_files[0])\n\tif not res1:\n\t\traise exceptions.InventoryValidationError(res1)\n\t\n\t# Get the files in the inventory.\n\tfixity_files = [line.rstrip() for line in fixity.findall('./fixities/fixity[@type]')]\n\tresult = []\n\tfor filename in fixity_files:\n\t\tfilename = os.path.join(get_bases_directory(os.path."}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif fixity is None:\n\t\tself.add_error(\"missing_fixity\", manifest_files)\n\t\treturn \"\"\n\t\n\t# Create the new fixity entry. The new fixity entry will be a dictionary like:\n\t# { filename: filename, line_range: [line1, line2],\n\t#  content: [content] }, file=<filename>.bbl.py #\n\t# The line_range is in the form [(line, line),...]. The ',' is the\n\t# newline character.\n\tfixity_dict = {}\n"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn path.split(\"/\")[1:]\n\ndef parse_files(files_list):\n\t\"\"\"\n\t\tReturn the list of paths for files specified as list of strings\n\t\"\"\"\n\toutput = []\n\tfor path in files_list:\n\t\tfiles = files_list[_files_list.get(path)]\n\t\toutput += files\n\treturn output \n\ndef get_files(path):\n\t\"\"\"\n\tReturn the name of the file specified as a string or `None` if not found.\n\t\"\"\"\n\tif path == None:\n\t\treturn None"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfiles = []\n\tfor filename in os.listdir(path):\n\t\tfilename = os.path.join(path, filename)\n\t\tif os.path.isfile(filename):\n\t\t\tfiles.append(filename)\n\treturn files\n\ndef run(params, *args):\n\t\"\"\"\n\t\tGenerates the actual parameters that need to be given to the function\n\n\t\t:param params: a python list containing all the parameters for each run\n\t\t:type params: a list of tuples ('type', 'name', 'description', 'params')\n\t\treturns a dictionary that maps"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn glob.glob(path)\n\ndef find_logger(root, log_dir):\n\n\tfor logger in root.getLogger().handlers:\n\t\tif isinstance(logger, Logger):\n\t\t\treturn logger\n\n\tfor logger in root.getLogger().root.logger.handlers:\n\t\tif isinstance(logger, Logger):\n\t\t\treturn logger\t\n\n\n#\tlogger_root.addHandler(HANDLER)\n\treturn find_logger(root, log_dir)\n\ndef print_dir(dir):\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\ttry:\n\t\treturn open(os.path.join(path, \"main.log\")).readlines()\n\texcept Exception as ex:\n\t\traise Exception(\"error\")\n\tfinally:\n\t\tpass\n\ndef logpath_files(path):\n\t\"\"\"\n\t\tMake a list of filenames from main.log that have filepaths\n\t\t(in the same directory as this file)\n\t\tReturn a list of (filepath, filename) tuples\n\t\"\"\"\n\tif not os.path.exists(path):\n\t\traise Exception(\"File path does not exist"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tres = [os.fsdecode(os.path.join(path, name)) for name in sorted(os.listdir(path))]\n\tres.extend([\"..\"]*len(res) + os.listdir(path))\n\treturn res\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\tfiles = [path+'/'+f for f in files]\n\treturn files\n\ndef file_exist_check(filepath):\n\t\"\"\"\n\tcheck the file exists\n\t\"\"\"\n\tif os.path.isfile(filepath):\n\t\tif  os.path.exists(filepath):\n\t\t\tpass\n\t\telse:\n\t\t\treturn False\n\n\n\t\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn [line.rstrip(os.sep) for line in open(path)]\n\n\ndef isFile(filepath):\n\t\"\"\"\n\t\tReturn True if `filepath` is a file\n\t\"\"\"\n\treturn os.path.isfile(filepath)\n\n\ndef isDirectory(path):\n\t\"\"\"\n\t\tReturn True if `path` is a directory\n\t\"\"\"\n\treturn os.path.isdir(path)\n\n\ndef fileReader(filepath):\n\t\"\"\"\n\t\tReturn the content of `filepath`\n\t\"\"\"\n\tif isDirectory("}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path,f))]\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tif not path: raise TypeError(path)\n\n\treturn list(set(dirname(realpath(file)) for file in listdir(path) if not file.startswith('.')))\n\n\nclass File:\n\t\"\"\"File class representing a file (file object, which contains a `path`, `contents` and `size`)\n\t\tAttributes:\n\t\t\tpath: File path\n\t\t\tcontents: Bytes read from this file\n\t\t\tsize: Size of the content of this file\n\t\tMethods:\n\t\t\tget_contents(path) \u2014 Read contents of file at `path"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfiles = set([f.name for f in os.scandir(path=path) if f.is_file()])\n\treturn files\n\n\ndef file_types(path):\n\t\"\"\"\n\t\tReturn all file types in the folder\n\t\"\"\"\n\treturn [os.path.basename(f.name).lower() for f in os.scandir(path=path) if f.is_file()]\n\n\ndef fullPath(path):\n\t\"\"\"\n\tReturn the full path of a file\n\t\"\"\"\n\treturn os.path.join(path,"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\treturn {filename: Package.parse(filename) for filename in xmls + files}   \n\n@dataclass\nclass PackageException(Exception):\n\t'''Exception base class, all other exceptions\n\t'''\n\tmessage:str\n\nclass Package():\n    '''\n        Package class, implements methods that are used \n        once the package is created/used and then added to the global list\n    '''\n    def __init__(self, package = 'nist', files = None):\n        self._package_namespace = package\n        self._files = files"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\td = {}\n\n\tfor f in files:\n\t\tbname = os.path.basename(f)\n\t\tpk = None\n\t\tif os.path.isdir(f):\n\t\t\tpk = Package(bname)\n\t\telif zipfile.is_zipfile(f):\n\t\t\td = _parse_zip_file(f, d, pk)\n\t\telse:\n\t\t\td = _parse_file(f, d, pk)\n\t\n\treturn d\n\t\ndef _parse_zip_file(f, d, p"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txmls = [filename.rstrip('.zip') for filename in xmls]\n\txmls = [os.path.join(os.path.dirname(filename), os.path.basename(filename)) for filename in xmls]\n\tfiles = [path for path in files if filename_not_found(path)]\n\tfiles, xmls = zip(*(zip(files, xmls) for filenames in files for filename in filenames))\n\tfiles_grouped = {filename: group_files_by_xml(source, xml, files) for"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txmls_in_dict = []\n\tfor filename in files:\n\t\txml_basefile, extract_id = os.path.splitext(filename)\n\t\tif extract_id:\n\t\t\txml_basefile = extract_id + \".xml\"\n\t\txml_filename = xml_basefile + \".xml\"\n\t\txmls_in_dict.append((xml_filename, open(filename, 'rb')))\n\treturn dict(zip(files, xmls_in_dict))\n\ndef _group_modules_by_xml_filename(source"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t# XML filename, based on the last path element in the folder\n\txml_filename = source[int(source.rfind('\"/'))-1] \n\txml_basename = filename_base(xml_filename)\n\tif xml_basename in ['PAM_EOM']:\n\t\t# Get all PAM EOM files\n\t\txml_filename = '{}.xml'.format(xml_basename)\n\t\tfilenames = _find_matching_files(source, files,\n\t\t\t'.xml', 'PAM_EOM', xml_basename"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tif \".xml\" in xmls:\n\t\txmls = [xml for xml in xmls if \".xml\" not in xml]\n\t\t\n\tif len(files) == 0:\n\t\tfiles = [\"\"]\n\t\n\t_files = {}\n\tfiles = [x.lower() for x in files]\n\t\n\tfor xml in xmls:\n\t\tname = xml.lower()\n\t\tif name in _files:\n\t\t\tprefix = _files[name]\n\t\telse:\n\t\t\t_files[name] = [prefix + '/' + x for x"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tprint(\"Extracting files...\", end=' ')\n\textracted_files = []\n\t# Go through all the files\n\tfor filename in files:\n\t\t# If there is an image, save that as well\n\t\tif os.path.isfile(os.path.join(source, filename)):\n\t\t\textracted_files.append(filename)\n\t\t# Add other file, if any\n\t\tif os.path.isfile(os.path.join(source, filename.replace(\".zip\", \"\"))):\n\t\t\textracted_files.append("}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tgroups = {}\n\t\n\tfor filename, f in zip(files, osutils.iterdirs(source)):\n\t\tif (\".xml\" in filename and\n\t\t\tfilename in xmls):\n\t\t\tgroup = groups.get(filename, [])\n\t\t\tgroup.append((osutils.relpath(filename, source),\n\t\t\t\tosutils.relpath(f, source)))\n\t\t\tgroups[filename] = group\n\t\t\t\n\tif len(groups) == 0:\n\t\traise ValueError(\"no XML files were used in {}\".format(source))\n\telif len(groups"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tfile_groups_by_xml_filename = {}\n\t\n\tif file_list == type(['nothing']):\n\t\treturn\n\n\tfor file in file_list:\n\t\tif file.endswith('.xml'):\n\t\t\tpath = os.path.join(os.path.normpath(source), xmls, file)\n\t\t\t\n\t\t\tfile_group = load_group_from_file(path, file.split(' ')[0])\n\t\t\t\n\t\t\tif file in file_group:\n\t\t\t\tfile_groups_by_xml_filename[file_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tif (files == None):\n\t\tif (isinstance(source, zipfile.ZipFile)):\n\t\t\tfiles = source.namelist()\n\t\t\tfiles.sort()\n\t\telse:\n\t\t\tfiles = glob.glob(os.path.join(source, '*'))\n\t\t\tfiles.sort()\n\t\t\t\n\telif isinstance(files, list):\n\t\txmls = xmls[:]\n\t\tfiles = dict()\n\t\tfiles.update(xmls)\n\t\tfiles.update(files)\n\t\n\telif isinstance(files, dict):\n\t\t"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn os.path.basename(path) == prefix\n\n\ndef is_package(file_path):\n\t\"\"\"\n\tIdentify if a file path belongs to a document package.\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn len(path.split(os.sep)) > 1 and os.path.basename("}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\ttry:\n\t\twith Path(file_path).as_posix() as file_path_:\n\t\t\tfile_path_str = file_path_.stem\n\t\t\tpkg_prefix = prefix + file_path_str[:prefix.__len__()]\n\t\t\treturn pkg_prefix in cwd().name.split('/') # TODO\n\n\texcept FileNotFoundError:\n\t\t# File was not found\n\t\treturn False\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tfrom winreg import REG_SZ, OpenKey\n\tfrom winreg import QueryValueEx\n\tfrom os.path import join, split, seperator\n\tfrom sys import argv\n\n\t# First search by prefix and then by seperator\n\tkey_name = join(prefix, seperator)\n\n\ttry:\n\t\tkey = OpenKey(None, key_name)\n\t\treturn bool(QueryValueEx(key, \"LFN/%s.%s\" % (file_path,file_path)))\n\texcept WindowsError as e:\n\t\t_,"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tif prefix == \"\":\n\t\treturn False\n\t\n\tfile_name = os.path.basename(file_path)\n\t\n\treturn file_name.startswith(prefix + '/')\n\t\n\t\n\ndef is_package(doc) :\n\t\"\"\"\n\tIdentify if `doc` is a file-relative path or a directory to a document.\n\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tdoc : os.PathLike \n\t    path to a"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn file_path.startswith(prefix + \"/\")\n\ndef match_file_by_path(file_path):\n\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given file path.\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn file_path.endswith("}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn any(file_path.startswith(prefix) for file_path in package_files)\n\ndef match_file_version_by_suffix(s: str, file_path):\n\tif file_path.endswith(s):\n\t\treturn True\n\telif file_path.endswith(s.capitalize()):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef extract(project_name, path: str, dest: str, output: str,\n\t    prefix='', output_suffix=None, output_prefix"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tfile = file_path.split('.')\n\tprefix_file = prefix.split('.')\n\tif len(file) == 0 or len(prefix_file) == 0:\n\t\treturn False\n\tif prefix == prefix_file[-1]:\n\t\tif file[-1] == prefix_file[-1]:\n\t\t\treturn True\n\tif len(prefix_file) == 0:\n\t\treturn False\n\tif len(prefix) == 0:\n\t\treturn False\n\n\tif file[-1] == prefix_file[-1]:\n\t\treturn True\n\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tif not file_path.endswith(config.document_path_endswith):\n\t\treturn False\n\tfile_path = f'{config.document_path}/{file_path}'.lstrip('/')\n\treturn prefix+file_path in package[config.document_name].keys()\n\t\ndef remove_prefix_directory(dir_path):\n    \"\"\"\n    Removes the `dir_path`'s prefix of each file in `dir_path`.\n    \n    Parameters\n    ----------\n    dir_path : str\n    "}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tif os.path.exists(prefix + '.txt'):\n\t\treturn True\n\telif prefix.endswith(\".txt\"):\n\t\treturn os.path.exists(file_path + '.' + prefix[:-4] + \".txt\")\n\telse:\n\t\treturn False\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tfile_paths = [file_path]\n\tif prefix == '/':\n\t\tprefix = '.'\n\telif prefix == '.':\n\t\tpass\n\telse:\n\t\tprefix = prefix.strip('/')\n\t\tif '/' in prefix:\n\t\t\tprefix, file_path = prefix.rsplit('/', 1)\n\t\t\ttry:\n\t\t\t\tfile_paths.append('/' + file_path)\n\t\t\texcept AssertionError as e:\n\t\t\t\t# If the `file_path` is not found, raise an exception.\n\t\t\t\traise"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [f for f in files if len(f.startswith(prefix))]\n\ndef build_command(document2json):\n\t\"\"\"\n\tBuild a command to retrieve the list of documents in the `document2json` folder.\n\n\tParameters\n\t----------\n\tdocument2json : str\n\t    Path to the directory with the folder containing the files, e.g. `document2json/`\n\n\tExample\n\t-------\n\t>>> from pywikibot import pagegen\n\t>>> import time\n\t>>> pages = pywikibot.pagegenerators.Wikid"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles = list(set(f[0].split('/')[-1] for f in files))\n\treturn [f for f in files if len(f.split('.'))>1]\n\t\n\t\ndef filter_path_files(paths, path_name, path_name2, path_name3):\n\"\"\"\n\tSepara os arquivos de `path_name` dentro de `other_path` como os arquivos\n\tdentro da lista `paths`.\n\t\n\tRetorna os arquiv"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles = _check_file_pattern(files)\n\t\n\tresult = []\n\t\n\tif len(files)==1 and (prefix == \"-\"):\n\t\tfiles = files[0]\n\t\n\tfor path in files:\n\t\tif isinstance(path, str):\n\t\t\tif _dir_contains(path, prefix):\n\t\t\t\tresult.append(path)\n\t\telif isinstance(path, tuple) and path[0].startswith(\".\"):\n\t\t\tresult.append(path[1])\n\t\t\t\n\treturn result\n\t\n\t\ndef"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tpath, ext = os.path.splitext(files)\n\t\n\tfiles = [os.path.join(path, f)[len(prefix):] for f in sorted(os.listdir(path))]\n\treturn files\n\ndef extract_files_to_directory(doc, dirpath):\n\t\"\"\"\n\t\tExtract all files in `dirpath` to `doc` in the directory `dirpath`.\n\t\tThe directory `dirpath` must contain a subdirectory `.js` containing\n\t\tfiles that you want to extract to `doc`.\n\n\t\tParameters\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles = [os.path.basename(filename) for filename in files]\n\t\n\tfiles = sorted(files, key = lambda x: int(x.strip('.txt')))\n\t\n\tfiles = ['' if filename.endswith('.txt') else filename for filename in files]\n\t\n\tfiles = [prefix + '/' + fila + '.txt' for fila in files]\n\t\n\tfiles = [f for f in files if f]\n\t\n\treturn files\n\ndef create_directory_from_filenames(dir_path"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles = os.listdir(files)\n\tfiles = [os.path.basename(x) for x in files]\n\tfiles = list(filter(lambda x: fnmatch.fnmatch(x, '%s%s' % (prefix,os.sep)), files))\n\treturn files\n\n\ndef extract_images(filename, path_to_image):\n\t\"\"\"\n\tExtract images in `path` to `path_to_image`\n\t\n\tParameters\n\t----------\n\tfilename : str\n\t    file name\n\tpath_to_image"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [f for f in files if f.startswith(prefix)]\n\ndef parse_files(input_text):\n\t\n\tr = []\n\tcurrent_package = \"\"\n\tfor line in input_text.splitlines():\n\t\tif line.startswith('['):\n\t\t\tif current_package!= \"\":\n\t\t\t\tr.append(current_package)\n\n\t\t\tcurrent_package = \"\"\n\t\t\tif line.endswith(')') or line.endswith(']'):\n\t\t\t\tr.append(current_package)\n\t\t\t\tcurrent_package = \"\"\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles = files.split()\n\treturn [f[len(prefix):] for f in files if f.startswith(prefix)]\n\t\n\t\ndef get_filename(base, prefix, filenames):\n\t\"\"\"\n\tGet filename of 'filename' within `files`\n\n\tRetorna a linha de cima destes arquivos para selecionar e\n\tpara um filo da lista `filenames`\n\t\n\tParameters\n\t----------\n\tbase : str\n\t\tBase directory\n\tprefix : str\n\t\tPrefix"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t# if self.docinfo is None:\n\t#\traise RuntimeError(log.MESSAGE_ERROR_CORE_DBINIT % self.__class__.__name__)\n\t\n\tfiles_filenames = {}                  # files to get filenames\n\tfiles_filenames_path = {}             # get filenames of `files` from `files`\n\tfor file in files:\n\t\tfilename = file.name            # name of file\n\t\t# fullpath = os.path.abspath(os.path.join(self."}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles_ = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfiles_.append(f)\n\treturn files_\n\ndef formatDF(file):\n\t\"\"\"\n\tConvert an extension `file` from a string with a `file.ext` in it to\n\tstring with its extension.\n\t\n\tParameters\n\t----------\n\tfile: str\n\t    File type, extension\n\tReturns\n\t-------\n\tstr\n\t    file type and extension (extension overrides extension\n\t\"\"\"\n\treturn \"%s\" % file"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpackages = {}\n\tfor root, dirs, files in os.walk(folder):\n\t\tif dirs[0]!= \"..\":\n\t\t\tcontinue\n\t\tfname   = os.path.join(root, files[0])\n\t\tpackage = fname.split(\".xml\")[0].rstrip(os.sep)\n\t\tfile_basename = os.path.basename(fname)\n\t\t\n\t\ttry:\n\t\t\tdata = etree.parse(fname).getroot()\n\t\t\tif package in packages:\n\t\t\t\tpackages[package].append("}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpackage_data = {}\n\tfilenames = os.listdir(folder)\n\n\tfor filename in filenames:\n\t\tif filename.endswith(\".xml\") and not os.path.isdir(os.path.join(folder, filename)):\n\t\t\twith open(os.path.join(folder, filename), 'r') as XML:\n\t\t\t\tcontent = XML.read()\n\t\t\t\troot = etree.fromstring(content)\n\t\t\tname = root.attrib['name']\n\t\t\t# Create list of elements in package_data with name as key\n\t\t\t"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles = glob.iglob(folder + \"/**/*xml\", recursive=True)\n\toutput = {}\n\tfor file in files:\n\t\t_, filename = os.path.split(file)\n\t\tif filename in output: # skip folders\n\t\t\tcontinue\n\t\ttry:\n\t\t\tdom = xml.parsers.expat.parseFile(file)\n\t\texcept Exception as e:\n\t\t\t# print(f'{filename} failed to parse')\n\t\t\tcontinue\n\t\tnames = []\n\t\tfor child in dom.getElementsByTagName(\"file\"):\n\t\t\t"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfile_list = os.listdir(folder)\n\tfolder_dict = dict()\n\t\n\tgroup_1_file_list = []\n\tgroup_2_file_list = []\n\tfor file in file_list:\n\t\tbase_name, extension = os.path.splitext(file)\n\t\tif extension == \".xml\":\n\t\t\tgroup_1_file_list.append(file)\n\t\telif extension == \".xsd\":\n\t\t\tgroup_2_file_list.append(file)\n\t\n\tfor group_1_file in group"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfile_basename = glob.glob(folder+\"/\"+folder+\"*\")\n\tfor file_item in file_basename:\n\t    with open(file_item) as fin:\n\t\t    file_text = fin.read()\n\t    xml_node = ET.fromstring(file_text)\n\t    packages = package_dict_from_xml(xml_node)\n\tpackages = packages.pop('packages',{})\n\treturn packages\n\ndef package_dict_from_xml(xml_node):\n\t\"\"\"\n\tCreate package dict"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Get basename of each data set\n\tdataset_keys = {1: '1_1', 2: '1_2', 3: '1_3'}\n\t# Init list that will store data\n\tdata = {}\n\t# Search in 'folder' folder content and extract data\n\tfor file in os.listdir(folder):\n\t\t# Convert file name to dataname\n\t\tdata_file_name = os.path.splitext(file)[0]\n\t\t# Check if data already stored\n\t\tif data_file_name in dataset_"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\txml_files = set()\n\tdata = {}\n\tfor file in os.listdir(folder):\n\t\tif file.startswith('.'):\n\t\t\tcontinue\n\t\tif os.path.isdir(os.path.join(folder, file)):\n\t\t\txml_files |= set(\n\t\t\t\t[file.replace('.xml', '') for file in os.listdir(\n\t\t\t\t\tos.path.join(folder, file)) if file.startswith('.')])\n\t\telse:\n\t\t\tdata.update({file.replace('.xml', ''):"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t# Find package folder\n\tfolder = folder.strip(path.strip_prefix('{})'.format(_path_package_name_folder)))\n\t\n\t# Get folder subfolders\n\tfolder_subfolders = filter(bool, os.listdir(folder))\n\t\n\t# Sort by name\n\tfolder_subfolders = sorted(folder_subfolders, key=sort_file_extension)\n\t\n\t# Get info from XML\n\tpackage_files = get_files_from_folder(os.path.join(folder, 'Files"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tresult = {}\n\tfor file in sorted(glob.glob('{}/**/*.xml'.format(folder)),\n                       recursive=True):\n\t\tif os.path.isdir(file):\n\t\t\tcontinue\n\t\tfileparts = os.path.split(file)\n\t\tbasename = fileparts[1]\n\t\t_, ext = os.path.splitext(fileparts[2])\n\t\tresult[basename] = {'basename': basename, 'ext': ext}\n\treturn result\n\n\ndef _get_unformatted_"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfilenames = glob.glob(\"{}/**/*.xml\".format(folder), recursive = True)\n\tfilenames = sorted(filenames)\n\t\n\tdict_of_all_files = dict()\n\tfor filename in filenames:\n\t\tkey = os.path.basename(filename)\n\t\tdict_of_all_files[key] = {\n\t\t\t'success': False,\n\t\t\t'text': \"\",\n\t\t\t'time': \"\",\n\t\t\t'size': \"\",\n\t\t\t'hashsum': \"\",\n\t\t\t'comp"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Para cada arquivo xlsx na pasta do m\u00f3dulo a analise\n\t# se conseguiu ter pelo menos um problemas\n\tfor extension in [\"xlsx\", \"xlsm\"]:\n\t\ttry:\n\t\t\texists_archive = False\n\t\t\tif \".\" in prefix or \".vox\" in prefix:\n\t\t\t\texists_archive = os.path.exists(\n\t\t\t\t\t\"./data/asset/{}/{}\".format(\n\t\t\t\t\t\tprefix, os.path.splitext(os.path.bas"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfile_type = re.search(r'^(\\w+_asset_.*)\\.xml$', prefix).group().replace('_', '')\n\tfile_folder = re.search(r'^([a-zA-Z]*)(/.)+$', file_path).group().replace('/', '').replace('_', '').replace('\\\\', '')\n\n\tif file_folder in ['', None, '.']:\n\t\tfile_folder = os.path.dirname(file_path)\n\t\n\tfile_name = os.path"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# verifica se o filtro desejado \u00e9 apenas unidade para uma imagem\n\t# qualquer o texto, n\u00e3o sabe transformar por muitas fun\u00e7\u00f5es\n\tfilename = prefix.split(\".\")[0] + \"_\" + file_path.split(\".\")[-1]\n\t\n\ttry:\n\t\twith ZipFile(file_path, \"r\") as zip_zip:\n\t\t\tif filename in zip_zip.namelist():\n\t\t\t\traise ValueError\n\t\t\telse:\n\t\t\t\tzip_zip.extract"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfile_folder = '/content/assets/renditions/'\n\tassets_files = os.listdir(file_folder + prefix)\n\tassets_types = ['assets','renditions']\n\t# Assume assets/imgs, assets/imgs.xml are always images\n\tfile_folder = '/content/assets/' + filename\n\tif prefix in ['imgs', 'images']:\n\t\t# For assets/imgs, use an xml instead\n\t\tassets_files.remove('assets.xml')\n\t\tassets_files.remove('assets.imgx')\n"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif filename.find('.')!= -1:\n\t\tfile_type = 'asset'\n\t\tasset_file_fmt = filename.split('.')[0]\n\t\tfile_name = filename.split('.')[1]\n\t\tpackage_folder_path = \"/\".join([\"assets\", str(asset_file_fmt)]).replace(\" \", \"\")\n\telif filename.find('.')!= -1:\n\t\tfile_type ='rendition'\n\t\trendition_type = filename.split('.')[1].lower()[:-1"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\ttry :\n\t\t# Identifica a cidade e de localize-se\n\t\tcidade, localize= get_cidade_localize(file_folder)\n\texcept Exception as e:\n\t\traise Exception('Problema ao conectar a API OpenFoodFacts.'+ str(e)) from None\n\t\n\t# Abrimos o arquivo XML e carrega\n\twith open('{}{}'.format(file_folder,prefix), encoding='utf-8') as xml:\n\t\txml_root = et."}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tfile_folder = os.path.dirname(__file__)\n\tasset_filename = prefix + '.asset'\n\t\n\tif not os.path.isfile(re.sub(r'[^\\w]+', '.', asset_filename)):\n\t\traise ModuleNotFoundError('Asset %s is missing in %s' % (asset_filename, file_path))\n\t\n\tcontent_type, content_length = self._open_file(file_path, 'content=asset')\n\tpackage = { 'type':"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdef _normalize(tag):\n\t\t\"\"\"\n\t\tNormalize a string e retornar o item normalizado.\n\t\t\n\t\tNormalize e retorna:\n\t\t- o segundo item, que \u00e9 o tipo\n\t\t- o terceiro item, que \u00e9 o arquivo\n\t\t- o quarto itambg\u00e9 da string do tag\n\t\t\n\t\tReturns\n\t\t-------\n\t\tstr\n\t\t\"\"\"\n\t\tnormalized = tag.lower()\n\t\t\n\t\tfor t in ['s', 'ns']:\n\t\t\tnormalized = normalized.replace("}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\txml_identifier = {}\n\tfile_extension = \"xml\"\n\tif file_path.endswith(file_extension):\n\t\txml_identifier[\"extension\"] = file_extension\n\t\txml_identifier[\"filename\"] = str(prefix) + file_path.split(\".\" + file_extension)[1]\n\t\tpackage_path_path = file_path.split(\"/\")\n\t\tif len(package_path_path) == 2:\n\t\t\txml_identifier[\"name\"] = package_path_path[0]\n\t\tel"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpkg = []\n\tpath = utils.join_paths(file_folder, \"{:s}.{:s}\".format(prefix, file_path.split(\".\")[-1]))\n\tfor root, dirs, files in os.walk(path):\n\t\tfor filename in files:\n\t\t\tbase, ext = os.path.splitext(filename)\n\t\t\tif base.startswith(prefix):\n\t\t\t\t# noinspection PyDataDescriptor\n\t\t\t\trel_filename = base[len(prefix) + len(\".{:s}\".format("}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tif not self.check_exist_file(lang):\n\t\treturn {\"code\":1,\"message\":\"\u7edd\u5bf9\u8def\u5f84\u672a\u627e\u5230\uff0c\u6ce8\u610f\uff1a\u4ec5\u4ec5\u8fd0\u884c\"+file_path}\n\tfile_name = os.path.join(self.basedir, lang, file_path)\n\tif os.path.isfile(file_name):\n\t\tif self.check_exist(lang):\n\t\t\t# \u524a\u9664\n\t\t\t# shutil.rm(file"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tf = open(file_path, \"r\", encoding=\"utf-8\")\n\tif lang == \"en\":\n\t\tdata = f.read()\n\telse:\n\t\tdata = f.read()\n\tf.close()\n\t\n\t#print(data)\n\t\n\treturn HttpResponse(data)\n\ndef add_rendition_for_image(self, lang, image_path):\n\t\n\tif lang == \"cn\":\n\t\tdata = imread(\"data/Images/article/cn.jpg\")\n\telse:\n\t\tdata = imread"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t# Load the PDF\n\twith lzma.open(file_path, mode='r') as pdf_file:\n\t\t# Parse the PDF\n\t\tpdf_file.seek(0)\n\t\tpdf_page = pdf_file.read()\n\n\t\t# Load the template text file\n\t\twith open(f\"{APP_DATA_PATH}/template.xml\", \"rb\") as template_file:\n\t\t\ttemplate_text = template_file.read().decode(\"utf-8\")\n\n\t\t# Compile the response\n\t\tresponse = self._make_response(dict("}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tdata = base64.b64encode(open(file_path, \"rb\").read())\n\tenc_data = data.decode(\"utf-8\")\n\turl = \"https://api.github.com/content/{}?size=65536\".format(enc_data)\n\tres = requests.post(url,headers=headers)\n\tprint(res)\n\ttry:\n\t\tres.raise_for_status()\n\texcept requests.HTTPError as err:\n\t\tprint(err)\n\t\traise\n\n\twith open(f\"{self."}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tresponse = self.es.post(f\"{self.api_key}/{self.api_token}/m/search/\", {\"lang\": lang,\n\t \"q\": file_path, \"recoq\": 100, \"limit\": 100, \"offset\": 0})\n\treturn response\n\ndef add_file_to_db(lang, data):\n\t'''\n\t\tThis function create file on db and attach file to it.\n\t'''\n\tfile = \"/mnt/data/images/\"+str(data['filename'])\n\twith open(\""}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tdata = dict()\n\tdata['original'] = file_path\n\tdata['en'] = \"/tmp/\" + lang + \"/\" + file_path\n\treturn json.dumps(data)\n\n\n\t#\n\ndef get_lang():\n\twith open('config', 'r') as f:\n\t\tcfg = f.read()\n\t\treturn cfg\n\n\ndef read_enumerated(lang):\n\tdata = []\n\twith open('data/enumerated/' + lang + '.json', 'r') as f:\n\t\ten = json.loads("}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\twith open(file_path, 'rb') as f:\n\t\tuploaded_file = {'file' : f }\n\t\tres = self.upload_to_api(\n\t\t\t'https://images.weibo.com/weibo/mb_200x20/image/upload/original',\n\t\t\tdata=uploaded_file\n\t\t\t\t)\n\t\treturn res['code']\n\n\tdef get_content(self, path):\n\t\twith open(path, 'rb') as f:\n\t\t\tupload = {'file' : f }\n\t\tres = self.upload_to"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tif lang in self.languages and file_path is not None:\n\t\tfile_data = {\"original\": file_path}\n\n\t\tfile_data.update(self.get_page(lang, \"original\").extract_page_data())\n\t\tfile_data.update(self.get_page(lang, \"en-us\").extract_page_data()) # En es pais\n\n\t\tself.set_url(file_data.get(\"original\"))\n\t\tself.set_lang(lang, \"en\")\n\n\t\tfile_data.update(self.get_"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t# read file and get file data\n\twith open(file_path, 'rb') as file:\n\t\tcontent = file.read()\n\n\t# check if it is language only, if not, let server error\n\tif lang not in self.langs:\n\t\treturn \"ERROR: not supported language!\"\n\t# format name\n\tfile_nm = file_path.split('/')[-1]\n\n\t# if file not found,\n\tif self.path not in content:\n\t\t# if not in path, move to next element\n\t\tif self.path not in"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tf = FileFinder(root_directory=self.root_directory)\n\toriginal_file = f.find_first(\"original\", file_path)\n\t# add data with a file extension to the path\n\t# if lang not equal 'ko' or empty string, add language\n\tif lang.lower() in DEFAULT_LANGUAGE:\n\t\tif not original_file:\n\t\t\tfile_path = file_path[:file_path.find(\".\")]\n\t\toriginal_file = f.add_file(file_path + \"-\" + lang + \".pdf"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif basename in [\"artigo02-gf03.tiff\", \"artigo02-gf03.jpg\", \"artigo02-gf03.png\"]:\n\t\tprint(\"Not adding asset {}, it already exists!\".format(basename))\n\t\treturn\n\n\tself.add_files(basename, file_path)\n\n# TODO remove for jupyter\n#TODO move into a new folder\ndef download_and_extract_and_unzip(asset_full_path, save_path):\n\t\"\"\"\n\tDownload"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\traise NotImplementedError(\"This method should be implemented by subclass.\")\n\n\tif os.path.isfile(file_path):\n\t\tos.rename(file_path, file_path + \" - renamed\")\n\n\tprint(\"File path\", file_path)\n\nclass Image(ObjectIO):\n\tdef __init__(self, img, size):\n\t\tsuper().__init__(img, False)\n\t\tself.max_zoom = 0\n\n\t\tself.size = size\n\t\tself.size_x, self.size_y = self.size\n\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.new_assets.setdefault(basename, {\n        \"file_path\": file_path\n      })\n  "}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif self.current_task_name!= 'inference':\n\t\tself.__add_asset_to_inference(file_path, basename)\n\t\tself.current_task_name = 'inference'\n\n\treturn\n\n    def clear_assets(self):\n    \t\"\"\"\n\t    Clear all the asset files\n\t    :param name:\n\t    :param path:\n\t    :return:\n\t\"\"\"\n    \t# Remove asset\n    \ttmp = glob.glob(os.path.join(self.working_path, '*.%s"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif isinstance(file_path, str):\n\t\tfile_path = [file_path]\n\n\tfiles = glob.glob(os.path.join(self.base_path, '**', '*.tiff'), recursive=True)\n\tfor base_folder in files:\n\t\tfolder = os.path.split(base_folder)[-1].split('.')[0]\n\t\tif folder not in [\"artigo1\", \"artigo2\", \"artigo3\"]:\n\t\t\tcontinue\n\t\tfilepath = os.path.join(base_folder, basename"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\ttry:\n\t\tt = TaggedImage.open(basename)\n\t\tos.stat(basename)\n\t\tt.path = file_path\n\t\tself.append(t)\n\texcept (TaggedImageException, IOError):\n\t\tif not os.path.exists(file_path) == False: # assume it's gone\n\t\t\tprint(\"%s not found, creating\" % basename)\n\t\t\tprint(\"  %u/%u (basename: %s)\" % (len(self), len(self), basename))\n\t\t\tt = TaggedImage"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.artigo_paths[basename] = file_path\n\treturn self\n\n\ndef find_asset(self, filename: str):\n\t\"\"\"\n\t\tsearch assets for filename <filename>.\n\t\"\"\"\n\tfilename = str(filename).split(\".\")[0]\n\t# print (filename)\n\tfor entry in self.assets.values():\n\t\t# print(entry.basename, entry.get_name())\n\t\ttry:\n\t\t\t# self.assets[entry.basename][entry.get_name()]\n\t\t\tif os.path.isfile"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\timage = QImage(file_path)\n\timage = image.scaledToHeight(480, Qt.SmoothTransformation)\n\tself.asset_group.addWidget(self.file_frame_widget)\n\tself.asset_group.addWidget(self.file_label)\n\tself.asset_group.addStretch()\n\tself.asset_group.addWidget(self.image_widget)\n\tself.asset_group.addWidget(self.asset_group_button)\n\tself.asset_group.set"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.files[basename] = basename\n\n\t_, ext = os.path.splitext(file_path)\n\n\tif ext == '':\n\t\tif _path_split(file_path)[-1] in [\"_gf01\", \"_gf2\", \"_gf03\", \"_gf03.tiff\"]:\n\t\t\t# path_split() return [0], [0] because you don't split at the last part, which is part of last part of the path\n\t\t\treturn \"/\"\n\t\telse:\n\t\t\tprefix = '/'\n\telse:"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\titem = asset.Asset(basename, file_path)\n\tself._assets.append(item)\n\t\n\tdir, name = os.path.split(file_path)\n\tself._names.append(name)\n\t\n\t# self.__asset_map[basename] = dir\n\t# if dir and dir!= \".\":\n\t# \tself.__sorted_filenames[dir] = sorted(glob.glob(f\"{dir}/*_\" + name))\n\t# else:\n\t# \tself"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif os.path.isfile(zip_path):\n\t\twith zipfile.ZipFile(zip_path, \"r\") as zf:\n\t\t\tnamelist = zf.namelist()\n\t\tfiles = [item.decode(\"utf-8\")[len(r\"/\\!\\S+\"):] for item in namelist\n\t\t\t\t if not item.endswith(\"/\") and not item.endswith(\"\\\\\")]\n\t\tdata = dict(zip(files, glob.glob(os.path.join(\n\t\t\t\"/\".join("}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdata = {}\n\tbase = os.path.basename(zip_path)\n\tzip_fn = zip_path.rsplite(zip_path.index('\\\\') + 1)\n\t# Extract zip file to its file name with basename\n\tdata['dir'] = zip_fn\n\t# Create subdir for each dir inside file_dir\n\t# For example, \"basket_bases_2019-09-24.zip\"\n\t#   subdir:\n\t#       basket_bases_2019-09-24_2017-10"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpack_data = {}\n\tprint(\"Start to read\", zip_path)\n\tbasename = [x[17:] for x in os.listdir(zip_path) if os.path.isdir(join(zip_path, x))]\n\tfor base in basename:\n\t\tprint(\"start loop here for zip file\", base)\n\t\tfor zip_name in os.listdir(join(zip_path, base)):\n\t\t\ttry:\n\t\t\t\tif zip_name == \".gitkeep\":\n\t\t\t\t\tcontinue\n\t\t\t\tzipfilepath = join(base, zip_"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpkgs = {}\n\tdef _read_xml_item(xml_item):\n\t\t\"\"\"\n\t\t\tParse an XML item\n\t\t\t\n\t\t\tGet metadata of package_name for data\n\t\t\t\n\t\t\t@param\n\t\t\t\txml_item\n\t\t\t@return\n\t\t\t\t(str, (str, str)) if metadata is found, return (None, None)\n\t\t\"\"\"\n\t\tpkgname = xml_item.xpath(\"./Metadata/Name\")\n\t\tif pkgname:\n\t\t\tpkgname = pkgname[0].text\n\t\tif not pkgname:\n\t\t\t#"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\toutput = {'zips' : []}\n\tfor _, z in zip_path.infolist():\n\t\tzname = z.filename\n\t\tif zname.endswith('.xml'):\n\t\t\toutput['zips'].append({'filename' : zname, 'contents' : z.contents})\n\t\t\tfor x in z.file_size: output['contents'].append(x)\n\t\n\treturn output\n\ndef _parse_file_list(files_path):\n\toutput = []\n\tfor f in files_path"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# load zips\n\tarchive = zipfile.ZipFile(zip_path, 'r')\n\tinfo = [zip.infolist() for zip in archive]\n\n\t# get path for outfile\n\toutfile_path = 'data/datasets'\n\n\t# create dict to store data\n\tdatasets = {}\n\t\n\t# for each file\n\tfor i in range(len(info)):\n\t\tprint(i)\n\n\t\tpath = info[i].filename\n\n\t\tif i == 0:\n\t\t\t# extract files\n\t\t\tname = path.split"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith ZipFile(zip_path, 'r') as zip_ref:\n\t\tdata = dict()\n\t\tfor file in zip_ref.infolist():\n\t\t\tinfo = zip_ref.infolist()[file]\n\t\t\tmd5sum = getattr(info,'md5sum', None)\n\t\t\tsha256sum = getattr(info,'sha256sum', None)\n\t\t\tdata[info.filename] = {\n\t\t\t\t'docs': [Document(d.file_size, d.compression_type)\n\t\t\t\t\t\t\tfor d in info.file_info"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport os\n\tfrom os.path import dirname, isdir\n\timport zipfile\n\timport json\n\timport py2json\n\timport requests\n\n\tclass DownloadZipError(Exception):\n\t\tpass\n\n\tdef _load_zipfile(zip_path):\n\t\t\"\"\"\n\t\tUnpack zip file in current directory and its subdirectories\n\t\t\n\t\tParameters\t----------\n\t\tzip_path : str\n\t\t\tfile path\n\t\t\"\"\"\n\t\tzip_file = zipfile.ZipFile(zip_path)\n\t\tfor item in zip_file.infolist():\n\t\t\tif"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tresult = {}\n\twith ZipFile(zip_path) as z:\n\t\tfor zip_item in z.infolist():\n\t\t\twith z.open(zip_item) as file2_item:\n\t\t\t\tname = file2_item.filename\n\t\t\t\tif zip_item.FlagCreateFileMetadataOnDisk:\n\t\t\t\t\ttype = 'file'\n\t\t\t\telif zip_item.FlagCreateFileMetadataOnDisk:\n\t\t\t\t\ttype = 'dir'\n\t\t\t\telse:  # file (not metadata)\n\t\t\t\t\ttype = 'none'\n\t\t\t\tif ((\n\t\t\t\t\t\tname.lower"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# get data as list\n\tdata_list = list(walk_zip(zip_path))\n\t\n\t# process data, return one dict per file\n\traw_data = []\n\tfor data_item in data_list:\n\t    try:\n\t        raw_data += [data_item[key] for key in data_item['filename']]\n\t        raw_data += data_item['extras']\n\t    except KeyError:\n\t        pass\n\t    \n\t# organize data by basename\n\textras = []\n\tfilename_index"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, \"r\") as zip_file:\n\t\treturn [file for file in zip_file.namelist()\n\t\t\t\tif not file.startswith(\".\") if file.startswith(\".\") else file]\n\ndef _read_files(url_list):\n\t\"\"\" \n\t\tRead the list of files from url and create a list of files object\n\t\t\n\t\tExample:\n\n\t\t```\n\t\t[\n\t\t    '2318-0889-tinf-33-0421/2318-0889-tinf-"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_obj:\n\t\tfor fn in zip_obj.namelist():\n\t\t\tfiles.append(fn)\n\treturn files\n\ndef merge(f1 = None, f2 = None, output_path = None):\n\t\"\"\"Merge two files\n\t\n\tCreate a zipfile if one is None. If the second file is None, then\n\tthe first file will be merged.\n\t\"\"\"\n\t# Create a zip object\n\tzip_obj = zipfile.ZipFile("}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\treturn [os.path.join(zip_path, file) for file in os.listdir(zip_path)]\n\"\"\"\n\t\n\t\n\treturn [\n\t    file.lower().replace('_e2000','_') for file in files_list_from_zipfile(zip_path)\n\t        ]\n\t\"\"\"\n\ndef file_name_from_zip_path(zip_path):\n\t\"\"\"\n\tGiven a `zip_path` return the file name without the extension\n\t\n\tExample:\n\t\n\t```\n\t'2318-"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\tout = []\n\tif zipfile.is_zipfile(zip_path):\n\t\t# Check if passed file does not end with.ZIP\n\t\tname = os.path.basename(zip_path)\n\t\tif name.endswith(\".zip\"): return [out]\n\t\t# Try reading the archive\n\t\ttry:\n\t\t\twith zipfile.ZipFile(zip_path, compression=\"pack\", mode=\"r\") as zipf:\n\t\t\t\tnamelist = zipf.namelist()\n\t\t\t# If all files are here, return\n\t\t\tif len("}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\treturn [(zip_path + file_name + \".pdf\", file_name + \".pdf\")  for file_name in os.listdir(zip_path) \\\n\t    if is_valid_zip_file(zip_path + \"/\" + file_name + \".zip\")]\n\ndef create_metadata_list(pdf_input_filelist, xml_input_filelist):\n\t\"\"\"\n\tCreate the list of metadata for the given PDF input files and XML input files.\n\n\tExample:\n\t\n\t```\n\t[\n\t    (3, '3"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\timport base64\n\timport gdal\n\timport struct\n\n\t# Read in the entire tarball as a string and set the\n\t# compression flag to gzip.\n\ttar_data = gdal.Open(zip_path).ReadFile()\n\ttar_data = gdal.ByteTranslate(tar_data, target_name='-gdal')\n\ttar_data = base64.b64decode(tar_data)\n\t\n\t# Extract the tarball as a TAR to a string\n\text = '/'.join([os.path.spl"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfiles = []\n\n\tfor f in sorted(glob(os.path.join(zip_path, '*'))):\n\t\tfiles.append(os.path.basename(f)[:-4])  # remove.pdf,.xml\n\t\t#if os.path.basename().endswith(\".xbm\"):\n\t\t#\tfiles.append(os.path.basename(f)[:-4] + \".pgm\")\n\t\t#elif os.path.basename().endswith(\".png\"):\n\t\t#\tfiles.append(os"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\treturn [pjoin(zip_path, f) for f in os.listdir(zip_path) if f.endswith(\".tif\")]\n\ndef is_file_in_path(path):\n\t\"\"\"\n\tReturn whether `path` exists or not\n\n\tExample:\n\t\n\t```\n\t5\n\t10\n\t```\n\t\"\"\"\n\tif not osp.exists(path): return False\n\ttry:\n\t\tos.stat(path)\n\t\treturn True\n\texcept Exception as e:\n\t\treturn False\n\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\twith open(zip_path, \"rb\") as i:\n\t\tfile_list = []\n\t\tzip_name = zip_path.split(\"/\")[-1]\n\t\tfor chunk in i:\n\t\t\tchunk_name = chunk.decode(\"utf\").split(\"/\")[-1]\n\t\t\t# If file with same name and extension. zip file.\n\t\t\t# If not, extract file with extension and name.\n\t\t\tif chunk_name == zip_name:\n\t\t\t\tfile_list.append(chunk_name)\n\t\t\telse:\n\t\t\t\tif chunk_name"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\t# Make a list of the subfiles\n\tfiles = []\n\tfor f in zip(zip(zip_path.split('/')[1:], map(list, zip(zip(zip(os.scandir(zip_path), os.listdir(zip_path)))))), \n\t\t\t\t  dir(zip_path)):\n\t\t\tfiles.append(f)\n\t\n\t# Clean up subfiles\n\tfor f in range(len(files)):\n\t\ttry:\n\t\t\tos.remove(files[f][-1])\n\t\texcept O"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content[0] == 'w':\n\t\tname = content[1:]\n\telif name =='st':\n\t\tname ='st='\n\treturn ''.join([f'{(w):=s}' for w in name.split()]) \n\ndef get_xmlns(element):\n\treturn 'rpc=\"http://www.ibc.com/xml/2010/\"' if element.tag == 'rpc' else 'xmlns=\"rpc/\"'\n\n#---------------------------------------------------------------------------\n# 'rpc' Element.\n#---------------------------------------------------------------------------\n\ndef"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\treturn wx.StringCtrl_Replace(content,\"w:st=\",\"w-st=\")\n\ndef split_namespace_prefix(contents):\n\treturn wx.StringCtrl_Split(contents,\"w-st1\",\"\",wx.ID_ANY|wx.TE_PASSWORD|wx.TE_READONLY,pos=(3,3),size=(120,30))\n\ndef add_parameter(parameter,kind,description):\n\tif kind!='string' and kind!= 'integer':\n\t\tkind ='string'\n\n\treturn"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tregex = re.compile(r'''((?:[a-zA-z0-9:_-$]*)|([A-Za-z0-9\\-]+))''')\n\tout = StringIO()\n\tfor line in content.splitlines():\n\t\tm = regex.findall(line)\n\t\tif m and m[0]!= \"\":\n\t\t\tline = ''.join(m)\n\t\tout.write(line+' ')\n\treturn out.getvalue()\n\ndef add_newline_to_end(lines):\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = content.split('w-st=\"', 1)[1].replace('\"','\"')\n\treturn ('w:' + content)\n\t\ndef fix_namespace_prefix_z(content):\n\t\"\"\"\n\tFix 'z:x' and 'Z:' for XML attributes\n\t\"\"\"\n\tcontent = replace_chunks(1, content, {\n\t\t'x':'w:x', 'Z':'w:Z', '_':'w:_',\n\t\t'\\\\n':'\\\\xa0', '_':'\\\\xa0',\n\t\t'"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tdef fix_namespace_prefix_x(text):\n\t\treturn re.sub(r'(\\w)\\:', r'\\1w-\\2', text)\n\treturn re.sub(r'\\.:','-',fix_namespace_prefix_w(content))\n\nclean_texts = get_content_from_url.get_content(content_link)\n#get_content_from_url.process_content(clean_texts)\nget_content_from_url.generate_json_files(clean_texts)\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\treturn ''.join(['r' + c + '\\n' for c in content if c not in ['w:st', 'w-st']]).strip()\n\ndef get_stylesheet(stylefile):\n\twith open(stylefile, 'r', encoding='utf-8') as f:\n\t\treturn f.read()\n\ndef get_table_names(dbname):\n\t\"\"\"Returns a list of tables in database named 'dbname'\"\"\"\n\ttables = get_all_tables(dbname)  # type: list\n\ttables."}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t# if the first index does not match, make sure it's lowercase\n\tindex = content.index(\":\")\n\tname = content[:index]\n\tname = text_normalize_whitespace(name)\n\treturn (name, name)\t\t# w:st=\"...\n\ndef fix_enum_w(content):\n\t\"\"\"\n\tConvert text where each enumeration item has a word separated by spaces, \n\te.g.  \"the\", \"is\",\"the\", \"a\", \"a\", \"some\", \"in\", \"that\", \"and"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\ttext = content.replace('<', '<w:')\n\tif \"-\" in text:\n\t\ttext = text.split(\"-\")\n\t\ttext = \"-\".join(text[:-1])\n\t\ttext = text[1:] + \"-\"\n\ttext = text.replace(\"-\", '-')\n\treturn text.replace(\"w\", \"w-\")\n\ndef fix_namespace_prefix(content):\n\t\"\"\"Fix a content like #w:st=\"r\">\"\"\"\n\tif \"w:st=\" in content:\n\t\tcontent = content.replace(\"w"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t# content = content.replace('=w=\"', '==w=\"')\n\t# content = content.replace('=w-st=\"', '==w-st=\"')\n\tif content.startswith('w:'):\n\t\tcontent = content[len('w:')::]\n\telif len(content) > 0:\n\t\treturn 'w:' + content.replace('\"', '&quot;')\n\treturn content\n\n\ndef parse_xml(filepath):\n\timport lxml.etree as etree\n\ttree = etree.parse(filepath"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content.isspace():\n\t\t# Nothing to do - we assume this is an email address\n\t\treturn content\n\tlines = content.splitlines(True)\n\t# First line is always blank which defines a localhost prefix\n\tif len(lines) == 1 and lines[0].isspace():\n\t\t# Localhost prefix\n\t\tcontent = 'w:' + lines[0]\n\telse:\n\t\t# w=st or w-st\n\t\tst, sep, tail = lines[0].partition(':')\n\t\t# If there's a '?' in the tail strip"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch = [True]\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = (node.xpath(pubdate_xpath),)\n\t\tmatch_xpath = pubdate[0]\n\t\tlast_found = node.xpath(match_xpath)\n\t\tif (\n\t\t\t\tnode.count(_tag) <= 6\n\t\t\t\tand node.count(last_found[0]) <= 1\n\t\t):\n\t\t\tmatch[0] = False\n\treturn match[0]\n\ndef find_titles(node, titles_xpath"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_path in pubdate_xpaths:\n\t\tmatch = re.match(r'^\\s*(\\d{4}\\-\\w{3}\\s\\d{2}\\s\\d{2}\\s\\d{4})', pubdate_path, re.VERBOSE)\n\t\tif match:\n\t\t\treturn match.group(1)\n\tassert False, \"pubdate_xpaths list is empty, please check\"\n\ndef get_all_loc_nodes(text, elem_xpaths):\n\"\"\"\n\tReturns an"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tif re.search(pubdate_xpath, node) is not None:\n\t\t\treturn node.text\n\treturn \"\"\n\n\ndef generate_rss(rss_dir, out_dest):\n\troot = Element(\"rss\")\n\trss_id = find_rss_id(rss_dir)\n\troot.id = rss_id\n\titem_node_xpaths = [\n\t\t# \"//item[@id='%s']/title/*\" % item_id\n\t\t\"//"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatches = []\n\tfor pattern in pubdate_xpaths:\n\t\tmatches.extend(regex.findall(pattern, node.get_text()))\n\tmatches = list(filter(None, matches))\n\treturn matches[0]\n\ndef add_link(node_list):\n\t\"\"\"\n\tadds a link to a pubdate link if not existing in the node list\n\t\"\"\"\n\tif len(node_list) == 0:\n\t\treturn\n\tif len(node_list) > 1:\n\t\traise Exception(\"Link should"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tpubdate_xpath = node.xpath(pubdate_xpaths[0])\n\tif pubdate_xpath:\n\t\tfor pubdate_element in pubdate_xpath.iter():\n\t\t\tif \"start-date\" in pubdate_element.get():\n\t\t\t\treturn pubdate_element\n\treturn\n\n\ndef get_text_from_node(node, node_xpath_list):\n    \"\"\"\n    Prints the text that the element has and returns as text.\n\n    If there are more than one element in a given node, this function\n    output"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\ttry:\n\t\tmatch = next((x for x in pubdate_xpaths if x[1] == node.tagname), None)\n\t\tif match:\n\t\t\treturn match\n\t\traise KeyError('Unrecognized element:'+ node.tagname)\n\texcept StopIteration:\n\t\traise KeyError('No match')\n\ndef process_article(node):\n\t# The data\n\tauthors_xpaths = node.xpath('//span[@role=\"author\"]')\n\t# The date\n\tpubdate_xpaths = node.xpath"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch = ''\n\tfor pattern in pubdate_xpaths:\n\t\tmatch = re.compile(pattern).findall(node.get_text())\n\t\tmatch = [m for m in match if m in match[0]]\n\t\tif len(match) == 1:\n\t\t\tbreak\n\ttext = node.get_text()\n\n\tif not match:\n\t\traise \"Unable to find match with pattern {0}: \".format(pattern) + text\n\n\tmatch = match[0]\n\n\treturn match\n\n\ndef match_name(node, name_xpaths"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\t# Find any matches\n\tmatches = [k for k in pubdate_xpaths if re.search(node, k)]\n\tif len(matches) == 0:\n\t\treturn None\n\telif not matches[0]:\n\t\treturn None\n\telse:\n\t\t\n\t\t# Use the first found match\n\t\tfor date_xpath in matches:\n\t\t\tdate_xpath = date_xpath.replace('/', '//*[@id=\"_\"]/'+date_xpath.split('/')[1].split('_')["}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tnode = node.xpath(pubdate_xpath)[0]\n\t\t_, matches, _ = node.iter_matches([(re.compile(pubdate_xpath), 1)])\n\t\tif matches:\n\t\t\treturn matches[0][1]\n\t\telse:\n\t\t\treturn ''\n\t\n\treturn ''\n#-----------------------------------------------------------------------------------------\n\npubdate_xpaths = (\n\t'/a/span',  # Year\n\t'/a[1]/span',  # Year\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\tresponse = urlopen(\n\t\t\t\tnode.get(node.find(xpath=pubdate_xpath)).attrib['data-id'])\n\t\t\tsoup = BeautifulSoup(response, \"html.parser\")\n\t\t\tif 'dateTime' in str(soup.head):\n\t\t\t\treturn str(soup.head).split(\"</head>\")[0].split(\"<title>\")[1]\n\t\t\telif'version' in str(soup.head):\n\t\t\t\t"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = ''\n\tsupply = ''\n\tfor number_elem in issue.find():\n\t\tif number_elem.tag == 'number':\n\t\t\tif number_elem.text.strip().find(issue.number)!= -1:\n\t\t\t\tnumber = number_elem.text.strip().replace('\uff0e', '')\n\t\telif number_elem.tag =='suppletion':\n\t\t\tif number_elem.text.strip().find(issue.number)!= -1:\n\t\t\t\tsupply = number_elem.text.strip"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumType = None\n\tsupplType = None\n\n\t# check if the value of question element contained the issue label\n\tif not issue.label and not issue.html:\n\t\treturn [], None, None, None, None, None\n\t# Check if the question elment contains <label> and <strong> tag\n\telif issue.label == '-':\n\t\tnumType = issue.labels[0].text\n\t\tsupplType = issue.labels[1].text.lower()\n\n\treturn [numType, supplType]\n\n\ndef add_issue_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tif not issue.body or '<b' not in issue.body:\n\t\treturn (None, None)\n\n\t# extract the number and unit\n\tbody = _html_decode(issue.body)\n\tre_number = r'\\D+'\n\ttry:\n\t\tre_number_match = re.finditer(re_number, body, re.IGNORECASE)\n\texcept:\n\t\tre_number_match = []\n\n\tif re_number_match and len(re_number_match) == 1:\n\t\tnumber = re_number"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t# Extract the issue details element for the product\n\tif issue.xpath(\".//details/details-field>@scope\"):\n\t\tproduct_code = issue.xpath(\".//details//field/field-code\").extract_first()\n\t\tproduct_name = issue.xpath(\".//details/description/descendant-or-self::field-name\").extract_first()\n\t\tunit_name = issue.xpath(\".//details/description/abbreviation/text()\").extract_first()\n\t\tunit_value = issue.xpath("}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumeral = issue.xpath(\n\t\t'.//td[1]//text()|.//td[2]//text()|.//td[3]//text()|.//td[4]//text()|.//td[5]//text()|.//td[6]//text()|.//td[7]//text()|.//td[8]//text()'\n\t)\n\tsupp = issue.xpath('.//td[10]//text()')\n\tval = list(filter(None, set(numeral) -"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tsuppL = ''\n\tnumber = ''\n\tif 'number' in issue and issue['number']:\n\t\tnumber_value = issue['number']\n\n\t\tif issue['number'] and  isinstance(number_value, basestring):\n\t\t\tnum = int(number_value)\n\t\t\tif  num >= 0:\n\t\t\t\tnumber = '0' + str(num)\n\t\t\telse:\n\n\t\t\t\t#  If we have a negative number in it, we'll remove it and start from the next iteration till the current number is the same\n\t\t\t\ttry:\n\t\t\t\t\tnum = int(number"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_values = []\n\tfor issue_type in ('#issue-number', '#number', '#number-suffix',\n\t\t# TODO: Remove support the other values\n\t\t'modules.issue-number','mod.issue-number','mod.number'):\n\t\ttry:\n\t\t\tif not getattr(issue, issue_type):\n\t\t\t\tcontinue\n\t\t\tnumber = getattr(issue, issue_type)\n\t\t\tif number:\n\t\t\t\tnumber_values.append(number)\n\t\texcept TypeError:\n\t\t\tpass\n\treturn number_values\n\n\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tissue_title = \"Issue {0} : {1}\".format(issue.number, issue.raw_title)\n\tlog.debug(issue_title)\n\tif issue.raw_summary:\n\t\tissue_summary_soup = BeautifulSoup(issue.raw_summary, 'html.parser', from_encoding='utf-8')\n\t\tissue_summary = issue_summary_soup.text\n\t\tlog.debug(issue_summary)\n\t\tif issue_summary and issue_summary.strip()!= '' and issue_summary not in issue_summary_s"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = issue.find(\"cc-num\", class_=\"bloko-comp-num\").text\n\tsuppl = issue.find(\"cc-text\", class_=\"bloko-comp-text\").text\n\treturn (number, suppl)\n\n\"\"\"\n    Parse the html source of issue elements and extract all the required information \n    e.g. from the div of the textbox at <div class=\\\"bloko-comp-text\\\"> issue element\n    extract the <a class=\\\"bloko-comp-link text-mso-default\" href=\\\"//www.bl"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tmapping_number = {\n\t\t'0': 0,\n\t\t'1': 1,\n\t\t'2': 2,\n\t\t'3': 3,\n\t\t'4': 4,\n\t\t'5': 5,\n\t\t'6': 6,\n\t\t'7': 7,\n\t\t'8': 8,\n\t\t'9': 9\n\t}\n\tmapping_suppl = {\n\t\t'L': 'L',\n\t\t'M': 'M',\n\t\t'B': 'S',\n\t\t'D': 'D',\n\t\t'C': 'C',\n\n"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tline = ''\n\tfor m, k, v in self.iter_pairs():\n\t\tline += '\\n' + indent*' '+ m.pretty(indent+1)\n\t\tif not v:\n\t\t\tprint(line)\n\t\t\tline = ''\n\tif line:\n\t\tprint(line)\n\n\n#--------------------------------------------------------------\n\n\nclass ClassTest(unittest.TestCase):\n\tdef runAll(self, *args):\n\t\tclass_name = self.__class__.__name__\n\t\tall_tests = _walk_class(self."}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif not debug:\n\t\tprint('>>> Pretty('+ self.__class__.__name__ +')\\n')\n\t\tfor line in self.__str__().splitlines():\n\t\t\tprint(indent *''+ '|' + (''* indent) +''+ line)\n\telse:\n\t\tprint(indent *''+ '|' + (''* indent) +''+ str(self))\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tres = \"|\"\n\tfor row in self.headers:\n\t\tres += row.split(\"|\")[indent] +\"|\"\n\tres = res[:-indent]\n\treturn res + \"|\"\n\n# endregion definitions\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tline = ''\n\tfor key in self._fields:\n\t\tif self._fields[key]!='': # field is not None\n\t\t\tif indent+2 > 30: # maximum indent\n\t\t\t\ts = line_template % (key,indent)\n\t\t\t\tline += s\n\t\t\telse:\n\t\t\t\tline = line_template % (key,indent+2)\n\t\t\t\tline +=''*indent\n\t\t\t\tline += '{'\n\t\t\t\tline += self._pretty(self._fields[key],indent+2,debug,key=key)\n\t\t\t\tline += '}'"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug: print(indent*' ', self)\n\t#\n\tif len(self)!= len(set(self)):\n\t\traise ValueError('{} not unique'.format(str(set(self))))\n\tout = []\n\tfor i in range(len(self)):\n\t\to = self[i]\n\t\tout.append('{}{}'.format(indent*' ',o))\t\n\treturn print(' = '.join(out))\t\t\n\n#\n# Functions for serializing dictionies.\n#\ndef load(d, strict="}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ts = \"\"\n\tif debug:\n\t\ts += \"Parameters : \"\n\t\tfor n, p in self.params.items():\n\t\t\ts += \", \" + str(n) + \" : \" + str(p)\n\t\ts += \"\"\n\t\ts += \"Fitting values : \"\n\t\tfor k, v in self.fit_values.items():\n\t\t\ts += \", \" + str(k) + \" : \" + str(v) + \", \"\n\t\ts += \"\"\n\tif debug:\n\t\ts += \"Parameters : \"\n\t\ts += ', '.join(\""}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\tprint >>sys.stderr, self\n\treturn self\n\nclass Parser:\n\t\"\"\"\n\t\tA representation of a parser (used by Parse)\n\t\t\n\tParse parses the command line using the argparse module, allowing them\n\t\tto be used inside the main code.  A file like object can be passed,\n\t\tor the contents of a string/file can be used as argument.\n\t\"\"\"\n\tdef __init__(self, filename, filesrc = None, debug=0, add_help = False):\n\t\t\"\"\"\n\t\t\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\treturn self._pretty(indent, debug)\n\telse:\n\t\treturn self._pretty(indent)\n\n\nclass T(type):\n\t\"\"\"\n\t\tA metaclass to automatically construct class instances on this class (based on the type name from cls).\n\t\"\"\"\n\tdef __new__(cls, name, bases, class_dict):\n\t\t\"\"\"\n\t\t\tFunction to determine if the class can be a base class.\n\t\t\tIf True then the self.is_base_class is set to True.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tres = \"{:\" + \"(%s)\"  % \"{:0>%dd}\".format(len(self.keys)) + \" = {\",\"\n\tfor key in self.keys:\n\t\tif(key!= self.getEntry(0)):\n\t\t\tres += \"{:0>%ds}:{:0>%ds},\" % (indent, indent, key)\n\tres += \"}\"\n\tif(debug):\n\t\tprint(res)\n\treturn res\n\t\n#==========================================================================\n#\n#\tFunction to check if a key is a binary prefix of another"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\tlines, indent = inspect.getouterframes(inspect.currentframe())[indent]\n\t\tlines = [lines]\n\tdef pretty(self, indent=0, debug=False):\n\t\tif debug:\n\t\t\tindent += 5\n\t\t\tstrs = ['{indent}class:']\n\t\t\tstrs.append('{indent}{objp})'.format(indent=indent,objp='self'))\n\t\t\tindent += 4\n\t\t\tstrs.append('{indent}{objp}:'.format(indent="}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\timport itertools\n\t\n\tdef __absorb_arg(args):\n\t\tfor arg in args:\n\t\t\tif arg.rel or arg.neg:\n\t\t\t\treturn list(args)\n\t\t\telse:\n\t\t\t\tfor _ in range(arg.arg):\n\t\t\t\t\tyield arg\n\t\t\t\t\n\tif len(args) == 1:\n\t\tfor arg in args:\n\t\t\tif arg.neg:\n\t\t\t\tfor idx, expr in enumerate(arg.as_seq()):\n\t\t\t\t\tfor expr_i, expr_i in enumerate(arg.as_seq()):\n"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\t# TODO: generalise the above process for any input sequence\n\t\n\t# TODO: incorporate all non-trivial cases here\n\t\n\t# return the absorbtion and negatives\n\tif not args:\n\t\treturn self\n\telif len(args) == 1:\n\t\texpression = args[0]\n\t\treturn expression * self.absorption() + (self - self) * self.negativity()\n\telse:\n\t\treturn Absorption(args)\n\t\n\t\nclass Relation:\n\tdef __init__(self, args, target"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tabsorptions, negative_absorptions = list(map(Absorption, args))\n\n\t# The term `B` is absorbed in the expression\n\texpression = And(List(absorptions), or_(Not(B), Not(Not(B))))\n\t# The term `A` is absorbed in the expression\n\t# Note: We can combine B & A since if the one of them was positive\n\t# the other one was negative. We have to combine the two possibilities\n\t# for A into B.\n\treturn positive_absorption(expression, or_, and_,"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif len(args) <= 2:\n\t\traise ClaripyIndexError(\"too few arguments to absorption\")\n\tif len(args) == 2:\n\t\tA = args[0]\n\t\tB = args[1]\n\telif len(args) == 3:\n\t\tA = args[0]\n\t\tB = args[1]\n\t\tC = args[2]\n\telse:\n\t\traise ClaripyIndexError(\"too many arguments to absorption\")\n\t# print(A, B, C)\n\tdef process(expr, A, B, C, sign"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn list(self.absorb_seq(args, lambda x, y: self.absorb_withprob(x | y), lambda x, *y: x | self.absorb(args, *y) - self.absorb_withprob(x & y)))\n\t\n\tdef absorb_seq(self, seq, partial_resurrect, absorb, return_result):\n\t\t\"Given a sequence of arguments, return the resulting set of arguments.\"\n\t\tif len(seq) == 0:\n\t\t\treturn []\n\t\telse:\n\t\t\tnewargs = self._"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tout = []\n\tif type(args[-1])!= list:\n\t\toutlist += [args[-1], args[-1]] + args[:-1]\n\t\targs[:-1] = []\n\telse:\n\t\tfor a in args[-1]:\n\t\t\toutlist += [a, args[-1]] + args[:-1]\n\t\t\targs[:-1] = [a]\n\t\tout += list(args[-1]) + list(args[:-1])\n\treturn list(out)\n\t\n\t\ndef neg(args, is"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tA = 0.95\n\tB = 0.05\n\tL = len(args)\n\tresult = [a.__absorb_expr__(self) for a in args]\n\tresult.extend([self.__neg_expr__(a) for a in args])\n\tif L == 0:\n\t\treturn result\n\tif 1-B >= (L-1) / self.T*L:\n\t\tresult.extend(self.__absorbs(args))\n\t\treturn result\n\treturn [absorb_expr_implicit(a.args,"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tresult = []\n\tfor arg in args:\n\t\tif arg.has('A'):\n\t\t\t# abs(A) or abs(A | B) or abs(A | ~B)\n\t\t\tresult.append(arg.apply(lambda expr: -expr))\n\t\telif not arg.has('B'):\n\t\t\t# abs(A) or A | B\n\t\t\tresult.append(arg.apply(lambda expr: self.A() | expr))\n\t\telif arg.has('~'):\n\t\t\t# A | (A | ~B) or ~(A &"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif argparsed: # @TODO: check if arguments are correct type for some arguments/args,\n\t\t\t#     just see what happens.\n\t\tself.args = args\n\n\tif len(args) > 1:\n\t\t\"\"\"\n\t\t\tThis is not the only way to get absorption from the current state\n\t\t\tin a program. It's a more interesting way to get absorption.\n\n\t\t\tExamples::\n\t\t\t\n\t\t\t\tself.args = [\n\t\t\t\t\t'A & A | B',\n\t\t\t\t\t'A & B | (A | B) = A, A | (A & B)"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\tif len(args) == 0:\n\t\traise ValueError(f\"absorb(1) requires at least 1 argument\")\n\telif len(args) == 1:\n\t\treturn 1 if args[0].is_true() else -1\n\telse:\n\t\treturn 2 * self._add(args)\n"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tlog.info(\"[On] Hook name: {}. Handler callable: {}\".format(hook.name, getattr(hook, 'handler', None)))\n\tself.on_names[hook.name] = hook\n\treturn hook.handler\n\n\ndef on_register(**kargs):\n\t\"\"\"\n\tEnsure that all supplied hook methods are run with the default hook.\n\t\n\tArgs:\n\t    kwargs: All keyword arguments will be treated as valid, using\n\t    their default values.\n\n\tReturns:\n\t    None\n\t\"\"\"\n\tif has"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef add_listener(_cls, handler, *args, **kwargs):\n\t\t\"\"\"\n\t\tAdd listener for a hook.\n\n\t\tThe listener is added to handlers of the specified type.\n\n\t\tArgs:\n\t\t    hook (HookType): Attribute to the hook type for which to add\n\t\t\tthe listener.\n\t\t    _cls (type): Type of the listener to be added.\n\t\t    *args (tuple): Positional arguments for the listener.\n\t\t    **kwargs (dict): Keyword arguments for the listener.\n\t\t\"\"\"\n\t\tname = hook."}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef _internal_new(self):\n\t\tself.listeners.append(_NewListener(self, hook))\n\t\treturn self\n\t_internal_new = Internal(doc=doc).register(\n\t\t_internal_new, name=name, scope=None,\n\t\targ_names=list(arg_names), arg_names_set=arg_names_set,\n\t\treturn_null=return_null, doc=doc, args_as_params=args_as_params\n\t)\n\treturn _internal_new\n\nclass _NewListener(HookType,"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef outer(handler):\n\t\tif isinstance(hook, HookType):\n\t\t\told_hook = getattr(handler, '_cached_hook', None)\n\t\t\tassert old_hook is None\n\t\t\thook._cached_hook = handler\n\n\t\tdef _fn(function, *args, **kwargs):\n\t\t\tif isinstance(hook, HookType):\n\t\t\t\thandle = getattr(handler, '_cached_hook', None)\n\t\t\t\tif handle is not None:\n\t\t\t\t\th = handle\n\t\t\t\t\tif not isinstance(h, HOOK):\n\t\t\t\t\t\traise TypeError"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t# Don't forget the parent.\n\tparent = self.parent\n\t\n\tif parent:\n\t\tsetattr(\n\t\t\tparent,\n\t\t\thook,\n\t\t\tDecorators.Hooks_set(\n\t\t\t\t\"on\",\n\t\t\t\t[\n\t\t\t\t\tDecorators.Hook_Event(\"on_{{HookName}}_{{hook}}\".format(hook=hook)),\n\t\t\t\t\tDecorators.Hook_Event(\"on_{{HookName}}_{{hook}}_before\".format(hook=hook)),\n\t\t\t\t\tDecorators.Hook_Event(\"on_{{H"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif not hasattr(hook, \"HookType\"):\n\t\tprint \"WARNING: Attempting to register as a new HookType: \" + str(hook)\n\t\tsys.exit(1)\n\tself._registered[hook] = 1\n\treturn hook\n\n\n# --- MAIN ---\n\n\ndef register(hook):\n\t\"\"\"\n\tRegisters a method in the registry on all HookTypes.\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the method.\n\t\n\tReturns:\n\t    None\n\t\"\"\"\n\tfor key"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif not isinstance(hook, HookType):\n\t\traise TypeError(\"hook must be a HookType.\")\n\tif self.count() == 0:\n\t\tself._add_hook(hook)\n\treturn hook\n\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif hasattr(hook, \"name\"):\n\t\tdef wrap(arg):\n\t\t\t# check if already registered\n\t\t\tif hook.get_name() in HOOKS:\n\t\t\t\t# if not add handler\n\t\t\t\tLOG(\"A previously registered hook has already been added.\")\n\t\t\telse:\n\t\t\t\t# else add the listener\n\t\t\t\tif hasattr(hook, \"type\"):\n\t\t\t\t\tLOG(\"Register callback\")\n\t\t\t\t\tif hook.type == HOOK_TYPE_HOOK_NAME:\n\t\t\t\t\t\tLOG(\"Wrapper registered\")\n\t\t\t\t\t\tHOOK_META = []\n\t\t\t\t\t\tself.add_"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef new_decorator(func):\n\t\tself.registry.append((hook, func))\n\treturn new_decorator\n\ndef remove(hook: HookType, remove_hook: Callable, remove_registry: Callable = None) -> None:\n\t\"\"\"\n\tRemove a listener that listens to changes on a hook attributes, and return.\n\t\n\tThe hook attribute changes will be deleted on exit.\n\t\n\tArgs:\n\t    hook (HookType): Hook to listen to.\n\t    remove_hook (Callable): Function to remove."}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif isinstance(hook, HookType):\n\t\tregistry.listeners[hook.name] = hook\n\t\treturn hook\n\telse:\n\t\traise TypeError(hook_type, 'Type of argument must be str or HookType')\n\ndef on_event(event):\n\t\"\"\"\n\tDecorator function to add a listener to the event.\n\t\n\tArgs:\n\t    event(ObjectRef): Event parameter.\n\t\n\tReturns:\n\t    callable: A Handler function decorated as well as the event type.\n\t\"\"\"\n\tdef _(hook):"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {\n\t\t\"user\": user,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t}\n\n\treturn config\n\n\ndef base_config_with_options(\n\thost,\n\tport,\n\tetcd_host,\n\tetcd_port,\n\tkey_prefix=\"my_app\",\n\tcidr=\"0.0.0.0/0\",\n\tsecurity=False,\n):\n\t\"\"\"\n\tCreates a configuration with some simple parameters.\n\t\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn dict(\n\t\tuser=user, etcd_host=etcd_host, etcd_port=etcd_port,\n\t\tdb_host=\"\", db_port=26379, db_user=\"\", db_pass=\"\",\n\t\tdb_type=None, db_index=None, db_ignore=None, db_sync=None,\n\t\tdb_sync_max_diff=2000, db_sync_per_sec=3, db_max_diff=2,\n\t\tdb_max_idle=500, db_interval=1, db"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tapp = {\n\t\t'auth': {\n\t\t\t'django.db': {\n\t\t\t\t'user': user,\n\t\t\t\t'password': hash(user),\n\t\t\t\t'host': etcd_host,\n\t\t\t\t'port': etcd_port,\n\t\t\t\t'cert_cert': True\n\t\t\t},\n\t\t\t'root': {\n\t\t\t\t'static': [\n\t\t\t\t\t'root/passwords_etcd/root/.*'\n\t\t\t\t],\n\t\t\t\t'strict': True\n\t\t\t}\n\t\t}\n\t}\n\n\tconfig = {}\n\tconfig['auth']['root']["}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconf = {\n\t\t'client': {\n\t\t\t'user': user,\n\t\t\t'password': ''\n\t\t},\n\t\t'database': {\n\t\t\t'host': etcd_host,\n\t\t\t'port': etcd_port\n\t\t},\n\t}\n\treturn conf"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tparams = {\n\t\t\"user\": user,\n\t\t\"host\": etcd_host,\n\t\t\"port\": etcd_port\n\t}\n\treturn params\n\nclass BaseConfig(Config):\n\t\"\"\"\n\tConfiguration for the user.\n\t\"\"\"\n\tuser = \"root\" # the name of the user\n\tpasswords_dir = \"/etc/passwords\" # the name of the passwords file. Default value is the root password for root. Passwords can be read from /etc/passwords/login.conf\n\tconfig = None # the current configuration.\n\tlogin"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfiguration = {\n\t\t\"user\": user,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"is_ssl\": True,\n\t\t\"password_required\": False\n\t}\n\treturn configuration\n\n\t\n\t\ndef create_auth_credentials(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\t\tCreates the authentication credentials for a system.\n\t\tIf the credential does not exist, the credentials are created.\n\t\tThe credentials are loaded/created"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\n\t\t'server_name':'server',\n\t\t'server_name_user': user,\n\t\t'username': None,  # use the current username\n\t\t'reg': None,     # the server auth for this server?\n\n\t\t'clients': {  # client dicts\n\t\t\t}\n\t}\n\t#\n\n# create base config\nconfig = base_config(user='guest')\n\n# Add the rest of the config to the base one.\nwith open('./utils/config.json') as f:\n\tfile_data = json."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {\n\t\t\"user\": user,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"default\": {\"name\": \"config\", \"value\": \"{{config}}\"}\n\t}\n\treturn config\n\n\nconfig = base_config(\"root\", etcd_host=\"localhost\", etcd_port=2379)\n\nlogging.info(\"Using base config: %s\", config)\n\n\n## Static methods\n\n## Base Class ##\n\nclass User(ABC):\n\t\"\"\"\n\tClass defining"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tdata = {\n\t\t\"api\": etcd_host,\n\t\t\"api_port\": etcd_port,\n\t\t\"auth-type\": \"http\",\n\t\t\"auth-user\": user,\n\t\t\"auth-password\": \"password\",\n\t\t\"auth-ca-file\": \"\",\n\t\t\"auth-key-file\": \"/etc/ansible/ssh/private\",\n\t\t\"logging\": logging._stdlib.LOGGER_FMT.format(color=\"blue\", method=\"GET\"),\n\t\t\"caching\": False,\n\t\t\"hashing\": False,\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {}\n\n\tconfig['host'] = etcd_host\n\tconfig['port'] = etcd_port\n\tconfig['login_user'] = user\n\tconfig['login_pw'] = user\n\n\treturn config\n\n\nclass EtcdObject(object):\n\t\"\"\"\n\t\tA ETLS object. A tree structure is structured very similar to a dictionary.\n\t\tIt will contain a common structure for the database, the path\n\t\tand a configuration dictionary. This structure is only\n\t\tconfigurable during a new object's creation.\n\t\"\"\"\n\n\tclass EtcDictConfig("}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfrom django.utils.timezone import utc as utc_tz\n\tfrom django.utils.timezone import now as now_tz\n\t\n\tif not isinstance(dt, datetime.datetime):\n\t\traise ValueError\n\tif not dt.tzinfo:\n\t\tdt = dt.replace(tzinfo=utc_tz)\n\t\n\tif dt!= now_tz():\n\t\tdt = utils.get_iso_now():\n\t\treturn dt.astimezone(self.local_timezone)\n\t\n\td"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\tt = dt.utcoffset()\n\t\tdt = dt.replace(tzinfo=None) - t\n\tif dt > utc_offset:\n\t\tdt -= utc_offset\n\tfold = False\n\tif dt < utc_offset:\n\t\thour_offset = fold = True\n\t\tdt, fold = _utcoffset(dt - utc_offset)\n\telif dt < utc_offset + utc_fuzz:\n\t\thour_offset = fold = True"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# TODO : remove this and change the code below\n\t# to a pure python implementation?\n\t\n\t# Note that we *can't* do \"isinstance(dt, datetime.tzinfo)\" here because\n\t# we need to convert both values of the datetime attr, dt.date, dt.timetuple\n\t# and dt.tzinfo to Datetime with pytz (or the other way around).\n\t#\n\t# Since the first argument to _fromutc is datet or None, an attempt\n\t# to convert it to a dat"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tz.tzlocal())\n\t# if dt is a datetime, it has no tzlocal attribute (i.e. it is UTC)\n\t# but it has pytz and it has timezone information, so its \"safe\"\n\treturn dt.astimezone(tz.tzlocal())\n\n\ndef _tzname_in_utc(name):\n\t\"\"\"\n\tConverts Python name into UTC-like timezone name that is unlikely\n\tto have a tz"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not hasattr(dt, 'tzinfo'):\n\t\tdt = dt.replace(tzinfo=UTC)\n\ttry:\n\t\tdt = dt.astimezone(local=self._local)\n\texcept (OverflowWarning, ValueError, UnicodeEncodeError):\n\t\t# We might happen if:\n\t\t#  - the original datetime contains timezone information\n\t\t#  - the datetime has UTC information\n\t\t#  - the datetime it was aware have no timezone information\n\t\t#  - 'now' is not daylight savings time (usually its the"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t_check_for_ambiguous(dt)\n\t\n\tunambig = dt.utcoffset().total_seconds() <= 0\n    \n\tif unambig:\n\t\tdt = dt.replace(tzinfo=None)\n\t\tif dt == dt.utcoffset():\n\t\t\treturn dt\n    \n\tfold = dt.dst() is None\n\tdt_folded  = dt.replace(fold=fold)\n    \n\tfraction_of_day   = dt.hour * 3600 + dt.minute"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tdatetime_object = getattr(dt, \"astimezone\", dt)\n\n\tif isinstance(datetime_object, datetime):\n\t\tdt_datetime = datetime_object\n\telse:\n\t\tdt_datetime = getattr(datetime_object, \"datetime_field\", dt_datetime)\n\n\tif dt_datetime.tzinfo is not None:\n\t\tlocal_tzoffset = dt_datetime.tzinfo.utcoffset(dt_datetime)\n\t\tlocal_timedelta = local_tz"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not self._utc_fold:\n\t\traise ValueError(\"timezone cannot handle ambiguous timezone\")\n\t\t# TODO - this is still wrong; it is incorrect, we should fix\n\t\t# a more generic \"fold\" state that mandates UTC timepicker\n\t\t# for ambiguous timezones.\n\t\n\tst = dt.tt_zoned.astimezone(self._utc_fold)\n\tif st.tzinfo is None:\n\t\t# TODO - should we get back the correct fold state?\n\t\traise ValueError(\"timezone must support folding\")"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\treturn dt.replace(tzinfo=None)\n\t\n\t\nclass _BaseTimezone(object):\n\t\"\"\"\n\tPossibly a base class for classes that want timezone-aware datetime.\n\t\"\"\"\n\tZERO = py23_plus.PY23_INTEGER\n\t\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\t\tConstructs this base object.\n\t\t\"\"\"\n\t\tself.timezone = _BaseTimezone.ZERO\n\t\tself.utcoffset = _BaseTimezone.ZERO\n\t\tself.hasd"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt is None: return None\n\n\t# First, find out whether it's ambiguous.\n\tambiguous = False\n\ttry:\n\t\tdt_fold = dt.fold\n\texcept AttributeError:\n\t\tambiguous_dt = time.time()\n\telse:\n\t\t# Is it ambiguous?  Remember this fact that a timezone-aware datetime\n\t\t# object is ambiguous if both the datetime in question and the datetime\n\t\t# in the fold are ambiguous.\n\t\tif dt_fold == -1 and dt.utcoffset() is not"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(**self.__dict__)\n\nclass relativedelta(object):\n\t\"\"\"\n\tProvides addition, subtraction, and modulo operations on relativedelta\n\tobjects which support comparison, manipulation, and addition/subtraction\n\toperations.\n\t\n\tBoth :class:`relativedelta` instances are conforming to the\n\tdefinition of a \" \"relativedelta\" object whose \"sign\" refers to how the\n\toperations are performed.\n\n\t:param second:\n\t    An optional second input to ``numerator/"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn self.fromis(self.tois(0.0))\n\n\nclass relativedelta(datetime.timedelta):\n\t\"\"\"\n\tA relativedelta object represents the change to determine the\n\tdelta between two specific instances of a datetime, both of which\n\tperiodically doubles.\n\t\n\tThe :class:`relativedelta` object progresses by ``from_date``, and\n\tthen proceeds to calculate it according to the arguments given to\n\t:func:`dateoffset`.\n\t\n\tIt is currently subclassed from :class:`date"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdelta = self\n\tif delta.days < 0:\n\t\treturn - delta.year * 12.0 + (delta.days / 365.0)\n\telse:\n\t\treturn delta\n\ndef mx(d, y):\n\t\"\"\"\n\tCalculate the maximum occurrence of a date within a time block.\n\n\t:param d:\n\t:type d:\n\t:param y:\n\t:type y:\n\t:return:\n\t:rtype: int\n\t\"\"\"\n\tif not d: return\n\tif not y: return\n\n\tday"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(day=+1, hours=-14, minutes=-30, seconds=-1)\n\t\n\t\n__all__ = [\n\t'Absolute',\n\t'add',\n\t'AppliedRel',\n\t'AppliedRelDays',\n\t'AppliedRelHours',\n\t'AppliedRelMonths',\n\t'AppliedRelSeconds',\n\t'AppliedRelYears',\n\t'cache_result',\n\t'dateutil',\n\t'Date',\n\t'diff_days', \n\t'day',\n\t'date',"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t# Note: dateutil doesn't support negative days, and for dates before\n\t# the calendar year very obvious to me (i.e. -82 => -84)\n\t# So, we make it infinite by using a special case (now is negative)\n\tif self.is_leap_year():\n\t\tnow = datetime.today()\n\t\tdays_in_year = 366 if now.month == 1 else 365\n\telse:\n\t\tnow = datetime.today()\n\t\tdays_in_year = 365 if now.month == 12 else 366\n\n\t# Deal with"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tif (self.days!= days) or (self.hours!= hours):\n\t\traise TypeError(\"objects don't have the same type of relative attributes\")\n\trelative = relativedelta(seconds=self.seconds / 3600.0, microseconds=self.microseconds / 1000.0)\n\treturn relative\n\ndef r2d(r):\n\t\"\"\"\n\tReturn a version of r, which is relative to now().\n\n\t>>> r = relativedelta(months=2)\n\t>>> r2d(r)\n\trelativedelta(months=+2)"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdef normalize(self):\n\t\t\"\"\"\n        \n        Normalize the object self.\n        \n        :return:\n        \tReturns a :class:`dateutil.relativedelta.relativedelta` object.\n        \"\"\"\n\t\treturn relativedelta(self)\n\t\t\n\tdef __abs__(self):\n\t\treturn dateutil.relativedelta.relativedelta(self)\n\tdef __float__(self):\n\t\treturn self.days * 24 + self.hours + self.minutes / 60.0 + self.seconds / 3600"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn self.today\n    \n  \n    def to(self, dt):\n        \"\"\"\n\tConvert a :class:`date` or a :class:`datetime` to an aware\n\t:class:`date`.  The given ``dt`` should be date or datetime object,\n\tand will only be converted to an aware datetime object.\n\n        If the ``dt`` is naive, then the tzinfo member will be set to\n        ``None``, unless the naive datetime is tz-aware.\n\t\n\t>>> from dateutil.relat"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn self.replace(years=divmod(self.years, 365)[0]).replace(months=divmod(self.months, 12)[0]).replace(days=divmod(self.days, 365)[0])\n\n\tdef __floor__(self):\n\t\treturn float(\"{:.2f}\".format(self.days//365*366 + self.seconds//3600/24))\n\n\tdef __ceil__(self):\n\t\treturn float(\"{:.2f}\".format((self.days+self.seconds+3600000)/365*366-1"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdef strp(self):\n\t\t\"Return string representation of this date value in an easy-to-parse format\"\n\t\treturn time.strftime(\"%Y-%m-%dT%H:%M:%S\", self._d).rstrip()+'Z'\n\tdef datetime(self):\n\t\t\"Return datetime representing current date, time, and time zone.\"\n\t\treturn self.replace(hour=0, minute=0, second=0, microsecond=0)\n\tdef __repr__(self):\n\t\treturn \"Date(year="}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef tzname_in_python3(name):\n\t\treturn utils.tzname(namefunc(name))\n\treturn tzname_in_python3\n\ndef from_string(s, sep=' '):\n\t\"\"\"\n\tGenerator of unicode string value from an integer value. (e.g. '1' in python 3)\n\n\t* sep : string between every single char of the generator\n\t\"\"\"\n\ttry: return value(lambda arg: s.encode(encoding='utf-8')(arg))\n\texcept UnicodeEncodeError:"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn namefunc.im_func.__name__\n\n\ndef tzname_in_python3(namefunc):\n\treturn namefunc.im_self().__class__.__name__.upper()  # All names will be upper case\n\n\ndef tzoffset_in_python2(namefunc):\n\t\"\"\"\n\ttzoffset() uses UTC timezone as standard timezone\n\ttzoffset takes an offset(seconds) only\n\n\t>>> from datetime import datetime\n\t>>> from pytz import timezone as tz\n\t>>> from pytz.tzinfo import t"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\ttry:\n\t\tname = namefunc('tzname')\n\t\tif len(name) > 255:\n\t\t\tname = unichr(int(name[:3], 36)) + name[3:7].upper() + name[7:12].lower() + name[12:]\n\t\telse:\n\t\t\tname = name.title()\n\texcept:\n\t\treturn\n\treturn name\n\ndef pytzname_in_python2(namefunc):\n\t\"\"\"\n\ttzname in Python 2.x returns a string, while tzname in Python 3.x\n\twill"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\ttry:\n\t\tfrom zlib import compressobj\n\texcept ImportError:\n\t\t# We're on Python 3, fallback to the old name for compatibility\n\t\tif sys.version > '3':\n\t\t\tnamefunc = compressobj(level = 1).compress(namefunc)\n\t\telse:\n\t\t\traise NotImplementedError('namefunc does not work in Python 3')\n\telse:\n\t\t# Compress bytestring into a bytestring\n\t\tnamefunc = compressobj(level = 1).compress(namefunc)\t\t\n\ttry:\n\t\t"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif sys.version_info[0] == 2:\n\t\treturn namefunc\n\telse:\n\t\treturn namefunc().decode(\"utf-8\")\n\n\nclass TZSet:\n\t\"\"\"\n\t\tTZSet class\n\t\n\tUse this class instead of a naive timezone, in case you want a timezone\n\twith several clock zones.\n\n\tBy default, the timezone can be set to the string \"EST\" or \"EDT\"\n\t\"EST4\" and then set to 1:1 so that offsets are matched\n\t\"\"\"\n\tdef __init__("}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif PY2:\n\t\t# Python 2: unicode -> bytestring\n\t\tdef unicode_bytes(name):\n\t\t\treturn name\n\t\n\t# Python 3: unicode -> bytestring\n\t\tdef unicode_bytestring(name):\n\t\t\treturn name\n\t\n\tif PY35:\n\t\t# Python 3: unicode -> bytes\n\t\tdef unicode_bytes(name):\n#\t\t\tif name.isupper():\n\n\t\t\t# Python 3.3 compat.\n\t\t\tif not isinstance(name, str):\n\t\t\t\tname = name.decode(\""}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef name(name):\n\t\treturn unicodedata.east_asian_width(name)\n\n\tdef tzname(zone):\n\t\tif TzModule is None:\n\t\t\t# no tzdata (yet)\n\t\t\treturn name(zone.tzoffset)\n\n\t\ttry:\n\t\t\tresult = zone.tzname(None)\n\t\texcept tzfile.UnknownTZnameException:\n\t\t\treturn zone.tzname(namefunc)\n\t\texcept tzfile.BadTZnameException:\n\t\t\t# Sometimes pytz gives bad results when the name appears to\n\t\t\t#"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n\t\n\tdef __new__(cls, *args, **kwargs):\n\t\t\"\"\"\n\t\t\t__new__() - class creation  \n\t\t\t\n\t\t\tNew object is called when the namefunc() function of a new-style class\n\t\t\tis called\n\t\t\"\"\" \n\t\treturn super().__new__(cls, *args, **kwargs)\n\treturn staticmethod(new__new__)\n\ndef _tzset(tz):\n\t\"\"\"\n\t\t_tzset(tz) - set the current time-zone\n\t\t\n\t\ttzset() provides a way of"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn namefunc('ascii')\n\n\ndef gettz(name, stdout=None, stderr=None, tzcache_key=TZCACHE_KEY, verbose=0):\n\t\"\"\"\n\t\tReturns the TAIWIKI time zone for the given locale.\n\t\t\n\t\tIf output is not available, defaults to the system time zone.\n\n\t\t:param name: The name of the locale (e.g. 'en_US','sv_SE')\n\t\t:param stdout: If output is a string, set stdout == True to return"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef wrapper():\n\t\treturn unicode(namefunc(), 'iso8859_2', 'backslashreplace')\t\n\treturn wrapper\ntzname_in_python2 = tzname_in_python2(tzname)\n\n@ti.functions\ndef tzname_in_python3(namefunc):\n\tdef wrapper():\n\t\t# On Python3, the namefunc parameter is the name of the function\n\t\t# that was originally called. On Python2 it is the name of the\n\t\t# method that the variable was bound, or None.\n\t\ttry:"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I see no use for this method so I assume names do not change between\n\t# releases.\n\tversion = get_config_version()\n\tdist = get_distribution(\"setuptools\")\n\ttry:\n\t\treturn dist.get_versions()\n\texcept Exception:\n\t\treturn version\n\ndef strip_plugin_prefixes(plugin_names):\n\t\"\"\"Strip plugin prefixes from plugin ids.\n\n\tParameters\n\t----------\n\tplugin_names: list\n\t\tlist of plugin names\n\n\tReturns\n\t-------\n\tfiltered_plugin_names: list"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to the root. Some\n\t# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n\t# case we can only use expanded keywords.\n\n\tcfg = get_config()\n\tverbose = cfg.verbose\n\n\ttry:\n\t\treturn git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tpaths = [jsondir, resources, stringsdir]\n\tfor path in paths:\n\t\tif os.path.isfile(os.path.join(path, \"package.json\")):\n\t\t\t# file exists, can be imported, etc.\n\t\t\tversioninfo = {\n\t\t\t\t\"version_string\": get_version_string(),\n\t\t\t\t\"version_data\": get_version_data(path),\n\t\t\t}\n\t\t\t# if it uses setuptools, be more specific\n\t\t\tif usesetup:\n\t\t\t\tversioninfo[\"setuptools\"] = get_version_data"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to the root. Some\n\t# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n\t# case we can only use real files.\n\n\treturn get_versions()\n\n\ndef make_vh_params(options):\n\t\"\"\"\n\tmake a dictionary to store all vh_args, then update it with the command line options for all VH programs\n\tfor"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to the root. Some\n\t# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n\t# case we can only use expanded keywords.\n\n\tcfg = get_config()\n\tverbose = cfg.verbose\n\n\ttry:\n\t\treturn git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to the root. Some\n\t# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n\t# case we can only use expanded keywords.\n\n\tcfg = get_config()\n\tverbose = cfg.verbose\n\n\ttry:\n\t\treturn git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to the root. Some\n\t# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n\t# case we can only use expanded keywords.\n\n\tcfg = get_config()\n\tverbose = cfg.verbose\n\n\ttry:\n\t\treturn git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to the root. Some\n\t# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n\t# case we can only use expanded keywords.\n\n\tcfg = get_config()\n\tverbose = cfg.verbose\n\n\ttry:\n\t\treturn git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to the root. Some\n\t# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n\t# case we can only use expanded keywords.\n\n\tcfg = get_config()\n\tverbose = cfg.verbose\n\n\ttry:\n\t\treturn git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n\t# __file__, we can work backwards from there to this file -- This one is\n\t# automatically generated by the builder - unless we fit another\n\t# `_version.py` that sets __file__.\n\n\t# This file is present under the distribution package build\n\t# directory. Find the version string in this directory.\n\tglobal __version__\n\tg = re.search(r'^__version__ = [\\'\"]([^\\'\"]+)[\\'\"]"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\n\t\t\"version\": \"unknown\",\n\t\t\"full-revisionid\": pieces.get(\"long_revisionid\", None),\n\t\t\"dirty\": None,\n\t\t\"error\": pieces[\"error\"],\n\t\t\"date\": None,\n\t}\n\n\tvcs_backend = vcs.get_backend_for_dir(here)\n\n\tif style == \"vs7\":\n\t\tvcs_backend = vcs.get_backend(\"vs7\")\n\telif style == \"vs8\":\n\t\tv"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\t\"date\": None}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\t"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".post%d\" % pieces[\"distance\"]\n\t\t\telse:\n\t\t\t\trendered += \".pre%d\" % pieces[\"distance\"]\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".dev0\"\n\t\t\trendered += plus_or_dot(pieces)\n\t\t\trendered += \"g%s\" % pieces[\"short\"]\n\telse:\n\t\t# exception #1\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"a\"]:\n\t\treturn \"a\"+pieces[\"a\"]\n\telif pieces[\"short\"]:\n\t\treturn \"s\"+pieces[\"short\"]\n\telif pieces[\"dirty\"]:\n\t\treturn \"d\"+pieces[\"dirty\"]\n\telif pieces[\"branch\"]:\n\t\treturn \"b\"+pieces[\"branch\"]\n\telse:\n\t\treturn \"q\"+pieces[\"dirty\"]\n\ndef get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py,"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\trendered += \".post%d\" % pieces[\"distance\"]\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".dev0\"\n\t\t\trendered += plus_or_dot(pieces)\n\t\t\trendered += \"g%s\" % pieces[\"short\"]\n\telse:\n\t\t# exception #1\n\t\trendered = \"0.post%d\" % pieces[\"distance\"]\n\t\tif pieces[\"dirty\"]:\n\t\t\trendered"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\t\"date\": None}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\t"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"closest-tag\"] is not None:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\trendered += \".post%d\" % pieces[\"distance\"]\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".dev0\"\n\t\t\trendered += plus_or_dot(pieces)\n\t\t\trendered += \"g%s\" % pieces[\"short\"]\n\telse:\n\t\t# exception #1\n\t\trendered = \"0.post%d\" % pieces[\"distance\"]\n\t\tif pieces[\"dirty\"]:"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"]:\n\t\t\trendered += \"-{}\".format(pieces[\"distance\"])\n\telse:\n\t\t# exception #1\n\t\trendered = \"0\"\t\t# 0\n\treturn rendered\n\n\ndef render_pep440_branch(pieces):\n\t\"\"\"TAG[.postDISTANCE[.dev0]+g<accelerate>].\n\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"clos"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trendered = pieces[\"closest-tag\"]\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\tif pieces[\"dirty\"]:\n\t\t\t\trendered += \".dirty\"\n\t\t\trendered += \"+untagged.0\"\n\telse:\n\t\t# exception #1\n\t\trendered = \"-zero-version\"\n\t\tif pieces[\"distance\"]:\n\t\t\trendered += \".post0.dev0\"\n\t\t\trendered += describe_whitespace(pieces)\n\t\telse:\n\t\t\trendered += \".dev0\"\n\treturn rendered\n\n\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tvtag = {'v' : ''}\n\tif pieces['v']:\n\t\tvtag['v'] = pieces['v']\n\treturn render_etab('evrml', vtag, pieces, outputstyle=style, options=dict(style=style))\n\n\ndef parse_mode(ctx):\n\t\"\"\"\n\tParse the given mode declaration and return the corresponding\n\tPython code snippet of that mode.\n\t\"\"\"\n\tmode = dict(\n\t\tdebug=Context.mode_debug,\n\t\tnormal=Context.mode_normal,\n\t\tread=Context."}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.copy():\n\t\treturn \"+\"\n\treturn \".\"\n\n\ndef execute(cmd, args, verbose, hide_stderr, env, force, **kwargs):\n\t\"\"\"\n\t\texecute(cmd, args, verbose, hide_stderr, env, force, **kwargs)\n\t\"\"\"\n\tif verbose:\n\t\tprint(\"Executing: \" + cmd + \" \" + args)\n\tif hide_stderr:\n\t\tstderrfd, stdoutfd = os.pipe()\n\t\tset_loglevel"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.get(\"git_diff\", \"\"):\n\t\treturn \".\"\n\treturn \"+\"\n\n\ndef process_template(git_diff):\n\tglobal template, name\n\tif not template:\n\t\treturn ''\n\tif os.path.isdir(template):\n\t\treturn '!'+ template +''\n\n\tgit_diff_lines = repo_git_diff(\n\t\t\t\t\ttemplate, git_diff).splitlines()\n\tfirst_line = git_diff_lines[0]\n\tif first_line[0] is not'':\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.website  and not \"+\" in pieces.website:\n\t\treturn \"+\"\n\twhile \"+\" in pieces.website:\n\t\tlast_item = pieces.website.rindex(\"+\")\n\t\tpieces.website += pieces.website[:last_item] + \"+\"\n\treturn pieces.website\n\ndef build(pieces, args):\n\t\"\"\"\n\t\tBuild piece using argparse\n\t\t@params\n\t\t\tpieces: pieces file\n\t\t\targs: --html, --file out,.\n\t\t\"\"\"\n\n\tif args == '"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.get(\"closest-tag\", \"\"):\n\t\treturn \".\"\n\treturn \"+\"\n\ndef render_pep440(pieces):\n\t\"\"\"\n\tRender the PEP440 string with quotes and extra info (which depends\n\ton the number of days since last release).\n\t\"\"\"\n\tdate = \" \".join(pieces[\"releases\"][\"date\"])\n\treturn \"%s \\\\| %s %s\" % (\"+\" + date, \" \".join(pieces[\"releases\"][\"tag_date\"]), pieces[\"previous-"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.keys():\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\ndef make_version(pieces):\n\t\"\"\"Takes a version string\\n\n\t   e.g. \"1.0\" or \"1.2.0a1\" or \"1.2.3\" or \"1.2.3a2\"\\n\n\t   and returns an immutable id \"1.2.3a2\". * that means it has to be\n\t   reversed.\n\t   \"\"\"\n\treturn str(version(pieces))\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.copy():\n\t\treturn \"+\"\n\treturn \".\"\n\ndef execute(cmd, args, stage, **kwargs):\n\tif os.getcwd()!= getcwd():\n\t\tpath = os.path.abspath('./')\n\t\tprint(\"Current directory: \", path)\n\t\texit()\n\tprocess = subprocess.Popen(args, env=kwargs)\n\treturn process.wait()"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tn = 1\n\tfor segment in pieces:\n\t\tif segment!= \"*\":\n\t\t\tn += 1\n\treturn.u + str(n) if n!= 1 else \".\"\n\n\t\n\ndef get_peel_version():\n\tversion = \"\"\n\tfor i in range(3):\n\t\tversion += plus_or_dot(get_version_string())\n\t\tversion += \".\"\n\tversion += five_or_more_zeros(get_version_prefix_string())\n\treturn version\n\n\ndef update_version_file(version):\n\t\"\"\"\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.keys() and \".\" not in pieces.values():\n\t\treturn \"+\"\n\telif \".\" in pieces.keys() and \"+\" not in pieces.keys():\n\t\n\t\treturn \".\"\n\t\n\treturn \"\"\n\t\n\nfor line in open('input'):\n    #line = line.strip()\n    input_string.append(line)\n\ndef split_all_into_n(input_string):\n    \"\"\"\n    Split all the words on the spaces into lines.\n    \"\"\"\n    output_list = []\n    "}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"0\" in pieces.vsb()[0]:\n\t\treturn \".\"\n\treturn \",\".join(pieces.vsb())\n\ndef render_tree(t, depth=0):\n\tr = \"\"\n\tif depth % 4 == 0:\n\t\tr += '<br/>'\n\tel += '<br/>'\n\tfor entry in t:\n\t\tif entry.type not in ['branch','subbranch', 'path']:\n\t\t\tr += '<b>%s</b>:' % entry.name\n\t\t\tfor child in entry"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.keys():\n\t\treturn \"+\" \n\telse:\n\t\treturn \".\"\n\ndef main():\n\tparser = optparse.OptionParser()\n\tparser.add_option(\n\t\t\"-i\",\n\t\t\"--input_dir\",\n\t\tdest=\"input_dir\",\n\t\thelp=\"Input directory.\",\n\t\tmetavar=\"dir\",\n\t)\n\tparser.add_option(\n\t\t\"-o\",\n\t\t\"--output_dir\",\n\t\tdest=\"output_dir\",\n\t\thelp=\"Output directory.\",\n\t\tmetavar=\"dir"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t# load the command module (NOTE: override subclasses to use real command module)\n\tmodule = _get_command_module(commands, verbose, hide_stderr)\n\targs=[] if args is None else args\n\tmsg = 'running {0}'.format(' '.join(commands + args))\n\twith module.assert_not_called():\n\t\tmodule.run_command_environ_update(env)\n\treturn module.run_command(args, cwd=cwd, verbose=verbose, hide_stderr=hide_"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd_args = get_command_args(args, True)\n\tfor cmd in commands:\n\t\tcall_pip(cmd, cmd_args, cwd, verbose, hide_stderr, env)\n\ndef call_pip(args, additional_args=None, index_urls=None, index=False,\n                   verbose=False, hide_stderr=False, env=None):\n\t\"\"\"Call pip with the --verbose flag.\"\"\"\n\tif hide_stderr:\n\t\tstderr_level = logging.WARNING\n"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tassert isinstance(commands, list)\n\tout, rc = None, None\n\tif args:\n\t\ttry:\n\t\t\tout, rc = commands[0](args, cwd, verbose, hide_stderr, env)\n\t\t\tfor execution in commands[1:]:\n\t\t\t\tout, rc = execution(args, cwd, verbose, hide_stderr, env)\n\t\t\t\tif rc not in (0,1):\n\t\t\t\t\tout, rc = None, None\n\t\t\t\t\tbreak\n\t\texcept (BadJobID, OSError) as error:\n\t\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tassert isinstance(commands, list),\"commands must be a list of commands\"\n\tprocess = None\n\tfor command in commands:\n\t\ttry:\n\t\t\tif verbose:\n\t\t\t\tprint(f\"Running {command}\", end=\"\")\n\t\t\tif isinstance(command, list):\n\t\t\t\tcommand_parts = command\n\t\t\telse:\n\t\t\t\tcommand_parts = shlex.split(command)\n\t\t\tif args is None:\n\t\t\t\tproc = sp.Popen(command_parts, stdout=sp.PIPE, stderr=sp.STDOUT, cwd="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t# load the environment\n\tenv = {}\n\tif env:\n\t\tenv = dict(env)\n\telse:\n\t\tenv = os_profile_get_env(cwd=cwd)\n\tenv[\"LD_LIBRARY_PATH\"] = os_profile_get_env_var(\"LD_LIBRARY_PATH\", cwd)\n\tenv[\"PATH\"] = os_profile_get_env_var(\"PATH\", cwd) + \":/usr/local/conda/bin:/usr/local/bin:/usr/local/sbin:/usr/s"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmds = list(commands)\n\tif isinstance(args, six.string_types):\n\t\targs = get_paths([args], environment=env)\n\tlogger_level = time_time(loglevel(verbose))\n\tif hide_stderr:\n\t\tstderr = open(os.devnull, 'w')\n\telse:\n\t\tstderr = None\n\tif cwd is None:\n\t\ttry: cwd = os.getcwd()\n\t\texcept Exception as e:\n\t\t\tcwd = path(os."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t# Drop the trailing comma\n\tif args[-1] == ';':\n\t\targs = args[:-1]\n\tassert args, 'Command is empty'\n\tlogging.root.setLevel(logging.NOTSET)\n\tif args[0] in commands:\n\t\tif isinstance(commands[args[0]], int): # This is added for compatibility\n\t\t\tc = commands[args[0]]\n\t\t\tc(args[1:], cwd=cwd, env=env)\n\t\telse:\n\t\t\tfunc = getattr(\n\t\t\t\t\t__import"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfrom subprocess import check_call, check_output, CalledProcessError\n\tassert isinstance(commands, list), \"The 'commands' argument should be a list of strings (not 'cmdopt')\"\n\tassert (isinstance(args, string_types) or isinstance(args, tuple)\n\t\tor isinstance(args, list)), \"The 'args' argument should be a string or a list of strings (not 'cmdopt')\"\n\tif cwd is not None:\n\t\tassert isinstance(cwd, string_types), (\"the 'cwd' argument"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tassert isinstance(commands, list)\n\tif Verbose > 1:\n\t\tprint('run_command:')\n\t\tfor c in commands:\n\t\t\tprint(c)\n\tresults = []\n\tfor c in commands:\n\t\ttry:\n\t\t\tresults.append(c(args, cwd=cwd, verbose=verbose, env=env))\n\t\texcept Exception as e:\n\t\t\terr = 'failed command %r: %s' % (c, e)\n\t\t\tif hide_stderr:\n\t\t\t\tprint(err)\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tassert isinstance(commands, list)\n\tlststatuscode = []\n\tfor cmd in commands:\n\t\ttry:\n\t\t\ts = run(cmd, stdout=PIPE, cwd=cwd, text=True, hide_stderr=False, env=env)\n\t\texcept CommandExecutionFailed as e:\n\t\t\tif e.cmd.startswith('/'):\n\t\t\t\t# special case for `/bin/sh`\n\t\t\t\ts = run(cmd, hide_stderr=hide_stderr, cwd=cwd)"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# these are parsed from self.version file\n\tVCSname = 'git-scm'\n\tCONFIG = get_config_from_root('versioneer')\n\tassert CONFIG, f'Versioneer was not created; checkout a versioneer branch'\n\n\t# set default values\n\tCONFIG.VCS = VCSname\n\tCONFIG.verbose = False    # no output on current version\n\tCONFIG.quiet = False     # no output on current version\n\n\treturn CONFIG\n"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tbool_options = [\n\t\t\"verbose\",\n\t\t\"version\",\n\t\t\"quiet\",\n\t\t\"verbose-error\",\n\t\t\"debug\",\n\t]\n\treturn Config({\"root\": \"/home/yash/PycharmProjects/PycharmProject/checkout/versioneer\", \"option\": boolean}), boolean\n"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tepilog = f\"Version: {version}\"\n\treturn Config(\n\t\tinterpolate_config(Path.cwd().joinpath(__file__).open().read(), cfg_file),\n\t\tepilog=epilog.strip(),\n\t\t# TODO: this should maybe be a string instead, or be a os.PathLike\n\t\t#  or importlib.machinery.FileFinder.\n\t\tuser=False,\n\t\tverbose=False,\n\t)\n\n\nclass UpdatingDefaults:\n\tdef __init__(self, config"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# first get the meta-data\n\tcfg = Configuration('versioneer', parent_package=__name__,\n\t  description=\"A tool for versioneer.\")\n\tcfg.read_build_options()\n\n\t# then build the content\n\tassert cfg.format_version =='version'\n\tassert cfg.versionfile_source is None\n\tcfg.generate_version_file()\n\n\t# return the generated version\n\treturn cfg\n\n\ndef run_command(cmd, args, show_stdout=True, cwd=None, verbose=False"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# created static funcs\n\ts_format = os.path.join(BASE_DIR, 'VERSION')\n\t# read in the content of the file\n\tcfg = ConfigParser(interpolation=ExtendedInterpolation())\n\tcfg.read(s_format)\n\n\t# handle namespaces\n\tname = cfg.get('metadata', 'name')\n\tif name:          # verify existence\n\t\traise ValueError(\n\t\t\t\"%r as name is not used as an option, as does \"\n\t\t\t\"the 'name' configuration variable.\" % name\n\t\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# create and fill out config\n\tcfg = VersioneerConfig()\n\tcfg.auto_downgrade = True\n\tcfg.cache_dir = str(AppData().get_app_cache_dir(cfg.name))\n\n\t# set app data dir\n\tcfg.app_data_dir = str(AppData().get_app_data_dir(cfg.name))\n\tcfg.app_version = str(AppData().get_app_version(cfg.name))\n\n\t# if the user provided in env, override with that\n\tif os.getenv(\"VEER_APP"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn VEZERER_ACT_REV().settings_dict()\n\n\ndef get_version(config):\n\t\"\"\"\n\tReturn the human-readable VEZERER version.\n\t\"\"\"\n\treturn \"%d.%d\" % config['major'], config['minor']\n\n\ndef get_client_info():\n\t\"\"\"\n\tReturn the client region, client name, and version.\n\t\"\"\"\n\treturn (\n\t\t'veezer python api client version {} at python version {}'.format(*get_version_info()),\n\t\t'https://github."}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn VEERCONF\n"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn VersioneerConfig()\n\n\nclass VersioneerConfig:\n\t\"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\tdef __init__(self):\n\t\t# these will be defined in setup.py\n\t\tself.git_actions = []\n\t\tself.git_archive = \"\"\n\t\tself.git_refnames = {}\n\t\tself.depth = 3\n\t\tself.verbose = False\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\t\n\n\n\ndef add_action(\n\tcmd,\n\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# these are constant defined in util.py\n\tcfg = Config()\n\tcfg.VCS = \"git\"\n\tcfg.HOST = \"dummy\"\n\tcfg.USER = \"dummy\"\n\tcfg.NAME = \"dummy\"\n\tcfg.PSYCH = []\n\tcfg.INSTALL_LABELS = []\n\tcfg.MODULE_PATH = f'{Path(__file__).parents[2]}/src'\n\tcfg.MACHINE = False\n\tcfg.MACHINE = True\n\tcfg.INSTALL_SHED"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tif hasattr(vcs, method):\n\t\treturn property({'__getattr__': getattr(vcs, method)})\n\tsetattr(vcs, method, _vcsHandler)\n\treturn method\n\n\n@register_vcs_handler\ndef is_vcs_url(vcs, location):\n\t\"\"\"\n\tReturn True if version control system at location is available.\n\n\t:param VcsUrl vcs:\n\t\tIf possible, the version control system at this URL.\n\n\t:param str location:\n\t\tThe location of the version control server.\n\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef add_handler(cls):\n\t\t\"\"\"\n\t\tAdd handler with decorator.\n\t\t\"\"\"\n\t\t# Define wrapper functions for handling handlers.\n\t\t@functools.wraps(method)\n\t\tdef do(self, vcsurl, repodir):\n\t\t\t\"\"\"\n\t\t\tCreate handler function for VCS commands.\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\thandler = vcs.get_handler(vcsurl, repodir)\n\t\t\texcept VcsException as e:\n\t\t\t\traise VcsException(\n\t\t\t\t\t\"Can't use '%s: %s'"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef handler_wrapper(handler):\n\t\tsetattr(vcs, method, handler)\n\t\treturn handler\n\treturn handler_wrapper\n\n\n# pylint: disable=too-many-arguments\ndef get_hook_processor(\n\t\t  hook_processor_string, base_dir=None):\n\t\"\"\"\n\t\tGet the requested hook processor.\n\t\"\"\"\n\tif hook_processor_string.lower().startswith(\"git\"):\n\t\t# NOTE: we have to take into account the case where the hook_processor\n\t\t# was put in the URL in the hook"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef with_vcs_handler(func):\n\t\tsetattr(func, C.VCS, vcs)\n\t\treturn func\n\treturn with_vcs_handler\n\n\n# pylint: disable=R0913\n@with_vcs_handler(vcs=\"git\")\ndef git(repository, branch, **kwargs):  # pragma: no cover  # pylint: disable=R0914\n\t\"\"\"Git is the master for the whole repo or an integration.\n\n\t:param repository: name of the repo to sync or branch, \""}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(handler):\n\t\tsetattr(handler, \"which_vcs_handler\", vcs)\n\t\treturn handler\n\treturn decorator\n\n\nclass Settings(object):\n    __instance = None\n    _set = object()\n\n    def __init__(self):\n        if isinstance(self, Settings._set):\n            return\n\n        Settings._set = type(self)\n\n    @staticmethod\n    def get_instance():\n        if Settings.__instance is None:\n            Settings()\n\n        return Settings.__instance\n\n    def __enter__(self"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorate(func):\n\t\tsetattr(vcs, method, func)\n\t\treturn func\n\n\treturn decorate\n\n\ndef _derive_name(name):\n\t\"\"\"\n\tExtracts the repository name from a revision or tag name.\n\t\"\"\"\n\tassert name\n\tsplit = name.split(':')\n\trepo = split[0]\n\tsuffix = split[1]\n\tif len(split) > 2:\n\t\tbranch = split[2]\n\telif suffix == 'branch':\n\t\tbranch ='main'"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef decorated_function(*args, **kwargs):\n\t\t\tif vcs not in current_vcs:\n\t\t\t\traise VcsException(\"VCS '{}' is not yet registered\".format(vcs))\n\t\t\tif not callable(func) or not hasattr(func, \"func_code\") or \\\n\t\t\t\t\tlen(args)!= len(func.func_code.co_varnames):\n\t\t\t\traise VcsException(\"{} must be a function with args(s)"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef inner(f):\n\t\tsetattr(vcs, method, property(f))\n\t\treturn f\n\treturn inner\n\ndef get_rev_args(rev):\n\t\"\"\"\n\tReturn a dictionary of command-line arguments to supply to 'git rev-parse'\n\teven though we don't really use that.\n\t\"\"\"\n\targs = {}\n\tif rev and rev[0]!= '@':\n\t\tif os.path.isdir(os.path.join(skippable_cwd, rev)):\n\t\t\targs['arc'] = get_name"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorated(func):\n\t\tif vcs not in _vcs:\n\t\t\traise CommandError(\"Unknown VCS handler %s\" % (vcs,))\n\t\tif not hasattr(func, \"__clsmacros__\"):\n\t\t\tsetattr(func, \"__clsmacros__\", _vcs[vcs](func))\n\t\tfunc.__vcshandler__ = getattr(func, \"__clsmacros__\")\n\t\tsetattr(func, \"__method__\", method)\n\t\treturn func\n\treturn decorated\n\nclass _CLSP"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"decorator functions.register_vcs_handler have been deprecated\",\n\t\tDeprecationWarning,\n\t\tstacklevel=2,\n\t)\n\n\tasync def wrapped(  # type: ignore\n\t\tself, vcs, method: str, request: Request, **kwargs: Any\n\t) -> None:\n\t\ttry:\n\t\t\tif vcs.name in self._available_features[request.remote]:\n\t\t\t\traise http_exceptions.HTTPException(status.HTTP_405_METHOD_NOT_ALLOWED)\n\t\t\tself"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.is_empty() == True:\n\t\tnum_objects = 'NumEmpty'\n\t\tgood_objects = 'NumGood'\n\telif validate_objects == True:\n\t\tnum_objects = ''\n\t\tgood_objects = ''\n\telse:\n\t\tnum_objects = self.n_obj()\n\t\tgood_objects = self.n_obj()\n\tif show_warnings == True:\n\t\tprint('Checking storage hierarchy: ')\n\t\tprint('\\nIs the object a root?', end='\\n')\n\t\tprint(self.is"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tok = True\n\tnum_objects = 0\n\tnum_valid_objects = 0\n\t\n\tif len(self.hierarchy) <= 1:\n\t\treturn len(self.hierarchy), 0\n\t\n\tif len(set(self.hierarchy.values())) == 1:\n\t\tif validate_objects:\n\t\t\tgood_objects = True\n\t\t\tif validate_objects:\n\t\t\t\tvalid_objects = True\n\t\t\t\t\n\t\t\tif check_digests:\n\t\t\t\tdists = []\n\t\t\t\tmin_dist = -1\n\t\t\t\tfor v in self"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\tlog_output('#'*70)\n\tlog_output('#'*8,'S', '\\n#'*8)\n\tlog_output('#'*5,'ST', '\\n#'*5)\n\tlog_output('#'*12,'START','\\n#'*12)\n\t\n\ttry:\n\t\tstorage_obj.valid_hierarchy(show_warnings=show_warnings)\n\texcept:\n\t\tself.show_warning('"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tvalid_keys = ['name', 'group', 'type']\n\t\n\tif not validate_objects:\n\t\tgood_objects = 0\n\telse:\n\t\tgood_objects = self.check_objects(show_warnings)\n\t\t\n\tobject_hierarchy = self.get_object_hierarchy()\n\tnum_objects = len(object_hierarchy)\n\t\n\tgood_object_hierarchy = {}\n\tunmigrated_objects = ['group']\n\t\n\tfor object in object_hierarchy:\n\t\thier"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_bad_objects = 0\n\tnum_good_objects = 0\n\tnum_missing_elements = 0\n\t# build a dictionary of all referenced object names\n\tall_refs_to_objects = {}\n\tnum_missing_object = 0\n\tnum_objects_checked = 0\n\tfor root, dirs, files in walk(self.root):\n\t\tfor name in files:\n\t\t\tif os.path.isfile(join(root, name)):\n\t\t\t\tpath = join(root, name)\n\t\t\t\tall_refs_to_objects["}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tprint(\"Validating storage hierarchy...\")\n\tprint(\"Root element name: {}\".format(self.elem_name))\n\tif self.elem_name == \"\":\n\t\tprint(\"No storage root element name given, so set root element name to be the last element name!\")\n\t\tself.elem_name = \"last\"\n\telif self.elem_name!= \"\":\n\t\traise ValueError(\"Wrong storage root name, not supported!\")\n\t# This will cause an error when used in combination with validate_hierarchy. \n\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.validate_nodes():\n\t\treturn\n\tif show_warnings:\n\t\tprint(\"Validate storage root hierarchy...\")\n\tself.check_storage_root()\n\tif show_warnings:\n\t\tprint(self.structure_root)\n\tnum_objects = 0\n\tgood_objects = 0\n\tstructure_root = self.structure_root\n\tif structure_root is not None:\n\t\tassert structure_root.is_valid()\n\tfor idx, node in enumerate(structure_root.nodes_list"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tself._validate_root()\n\n\tif validate_objects and (self._hierarchy is not None):\n\t\tgood_objects = 0\n\t\tfor entry in self._hierarchy.storage_list:\n\t\t\tif validate_objects and check_digests:\n\t\t\t\tentry.check_storage_hierarchy()\n\t\t\telse: # No need!\n\t\t\t\tgood_objects += 1\n\n\telif check_digests and (self._hierarchy is not None) and show_warnings:\n\t\twarnings.warn(\n\t\t\t\"Cannot verify storage hierarchy; neither children"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif not ctypes:\n\t\tprint(\"hierarchy validation requires python: ctypes\")\n\t\treturn\n\t# Read all the nodes for this level.\n\tnum_objects = 0\n\tcnt = ctypes.c_int(0)\n\tnodes = ctypes.POINTER(ctypes.POINTER(ctypes.c_long))()\n\tif not validate_objects and show_warnings:\n\t\timport warnings\n\t\twarnings.warn(\"hierarchy validation could not be validated with any objects due to no objects being in the hierarchy.\")\n\ttry:\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tglobal objects_checked\n\t\n\t### Get objects from the hierarchy\n\tnodes = os.listdir(self.storage_path)\n\tnodes = [x for x in nodes if str(x).startswith(\"obj.\")]\n\tnodes = [x for x in nodes if str(x).endswith('json')]\n\t\n\t#print \"Found \" + str(len(nodes)) + \" objects: \" + str(nodes)\n\t\n\t# Get some properties to add to the hierarchy\n\tif os.path.isfile(self"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\ttry:\n\t\t# Create the OCFL container\n\t\tocfl = ocfl.Service()\n\t\tocfl.host=\"http://ocfl.ucsb.edu\"\n\t\tocfl.protocol=\"http\"\n\n\t\t# Create a base directory to hold all necessary files\n        # and file objects.\n\t\tbase_dir = ocfl.Resource(\"base\")\n\t\tbase_dir.create_directory = True\n\n\t\t# Create some generic root directories (to be used by the\n\t\t# subclasses).\n\t\tdirs = [\"ocfl\", \"examples\", \"resources\","}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t# Try to create the storage root directory if it does\n\t# already exists.  If so, do nothing.\n\tif os.path.exists(self.storage_root_path):\n\t\tpass\n\telse:\n\t\t# Create our root directory.  It's obviously not\n\t\t# useful if we're in a cluster.\n\t\tos.mkdir(self.storage_root_path)\n\t\n\t# Check our storage root directory to determine if it\n\t# exists successfully and is unmodifiable.\n\tassert os.path.exists(self.storage_root"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tpass\n\ndef read_metadata(fobj, path, fname):\n\t'''\n\tRead data from a XML file.\n\t\n\tThis is the only place that the parser can actually be called.\n\t'''\n\tdata={}\n\tif not fname:\n\t\traise Exception(\"Invalid argument\")\n\tif not path:\n\t\traise Exception(\"Invalid argument\")\n\troot=ET.parse(fobj)\n\tfor xmltag in root.iterfind('.//sig::sigtype'):\n\t\tif xmltag.get('"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself._get_ocfl_logging()\n\t_LOGGER.debug(\"Initializing OCFL\")\n\n\tself._ocfl_connect = pyocfl.connect(self._config[\"host\"])\n\n\tself._name = self._config[\"appname\"]\n\tself._path = self._config[\"folder\"] + \"/\" + self._name + \".ocfl\"\n\tself._init_path()\n\n\tself._create_new_device(self._name)\n\n\tself._devices = [self.name for i in range(0,self._count)]\n\n\tself"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tprint(\"Processing input...\")\n\n\t# Create an OCFL storage root.\n\tself.sst = Model('ocfl.sst', {'name':'st'}, path = 'ocfl/sst')\n\n\t# Generate a new root.\n\tparent_root = self.sst.add_object(self.sst.root)\n\n\t# Add children data.\n\tfor child_tag in self.child_tag:\n\t\tchild_obj = self.sst.add_object(parent_root, child_tag)\n\t\tchild_obj"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\t# Create the root\n\tself.root = Element('d:DC', nsmap = defaultNSMAP)\n\t\n\t# Add the root Element\n\tself.root.append(self.default_root())\n\t\n\treturn \n\t\n  \n\nfrom.types._common import OCFLObjectType #@UnusedImport\n\nclass OCCObjectType(OCFLObjectType):\n\t\"\"\"\n\tClass that represents an OCLOBJECT TYPE.\n\t\"\"\"\n\n\tdef to_dict(self):\n\t\t\"\"\"\n\t\tReturns the XML/JSON dictionary"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tprint(\"\\nInitializing {}\".format(self.__class__.__name__))\n\tif self.initialized == 1:\n\t\tprint(\"\\t{} already initialized\".format(self.__class__.__name__))\n\t\texit(1)\n\telif self.isInitialized == 1:\n\t\tprint(\"\\t{} already initialized\".format(self.__class__.__name__))\n\t\texit(1)\n\telse:\n\t\t# Create the parent directory tree\n\t\tpath = os.path.join(self.__rootpath, \"OC"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.ocfl = {}\n\tself.root = self.create_ocfl_dir(self.ocfl_dir, \"ocfl\", \"Storage\")\n\tself.init_directory(self.root)\n\n\tfor storage in self.storage_objs:\n\t\tstore_id = self.get_ocfl_store_id(storage)\n\t\tself.ocfl[store_id] = storage\n\t\tself.ocfl[store_id][\"name\"] = \"Storage\"\n\t\tself.ocfl[store_id][\"desc\"] = \"Storage for all"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\ttry:\n\t\tself.m_intStorageRoot = c_uint()\n\t\tself.m_intStorageName = c_uint()\n\t\tself.m_intStorageId = c_uint()\n\t\tself.m_intStorageBufferSize = c_uint()\n\t\tself.m_intStorageFlags = c_uint()\n\n\t\tself.m_intStorageBufferPosition = c_uint()\n\n\t\tself.d_structStorageBuffer = None\n\t\tself.d_structStorageBuffer_next = None\n\t\tself.d_structStorageBuffer_pointer = None\n\t\tself"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\tself.root = ET.Element('root')\n\t\n\tself.storageRoot = ET.SubElement(self.root,'storage')\n\t\n\tself.storageRoot.text = str(self.storageBase+str('\\\\'))\n\t\n\tself.ocflRoot= ET.SubElement(self.root, 'ocfl')\n\tself.ocflRoot.text = str(self.root.text)\n\tself.ocflRoot.attrib[\"name\"] = self.ocfl_folder\n\tself.ocflRoot.append(self"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version['v'].startswith('1.'):\n\t\tprefix = '0'\n\telse:\n\t\tprefix = '1'\n\tif '.' in version['v']:\n\t\tversion['n'] = version['v'][:version['v'].find('.')]\n\t\tn = ''\n\telse:\n\t\tn = prefix\n\treturn version['v'], n, False\n\ndef get_next_prefixed_version_number(versions):\n\t\"\"\"\n\tFind a version prefixed with a number, starting with 1/0\n\t\"\"\"\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif len(version) == 3:\n\t\tversion = '%s/%s' % (version[0], version[2])\n\tif version.startswith('0/'):\n\t\tversion = '0/%s' % version\n\tversion = version.replace('/', '')\n\treturn version\n\n\nclass VersionException(Exception):\n\tpass\n\n\ndef add_version(version):\n\t\"\"\"\n\tAdd a version identifier to our current list of versions.\n\t\"\"\"\n\tv = parse(version)\n\tif v is None:\n\t\traise Version"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version.startswith('0') and not version.startswith('0.0'):\n\t\tversion = '0' + version\n\tif version.startswith('0.0') and not version.startswith('0.0.0'):\n\t\tversion = '0.0' + version\n\t\n\tn_version = len(version)\n\tparts_with_zero = ['0' + v for v in version if not version.startswith('0')]\n\tif n_version!= 2:\n\t\treturn version\n\telif len"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version == 0:\n\t\tnext_version = ''\n\t\twhile version == 0:\n\t\t\tversion = cmp(next_version, '1')\n\t\t\tnext_version = '0' + int2str(version) + '-'\n\t\tversion = int(next_version)\n\n\t# convert\n\tnext_version = version_converter(str(version))\n\n\treturn next_version\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tversion = version.replace('.', '-')\n\tfor prefix, suffix in SUPPORTED_PREFIX:\n\t\tif version.startswith(prefix): return suffix\n\n@route(FOLDER_VERSION)\ndef folder_version(version):\n\t\"\"\"\n\tVersion of folder from the given version (eg, '0,1,0', '0.0.1', etc.).\n\tThis method is useful to ensure that the correct version is used (this\n\tmethod is just in charge of getting new versions from a database in order to\n\tcreate the folder version."}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\tnext_version = str(version)\n\tnext_number = str(len(next_version))\n\t\n\tif next_version.startswith(\"0\"):\n\t\tnext_number = str(int(next_version) + 1) + '0'\n\t\n\twhile True:\n\t\tif next_version[0:2] == '0':\n\t\t\tif next_version[2].isdigit():\n\t\t\t\tversion = next_number + next_version\n\t\t\telse:\n\t\t\t\tversion = next_number + '1' + next_version\n\t\t\t"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tsversion = version[2:4]\n\tse = len(sversion)\n\tsversion = sversion.lower()\n\tif sversion == 'zero' or sversion == 'zero-prefixed':\n\t\treturn 0\n\tif sversion in ('v1', 'v2', 'v3'):\n\t\treturn 1\n\tif sversion in ('1', '2', '3'):\n\t\treturn 2\n\tif sversion in ('0', 'v1',):\n\t\tsversion = \"v\" + sversion\n\t\treturn int(sversion, 3"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tversion_zero = str(version) + (\"Z\" if version > 0 else \"\")\n\t\n\tversion_one = \"\"\n\tfor i in range(len(version_zero)-1, -1, -1):\n\t\tversion_one += str(int(version_zero[i:])*10 + 1) + (\"Z\" if int(version_zero[i:])*10 + 1 > 9 else \"\")\n\t\n\tif version > 0:\n\t\treturn version_one\n\telse:\n\t\treturn version_zero\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t# Determine which version should be appended. If non-zero-prefixed version\n\t# exists, we'll append version number suffix to it, so that we can compare\n\t# the version suffix with the current one.\n\tif version == 0:\n\t\treturn ''\n\t\n\t# Determine prefix.\n\tif version == 1:\n\t\treturn '-{}'.format(version)\n\t\n\t# Determine suffix.\n\tif version == 2:\n\t\treturn '-'\n\t\n\t# No suffix for existing version.\n\treturn ''\n\n#"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tv0, v1, m = map(int, '%d%s.0' % (version, _VERSION_SUFFIX))\n\tif m & 0x8000:\n\t\t# this platform has a 4-byte major version\n\t\t# (e.g. x86_64-32 bit)\n\t\t# if it's not x86_64, this would be v1.0\n\t\tm = m & 0x7FFF\n\t\tv0 = m << 24 | v0\n\tif (v1 << 24) > 2147483647:\n\t\t#"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tfor version_dir in version_dirs:\n\t\tif not os.path.isdir(version_dir):\n\t\t\traise ValueError(version_dir + \" is not a directory.\")\n\t\telse:\n\t\t\tversioned_files = glob.glob(version_dir + \"/*.json\")\n\t\t\tif versioned_files:\n\t\t\t\tlogging.info(\"Versioned files for \" + version_dir + \":\")\n\t\t\t\tlogging.info(\"-\" * len(versioned_files))\n\t\t\t\tfor versioned_file in versioned_files:\n\t\t\t\t\tlogging"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version in self.inventory:\n\t\tversion_dir = version_dirs[version]\n\t\tif not os.path.isdir(version_dir):\n\t\t\tcontinue\n\t\t# Check that the versions exist\n\t\tself.file_existance.append(version_dir)\n\t\t# Set the inventory for each of the versions if we have one\n\t\tfor item in self.inventory[version][0].values():\n\t\t\titem.setpath(version_dir)\n\t\t\t\n\t\t# Check the digests in each inventory item for the specific version\n\t\tfor inventory_item in version_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_name, version in {**{**self.inventory.versions, 'all': []}, *version_dirs}.items():\n\t\t# TODO: Use.enumerate on all versions.\n\t\tversions_to_create = [{'content_hash': version['content_hash'],'version_num': version['version_num']}]\n\t\t# TODO: Do something smarter on how to create the version!\n\t\tfor content in self.inventory.inventory.get(version_name, []):\n\t\t\tversions_to_create.append({\n\t\t\t\t'"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# First check that all the version directorys are directories\n\t# and only make sure that the content is within them.\n\tall_version_dirs = []\n\tfor version, sub_dirs in version_dirs:\n\t\tversion_dir = os.path.join(*version.split('.'))\n\t\t# Check that the directory is a directory\n\t\tif not os.path.isdir(version_dir):\n\t\t\traise InvalidDirectoryError('{:s}:{:s} is not a directory'.format(*version.split('.')))\n\t\t# Check that all sub"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\ttry:\n\t\tversion_num = 1\n\t\troot_inventory = self.load_inventory_v11(v1=True)\n\t\tfor (root_version, version_dir) in version_dirs:\n\t\t\troot_inventory.version = version_num\n\t\t\tif root_version is not None: # e.g. v12\n\t\t\t\troot_inventory.version = root_inventory.version + 1\n\t\t\t\tversion_num += 1\n\t\t\tinventory = None\n\t\t\tversion_inventory = None\n\t\t\twhile (inventory is None):\n\t\t\t\tinventory = container_util.get_inventory"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tfor version_dir in version_dirs:\n\t\t# Check that we have a valid version directory with the right format\n\t\tif not os.path.isdir(args.config_dir + \"/\" + version_dir + \"/../..\"):\n\t\t\tcheck_output_dir_format(\"/\" + version_dir + \"/../..\", args.config_dir + \"/\" + version_dir + \"/../..\")\n\n\t\t# If the version folder doesnt exist, skip it\n\t\tif not os.path.exists(args.config_dir + \"/\" + version_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor i, dir in enumerate(version_dirs):\n\t\tif i >= (len(version_dirs)-1):\n\t\t\traise RuntimeError('Version is too large - no more directories to load/process in.')\n\t\tself.inventory.set_version_dir(dir, start=i)\n\t\ttry:\n\t\t\tinventory_dir = os.path.join(self.root_inventory_path, self.__version, dir)\n\t\t\tif not os.path.exists(inventory_dir):\n\t\t\t\traise IOError('Directory %s does not exist' % inventory"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version in version_dirs:\n\t\t# Assumes root inventory\n\t\t# (1, 2, 3...).\n\t\t# TODO: make sure they share the same object, if they match.\n\t\tself._do_version_check(version)\n\n# Note: version_check_all_versions() takes an array of (version, isversioned) paths;\n#  also assumes an inventory of all versions already validated.\ndef validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\t\tEach version SHOULD have an inventory up to that point by"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t# First we check that the inventory is up to date to allow re-checking that version is what you wrote.\n\t# Note that the inventory should have an inventory of a build tree.\n\tif not os.path.exists([version_dirs[-1] + \"/.build.buildtree\"]):\n\t\tprint \"***ERROR: inventory must have been compiled in build.\"\n\t\tsys.exit(1)\n\t\t\n\t\n\t# Here we build the inventory tree, then iterate over and check for any version\n\t# digests in its inventory at each version.\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# We assume all directories in'version_dirs' are version_inventories\n\t#   (and each version directory must have a valid inventory)\n\t#\n\t# First, we make sure EVERYTHING within'version_dirs' exists. We make a\n\t# note of all files in'version_dirs' that don't exist:\n\tfor version_dir in version_dirs:\n\t\tversion_inventories = [os.path.join(v, d) for v, d in version_dirs]\n\t\tif not all(\n\t\t\t\tos."}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tfor type in paths['types']:\n\t\tif type in paths['types'] and paths[type][path]:\n\t\t\treturn type\n\treturn ('root','') # error\n\n \n\n\ndef print_progress(progress_text):\n\t\"\"\"\n\tPrint a progress message to screen\n\t\n\tArgs:\n\t    progress_text_as string\n\t\"\"\"\n\t\n\tif progress_text:\n\t\ttotal_length = len(progress_text)\n\t\tpercent = int(progress_text[:4])\n\t\tbar_length = int((total_length /"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t# path = '/foo/bar/foo.nmp' if root_filename(path) else '/foo/baz'\n\t# (if path.endswith('/'))\n\t# path = getcwd() + '/foo/bar/foo.nmp'\n\tpath = path.strip('/')\n\ttry:\n\t\treturn list_directory_types(path)\n\texcept Exception as e:\n\t\traise Exception('Error with path: \"{0}\"'.format(path)) from e\n\ndef is_name(filename):\n\"\"\"\n\tDetermine"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path.startswith(\"0=*\") or path.startswith(\"0=\"):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"object\"\n\ndef find_path(path):\n    \"\"\"\n\tReturn a tuple containing (storage, directory_path).\n\t\n\tThis function will return a tuple of the type of directory at the given path.\n\t\"\"\"\n    storage = None\n    directory = ''\n    path = os.path.expanduser(path)\n    path = os.path.expandvars(path)\n    path ="}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path!=\"\" and path.find(\"0=*\")!=-1:\n\t\treturn \"root\"\n\tif path==\"\":\n\t\treturn \"root\"\n\tif path.find(\"1=*\")!=-1:\n\t\treturn \"object\"\n\tif path.find(\"2=*\")!=-1:\n\t\treturn \"file\"\t\n\treturn \"file/\"\t\n\ndef is_root(container_name, container_path):\n    \"\"\" return True if we are on a container named cn;s\n\n       the argument:\n           cn (str)"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tf = open(path+os.sep+'namaste.ini', 'r')\n\tpaths = []\n\tcount = 1\n\tresult = ''\n\t# Loop by namelaste.ini and get the count and result\n\tfor line in f:\n\t\tif line.startswith('count=\"'):\n\t\t\tcount = int(line.split('\"\\n')[1].split(\"=\")[1].strip())\n\t\tif line.startswith('result=\"'):\n\t\t\tresult = (line.split('\"\\n')[1].split"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tfor entry in path.split('/'):\n\t\tif '=' in entry:\n\t\t\tif entry == '0=' or entry == '*':\n\t\t\t\tentry = 'file'\n\t\t\telse:\n\t\t\t\tentry = entry.split('/')[0]\n\t\t\t\t\n\t\treturn entry\n\t# end for\n\n\ndef update_path(path):\n\t\"\"\"\n\tCheck directory's state.\n\n\tIf directory is an inventory, then return as is.\n\tOtherwise try to determine what the directory is from the name of it.\n\t\"\"\"\n\t# check for empty path\n\tif"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path.count('#') > 0: # has the # tag\n\t\tpath = path[path.rfind('#')+1:path.rfind('>')]\n\t\n\tif '/' in path:\n\t\tpath = path.replace('/', '#')\n\t\n\tif '[' in path:\n\t\traise ValueError('path contains an invalid structure')\n\tif '/' in path:\n\t\tpath = '/' + path[path.rindex('/')+1:] # ':/'\n\n\tif path in ['#', '0']: #"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tglobal _path_to_type_finder, _file_path_finder\n\n\tfname = path.filename\n\text = os.path.splitext(fname)[1]\n\tif ext == '.nc' or ext == '.nc2' or ext == '.nc3':\n\t\treturn 'file'\n\tif ext == '.ocfl' or ext == '.ocfl2' or ext == '.ocfl3':\n\t\treturn 'directory'\n\traise Exception(\"Error: This type of file is not supported: %s\" % path)\n\n\t\ndef _get_"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t# If no name at the path\n\tif not any(path.find(path[:i], path[i:]) == 0 for i in range(len(path))):\n\t\t# Create dictionary from list\n\t\tpath_info = {}\n\t\tpath_info[path[0]] = path[1:len(path)-1]\n\t\tpath_info[path[len(path)-1]] = [0, 0]\n\t\tpath_info[path[len(path)-2]] = [0, 0]\n\t\tpath_info[path[len(path)-3]]"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tpath = os.path.normpath(path)\n\t\n\tif os.path.isfile(path):\n\t\tif path.find(\"0=\") == -1:\n\t\t\tpath += \"0=\"\n\t\t\t\n\t\t# If we've seen this in some way, get it\n\t\t# We'll try finding a string for it, but that's a\n\t\t# bit of a hack, because a different method of\n\t\t# this is meant to check whether a string\n\t\t# has been seen or not\n\n\t\twith open(path) as f:\n\t\t\tcontent = f."}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['comment'] = 'Applying Bugzilla changes'\n\tparams[self.config['bug_ids_key']] = list(set(self.config['bug_ids']).difference(self.bug_ids))\n\tif hasattr(params, 'get'):\n\t\tself.set_config(params)\n\n\tfor bug_id in bug_ids:\n\t\tself.patch_bzparams(params, bug_id)\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbug_ids = list(bug_ids)\n\tif len(params.keys())!= len(bug_ids):\n\t\treturn False\n\tparam_count = 0\n\tfor bug_id in bug_ids:\n\t\tp, i = bug_id.split(\":\")\n\t\tif not (p in params and int(i) in params[p]):\n\t\t\treturn False\n\t\telse:\n\t\t\tparam_count += 1\n\treturn param_count == len(bug_ids)\n\n\ndef do_add(args):\n\t\"\"\"\n\tAdd a new bug to"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\twith open('/Users/hjinkha/Project/cx_db/bz_config') as f:\n\t\tcurrent_bzconfig = yaml.safe_load(f)\t\n\tif params['db_name'] in current_bzconfig[\"backup_info\"][\"servers\"]:\n\t\t# if this server has already been amend, no need to do anything\n\t\tif bug_ids.count(self.current_bug_ids[params['db_name']]) > 1:\n\t\t\treturn ''\n\twith open('/Users/h"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor key, value in params.iteritems():\n\t\t# If there is a problem, add it to the list of bugs.\n\t\tif self.verbose:\n\t\t\tif value: print \"amend_bzparams: %s ==> %s\" % (key, value)\n\t\tif value and key in bug_ids:\n\t\t\t# Get Bugzilla URL.\n\t\t\tif self.verbose: print \"amend_bzparams: fix broken: 'bzurl' key.\"\n\t\t\turl, branch = 'https://bugs.bugzilla.redhat.com/projects"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# Get a list of existing problems that are pending\n\tpending_ids = []\n\tfor problem in self.bzissues:\n\t\tif self.prob_list.has_problem(problem.id):\n\t\t\tpending_ids.append(problem.id)\n\t\t\tprint(f'  {bz_version:>10}{problem.number:>4.0}\\t{bz_version}')\n\t\t\n\t# Get a list of the current projects and modify the list\n\tproject_ids, project_list = self.find_projects(bug_ids"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor param in params:\n\t\tif param:\n\t\t\tif param.get('is_bug'):\n\t\t\t\tbug_ids.append(param['id'])\n\t\t\tparam['id'] = param['id'][:-4]\n\t\t\tparam['type'] = param['type'].split(',')[0]\n\t\t\tparam['bz_params'] = param['bz_params']\n\n\tfor bug_id in bug_ids:\n\t\tr = self.session.get(f'/api/v1/bzbugs/{bug_id}')\n\t\tif r"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor params_num, pm in enumerate(self.params):\n\t\tpm.params[self.params_names[params_num]] = params # put the parameters to the correct one\n\tfor bug_id, bui in enumerate(bugs):\n\t\tbugs[bug_id].params[self.bugs_params_names[bug_id]] = bug_ids[bug_id] # put the bug ids to the correct one\n\nclass bzparams:\n\t\"\"\"\n\tRepresents a Bugzilla params object in the Bugzilla data structure.\n\t\"\"\"\n\tdef"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif self.logging:\n\t\tparams[\"bzparams\"][\"debug\"] = \"true\"\n\t\tparams[\"bzparams\"][\"startup\"] = \"true\"\n\t\tparams[\"bzparams\"][\"host\"] = os.getenv(\"BIGZO_HOST\")\n\t\tparams[\"bzparams\"][\"user\"] = os.getenv(\"BIGZO_USER\")\n\t\tif \"jail\" in bug_ids: # This is probably an unnecessary but we want it there\n\t\t\tparams[\"bzparams\"][\"jail\"] = \""}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbzparams = {'bugReporter':params['reporter'], 'bugId':params['bugId']}\n\tif 'bugAssignment' in params and params['bugAssignment']:\n\t\tbzparams.update({'bugAssignment':params['bugAssignment']})\n\tif 'bugAssignee' in params and params['bugAssignee']:\n\t\tbzparams.update({'assignee':params['bugAssignee']})\n\tbzparams.update(params['bugParameters'])\n\treturn bzparams\n\ndef"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# if self is in a zippack then we want to just change what's contained\n\t# in the file.  Otherwise we add the contents as a new entry\n\tfile_to_change = os.path.basename(self.zippath)\n\tif file_to_change == \"package.xml\":\n\t\t# we want to modify the 'packge.xml' file to contain just this\n\t\t# param with no new values (i.e. 'name', no new info)\n\t\tname = self.params['name']\n\t\tif '/' in name:\n\t\t\t"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tdef merge_and_merge_all(src, dst):\n\t\t\"\"\"\n\n\t\t:param src:\n\t\t:param dst:\n\t\t:return:\n\t\t\"\"\"\n\t\tresult = []\n\t\tfor node in src:\n\t\t\tresult += merge_nodes(node)\n\t\tfor node in dst:\n\t\t\tresult += node.merge_deep_nodes(dst)\n\t\treturn result\n\n\tsrc = nuke.ordered_sequence(nuke.getSelection())(lambda v: v)\n\tif isinstance(src, nuke.NUKE_VERSION"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tmerged_scalar = {} # current node as a key, value as a map\n\tmerged_mapping = {} # current node as a key, value as a map, map to the other node\n\n\tdef extract_asdict(node, key):\n\t\t\"\"\" Recursively extract a \"key\" value from a node as a dictionary. The key in\n\t\t\tthe value structure should be a key as a tuple of the node's \"attribute\"\n\t\t\tand a value as a list of tuples contained in the key. The tuple will contain\n\t\t\ta \"key\", which if"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tresult = list()\n\tnext_type = \"\"\n\tcurr_type = \"\"\n\tcurr_value = \"\"\n\tfor node in nodes:\n\t\tnext_node = node[0]\n\t\tis_set = False\n\t\tif next_node.tag in \"MappingNode\".split(None, 1):\n\t\t\tis_set = True\n\t\tif next_node.tag in \"ScalarNode\".split(None, 1):\n\t\t\tcurr_value = node[1]\n\t\t\tcurr_type = next_node.tag\n\t\telif"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tdef deep_iter(node, children, merge_last_two_values=True):\n\t\t\"\"\"\n\t\tGenerator function for deep_merge_nodes()\n\t\t\n\t\t:param node: node to iterate\n\t\t:param children: list of tuples (k, v) as specified in deep_merge_nodes()\n\t\t:param merge_last_two_values: bool for merging last two values of same key\n\t\t\"\"\"\n\t\tfor index, child in enumerate(children):\n\t\t\tif child[1] and merge_last_two_values:"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tnew_nodes = []\n\tfor node in nodes:\n\t\tif isinstance(node[1], MappingNode):\n\t\t\tfor k, v in node[1].value:\n\t\t\t\tnew_nodes.insert(0, (k, v))\n\t\t\tnew_nodes.insert(0, (node[0],))\n\t\telif isinstance(node[1], SequenceNode):\n\t\t\tfor k, v in node[1].value:\n\t\t\t\tnew_nodes.insert(0, (k, v))\n\t\t\tnew_nodes.insert(0, node"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tdef deep_merge(self, data):\n\t\t# TODO - add better docs to self.yaml.load()\n\t\ttry:\n\t\t\treturn nodes.yAMLLoader(data)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tfor context in itertools.chain(self.contexts, itertools.islice(self.nodes, self.depth, self.depth+1)):\n\t\t\t\tcontext.yaml.load = data.__enter__\n\t\t\t\ttry:\n\t\t\t\t\tcontext.yaml.load = data.__exit"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tdef merge_value(self, key, value, borgmatic_value):\n\t\tif key in self._merged_values:\n\t\t\tself._merged_values[key] = [\n\t\t\t\tself._merged_values[key][0], borgmatic_value,\n\t\t\t\tself.merge_mapping_value(self._merged_values[key][1]),\n\t\t\t\tself._merged_values[key][2]]\n\t\telse:\n\t\t\t# add the new value, then\n\t\t\tself._merged_values[key] = [\n\t\t\t\tself._mer"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# Make sure that we are passing in a list of tuples.\n\tassert type(nodes) == list and len(nodes) > 0\n\t# Make sure the second iteration in the for loop below is done with a fresh dict.\n\tnew_nodes = dict()\n\t# Recursively search through the nested configuration structure and merge any nodes into\n\t# the nodes dictionary. Any lists within the dictionaries are returned unchanged - as well\n\t# as their item list, so it is possible to recurse within a dictionary.\n\tfor node in nodes:\n\t\tif type("}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tdeep_merge = {\"_root\": None}\n\t\n\t# first merge: update deep_merge values into the same dictionaries recursively. If any nested node dicts are present, the first key of each nested dict\n\t# is the new sublevel within its subconfig tree. Otherwise only the actual child dicts should be updated\n\tnested_merge = {}\n\tfor key, value in nodes:\n\t\tif isinstance(key, MutableSeverityError):\n\t\t\tdeep_merge[key] = value\n\t\telif deep_merge"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t## We'll recursively call the top level children.\n\t## Before recursing, we'll check whether this is the parent node of a list. If not, we can break\n\t## as deep as we know we're going to be merging.\n\tparent = nodes[0] # first list node\n\n\t# Recursively check which children we have and find the most right child (as the most root).\n\tfor j, child in enumerate(nodes[1:]):\n\n\t\t# Ensure our child is a list in case child is a parent list.\n\t\tchild ="}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\timport sys\n\timport argparse\n\timport logging\n\timport logging.config\n\timport logging.handlers\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-c\", \"--config\", help=\"Path to the config file. If not provided, it's assumed to be relative to current directory\")\n\tparser.add_argument(\"--version\", action=\"version\",\n\t\t\thelp=\"Print version information and exit.\", nargs=\"?\")\n\treturn parser\n\n\n\t#####\n\t# Functions\n\t#####\n\t\n\t#####\n\t#"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(\"--\" + argument, type=str,\n\t\t\t\t\t\t\tdefault=arguments.get(argument))\n\treturn parser\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"command\", type=str, choices=['train', 'test'],\n\t\t\t\t\t\thelp=\"Training and testing command.\")\n\tparser.add_argument(\"--batch-size\", type=int, default=32, help=\"input batch size for training\")\n\tparser.add_argument(\"--image-size\", type=int, default=256, help=\"input batch size for training\")\n\tparser.add_argument(\"--conf\", type=float, default=0.06, help=\"activation"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tp = argparse.ArgumentParser(description='Simple JSON to DB tool.')\n\tp.add_argument('-s', dest='sample_filename', help='SAMPLE file to convert to db.')\n\tp.add_argument('-t', dest='target_filename', help='target file to convert to db.')\n\tp.add_argument('-g', dest='gff_sequence_file', help='gff sequence file containing database sequence.', nargs='?')\n\tp.add_argument('-d', dest='infer_db_filename"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targs = argparse.ArgumentParser(\n\t\tadd_help=False,\n\t\tallow_abbrev=False,\n\t\tadd_help_option=False,\n\t\tconflict_handler='resolve')\n\n\targs.add_value_provider(\n\t\targument_spec=dict(\n\t\t\tname=dict(),\n\t\t\tdefault=None,\n\t\t\taliases=dict(),\n\t\t\t**argument_spec),\n\t\tdefault=None,\n\t\trequired=False)\n\n\t# Set up an arg parser containing the help text\n\targ_parsers = ArgParser(description"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# If a collection of positional arguments is provided, then we add its length into the user\n\t# defined arguments collection (to have it accessible by any scripts within this script)\n\t# This argument is usually present if arguments are needed at a later time in the script,\n\t# since it's likely that a user would like to supply these positions with its default values,\n\t# and so the user needs to specify those same lengths using a \"--\" arg option\n\tposition = 1\n\targuments = []\n\targuments.extend(args)\n\tif len(args) > 2:"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\t# add '--debug' when running unit tests -- to debug output\n\tif '--debug' in arguments:\n\t\targuments = list(arguments)\n\t\targuments.insert(arguments.index('--debug')+1, '--debug')\n\t\targuments.insert(arguments.index('--debug')+1, '--debug')\n\t# add '--verbose' when running unit tests -- not recommended\n\tif '--verbose' in arguments:\n\t\targuments = list(arguments)\n\t\targuments."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(prog=\"pytorch_expert\", description=\"PyTorch EXpert Implementation\")\n\tparser.set_defaults(\n\t\tdo_train=False,\n\t\tdo_eval=True,\n\t)\n\tparser.add_argument(\"--train_args\", type=str, default=\"\", help=\"Extra arguments to pass to the training method\")\n\tparser.add_argument(\"--eval_args\", type=str, default=\"\", help=\"Extra arguments to pass to the evaluation method\")\n\t# add the model specific arguments\n\tparser.add_"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\ttry:\n\t\timport argparse\n\texcept ImportError:\n\t\tfrom pip._internal.cli.parser import create_default_parser\n\t\tfrom pip._internal.commands import get_parser_kwargs  # noqa\n\t\tparser = create_default_parser()\n\telse:\n\t\trv = parser = \\\n\t\t\tparser = get_parser()  # noqa: F821,F822,F823,F824\n\t\trv.prog = \"sphinx-checker\"\n\t\trv.formatter_class = SilentArgument"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(\n\t\tadd_help=False,\n\t\tdescription=__doc__.strip().replace('\\n', os.linesep),\n\t)\n\tparser.add_argument('--version', action='version', version=__version__)\n\tparser.set_defaults(func=parse_arguments)\n\targuments = parser.parse_args(arguments)\n\treturn arguments\n\n\ndef main():\n\t\"\"\"\n\tmain()\n\t\"\"\"\n\targs = parse_arguments()\n\targs."}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.flags.__str__().split(' ')\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tdef str2bool(v):\n\t\t# convert from the vrml True/False string to true / false\n\t\tif isinstance(v, str):\n\t\t\tv = v.lower()\n\t\t\tif v in ['1', 'on']:\n\t\t\t\treturn True\n\t\t\telif v in ['0', 'off']:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\traise argparse.ArgumentTypeError('Boolean value expected.')\n\t\treturn v\n\n\tparser.add_argument('-v', '--verify-certs', type=str2bool, nargs='"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# Flags\n\tparser = parser.parse_args([\"--help\"])\n\treturn parser.flags.as_list()\t\n\n\ndef parser_arguments(args):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its arguments as a space-separated string.\n\t\"\"\"\n\targs_json = args.__dict__\n\treturn json.dumps(args_json)\n\ndef parser_commandline():\n\t\"\"\"\n\tConstruct and return an argparse.ArgumentParser instance to be used in a new call to \n\t``parser"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags_kwargs = []\n\tfor opt_string in parser._action_strings:\n\t\tif is_flag(opt_string):\n\t\t\tif opt_string.count('='):\n\t\t\t\tflags_kwargs.append(\"{}={}\".format(opt_string[0].upper(), opt_string[1:]))\n\t\t\telse:\n\t\t\t\tflags_kwargs.append(opt_string.upper())\n\tif len(flags_kwargs) > 0:\n\t\treturn \"({})\".format(\",\".join(flags_kwargs))\n\telse:\n\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = ''\n\n\tfor item in parser._actions:\n\t\tif hasattr(item, 'action') and not item.action is Action.NO_ACTION:\n\t\t\tflags = flags + item.help +'' + str(item.action.__class__.__name__)\n\n\tflags = flags[0:-2]\n\n\tif parser.prog is None and cli_flags[-1] == '--':\n\t\tparser.prog = '%s %s %s %s' % (\n\t\t\tparser.prog[0], cli_flags[0"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \" \".join([\"{}={}\".format(k, v) for k,v in (parser.flags.items() if v is not None)])\n\treturn flags\n\ndef get_flags(parser, include_required=True, include_user=True):\n\t\"\"\"Parse an ArgumentParser instance's flags to a space-separated string.\n\t\n\tIncludes any required argument variables unless include_required is False:\n\t\n\t\tif None, use args.flags to default to get_args.args for argparse parser (for the purpose of defaulting\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags_str = parser.format_help().split()[2:]\n\tflags_dict = {}\n\tfor i, flag in enumerate(flags_str):\n\t\tflags = '{}'.format(flag)\n\t\tif flags not in flags_dict:\n\t\t\tflags_dict[flags] = 1\n\t\telse:\n\t\t\tflags_dict[flags] += 1\n\treturn list(flags_dict.keys())\n\nclass ArgError(Exception):\n\t\"\"\"\n\tException subclass for usage in the argparser parser.\n\t\"\"\"\n\tpass\n\nclass ArgparserError"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.flag_strings():\n\t\tif flags[-1]:\n\t\t\tflags += \", \"\n\t\tflags += re.escape(flag.capitalize())\n\treturn flags\n\n\ndef parser_parser(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument parser.\n\t\"\"\"\n\tfor flag in parser.flag_strings():\n\t\tprint(flag)\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.flag_values._flag_values:\n\t\tflags += \" \" + flag.get_help()\n\treturn flags\n\n# END OF FUNCTION(parser)\n\n\n# START OF FUNCTION(parser_main)\ndef parser_setup(parser, args):\n\t\"\"\"\n\tDefine a flag/argument group for use in parsing. Return a list of used flag/argument pairs.\n\t\"\"\"\n\tgroup = parser.add_argument_group(title=\"General Options\")\n\t\"\"\"\n\tA boolean flag for controlling debug output"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flagname, flagvalue in vars(parser.format_help(var_type=\"string\")).items():\n\t\tflags += \" \" + flagname + \"=\" + flagvalue\n\n\treturn flags\n"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tlogger.debug(\"Parsing arguments.\")\n\t\n\tparser_opts = argparse.ArgumentParser(add_help=False, allow_abbrev=False, conflict_handler='resolve')\n\t\n\t# Add subparser-defined options.\n\tadd_subparsers_settings_group = parser_opts.add_argument_group(title='Sub-command arguments')\n\tfor name, subparser in parser_opts.subparsers.items():\n\t\tassert isinstance(name, str), f'Name should be a string,"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# Add all positional args in the order they came in.\n\targ_names = []\n\tfor argument in unparsed_arguments:\n\t\tif argument.endswith('=') and argument.strip().endswith('='):\n\t\t\t# Don't have to strip off the '='\n\t\t\targ_names.append(argument[0:-1])\n\t\telse:\n\t\t\t# Remove hyphen and '=' from positional argument name\n\t\t\targ_names.append(argument[:len(argument)-1])\n\t# Add options and groups for all the subparsers\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = ArgumentParser(\"{!s} -a {!r}\".format(\n\t\tmodule_name.replace(os.sep, \"/\"),                                              # pylint: disable=no-member\n\t\t','.join(unparsed_arguments)\n\t)).add_help(\"{:<32}\".format(help))\n\tparsed = parser.parse_args()\n\treturn parse_arguments_out(parsed)\n\ndef parse_arguments_out(parsed):\n\tfor idx, subparser in"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# Make a map of command-line options to argparse sub-parser instances\n\t# To keep the API consistent, the command-line options with which this script was\n\t# invoked should already be mapped to an argparse ArgumentParser instance. In that case\n\t# they will automatically be replaced by the subparser name for the subparser.\n\toption_map = {}\n\tfor i, unparsed in enumerate(unparsed_arguments):\n\t\tif isinstance(unparsed_arguments, _CommandLine):\n\t\t\tunparsed_arguments ="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description = \"Subcommand subinterface to interact with and query the DB.\")\n\n\tsubparsers_map = {}\n\n\tfor name, kwargs in unparsed_arguments:\n\t\tif name == \"global\":\n\t\t\tcontinue\n\n\t\tsubparser = subparsers_map.get(\"subparsers_map\", {}).get(name, None)\n\t\tif subparser is None:\n\t\t\tsubparser = argparse.ArgumentParser(description=\"Subcommand subparser is not supported yet!\")\n\t\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targuments = {}\n\tdef add_argument(name, description, argument_type=str, argument_to_opt=(), required=False, action=None):\n\t\tif ArgumentParser(add_argument=False).add_argument(name, description=description, type=argument_type,\n\t\t\t\targument_to_opt=argument_to_opt, nargs=None, accept_abbrv=False, required=required,\n\t\t\t\tcallback=None, metavar=None, help=description, dest=name) == True:\n\t\t\tif action is not None:\n\t\t\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targuments = {}\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif unparsed_argument[0] == '-h':\n\t\t\tprint(\"-------------------------------------------------------------\")\n\t\t\tprint(\"Usage: {0} [-hn <option>] [--option=value]...\"\n\t\t\t\t\" [-i <input_file>...]. The argument passed in after is ignored.\")\n\t\t\tprint(\"Usage:\\n -h --help shows this information and\\n    examples:\")\n\t\t\tprint(\"       {0} -H      (list all sub-parsers with the given"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal subparser_map\n\targuments = dict()\n\n\thelp_ = _('Show a list of valid commands.\\nSee the help output of: ')\n\thelp_ += _('pv-help <cmd> [<args>...]\\n'\n\t\t\t\t'for more details on a desired subcommand. ')\n\n\tif len(unparsed_arguments) > 0:\n\t\thelp +=''.join(unparsed_arguments) + '\\n'\n\telse:\n\t\tpass\n\n\targuments['sub'] = subparsers("}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targs = {}\n\t# Collect the command-line arguments from the run script\n\tfor unparsed_arg in unparsed_arguments:\n\t\t# If the arg is \"global\" with no name,\n\t\t# we assume it was an implicit subparser;\n\t\t# Otherwise, it must be a subparser's name,\n\t\t# which is a key in the argv.\n\t\tif unparsed_arg == \"global\":\n\t\t\targv = \"\".join(unparsed_arguments).split()\n\t\t\t# Check if the \"-h\" option is present"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targuments_dict = {}\n\tsubparsers = {}\n\t# Parse the command-line arguments.\n\targuments = dict(unparsed_arguments)\n\n\t# Create the subparser for each command (and, if a subparser was specified, the subparser for that command).\n\tfor command, subparser in parser_map.items():\n\t\tif len(subparsers) == 0:\n\t\t\tif \"global\" not in subparser_map:\n\t\t\t\tsubparsers['global'] = create_parser(sys.argv[1:"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif len(unparsed_arguments)!= len(subparsers):\n\t\tprint('ERROR: Argument argument parser passed is empty; incorrect number of commands')\n\t\texit(1)\n\tsubparser_args, remaining_args = {}, None\n\tfor arg in unparsed_arguments:\n\t\tif len(remaining_args) == 0 or remaining_args[0] == '--' or remaining_args[-1] == '--':\n\t\t\tbreak\n\t\tsubparser_args[arg.split('=')[0]] = subparsers."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\targs = dict()\n\tnamespace = dict()\n\tremaining_args = set(unparsed_arguments)\n\t\n\ti=0\n\twhile i<len(remaining_args):\n\t\t\n\t\tif remaining_args.pop()=='--repository':\n\t\t\ti=0\n\t\t\targs.update(namespace.pop(remaining_args.pop(),dict()))\n\n\t\t\twhile remaining_args.pop(i)!='--repository':\n\t\t\t\ti+=1\n\t\t\t\targs[remaining_args.pop"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tremaining_arguments = []\n\tfor _, subparser in subparsers.items():\n\t\tresult[subparser] = subparser.parse_args(unparsed_arguments)\n\t\tremaining_arguments.extend(result[subparser]._actions)\n\treturn result, remaining_arguments\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tfor parser in subparsers.values():\n\t\tparser.add_argument_group_factory(subparsers)\n\t\tparser.add_argument_group(subparsers)\n\t\tfor subarg in parser._actions:\n\t\t\targname = subarg.dest\n\t\t\tif argname in subparsers:\n\t\t\t\tsubparser = subparsers.get(argname)\n\t\t\t\tcontinue\n\t\t\tkwargs = {}\n\t\t\tif subparser is None:\n\t\t\t\tsubparser = subparsers[argname]\n\t\t\tif subparser is not None:"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_name = unparsed_arguments[0]\n\targparser = subparsers.get_parser(subparser_name)\n\tpositional_args = argparser.add_subparsers().from_iterable(\n\t\targ.strip() for arg in unparsed_arguments[1:]\n\t\tif arg.endswith(\"=\") and not arg.endswith(\"=\") + arg[0:-1])\n\tkwargs = dict()\n\tfor arg in positional_args:\n\t\tif arg.endswith(\"=\"):\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Map from subparser name to argparse.ArgumentParser instance\n\tsubparsers_map = {}\n\tsubparsers_additional_arguments = []\n\t\n\t_map_subs_args_to_dict = defaultdict(dict)\n\t\n\t# Map subparsers, from subparser name to argparse.ArgumentParser instance\n\tfor i, subparser in enumerate(unparsed_arguments):\n\t\tsubparsers_map[subparser] = subparsers[i]\n\t\t\n\t\t# Get sub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tfor name, subparsers in subparsers.items():\n\t\t\n\t\tnamespace: Optional[dict] = {}\n\t\targuments = []\n\n\t\taction_parser: ArgumentParser = subparsers.get(name)\n\t\taction_parser.add_argument(\n\t\t\t\"-R\", \"--repodata\", dest=name, required=False, help=f\"{name} Repository\"\n\t\t)\n\t\t\n\t\tnamespace[\"repodata\"] = subparsers.get(name)\n\t\t\n\t\taction_parser.add_argument("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\targuments = set(unparsed_arguments)\n\tfor subparser in subparsers.values():\n\t\tif subparser is not None:\n\t\t\tresult.update(copy.deepcopy(subparser.choices[choice]))\n\tremaining_arguments = set(arguments)\n\tfor arg_name, arg_parser in subparsers.items():\n\t\tif arg_name not in remaining_arguments:\n\t\t\tremaining_arguments.discard(arg_name)\n\n\targs = {parser.prog: remaining"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = dict() # to preserve the argparse namespace for all subparsers\n\tfor _, subparser in subparsers:\n\t\tif subparser in result:\n\t\t\t# only one subparser is going to parse this one\n\t\t\tcontinue\n\t\tresult[str(subparser)] = parse_subparser(unparsed_arguments, subparser)\n\treturn result, unparsed_arguments\n\ndef parse_subparser_name_arguments(unparsed_arguments, subparsers):\n\t\"\"\"Get dict for subparser name arguments, based on subp"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_names = []\n\tall_args = []\n\tunparsed_names = set(unparsed_arguments)\n\t\n\tfor argument_name, subparser_name in subparsers_names:\n\t\tif subparser_name in unparsed_names:\n\t\t\traise SubcommandParseError(['argument %r appears to be already claimed by subparser %r' % (argument_name, subparser_name), sys.argv[1:]])\n\n\t\tif argument_name in unparsed_names:\n\t\t\t#"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser('PyTorch text extraction')\n\tsubparsers = parser.add_subparsers(dest='model_name')\n\ttext_extract_parser = subparsers.add_parser('text_extract', formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\t\t\t\t\t\t\t\t\t\t\t\thelp='Perform text extraction on data by calling `%(prog)s text_extract <texts> <output_path> <model_name>`')\n\ttext_extract_parser.add_argument"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\treturn make_parsers_and_functions()[0](*make_parsers_and_functions()[2])\n\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Create parser for user-specified commands and subparsers\n\tif is_arg_string(argv[1:]):\n\t\tparser = make_parsers_for_args()\n\telse:\n\t\t# Make it into a subparser and call it\n\t\tdefault_parser = make_parsers_for_subparsers()\n\t\tparser = make_parsers_for_subparsers(default_parser)\n\treturn parser\n\ndef parse_args(argv=None, usage=None, description=None, parser=None):\n\tparser"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tdef has_parser(p):\n\t\t\"\"\"\n\t\t\tReturn true iff a parser should function\n\t\t\"\"\"\n\t\tif p['options']['debug']:\n\t\t\treturn True\n\t\telif getattr(p['options'], 'parsed', None):\n\t\t\treturn True\n\t\telif p['command']=='check' and 'options' in p and p['options']['debug']:\n\t\t\treturn True\n\t\telif not p['command']=='check' or not getattr(p['options'], 'parsed', None):\n\t\t\treturn False"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=_program_name)\n\t\n\t# Required arguments\n\tparser.add_argument(\n\t\t'-p',\n\t\t'--parsers',\n\t\tmetavar='file|folder',\n\t\ttype=str,\n\t\tnargs='?',\n\t\thelp='read a file or a folder of parsers. (Optional)',\n\t\tdefault = None)\n\n\t# Optional arguments\n\tparser.add_argument(\n\t\t'-k',\n\t\t'--pkls',\n\t\tmetavar='file|folder',"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparsers = [make_parser(),\n\t\t\t  make_parser(),\n\t\t\t  make_parser()]\n\tfor parser in parsers:\n\t\tif parser.parse_args(argv=argv):\n\t\t\treturn parser, parsers\n\t# atleast one parser (the highest priority) was not found\n\tmake_parser().print_help()\n\tsys.exit(1)\n\ndef main(argv=None):\n\t\"\"\"\n\t\tStart processing command-line options and call the main function.\n\t\"\"\"\n\tparser, parsers = make_p"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# parser.\n\tparsed = []\n\t# {'__doc__': '', '__file__': '', '__name__': '__main__', '__package__': None, '__path__': None, '__spec__': <class 'PackageDocInfo'>}\n\tfor name_or_path in sys.path:\n\t\tparsed.append(p.parse(name_or_path))\n\treturn parsed\n\n\ndef main(argv=None):\n\tif argv:\n\t\tif argv[0] == '-h"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_parser = argparse.ArgumentParser()\n\tdummy_parser = argparse.ArgumentParser()\n\tparsers = (top_parser, dummy_parser)\n\treturn parsers\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparsefactory import argparsefactory\n\timport utils, sys\n\n\toptions = [\"cflags\", \"cxxflags\", \"libraries\"]\n\toptions = list(options)\n\tparser = argparsefactory()\n\tparser.add_argument(\"-p\", \"--prefix\", dest=\"prefix\", action=\"store\",\n\t\tdefault=\"\", help=\"prefix in filename\",\n\t\tnargs=\"?\", metavar=\"%(prefix)s\")\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\","}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tp_parser = argparse.ArgumentParser(add_help=False)\n\tp_group = p_parser.add_mutually_exclusive_group(required=True)\n\tp_group.add_argument(\"--verbose\", \"-v\",\n\t\thelp=\"Set logging to DEBUG\", dest=\"verbose\", action=\"store_true\")\n\tp_group.add_argument(\"--info\", \"-i\",\n\t\thelp=\"Set logging to INFO\", dest=\"info\", action=\"store_true\")\n\tp_parser.set_defaults(verbi=False,"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# First split any sequence element tuples into its constituent values (by their position's second value)\n\t(sub_sub_sub_tuple,) = re.compile(r\"(\\d+):\\s(.+)\")\n\tsub_sub_tuple = sub_sub_tuple.findall(nodes[0])\n\n\t# Now collect the sub sub sub_sub_tuples into a list with their values as it comes in\n\tsub_sub_sub_tuple_list = []\n\tfor node in sub_sub_tuple:\n\t\tkeyval"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tfor node in nodes:\n\n\t\t# the key of the node is a list of keys that are being merged (a key is a tuple which consists of:\n\t\t#   tag:yaml.org,2002:str, value:dict), and the second element of the key is an optional dict if\n\t\t#   tag:yaml.org,2002:map instead of tag:yaml.org,2002:str or another tag. This is because\n\t\t#   the given node can have several values for the same key. The list of all keys corresponding\n\t\t#   to a given list"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t# get keys from configs as a dictionary\n\tconfigs = dict((pair[0], dict(pair[1:]))\n\t\tfor pair in nodes)\n\t\n\t# make the merge as a group with a list of nodes, to be compatible with recursive traversals\n\tmerge_nodes = list()\n\trec = deep_merge_nodes\n\t\n\tfor node in nodes:\n\t    \n\t\tif type(node) == list:\n\t\t\t\n\t\t\tif configs[node[0]]['deep_merge_node_type'] == Rec"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tif len(nodes) == 0:\n\t\treturn\n\n\t# recursed down from depth first to perform merges\n\tif len(nodes)!= 0:\n\t\tdeep_merge_nodes(nodes[0])\n\telse:\n\t\treturn\n\n\t# deepmerge and merge any duplicate keys\n\tfor node in nodes:\n\t\tif type(node[1]) == DictNode:\n\t\t\tdeep_merge_nodes(node[1])\n\n###############################################################################\n# \tYAML document class\n###############################################################################\n#"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# TODO: merge a single key value dict as a merge-on\n\t#     a new key_node: [key_value_pair]\n\t#     a new mapping_node: (key_value_pair, value)\n\t#     a new key (key, value) node: (1)\n\t#     a new list_node value: [list_pairs]\n\t#     a list of list_pairs -> node: lists: [1,1]\n\t#     a nested list_node value: [2,['a'].., [['e"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tdef make_deep_merged_list(nodes):\n\t\tfor item in nodes:\n\t\t\tif isinstance(item, ScalarNode):\n\t\t\t\tyield item\n\t\t\t\tfor sub_item in make_deep_merged_list(item) or ():\n\t\t\t\t\tyield sub_item\n\t\t\telif isinstance(item, MappingNode):\n\t\t\t\tfor sub_item in make_deep_merged_list(item) or ():\n\t\t\t\t\tyield sub_item\n\tdef deep_merge_nodes(nodes):\n\t\tnodes = make_deep"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tassert all([type(el) == list for el in nodes])\n\tassert all([len(el) == 2 for el in nodes])\n\n\t# first, get the list of keys and value nodes\n\tprev = None\n\tcur = []\n\tfor el in nodes:\n\t\tprev = el\n\t\tcur.append(el)\n\tkey = list(sorted(set(el[1] for el in cur)))[-1]\n\tif key in prev:\n\t\t# if this is an implicit match, skip it\n\t\tcontinue\n\tprev = cur[0]\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tres = deep_merge_dict(nodes)\n\n\t# res now only contains scalars, so we have to traverse the tree\n\tif isinstance(res, list):\n\t\tfor node in res:\n\t\t\t# for each item in a list found in the current nested,\n\t\t\t# deep_merge_nodes calls deep_merge_dict, in the form of:\n\t\t\t#   node:   deep_merge_dict(node[\"content\"])\n\n\t\t\tnode_type = resolve_type(node[\"content\"])\n\t\t\tif node_type is not"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tnodes = [(o.key, o) for o in nodes]\n\t\n\t# first merge every value\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tif type(node) is tuple:\n\t\t\tmapping = node[1]\n\t\telse:\n\t\t\tmapping = {}\n\t\tif type(node] is dict:\n\t\t\tmerged_nodes[node['key']] = node\n\t\telse:\n\t\t\tmerged_nodes[node['key']] = node['value']\n\t\t\n\t# merge duplicates\n\t"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# TODO:\n\t# - create a deep_merge_scalar_node for merge_nodes\n\t# - convert node values and insert them into the first level of the merging list for the\n\t#   bogon (bogon/deep_merge_nodes function)\n\t# - merge the values in the second level of the merging list\n\t\n\tdef deep_merge_scalar_nodes(nodes):\n\t\t\"\"\"\n\t\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\t"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\ttry:\n\t\tfrom argparse import ArgumentParser\n\n\t\treturn ArgumentParser(*arguments)\n\texcept ImportError:\n\t\tprint(\"ImportError: cannot import argparse: could not find subcommand '\" + \" \".join(arguments[1:]) + \"'\")\n\t\tprint(sys.exc_info()[2])\n\t\texit(-1)\n\ndef main(*args, **kwargs):\n\t\"\"\"\n\tThe main function of the Program.\n\t\"\"\"\n\targs = parse_arguments(*args)\n\targs.print_help()\n\targs.run("}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targuments = arguments[0].split(\" \")\n\t# Handle single or double quoted strings\n\tif any(arg in arguments[0] for arg in [\"'\", '\"']):\n\t\targuments = arguments[0].split(\"'\") + arguments[1:]\n\n\tif len(arguments) == 0:\n\t\tparser = ArgumentsParser()\n\t\tif args.arguments is not None: # for python3 compatibility\n\t\t\tparser.arguments_from_string(args.arguments)\n\t\treturn parser\n\n\targuments = []\n\t# This should be handled by"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(prog=os.path.basename(sys.argv[0]), add_help=False)\n\tparser.parameters_ = arguments\n\tparser.add_argument(\"--help\", dest=\"help\", action=\"help\")\n\tfor cls in parser._get_actions():\n\t\tif cls.hidden:\n\t\t\tcontinue\n\t\tif hasattr(cls, \"add_args\"):\n\t\t\tcls_args = cls.add_args(parser)\n\t\t\tparser.add_argument(*cls_args.args)\n\t\t\tif"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\",\n\t\t\t\t\t\t\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument('-i', '--interface', dest='interface', help='Interface to change MAC Address', metavar=\"INTERFACE\", default=\"lo\")\n\n\tmask = IPNetwork(arguments[-1])\n\tmasked1 = mask / ip.IPNetwork('255.255.255.255')\n\n\tparser.add_argument('-c', '--new-mac', dest='new_mac',"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser()\n\tparser.add_argument(\"--t1_file\", dest=\"t1_file\", default=\"\",\n\t\thelp=\"input T1 data file\")\n\tparser.add_argument(\"--t1_index\", dest=\"t1_index\", default=\"\",\n\t\thelp=\"input index name\")\n\tparser.add_argument(\"--t1\", dest=\"t1\", default=\"\",\n\t\thelp=\"input T1 file\")\n\tparser.add_argument(\"--t2_file\", dest=\"t2_file\", default=\"\",\n\t\thelp=\""}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Parse global arguments\n\tglobal debug\n\tglobal test\n\tglobal test_list\n\tglobal testlist\n\n\t\"\"\"\n\tCommand-line arguments\n\n\targuments = [...]\n\n\t# Default arguments\n\targuments = (sys.argv[:1] + arguments + sys.argv[1:])\n\n\t# Suppoer debug only arguments\n\t# if len(arguments) > 1: arguments.remove(arg_spec)\n\targuments = arguments + sys.argv[1:]\n\n\t# Enumerator\n\tenumerator = enumerate(arg"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Get command line arguments (i.e., ones with __args__) as an ArgumentParser instance.\n\tparser = ArgumentParser(*arguments)\n\t# Store, then, defaults values, via the module-level parse_arguments, as variables.\n\tparser._optionals = parser._actions = parser._mutually_exclusive_groups = parser._has_required = parser.add_mutually_exclusive_group()\n\tparser.add_argument(\"file\", metavar=\"FILE\",\n\t\t\t\t\t\thelp=\"Input DIR/FILE as either a DIR/FILE in which to work"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Create a parser with all the necessary command-line arguments.\n\tparser = get_argument_parser()\n\n\t# Get all of the command-line parameters.\n\ttry:\n\t\tif arguments:\n\t\t\toptions, _ = parser.parse_known_args(arguments)\n\t\telse:\n\t\t\toptions, _ = parser.parse_known_args()\n\texcept (TypeError, ValueError) as err:\n\t\t# If there are any errors or arguments provided that aren't type or value-error,\n\t\t# raise the exception and continue with the next.\n\t\traise Exception"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tfrom argparse import ArgumentParser as ArgParser\n\n\tparser = ArgParser()\n\tparser.add_argument(\"-f\",\"--filename\", type=str,\n\t                    help=\"File to process\",default=None)\n\tparser.add_argument(\"-p\",\"--process\", action=\"store_true\",\n\t                    help=\"Process all.csv files in one place\",required=False)\n\tparser.add_argument(\"-s\",\"--stats\", action='store_true',\n\t                    help=\"Print some statistics about the input dataframe\",required=False)\n\tparser.add"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\n\t# create an ArgumentParser instance, which represents the argparse\n\t# interface for our script, and takes the defaults from which we can build a parser with later configuration\n\tparser = ArgumentParser(\n\t\tdescription=\"Reads a single.c3 files and calls a script.\",\n\t\tformatter_class=RawDescriptionArgumentDefaultsHelpFormatter\n\t\t)\n\n\t# add some required arguments\n\tparser.add_argument(\n\t\t\"-i\",\n\t\t\"--input\",\n\t\tdest=\"input\",\n\t\taction=\"append\",\n\t\ttype=path"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tfor flag in parser.flags.keys():\n\t\tprint(\"\\t    -   %s\" % (\":\".join((flag, parser.flags[flag]))))\n\treturn \"\"\n\n\ndef parser_log_format(parser, *args):\n\tlog_format = parser.log_format\n\tlog_format_str = []\n\tfor arg in args:\n\t\tfor prefix in ARG_PREFIX.keys():\n\t\t\targ_flag = args.index(arg)\n\t\t\tif flag_to_flag(flag_str(log_format[_prefix]), arg_"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = parser.flags.as_dict()\n\n\t# flag_defs = [flag.name for flag in parser._action_groups[0].get_group('FlagGroup').actions if flag.type == 'GROUP']\n\t# for flag_def in flags.keys():\n\t# \tflags[flag_def] += '| '+flags_to_string(flags[flag_def])\n\n\tflags_dict = {}\n\tfor flag in flags.keys():\n\t\tflags.update({flag: flags[flag]})\n\treturn flags_dict\n\n\ndef get"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tstr_ = \" \".join(sys.argv)\n\treturn str_.format(*sys.argv)\n\ndef parser_error_message(error_message, help=None, sub_help=None):\n\t\"\"\"\n\tHelper function to display an error message and, optionally, add a subparser to display it in a formatted\n\tmore suitable way.\n\t\"\"\"\n\tif help is None:\n\t\thelp = \"\"\n\tif sub_help is None:\n\t\tsub_help = \"\"\n\tdescription = \"\"\n\tif len(help) <= MAX_HELPL"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tif parser.prog:\n\t\tflags = flags + \".{}\".format(parser.prog)\n\tif parser.verbose or parser.verbose_count > 1:\n\t\tif isinstance(parser.verbose_count, int):\n\t\t\tflags = \" \".join((flags, repr(parser.verbose_count)))\n\t\telse:\n\t\t\tflags = \"{0}.{1}\".format(flags, 'T'.join(parser.verbose_count))\n\tif parser.trace:\n\t\tif isinstance(parser.trace_count"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tif parser.formatter_class:\n\t\tflags.extend([fmt.flag.no_hyphen(flag.no_hyphen_flag) for flag in parser.get_formatter().format_flags()])\n\tif getattr(parser, 'auto_wrap_lines', True):\n\t\tflags.extend([flag.wrap() for flag in parser.get_formatter().format_flags()])\n\treturn \" \".join(flags)\n\n####################################\n# MAIN\n####################################\ndef parseCmdline():\n\t\"\"\"Get arguments"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = parser.parse_args().flags\n\targs_joined = parser.format_args()\n\treturn args_joined + len(flags.argv) * \" \"\n\n\ndef get_flag_value_by_key(flag, section):\n\t\"\"\"\n\tGiven a flag name and its corresponding flag section ('auth'), return its associated default value.\n\t\"\"\"\n\ttry:\n\t\tflag_value = FLAGS[section][flag.lower()]\n\texcept KeyError:\n\t\tflag_value = None\n\n\treturn flag_value\n\n\ndef print_help_"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tfor flag in parser.flags:\n\t\tif flag[0] not in parser.option_strings:\n\t\t\tval = parser.getdefault(flag[0])\n\t\t\tflag = \"--\" + flag[0] \n\t\t\tif val is not None:\n\t\t\t\tdescription = val\n\t\t\telse:\n\t\t\t\tif parser.option_strings:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tdescription = parser.get_default(flag[0])\n\t\t\t\t\texcept IndexError:\n\t\t\t\t\t\tdescription = repr(flag[0])\n\t\t\t\telse:\n\t\t\t\t\tdescription = repr(parser.option_strs[flag])\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = dict(parser.flags)\n\tif hasattr(parser, '_flag_values_cache') and hasattr(parser._flag_values_cache, 'items'):\n\t\t# Only for test suite\n\t\t# workaround for an issue #1775\n\t\t# https://github.com/PyYAML/pyyaml/issues/1775\n\t\tflags_py = dict([(n, v) for n, v in parser._flag_values_cache.items() if v is not None])\n\t\tif flags_py:\n\t\t\t# Only for test suite\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags=[]\n\tfor a in parser.flags:\n\t\tflags.append(a.ljust(80))\n\treturn \"\".join(flags)\n\ndef parse_arguments(parser):\n\t\"\"\"\n\t\tGiven a parser.parse_args() instance, return the parser arguments.\n\t\"\"\"\n\treturn parser.parse_args()\n\ndef args_to_dict(args):\n\t\"\"\"\n\t\tGiven a parser.parse_args(), return a Dictionary of Arguments.\n\t\"\"\"\n\treturn vars(args)\n\ndef get_cmd_str(msg"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser_flags = \" \"\n\n\tif parser._option_string_actions:\n\t\t# If we have option-groups, we merge them with the existing string action\n\t\t# defaults into the parser.\n\t\tif not getattr(parser._action_groups, \"__getattr__\", None):\n\t\t\tif isinstance(parser._option_string_actions, action._StoreAction):\n\t\t\t\tparser._option_string_actions.__setattr__(\n\t\t\t\t\t\"defaults\", [(None, None) for k in range(len(parser._option_string_actions))])\n\t\t\telse"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\targs = os.environ\n\t# pylint: disable=protected-access\n\tcommand_line_arguments = _completion_arguments_from_completion_script(args=[_completion_script])\n\tfrom jinja2 import Environment, PackageLoader\n\tenv = Environment(loader=PackageLoader(\"borgmatic\", \"templates/borgma\"))\n\n\t# find the longest common prefix with all the command arguments.\n\tbash_completion_prefix = max(map(len, command_line_arguments), key=len) + len('-') +"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport os\n\ttry:\n\t\tfrom subprocess import PIPE_BUF, Popen, check_call\n\texcept ImportError:\n\t\ttry:\n\t\t\tfrom pipes import PIPE_BUF\n\t\t\tfrom pipes import PIPE_CLID\n\t\texcept ImportError:\n\t\t\tfrom tempfile import TemporaryFile\n\t\t\tclass PopenShell(object): # pylint: disable=redefined-builtin\n\t\t\t\tdef __init__(self, *args, **kwargs):\n\t\t\t\t\tself.args = args or []\n\t\t\t\t\tself.kwargs = kwargs or"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tscript = \"\"\"\n#\n# Copyright 2013, Michael Collins, mcolin@gmail.com\n#\n# Distributed under the terms of the BSD or the Apache license, see the\n# COPYING file.\n#\n# This implementation adapted from https://github.com/yelp/mockup/blob/master/mockup/runbot/runbot.sh -> runbot_gmake.c\n#\n#\n# This file helps to setup the command-line parsers for this code. We used\n# it to determine the command-line arguments and some helper"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn \"\"\n\ndef do_bash_completion():\n\t\"\"\"\n\tPerform a bash completion script based on borgmatic command line parsers.\n\t\"\"\"\n\n\t# Prepare a bash call for the borgmatic command. This will create a temporary\n\t# bash command that needs to be executed before the command line parsers can\n\t# be evaluated.\n\tbash_cmd =''.join(*(f'{bashCommandPath}', bashcompletion.bash_completion))\n\n\t# Create bash command to invoke. The return code will match the status code of\n\t# the"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport os\n\timport subprocess\n\tfrom distutils.version import StrictVersion\n\n\t# Check for compatibility only\n\tif StrictVersion(subprocess.check_output('git version --num', shell=True).decode('utf-8').strip()).is_prerelease:\n\t\tprint((\"*** WARNING: Shell subprocess version and above is recommended. Consider it to be the last python version used (git version --num) to be backwards-compatible. \"\n\t\t\t \"(See https://www.python.org/dev/peps/\"\n\t\t\t \"#current-python-versions"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport os\n\timport subprocess\n\n\tclass FileGet(object):\n\t\tdef __init__(self, file_directory):\n\t\t\tself.directory = file_directory\n\t\t\tself.script_path = \"file://{}\".format(os.path.realpath(\".\"))\n\t\t\tself.command = 'cd {} && bash -c \"bash -i\"'\n\t\t\tself.result_path = '{}/result.txt'.format(self.directory)\n\t\tdef get_command(self, result):\n\t\t\tsubprocess.check_call([self.command, '-"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t# Generate bash completion script\n\tbashcomplete = \"bash bash_completion.sh\"\n\n\t# Do a getopt call and convert to string\n\tbash_options = [\"bash\", \"-c\", \"bash_completion\"]\n\topts, args = getopt.getopt(bash_options, \"h\")\n\n\t# If no args, report user what the argument is given.\n\tif not args:\n\t\tprint(\"You must specify a command to display.\")\n\t\tprint(bashcomplete)\n\t\texit()\n\n\tbash_help = \"Returns a bash script for interpreting the"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn \"\"\"if! __name__ == \"__main__\":\n\t\timport shlex, os\n\t\tcmdline = [ \"$0\" ]\n\t\tsysargs = os.path.split( os.path.expandvars( sys.argv[1] ) )\n\t\targs = shlex.split( sysargs[1] )\n\t\tprint( \"$0\" )\n\t\tprint( args )\n\t\tprint( \"$0\" )\n\"\"\"\n\n\ndef my_print_sh(arglist, args, tkinter_arguments=None):\n\t\"\"\"\n\tRun a"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport borgmatic_command_parser.bashcomp\n\n\treturn bash_completion.bash_exec(\n\t\tshell=False,\n\t\tcomp=dict(\n\t\t\tpython=dict(\n\t\t\t\tbash_args=[\"-x\", \"-y\"],\n\t\t\t\texec=[\"-e\", \"python\"]),\n\t\t)\n\t)\n\nbash_completion_bash = \"\"\"\\\n{\n\t\"bash_completion\": {bash_exec: {\n\t\t\"python\": [\"-g\", \"-x\", \"-c\", \"import sys;  print(\",\".join(sys.arg"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tbash_completion__ = {}\n\tcompletion = open('bash_completion.sh', 'w')\n\tfor key in bash_completion:\n\t\tbash_completion__['bash_completion'] = 'bash_completion.bash_completion_' + key\n\n\tbash_completion.write(open('../../bash_completion.sh').read())\n\tbash_completion.close()\n\treturn bash_completion__\n\n\ndef run_bash(command: str) -> str:\n\t\"\"\"\n\n\tParameters:\n\t\tcommand (str):"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\toutput = defaultdict(dict)\n\t# Parse the passed command-line arguments\n\ttry:\n\t\tfor unparsed_arg in unparsed_arguments:\n\t\t\tname, arg = unparsed_arg.split('=', 1)\n\t\t\toutput[name]['output'] = arg\n\texcept ValueError:\n\t\tpass\n\t# Parse the arguments passed to the script\n\tfor argument in args:\n\t\tfor name, arg in argument:\n\t\t\toutput[name]['args'] = arg\n\treturn output\n\ndef get_file_list(dir"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# pylint: disable=too-many-branches\n\tglobal_parser = None\n\targuments = {}\n\n\t# parse subcommands\n\tif subparsers is None:\n\t\tsubparsers = parser.add_subparsers()\n\n\t# parse first subparser in the list of subparsers. This is needed for cases where all subparsers are run, but\n\t# some of the subparsers are missing:\n\twhile subparsers is not None:\n\t\tcurr = subparsers.pop(0).get_parser()\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targparser = argparse.ArgumentParser()\n\targparse.ArgumentParser.add_argument('dir', help='Directory to source and convert')\n\trootdirs = argparser.add_argument_group('roots')\n\trootdirs.add_argument('-v', '--verbose', action='store_true', help='Display verbose information')\n\trootdirs.add_argument('--crcs', type=int, default=0, choices=[0, 1, 2, 3, 4, 5],\n\t                     help='For each source root and"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tret = {}\n\tfor subparser_name in subparsers.names:\n\t\tsubparser = subparsers[subparser_name]\n\t\tif subparser is not None:\n\t\t\tret[subparser_name] = parse_arguments(subparser)\n\treturn ret\n\ndef main(argv):\n\t# Set up argparse.\n\tsubparsers = get_argparsers()\n\targuments = parse_arguments(*argv)\n\t# Print output.\n\t# TODO: If this is ever fixed/modified to use a loop"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription     = \"A set of tools for running a task in multiple processes across an input and output.\"\n\t)\n\n\tgroup = parser.add_argument_group(\"input arguments\")\n\n\tgroup.add_argument( '-a',\n\t\thelp    = \"Specifies an input (input_filename or pipe, one of, pipe, stdin, stderr, stream, file, stdout, stderr as the default).\",\n\t\tdefault = 'pipe'\n\t\t)\n\n\tgroup.add_argument('--output',\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparsed_arguments = {\n\t\tparser.prog: parser for parser in subparsers.choices if parser.option_strings}"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# TODO(@dwyerw): move this from ArgumentParser to ArgumentParser.py so we can easily handle\n\t#       multiple subparsers with this utility function.\n\t#       It should be possible to change the behavior here.\n\tparser = ArgumentParser()\n\tsubparsers = parser.add_subparsers(title='subcommands',\n\t\tdescription='subcommand_name (optional), usage=usage_text (optional), help=help_text (optional)')\n\t#   name, usage, help, and long help support\n\tparser.set"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport os\n\n\t# This is where the main() method is instantiated...\n\t# It will be overridden in sub-classes that override \"main\".\n\treturn {}\n\n\t# This is where the subparsers (grouped at the top) are instantiated\n\n\t# And here's what it says...\n\n\n\treturn {}\n\n\t# This is the default subparser configuration\nclass DefaultConfig(argparse.ArgumentParser):\n\t\"\"\" The class of the default subparser configurator (which is just a wrapper around a 'global' ArgumentParser object).\n\t\tThis class is also"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# In the global namespace, load all options from the command-line in section \"Options\"\n\toptions = dict(vars(parse_arguments.original_parse_args(cmd_args)[0]))\n\t# Iterate over all the command line arguments (i.e., subparser) and replace any references to them in the\n\t# global namespace with the argparse parse values (i.e., namespace instance)\n\tfor name, option in command_line_arguments.__members__.items():\n\t\tif name!= \"original_parse_args\":\n\t\t\toptions["}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal parser\n\t\n\ttry:\n\t\tnamespace = cli_parse_arguments(*unparsed_arguments)\n\texcept ArgumentParserError as err:\n\t\tparser.exit(args=unparsed_arguments, message=str(err), exit=1)\n\telif namespace is None:\n\t\tparser.exit()\n\tif namespace['command']!= 'predict-model':\n\t\tparser.exit('Only subparsers \"predict-model\" can be subparsed.')\n\tif namespace['dataset'].lower() not"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targs = []\n\tremaining_argv = []\n\n\tparser_class = None\n\n\tfor arg in unparsed_arguments:\n\t\tname, help = argv_split(arg)\n\t\targs.extend(namespace(parser, name, help))\n\t\tremaining_argv.extend(unparsed_arguments[arg])\n\n\t# Use this to skip one subparser in the list above (because name cannot depend on\n\t# subparser's parent)\n\tif subparsers.get_nth_node(1):\n\t\tparser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tnamespace = {}\n\tsubparsers_used = set()\n\tfor arg in unparsed_arguments:\n\t\tparts = arg.split('=', 1)\n\t\targ = parts[0]\n\t\tif arg[0] == '_':\n\t\t\targ = arg[1:]\n\t\ttry:\n\t\t\tcommand = namespace[arg]\n\t\texcept KeyError:\n\t\t\tif arg in subparsers_used:\n\t\t\t\traise ArgumentError(\"Subcommand '%s' was not used.\" % arg.capitalize())\n\t\t\tcommand = subparsers._add_parser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\tremaining_args, subparser_args, remaining_default_args = {}, {}, {};\n\t\n\tfor action_type, subparser_dict in subparsers.items():\n\t\tif action_type == \"init\":\n\t\t\t# If init, just populate \"args\" with the subparser args and the rest of the args,\n\t\t\t# and add default_args for subparser \"setup\", \"cleanup\" and \"prepare\"\n\t\t\tsubparser = subparser_dict[\"setup\"]\n\t\t\t\n\t\t\tparsed_args, _ = subparser."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Initialize namespace dict with arguments and sub parsers\n\tnamespace, subparsers_args  = {}, {}\n\tfor parser in subparsers.values():\n\t\t# Subparsers are called with the current parser as the first argument. Find all remaining arguments\n\t\t# and split them into list.\n\t\targs = parser.parse_args(unparsed_arguments[1:])\n\t\tnamespace[parser.name] = ArgNamespace(parser, args)\n\t\tsubparsers_args[parser.name] = args\n\n\t# Return the resultant"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Create a name to hold parsed namespace so that it is never overwritten.\n\t# This is so that an add subparser action can easily update namespace from the\n\t# namespace in a subsequent call.\n\targ_names = argparse.Namespace(subparsers.add_parser(name, help=\"subparser for %r: %r\"%(unparsed_arguments, unparsed_arguments[1:],), parents=[unparsed_arguments]))\n\targs_dict = {arg.dest: arg for arg in subparsers."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsub_parser_names = []\n\tsub_parser_names_for_args = []\n\tknown_args = {}\n\tparsed_namespace = {}\n\tfor argument, parsed_namespace in zip(unparsed_arguments, subparsers.actions):\n\t\tname = argument.dest\n\n\t\targ_kwargs = parse_required_arguments(argument)\n\t\tparsed_namespace[name] = arg_kwargs[ATTR_TYPE]\n\n\t\tif hasattr(argument, \"required_by\"):\n\t\t\ttry:\n\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tnamespace_dict = {}\n\n\t# A map from subparser name to a dict of subparsers to their namespaces (a.k.a.'subparsers' or\n\t# a dictionary). This is necessary as subparsers are registered by their top-level namespace\n\t# and are not explicitly given in the arguments at a given subparser level. This is a list of\n\t# subparsers to which the named arguments will be added.\n\tsubparsers_namespaces_map = {}\n\n\tparsers = []\n\n\targ_group = \"\"\n\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_parsed_argsuments, argparse_namespace_to_argument = {}, []\n\tall_parsed_arguments_by_subparsers = []\n\n\t# First, give all commands and add subparsers to namespace using args\n\tfor k, v in unparsed_arguments:\n\t\tsubparser, _ = find_subparser_name(k, v)\n\t\tall_parsed_argsument_by_subparsers.append(k)\n\t\tif subparser is not None:\n\t\t\tnamespace = argparse"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\tparsed_subparsers = {}\n\t# Find all known subparsers\n\tknown_subparsers = set(subparsers._subparsers._keys())\n\tif \"all\" in known_subparsers:\n\t\tknown_subparsers.remove(\"all\")\n\t\n\tfor subparsers_name in known_subparsers:\n\t\tsubparser = subparsers[subparsers_name]\n\t\t# Create namespaced parser\n\t\tsubparser_name = subparsers_name.replace(' ',"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser, missing_names, remaining_names = next((tup for tup in args_to_names(parsed_arguments).items() if tup[1]), None)\n\tsubparsers = subparsers.as_dict()\n\tif subparser:\n\t\targparse_config.add_subparsers(**subparsers)\n\t\tparsed_arguments = dict((tup[1], subparsers.get(subparsers.pop(subparsers.keys()[0]))(\n\t\t\t*"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t_PARSER = OptionParser()\n\t_PARSER.add_option(\n\t\t\"-V\", \"--version\", dest=\"version\", metavar=\"version\",\n\t\thelp=\"print program's version number and exit\")\n\n\treturn _PARSER.parse_args()[0:2]\n\n\nparsers = make_parsers()\n\n# Run a routine\n#\ndef run():\n\t\"\"\"\n\tThe entry point for all commands. This entry point will be the main\n\tfunction of this module. Execute this one by command line:\n\t./makecommand."}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# initialize parser\n\tparser = argparse.ArgumentParser()\n\t# allow a subparser to be specified as well\n\tsubparsers = parser.add_subparsers()\n\t# let a parser have its own subparser\n\treturn parser, subparsers\n\n\ndef configure(options):\n\t\"\"\"\n\t\tConfigure the parser for the commands with a default arguments.\n\t\"\"\"\n\t# initialize and execute command if option is set or called with the correct arguments.\n\tif options.cmd in ['make', 'build' ]:\n\t\tcommand(options)"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = make_parser(description)\n\tsubparsers = parser.add_subparsers(help=\"command to run\", dest=\"cmd\")\n\n\t# make the parse from the subparser\n\t# add_subparser('cmd', subparsers=['cmd'])\n\targs = parser.parse_args()\n\treturn args.cmd, subparsers\n\n\n'''\n\tTest\n'''\nif __name__ == \"__main__\":\n\n\t# Parse the command-line and parse it,\n\t# for a list of commands use the parse"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport sys\n\tglobal parser\n\tfrom optparse import make_option\n\n\t(parser) = make_parser(usage='usage: %prog [options] [-h] [package [depends]]...')\n\t(opt, args) = parser.parse_args()\n\n\tif not hasattr(opt, 'help'):\n\t\tif opt.script is None:\n\t\t\tparser.error('[-] please specify script to show or list it with -h')\n\t\t\tsys.stderr.write('[-] please specify script to show or list it with -h\\"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = ETTreeParser()\n\tparser.add_tag('tagname', 'tagname')\n\tparser.build_parser(None, parser)\n\tparser.add_child(parser.make_parser('args')) # note the parent is used for the child\n\treturn parser.get_parser('args') # return the parser as a tuple\n\n\ndef parse(source,\n\t\tparser=None,\n\t\tdebug=None):\n\t\"\"\"\n\tLoad a source and parse it with a parser defined in make_parsers().\n\t\"\"\"\n\t# if a parser is"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = parser.add_subparsers(help='your custom help')\n\n\tparser.add_argument(\n\t\t'--debug', action='store_true', default=False,\n\t\thelp='Run in debug mode')\n\n\tparser.add_argument(\n\t\t'--loglevel', action='store', type=int, default=20,\n\t\thelp='Log level')\n\n\tparser.add_argument('--host', '-h', default='127.0.0."}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport parsers\n\tparser = parsers.parse_args()\n\tif parser.description:\n\t\ttext = open(parser.description, 'r', encoding='utf-8').read()\n\t\tparser.description = parser.description.replace('\\\\n', '\\n')\n\t\tparser.description = parser.description.replace('\\n', '\\n\\n')\n\t\ttext_lines = text.split('\\n')\n\t\tfor i in range(len(parser.toplevel)):\n\t\t\tparser.toplevel[i] = parser.toplevel["}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tp = argparse.ArgumentParser(prog='clap', usage='')\n\tp.add_argument('-s', '--sensor', type=int, default=0, help=\"The sensor\")\n\tp.pass_argument('args', argparse.Namespace())\n\treturn (p, p.add_subparsers().add_parser('clap',\n\t\tparents=[p], usage='<CLAP COMMAND: [args]>'))\n\"\"\"\n\tRun the parser, returning the root parser. It exits to the main loop on the command"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tpars = argparse.ArgumentParser()\n\t\n\t# Create and add subparsers to parser\n\tadd_parser_args(pars)\n\t\n\t# Return tuple\n\treturn pars, {}\n\t\n# end function\n\n# Main Function\ndef main(args = None):\n\t\"\"\"\n\tWrapper for argparse.\n\t\"\"\"\n\tparser, kwargs = make_parsers()\n\tparset, kwargs = parser.parse_known_args(args)\n\t\n\t# Return argparse results\n\tkwargs."}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = cmd_parser.parse_args()\n\tparser._name = ('',)\n\n\t# Add some subparsers for subcommands\n\tsubparsers, subcommands = parser.add_subparsers()\n\n\t# Add the common options (i.e., the subparser name)\n\targs = parser.parse_args()\n\tparser._name = args.sub\n\tp = subparsers.add_parser(args.name, help='Subcommand.py <option>\\n\\n%s', formatter_class=argparse.Arg"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor result in response:\n\t\tfor _ in response.resultIterator(result):\n\t\t\tyield _\n\t\treturn None\n\traise StopIteration\n\nclass WapiParser(Wapi):\n\t\"\"\"\n        A parser for the XML in a WAPI Response.\n        \n        A WAPI Response object can be accessed via attributes.\n        In addition to the response object it can contain additional\n        data that will be automatically converted to Python objects.\n        This can be accessed via an additional attributes (e.g. the\n        body as dict), as well as the response body.  Note"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tnext_link = []\n\twhile response:\n\t\tlinks, data = response[0]['paging']['items']\n\t\tnext_links = response[0].get('nextLink', False)\n\t\tdel response[0]\n\t\t\n\t\tif next_links:\n\t\t\t# If we have only items, then we can split the page by page\n\t\t\t# so we do not need to make a page per page of the\n\t\t\t# data.\n\t\t\tfor page_index in range(1,len(links) + 1):\n\t\t\t\tlink = links[page_index]['"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t#\n\tfor i in response['paging']['totalItems']:\n\t\t# For each page/pages\n\t\tfor j in range(0,int(i['counts'])):\n\t\t\t# Collect objects\n\t\t\tself.objects_all.append({\n\t\t\t\t'filename':i['objectId'],\n\t\t\t\t'id':i['id'],\n\t\t\t\t'name':i['name'],\n\t\t\t\t'owner':i['ownerId'],\n\t\t\t\t'width':i['width'],\n\t\t\t\t'height':i['height'],\n\t\t\t\t'size':i['size"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif len(response) == max_results:\n\t\treturn response\n\tpage = response[:max_results]\n\treturn page\n\ndef _paging_generator(response_page):\n\t\"\"\"\n\tHelper class for paging function\n\t\n\tArgs:\n\t    response_page (list): generated page by generator function.\n\tReturns:\n\t    Iterator with WAPI response for each item in the page. \n\t\"\"\"\n\tfor response in response_page:\n\t\tyield response\n\t\t#next_page = response['next_page'] # may not"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_iterator = iter(response)\n\ttry:\n\t\tyield next(page_iterator)\n\texcept StopIteration:\n\t\t# No more pages to iterate over.\n\t\treturn\n\telse:\n\t\tcurr_page_iterator = page_iterator\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tcurr_page = next(curr_page_iterator)\n\t\t\texcept StopIteration:\n\t\t\t\traise StopIteration\n\t\t\telse:\n\t\t\t\tif not hasattr(curr_page.paging.page, 'paging'):\n\t\t\t\t\t# This is"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < max_results:\n\t\tif response[i] not in _VALID_KEYWORDS:\n\t\t\tresponse.remove(response[i])\n\t\t\ti -= 1\n\t\tyield response[i]\n\t\ti+=1"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < len(response):\n\t\tyield response[i:i+max_results]\n\t\ti += max_results\n\n##  @_\n##  @file methods.py\n##\n##  Copyright (c) 2019 Oliver Horeng (allot@google.com)\n##\n##  Permission is hereby granted, free of charge, to any person obtaining a copy\n##  of this software and associated documentation files (the \"Software\"), to deal\n##  in the Software without restriction, including without limitation the rights\n##  to use, copy"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\tdef sort_by_id(d: dict, s: str):\n\t\tif s == 'createdAt':\n\t\t\td.sort(key=sig_to_date(d.get('createdAt')), reverse=True)\n\t\telif s == 'updatedAt':\n\t\t\td.sort(key=sig_to_date, reverse=True)\n\t\treturn d\n\n\tsortables = ['createdAt', 'updatedAt']\n\t\n\tfor d in response:\n\t\td['paging'] = {k: int(d[k]) for k in"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t# Generate a list of WAPI objects\n\tfor i in response:\n\t\tyield i\n\t\t\n\t\t# If there's max_results left at current page then go to next page\n\t\tif (len(response) == max_results):\n\t\t\tcontinue\t# Go to next page\n\t\t\n\t\t# Stop here if number of remaining pages is less than max\n\t\tif (i['total'] > max_results):\n\t\t\tbreak"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_size = 100\n\tpage = [{'page': 1,'results': max_results,'size': max_results, 'total': 200}, \n\t\t\t{'page': 1,'results': max_results,'size': max_results, 'total': 200},\n\t\t\t{'page': 2,'results': max_results,'size': page_size, 'total': 200},\n\t\t\t{'page': 2,'results': max_results,'size': page_size, 'total': 200},\n\t\t\t{'page': 3,'results':"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tsize = to_bytes_int(int(size)).to_int()\n\t\n\tfor i in range(1, 10):\n\t\tif int(size * (10 ** i)) == int(size * (10 ** i)):\n\t\t\tbreak\n\t\n\treturn int(float(int(size) * (10 ** i)))\n\t\n\t# result = (math.floor(math.log10(file)) - 1)\n\t\ndef time_to_seconds(time: str) -> float:\n\t\"\"\"\n\t\tConvert human readable seconds from human representation to fraction"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t# NOTE\n\t# https://stackoverflow.com/a/28443523/395844\n\tif size < 1000:\n\t\treturn round(size)\n\treturn size / thousands_to_bytes(size)\n\n# https://stackoverflow.com/a/45339962/395844,https://stackoverflow.com/a/61782068/395844\ndef size_to_thousands(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to decimals.\n\t\n\tResulting"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tnbytes = -1\n\tparts = size.split('K')\n\tif len(parts) == 1:\n\t\tnbytes = 10 ** -len(size[:-1])\n\telse:\n\t\tfor part in parts:\n\t\t\tnbytes *= 10\n\t\t\tnbytes += int(part) ** -len(part)\n\treturn int(nbytes)\n\t\t\n\ndef convert_bytes_to_size(bytes: int,\n\t\t\t\t\t\t from_human: str = \"\",\n\t\t\t\t\t\t from_human_readable: str = \"\",\n\t\t\t\t\t\t from_natural: str ="}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tif size.startswith(\"K\"):\n\t\treturn int(size[1:])\n\treturn int(size[0:2])\t   \n\n\n#-------------------------------------------------------------------------\n# Function: is_file_in_path\n#-------------------------------------------------------------------------\ndef is_file_in_path(path: str) -> bool:\n\t\"\"\"\n\t\tTakes a path and returns true if the path exits and false otherwise.\n\t\t\n\t\tArgs:\n\t\t    path (str): a path to check\n\n\t\tReturns:\n\t\t    boolean: whether a path exists or not\n\t\t    Examples::"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tparts = size.split(\"K\", 1)\n\tlength = abs(int(parts[0]))\n\tsize_split = parts[1]\n\tif size_split == \"M\":\n\t\tlength *= 1024\n\treturn length\n\n\n@type_check(value: Optional[Callable[[object, str, str, Callable[[str], str]], bytes]) -> Optional[object]\ndef is_path_writeable(\n\t\tobject: Callable[[object, Callable[[str], str]], bytes],\n\t\tpath: str,\n\t\tis_dir:"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn int(size)*1024\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\ttry:\n\t\tunit = size.strip(\"K \").strip(\"M \").strip(\"G \")\n\t\tif unit == \"\":\n\t\t\traise ValueError(\"'size' cannot be empty string.\")\n\t\treturn -2 ** 31\n\texcept:\n\t\tsize = float(size) / file_size\n\n\t\t### if size is an average, convert to bytes ###\n\t\tif not unit.upper().startswith(\"A\"):\n\t\t\tunit = unit.upper()\n\t\t\tif unit not in [\"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \""}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn int(size, 10)\n\ndef make_safe(string: str) -> str:\n\t\"\"\"\n\t\tConvert `string` to ascii and remove spaces and convert to upper case in case there is leading spaces or symbols in a string.\n\t\t\n\tArgs: \n\t\tstring: A string\n\n\tReturns: A string\n\n\tExamples::\n\n\t\t>>> make_safe('test')\n\t\t'test'\n\t\t>>> make_safe(\"   This problem needs resolving\")\n\t\t'This problem needs resolving'\n\t\t>>> make_safe('1 -2')\n\t\t'-2"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tif \".\" in size:\n\t\treturn int(size)\n\telse:\n\t\treturn 1024 * 1024 * 1000\n\ndef get_path(path_str: str) -> str:\n\t\"\"\"\n\tGiven an absolute or relative path string, return absolute or relative path.\n\t\n\tArgs:\n\t    path_str: String in absolute or relative path (e.g.../../src,../src)\n\t\n\tReturns:\n\t    Full absolute path to file\n\t\n\t    Example::\n\t\n\t        >>> abs_path = \".\"\n\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tif 'K' in size:   # if size string has'kilogram', convert it to a bytes\n\t\tsize = size[:size.find('K')]\n\tif 'G' in size:   # if size string has'gaussian', convert it to a bytes\n\t\tsize = size[:size.find('G')]\n\tif 'M' in size:   # if size string has'mile', convert it to a bytes\n\t\tsize = size[:size.find('M')]\n\treturn int(size)\n\n@contextlib.contextmanager\ndef"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn type(dicts)(dict(i=i, **dicts_) for i in range(len(dicts)))\n\ndef _dictzip(*dicts):\n\t\"\"\"\n\t\tGiven a number of dictionaries with same keys, create a single dictionary object\n\t\twith all the values paired together.\n\t\t\"\"\"\n\tdicts_ = [_dict(d) if isinstance(d, Mapping) else d for d in dicts]\n\treturn type(dicts).fromkeys(keys=dicts_.pop(0)[0], values=dicts_)\n\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn {key: dictsum(val_iter) for key, val_iter in groupby(dicts)}\n\t\t\t\n################################################################################\n#\tSINR \n################################################################################\n\"\"\" \n\tCalculate the sinr (sub)normalized total resistance\n\t:param list of dicts:\n\t\ta list of dicts to calculate the subnormalized sinr values for\n\t:return dict:\n\t\treturn the dict with the result calculated\n\"\"\"\n\ndef _sinnormalize(dicts):\n\t\n\t\"\"\"\n\tCalculate the sinr"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\ttry:\n\t\t# return the first dict in the given list.\n\t\treturn next(map(func, (d for k, d in dicts)))\n\texcept StopIteration:\n\t\tpass\n\n\t# If the dictionary does not contain any key, then nothing to be done.\n\treturn dict({})\n\n# Function to generate the text for a template\ndef _textgen_1(num_lines=1, width=60):\n\tdef textstr(obj):\n\t\tobjstring = repr(obj)\n\t\tif len(objstring) <= width:\n\t\t\treturn obj"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn sum(dicts, lambda x: x)\n\t\n\t\ndef _dictupdate(dicts, update):\n\t\"update a dictionary with a specific amount of values\"\n\treturn _dictsum([dicts, update])\n\t\n\t\ndef _dictsort(dicts, key, reverse, keylist):\n\t\"\"\"\n\tSort dict entries into sorted keylist according to the given\n\tkey.\n\t\n\t>>> _dictsort([{'a': 1, 'b': 1, 'd': 'c'},\n\t                    {'c': 10, '"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tfor key in dicts:\n\t\tfor val in [dict.get(key, None) for dict in dicts]:\n\t\t\tyield {**val, **key}\n\tfor dicts in map(dictsum, dicts):\n\t\tyield {**dicts}\n\ndef _dictsum(dicts):\n\t\"\"\"\n\t\tHelper iterative routine for generating a result set\n\t\"\"\"\n\tdef sum(x):\n\t\treturn {\n\t\t\tk: v if v is not None else np.zeros_like(v)\n\t\t\tfor k, v in x.items"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn dict(itertools.chain(itertools.chain(*dicts), ((),)))\n\t\t\t\n\t\n\n\ndef _merge(*dicts, dict_class=dict):\n\t\"\"\"\n\t\tMerge multiple dictionaries to one new dictionary.\n\t\t\"\"\"\n\t\n\tobj = {}\n\t\n\tfor d in dicts:\n\t\t# Try adding data.\n\t\ttry:\n\t\t\tobj.update(d)\n\t\texcept Exception:\n\t\t\tobj.update(d.to_dict(dict_class))\n\t\t\t\n\treturn obj\n\ndef _zip(*"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresult = {}\n\tfor item in dicts:\n\t\tfor k, v in item.items():\n\t\t\tif type(v) == dict:\n\t\t\t\tv = _dictsum(v)\n\t\t\tresult[k] = v\n\treturn result\n\n\n# print (somelist(_list1))\nprint(sum(list(map(_dictsum, list(_list1)))))\nprint(_dictsum(list(_list1)))\n\n# print(somelist(_list1))\n# somelist([{'a': 1, 'b': 2}, {'a': 5"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn {k: sum([v for _,v in d.items()]) for k,d in dicts}\n\t\n\t\t\t\t\t\ndef _filter_zero_and_nans(lst):\n\t\"\"\" Return any element whose nan is True. \"\"\"\n\tfor elem in lst:\n\t\tif lst[elem] == _np.nan:\n\t\t\tlst[elem] = 0.0\n\t\t\tlst[elem] = None\n\treturn lst\n\ndef _find_duplicates(lst):\n\t\"\"\" Find the non-increasing"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn dsum(_dictitems(dicts), _dictsum)\n\t\ndef _dictadd(dicts):\n\t\"\"\"\n\t\tConcatenate a dict of iterable keys and values into an object.\n\t\"\"\"\n\td = {}\n\tfor d_item in _dictitems(dicts):\n\t\td.update(d_item)\n\tif len(d) > 0:\n\t\treturn d\n\treturn None\n\t\ndef _getdelta(x, fdict={}, lis=None):\n\t\"\"\"\n\t\tReturn a delta for each list of"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn {key : value if isinstance(value, dict) else dict(value) for key, value in dicts}\n\nclass UnsortedVector(object):\n\n\t\"\"\"A vector whose elements are not equal because of the lack of ordering features.\n\t\n\t>>> uvl = UnsortedVector([1, 2, 3], 'first')\n\t>>> uvl.first\n\t''\n\t>>> sum(vl.first for vl in uvl)\n\t3\n\t\"\"\"\n\n\t# pylint: disable=invalid-name\n\n\tdef __init__(self"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl = re.sub(pattern=url_pattern, repl=url_regex, string=url, flags=re.U)\n\treturn url\n\t\n\n# define default parameter values\nurl_args = {\n\t\n}\n\nurl_regex = r'(?<!\\\\):(?:[^:\"]+(?::[^:\"]+)?|(?:[^:\"]+))'\nparser = html.HTMLParser()\n\n# parse page from URL\n\nwith requests.Session() as session:\n    with session.get(url=url,"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\ttry:\n\t\t# url can contain any string value\n\t\tif type(url) == str:\n\t\t\t# remove any arguments after the url (e.g. if its a url and a query)\n\t\t\tif url.rfind(\"?\") > 0:\n\t\t\t\t# now we know there are any arguments remaining\n\t\t\t\turl = url[:url.find(\"?\")]\n\t\t\targs = dict()\n\t\t\tfor arg_name, arg_value in url_args.items():\n\t\t\t\t# check whether it's a variable\n\t\t\t\tif not arg_name.startswith(ARGUMENT"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\ttemp_url = url\n\targs = parse.parse_qs(parse.urlsplit(url).query, keep_blank_values=True)\n\tfor k, v in args.items():\n\t\tif k in url_args:\n\t\t\turl_args[k] = v[0]\n\t\telse:\n\t\t\tr = _replace_url_args('%' + k + '%', args)\n\t\t\turl_args[k] = parse.unquote(r)\n\treturn url_args\n\n\ndef get_args_from_url_file(url_file"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor key, val in url_args.items():\n\t\tif key in url:\n\t\t\turl[key] = val"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t# url = remove url params\n\tif 'q=' in url:\n\t\turl = re.compile('[^#]+').sub(r'\\1\\1', url)\n\t# url = remove quotes\n\tif '&' in url:\n\t\turl = re.compile('\\\\.{2}').sub(r'\\1', url)\n\t# get args for url (replace q=...) with values from url_args\n\t\tprint \"**** GET URL args... ****\"\n\t\tfor key, value in url_args.items():\n\t\t\t\t\t\t\t\t\t\t\t# make"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor key, key_val in iteritems(url_args):\n\t\tif isinstance(key, string_types):\n\t\t\tif not key:\n\t\t\t\traise ValueError('Invalid url argument provided: key={0!r}'.format(key))\n\t\t\ttry:\n\t\t\t\turl = re.sub(key, url_val, url)\n\t\t\texcept Exception as e:\n\t\t\t\traise ValueError('Invalid url argument provided: {0!r}=%r'.format(key)) from e\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif url is None:\n\t\treturn url_args\n\telse:\n\t\t# find out whether the url ends in an asterisk\n\t\tif '*' in url:\n\t\t\turl = re.sub('(.*?)\\*.*$', '%s/*.*' %url, url, 1)\n\t\t\ttry:\n\t\t\t\t# remove leading asterisks from url\n\t\t\t\turl = re.sub('^.*(\\*.*)', '', url)\n\t\t\texcept ValueError as e:\n\t\t\t\t# ValueError could be raised if url is not a single string\n\t\t\t\t# like http://www"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl = re.sub('(?i)(?:window_url|url|file_url)(?:\\\\S+?)', r'\"\\1\"', url.strip('\"'))\n\turl = re.sub('%3D(&\\')((?:.+?));|%5C&([a-z._-]+);', '\\\\1=\"\\\\2\"', url.strip('\"'))\n\turl = re.sub('(&#[0-9]+)(.*)\\(\\);',r'\\\\271\\\\2\\1', url).strip('\"')\n\tp = r"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif '$' in url:\n\t\turl = unquote(url).replace('$', '%23')\n\t# $' and '%23' are not special but are not supported in URL arguments\n\tif '%' in url:\n\t\turl = url.replace('%', '$')\n\n\treturn url\n\n\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor k in ['host', 'username', 'password', 'path', 'port','method']:\n\t\tif k in sys.argv:\n\t\t\turl = url.replace('<{}>'.format(k), str(url_args[k]))\n\n\turl = url.split('?')[0]\n\n\tif url.startswith('http'):\n\t\turl = url.replace('https', 'http')\n\telif url.startswith('/'):\n\t\turl = '/'.join(url.split('/')[:-1]) + '/"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None\n\n\ndef is_empty_string(val: any) -> bool:\n\t\"\"\"\n\t\"\"\"\n\treturn len(val) == 0\n\n\ndef is_number(val: any) -> bool:\n\t# check if the last numeric element of the list is a decimal number\n\t# e.g. '16' => 16.0\n\tif isinstance(val, (float, int)):\n\t\tlast_value = val.replace('.', '', 1)\n\telse:\n\t\treturn False\n\n\treturn bool(last_value.isn"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tret = False\n\ttry:\n\t\tval = str(val)\n\texcept:\n\t\tpass\n\ttry:\n\t\tval = val.decode('utf-8')\n\texcept:\n\t\tpass\n\ttry:\n\t\tval = val.encode('utf-8')\n\texcept:\n\t\tpass\n\ttry:\n\t\tval = val.hex()\n\texcept:\n\t\tpass\n\ttry:\n\t\tval = val.hex()\n\texcept:\n\t\tpass\n\ttry:\n\t\tval = val.encode(encoding=\"utf-8\")"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif isinstance(val, str) or len(val) > 0:\n\t\tif (val.count(None)!= 0 and not is_none_string(val)) or str(val).startswith(\"None\"):\n\t\t\treturn val\n\n\treturn None\n\n\n@dataclass(frozen=True)\nclass NestedIn:\n\t\"\"\"Wrapper class for nested value type in the data object.\n\t\"\"\"\n\ttype: type\n\t\"\"\"Type of the nested value as a nested object.\n\t\"\"\"\n\tvalue: any = None\n\t\"\"\"The value inside"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val is None:\n\t\treturn True\n\treturn not isinstance(val, (bool, int, float, str, bytes))\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\ttry:\n\t\tval.is_none\n\t\tif val.is_none:\n\t\t\treturn True\n\t\treturn False\n\texcept AttributeError:\n\t\treturn False\n\n\n# -----------------------------------------------------------------------------\n# Error Messages\n# -----------------------------------------------------------------------------\nclass NumericIOError(Exception):\n\tpass\n\nclass InvalidTextType(NumericIOError):\n\t\"\"\"Raised when a text is not an acceptable type.\n\t\"\"\"\n\tpass\n\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\ndef list_from_object(obj_: Any):\n\t\"\"\"\n\t\tReturn a list of"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None\n\n\n\n#\n# ----------------------------------------------------------------\n# ----------------------------------------------------------------\n#\n#\n#\n\ndef is_false(val: any) -> bool:\n\"\"\"\n\tReturn true if a predicate is set to False.\n\n\t:param val: the predicate\n\t:type val: any\n\t:return: True or False\n\t:rtype: bool\n\"\"\"\ndef is_none(val: any) -> bool:\n\"\"\"\n\tReturn True if the predicate is set to None.\n\n\t:param val: the predicate\n\t:type val: any\n\t:return: True"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn len(str(val)) == 0 or val in (None, '')\n\ndef is_instance_of(type_class: type, val: any) -> bool:\n\t'''\n\t\tCheck if the given value is of type_class.\n\t'''\n\treturn type_class is instance_of(val)\n\ndef is_instance_of_list(type_class: type, val: any) -> bool:\n\t'''\n\t\tCheck if the given value is of type type_class and has at least one element\n\t'''\n\tif type"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn bool(val is None)\n\n\n@classmethod\ndef is_string(val: any, skip_none=False) -> bool:\n\t\"\"\"\n\t\tTest whether any object is a string object.\n\t\"\"\"\n\treturn bool(is_string(val)) or (is_none_string(val, skip_none=skip_none))\n\n\n@classmethod\ndef is_number(val: any, numeric_type=float) -> bool or not is_number_valid(numeric_type):\n\t\"\"\"\n\tCheck if value is a numeric type and that value"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn (\n\t\tval is None or\n\t\ttype(val) is str and\n\t\tval == 'None'\n\t)\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t# Check for an empty string\n\tif set(map(chr, range(ord(\" \"), ord(\" \") + 1))) == set():\n\t\treturn True\n\n\t# Check for an empty string itself\n\telif map(chr, range(ord(\" \"), ord(\" \") + 1)) == map(chr, range(ord(\" \"))):\n\t\treturn True\n\n\telif len(val) == 0:\n\t\treturn True\n\n\t# We assume this is just a placeholder when the field is empty\n\telse:\n\t\treturn False\n\n\ndef map_value"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tall_flags = \", \".join(parser.flag_names)\n\tif len(parser.flag_values) > 0:\n\t\tflags ='' + parser.join(parser.flag_values)\n\telse:\n\t\tflags = parser.formatter_class.flag_join(all_flags)\n\n\treturn flags\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\thas_arg = True\n\tfor i, (flag, default) in enumerate(parser._group_actions_kwonly):\n\t\tif not flag.startswith('--'):\n\t\t\tdel parser._group_actions[i]\n\t\t\tcontinue\n\t\tflag = flag[2:]\n\t\tif i == 0:\n\t\t\thas_arg = default is not None\n\t\tflags.append(\"--\" + flag)\n\tflags.reverse()\n\tif has_arg:\n\t\tflags.insert(0, parser.add_argument(\"-a\", \"--"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tkwargs = OrderedDict()\n\tflags = [n.option_strings for n in parser._action_groups.values()]\n\tfor name, flags, _ in flags:\n\t\tflags = [x for x in flags if x]\n\t\tfor flag in flags:\n\t\t\ttry:\n\t\t\t\t# argparse will accept the unicode name if it is there, and not\n\t\t\t\t# the full unicode name with the unicode characters of the escape chars\n\t\t\t\t# if it is an actual argument that contains these special characters\n\t\t\t\t#\n\t\t\t\t# see _flag_values_factory"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = ''\n\tfor k, v in vars(f.flags).items():\n\t\tif v:\n\t\t\tflags += ('-' + k if v < 0 else '-' + k.lower().replace('_', '-'))\n\t\t\t\n\tflags = flags.replace('-l','')\n\treturn flags\n\t\t\ndef args_parser_flags(ap, default):\n\t\"\"\"\n\tGiven an ArgumentParser instance, create a dictionary of all known argument names defined as flags.\n\t\"\"\"\n\treturn {k:getattr(v, 'name', None) fork"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.flags\n\n\ndef arg_parser(\n\tdesc,\n\targ_prefix=None,\n\targ_conflict_handler='error',\n\targ_allow_nesting=False,\n\targ_help=\"\"\"\n\tUsage: [{arg_prefix}{arg}] [--args ARGS...]\n\t--args -t: -a  add an argument to parser.\n\n\toptional args:\n\n\t-c: --config\n\t-H: --help\n\t-h: --help\n\t-P: --args\n\t-u: --und"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# return the argument flags as a list containing the flag's value as an int\n\tdef listToString(s):\n\t\treturn''.join([str(e) for e in s])\n\n\t# for every argument, split into the string and check if it is a command or flag\n\tFLAG_FLAGS = {\n\t\t'--disable': False,\n\t\t'--disable-tests': False, '--disable-tests-mode': False,\n\t\t'--disable-tests-per-line': False, '--allow-run-from-dir': None, '--allow-run"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor sub_flag in parser._actions:\n\t\tflags.extend(sub_flag.option_strings)\n\treturn \" \".join(flags)\n\t# return parser._actions\n\n\ndef get_subfolders(aPath):\n\t\"\"\"\n\tGet the list of subfolders of argument aPath.\n\t\"\"\"\n\tif aPath[:4]!='/' or aPath[len(aPath)-1]=='/':\n\t\traise ValueError('Path not recognized: %s' % aPath)\n\n\tlistOfFolders"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# Note, parser.parse_flags below assumes parser is initialized.\n\topts = parser.option_class\n\tdefaults = dict(parser.defaults)\n\tkwargs = dict(parser.kwargs)\n\t# kwargs is updated in __init__() to keep these fields in sync while we have parsed its options\n\tfor k, v in kwargs.items():\n\t\tif opts.get(k) is not None:\n\t\t\tif v is None:\n\t\t\t\tparser.set_defaults(**dict(opts.get(k), **default"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \" \".join(parser.flag_values)\n\n\ndef parse_args_line(line):\n\t\"\"\"\n\tReturns an array of strings, the argument values and a dictionary of option flags.\n\tThe result is stored in that dictionary, without quotes.\n\t\"\"\"\n\ttype_line = line[2:]\n\ttype = TYPE_TABLE.get(type_line)\n\tif type is None:\n\t\traise ValueError('Invalid type: %s' % (type_line,))\n\tparser = argparse.ArgumentParser(description = type.__doc__)"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tsflags = parser.add_argument_group('Flags')\n\tflags_dict = sflags.add_mutually_exclusive_group()\n\tflags_dict.add_argument('-f', '--file', type=str, dest='filename', help='A file name to open. Will be stored in the directory in which the config file resides.')\n\tflags_dict.add_argument('-p', '--path', type=str, dest='path', metavar='/path/to/config.cfg', default='./config.cfg"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\treturn False\n\telse:\n\t\tprocessed.add(path_name)\n\t\tret = was_processed(processed, path_name, verbose)\n\t\treturn ret\n\n\n\n# --------------------------------------------------------------------------- #\n# Functions below.\n# --------------------------------------------------------------------------- #\nclass AppEndError(Exception):\n\t\"\"\"\n\t\tException used for exceptional cases.\n\t\"\"\"\n\tpass\n\n# =============================================================================\n# The logic here is to do all the necessary checks for the entry within the\n# application. We do this here to allow quick (but poorly-"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif verbose:\n\t\tprint(processed)\n\t\n\tif not path_name in processed:\n\t\tprocessed.add(path_name)\n\t\treturn True\n\n\treturn False\n\n\ndef unpack_zipfile(zipfile, destination, verbose):\n\t\"\"\"\n\t\tUnpack each member of the zipfiled content or folder.\n\t\tThis function accepts a zipfile and unarchive each member\n\t\tinto the destination directory.\n\n\t\tArgs:\n\t\t    zipfile (zipfile or file):\n\t\t    destination (str):\n\t\t    verbose (bool):"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\treturn False\n\t\n\tif path_name.startswith(\"\\\\\") or path_name.startswith(u\"/\"):\n\t\tfull_path = path_name\n\t\ts3path = \"s3://\"+processed_bucket_name+\":\"+processed_bucket_region+r\"/\"+full_path\n\t\tif s3path.startswith(\"s3://\"):\n\t\t\tfull_path = s3path\n\t\t\ts3path = \"s3://\"+processed_bucket_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif not(isinstance(processed, set)):\n\t\tprocessed = set([processed])\n\t\t\n\tif (\n\t        path_name in processed or\n\t        processed.isdisjoint(path_name)\n\t):\n\t\t#if we have a set, check the set for containment\n\t\t#if we have an inbuilt set check if we're in the set, i.e.\n\t\t#not an infinite loop for containment\n\t\t#if that's already true return false\n\t\t\n\t\tif (\n\t\t\tnot(processed.isdisjoint("}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = expand_path_to_abs(path_name)\n\tif not processed.__contains__(path_name):\n\t\tif verbose:\n\t\t\twarn('Not processed: %s' % path_name, file=sys.stderr)\n\t\treturn False\n\telse:\n\t\treturn True\n\t\ndef expand_path_to_abs(path_name, verbose):\n\t\"\"\"\n\tExpand the path name to a absolute path, given that it's not a directory.\n\t\n\tThe directory is only expanded when the path appears in"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tfor entry_name, entry_set in processed.items():\n\t\tif entry_name == path_name:\n\t\t\tentry = entry_set[0]\n\t\t\tprocessed[entry_name] = False\n\t\t\tmessage = \"Path {} was already processed. Reason: {}\"\n\t\t\tif verbose:\n\t\t\t\tprint(message.format(path_name, entry.get(\"reason_fullname\", \"\")))\n\t\t\treturn True\n\telse:\n\t\tentry = entry_set\n\tprocessed[path_name] = entry\n\treturn False\n\nclass Directory(object):\n    "}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tnew_set = set()\n\tif os.path.exists(path_name) and not verbose:\n\t\treturn True\n\telse:\n\t\tnew_set.add(path_name)\n\tif processed:\n\t\treturn any(processed.__contains__(n) for n in new_set)\n\treturn False\n\ndef get_unprocessed_dir(path):\n\t\"\"\"\n\tGet the path to the directory where unprocessed files are stored.\n\tThe directory is not removed when it is deleted as there might be some\n\tprocessing tasks"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tpath = os.fsdecode(path_name)\n\t\n\tif path in processed:\n\t\tif verbose:\n#\t\t\tprint \"SKIPPED\"\n\t\t\treturn False\n\t\telse:\n#\t\t\tprint \"Skipping\"\n\t\t\treturn False\n\telse:\n\t\tprocessed.add(path_name)\n\t\t\n\t\treturn True\n\t\n\t\ndef mk_dir(directory_path, verbose):\n\t\"\"\"\n\tCreate a directory named in directory_path.\n\t\n\tArgs:\n\t    directory_path: Folder path where the directory will"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif processed.isdisjoint(list(processed) + [path_name]):\n\t\treturn False\n\n\treturn True\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t# This is a performance bottleneck that allows the caller to\n\t# reduce load if very little to check if it's already processed.\n\tcurrent = len(processed)\n\t\n\twhile current > 0:\n\t\tcandidate = processed.pop()\n\t\t\n\t\t# Find if this is the actual source path\n\t\tif os.path.join(path_name, candidate) == path_name:\n\t\t\treturn True\n\t\n\t\t# If this is a dir, expand it\n\t\telif os.path.isdir(candidate):\n\t\t\tcurrent += len(processed)\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tverts = []\n\tfor v in vertices:\n\t\tpt = v[0] + v[1],v[0] - v[1], v[0] - v[1]\n\t\tverts.append(pt)\n\treturn verts\n\t\ndef isPointInPolygon(pt1,pt2,vertices):\n\t#check that the point pt1 and pt2 are 4 vertices, not the 3 point tuple\n\t#also check that pt2 is inside the polygon.   \n\tpt1 = vertex3tuple(pt1)\n\tpt2"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\tpt = 0\n\tfor vertex in vertices:\n\t\tpt += (vertex[0] << 8) + vertex[1]\n\tpolygon_tup = ((vertices[0][0], vertices[0][1], vertices[1][0]), \n\t\t(pt & 0xff0000, (pt >> 8) & 0xff0000, (pt >> 16) & 0xff0000),\n\t\t(pt >> 24 & 0xff, vertex2tuple(vertices[1:])))\n\treturn polygon_tup\n\n\ndef vertex2tuple("}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tvert_3tuple = vertices.tolist()\n\treturn [[vert_3tuple[i],vert_3tuple[i+1],vert_3tuple[i+2]] for i in range(0, len(vert_3tuple), 3)]\n\ndef vertex4tuple(vertex):\n\"\"\"\n\treturn the vertex's 3 tuples as a list::\n\n\"\"\"\n\tver_tuple = {}\n\tver_tuple[0]=vertex[0]\n\tver_tuple[1]=vertex[1]\n"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpoints = [0] * (len(vertices[0]) - 2)\n\treturn tuple(points) \n\ndef vertex2tuple(v):\n    \"\"\"\n\treturn 2 vertex tuples by v\n\t\"\"\"\n    lbl = len(v)\n    points = [0] * lbl\n    return tuple(points)\n\ndef triangle_from_vertex(vertices, a, b, c):\n    \"\"\"\n\treturn line of 3 triangle\n\t\"\"\"\n    # vertices point of 3 vertices\n    triangle = []\n    v1"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tvertices = [x.split(\",\") for x in vertices]\n\t# iterate through each vertex's 2\n\tfor j in range(0, len(vertices)):\n\t\ti = (j - 0.5 )\n\t\t# get coordinates of the vertex and flip orientation\n\t\tx = vertices[j][0]\n\t\ty = vertices[j][1]\n\t\tz = vertices[j][2]\n\t\t# get rid of negative values\n\t\tz = z.replace(\",\",\"\")\n\t\tx = x.replace(\",\",\"\")\n\t\t"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpoint_list = []  # 3 points needed for triangles\n\tx0 = vertices[0]\n\ty0 = vertices[1]\n\tzx = vertices[2]\n\tzy = vertices[3]\n\tzw = vertices[4]\n\txm1 = max(x0[0], y0[0], zw[0])\n\tym1 = max(max(y0[0], zw[0]), max(x0[1], zw[1]), xm1)\n\tzm1 = max(max("}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\tvertex2 = vertices[0]\n\tvertex3 = vertices[1]\n\tvertex4 = vertices[2]\n\tlist_verts = set([x[0] for x in vertices]) # return the set of the elements\n\treturn vertex2, vertex3, vertex4, list_verts\n\t\ndef vertex4tuple(vertices):\n\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpoints = []\n\tlength = len(vertices)\n\tn = length // 3\n\tfor i in range(0,n):\n\t\tpoints.append(vertices[3*i:3*(i+1)])\n\tfor i in range(0,n):\n\t\tfor j in range(0,n):\n\t\t\tpoints.append(vertices[3*i:3*(i+1)])\n\t\t\tpoints.append(vertices[3*j:3*(j+1)])\n\treturn points\n\ndef test():\n    vertex"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tassert vertices!= ()\n\t\n\tpoints = ()\n\tfor vertex in vertices:\n\t\tpoints = points + tuple(vertex) + tuple([vertex[1], vertex[2]])\n\t\n\treturn points\n\ndef face_edge_order(faces):\n\t\"\"\"\n\tsort all edge lengths\n\t\"\"\"\n\tassert faces!= ()\n\t\n\tedge_vectors = ()\n\tedge_tuples = ()\n\tedge_tuples = list( map( lambda ver : (ver[2], ver[0], ver[1]), faces ) ) if"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tV = 0\n\tfor v in range(len(vertices)):\n\t\tV = V + 1\n\t\tyield vertices[v][V], vertices[(v+2) % len(vertices)][V]\n\t\n\t#return 1st, last vertex + 1st point\n\t#print vertex3tuple([vertices[0], vertices[-2], vertices[-1], ]) # test\n\ndef edge(a, b):\n\treturn a[0] + b[0], a[1] + b[1]\n\ndef"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif not alphabet:\n\t\traise ValueError('No alphabet to convert to string, need at least one alphabet')\n\tdigits = [ch for ch in str(number)]\n\tif digits[-1] == \"0\":\n\t\treturn \"+\".join(alphabet[0:len(digits)-1])\n\t\n\tif padding is None:\n\t\tpadding = len(digits) - len(alphabet)\n\telif padding < len(digits):\n\t\tdigits = digits[:padding]\n\t\n\treturn \"\".join(alphabet[len(dig"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tresult = []\n\tfor letter in alphabet:\n\t\tif(number >= 0 and number<=26):\n\t\t\tresult.append(letter)\n\t\t\tnumber += number - number % len(letter)\n\t\telse:\n\t\t\tbreak\n\tif not padding:\n\t\tpadding = len(result) - len(alphabet)\n\t\n\treturn ''.join(result[:padding]) + alphabet[26 - (number - len(result) % len(alphabet))][:padding]\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tnum = (number / pow(10, len(alphabet))) \\\n\t    if number \\\n\t    else \"0\" if number + 1 \\\n\t    else \"1\"\n\tnum = str(num) if not (len(alphabet) == 1 and (len(num) == 1 \\\n\t\tor not alphabet[len(num) - 1])) \\\n\t\telse num\n\tnum = num[::-1]\n\tif number < 10:\n\t\tif padding is None:\n\t\t\tpadding = len(str(number))\n\t\treturn num.zfill("}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif not padding:\n\t\tif len(alphabet) == 1:\n\t\t\tpadding = 1\n\t\telif len(alphabet) == 0:\n\t\t\traise ValueError(\"alphabet must not be empty\")\n\t\telse:\n\t\t\tpadding = alphabet.index(\"!\")\n\t\n\tint_n = number\n\tif not int_n:\n\t\treturn \"\"\n\t\n\tint_n = int_n % len(alphabet)\n\t# the sign is the index of the character, as it is 0 if it's empty\n\tremainder = n % len(alph"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\t# If there's nothing to return, return nothing\n\tif len(alphabet) == 0:\n\t\treturn \"0\"\n\t\n\t# If padding, add \"0\" to the end of string\n\tif padding!= None:\n\t\tassert padding < 2 ** len(alphabet)\n\t\tstring = \"\".join(alphabet + [\"0\" for _ in range(padding)])\n\telse:\n\t\tstring = \"\".join(alphabet)\n\t\n\t# Initialize an empty string\n\tresult: str = \"\"\n\t\n\t# Calculate the string"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif padding == None:\n\t\tpadding = int_to_string_len(len(alphabet))\n\tr = str(number)\n\treturn ''.join(alphabet[int(x) - padding] for x in r)\n\t\ndef count_letters(text: str, alphabet: List[str], padding: Optional[int] = None) -> int:\n\tif padding == None:\n\t\tpadding = int_to_string_len(len(alphabet))\n\tletters = [c for c in text if c in alphabet]\n\ttotal = len(text"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tn = str(number)\n\tn = n.zfill(padding)\n\treturn ''.join([alphabet[int(c)] for c in n])\t\n\tpass(...)\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\talphabet = list(alphabet)\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tif number <= padding:\n\t\t# Exclude the smallest 0,\n\t\t# since it shouldn't be any smaller than 1.\n\t\talphabet = (alphabet[1] + alphabet[2] + alphabet[10]) * 2 + alphabet[9]\n\n\tfor digit in str(number):\n\t\tif len(digit) >= padding:\n\t\t\talphabet = alphabet[:padding]\n\n\treturn ''.join(alphabet)\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t# If both the padding and the alphabet length are less than one,\n\t# just truncate the string to just the alphabet length\n\tif padding and len(alphabet) < 1:\n        \treturn ''.join(sorted(map(str, map(\n            lambda x: alphabet[:length[0]],\n             zip(chars(number),)\n               ))))\n\n\t# Check if the padded amount (padding) is in the string\n\t# and it begins with 0 and isn't an integer.\n\tif padding > len(alphabet)+4 or not (isinstance"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\toutput = \"\"\n\tif padding is None:\n\t\tpadding = \"0\"\n\tprev = alphabet[number // 10]\n\twhile number % 10!= 0:\n\t\toutput += prev\n\t\tnumber //= 10\n\t\tprev = alphabet[number // 10]\n\t# Add all letters except leading and trailing\n\tfor i in range(len(alphabet)):\n\t\tif i == alphabet.index(prev):\n\t\t\toutput += alphabet[i]\n\t\telse:\n\t\t\toutput += padding\n\t\t\toutput += alphabet[i]\n\treturn output\n\n\ndef create_key(keys"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\tregister_name = flow_params['registers'][register_number-1]\n\tflow_params['flows'][flow_params['flows'].index(flow_params[register_name])] = register_value\n\ndef _replace_flows(flow_params, flows_keys, flows_values):\n\t\"\"\"\n\t\tReplace flows with provided keys and values\n\t\n\t\tFlow dict has to have keys with value provided in 'flows_keys' for flow\n\t\tto be replaced\n\t\"\"\"\n\t\n\tfor i in list(range(len(flows_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflows_to_replace = flow_params\n\tflows_to_replace[register_number] = register_value\n\n\n#============================================ #\n#  Functionality : replace keys value   #\n#============================================ #\ndef _replace_keys(x,y,register,flow_params):\n\t\"\"\"\n\tFunction to replace the values from one flow in one block for given register\n\t\n\t:param x: Flow key to be replaced\n\t:param y: Key to be replaced with another value\n\t:param register: Register where the key is given\n\t:param flow_params"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor k, v in flow_params.items():\n\t\tif k =='register_number':\n\t\t\tflow_params[k] = register_number\n\t\telif k =='register_value':\n\t\t\tflow_params[k] = [int(flow_params[k][i]) for i in range(0, len(flow_params[k]))]\n\n# ------------------------------\n# Function to remove flow from flow_details\n# ------------------------------\ndef _remove_flow(flow):\n\tremove_list = []\n\tfor x in flow_details:\n\t\tif"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t# Iterate through all FlowParams and replace the specified value\n\tfor flow, register in flow_params.items():\n\t\tif isinstance(register, list):\n\t\t\tfor reg in register:\n\t\t\t\tif reg == register_number:\n\t\t\t\t\tregister_value = reg\n\t\telif isinstance(register, int):\n\t\t\tif register == register_number:\n\t\t\t\tregister_value = register\n\n\t# Set value to the specified register\n\tflow_params[flow_params['flow_name']][register_number] = register_value\n\n\ndef create_list(key):\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow in flow_params['flows']:\n\t\t# Replace value with register number\n\t\tif flow['name'] == 'input' and register_number == flow['input'][0]:\n\t\t\tflow['input'][1] = register_value\n\t\tif flow['name'] == 'input' and register_number == flow['input'][-1]:\n\t\t\tflow['input'][2] = register_value\n\t\t# Replace value with register number given to the corresponding flow\n\t\tif flow['name'] == 'output' and register_number == flow['output'][0]:\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params.update({\n\t\t'number': register_number,\n\t\t'register_number': register_number,\n\t\t'register_value': register_value\n\t})\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tglobal _flow_params\n\t_flow_params = [flow_params['registerDict'][register_number]]\n\t\n\tfor node, value in sorted(_flow_params):\n\t\t_input_node, _output_node = node\n\t\t_out_var, value_var, _out_flow = _output_node\n\t\t\n\t\tflow_name = '.'.join([_flow_params[0][0],_out_node])\n\t\t_out_flow.evaluate_value(value_var, flow_name)\n\t\t\n# Register flow to device"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif register_number in flow_params:\n\t\tflow_params[register_number] = register_value\n\t\n\ndef _get_in_flows(flow_params, register_dict, flow_number):\n\t\"\"\"\n\tGet the flow's in_flow\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_dict: Dictionary to store flows whose in_flow can be retrieved\n\t:param flow_number: Flow number\n\t:return: Dictionary with flow params and flow number to get flow's in_flow\n\t\"\"\"\n\tif flow"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\tfor i in flow_params.keys():\n\t\t\n\t\t# if flow has already an index in its dict, we don't try to replace the actual value\n\t\tif i in flow_params[i].index.get_unused_names(): \n\t\t\tcontinue\n\t\t\t\n\t\t# if not\n\t\tif type(flow_params[i].value) in ['string', 'float']:\n\t\t\tcontinue # skip\n\t\t\t\n\t\tif type(flow_params[i].value) in [float, 'double']:\n\t\t\tif not (flow_params[i].value <="}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\tregister_value_to_replace = flow_params[register_number]['value']\n\tflow_parameters = flow_params[register_number]['details']\n\tif 'default' not in flow_parameters and register_number < len(flow_params):\n\t\tflow_parameters['default'] = flow_params[register_number]['value']\n\t\t\n\t# TODO: Use 'FlowSpecification' class variable to implement replacement process\n\t# in this function\n\t_flow_spec = flow_spec_factory.get(flow_params"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn u(\".\" + value).replace(u(\".\"), arg)\n\ndef is_valid_url(url, allow_local=False):\n\t\"\"\"\tChecks whether passed string is a valid url\n\t\"\"\"\n\tif not url:\n\t\treturn False\n\tparsed = urlparse.urlparse(url)\n\treturn bool(parsed.netloc) and bool(parsed.scheme) and bool(parsed.path)\n\nclass CVSFileInfo(object):\n\tpass\n\nCVSfileInfo = CVSFileInfo()"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif value == '.':\n\t\treturn arg\n\telif isinstance(value, (str, unicode)):\n\t\tif value[-1] == '.':\n\t\t\tif value.startswith('!'):\n\t\t\t\treturn '!' + arg\n\t\t\telse:\n\t\t\t\t# if its a string and has a '.'- then its probably a boolean value\n\t\t\t\tif value.startswith(('true', 'false')):\n\t\t\t\t\treturn value.replace('.', arg, 1)\n\t\t\t\telif value.endswith('.'):\n\t\t\t\t\t# if its a single character"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif '/' in value:\n\t\tindex = value.index('/')\n\t\tend = value.index(' ', index + 1)\n\t\treturn value[:index] + arg + value[end:]\n\n\treturn value\n\n\t# return arg.replace('.' + arg, arg)\n\ndef find_indices(string, marker):\n\tres = []\n\tix = string.index(marker, 0)\n\tstart = len(string)\n\twhile ix < len(string) and string[ix]!= marker:\n\t\tix += 1\n\tif ix"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif value.find('.')!= -1:\n\t\targ = value.replace('.', arg)\n\treturn(arg)\n\ndef removeDots(value):\n\tif value[-1]!= '.':\n\t\treturn(value + '.')\n\telse:\n\t\treturn(value[:-1])\n\t\ndef isValidFile(file):\n\tif os.path.exists(file) == False:\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef isValidDirectory(dir):\n\tif os.path.exists("}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn re.sub('\\.+', arg, value)\n\ndef replace_dots(value, args):\n\t\"\"\"\n\t\tReplaces all values of '.' with arg for a given string for a given arg list\n\t\"\"\"\n\tfor arg in args.split(','):\n\t\tvalue = value.replace(\".\", arg)\n\treturn value\n\ndef replace_whitespace(value, args):\n\t\"\"\"\n\t\tReplaces every value with arg from the given list of strings separated by a whitespace\n\t\"\"\"\n\treturn re.sub('\\s', arg, value)"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tdots = string.find(value, '.')\n\tif dots >= 0:\n\t\tnvalue = value[dots:].lstrip('.')\n\t\targ = arg.replace(nvalue, arg)\n\treturn arg\n\n\n# define a regex pattern for validating an input\n# note that this is a *strong* regexp, not a *weak* one\n# so we can run some debugging stuff in here\n_regex_invalid = re.compile(r'[^-.\\W\\.,~]+')\n\n\n#=================================================================================\n# find"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif not value:\n\t\treturn value\n\telse:\n\t\tmatch_str = r'(\\.(?=[.* ]|$))|(\\.\\.|$)'\n\t\tmatch = re.findall(match_str, value, re.M)\n\t\tfor i, m in enumerate(match):\n\t\t    value = value.replace(m, arg)\n\t\t    if i+1 < len(match):\n\t\t\t\treturn match_str + '%.2f' % float(value)\n\treturn value\n\n# TODO: remove this function\ndef create_config_"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn value.replace(\".\", arg)\n\n@app.route('/convert', methods=['POST'])\ndef convert():\n\timport json\n\tfrom datetime import datetime\n\tfrom os.path import join\n\tfrom jinja2 import Environment, FileSystemLoader\n\tfrom werkzeug.utils import redirect\n\tfrom config import config\n\n\tenv = Environment(loader = FileSystemLoader('web-templates/') )\n\tjinja_env = env.from_string(config['JINJA'])\n\t# load the Jinja environment template variables"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn not value.startswith('.') and re.sub(r'(.|\\d)!', arg*' ', value)\n\n\t# if (arg.find('.') > -1) or (value.find('.') > -1):\n\t# \treturn arg\n\n\t# raise NotImplementedError(\"\"\"'replace_dots' not implemented for '%s' \"\"\"\n\t# \t                                                         % (str(value),))\n\n\t# return arg\n\n    def replace_dots2(value, text):"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tdotIndex = value.find(\".\")\n\tif dotIndex!= -1:\n\t\treturn value[:dotIndex+1].replace(\".\", arg)\n\treturn value\n\n\ndef remove_dots(value):\n\t\"\"\"\n\tRemoves all dots from the string\n\t\"\"\"\n\twhile \".\" in value:\n\t\tvalue = value.replace(\".\", \"\")\n\treturn value\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn [subclass for subclass in classes(cls)\n\t\t\tif isinstance(subclasses(subclass), (list, set)\n\t\t\t\t\t) and (all(subclass.__dict__.get(obj, False)\n\t\t\t\t\t    for obj in subclass.__dict__.get(cls, [])\n\t\t\t\t\t\t\t\t\t   if not (obj == cls or obj == '__new__' or obj\n\t\t\t\t\t\t\t\t\t\t == type.__setattr__\n\t\t\t\t\t\t    (class, obj)))]\n\t\t]\n\t\n\t\n\t\n\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tif not hasattr(cls, \"_subclasses\"):\n\t\tcls._subclasses = []\n\t\tfor klass in class_hierarchy(cls):\n\t\t\tklass._subclasses += subclasses(klass)\n\treturn cls._subclasses\n\t\n\n_CmpType = types.new_list_type(((types.NoneType, types.BooleanType),\n                                 types.IntType, types.FloatType,\n                                 types.StringType, types.TupleType))\n\ncmp = types.new_instance_method(_C"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn [cls] + list(cls.__subclasses__())\n\n\ndef get_root_path(path):\n\treturn osp.dirname(osp.dirname(osp.dirname(osp.dirname(abspath(__file__).replace('.py', ''))))))\n\n\nclass Timer(object):\n\n\tdef __init__(self, mode='max'):\n\t\tself.st = time.time()\n\t\tself.timer = None\n\n\n\tdef __getattr__(self, name):\n\t\tif not self.timer:\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclases = []\n\tfor subcls in cls.__subclasses__():\n\t\tsubclases.extend(subclasses(subcls))\n\treturn subclases\n\ndef _find_subclasses_recusived(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\n\t\"\"\"\n\t# XXX: not safe to call from within C of the object being checked, but\n\t# subclasses of a parent class are not OK for recursion\n\tif cls not in mro:\n\t\ttry:\n\t\t\tanc"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor subclass in cls.__subclasses__():\n\t\tyield subclass\n\t\tyield from subclasses(subclass)\n\ndef has_attrs(klass):\n\t# See also has_selfref\n\tattrs = getattr(klass, \"__mro__\", [])\n\treturn len(attrs) > 0\n\ndef is_builtin(*type_args):\n\tfor type in type_args:\n\t\tif isinstance(type, Callable):\n\t\t\treturn True\n\n\treturn False\n\ndef is_list(object):\n\t\"\"\""}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = []\n\n\t# Add the base to all subclasses\n\tfor c in cls.__mro__:\n\t\tif not c.__name__.startswith('_'):\n\t\t\ttry:\n\t\t\t\tsubs.append(subclasses[c])\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\treturn subs\n\ndef get_fields(obj):\n\t\"\"\"\n\tGet the class fields, as a list of Field objects.\n\t\"\"\"\n\tfields = []\n\n\tfor c in type(obj).__mro__:\n\t\tfields += c.__"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tif isinstance(cls, type):\n\t\tfor c in cls.mro():\n\t\t\tyield c\n\t\t\tif isinstance(c, type):\n\t\t\t\tfor i in subclasses(c):\n\t\t\t\t\tyield i\n\telse:\n\t\tyield cls\n\n\ndef make_class(name, bases, adict, cb, pkg=True):\n\t\"\"\"\n\tCreates a new sub-class of a class\n\t\"\"\"\n\tif pkg:\n\t\tcurrent_package = __import__(name.rsplit('.', 1)[0])"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn tuple(cls.__subclasses__())\n\n# \n# This only works on Python 2.\n# \ndef add_attr(obj, **attrs):\n\tfor k, v in attrs.items():\n\t\tsetattr(obj, k, v)\n\treturn obj\n\n\ndef is_python_file(path):\n\t'''\n\t\tCheck if a path is a python file.\n\t\tIf the file is a module, not a python file.\n\t\t'''\n\t\n\t# check if a file is a python file\n\tif os."}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tif '_subclasses' in cls.__dict__:\n\t\treturn cls._subclasses(\"_subclasses\")\n\treturn list()\n\nclass subclasses(list):\n\t\"\"\"\n\tGet all subclasses of a class, recursively\n\t\"\"\"\n\tdef __init__(self, supercls):\n\t\tlist.__init__(self, subclasses(supercls))\n\tdef __call__(cls):\n\t\treturn super(cls, cls)._subclasses(lambda: self)\n\ndef hasTraversal(obj,"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tif not hasattr(cls.__dict__, \"subclasses\"):\n\t\ttry:\n\t\t\tcls.subclasses = list(cls.__bases__)\n\t\texcept AttributeError:\n\t\t\tpass\n\treturn [\n\t\tsub\n\t\tfor sub in cls.subclasses\n\t\tif (not child in sub.__dict__) and (child.__dict__ is not None)\n\t]\n\ndef attrs_to_int(attrs):\n\t\"\"\"\n    Convert all given attributes (list of tuple (attr,value))\n    to"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t# initialize result\n\tresult: int = 0\n\t\n\t# convert to character sequence\n\tsequence = [int(c) for c in alphabet.index(letter) if c!= '0' and c.isdigit()] # get character values\n\t\n\t# sum the given number in the string to get the result\n\tfor letter in string.upper():\n\t\tif letter in sequence:\n\t\t\tresult += (10**(len(sequence)-sequence.index(letter)-2)+1) # sum the resulting char positions\n\t\t\t\n\treturn result\n\n\n# test\nprint("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif not string:\n\t\treturn 0\n\t\n\tdigits = list(string)\n\tdigits.reverse()\n\t\n\t# Create an integer index list (a list of ints). Sort it. \n\t# For each index i, use its value + the alphabet.\n\t# Find the next index 0 + value - the alphabet and use it. \n\t# Concatenate those two integers together. \n\t\n\t# The answer is a string of two integers that represents \n\t# how many digits left after the first non-zero digit.\n\tn_"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(\"\".join(alphabet[i] for i in sorted(string)))\n\ndef read_ints(ints):\n\t\"\"\"\n\t\tLoad in a list of integers from disk.\n\t\"\"\"\n\treturn [string_to_int(ints_line[:-1], alphabet) for ints_line in integers_lines]\n\ndef read_int_str(ints):\n\tints_str = \"\"\n\tfor i in range(len(ints)):\n\t\tints_str = ints_str + str(ints[i])\n\treturn ints_str\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(string) % 26 + alphabet.index(string[:1]) * 97 + alphabet.index(string[1:]) * 26\n\n\ndef calculate_sum(n):\n    \"\"\"\n        Return the sum of all the digits of n in the form (10^k)*10^k\n    \"\"\"\n    tot = 0\n\n    for _ in range(0, n, 1):\n        tot += 10 ** _\n\n    return tot\n\n\ndef print_instructions() -> None:\n    \"\"\"\n        Print all of the strings used\n    \"\"\"\n    print"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(\"\".join([c[-1] if c.isdigit() else alphabet.index(c.lower()) + ord(\"a\") for c in string]), base=2)\n\nclass AlphabetSet(object):\n\t\n\t\"\"\"\n\t\tAn abstract base class containing the following interface for each value:\n\t\t- __repr__(self)\n\t\t- is_in_alphabet(self, letter: str)\n\t\"\"\"\n\t\n\tdef __init__(self, letters: List[str]) -> None:\n\t\tself.alphabet = \"\".join("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\timport string\n\ts = []\n\tfor char in alphabet:\n\t\ts.append(string[:len(char)])\n\t\tstring = string[len(char):]\n\ts = s[::-1]\n\treturn int(\"\".join(s), 2)"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t# https://stackoverflow.com/questions/26382781/python-replace-a-single-character-to-another-character\n\talphabet = alphabet + alphabet[::-1] # alphabet + alphabet[::-1] is a list in the same order as the code in cnt_digit() and int_digits()\n\treturn int(str(string), 36) # we turn string onto an decimal number, so the result is an int\n\ndef int_digits(digit_sum: int, digits: List[int]) -> None:\n\t\"\"\""}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\treturn 0\n\n\ndef int_to_string(al: int) -> int:\n\t\"\"\"\n\tConvert the given number from the algorithm to a string\n\te.g. 0 -> \"zero\"\n\t\"\"\"\n\t\n\treturn \"zero\"\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\timport hashlib\n\tint_out = \"\"\n\tfor char in list(string):\n\t\tif char == \"-\":\n\t\t\tint_out += \"0\"\n\t\telse:\n\t\t\tint_out += alphabet[hashlib.sha256(char.encode()).digest() % (len(alphabet)-1].to_bytes(1, \"big\"))\n\treturn int_out\n\ndef main():\n\t# TODO: Make this work!\n\tf = open(\"input.txt\", \"r\")\n\ttest = f.read()\n\tprint(len("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tint = -1\n\tint = string.replace(\"-\", \"\").translate(\n\t\tstr.maketrans(\n\t\t\t\"\",\"\",\n\t\t\talphabet\n\t\t)\n\t).replace(\n\t\t\" \",\"\"\n\t)\n\tint = int.lstrip(\"0\")\n\treturn int\n\ndef get_int(s: str) -> int:\n\tres = 0\n\tfor item in s[::-1]:\n\t\tres = (res * 10) + int(item)\n\treturn res\n\ndef main():\n\tprint(\"Welcome to"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\twith requests.get(url, stream=True) as f:\n\t\twith open(destination_path, 'wb') as df:\n\t\t\tshutil.copyfileobj(f, df)\n\n\ndef extract_archive_to(url, destination_path):\n\t\"\"\"extract archive to a path\"\"\"\n\twith open(destination_path, 'wb') as df:\n\t\tshutil.copyfileobj(requests.get(url, stream=True).raw, df)\n"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t# archive_name: Path where archive has been downloaded\n\tarchive_path = destination_path + \"/desc/\" + archive_name\n\tresp_s = requests.get(url)\n\tout_dir_name = archive_name + \"out.tar.gz\"\n\twith open(out_dir_name, \"wb\") as f:\n\t\t# write only if a file was not downloaded\n\t\tf.write(resp_s.content)\n\t# remove.tar.gz archive\n\tremove(out_dir_name)\n\treturn archive_path\n\t\n\n"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t# extract archive and return directory which will eventually contain files\n\twith tarfile.open(url,'r:gz') as tar:\n\t\ttar.extractall(path = tar.getmembers()[2], level = 4, members = None)\n\t\tfor member in tar.getmembers():\n\t\t\tif '.pyz' in str(member).lower():\n\t\t\t\treturn Path(destination_path,Path(str(member.name)[:-7]).name)\n"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\nimport os\nimport tarfile\nimport time\nstart_time = time.time()\nfilename = ''\n\ndef download(url):\n    r = requests.get(url, allow_redirects=True)\n    filename = os.path.split(r.headers['Content-Disposition'])[-1].split('.')[0]\n    with open(filename, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:   \n                f.write(chunk)\n\ndef extract"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\timport tarfile\n\timport datetime\n\timport os\n\t\n\t# Read archive from disk\n\tarchive_path = '/mnt/e/' + datetime.datetime.strftime(datetime.datetime.today() - datetime.timedelta(days=1),'%Y%m%d') + '.tar.gz'\n\tarchive = open(archive_path, 'wb').write(''.join(requests.get('https://www.dropbox.com/s/gqhqjr0i6a9yq1"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\tarchive_path = destination_path + \"/archives/archives\"\n\tprint(archive_path)\n\t\n\tr = requests.get(url, allow_redirects=True, verify=True)\n\topen(archive_path, \"wb\").write(r.content)\n\n\treturn archive_path\n"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\turl=url+'tar.gz'\n\tfilename, extension = os.path.splitext(url)\n\tfilename = filename+'.tar'\n\tos.path.join(destination_path, filename)\n\ttry:\n\t\t# Download archive from server\n\t\theaders={'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36', 'Accept':'*/*"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\tarchive_url = url + \"archive/desc/desc-0.0.tar.gz\"\n\tarchive_name = archive_url.split(\"/\")[-1]\n\t\n\t# download archive\n\tif not url:\n\t\traise UserWarning(\"No url given, will get archive from internet\")\n\telif not destination_path:\n\t\traise UserWarning(\"Destination_path is not given, will get archive from internet\")\n\telse:\n\t\tdestination_path = Path(destination_path)\n\t\tresponse = download_url(archive_url"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\ttar_archive = get_tar_archive_path(url, destination_path)\n\ttar_archive = os.path.abspath(tar_archive)#extractor the destination archivray to a path\n\ttar_archive = os.path.basename(tar_archive)\n\tdestination_path = os.path.join(destination_path, tar_archive)\n\n\tif os.path.exists(destination_path):\n\t\tos.rmdir(destination_path) #delete folder if exists\n\tos.rename(tar_"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\turl = 'https://raw.githubusercontent.com/OpenArch/compas/master/compas_tar.tar.gz'\n\tdestination_path = 'compas/archives'\n\tsolved_destination_path = \"solutions:/compas-compas-archives/\"\n\tos.makedirs(destination_path, exist_ok=True)\n\ttar = tarfile.open(url, 'r:gz')\n\tarchives = tar.getnames()\n\tsolved_archives = ['']\n\tfor archive in archives:\n\t\tarchive"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\t\n\t\nclass OSInfo():\n\t\n\t\"\"\"\n\t\tStores information about the machine\n\t\"\"\"\n\t\n\t\n\t@staticmethod\n\tdef getCpuType():\n\t\t\n\t\t\"\"\"\n\t\t\t- Returns a string representing the CPU type and OS\n\t\t\"\"\"\n\t\tosInfo = OSInfo\n\t\t\n\t\tresultString = \"\"\n\t\tif(osInfo._osFamily == \"Darwin\"):\n\t\t\tresultString = \"i386\"\n\t\telif(osInfo._osFamily == \"Linux\"):\n\t\t\tresultString = \"x86_64\"\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\troot_path = os.path.join(\n\t\tos.path.dirname(__file__), os.path.pardir)\n\n\tif \"posix\" in os.uname().sysname:\n\t\tif root_path not in sys.path:\n\t\t\tsys.path.append(root_path)\n\telse:\n\t\tprint(\"Not OS\")\n\t\texit()\n\nfrom utils import os_utils\nimport requests\nfrom datetime import date\nfrom datetime import timedelta\nimport json\nimport re\nimport yaml\n\n\ndef check_os_s"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif os.uname()[0] == \"Darwin\":\n\t\treturn os.uname()[4] == \"AppKit\"\n\ndef createDirs(path):\n\t\"\"\"\n\tCreates a directory inside a path\n\t:param path: Path to the directory \n\t:return: The path where the directory was created\n\t\"\"\"\n\tif os.path.exists(path) == False:\n\t\tos.mkdir(path)\n\treturn path\n\ndef find_file(path, ext, file_list):\n\tif ext==\"png\""}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tMACOS=False\n\ttry:\n\t\timport Tkinter as tk\n\t\timport tkFileDialog\n\t\timport tkMessageBox\n\t\timport os\n\t\tMACOS=True\n\t\treturn True\n\texcept:\n\t\tMACOS=False\n\t\treturn False\n\n\nprint \"--------------------------------------------------------\"\nprint \"--------------------------------------------------------\"\n\nif os.name==\"nt\":\n\tprint \"Detecting windows systems\"\n\tprint \"-----------------------\"\n\tif os.name==\"nt\":\n\t\tprint \"Windows Detected - Linux\"\n\t\tprint \"-----------------------------\"\n\t\tfrom tkinter"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\timport datetime\n\timport platform\n\tmac_ver = platform.mac_ver()\n\tif \"Darwin\" in mac_ver[0].lower():\n\t\tos_name = \"{}OS\".format(mac_ver[0][0].upper())\n\telse:\n\t\tos_name = {'x86': 'iOS', 'x86_64': 'iPhone', 'arm': 'ARM'}[mac_ver[3]]\n\n\treturn os_name\n\n\ndef check_path(path):\n\t\"\"\"\n\tChecks if the os is macOS\n\t\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tdef isMacOS(self):\n\t\treturn \"darwin\" in sys.platform\n\n\t\"\"\"\n\t\tChecks if the user's language of the program uses English\n\t:return: True if EN and false otherwise\n\t:rtype: bool\n\t\"\"\"\n\n\tdef containsEnglishEn(self):\n\t\treturn \"english\" in self.platforms\n\n\t\"\"\"\n\t\tChecks if a user uses the Windows operating system as its base for Linux/MacOS\n\t:return: True if Windows and false otherwise\n\t:rtype: bool\n\t\"\"\"\n\n\tdef containsWindowsBase(self"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\ttry:\n\t\tos.name\n\texcept:\n\t\tpass\n\telse:\n\t\treturn False\n\t\n\tosName = osl.uname()\n\tosPlatform = oss.platform()\n\tif osl.getenv('DISPLAY'):\n\t\tif(osl.getenv('DISPLAY').find('Mac')!= -1):\n\t\t\t#print 'Using Mac'\n\t\t\tscreen = osl.getenv('DISPLAY')\n\t\telse:\n\t\t\tprint('ERROR: Invalid Screen')\n\t\t\treturn False\n\t\tcmd = \"tellme -f '%"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.name == \"posix\" or os.name == \"nt\"\n\n\n# ###############################################################################\n# ##### CONFTYANTS\n# ###############################################################################\n\ndef get_user(env_or_user_arg):\n\t\"\"\"\n\tParse the user argument\n\t:param env_or_user_arg: the argument passed to the command line: \"user\" or \"env\"\n\t:return: command line argument or None\n\t\"\"\"\n\tif env_or_user_arg is None:\n\t\tusername = os.environ"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif os.name == \"posix\" or os.name == \"nt\":\n        \treturn True\n\treturn False\n\n\n#os_name = platform.system()  # Mac, Windows & Linux\nos_name = sys.platform  # Linux\n\n#os_platform = sys.platform  # Linux\n\nif os_name == \"nt\":\n\tx = 1\nelse:\n\tx = 0\n\n\n\"\"\"\n\tThe following is a simple string that is executed each time\n\tthe OS needs to check if the OS is using a file system\n \n\"\"\"\n#"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\treturn \"darwin\" in sys.platform  \n\n\n\"\"\"\n\tGet the current OS file system\n\"\"\"\n\ndef find_os_file_system():\n\ttry:\n\t\t# Note: For macOS, platform.uname is available while macOS, but this cannot be supported in Windows because macOS does not have it natively\n\t\tif \"linux\" in platform.system():\n\t\t\t# note: this has never to get the file system and there is a problem, so use the sys.filesystem_prefix to fix for the platform\n\t\t\tfullpath = os.path.join"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\tnew_item = dict()\n\t\n\tfor key in item:\n\t\tcurrent = item[key]\n\t\tkeys = list(current.keys())\n\t\t\t\t\n\t\tif '*' in current:\n\t\t\tkeys.remove('')\n\t\t\tnew_item[key] =  re.match('^{}'.format(key), current)\n\t\telse:\n\t\t\tnew_item[key] = re.sub('[^a-zA-Z_0-9]', '', current)\n\t\t\n\treturn new_item\n\n\"\"\"\n\n\"\"\"\nre"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\trules = {}\n\t\n\tfor filename in item.keys():\n\t\tfor key in item[filename]:\n\t\t\tregex_str = key\n\t\t\t\n\t\t\tif regex_str in regex_dict.keys():\n\t\t\t\tfor key_list in regex_dict[regex_str].keys():\n\t\t\t\t\tregex = regex_dict[regex_str][key_list]\n\t\t\t\t\tif isinstance(regex, str) and (re.search(r'[^{}]'.format(regex), filename)):\n\t\t\t\t\t\tif regex_key.search(key"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys = {}\n\tfor file_fullname, rules_from_file in item.items():\n\t\tkey_regex_dict = convert_to_regex(item=rules_from_file,\n\t\t\tkey=[re.escape(f) for f in re.escape(file_fullname)])\n\t\tfor key in key_regex_dict:\n\t\t\tkey = re.escape(key)\n\t\t\tkey = re.escape(key)\n\t\t\tpattern = '([{}])'.format(key)\n\t\t\tvalue = key_regex_dict["}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\trules = {}\n\tregex_keys = {}\n\t\n\tdef is_key_in(path, keys):\n\t\tmatch = False\n\t\tfor key in keys:\n\t\t\tif re.fullmatch(path+str(\"\\\\\"+key), path):\n\t\t\t\tmatch = True\n\t\t\t\tbreak\n\t\treturn match\n\t\n\tfor item_dir, sub_files in items.items():\n\t\tif item_dir not in rules:\n\t\t\trules[item_dir] = {}\n\t\tif '*' in sub_files:\n\t\t\tsplit_key = sub_files.split"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n# TODO: handle case-sensitive keys\n\ndef regex_dict(rules):\n\t\"\"\"\n\t\tRegex convert all keys to regex keys\n\t\t\n\t\tGiven a dict where the keys are all filenames with wildcards, convert only\n\t\tthe keys into equivalent regexes and leave the values intact.\n\t\t\n\t\tExample:\n\t\t\n\t\t\n\t\tdict_rule = {\n\t\t    '*.cpp':\n\t\t        [['.*/[cC]o[oO]p.*?\\.cpp'], ['.*/[cC]o[oO]p."}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor filename, cpp_files in item.items():\n\t\tkey = filename\n\t\tif regex_dict.has_key(filename):\n\t\t\tfor key_to_replace in regex_dict[filename].keys():\n\t\t\t\tif regex_dict[filename].has_key(key_to_replace):\n\t\t\t\t\tregex_dict[filename][key_to_replace] = regex_dict[filename][key_to_replace][0]\n\n\t\t\t\telse:\n\t\t\t\t\tregex_dict[filename][key_to_replace] = compile(key_"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\trules = item['rules']\n\tkey_dict = item['key_dict']\n\tdel item['key_dict']\n\t\n\tif not key_dict: # pragma: no cover\n\t\treturn rules\n\t\n\tregex_rules = {key:key_dict[key] for key in key_dict.keys() if re.match('^.*\\\\.', key)}\n\t\n\tfor item_name, item_dict in item['item_dicts'].items():\n\t\titem_name = item_name.replace('.', '[\\._-_"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\tregex_keys = {}\n\tcomponents = []\n\t\n\tfor filename in sorted(item['files_with_wildcard'].keys()):\n\t\tcomponents.append(filename)\n\t\tfor key in sorted(item['files_with_wildcard'][filename].keys()):\n\t\t\texp = re.compile(r'(.*)_(.*)$')\n\t\t\tregex_key = exp.search(key)\n\t\t\tregex_keys[filename,key] = regex_key.group(1).zfill(4)\n\treturn regex"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t# convert regex_dict into regex patterns as regexes are keys of dictionaries\n\tregex_keys = dict(item)\n\t\n\t# convert regex_keys to tuple (names_regex, regexp_key)\n\tnames_regex = [\n\t    (re.compile(regex_keys[\"namesregex\"][key]), regex_keys[\"namesregex\"][key])\n\t    for key in regex_keys[\"namesregex\"]\n\t]\n\t\n\tregex_pairs = [(key, {re.compile(key): re"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys = {os.path.basename(key):regex_dict(value)\n\t for (key,value) in iteritems(item)}\n\t\n\t# convert all keys to regexes\n\tfor key in regex_keys:\n\t\tregex_keys[key] = re.compile('([0-9a-fA-F]+)') # replace 0 and 6 with a dummy one\n\t\n\tdef is_valid_regex(regex_str):\n\t\treturn  re.match(regex_str, '*')\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = name.replace('\"', '')\nname = name.replace(\"'\", '')\nname = name.replace(\"\\n\", \"\")\nname = name.replace(\"\\t\", \"\")\nname = name.replace(\"\\s\", \"\")\nname = name.replace(\"\\r\", \"\")\n\n\n\t# if name.startswith('$'):\n\t#     name = name[1:]\n\nwith open('data.csv', 'r') as file:\n\tlines = file.readlines()\n# 1.2\nfor line in lines:"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t@staticmethod\n\tdef remove_quote(name):\n\t\tdef _remove_quote(text):\n\t\t\tdef remove_quote(text):\n\t\t\t\ti = 0\n\t\t\t\twhile i < len(text):\n\t\t\t\t\tif text[i] == u'\"':\n\t\t\t\t\t\tif text[i-1] in u' \"!':\n\t\t\t\t\t\t\ttext = text[:i] + text[i].replace(u'\"','') + text[i:].replace('\"','')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttext = text[:i] + text[i+1:].replace(u"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef __init__(self, name):\n\t\tself.name = name.strip().lower()\n\t\t\n\tdef do(self, text):\n\t\treturn re.sub(r'\\b{}', '', text, flags=re.MULTILINE)\n\t\t\n\tdef get(self):\n\t\treturn self.name\n\t\t\n\"\"\"\n\tRegex patterns for the remove quotes from name.\n\"\"\"\n\tdef __init__(self, name):\n\t\tself.name = re.sub(r'\\b\"\\b', \"\", name)\n\n\"\"\""}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef removeQuote(self, name):\n\t\t# First check if the name ends with a quotation mark \n\t\t# If it does remove the quotation from the end of the name.\n\t\tif re.search(r'^\\s{2,}', name):\n\n\t\t\t# This part of the code is to remove the spaces in the end of quotation marks.\n\t\t\tquote = \" \\n\\t\\n\"\n\t\t\tremove = re.replace(quote,r'\\n\\n','')\n\t\t\treturn remove\n\t\telse:\n\t\t\treturn name\n\t\t# return name\n\t\"\"\""}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef __init__(self,name):\n\t\tself.name = name\n\tdef __GetTokenCount(self,tkn_list,token_type):\n\t\ttkn_count = 0\n\t\tfor tkn in tkn_list:\n\t\t\tif tkn == token_type:\n\t\t\t\ttkn_count += 1\n\t\treturn tkn_count\n\tdef RemoveQuot(self):\n\t\tquote_indic = self.name.find('\"') + 1\n\t\tnew_name = self.name[:quote_indic] + re.sub('\"(.*"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef remove_quote(s):\n\t\tif ('\\\"' in s) : return \"\\\"\" + s[s.index('\\\"'):-1] + '\\\"'\n\t\treturn '\"' + s + '\"', s[s.index('\\\"'):].index('\\\"')+1\n\t\n\t\"\"\"\n\t\tFind and return all characters between start_index and end_index. Example 'camel_case' = 'camel_case'.\n\t\tNot the last character(eg. camel_case will not have the last character)\n\t\"\"\"\n\tdef find_single"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\treplacements = {\"(\", \")\", \".\", \"_\",\"`\",\"$\",\"$\":\"'\", \"-\":\"_\"}\n\tdef fix_name(name):\n\t\tname = name.lower().strip()\n\t\tfor replace in replacements:                  # loop over the keys in the dict\n\t\t\tname = re.sub(replace, \"\", name)\n\t\treturn name\n\tdef remove_quote(name):\n\t\tpattern = r\"\\\"?[^\\\"\\\"]*?\"\n\t\tname = re.sub(pattern, '', name)\n\t\treturn name\n\t\"'.\", \"' \", \""}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef clean_name(self, name):\n\t\tname = re.sub(\"\\[.*?\\]\", \"\", name)\n\t\tname = re.sub(\"\\\"(.|\\n)*?\\\"\", \"\", name)\n\t\tname = re.sub(\"(\\.)(?=[^a-zA-Z])\", \"\", name)\n\t\tname = re.sub(\"(\\.|\\s+)$\", \"\", name)\n\t\tname = re.sub(\"\\w*\\d\\w*\\.?\\w*\\d\\w*\", \"\", name)\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tif name == \"\" or name is None:\n\t\treturn None\n\telse:\n\t\tname = name.replace('\\\"', '')\n\t\tname = name.replace('\\'', '')\n\t\treturn name.replace('\\\\', \"\\xc2\\xe2\")\n\ndef match_email(email):\n\t\"\"\"\n\t\tMatch an email address into a valid\n\t\tvalid email and return the email, or\n\t\tNone if we don't understand that.\n\t\"\"\"\n\tpattern = re.compile(r'(([a-zA-Z0-9\\.\\-_]+\\"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tregex = \"\\\\(('[^')]*|\\()|([^()])\\)\"\n\treturn re.sub(regex, \"\\\\1\", name.lower())\n\ndef check(name):\n\t\"\"\"\n\tCheck if the given name is clean or invalid.\n\t\"\"\"\n\tname = clean(name)\n\tif not name: return False\n\tname = name.replace(\"  \", \" \")\n\tname = re.sub(\n\t\t\"\\t|\\n|&nbsp\",\n\t\t\" \",\n\t\tname\n\t)\n\tfor"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\timport os\n\timport fnmatch\n\timport sys\n\tcmdprefix=\"\"\n\tif sys.platform[:4] == 'freebsd':\n\t\tos_name=\"linux\"\n\t\tos_arch=\"\"\n\t\tplatform_val=0\n\t\t\n\telif sys.platform[:5] == 'darwin':\n\t\tos_name=\"mac\"\n\t\tplatform_val=3\n\t\tos_arch=\"\"\n\t\t\n\telif sys.platform[:3] == 'win':\n\t\tos_name=\"win\"\n\t\tplatform_val=1\n\t\tos_"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\timport string\n\timport re\n\timport os.path\n\timport getopt\n\timport shutil\n\tfrom os import system, path\n\tfrom subprocess import Popen, PIPE\n\t\n\tdef version():\n\t\t\"\"\" Returns a tuple containing the version in string form and the\n\t\t\tversion in int form.\n\t\t\t\n\t\t\tUsage: version()\n\t\t\"\"\"\n\t\treturn \"%s %s\" % (os.versionrel, os.version())\n\t\t\t\n\tdef version_compare(version1, version2):\n\t\t\"\"\" Return True iff two version strings are in"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tregex = [ r'(?!//)(.*?)(\\\\\\\\)|(?<!\\\\\\\\)(\\\\\\\\)*(>|[Xx]|[+-]?(?=.*?[1-9]|\\-[0-9])))',\n\t\t\t r'(?<!\\\\\\\\)(\\([^)]*\\)\\d*?(?=\\\\\\\\)\\))',\n\t\t\t r'(?<!\\\\\\\\)(<[^\\s<>]){2,}(?=>,)',\n\t\t\t r'(?<!\\\\\\\\)(?:=|-|>,|:"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef shlex_split(self, s, posix=None):\n\t\t\"\"\"Split string literal or cmd-line argument strings into a sequence of subprograms.\"\"\"\n\t\tif posix == 1:\n\t\t\treturn self.shlex_split_posix(s)\n\t\tcmd = s\n\t\tcmd_pattern = r'((?:[\\'\"](?:[^\\\\\\'\"]|\\\\.)*[\\'\\\"]))'\n\t\tif re.match(cmd_pattern, s, re.M):\n\t\t\treturn self.shlex_split_posix(s)\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tplatform = None # this will be updated with new args.\n\n\tdef __init__(self, argv, cwd=\".\", bufsize=-1):\n\t\t'''\n\t\t\targv: split args as list of strings.\n\t\t\tcwd: working directory for executing, default to the current directory.\n\t\t'''\n\t\tself.is_posix = self.is_posix_or_curl_for_platform(argv)\n\t\tself.cwd = cwd\n\t\tself.bufsize = bufsize\n\n\tdef __call__(self, argv, c"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t'''\n\t\tre.match(), 'a'[0] = match returns first match, ie first a.\n\t\tre.search, 'a'[1] = search_noescape = is only look at first n characters of string;\n\t\t\t\t\t\t\t\t\t\t\t\t1 if match, None if n matched only 1 char.\n\t\tre.split, 'a'[2:4] = split_noescape = split string on 'a'\n\t\tre.split('b..', 'x', maxsplit=2).\n\n\t\tre.split('b', 'xe', maxsplit=2),"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tif sys.platform == \"posix\":\n\t\tREGEX_PATTERN = re.compile(r\"^((.|\\s)*?)( |+(?![0-9][0-9-]*)\\.)*((.|\\s)*?)( |+)(?:.*|$)\\s\")\n\n\tif sys.platform == \"win32\":\n\t\tREGEX_PATTERN = re.compile(r\"_([0-9]+\\.?[0-9]+)\\.([A-Z_]+)(?:$|\\s)\")\n\n\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef __str__(self):\n\t\treturn '.'.join(self.args)\n\n\tdef split(self):\n\t\t\"\"\"Splits command-line or argv into single-element list.\"\"\"\n\t\timport string\n\t\tself.args = []\n\t\t\n\t\tc = string.split(self)\n\t\twhile c: #for each element in tuple\n\t\t\tif c[0] in self.options:\n\t\t\t\tself.args.insert(0, c.pop(0)) #the first element can be a substring(which is the option to pass on), if this is a"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\ndef split_cmd(arg):\t\n\t# Split by quotes:'split\"arg\" quotes: -S'.\n\t# Remove trailing'' in quote and quote pair, but leave's' as it is\n\tm = re.findall('\"((?:[^\"]|\\\\\")+)\"', arg)\n\t# Convert \" s\" to \"' s'\n\tm.append(m[-1].replace(\" \",\"\"\" \" \"\"\",1).replace(\" \",\"\"\" \"' \"\"\",1))\n\t# Replace \"'\" with an \"\n\treturn re.split('"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\nimport shlex\nimport argparse\n\n__version__ = 0.2\n\ndef shlex_split(string, sep=None, comment=None):\n\t'''\n\t\tCompiles a string of characters into a sequence of strings\n\t\t\n\t\t\tstring: String for compilation with shlex.split().\n\t\t\tsep: String character to split on\n\t\t\tcomment: Optional Comment to add to the end of string, for use in a 'comment' format\n\t\t\t\n\t\t\tIf not None, will split string at the sep (the default is a space).\n\t\t\n\t\t\tReturns list of strings"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tarchive_path = subprocess.check_output(['tar', 'xz', '--strip-components=1', '--preserve-mode', archive_path]).decode('utf-8')\n\n\tprint(archive_path)\n\n\t# This is the final function is to call the command to unzip everything and return it in a url.\n\n\treturn_code = str(subprocess.run('unzip -o {} -d {}'.format(archive_path, output_repo),capture_output=True))\n\n\n\treturn return_code\n\n\n\n\n#"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tarchive_path = \"\"\n\ttry:\n\t\tarchive_path = subprocess.check_output([\"archive\", \"-z\", \"--unzip\"]).decode().strip().split(\"\\\\\")[-1]\n\texcept:\n\t\tassert False, \"Unzipping archive failed.\"\n\tarchive_path = archive_path.replace(\"<RELEASE>\", \"master\")\n\tif archive_path.endswith(\"tar.gz\"):\n\t\tarchive_type = \"archive.tar.gz\"\n\telif archive_path.endswith(\"mkv\"):"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tdef uncompress(self, archivepath, filename):\n\t\t\"\"\"\n\t\t\tGet repository remote URL by file.\n\t\t\t\n\t\t\t:param archivepath: Path to the archive e.g. /tmp/archive.tar\n\t\t\t:param filename: Local filename ie name of the directory in the archive.\n\t\t\t:return:\n\t\t\"\"\"\n\t\turl = archivepath+\"/\"+filename\n\t\tcmd = self.compression+\" --remote -f \" + url +\" --out \"+archivepath+\"/\"+filename\n\t\toutput = subprocess.check_output(cmd)\n\t\tremoteurl"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t# Read the archive\n\tarchives = archive_path.glob(\"**/*.zip\")\n\tuncompressed_archives = [[archive.as_posix(), archive] for archive in archives]\n\tdef uncompress_archive(archivo):\n\t\t\"\"\"Uncompress a file given a path.\"\"\"\n\t\tif archivo in uncompressed_archives:\n\t\t\tarchive = uncompressed_archives[ar.as_posix()]\n\t\telse:\n\t\t\t# This is a simple zip in case the archive doesn't exist,\n\t\t\t# because the archive passed along already does not have"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t\n\t\n\t\n\tdef uncompress(archive_path):\n\t\t# Create a log and write to pipe and pipe to be used to read it.\n\t\tlog = LogManager()\n\t\tlog(logging.DEBUG, 'Uncompression start')\n\t\ttry:\n\t\t\trecode_string = f'uncompress archive from repo {archive_path}'\n\t\t\tfile_url = log.run_cmd(f'uncompress \"{recode_string}\"', shell=True)\n\t\texcept Exception as err:\n\t\t\tlog(sys.exc_info()["}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef zip_repo(archive_path, output, zip_method=None, **kwargs):\n\tif zip_method == 'gz':\n\t\t#gzip the file\n      \n    cmd = 'tar -zxv %s -C %s -f%s' %(''.join(kwargs.get('path', './').split(\"/\")),\n\t\t\t\t\t\t\t\t\t  archive_path + '.tar.gz');\n\tsubprocess.log.info(' '.join([cmd, str(int(os.path.getsize(archive_path) / 1024 **2))"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tarchive_path = archive_path.strip('/')\n\tif '.tar.bz2' in archive_path or '_dummy-archive' in archive_path: \n\t\t#archive_path_complement = archive_path.split('/')[-1]\n\t\t#archive_path_complement = \".\".join(archive_path.split('/')[:-1]) + \".tar.bz2\"\t\n\t\tarchive_path_complement = archive_path.split('/')[:-1]\t\n\t\tarchive_path_complement"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef uncompress(archive_path: str, out_dir: str, dry_run: bool = False):\n\t\n\tfile_repos = None\n\t\n\t# Try to open the archive and determine the size\n\ttry:\n\t\twith open(archive_path, \"rb\") as file_h:\n\t\t\tarchive_archive = file_h.read(4096)\n\texcept OSError as e:\n\t\traise Exception(f'\"{archive_path}\" is corrupted. Please try again.') from e\n\t\tprint(e)\n\t\traise e\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tdef __uncompress(self, archive_path):\n\t\tfilename = os.path.basename(os.path.normpath(archive_path))\n\t\tarchive_path = os.path.dirname(archive_path)\n\t\tif not os.path.exists(archive_path):\n\t\t\traise ValueError('Path %r does not exist!' % archive_path)\n\t\t\n\t\tif not os.path.exists(archive_path):    \n\t\t\tarchive = tarfile.open(archive_path, \"r:gz\")\n\t\t\tfor member in archive.get"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t# TODO: Move this to ArchiveUtil?\n\tarchive_path = url.path + \".compressed\"\n\n\tif len(archive_path) == 0 or not sub(archive_path, \"http://\", url.location_filename) == 0:\n\t\traise ValueError('Archive path not found: %s' % archive_path)\n\t\treturn None\n\n\tlocal_archive_path = archive_path.replace('.', '/')\n\tlocal_archive_path = os.path.realpath(local_archive_path)\n\n\tcommand = ['tar', '"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t#!/usr/bin/env python\n\t# -*- coding:utf-8 -*-\n\t\"\"\"\n\tCreated on:\n\t@timing\n\t@author:\n\t@file: git_ignore()\n\t@email:\n\t@site:\n\t@license:\n\t@description:\n\t@copyright:\n\t@sourcecode(C) 2020 - 2021 Yejin Kao (yejin.kao@gmail.com)\n\t@license(CC 0, see COPYING file)\n\t@github(https://github.com/yejink"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\npath = os.path.dirname(os.path.abspath(__file__)) # Get the directory where this file is present\nabsPath = os.path.join(path, 'Git-Commands') # Absolute path of this file\ngitCommand = ['git', 'add'] # Add additional files to use later\nfilesInDirection = [] # Files in directory\nignoreList = [] # List of ignored files\nignoreFileList = [] # List of ignored files in subprocess to print later\n\n# This is the absolute path to this file\nprint(\"[+]"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef get_files(source_dir_path):\n\tfile_list = []\n\tsubprocess.check_call(['git', 'ls-files', '-a', '--format=%cd', '--ignore-hidden'], \n\tstdout = subprocess.DEVNULL)\n\tfor file in subprocess.check_output(['git', 'ls-files', '-a', '--format=%cd', '--ignore-hidden', source_dir_path]).decode(\"utf-8\"):\n\t\tfile_list.append(file)\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ncomm_cmd = ['git', '--git-dir', '.git', '--no-pager', '--command-opts', '-c', 'grep \"%s\"\\n' % \".gbl_%s_ignore_%s.py\" % (GIT_REF, GIT_DIR_ID),\n\t\t\t'-l']\ndef process(ignorefile):\n\tignore = ['.', '/dev/shm', 'lib']\n\tignore.extend(ignorefile)\n\t\n\tfiles = []\n\tfor f in subprocess.Popen"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t#git_output = subprocess.run(['git', 'ls-files'], stdout=subprocess.PIPE)\n\t#git_output.check_returncode()\n\n\tgit_output = subprocess.run(['git','rev-parse', '--short HEAD'], stdout=subprocess.PIPE)\n\tgit_output.check_returncode()\n\n\tgit_output = subprocess.run(['git','rev-parse', '--short', 'HEAD'], stdout=subprocess.PIPE)\n\tgit_"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tdef cmd(self,args=[]):\n\t\toutput = subprocess.Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()\n\t\tlines = list(filter(None,output))\n\t\treturn ''.join([ l.strip() for l in lines ])\n\t\"\"\"\n\t\tFind out if all the files in the current directory are in the list of a current repository (for example the.git directory).\n\t\tReturns the directory path.\n\t\"\"\"\n\tdef isfile(self,path=\"/\"):\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n# TODO Make it run on localhost to avoid an exception in git\ndef get_list_files(dirpath):\n\ts=[os.path.join(a,file) for (a,b, files) in os.walk(dirpath) for file in files]\n\ts.sort( key=lambda s:s[len(s)-1:len(s)-1:1] )\n\treturn s\n\n\"\"\"\n\tWrite an array as a csv, one file per command\n\"\"\"\nimport csv\nclass csv_writer(object):\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfiles = subprocess.check_output(\"git status --porcelain --porcelain -s --name-only\", shell=True, encoding=\"utf-8\")\n\tfiles = str(files)\n\tfiles = files.split(\",\")\n\tfiles = list(filter(None, files))\n\tfiles.sort()\n\tfiles = [\"\\\"\" + files[i] + \"\\\"\" for i in range(len(files))]\n\treturn \"/\".join(files)\n\ndef filesMerge(files, path=\".\", directory=None):\n\t\"\"\"\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef create_dir(path):\n\tsubprocess.check_call(['git', 'init'])\n\tsubprocess.check_call(['git', 'config', 'path.git'])\n\tsubprocess.check_call(['git', 'config', '--global', 'user.username', 'kotlin'])\n\tsubprocess.check_call(['git', 'config', '--global', 'user.name', 'kotlin'])\n\tsubprocess.check_call(['git', 'config', '--global', 'user.email"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\twith open(\"ignored_file.txt\", \"w\", encoding=\"utf-8\") as f:\n\t\tf.writelines(file)\n\t\tf.close()\n\tsubprocess.check_output([\"git\",\"config\",\"--global\", \"user.name\", \"bionge-ai\"])\n\tsubprocess.check_output([\"git\",\"config\",\"--global\", \"user.email\", \"ai.bionge.ai@gmail.com\"])\n\tsubprocess.check_output([\"git\",\"config\",\"--global\", \"user.data_dir\", \""}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\ndef is_type_supported(filename):\n\t\"\"\"\n\tChecks if a file type supported by the module is supported.\n\n\tArgs:\n\t    filename: File to match against filename\n\tReturns:\n\t    Boolean denoting if the file has a supported file type\n\t\"\"\"\n\n\t# check to see if file is a python.py file\n\tif filename.endswith('.py'):\n\t\treturn False\n\n\treturn True\n\n\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Fil"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif \".jpg\" in filename:\n\t\treturn not (\"\\(a\\)\" in filename and \"\\(b\\)\" in filename)\n\n\treturn False\n\n#-----------------------------------------------------------------------------#\n\n# Check that the file is an jpg file\ndef check_image_type(filename):\n\tif not is_jpg(filename):\n\t\treturn False\n\n\treturn True\n\n#-----------------------------------------------------------------------------#\n\n# Check whether the given file is a jpg file\ndef check_jpg_file(filename):\n\treturn check_image_type(filename)\n\n#-----------------------------------------------------------------------------#\n\n# Check"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif not filename.startswith(\"__\"): return False\n\t\n\tif os.path.isdir(filename) == True:\n\t\treturn True\n\n\tif os.path.isfile(filename) == False or os.path.islink(filename) == True:\n\t\treturn False\n\t\n\tif os.path.isdir(filename) == False:\n\t\treturn False\n\t\n\tif os.path.basename(filename) in (\".egg\", \".bz2\", \".gz\", \".zip\"):\n\t\treturn False\n\t\n\treturn True\n"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tglobal _validateC, _validateH\n\ttry:\n\t\tif _validateC == 'c':\n\t\t\tif _validateH == 'h':\n\t\t\t\tif os.path.exists(filename):\n\t\t\t\t\tf = open(filename,'r')\n\t\t\t\t\tcontent = f.read()\n\t\t\t\t\tif content == _c_format_:\n\t\t\t\t\t\treturn True\n\t\telif _validateC == 'h':\n\t\t\tif _validateH == 'c':\n\t\t\t\tif os.path.exists(filename):\n\t\t\t\t\tf = open(filename,'r')\n\t\t\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tsupported = False\n\textension = os.path.splitext(filename)[1]\n\tif extension == \".py\": # it can't be a Python file\n\t\treturn False\t\n\telif extension == \".xlsx\":\n\t\tsupported = True\n\t\tfor dirpath, dirnames, filenames in os.walk(\".\"):\n\t\t\tfor filename in filenames:\n\t\t\t\tif filename[-3:]==\".txt\":\n\t\t\t\t\tsupported = True\n\t\t\t\t\tbreak\n\t\treturn supported\n\telif extension == \".png\":\n\t\tsupported = True\n\t\tfor dirpath"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif not os.path.exists(filename) or not os.path.isfile(filename):\n#\n#\t\tprint 'No such file or directory. Skipping...\\n'\n#\n\t\treturn True\n#\n#\tprint 'File or directory found...\\n'\n#\n\tpath = os.path.split(filename)\n\tfilename = os.path.basename(filename)\n#\n#\tprint path\n#\n\tfor dirname in path[0:1]:\n\t\tif os.path.isdir(dirname):\n\t\t\tif len"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tsupportedTypes = {\"png\", \"pngx\", \"jpg\",\"jpeg\", \"JPEG\", \"bmp\", \"dib\", \"tiff\"}\n\t\n\tpath = os.path.abspath(__file__).replace(\"\\\\\",\"/\")\n\t\n\t\n\tif \"png\" in filename and (\"pngx\" in filename or \"jpg\" in filename or \"JPEG\" in filename):\n\t\treturn True\n\t\n\tfor typeOfFile in supportedTypes:\n\t\tfor f in os.listdir(path):\n\t\t\tif f.lower().endswith("}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\ttry:\n\t\text = os.path.splitext(filename)[1]\n\t\tif ext in allowed_extensions:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\"\"\"\n\tCheck if a list or tuple of sub-lists are passed as a file\n\nArgs:\n    filenames: Filenames to match\nReturns:\n    False if not a match, True if supported\n\"\"\"\ndef lists_matching(in_files, filter):\n\tfilenames = []\n\tfor i in range(0, len"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t# Check if dirname has trailing trailing pathsep\n\t# (ie. /folder/filename.py)\n\tif os.path.dirname(filename) and not os.path.dirname(filename).endswith(os.sep):\n\t\treturn False\n\t# If dir and filename are equal, do nothing\n\tif os.path.dirname(filename) == os.path.dirname(os.path.abspath(__file__)):\n\t\treturn False\n\n\t# Get base case name of the filename\n\tfilename_base = os.path"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif filename.lower().endswith('.py'):\n\t\treturn False\n\telif filename.endswith('.sqlite'):\n\t\treturn False\n\telif filename.upper().find(\"://\") > -1:\n\t\treturn False\n\telse:\n\t\treturn True\n\"\"\"\n\tThis function looks for a matching prefix in filename and\n\treturns it. If no match is found, a default name\n\tis used (the prefix can be changed by adding a leading '_o'\n\tbefore the filename).\n\"\"\"\ndef find_prefix(filename,"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t__INPUT_STRING_PATTERN = re.compile(r\"^(\\d{1,2}\\s)([-\\w./-]\\d{1,2}(?:(?:\\s*,\\s*([-\\w]' ]+)?)))?\")\n\n\tdef process_date(self, str_input: str):\n\n\t\tparts_input = str_input.split(\" \")\n\t\tday = datetime.timedelta()\n\t\tday_input = datetime.datetime()\n\n\t\ttry:\n\t\t\tday_input = datetime."}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tweeks_str = ''.join(map(str, range(9)))\n\tmonths = {'January':1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'July':8, 'August':8, 'September':9, 'October':10, 'November':11, 'December':12 }\n\tweeks = int(weeks_str) + 1\n\n\tif weeks < 0 or weeks > 12:\n\t\tmessage = \"Frequency string must be one"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tdef frequency_parser(self, freq):\n\t\tunits = ['Y', 'M', 'D', 'H', 'S', 'W', 'U']\n\t\tif not (freq in units):\n\t\t\traise ValueError(\"Frequency is not in %r\" % (units,))\n\t\tif freq == 'Y':\n\t\t\treturn relativedelta(years=-1)\n\t\telif freq == 'M':\n\t\t\treturn relativedelta(months=-1)\n\t\telif freq == 'D':\n\t\t\treturn relativedelta(days=-1)\n\t\t"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tdef parse_dt(f_type, t_str):\n\t\t\"\"\"\n\t\t\tConvert a (format, time type) tuple to datetime.date or None.\n\t\t\t\n\t\t\t@param f_type the type of the tuple entry\n\t\t\t@type str\n\t\t\t@param t_str the time string\n\t\t\t@type str\n\t\t\t@return a datetime\n\t\t\t@rtype datetime or None\n\t\t\"\"\"\n\t\t# parse the time type from the time\n\t\tif f_type in (\"w\", \"1d\", \"2d\", \"3d\"):\n\t\t\treturn _parse"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tdef is_valid_unit(freq, unit):\n\t\t\n\t\tif unit == \"ns\" or unit == \"ps\" or unit == \"us\" or unit == \"ms\":\n\t\t\treturn freq in [\"d\", \"weeks\", \"days\", \"w\"]\n\t\telif unit == \"h\" or unit == \"m\" or unit == \"s\":\n\t\t\treturn freq in [\"h\", \"hours\"]\n\t\telif unit == \"d\" or unit == \"D\":\n\t\t\tif freq in ['days'] or freq in ['DAYS']:  \n\t\t\t\t"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif freq == \"\": pass\n\telif freq[0] == \"-\": freq = freq[1:len(freq)]\n\telif freq[0] == \"+\": freq = freq[1:len(freq)]\n\tif freq == \"weeks\": return datetime.timedelta(weeks=int(freq[0]))\n\telif freq == \"days\": return datetime.timedelta(days=int(freq[0]))\n\telif freq == \"hours\": return datetime."}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n# TODO: Implement parse_and_check function\n\ndef check_frequency(freq):\n\ttry:\n\t\tfraction, unit = freq.split(' ')\n\texcept ValueError:\n\t\traise ValueError(\"Freq string can be in format 3w or 3 w or 3 wm seconds. \\\n\t\t\tGot {} given.\".format(freq))\n\tif unit == \"s\":\n\t\treturn None\n\t\n\ttry:\n\t\tsecond = datetime.timedelta(seconds=float(fraction))\n\texcept ValueError:\n\t\traise"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\ttry:\n\t\tfreq_type = datetime.datetime.strptime(freq, '%Y-%m-%d').time()\n\t\treturn datetime.timedelta(0)\n\texcept ValueError:\n\t\ttry:\n\t\t\tfreq_type = datetime.datetime.strptime(freq, '%w').time()\n\t\t\treturn datetime.timedelta(0)\n\t\texcept:\n\t\t\traise ValueError(f'Unable to parse {freq}: {freq}')\n\n\"\"\"\t\n\tGiven a range"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\nclass DateTimeFinderPattern:\n\tdef __init__(self, frequency, format=datetime.isoformat):\n\t\tself.frequency = self.convert_frequency(frequency)\n\t\tself.format = format\n\t\n\tdef convert_frequency(self, frequency):\n\t\tif isinstance(frequency, datetime.datetime):\n\t\t\treturn frequency\n\t\thour = int(frequency / 3600)\n\t\tminute = int(frequency - 3600 * hour)\n\t\tsecond = int(frequency - (3600 * hour + 60 * minute))\n\n\t\tif hour < 10"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tdef parse_and_check(self, freqstr, expected=None):\n\t\tr\"\"\"\n\t\t\tParse and check the given frequency string.\n\t\t\t\n\t\t\t:type freqstr: string\n\t\t\t:type expected: string\n\t\t\t:rtype: datetime.timedelta or None\n\t\t\"\"\"\n\t\tif not freqstr:\n\t\t\treturn None\n\t\telse:\n\t\t\ttry:\n\t\t\t\treturn datetime.timedelta(**self.parse(freqstr))\n\t\t\texcept:\n\t\t\t\traise ValueError(\"Error parsing '{}' for timedelta.\".format"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif socket.gethostname()!= socket.gethostname():\n\t\treturn False\n\n\t# Check for port\n\ttry:\n\t\t_, _, protocol = socket.gethostbyname(socket.gethostname()).split(':')\n\texcept:\n\t\tprotocol = 'unknown'\n\tif protocol == \"\":\n\t\tprotocol = \"http\"\n\n\t# Create a socket\n\tsock = socket.create_connection((host, 80))\n\n\t# Return true if the host is the localhost\n\t# and the port is up and running\n\tif __"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\tif s.connect_ex((host, 0)) == 0:\n\t\treturn True\n\ndef host():\n\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ts.connect((\"python.org\", 80))\n\ts.close()\n\n\t\"\"\"\n\t\tCheck if the host is the correct one\n\t\"\"\"\n\n\treturn False\n\ndef host_ip():\n\treturn socket.gethostbyname(socket.get"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif host == 'localhost':\n\t\treturn True\n\ttry:\n\t\t_socket.create_connection(\n\t\t\t(host, port), timeout=timeout)  # Create the client socket\n\texcept:\n\t\treturn False\n\treturn True\n\n\ndef get_hostname(sock):\n\ttry:\n\t\tresult = sock.getaddrinfo('', port, socket.AF_UNIX, socket.SOCK_STREAM, socket.AI_PASSIVE)[0]\n\t\thost = result[4][0]  # Lookup in the list of aliases\n\texcept (Index"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\tif host == \"\":\n\t\treturn False\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ts.bind((host,0))\n\ts.listen(5)\n\treturn True\n\n\n\n\"\"\"\n\tTroubleshooting at the moment\n\t\n\t:return:\n\t\"\"\"\n\nimport os, sys\nimport pickle, getpass, time, re, argparse\n\n\ndef main():\n\tglobal user_name\n\tglobal host_name\n\tglobal list_of_channels\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif socket.gethostname() == 'localhost':\n\t\treturn True\n\treturn False\n\n\"\"\"\n    Checks if the user's ip is on an IP range. If it's not return False\n\"\"\"\ndef is_ip_valid(ip_address, network_prefix):\n\tif ip_address.count(\".\") == 7:\n\t\treturn False\n\ti = ip_address.index(\".\")\n\ti = network_prefix[i : i + 6]\n\tnetwork = f'{network_prefix}0'\n\tj = int(i)\n\ti ="}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\treturn 'localhost' in host or '127.0.0.1' in host\n\n\ndef connect_to_ip(host):\n\t\"\"\"\n\t\tConnects to the ip specified by the 'host' parameter\n\t\t:param host: The ip address of the host\n\t\t:return: True if the host is the ip of the remote machine\n\t\"\"\"\n\ttry:\n\t\ta = socket.socket.sockfld\n\texcept socket.error as error:\n\t\treturn False\n\n\tprint(host)\n\tsock = socket.socket.socket()\n\tsock."}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\thost = socket.gethostname()\n\texcept Exception as e:\n\t\tprint(f'{e}')\n\t\thost = 'localhost'\n\n\t# Check if the host is on a system where this computer runs on\n\ttry:\n\t\tsocket.gethostbyname(socket.gethostname())\n\texcept Exception as e:\n\t\tprint(f\"{e}\")\n\t\thost = 'localhost'\n\treturn host == 'localhost'\n\n\n\nasync def check():\n\t\"\"\"\n\tChecks for valid connections\n\t\t\n\t:return:"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t# This checks if the given ip address is only in the range of 0.0.0.0 to 255.255.255.255.\n\tip = socket.gethostbyname(host)\n\tif ip == '':\n\t\tprint(\"That is an invalid ip.\")\n\telse:\n\t\treturn False\n\n\n\"\"\"\n\tCheck the validity of the name\n\n\"\"\"\n# I use the socket module to create a socket.socket object\n\n# This method is to create a socket\n\nimport socket as s\n\ndef create_sock():\n\t# Create a Socket\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\treturn host == socket.gethostname()\n\n# Checks if the host is an IP address\ndef is_valid_IP_address(host):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET if int(host) <= 255 else socket.AF_INET6, None)\n\texcept socket.error as msg:\n\t\traise\n\texcept socket.error:\n\t\treturn True\n\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, None)\n\texcept socket.error as msg:\n\t\traise\n\n\t# This"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\tif host[0] == '.':\n\t\t\thost = host.split('.')\n\t\t\treturn all(host)\n\t\telse:\n\t\t\treturn True\n\texcept Exception as error:\n\t\tlogging.error(error) \n\t\treturn not bool(error)\n  \n  \n\"\"\"\n\tChecks if the ip is an ipv4 \n\"\"\"\ndef checks(ip):\n\ttry:\n\t\treturn IPV4_Address.check_ipv4(ip)\n\texcept Exception as error:\n\t\tlogging.error(error) "}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef find_paths(self, fragment, root):\n\t\tpath = re.split(os.pathsep, fragment)\n\t\t# replace / with? to match os_sep\n\t\tpath = [os.path.join(root, f).replace('/', '?') for f in path]\n\t\tpath = re.split(os.pathsep, path)\n\t\tpath = [f for p in path for f in (p, p) if f]\n\t\treturn path\n\n\n\"\"\"\n\tGiven a string as passed to `--patterns`, transform everything matching"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# https://stackoverflow.com/a/45454060/386064\n\t@staticmethod\n\tdef match(pattern):\n\t\t\"\"\"\n\t\t\tReturn a match object for a glob pattern to be matched on `path_pattern_str`\n\n\t\t\t:param path_pattern_str: a pattern as a string that may or may not have a leading slash, such as 'folder/*.*'\n\t\t\t\t or so (ie. '/*foo.txt*/**')\n\t\t\t:type path_pattern_str: str\n\t\t\"\"\"\n\t\tmatch = re.search(fr'(["}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tif (pattern):\n\t\tpaths = glob.glob(pattern)\n\t\tif paths:\n\t\t\tif (is_dir): print paths\n\t\t\telse: paths = filter(lambda f: os.path.isfile(os.path.join(f, '*/', '**.glb')), paths)\n\t\t\tprint list(paths)\n\nclass NoSlicese(Exception): pass\n\n##############\n# Classes #\n##############\n\nclass pathFinder(object):\n\t\"\"\"\n\t\tPathFinder\n\t\t=================\n\n\t\tGiven a path string"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\nclass Trie:\n\t\"\"\"\n\t\tTrie to use as `Trie.getWord`\n\t\"\"\"\n\t\n\tdef __init__(self, prefix: str):\n\t\t\"\"\"\n\t\t\tInitialize trie with empty words, prefixed with empty list.\n\t\n\t\t\tIf prefix/word/pattern not specified, `prefix` will be the root directory.\n\t\t\"\"\"\n\t\t\n\t\t# store prefix, and set to empty list\n\t\tself.prefix = prefix if prefix is not None else None\n\t\tself.words: list[str] = []\n\t\tself.cur"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\"\"\"\t\n\tExample::\n\t\n\t  [-]   find_paths [['*'] + ['**']*[2, 3, 4]]\n\t\n\t    find_paths (['-', 'pp:root/somedir'])\n\t\n\t .   find_paths (['-', '**/*.txt'])\n\t\"\"\"\n\t# _____ begin _____\n\t# _____ end _____\n\n\tdef __init__(self, pattern):\n\t\tself.patterns = pattern\n\n\tdef __call__(self, pathname):"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\nclass _ExpandedPathFinder(object):\n\t__slots__ = '_matcher', '_glob', '_paths'\n\t\n\t##\n\t# Create a pathfinder to match one or more paths that match the expression.\n\t# Parameters are the regex expression, and a list of path fragments or patterns to transform.\n\tdef __init__(self, glob, patterns):\n\t\tself._matcher = re.compile(glob)\n\t\tself._glob = glob\n\t\tif isinstance(patterns, basestring):\n\t\t\tself"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n# import sys, os\nfrom functools import partial\nimport itertools\n# sys.setrecursionlimit(100000)\nimport multiprocessing\n\nfrom pathlib import Path\npaths = Path.cwd().parts\n\ndef is_path(path):\n\treturn not isinstance(path, Path) or not path.is_absolute()\n\ndef extract_fragments(path):\n\tfile_path = Path(path)\n\n\tp = path.parent if file_path.is_relative() else file_path\n\tp.mkdir"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t# find_paths() expects:\n\t# iterable\n\t# sequence of paths\n\t# sequence of patterns as passed to `--pattern`\n\t\n\t# iterable to iterate over\n\t# sequence of patterns; pattern string or sequence of patterns\n\t\n\t# sequence of patterns; each pattern, or patterns; e.g. list of strings or list of list of strings\n\t\n\t# iterable to ignore\n\t# sequence of pattern or pattern lists; each list; e.g. list of strings or list of lists\n\t\n\t# sequence of patterns after"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tdef find_paths(self, search_expr, *args, **kwargs):\n\t\treturn tuple(itertools.chain.from_iterable([frag.match_pattern(search_expr) for frag in self.path_frags]))\n\t\n\tpath_frags = tuple(\n\t\t[\"sh:*/\" + pattern if is_pattern else pattern.strip(\"/ \") for pattern in self.path] or\n\t\t[\"sh:**/\" + pattern for pattern in self.path]\n\t)\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\nimport argparse\n\n\"\"\"\n\tThe main routine.\n\"\"\"\n\n# Function for turning a path fragment into a glob pattern.\ndef to_glob_pattern(fragment):\n\t\"\"\"\n    Turn a path fragment as a glob string (e.g. 'foo*.txt').\n        \n        Parameters\n        ----------\n\t        \n        fragment \n\t\t    the fragment to be converted to a glob string (e.g. '**/pp/somedir')\n\n        Returns\n        -------\n        \t\n\t        The glob patterns as a list of strings"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn __typeOfGitShell.lower().endswith('cmd.exe.im').casefold()\n\n\"\"\"\n\treturns True if you are in Docker and should execute \n\t\n\t:return: True if Docker\n\t\"\"\"\ndef isDocker(hostname):\n\treturn os.path.exists(hostname)\n\n\"\"\"\n\treturns True if you are using a Linux environment\n\t\n\t:returns: True if Linux\n\t\"\"\"\ndef isLinux():\n\treturn os.path.exists('/etc/localtime') or os."}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\t@staticmethod\n\tdef IsGitbash():\n\t\treturn True\n\t\n\tdef __str__(self):\n\t\treturn \"git bash\"\n\n\tdef _GetCommand(self):\n\t\treturn \"git\"\n\n\t@classmethod\n\tdef __Generate(self, target_folder):\n\t\tos.system('''echo '' > '{0}\\{1}.vsh'.format(${0}.path, \"${0}.cmd\")'''.format(target_folder, os.path.splitext(target_folder)[0]))\n\t\t\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tos_platform = os.name\n\tsystem_path = subprocess.run('systeminfo', capture_output = True,text=True)\n\n\t# return true if current is Git Bash\n\tif system_path == os.path.join(os.getcwd(),\"gitbash\"):\n\t\treturn True\n\n\traise NotImplementedError(\"{} isn't supported\".format(system_path))\n\t\n# **************** General class and functions that need to be implemented by all other classes. \n\ndef add_to_path():\n\tif hasattr(path,"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'nt':\n\t\treturn True\n\n\ttry:\n\t\timport subprocess\n\t\tret = subprocess.check_call(['git', '--version'])\n\t\tif(ret!= 0):\n\t\t\treturn False\n\n\texcept Exception as e:\n\t\treturn True\n\n\ndef createDirectory(directory):\n\t\"\"\"\n\t\tCreates directory of a given path\n\t\n\t\t@type  directory: file or directory path (string or file or directory as a path)\n\t\t@param directory: The new path of the directory you want to create in your repository\n\t\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tlinux_version = os.getenv('LANG')\n\tif 'x86_64' in linux_version:\n\t\treturn True\n\t\n\t\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n#==========================================================================\n# Returns the root of a git repo\ngit_repo = os.path.expanduser(\"~\") + \"/git\" + os.sep + \"repo\"\n\nif os.path.isdir(git_repo) or os.path.isfile(git_repo):\n\ttry:\n\t\twith open(git_repo, 'r') as fobj:\n\t\t\troot = fobj.readline().rstrip('\\n')\n\texcept (OSError, IOError, FileNotFoundError) as e:\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'nt':\n\t\treturn True\n\treturn False\n\n# --------------------------------------------------\n\ndef make_dirs(dirs, delete=False):\n\t\"\"\"\n\t\tcheck and delete directory in list\n\n\t\t:params dirs: list / path where do want to create dirs\n\t\t:params delete: True or False, to delete dirs\n\t\"\"\"\n\tlistdir = os.listdir(dirs)\n\tfor f in listdir:\n\t\ttry:\n\t\t\t# this is the root dir\n\t\t\tos.chdir(os.path.join("}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tos.system('git '\n\t\t'--version &>/dev/null'\n\t\t' && git branch -a && git status -o | head --')\n\ndef get_branch():\n\t\"\"\"\n\tget_branch will returns the current branch\n\n\t:return: string \"master\" if there's no branch and it's current branch\n\t\"\"\"\n\n\t# Get the name of the last git branch\n\tgitbranch = input('Name of last git branch: ')\n\t\n\t# Execute get_branch() and make returned string\n\treturn gitbr"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn( os.name == 'nt' ) & ( os.system('git'.split())!= 1064 )\n\ndef is_admin():\n\t\"\"\"\n\t\tchecks if the current process is admin\n\t\t:return True if is admin\n\t\"\"\"\n\tfrom win32wnet import WhereIsAdmin\n\n\ttry:\n\t\tWhereIsAdmin(\"net.inet_ntrps.com\")\n\t\treturn True\n\texcept:\n\t\treturn False\n\ndef is_working_dir(path):\n\t\"\"\"\n\t\tchecks if the path is a working directory\n\t\t:return True if it"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\ttry:\n\t\timport config\n\t\tconfig.init()\n\t\treturn True\n\texcept:\n\t\treturn False\n\n\"\"\"\n\tThis function generates a key in a dictionary\n\tkey: value\n\n\t:param n: nth key\n\t:return: nth key\n\"\"\"\ndef generate_key(n):\n\timport string\n\tfrom os import getenv\n\tk = \"\"\n\ta = \"\".join(random.choice(string.ascii_letters) for i in range(16))\n\tfor i in getenv('gitserv').split(';'"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\tif not os.path.exists(os.path.expanduser(config['dir'] + targetFile)):\n\t\t# We don't want to overwrite anything we already have, and are not allowed to.\n\t\tif not overwrite and os.path.isfile(os.path.expanduser(config['dir'] + targetFile)):\n\t\t\tprint('No overwrite requested: skipping: %s'%path)\n\t\t\treturn\n\t\t\n\t\t# Make sure it's going out of date.\n\t\tsnow = os.stat(config['dir'] + targetFile)"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef validate_config_file_list(file_list, config_file):\n\tif  os.path.exists(config_file) and any(os.path.isdir(config_file) for config_file in file_list):\n\t\tprint(\"Warning: The config directory already exists\", file=sys.stderr)\n\t\treturn False\n\n\tif any(list(map(os.path.exists, file_list))):\n\t\tprint(\"WARNING: the following config files still exist:\", file=sys.stderr)\n\t\tprint(\" \"."}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tdef write_config(self, config_filename, new_config):\n\t\ttgt_config_file=open(config_filename, 'w')\n\t\ttgt_config_file.write(new_config)\n\t\ttgt_config_file.close()\n\t\tself.write_target_config(config_filename)\n\t\treturn\n\n\tdef write_target_config(self, config_filename):\n\t\tlines=\"\"\n\t\tlines+= \"## CONFIGURED VALUES\\n\"\n\t\tfor k, v in self.config.get().items():\n\t\t\tlines="}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tdef write(self, conf_filename, render_filename, overwrite, override=False, logger=None):\n\t\tif os.path.exists(conf_filename) and not overwrite:\n\t\t\treturn\n\n\t\tself._logger = logger\n\t\tself._overwrite = override\n\t\tself._overwriteCheck = logger.isEnabledFor(DEBUG)\n\n\t\t# Create config directory\n\t\tif not os.path.exists(os.path.dirname(render_filename)):\n\t\t\ttry:\n\t\t\t\tos.mkdir(os.path.dirname(render_filename))\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n# create target dir\nos.mkdir(os.path.join(BASE_DIR, 'out'))\n\ndef main_test():\n\t\"\"\"TODO, add docstring\"\"\"\n\tpass\n\n\ndef make_p(config_name, output_filename):\n\t# read config YAML\n\tconfig = yaml.load(open(config_name + \".yaml\"))\n\n\tdirout = config"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tdef saveConfigToFile(directoryBase, configDict, overwrite=False):\n\t\tif not os.path.exists(directoryBase):\n\t\t\tos.makedirs(directoryBase)\n\t\tif os.path.exists(directoryBase + '/config'):\n\t\t\tif not overwrite:\n\t\t\t\treturn True\t\n\t\tconfigFile = os.path.join(directoryBase, 'config.yaml')\t\t\t\t\n\t\twith open(configFile,\"w\") as outfile:\n\t\t\toutfile.write(yaml.dump(configDict, default_flow_style=False"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\tdef __init__(self, filename, data, overwrite=False):\n\t\tself.absname = os.path.abspath(filename)\n\t\tself.absdir = os.path.dirname(self.absname)\n\t\tif not os.path.exists(self.absdir):\n\t\t\tos.makedirs(self.absdir)\n\t\tself.renamed = False\n\t\tself.open()\n\t\tself.render(data, overwrite)\n\t\n\tdef open(self):\n\t\t\"\"\"\n\t\t\tOpens the config file if not already"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\ttarget_dir, target_fn = os.path.split(config_file)\n\n\tdest_dir, dest_fn = os.path.split(rendered_target)\n\n\t# If the file already exists, abort early.\n\tif dest_target_fn == dest_fn:\n\t\treturn True\n\t\n\tos.mkdir(dest_dir)\n\n\t# Write the target config YAML\n\trendered_file = rendered_template.replace('TARGET', target_fn).replace('TARGET_DIR', target_dir)\n\twith open(config_file"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n# If the user specifies --config flag, output the config filename with this flag.\n# Otherwise, we read the target config from YAML and use the format specified by the target.\nif args.config:\n\tconfig_path = args.config\n\t\t\n# If the configuration directory already exists, abort in order to try and overwrite or\n# use a newer filename. Otherwise if user wants to force file overwriting or create\n# a new folder with empty content, we'll force it to create until the new content has been\n# written to correct file. We will create the folder in this"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef main():\n\tos.chdir(__name__)\n\t# create empty directories\n\tfor c in os.listdir('.'):\n\t\tif 'configs' in c:\n\t\t\tos.rename(c, os.path.join('.', c))\n\n\t# create main config directory\n\tmainDir = os.path.dirname(os.path.abspath(__file__))\n\tfullname = os.path.join(mainDir, 'configs/config-build.yaml')\n\twith open(fullname,'r')"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tscript = textwrap.dedent(script)\n\treturn script.strip()\n\t\n\ndef getargs(name):\n\t\"\"\"\n\t:param args: the arguments to the script, passed to the script\n\t:return:\n\t\"\"\"\n\treturn shlex.split(filepath(name))\n\t\ndef convert_commands(commands):\n\t\"\"\"\n\t\tconverts commands to one line for the script.\n\t\t:param commands:\n\t\t:return:\n\t\t\"\"\"\n\tscript = ''\n\tfor command in commands:\n\t\tscript += convert_cmd_"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t# This uses the subprocess module (https://docs.python.org/3/library/subprocess.html)\n\t# which we're importing dynamically\n\n\treturn \" \".join(textwrap.wrap(script, width=10)).strip()\n\n\ndef build_arg_list(args):\n\t\"\"\"\n\t\treturns a list of arguments to run the specified ssh command\n\t\"\"\"\n\timport paramiko\n\tfrom getpass import getpass\n\t# get input from the console\n\tprint(create_script(command_line_arglist(args)))\n\n\tssh_conn ="}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tscript_commands = []\n\tfor command in textwrap.split(script, splitter=' | '):\n\t\tscript_commands.append(command)\n\tprint(' '.join(script_commands))\n\n\ndef convert_script(script, command, nargs={}):\n\t\"\"\"\n\t:param script: this is a python line of text to be converted\n\t:return:  a dict of key:value pairs where keys are the script elements and values are the converted commands.\n\t(See script for all commands)\n\t\"\"\"\n\tprint(f\"Converting"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tscript = str(script).strip(' ')\n\tscript = script.replace('+', '')\n\toutput =  textwrap.dedent(script).replace('#', '--- #').replace('.py','# ')\n\n\tcommands = output.split()\n\t\n\tcommands = commands[2:-1] #to remove the quotes at the end\n\tcommands = commands[:1] #not using last element here, just removing first command...\n\tfor cmd in commands:\n\t\t#print(cmd)\n\t\tout = re.split(r'\\"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tprint(\"Script: {}\".format(script))\n\ndef convert(command):\n\t\"\"\"\n\tConverts script to one line command via the shell script.\n\n\t:param command: script to be converted\n\t:return:\n\t\"\"\"\n\tcommand = command.replace(\"'\", '\"').strip()\n\tos.system(f\"ssh -i file@{f\"{user}@{host}\"} '{command}'\")\n\n\ndef change_password(old_password, new_password):\n    os.system('echo -e \\'user: {0}\\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tdef __init__(self, script, output=False):\n\t\tif type(script) is str:\n\t\t\tscript = re.split('([^\\s]*).*$', script)\n\t\tself.script = script\n\t\t\n\t\t# check that output is always a string. if not, error out.\n\t\tself.output = type(script[0]).__name__ + ':'+ str(script[0]) if output == False else script[0]\n\t\t\n\t\t# make a new dict, the key is the first script's name\n\t\tself.output_"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tscript=script.strip()\n\t# check if text is whitespace\n\tnum2string = \"[\", \"]\" \n  \tstr2list = [];    # empty list in string\n\t# script = script.replace(num2string[0] + \" \", \" \" + str(num2string[0] + \" \").replace(\"\\t\", \" \"))\n\tscript = script.replace(num2string[0] + \" \", str(num2string[0] + \" \").replace(\"\\t\", \" \"))  # add space automatically if it is an empty"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tshell, returncode, stdout = get_cmd(script)\n\tif returncode!= 0:\n\t\treturn None\n\tstdout = stdout.split()\n\tfor sg in stdout:\n\t\tprint(sg.split(\" \"))\n\treturn shell\n\n### end of file: scripts.py\n\n# def run_comm(script, verbose=False):\n# \twith open(\"{}.script\".format(script), \"w\") as text_file:\n# \t\ttext_file.write(script)\n# \t\tpass \n\t\t\n\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tdef __init__(self):\n\t\tif len(self.cmd)!= 1 and len(self.commands)!= 1:\n\t\t\traise ValueError(\"Script is not a single line script\")\n\n\tdef __str__(self):\n\t\tif len(self.cmd)!= 1 and len(self.commands)!= 1:\n\t\t\traise ValueError(\"Script is not a single line script\")\n\t\treturn self.cmd[0] + \" \" + \" \".join(self.commands)\n\n\tdef as_cmd(self):\n\t\treturn \" \".join(self."}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tscriptList = [cmd]\n\t\n\tcmdText = textwrap.dedent(script)\n\tscriptSliced = textwrap.fill(scriptText, 70, initial_indent='#', subsequent_indent='#')\n\tscriptSliced = scriptSliced + '\\n\\n#\\n'\n\tscriptList.append(scriptSliced)\n\t\n\tif textwrap.indentwidth > 2:\n\t\tscriptLen = len(script) + len(scriptSliced)\n\t\tindentSpaced = (textwrap.coll"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef __init__(self, func, args):\n\t\tself._func = func\n\t\tself._func_name = (self._func.__module__ + '.' + \\\n\t\t\t\t\t\t  self._func.__name__) if args else str(self._func)\n\t\tself.args = args\n\t\tself.extra_env = os.environ.copy()\n\t\tfor k,v in args.items():\n\t\t\tif k == \"env\" and isinstance(v,dict):\n\t\t\t\tfor u,vv in v.items():\n\t\t\t\t\tself.extra_env[u"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\tdef call(*args, env: dict = None) -> subprocess.Popen:\n\t\tif env == None:\n\t\t\treturn subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, capture_output=True)\n\t\telse:\n\t\t\treturn subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env, capture_output=True)\n\t\n\treturn call\n\ndef run_proc(args: str, env: dict = None)"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport sys\n\tsys.path.insert(1, \"./utils\")\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\timport matplotlib as mpl\n\t\n\tdef myfunction(a,b, c, d, e, f, n ):\n\t\treturn a +b +c + d + e +f +n\n\t\t\n\tfuncs = []\n\tfor ii in range(12):\n\t    a = np.array([10,20,30,40,50,80,90,100,110,120,"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef run_subproc(self, func, *args, **extra_env):\n\t\t\"\"\"Run the given module function in the given Python environment.\"\"\"\n\t\tif func is None:\n\t\t\tprint(\"Subcommand \\'%s\\''\" % extra_env.get('funcname'))\n\t\t\tprint(\"Error: 'func' must be a function.\")\n\t\t\tprint(\"If not a func name, pass 'None' to ignore this entry.\")\n\t\t\tsysexit(-1)\n\t\t\tpass\n\t\telse:\n\t\t\tprint(\"Subcommand \\'%s\\''\" % extra_env.get"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\tdef __init__(self, func, *args, extra_env = {}, **kwargs):\n\t\t\n\t\tif extra_env!= {} and isinstance(extra_env, dict): # avoid overriding an already-installed\n\t\t\t# copy so that these are not overwritten.\n\t\t\textra_env = os.environ.copy()\n\t\t\textra_env.update(extra_env)\n\t\t\n\t\t# Call `subprocess.run` with the given arguments.\n\t\tself._proc = subprocess.run( [ 'python', self.path ] +  list("}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif (type(func) is not types.FunctionType):\n\t\traise ValueError(\"Can only perform a function\")\n\tif (len(args)!= 0):\n\t\traise ValueError(\"Arguments not accepted\")\n\tfor var in ('subprocess.check_output', 'check_output', 'check_call'):\n\t\texec('%s.%s = types.FunctionType(%s, %s)' % (module_path, var, module_path, var.replace('check_output', 'run(%s)' % type(func))))\n\"\"\"\tRun a"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef run(self, func, *args, **extra_env):\n\t\tlog.debug(\"{function}()\".format(function=func.__name__))\n\t\tlog.debug(\"  env: {env}\".format(env=extra_env))\n\t\tlog.debug(msg=\"arguments:{args}\".format(args=args))\n\t\tlog.debug(msg=\"function:{function}\".format(function=func.__name__))\n\n\t\toutput = None\n\t\tstartupinfo = None\n\t\tif self.debug:\n\t\t\tstart = subprocess.Popen"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\treturn subprocess.run([self.__program, '-c', f'import {funcname}; {kwargs.get(\"args\")}={kwargs.get(\"extra_env\")}'], env=kwargs.get('env'), shell=kwargs.get('shell'))\n\t\n\t\t\t\n@click.command()\n@click.argument(\"cmd\")\n@click.option(\"--stdout\", type=str, default=\"stdout\")\n@click.option(\"--stderr\", type=str, default=\"stderr\")\n@click.option(\"--"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef __init__(self, func, *args, extra_env=None):\n\t\tself.proc = subprocess.Popen([str(arg) for arg in args],\n\t\t\t\t\t\t\t\tcwd=None, stdin=None, stdout=None,\n\t\t\t\t\t\t\t\tuniversal_newlines=True)\n\t\tself._proc = self.proc\n\t\tself.__init = True\n\t\tif extra_env is None:\n\t\t\tself.extra_env = {}\n\t\telse:\n\t\t\tself.extra_env = env = {}\n\t\t\tfor arg, value in env:\n\t\t\t\tif arg"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tp = subprocess.run(\n\t\targs[0], shell = True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, creationflags = subprocess.CREATE_NO_WINDOW\n\t\t, env = extra_env\n\t)\n\n\tprint(p)\nif __name__ == '__main__':\n\tmain()\n\n\"\"\"\n\tThe functions we have built include (and is not required to have)\n\t* make_directory_if_needed : Create output directory.\n\t* log_output : Write output to a"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t@staticmethod\n\tdef format_value_from_environment(matcher, environment):\n\t\tif type(environment) is dict:\n\t\t\treturn environment.get(matcher)\n\t\treturn environment.get(\"{}\".format(matcher))\n\n\t\"\"\"\n\t\tFunction to format the value in a format string.\n\n\t\t@param matcher The format string to format.\n\t\t@param is_str If value is a string, this function will format it with '{}'. Otherwise, it\n\t\t\t\t\t\t\twill simply return the value.\n\t\t@return The format string.\n\t\"\"\"\n\t"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t@staticmethod\n\tdef get_env():\n\t\t\"\"\"\n\t\t\tGet the value from environment\n\t\t\tReturns\n\t\t\t=======\n\t\t\tbool\n\t\t\t\tIf the variable is defined in environment: True. Else, False\n\t\t\"\"\"\n\t\tif \"AIRFLOW_ENV\" not in os.environ.keys():\n\t\t\traise ValueError(\"No environment variable 'AIRFLOW_ENV' was set.\")\n\t\telif os.environ['AIRFLOW_ENV'] == 'prod':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n# Set the default value"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef _from_env(self, env: dict = None) -> str:\n\t\tif env and self.env:\n\t\t\tfor key in self.env:\n\t\t\t\tif key in env:\n\t\t\t\t\treturn env[key]\n\t\tif self.default is not None and self.default is not False:\n\t\t\treturn self.default\n\t\traise EnvironmentError(\"Could not find variable in environment. Default: {}\".format(self.name))\n\n\t\"\"\"\n\t\tGet the value from given path to the given environment.\n\t\tReturns an error if the environment has no variable.\n\t\t\"\"\"\n\t"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\nimport getpass\nimport platform\nimport sys\n\nfrom src.config import cfg\n\nprint(\"-------------------------------\")\nprint(\"-- Script: {}\".format(os.path.basename(__file__)))\nprint(\"-- Date: {}\".format(platform.uname()))\nprint(\"-- Author: J. Hildebranduk, CSC.103\")\nprint(\"-- Version: {}\".format(\"2.0.0\"))\nprint(\"-- Release: {}\".format(\"1.0.0\"))\nprint(\"-- Config File: {}\".format(os.path."}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif type(matcher)!= dict:\n\t\traise ValueError('match variable is required as a dictionary')\n\tvariables = os.environ.get(matcher, None)\n\tif variables:\n\t\treturn variables\n\telse:\n\t\traise KeyError('variables variable is not present in your environment as expected')\n\n\n\nif not os.path.exists('../docs/doc.csv'):\n    raise IOError('doc.csv file not found')\n\nmatch = 'file_name'\nfile_data = f.csv_reader(f)\nvalue"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t#os.path.exists(file)\n\tmatch = re.search(r'^(\\w+)\\s*=(.*)', file, flags=re.MULTILINE)\n\t#print(match)\n\tvar = match.group(1)\n\tvalue = os.getenv(var, default_value)\n\treturn value\n\"\"\" Function to parse the variable from the file and return its value.\t\n\tExample: if the variable var is defined in file, function parser(file): return os.getenv('var')\n\tArguments : - file :"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\nclass VarUtil(object):\n\t\"\"\"\n\t\tThis class allows to get the value of the environment variable or a default value if the environment variable is not defined in a dictionary.\n\t\"\"\"\n\t\n\t@staticmethod\n\tdef get_value(name, as_str_type = False):\n\t\t\"\"\"\n\t\t\tGet the given environment variable or a value from the environment.\n\t\t\t:param name: name \n\t\t\t:param as_str_type: flag to be used to turn a string type to another type (boolean, int, float, list...)\n\t\t\t:return:"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef getValueFromEnvironment(self, env):\n\t\tfor name, value in env.items():\n\t\t\tif name == 'HOME':\n\t\t\t\treturn value\n\t\t\telif '.' not in name:\n\t\t\t\tvalue = self.getValueFromEnvironment(environment)\n\t\t\telse:\n\t\t\t\tself.assertPathIsValid(value, env, 'Environment')\n\t\treturn env['HOME']\n\n\t@classmethod\n\tdef assertPathIsValid(cls, value, env, name):\n\t\tif value is None:\n\t\t\traise ValueError('%s is null' % name)\n\t\t"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef getValue(self, var_name):\n\t\tif var_name not in self.env:\n\t\t\traise EnvironmentVariableDoesNotExist(var_name)\n\t\tif self.env[var_name]:\n\t\t\treturn self.env[var_name]\n\t\telse:\n\t\t\traise EnvironmentVariableNotProvided(var_name)\n\n\t\"\"\"\n\tSet the given variable as an environment variable.\n\t\"\"\"\n\tdef setValue(self, var_name, value, clearable=False):\n\t\tself.env[var_name] = value\n\t\treturn clearable"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif(name == None or value == None):\n\t\tValueError(\"A mandatory variable is missing!\")\n\n\t_env_dict = {\n\t\t\"CONFIGURATIONSDIR\" :  os.path.join(os.path.dirname(__file__), \"configurations\"),\n\t\t\"CONVERTERSDIR\"\t: os.path.join(os.path.dirname(__file__), \"templates\"),\n\t\t\"CONFUTEDIR\"\t: os.path.join(os.path.dirname(__file__), \"confused\"),\n\t\t\""}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tpass\n\n\ndef get_image_id(image_href):\n\t\"\"\"\n\tGet the ID of an image with the given href (as a string).\n\t\n\t:param image_href: href of an image\n\t:returns: ID of the image with that href\n\t:raises ValueError: if the href doesn't match an image\n\t\"\"\"\n\tpass\n"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttry:\n\t\turl = urllib.parse.urlparse(image_href)\n\texcept ValueError:\n\t\tpass\n\telse:\n\t\tnetloc: str = \"\"\n\t\tparsed = urllib.urlparse(image_href)\n\t\tif not url.netloc == parsed.netloc:\n\t\t\tnetloc = \"https:\" if url.netloc!= \"https:\" else \"http:\"\n\t\t\tbase_url = netloc + parsed.netloc\n\t\telse:\n\t\t\tnetloc = -1\n\t\tuse_ssl = True if parsed.scheme"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\treturn urllib.parse.parse_qsl(urllib.parse.urlparse(image_href).query, keep_blank_values=True)\n\n\ndef parse_image_id(image_href):\n\t\"\"\"\n\t\tGet the unique id from the image_href.\n\t\t\n\t\t:raises ValueError: an image_href did not contain a non-empty 'id' param: {'id': 'fake-id'}\n\n\t\t:param image_href: a url of an image\n\t\"\"\"\n\treturn urllib.parse.unquote_plus"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# This function is based on the algorithm described at: \n\t# http://cgit.freedesktop.org/ggcg/image-cache/tree/src/image_cache/lib-image_cache.c\n\t# This code performs the parsing the URL to grab image data\n\t# and strips it of reserved characters\n\tdef _parse_fetch_url(url):\n\t\t\"\"\"\n\t\tParse the href into parts as required by ImageCache.\n\t\t\n\t\t:param url: href of a server image\n\t\t\tthis returns a list of tuples of the following"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\turl \t= image_href.strip().lower()\n\tparsed = (url, '', 0)\n\tif url[0] == '/':\n\t\tparsed = (url.replace('/', ''), '', 1)\n\tif parsed[1] == '/':\n\t\tparsed = (parsed[0] + '', '', 0)\n\tuse_ssl = parsed[2]\n\tif not use_ssl:\n\t\ttry:\n\t\t\tparsed[2] = urllib.urlopen(parsed[0])."}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# https://stackoverflow.com/questions/45784086/split-an-image-to-png-and-tiff-to-pdf\n\t# The code below allows to use an image that has a single image tag, with one text tag at the end. The other two, the tag will always get the name of the image and not just the full URL.\n\t# https://stackoverflow.com/questions/38362218/how-to-split-a-url-into-two-parts-in-python-with-urllib\n\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\thref = image_href\n\tif 'http:' not in image_href:\n\t\tfor suffix in image_href.split('.'):\n\t\t\ttry:\n\t\t\t\t_image_id, _netloc, _use_ssl = int(suffix), '', 1\n\t\t\t\treturn (_image_id, _netloc, _use_ssl)\n\t\traise ValueError\n\n\t# netloc = urllib.parse.urlparse(_netloc)\n\t# ssl = int(urllib.parse.urlparse(_netloc).scheme.split(':')[0])"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_id = urllib.parse.parse_qs(urllib.parse.urlsplit(image_href)[4][1:])['id'][0]\n\turl = urllib.parse.urljoin(image_href, image_id)\n\tnetloc = '', urllib.parse.urlsplit(url)[2]\n\tuse_ssl = True\n\treturn image_id, netloc, use_ssl\n\n\ndef remove_image_file(image_id, image_link, image_extension, output_dir):\n\t\"\"\""}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tpass\n\ndef parse(image_href):\n\t\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, url, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tpass\n\ndef parseXml(xml_element):\n\t\"\"\"\n\tParse an XML tree into a dictionary.\n\t\n\t:param xml_element: xml element to parse\n\t:returns: a dictionary\n\t:raises ValueError"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttry:\n\t\tquery_str = urllib.parse.urlsplit(image_href)[0]\n\texcept IndexError:\n\t\traise ValueError('No image found')\n\t\n\t# Handle the various protocols.\n\tif query_str.startswith('://') or query_str.startswith('http:'):\n\t\tparts = tuple(cbor2.loads(query_str[len('://'):].split('='))['parts'])\n\t\t\n\t\t# It could be a path, a path relative to the image path, a full path\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tdef remove_trailing_os(input_list):\n\t\t\"\"\"\n\t\t\tIterate over a string list and remove trailing os seperator characters.\n\t\t\t\n\t\t\tArguments:\n\t\t\t\tinput_list: a list of arguments to run through a for and for until there are no more\n\t\t\t\t\t\t   remaining args it returns true.\n\t\t\t\n\t\t\tReturns:\n\t\t\t    True when all the needed args have been removed and false if all args have been removed.\n\t\t\"\"\"\n\t\t\t\tprint(\"Processing arg list...\")\n\t\targv = getArgs()\n\t\tfor arg in argv:\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\tdef test_list(input_list):\n\t\t\n\t\t# test if len is greater than 1\n\t\t\n\t\tif len(input_list) < 2:\n\t\t\traise TypeError(\"String list must have at least 2 elements\")\n\t\t\t\n\t\telif input_list[-1]!= '\\\\':\n\t\t\traise TypeError(\"Pathname seperator must be the last element\")\n\t\t\t\n\t\telse:\n\t\t\t# test if last character matches seperator character\n\t\t\t\n\t\t\tis_trailing_seperator = input_list[-1].find('\\\\')"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif type(input_list)!= list:\n\t\traise TypeError(\"input list must be of type list\")\n\tfor string in input_list:\n\t\tif len(string) > 1 or string.strip() == \"\":\n\t\t\traise TypeError(\"string must be 1-3 character and/or end with '/'\")\n\tprocessed_strings = []\n\tfor string in input_list:\n\t\tstring_without_seperator = string.strip()\n\t\tif string_without_seperator.lower().endswith(\"/\"):\n\t\t\tprocessed_strings."}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t# Test if input is a list\n\tif isinstance(input_list, list):\n\t\t# Ensure list has no duplicate items\n\t\tif len(input_list) == len(set(input_list)):\n\t\t\t# Remove trailing os separator characters\n\t\t\tstriped_list = [s.rstrip(os.path.sep) for s in input_list]\n\t\t\treturn [s.rstrip(os.path.sep) for s in stripped_list]\n\t\telse:\n\t\t\traise TypeError(\"List contents are not strings\")\n\telse:\n\t\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Input object is not list\")\n\t\t\n\tfor i in input_list:\n\t\tif len(i) <= 1:\n\t\t\traise TypeError(\"Invalid input\")\n\t\t\t\n\twhile True:\n\t\t# Remove trailing os seperator (if found)\n\t\ti = i.strip(\"OS\\n\")\n\t\tif not i.endswith(\"\\n\"):\n\t\t\tbreak\n\t\ti = i.rstrip(\"\\n\")\n\t\t\t\n\treturn input_list\n\t\n\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\ttry:\n\t\tlong_str = len(input_list)\n\t\tfor i in range(long_str-1):\n\t\t\tif len(input_list[i])==1:\n\t\t\t\ttry:\n\t\t\t\t\tint(input_list[i])\n\t\t\t\t\tinput_list.remove(input_list[i])\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\treturn input_list\n\texcept TypeError:\n\t\treturn []\n\n\n#print(strip_trailing_os_sep('C:\\\\Users\\\\admin\\\\Desktop\\\\'))\n\n#print(strip_trailing_"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tdef iter_strings(my_list):\n\t\tfor x in my_list:\n\t\t\tif len(str(x)) == 1:\n\t\t\t\tmy_list.remove(x)\n\t\t\telse:\n\t\t\t\ty = x.split(\"/\")\n\t\t\t\tmy_list.remove(y[-1])\n\t\treturn my_list\n\n\t# Input list with pathname seperators\n\tinput_list_of_pathnames = [pathname for pathname in input_list if os.path.isdir(pathname)]\n\t# Processed list of strings\n\toutput_"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tdef remove_os_seperator(s):\n\t\t# test if the last character of the list is the pathname seperator\n\t\tif len(s) and s[-1] == \"\\*\":\n\t\t\tos_seperator_index = len(s)\n\t\t\treturn s[:os_seperator_index]\n\t\telif len(s) >= 1:\n\t\t\t# if the last character is not the pathname seperator, make sure we remove it\n\t\t\tif s[-1]!= \"\\*\":\n\t\t\t\traise TypeError(\"Input list must contain"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\ttry:\n\t\t# Iterate over the input list\n\t\tfor i, value in enumerate(input_list):\n\t\t\ttry:\n\t\t\t\t# Remove the trailing seprators character\n\t\t\t\tif value.endswith(os.sep):\n\t\t\t\t\tinput_list[i] = value[:-1]\n\t\t\t# TypeError -> Invalid type\n\t\t\texcept TypeError:\n\t\t\t\traise TypeError(\"Cannot remove the trailing seprators character.\")\n\t\t# Return the processed list\n\t\treturn input_list\n\n\texcept TypeError:\n\t\t# TypeError -> Invalid type\n\t\traise Type"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\ttemp_list = []\n\tfor string in input_list:\n\t\tif len(string) > 1:\n\t\t\tif string[-1] == \"\\n\":\n\t\t\t\tstring = string[:-1]\n\t\t\tif string!= string[-1:]:\n\t\t\t\traise TypeError(\"Input is not valid\")\n\t\ttemp_list.append(string)\n\treturn temp_list\n\n\"\"\"\n\tIterate over a string list removing trailing os seperator characters. We do\n\tremoval where the last character is also a pathname separator character.\n\t\n\tArgs:\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef is_valid(pattern):\n\n\tif len(pattern)!= 7:\n\t\traise ValueError(\"You should give string in the format 'ABCD+' only(A+B+C+D+A+\")\n\t\n\t# find the group for the next letter in the string\t\n\tgroup = -1\n\tstart = 0\n\tend = 0\n\n\twhile end < len(pattern):\n\n\t\tif pattern[end] == \".\":\n\t\t\tend+=1\n\t\telif pattern[end] == \"$\":\n\t\t\tif end < 4:\n\t\t\t\tgroup ="}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tregex = r\"(?i)(^[sS]?)([^rR]+)$\"\n\n\tif (regex := re.match(regex=regex, string=phrase)):\n\t\tif r\"(\") == regex[0]:\n\t\t\treturn phrase\n\n\treturn None\n\n\ndef find_next(text, to_find):\n\t\"\"\"\n\tThis is the function to find next substring\n\t\"\"\"\n\tcurrent_index = text.index(to_find) + 1\n\twhile (text[current_index]!= \" \" and current_index <"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tp = re.compile('(.*?)(.*?).')\n\t\"\"\"\n\t\tCreate an re object for each string pattern\n\t\"\"\"\n\ttext = \"\"\n\treString = p.findall(text)\n\treturn [reString]\n\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef __init__(self, text):\n\t\tself.pattern = re.compile(\"[\\\\*](.*)\\Z\", re.S)\n\t\tself.text = text\n\t\"\"\"\n\t\tThis method is validating the given string provided\n\t\t\n\t\treturns True if passed string\n\t\"\"\"\n\tdef validate(self):\n\t\treturn bool(self.pattern.search(self.text))\n\t\"\"\"\n\t\tThis method returns regex pattern\n\t\"\"\"\n\tdef getPattern(self):\n\t\treturn self.pattern\n\t\"\"\"\n\t\tThis method returns text\n\t\"\"\"\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef __genRegexPattern(self, reString):\n\t\tregexPattern = re.compile(reString)\n\t\treturn regexPattern\n\tdef __genPatterns(self):\n\t\tkeywords = self.keyWordsLst[7]; specialKeywords = self.keyWordsLst[12];\n\t\tpatterns = []\n\t\tfor i in range(len(keywords)):\n\t\t\tpatterns.append(self.__genRegexPattern(keywords[i] +'' + specialKeywords[i]))\n\t\t# print(patterns)\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef stringToRegex(self,string):\n\t\tr\"(?P<type>\\w+):(\\w+):(?P<addr>[a-zA-Z]*)\"\n\t\tr\"(?P<num>:\\d{5,}):(?P<n>[0-1]?[0-9]:?[0-9]*)?\"\n\n\t\treturn re.compile(self.regex(string),re.IGNORECASE)\n\t\t# return re.compile(self.stringToRegex(string),re.IGNORECASE"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef __init__(self):\n\t\t# Pattern for the re module\n\t\tself.p = re.compile('[a-zA-Z_][a-zA-Z0-9_]*(-?\\.[0-9]+(?:\\.[0-9]+))?')\n\t\t# List of allowed values in the user's file\n\t\tself.simbols = ['=','-','/','*','/'&','#','|',':','.',',','?']\n\t\t# Pattern for the simbols in the pattern\n\t\tself.p"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef findRegex(self, text):\n\t\tif text is None:\n\t\t\treturn \"\"\n\t\t\n\t\tpattern = re.findall(r\"[a-z]{4}[0-9]+\", text)\n\t\t\n\t\ttry:\n\t\t\tpattern = [match.group() for match in re.finditer(r\"[a-z]{6}[a-z]{3}\", text)]\n\t\t\tpattern = []\n\t\t\tfor p in pattern:\n\t\t\t\tpattern.append(\"[a-z][a-z]{3}[a-z]{6"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef convert(self, value):\n\t\t\"\"\"\n\t\t\tconvert\n\t\t\tConverts the given regex pattern into a regex object\n\t\t\t\n\t\t\tArgs:\n\t\t\t\tvalue (string): the input value you wish to convert\n\t\t\t\n\t\t\tReturns:\n\t\t\t\tregex: The value converted to a compiled regex object\n\t\t\"\"\"\n\t\tif isinstance(value, str) == False:\n\t\t\treturn None\n\t\telif isinstance(value, str) == True:\n\t\t\ttry:\n\t\t\t\treturn re.compile(value)\n\t\t\texcept(UnicodeEncodeError, TypeError) as e"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef _convertFromString(self,pattern):\n\t\t\"\"\"\n\t\t\tThis method converts the given pattern from string to regex pattern\n\t\t\tparameters\n\t\t\t-----------------------------\n\t\t\tparameters\n\t\t\t\tpattern: string pattern\n\t\t\t-----------------------------\n\t\t\treturn\n\t\t\t\tconverted pattern\n\t\t\t-------\n\t\t\t\"\"\"\n\t\treturn re.compile(pattern)\n\t\"\"\"\n\t\t\tThis method convert the given string pattern to regex pattern\n\t\t\tparameters\n\t\t\t-----------------------------\n\t\t\tparameters\n\t\t\t\tpattern: string pattern\n\t\t\t-----------------------------\n\t\t\treturn\n\t\t\t\tregex pattern\n\t\t\t-------\n\t\t\t\"\"\"\n\tdef _"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\treturn subprocess.Popen(command, shell=True).wait()\n\n\ndef run_command(command):\n\t\"\"\"\n\t\tCall the given command(s).\n\t\tReturn the returned code.\n\t\"\"\"\n\treturn subprocess.Popen(command).wait()\n\n\ndef find_all_file(path):\n\t\"\"\"\n\t\tLocate all files.\n\t\t\"\"\"\n\tprint(\"\\n===================================================\\n\")\n\tprint(\"===      Searching...         ===\\n\")\n\tfiles = []\n\tif isinstance(path, (list, tuple)):"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\tstdout = subprocess.check_output(cmd, universal_newlines=True, stderr=subprocess.STDOUT)\n\t\tstdout = stdout.decode(\"utf-8\").strip(\"\\n\")\n\t\tif \"UNTRACKED\"!= stdout:\n\t\t\tstdout = False\n\t\telse:\n\t\t\tstdout = True\n\t\tif stdout:\n\t\t\tprint(stdout, file=sys.stderr)\n\t\treturn stdout\n\texcept subprocess.CalledProcessError as excp:\n\t\tprint(\"\\n####"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef call(self, arguments, shell=True, timeout=10):\n\t\tif''.join(str(arg) for arg in arguments):\n\t\t\tcommand = str(subprocess.Popen(['sh', '-c', self.sh_command % str(arguments)], \n\t\t\t\tstdout=subprocess.PIPE, shell=shell, stderr=subprocess.STDOUT))\n\t\telse:\t\n\t\t\tcommand = subprocess.Popen(['sh', '-c', self.bash_command % args, '-i'], \n\t\t\t\tstdout"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\nimport paho.mqtt.client as mqtt\nimport os, sys, random, time, datetime, threading, itertools, json, jsonpickle, collections\nimport uuid, re, string, time\n\nfrom pathlib import Path\nfrom pathlib import Path\n\nimport pyimgur\nfrom pyimgur import ImgurClient\n\nmy_imgur_client = PyImgurClient()\n\n\n#  I.Qt-based Pong\nclass UIActivity:\n    \"\"\"An instance of UIActivity.\n    \n    The UIAct"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t@staticmethod\n\tdef runFromScript():\n\t\t\"\"\"\n\t\t\tRuns a given script\n\t\t\"\"\"\n\t\tsubprocess.run(f\"python {path}/script/{sc.SCRIPT_NAME}.py {script_to_run}\")\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif sys.platform in [\"win32\", \"cygwin\"]:\n\t\t\n\t\tc = subprocess.Popen(['cmd', 'chkconfig', 'all:3'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n\t\to = c.communicate()[0]\n\t\t\n\t\tif 'Error' in o and o.split(\"Error:\", 1)[0].split()!= '' and len(o) > 1:\n\t\t\t\t\t\n\t\t\traise Exception"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t# Make sure we're not trying to use a proxy. This might happen if the proxy is running inside a docker machine.\n\t# If so, then make sure we run without the proxy (so that we don't break the current user due to a bug).\n\tif not use_proxy and subprocess.Popen([\"ping\", \"-c2\", str(proxies_cnt) + \"\"]).wait() in(0, 1):\n\t\tprocesses = []\n\t\twhile True:\n\t\t\tfor p in processes:\n\t\t\t\tif p.poll() is not None:\n\t\t\t\t\tbreak"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\tdef __call__(self, command):\n\t\t# We can access the process here because it's a tuple, without\n\t\t# using list() and []. Therefore, use pythonic syntax.\n\t\t\"\"\"\n\t\t\tCall the given command(s) and return a tuple, with the \n\t\t\tresult.\n\t\t\"\"\"\n\t\tproc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\tif proc.poll() == 0:\n\t\t\treturn (proc.stdout, proc.stderr)\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tproc = subprocess.Popen([command], shell=True) # shell=True  can specify the full path to the command\n\tproc.wait()\n\n\t\"\"\"\n\t\tReturn the exit code of our command and its result.\n    \t\"\"\"\n\treturn proc.returncode\n\n\n\n#\n#\tcall a shell script in a folder to be run as an external process\n#\n\n\n#\n#\trun a set of commands as an external process.\n#\ndef run_cmd2(script, folder_path=os.getcwd()):\n\t\"\"\"\n\t\tCall a"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\t# For some reason passing the output with no output\n\t# on stderr shows this as a different output\n\t# than expected and doesn't seem\n\t# to be what it should be?\n\t\n\tdef run(self,\n\t         args,\n\t         **kwargs):\n\t\ttry:\n\t\t\tp = subprocess.Popen(\n\t\t\t    ['python3', self.filename],\n\t\t\t    args=args,\n\t\t\t    stdout=subprocess.PIPE,\n\t\t\t    stderr=subprocess.STDOUT)\n\t\t    stdout,"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef test_ipv4_address(self):\n\t\tself.assertTrue(isIP(ipAddress(\"10.10.10.1\"), \"ipv4\"))\n\t\tself.assertFalse(isIP(ipAddress(\"9.9.9.9\"), \"ipv4\"))\n\t\tself.assertFalse(isIP(ipAddress(\"0.0.0.0\"), \"ipv4\"))\n\t\tself.assertFalse(isIP(ipAddress(\"192.168.10.1\"), \"ipv4\"))\n\t\twith self.assertRaises(TypeError) as"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tip = \"10.1.2.3\"\n\tresult = int(ipaddress.validate_ip(ip),16)\n\tassert(result == 255)\n\n\n\t\"\"\" Tests validate IPv4 address \"\"\"\n\tresult = str(ipaddress.validate_ip(\"10.1.2.3\"), 16)\n\tassert(str(result, \"utf-8\") == \"0x10.1.2.3\")\n\n\n\n\n\n\t\"\"\" Checks if an ip address is an IPv4\"\"\"\n\tip = \"255.255.255.255:\"\n\tresult ="}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t# Check v4 address\n\tv4 = ipaddress.ip_address('15.168.1.1')\n\tprint(\"v4 = \", v4.is_reserved)\n\tassert v4.is_reserved is False\n\n\t# Check v6 address\n\tv6 = ipaddress.ip_address('2001:db8::1')\n\tprint(\"v6 = \", v6.is_reserved)\n\tassert v6.is_reserved is False\n\n\ndef test_convert_ipv4_format():\n\t# Check v4 address"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tprint(ipaddress.ip_address(ip))\n\t\n\t\n\t\n\t\ndef test_a(socket):\n\tprint(socket.__class__)\n\tprint(socket.__module__)\n\t\n\t# print(type(socket))\n\t# print(type(socket.__class__))\n\tprint(socket.getpeername())\n\t\n\treturn None\n\t\ndef test_a2(socket, result):\n\tprint(\"test_a\")\n\treturn None\n\t\ndef test_b(socket, result):\n\tprint(\""}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tprint(ipaddress.IPv4Address(0x010101010110110000))\n\tIP = ipaddress.IPv4Address(0x010101010110110000)\n\t#print(str(IP))\n\tprint(\"IPv4 address\", IP.version)\n\tprint(ipaddress.IPv4Address(IP))\n\tprint(ipaddress.IPv4Address(0x0400410102223450033))\n\tprint(ipaddress.IPv4Address(0x007700000000))\n\n\"\"\"\n    IPV4"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tip_1 = str(ipaddress.IPv4Address(1))\n\tip_2 = str(ipaddress.IPv4Address(2))\n\tip_3 = str(ipaddress.IPv4Address(3))\n\n\tprint(\"> IPv4 test\")\n\tprint(\"input 1 : {}\".format(ip_1))\n\tprint(\"input 2 : {}\".format(ip_2))\n\tprint(\"input 3 : {}\".format(ip_3))\n\tprint(\"output 1 : {}\".format(False))\n\tprint(\"output 2 : {}\"."}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tip = \"\"\n\t# test for ipv4\n\tif (\"127.0.0.1\" in ipaddress.ip_network(\"192.168.1.0/24\").hosts()) == True :\n\t\tprint (\"IPV4 address\")\n\t\treturn True;\n\t\t# test for ipv4\n\t\tif (\"127.0.0.1\" in ipaddress.ip_network(\"192.168.1.0/24\").hosts()) == False :\n\t\t\tprint (\"NOT IPv4\")\n\t\t\treturn False;\n\t# test for ipv6\n\t"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\tdef test_single_address(self):\n\t\tfor ip in range(4):\n\t\t\ttry:\n\t\t\t\tself.assertEqual(ipaddress.ip_address(str(ip)), ipaddress.ip_address(ip))\n\t\t\texcept Exception as e:\n\t\t\t\tself.assertEqual({0/0,0/1}, set(ipaddress.ip_address(ip).real))\n\n\tdef test_single_address_failure(self):\n\t\tself.assertEqual({},ipaddress.ip_address(ipaddress.IPv4Address(0)))"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\tdef test_is_valid_ip4(self) :\n\t\t\"\"\"\n\t\t\"\"\"\n\t\ttest_data = \"\"\" \n\t\t\t2001 \n\t\t\t2001:bef0:51a5:3f7b:1cfbf:9acf\n\t\t\t2001         :bef0          :51a5::3      f7b    1cfbf    9acf\n\t\t\t2001:bef0:51a5:3f7b:1cfbf:9acf    -        f7b   1cfbf   9acf\n\t\t\t2001:bef"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef IPv4addrTest(self):\n\t\taddr1=ipaddress.ip_address('198.8.5.45')\n\t\taddr2=ipaddress.ip_address('202.0.2.255')\n\t\taddr3=ipaddress.ip_address('222.0.0.255')\n\t\taddr4=ipaddress.ip_address('200.0.0.8')\n\t\t\n\t\tfor x in addr1.natonb.split('.'):\n\t\t\tif (x[0]=='0' and x[1]=='0'):"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef find_roots(self,graph):\n\t\tif not isinstance(graph, list):\n\t\t\tgraph = graph.values()\n\t\troots = set()\n\t\tfor c in graph:\n\t\t\tif not isinstance(c,list):\n\t\t\t\tc = [c]\n\t\t\tfor p in c:\n\t\t\t\tif not (p in c):\n\t\t\t\t\troots.add(p)\n\t\treturn roots\n\t\n\t\n\tdef is_subclass(self,obj,clazz):\n\t\trdflib.RDFS.subClassOf.startswith\n\t\trd"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t# TODO:\n\t\n\t# 1. find the direct children of a super class of c.\n\t# 2. for each super class find the children of a super class.\n\t## TODO: find the children of a super class, with recursion\n\t## 2. find the children of a super class, with transitive dependencies.\n\"\"\"find_roots1(graph, rdflib.RDFS.subClassOf)\"\"\"\n\"\"\"find_roots1(graph, rdflib.RDFS.subClassOf, 1)\"\"\"\n\t## TODO:"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\nimport itertools\nimport rdflib\nimport rdcore.resource\n\n\nclass RootWalker(object):\n\tdef __init__(self, graph):\n\t\tself.graph = graph\n\t\tl = graph.subclassOf.walk()\n\t\t#print l\n\t\tfor c in l:\n\t\t\tif c.isAtomic():\n\t\t\t\tparent = c.target()\n\t\t\t\t\n\t\t\t\t#if there is a parent, we must recurse\n\t\t\t\tfound = set()\n\t\t\t\t\n\t\t\t\tfor cd in c.contributors().iter():\n\t\t\t\t\tfound.add("}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, predicate, **kwargs):\n\t\"\"\"\n\t\tReturns a set of all roots of the sub-class hierarchy.\n\t\t\n\t\tIf there are multiple roots of a single sub-class, use find_roots_2(...)\n\t\n\t\"\"\"\n\t\n\tif kwargs == {}:\n\t\traise Exception(\"You must specify a predicate on the graph.\")\n\tif len(predicate)!= 3:\n\t\traise Exception(\"A predicate on the graph has the wrong shape.\")\n\tif not graph.exists():\n\t\traise Exception(\"The subject"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tfrom rdflib.graph import DiGraph\t\n\tfrom rdflib import Namespace\n\tfrom rdflib.namespace import NamespaceManager\n\tNSMAP  = NamespaceManager()\n\tfrom graphml import graphml_serializer\n\tfrom graphml.tokens import Token\n\timport datetime\n\t\n\tNAMESPACE = Namespace(\"https://schema.org/\")\n\tNSMAP.bind(\"xsd\", \"http://www.w3.org/2001/XMLSchema\")\n\tNSMAP.bind(\""}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef _get_roots(self, g):\n\t\t# TODO: figure out how to calculate this\n\t\tif 'RDFS' in g.subjects():\n\t\t\ttry:\n\t\t\t\treturn g.subjects()[0].keys()\n\t\t\texcept IndexError:\n\t\t\t\treturn []\n\t\telse:\n\t\t\treturn []\n\n\t# @TODO\n\t# def find_siblings(self, relation, rdflib.Namespace, property):\n\t# \t\"\"\"\n\t# \tFind a list of Siblings in some kind of relation\n\t#"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\tdef find_roots(self, graph, prop_list):\n\t\tif prop_list[0][2] == 'RDFS.subClassOf':\n\t\t\tif self.node and \\\n\t\t\t\t((pred_name := self.pred[prop_list[0][3]]) and \\\n\t\t\t\t(self.node.hasPredicate(pred_name[4]))):\n\t\t\t\trooted_graph = graph.value(pred_name[4])\n\t\t\t\tif isinstance(rooted_graph, rdflib.URIRef):\n\t\t\t\t\trooted"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef __init__(self, graph):\n\t\t\"Initialize properties, relations, sets\"\n\t\t\n\t\tself.graph = graph\n\t\tsubClassOf = graph.namespace(\"skos\", \"2.0\")\n\t\tSKOS = graph.namespace(\"SKOS\", \"1.0\")\n\t\t\t\n\t\tself.s = SKOS.subPropertyOf\n\t\tself.o = SKOS.subClassOf\n\t\tself.c = SKOS.broader\n\t\tself.g = graph\n\t\t\n\t\tself.subClassOfRDFS = subClassOf"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, triple):\n\t\"\"\"\n\t\tFind the roots in some sort of transitive hierarchy.\n\n\t\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\t\tRDFS.subClassOf or SKOS.broader\n\t\t\"\"\"\n\n\t#print '----> find roots for RDF ', triple\n\tchild, prop, parent = triple\n\t#print '---> child  ', child\n\t#print '----> prop  ', prop\n\t\n\tif parent is not None:\n\t\tparent = graph.name"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tfrom collections import OrderedDict\n\t\n\tdef roots_for_subclass(graph, rdflib.RDFS.subClassOf):\n\t\tRDFS_child_prop = graph.namespace.new_variable(\"RDFS_child_prop\")\n\t\tgraph[RDFS_child_prop] = [rdflib.Literal(rdflib.IRI(\"http://rfc.net/rfc3986/rdfs/subClassOfClass\"))]\n\t\t\n\t\tgraph.verify(graph.validate(sub"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dumpToString(self, x):\n\t\tif x is None:\n\t\t\treturn u''\n\t\treturn yaml.dump(x).rstrip(u'\\n\\t')+u'\\n'\n\n\t\"\"\"\n\t\tDump a list of dict\n\t\t\n\t\tif `item` is a dict, they are ordered by the keys in the `item`.  Otherwise, they are ordered by items within\n\t\t`item`.\n\t\"\"\"\n\n\tdef dumpToDict(self, yaml_list):\n\t\tresult=dict()\n\t\tfor item in yaml_list"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump(self, obj, filename='', mode='w', *args, **kwargs):\n\t\tprint(filename, obj)\n\t\t\"\"\"\n\t\t# dump the object to a stream\n\t\t:arg obj: is the object to write\n\t\t:arg filename: is the name of file to which it shall be dumped\n\t\t:arg mode: is the mode in which it shall be stored\n\t\t:return str: the content of a file\n\t\t\"\"\"\n\t\t# convert a string(py2) to one\n\t\tif isinstance(obj, str):\n\t\t\tobj = obj.encode"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\n\tdef py2Str(data):\n\t\tif py3:\n\t\t\treturn str(data)\n\t\telse:\n\t\t\treturn data\n\n\t\"\"\"\n\tLoad a python yaml file\n\t\"\"\"\n\n\tdef fromYaml(fname):\n\t\twith open(fname, 'r') as stream:\n\t\t\ttry:\n\t\t\t\treturn yaml.load(stream, Loader)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint('ERROR: %s' % exc)\n\t\t\t\tprint('ERROR: Unable to convert from yaml file \"%s\"'"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\ts = f.read()\n\tif isinstance(s, str): #unicode data\n\t\tfor k, v in yaml.safe_load(s).items():\n\t\t\tif v is None or isinstance(v, unicode):\n\t\t\t\tprint \"  \", k, v, type(v)\n\telif isinstance(s, bytes):\n\t\t#if it's a byte-array\n\t\tfor k, v in yaml.safe_load(s).items():\n\t\t\tif v is None or isinstance(v, unicode):\n\t\t\t\tprint \"  \", k"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdumps = {}\n\tif isinstance(data, list):  # this is list\n\t\tfor entry in data:\n\t\t\tif (isinstance(entry, unicode)):  # if entry not unicode\n\t\t\t\tdumps[str(entry)] = entry  # is a string\n\t\t\telse:\n\t\t\t\tdumps[entry] = dumps.get(str(entry), entry)  # not a string\n\telif isinstance(data, dict):  # data is dictionary\n\t\tfor key, value in data.items():\n\t\t\tif isinstance(value, unicode):  "}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tp = ''\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dumps(self):\n\t\treturn self.yaml()\n\tdef dump(self, to):\n\t\treturn self.yaml(to) \n\n\t\"\"\"\n\t\tLoads a YAML file\n\t\t\"\"\"\n\tdef load(self, filename):\n\t\tloader = yaml.Loader\n\t\tparser = yaml.Parser\n\n\t\tstream = open(filename, 'r')\n\t\ttry:\n\t\t\tself.yaml(stream)\n\t\t\treturn True\n\t\texcept yaml.YAMLError as exc:\n\t\t\tprint(exc)\n\t\t\traise exceptions.InvalidY"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tpy2_unicode = True\n\"\"\"\n\tGet the type of an object\n\t\"\"\"\n\tpy3_string = True\n\t\"\"\"\n\t\tType of an object\n\t\t- str: String\n\t\t- int: Integer\n\t\t- tuple: Tuple\n\t\t- dict: Dictionary\n\t\t- bool: Boolean\n\t\t- complex: Complex\n\t\"\"\"\n\n\n\n\t# Create a new object\n\t# new is a reserved keyword, you can use it to create new objects\n\tdef __new__(cls, *args, new: str = ''):\n\t\tnew_instance = object"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef __init__( self, o, stream=\"stdout\" ):\n\t\tself.stream = stream\n\t\tself.out = open(o, mode=\"w\") if isinstance(o, str) else o\n\t\tself.out_stream = sys.stdout\n\n\tdef write( self, text ):\n\t\tself.out.write( text )\n\t\tsys.stdout.flush()\n\t\n\tdef writeln( self, text=None ):\n\t\tif text is not None:\n\t\t\tself.write( text )\n\t\tself.write(\"\\n\")\n\t\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tfrom functools import cmp_to_key\n\nimport os.path\n\nimport numpy as np\n\ndef dump(obj, fname, encoding='utf-8', line_numbers=False, sort_keys=True, **kw):\n    \"\"\" Dumps a Python object to a file with the given name and file-like\n        encoding. Object can be a dictionary, list, tuple, set, function,\n        which will be serialized as a dictionary.\n\n    Parameters\n        fname    : Path name of the file to write to\n        encoding : The"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t# set application logging level (0-9, 1 - 5)\n\t# NOTE: if you would like the levels to be displayed in the output, use the commented lines of the file application.py\n\tlogging.log(logging.INFO, \"\"\"Logging level configured to 'INFO' and 'ERROR' for the above script\n\t\t\t\t\t\tTo get more information you have to go back to the menu in the code editor, run:'''\n\t\t\t\t\t\t# /home/username/code_v2.3/application.py -l INFO\n\t\t\t\t\t\t# /home/username/code_v2."}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\ntry:\n\tfrom logging import handlers\n\tfrom logging.handlers import RotatingFileHandler\nexcept ImportError:\n\tfrom logging import StreamHandler\n\tfrom logging import FileHandler\n\tfrom logging import getLogger\n\tfrom logging import LoggerAdapter\n\nclass InterceptHandler(logging.Handler):\n\tdef emit(self,record):\n\t\t# Get corresponding Logger\n\t\tlogger=self.logger\n\t\t\n\t\trecord.levelname=logger.level(record.levelname)\n\n\t\t# Get message\n\t\tmessage=record.getMessage()\n\n\t\t#"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass Logger:\n\t\"\"\"\n\tGeneral purpose text log app\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\t# create logger\n\tif not os.getenv('LOGSTASH_LEVEL'):\n\t\tlogger.setLevel(logging.DEBUG)\n\t\tprint('Using LOGSTASH_LEVEL to set, no logging can be enabled...')\n\t\tlogger.debug('No logstash logging enabled, logging setup for logstash')\n\telse:\n\t\tlogger.setLevel(os.getenv('"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t# logging.getLogger(\"requests\").setLevel(logging.CRITICAL)\n\t# logging.getLogger().setLevel(logging.DEBUG)\n\t# logging.getLogger('requests_toolz.utils').setLevel(logging.NOTSET)\n\t# logging.getLogger('requests_toolz.cli').setLevel(logging.NOTSET)\n#   logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG, datefmt='[%d/"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass App: \n\tdef __init__(self,path):\n\t\t# get directory - file should exist\n\t\tfile_path = os.path.dirname(os.path.abspath(path)) \n \t\t# change directory - default -./log/\n \t\tfile_path = os.path.abspath(file_path) \n \t\tfile_path = os.path.join(file_path,'log')  \n \t\t# check if already setup\n \t\tif os.path.exists(file_path):\n\t\t\tmsg ="}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n#\tconstants\t\t\t/\t\n\n#\tlevels\t\t\t/\t\n\n#\tlogs\t\t\t/\t\n\n#\tlevel\t\t\t/\t\n\n#\tlogs_folder\t\t/\t\n\n#\tlogs_file\t\t/\t\n\n#\tdebug_logger\t\t/\t\n\n#\tinfo\t\t\t\t/\t\t\n\n#        for\t\n\n#\twarning\t\t\t/\t\t\n\n#\terror\t\t\t\t/\t\t\n\n#\tfatal\t/\t\t\n\n#\tcritical\n#\t\n\ndef get_root_logger(logger,filename=\""}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\ndef init_logger(filename='log.log'):\n    \"\"\"\n        Initialize logs and log handlers\n        Returns:\n            a new LoggingHandler object\n\t\tLoggers in this module only log to stderr\n    \"\"\"\n    # check where is called\n    try:\n        os.makedirs(os.path.dirname(filename))\n    except OSError as e:\n        if e.errno!= errno.EEXIST:\n            raise\n    # create handlers\n    logging.basicConfig(format='[%(as"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass AppLogger(object):\n\t\"\"\"\n\tUtility loggers for python\n\t\"\"\"\t\n\tdef __init__(self,output_directory):\n\t\tself.output_directory = output_directory\n\t\tself.logging = self.Logger(\"logs\")\n\n\t@staticmethod\n\tdef Logger(name,level=logging.DEBUG,date_format=\"%Y-%m-%d-%H-%M-%S.%f\"):\n\t\t\"\"\"\n\t\tlogging to file in specific directory\n\t\t\"\"\"\t\n\t\tlogger"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t#from flask_bootstrap import Bootstrap\n\t#from flask_wtf.csrf import CSRFProtect\n\t#from flask_bootstrap_components import Bootstrap\n\t#from flask_login import LoginManager\n\t#from flask_moment import Moment\n\t#from flask import Flask, request\n\n\t#bootstrap = Bootstrap()\n\t#login_manager = LoginManager()\n\t#login_manager.session_protection ='strong'\n\t\n\tapp = Flask(__name__)\n\t#app.config.from_object('config.Config')              "}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n### EXCEPTIONS \n\nclass PyLoggerError(Exception):\n\t\"\"\"\n\t\tBase Exception. A generic error during PyLogger configuration\n\tAttributes:\n\t\tmessage -- explanation of exception\n\t\terr_details -- list where list could store the details of\n\t\t\t\t\texceptions raised into errors dict\n\tReturns: None\n\t\"\"\"\n\tdef __init__(self,message):\n\t\tmsg = message +'-' + ''.join(traceback.format_exc().splitlines())\n\t\tself.err_details = {'message':msg, 'full_path':os"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\tdef __init__(self, shape, dtype):\n\t\tif type(shape) is not tuple:\n\t\t\terrmessage = \"`shape` input is not an element of a tup (array) \"\n\t\t\terrmessage += \"\\n\\tInput must be a tuple of integers\"\n\t\t\traise ValueError(errmessage)\n\t\telif len(shape) > 1:\n\t\t\terrmessage = \"`shape` input must be an \"\n\t\t\terrmessage += \"\\n\\tInput must be a tuple of integers\"\n\t\t\traise ValueError(errmessage)\n\t\tif type"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef empty(self,shape,dtype):\n\t\treturn np.empty(shape,dtype)\n\t\"\"\"\n\tFunction to create an array with shape and type.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\ttype : numpy.dtype\n\t    type of the array to create\n\t\"\"\"\n\tdef zeros(self,shape,dtype):\n\t\treturn np.zeros(shape,dtype)\n\t\"\"\"\n\tFunction to create an empty matrix of shape and dtype.\n\t\n\tParameters"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\tnp.zeros(1).astype(dtype=kwargs.get('dtype', np.int32))\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__(self, shape, dtype=None, order=\"C\"):\n\t\tsuper().__init__(np.empty(shape, dtype).data.view(self.__class__))\n\n\tdef _fill_from(self, arr, shape=None, dtype=None):\n\t\tif dtype is None:\n\t\t\tdtype = arr.dtype\n\t\tif dtype.kind == 'f':\n\t\t\tpass\n\t\telif dtype.kind == 'c':\n\t\t\tdtype = float\n\t\telse:\n\t\t\tdtype = dtype."}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__(self, shape, dtype=None):\n\t\tif dtype is None:\n\t\t\tself.dtype = np.dtype(shape)\n\t\telse:\n\t\t\tself.dtype = dtype\n\n\t\tif isinstance(shape, np.ndarray):\n\t\t\tshape = shape.shape\n\t\t\ttypecode = shape.dtype\n\t\t\tndim = len(shape)\n\n\t\telse:\n\t\t\tshape = (1,) * ndim\n\t\t\ttypecode = str(shape[0])\n\n\t\t\tndim = 1\n\t\t\tfor s in shape:\n\t\t\t\tnd"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__(self, shape, dtype):\n\t\tself.shape = shape\n\t\tself.dtype = dtype\n\t\"\"\"\n    \tfunction to create an array with size \n\t\tspecified by the size argument.\n\t\"\"\"\n\tdef create_with_array(self, data):\n\t\tself.data[:] = data\n\t\"\"\"\n\tfunction to create an array with elements of the \n\t\tspecified dtype.\n\t\"\"\"\n\tdef create_with_data(self, data):\n\t\tself.data[:] = data\n\t\"\"\"\n\tfunction to"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\ndef generate_random_array(shape, dtype=np.float64):\n\t\"\"\"\n\tFunction that returns an array of random data with specified shape and data type.\n\n\tParameters\n\t----------\n\tshape : `tuple`\n\t    dimensions of array to create\n\tdtype : `numpy.dtype`\n\t    data-type of array to create\n\n\tReturns\n\t-------\n\tarray with data in the format specified above\n\t\"\"\"\n\tassert len(shape) == 2, \"Invalid array shape. Shape should be square matrix.\"\n\tshape = tuple"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__( self, shape, dtype):\n\t\t\"\"\"\n\t\tParameters\n\t-----------\n\n\t\tshape : tuple\n\t\t\tshape of the array to create\n\t\tdtype : `numpy.dtype`\n\t\t\tdata-type of the array to create\n\t\t\"\"\"\n\t\tself.shape = shape\n\t\tself.dtype = dtype\n\n\tdef return_array( self ):\n\t\t\"\"\"\n\t\t\treturns an array of type `self.dtype` with shape `self.shape`\n\t\t\"\"\"\n\t\treturn np.zeros( self.shape, dtype=self."}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__(self,shape,dtype):\n\t\tself._shape = shape\n\t\tself._dtype = dtype\n\t\tself._arr = np.empty(shape,dtype=dtype)\n\t\tself._buffer = {self.__len__ : 0}\n\t\tself._ptr = 0\n\t\tself._n_elements = 0\n\n\t@staticmethod\n\tdef from_arr(arr,fill_value =None):\n\t\t\"\"\"\n\t\t\tCreate a new `Dataclass` object from an `numpy.ndarray`.\n\t\t\n\t\t\tParameters\n\t\t\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\tdef __new__ (cls, *args, **kwargs):\n\t\treturn np.zeros(shape=args[0], dtype=args[1])\n\t\n\tdef __init__ (self, *args, **kwargs):\n\t\tpass\n\n\"\"\"\n\tFunction to assign data to an array.\n\t\n\tParameters\n\t----------\n\tobj : `array`\n    data of the data to be assigned to the values\n\tdata_dtype : `numpy.dtype`\n    data-type that is to be assigned\n\t\"\"\"\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\ndef getCentroids(k):\n\t\"\"\"\n\t\tReturns the centroids of cluster k in a numpy array  \n\t\"\"\"\n\tcentroid_list = np.zeros( (k,2), dtype=np.float32) \n\tfor i in range(k):\n\t\tcentroid_list[i,: ] = np.where(k==i)[0]\n\tcentroids = np.concatenate( centroid_list, axis=1) # (k,num_features,2)\n\tnp.random.seed("}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tfrom numpy.core.fromnumeric import argsort\n\tfrom numpy.lib.function_base import append\n\tfrom numpy.random import uniform\n\timport numpy as np\n\tfrom numpy import linalg as LA\n\n\tfrom.gaussian_centered import gaussian_centered\ndef laplacian(x, order=1):\n\t'''\n\tReturns the (order+1) dimensional Laplacian graph\n\t:param x: (N, 3)\n\t\t\tNode features\n\t\tN features, where 1 is the node and 0 is the edges\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef gaussian(x, mean=0, cov=1, sigma=0.1):\n\t# x = np.abs(l) + np.abs(f) + np.abs(s) + np.abs(l+f+s) + np.abs(l+f-s)\n\t\treturn np.exp(-(x-mean)**2/2/cov**2)\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom datetime import datetime\nimport p"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n# Import dataset.\ntrain_data = np.load('../data/train_data_dilations_20.npy')\nnp.random.seed(2)\ntrain_labels = np.load('../data/train_label_dilations_20.npy')\ntest_data = np.load('../data/test_data.npy')\ntest_labels = np.load('../data/test_label.npy')\nprint('train_data.shape: ', train_data.shape)\nprint('train_labels."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef gaussian_smooth(X, sigma, epsilon):\n\t\t# print 'X:\\n'\n\t\t# print X.shape\n\t\t# print '\\n'\n\t\tX = (X - np.mean(X)) / np.std(X, ddof=1) * sigma + epsilon\n\t\t# print 'X:\\n'\n\t\t# print X.shape\n\t\t# print '\\n'\n\t\treturn X\n\n\tdef gaussian_centered(X_train, epsilon):\n\t\t# print 'X_train:\\n'\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nclass Gaussian(object):\n\t\"\"\"\n\t\tA gaussian function in [0, 1], with sigma = 0.01 and\n\t\tx, y, z = 0 and no derivative, for dt = 0 and t0 = 0.\n\t\"\"\"\n\tdef __init__(self, sigma, x, y, z):\n\t\tself.sigma = sigma\n\t\tself._x, self._y, self._z ="}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef center(self,data):\n\t\tmean, var = np.mean(data), np.var(data)\n\t\tx, y = np.arange(len(data))\n\t\tx = np.array(deltax * len(data)) / sum(np.shape(data)) or deltax\n\t\ty = np.array(deltay * len(data)) / sum(np.shape(data)) or deltay\n\t\tcenters = [mean - 0.2 * var ** 0.5 * np.sqrt(x), mean + 0.2"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nclass SIR:\n\tdef __init__(self, a1, a2, d, gamma=0.15, eta=0, delta=0.1):\n\t\tself.a = a1\n\t\tself.a2 = a2\n\t\tself.beta = round(eta/d, 4)\n\t\tself.eps0 = round(gamma/(d*(np.sqrt(2))), 4)\n\t\t# self.eta = eta\n\t\t# self.delta = delta\n\t\tself.g = gamma      # coefficient of change in prob\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t#sigma = np.random.rand()\n\t#sigma0 = np.random.rand()\n\t#mu0 = 100 * -1\n\t#r = np.sqrt(1/2/sigma0)\n\t#r0 = np.sqrt(1/2/sigma0)\n\t#print((mu0 - mu * r0)/2)\n\t#print((2*mu - mu0)*r)\n\n\t#mu0 = np.random.normal(mu0, np.sqrt(sigma))\n\t#r0 ="}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t# https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/fixes.py\n\tdef geometric_mean(array):\n\t\treturn (array ** (1. / len(array))).mean()\n\t\n\t\"\"\"\n\tThe gaussian (as per paper) is given by:\n\t\tN(\u03bc_0,\u03c3_0*R) = [log\u03c3**2/(2*\u03c3_0)].\n\t\t\n\t\t\u03c3_\u2225= (c**2 + N(c+mu_0,R) - \ufffd"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\tlog_handlers_count = 0\n\tfailed_logging_handlers_index_count = 0\n\n\t# Load the data into an array\n\tdef load(conf_map: dict):\n\t\tconf_data = []\n\t\tlogging.info('Loading data...')\n\t\tfor log_data_file in conf_map.get('log_data_files', []):\n\t\t\tlog_data_file = path.expanduser(log_data_file)\n\t\t\twith open(log_data_file) as f:\n\t\t\t\tlog_data = yaml"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n__author__\t\t= \"Thomas Sattemm <ssattemm@gmail.com>\"\n__copyright__\t= \"Copyright (C) 2010-2012 Thomas Sattemm\"\n__credits__\t= [\"Thomas Sattemm (THS)]\n__license__\t= \"\"\n__version__\t= \"0.0.1\"\n__email__\t\t= \"thomas.sattemm@gmail.com\"\n\nimport configparser\nimport glob\nimport logging\nimport os\nimport os.path\nimport time\n\nfrom pyutils.csv"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t#TODO: add configs.get()\n\n\tfrom optparse import OptionParser\n\n\tparser = OptionParser()\n\n\tparser.add_option(\"-q\", \"--output-qfile\", type=\"string\", help=\"Output qfile\", dest=\"outputQFile\")\n\tparser.add_option(\"-L\", \"--log-level\", choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],\n\t\t\t\t\tdest=\"logLevel\", default=\"ERROR\", help=\"The desired logging level. Default setting: ERROR\")\n\tparser.add_option(\""}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\timport json\n\timport os\n\timport sys\n\tfrom collections import OrderedDict\n\tfrom importlib.util import find_spec\n\tfrom warnings import warn\n\nfrom.configs import Config\nfrom.configs import loadDefaultConfigFile\n\n\n# This list contains the possible names of log formats\nLOG_FORMATS = {\n\t'json':    ['log', 'txt', 'json'],\n\t'plain':   ['log', 'txt'],\n\t'wtt':     ['log', 'txt'],\n\t'stderr':  ['log', '"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t#\n    \t#\n        \n    \t#\n        #\n    \n\nCONFIGURATIONS = {\n    #\n        'configuration_example.ini': {\n            'NAME': 'example_config.ini',\n            'DEFAULT_VALUE': '1',\n            'FORMAT': '%(message)s',\n            'description': 'Configuration file example config.',\n            'INPUT': 'example.ini',\n            'PRINT_ONLY': 'false',\n            'OUTPUT': 'example_output.txt'\n        },\n    "}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\nimport os.path as osp\n\nimport pandas as pd\nimport numpy as np\nfrom itertools import groupby\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n#torch.config.set_random_seed(0)\n\nimport re\nimport logging\nimport argparse\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\n\nfrom dllib.utils"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\timport os\n\tfrom os import path\n\tfrom shutil import copyfile\n\tfrom logging.handlers import TimedRotatingFileHandler\n\tfrom time import sleep\n\tfrom pathlib import Path\n\n\timport pandas as pd\n\tfrom ast import literal_eval as parse_input2\n\timport numpy as np\n\timport copy\n\timport functools\n\n\ttry:\n\t\timport pandas as pd\n\texcept ModuleNotFoundError:\n\t\tlogging.error(\"Pandas was not found because it's required.\")    # TODO find better way to setup a"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\timport os\n\timport re\n\timport sys\n\n\t# Load configuration file and validate it\n\ttry:\n\t\tconfs = OrderedDict()\n\t\tfor config_file in os.listdir(os.path.join(os.getcwd(), \"data/configs\")):\n\t\t\tif not re.match(\"^[^.]+(?![^.]).*$\", config_file):\n\t\t\t\tcontinue\n\t\t\t(config_file, version, line_no, function) = config_file.split(\".\")\n\t\t\tconfs[config_file] = config_file"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\timport argparse\n\timport collections\n\timport os\n\timport os.path as osp\n\timport time\n\tfrom tqdm import tqdm\n\tfrom typing import Dict, List\n\nfrom baselines.common.logger import Logger\n\n\nclass ConfigValidator(object):\n\tdef __init__(self, args: argparse.Namespace, env_name: str):\n\t\t\"\"\"\n\t\t\tInitialise a ConfigValidator object with the given arguments.\n\n\t\t\"\"\"\n\t\tself.args = args\n\t\tself.logger = Logger(*args, env"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\nclass ConfigFileValidator(object):\n\t\"\"\"\n\tThis class takes no arguments. When a configuration file is valid, a key with its associated\n\tnamespace is returned. It will be always valid for configuration files to work with this\n\tclasses.\n\n\tThis will be extended in future versions.\n\t\"\"\"\n\n\t### Configuration fields\n\tnamespace = None\n\t# TODO: add more fields\n\tconfig_files = dict()\n\t###\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tThe constructor for ConfigFileValidator.\n\t\tIf self."}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif type(obj) == bytes:\n\t\tret = obj\n\telif type(obj)==str:\n\t\tret = obj.decoded('utf-8')\n\telif callable(obj):\n\t\tret = obj(obj)\n\telif type(obj) == type and hasattr(obj, '__getitem__') and isinstance(obj[0], bytes):\n\t\tret = numpy.array(obj).tostring()\n\treturn ret\n\ndef getBounds(start,stop,N=1,step=1):\n\tif N"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t# If the given object is already a bytes object, just return it.\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\n\t# Ensure the object is not an object that cannot be returned.\n\telif not hasattr(obj, '__bounds__'):\n\t\t# Try to extract a byte-array object.\n\t\ttry:\n\t\t\t# Create a bytearray object that will hold the object.\n\t\t\t# We set the length of the returned array by the string size.\n\t\t\tbobj = bytearray(str(obj).encode('utf-"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn str.encode(obj)\n\traise TypeError(\"Expected bytes or unicode\")\n\nif __name__ == \"__main__\":\n\timport itertools\n\timport sys\n\tsys.argv += ['--file','mydata.csv']\n\n\t# import this module\n\tfrom data.convert_numpy import convert_numpy\n\tfrom data.convert_scipy import convert_"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\ttry:\n\t\treturn bytearray(to_binary(obj))\n\texcept:\n\t\treturn obj\n\n@n.functools.cache\ndef to_bytes(obj):\n\tif type(obj)!= bytes:\n\t\treturn to_bytes(obj.encode('utf-8'))\n\telse:\n\t\treturn to_bytes(obj)\n\ndef decode_obj(obj):\n\tobjstr = bytearray.fromhex(to_binary(obj))\n\tbytes = b\"\"\n\tfor i,b in enumerate(objstr"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tbyte_iter = iter(obj, \"utf8\") if type(obj)!= bytes else (iter(obj, \"utf-8\"),)\n\treturn b''.join(byte_iter)\n\n\nclass FileType:\n\t@staticmethod\n\tdef open(fq, b_obj):\n\t\t# Load the object to bytes\n\t\tbyte_obj = fio.load(fq, b_obj)\n\t\tif byte_obj.shape == (\"\",):\n\t\t\tbyte_obj = b_obj.encode(\"utf-8\")\n\t\telse:\n\t\t\tbyte_"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tdef b_(obj) :\n\t\tif type(obj) is bytes : return obj\n\t\tif type(obj) is str : return obj.encode(UTF8)\n\t\tif type(obj) is unicode : return obj.encode(UTF8)\n\t\tif type(obj) is bytearray : return obj\n\t\tif type(obj) is dict : return dict((k.encode(UTF8), b_(v)) for k, v in obj.items())\n\t\tif type(obj) is tuple : return tuple((b_(x) for x in obj))"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn bytes.fromhex(obj)\n\telse:\n\t\treturn obj\n\ndef __to_str(bytes_obj):\n\treturn bytes_obj.hex()\n\ndef __to_hex(bytes_obj):\n\t\"\"\"\n\tFunction to convert any Bytes object to string\n\t\"\"\"\n\ttry:\n\t\treturn binascii.hexlify(bytes_obj).decode('utf-8')\n\texcept Exception as ex:\n\t\traise ValueError('Cannot convert %s - %s' % (bytes"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, bytes): return obj\n\tif isinstance(obj, bytearray): return bytes(obj) # this isn't really unicode, we just need it to be\n\tif isinstance(obj, int):\n\t\treturn bytes([obj])\n\tif isinstance(obj, str):\n\t\treturn bytes([ord(obj[0])])\n\traise TypeError\n\n# https://docs.python.org/3/reference/datamodel.html?highlight=mempty#dict\nclass Mempty:\n\tdef __init__(self"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif type(obj) == str:\n\t\treturn obj\n\telif type(obj) == int or type(obj) == float:\n\t\treturn memoryview(obj).tobytes()\n\telse:\n\t\tobj.encode('utf-8')\n\t\treturn obj._asStr_()\n\t\ndef u(string=None): pass\ndef _zfill(d, places=None):\n\tif places is None:\n\t\tplaces = len(str(d))\n\treturn str(d).zfill(places)\n\ndef add_s_pads("}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif not isinstance(obj, bytes):\n\t\treturn object.__str__(obj).encode(\"utf-8\")\n\telse:\n\t\treturn obj\ndef list_to_bytes(l, encoding=\"utf-8\"):\n    \"\"\"\n\t\t\tConverts the ``l`` list into ``bytes`` object to send\n\t\t\tto BLE device.\n\t\t\t:arg unicode``l``: List ``l`` to be converted to ``bytes``.\n\t\t\t:arg ``unicode````encoding: encoding type of ``l``.\n\t\t\t:returns: Converted list ``"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\tif ticks < 0 or ticks > 86400000000000:\n\t\traise ValueError('ValueError: ticks must be within [0,8000000000000000)')\n\t\n\tif ticks < 4867599:\n\t\t# Time is relative\n\t\ttzinfo = pytz.timezone('UTC')\n\telse:\n\t\ttzinfo = pytz.timezone(tz)\n\t\t\n\treturn Time(ticks, tzinfo=tzinfo)\n\t\n\t\nclass TimeDelta(pytz.datetime):\n\t\"\"\"\n\tThe naive (no tz) dat"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t# check the ticks\n\ttry:\n\t\tcls(dt=ticks/1e9, tz_plus=tz)\n\texcept ValueError as e:\n\t\t# not out of bounds and time_tz is not supported yet\n\t\traise ValueError(e)\n\n\treturn cls(dt=ticks, tz_plus=tz)\n\n\nclass TimeMixedNaBounds(ValueError):\n\t\"\"\"\n\t\tTime MIX_TZ: cannot be converted to time object\n\t\t\n\t\tValueError: if bounds are different\n\t\"\"\"\n\tdef __init"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be positive\")\n\t\n\t# determine whether ticks are given microseconds\n\tif tz is None:\n\t\t# get the smallest integer microsecond from ticks\n\t\tmillisec = divmod(ticks, 1.0E6)[0]\n\t\tif millisec >= 8640000000000:\n\t\t\t# create a timezone using tz.fromtimestamp\n\t\t\tmillisec -= 8640000000000\n\t\t\t# get the timezone from the timezone variable\n\t\t\ttz = tz.tzname(datetime.datetime."}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(time.to_int(ticks), tz)\n\nclass TimeDelta(dt.datetime):\n\t\"\"\"\n\tA pseudo-base class for time delta objects.\n\t\n\t:py:class:`datetime.timedelta` has the attributes 'total_seconds'\n\tand 'delta' which are defined by the datetime class.\n\t\n\t:Parameters:\n\t\t- hours : (optional)\n\t\t\tThe hours will be converted to minutes by this method.\n\t\"\"\"\n\n\tdef __add__(self, other):\n\t\t\"\"\""}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif _ticks_type.get(ticks) == _ticks_from_nanoseconds:\n\t\treturn Time.nanoseconds_from_timestamp(ticks)\n\t\n\tfrom_mhz = ticks // (86400 * 1.0E6) // 100\n\tticks_per_hr = ticks // (60 * 1.0E6)\n\tticks_per_sec = ticks // 60.0 // 1.0E6\n\t\n\thours = (ticks // 100000) % 24\n\treturn Time(from_m"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tticks = ticks % 86400000000000\n    if ticks < 0:\n\t\traise ValueError(\"Invalid number of ticks: %d\" % ticks)\n     \n    # This assumes local midnight.\n    tz_deltas = (4, -1)\n     \n    dt_tzi = dt_fromtimestamp(ticks)\n    tz_offset = tz_deltas[tz_deltas.index(tz_deltas[0] > 0)]\n\n    if tz_deltas[0] == -1:\n        return"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn Time(ticks=(get(ticks, float(ticks)) / float(1e6)))\n\nclass Timestamp:\n\tpass\n\nclass Datetime(Timestamp):\n\tpass\n\nclass TimeStamp:\n\tdef __init__(self, value):\n\t\tself.time = value\n\n\tdef __repr__(self):\n\t\treturn datetime.utcfromtimestamp(self.time).isoformat()\n\n\tdef __format__(self, format=None):\n\t\tif not isinstance(format, str):\n\t\t\traise TypeError(\""}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not ticks >= 0 and ticks < 20200702000000:\t\t# allow a 1-second buffer of 2 ns ticks in case the server's bug\n\t\traise ValueError('ticks is out of bounds')\n\tif ticks >= 864000000000 and tz is None:\n\t\tticks = tz.mktime(tz.utctimetuple() + (0,0,0,1,0,0,0,0,0,0,0,0,0,2,0))\n\treturn Time(ticks, tz)\n\ndef to_datetime"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be greater than 0\")\n\treturn cls(dt_from_ticks(ticks, tz=tz), tz=tz)\n\ndef dt_trunc(dt, tz):\n\t\"\"\"\n\tReturn the time value corresponding to ticks since midnight\n\t:param dt: time\n\t:type dt: Time\n\t:param tz: timezone (optional)\n\t:type tz: datetime.tzinfo\n\t:rtype: int or float\n\t\"\"\"\n\treturn"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError('cannot convert ticks to timezones')\n\treturn Time(ticks, tz=tz)\n\t\n\t# return from seconds to milliseconds since midnight\n\t# since midnight is the midnight the year and the day now\n\t# we could just use ticks as the ticks since midnight which would cause\n\t# this to be far in the future, but its already in ms...\n\t# this is also the point of note, since we have now the exact point that\n\t# will be needed\n\tdef to_seconds(ticks):\n"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_version = protocol_version or ('', '', '')\n\treturn cls._protocol_handlers(protocol_version)\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t# If no handler class was passed in (empty), or if no protocol version\n\t# is passed then return an empty dictionary\n\tif not cls or not protocol_version:\n\t\treturn {}\n\t# If protocol version is a tuple, return the corresponding class\n\telif isinstance(protocol_version, tuple):\n\t\treturn dict([(version, cls)\n\t\t\tfor version in protocol_version])\n\t# If protocol\n\telse:\n\t\tprotocol_version = ProtocolVersion.from_string(protocol_version)\n\t\tsupported = []\n\t\tfor handler_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = {}\n\n\tif protocol_version is None:\n\t\tfor protocol_version in version_dictionary.keys():\n\t\t\thandler = version_dictionary[protocol_version]()\n\t\t\thandlers[protocol_version] = handler\n\t\t\t#\n\t\t\t#  Add protocols below as well as protocols of specific\n\t\t\t#  versions as we will do the right thing if an\n\t\t\t#  existing protocol is not supported at any of the\n\t\t\t#  versions.\n\t\t\t#\n\t\t\tfor additional_protocol in protocol_support:\n\t\t\t\t# if this is the default protocol and"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn list(Bolt.supported_protocol_versions.keys())\n\tmapper = [frozenset(map(int, v)) for v in Bolt.supported_protocol_versions.keys()]\n\n\treturn {v: mapper[i](protocol_version) for\n\t\t\ti, v in enumerate(mapper)}\n\n\nclass BoltDriver:\n\t\"\"\"\n\tAbstract driver used to start Bolt sessions. This class is used to\n\tcreate and setup the Bolt driver instance at runtime, the driver\n\tcan create sessions"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tretdict = {}\n\tif protocol_version:\n\t\t# support all existing protocols\n\t\tfor (version, handler_cls) in cls.protocol_versions:\n\t\t\ttry:\n\t\t\t\tversion = int(version)\t\t# allow string use\n\t\t\t\tif (version > 0 and version not in version_map):\n\t\t\t\t\traise ValueError\n\t\t\texcept ValueError:\n\t\t\t\t# unsupported version in use, allow None to mean all\n\t\t\t\tpass\n\t\t\tretdict[version] = handler_cls\n\t\t# no handler_configured\n\t\traise Exception(\"Protocol handlers not configured."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t# If the protocol version has been explicitly provided, try and\n\t# determine the corresponding protocol handler class for the\n\t# protocol version:\n\tif protocol_version is not None:\n\t\thandler_factory_map = cls.protocol_handler_factory_map(protocol_version)\n\t\tif handler_factory_map is None:\n\t\t\traise AttributeError('Missing protocol version handler factory map: %s' % str(sys.exc_info()))\n\t\treturn handler_factory_map\n\t# If the protocol version is not explicitly provided, fallback"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\ttry:\n\t\tprotocol_handlers = cls.protocol_handlers\n\texcept AttributeError:\n\t\tprotocol_handlers = cls.protocols_handlers\n\tif not isinstance(protocol_version, tuple):\n\t\tprotocol_version = (protocol_version, )\n\tversion_map = {}\n\tfor protocol_class in (tuple, frozenset):\n\t\tfor version, handler in protocol_class(protocol_version).items():\n\t\t\ttry:\n\t\t\t\tversion_map[version] = handler\n\t\t\texcept Type"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls.SUPPORTED_VERSIONS\n\telif isinstance(protocol_version, (types.TupleType, types.ListType)):\n\t\tfor version in protocol_version:\n\t\t\tif version not in cls.SUPPORTED_VERSIONS:\n\t\t\t\traise ValueError(\n\t\t\t\t\"Invalid protocol version %s. \"\n\t\t\t\t\"Please pass a tuple to protocol_handlers to \"\n\t\t\t\t\"avoid conflict between versions '%s' and '%s'\",\n\t\t\t\tversion, cls.SUPPORTED_VERS"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif not isinstance(protocol_version, tuple):\n\t\tprotocol_version = (protocol_version, 0)\n\tprotocol_handler_dictionary = dict()\n\tif protocol_version!= (0, 0):\n\t    protocol_handler_dictionary[((3, 5))] = cls\n\t    protocol_handler_dictionary[(3, 5)] = cls\n\t    protocol_handler_dictionary[((4, 0))] = cls\n\t    protocol_handler_dictionary[((4, 1))] ="}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn {}\n\telse:\n\t\tprotocol_version = _check_version(protocol_version)\n\t\tprotocol_handlers = get_protocol_handlers(cls.b3_protocol_handlers,\n\t\t\t\t\t\t\t\t\t\t\tprotocol_version)\n\t\treturn protocol_handlers\n\t\n\t\nb3_protocol_handlers = {}\n@deprecated(\"Use add_protocol_handler(handler_cls)\"\n\t\t\" with the add_handlers argument instead. This function can be \"\n\t\t"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tpass\n\n\t@classmethod\n\tdef from_raw_bookmark(cls, value):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a raw bookmark string value.\n\t\t\n\t\t:param value: ASCII string value (raw bookmark string)\n\t\t:type value: str\n\t\t:rtype: cls\n\t\t\"\"\"\n\t\treturn cls()\n\n\t@classmethod\n\tdef from_json_bookmark(cls, value, type):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a json.bookmark string value.\n\t\t\n\t\t:param value: JSON"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls(values)\n\n\ndef to_raw_bookmarks(class_name, bookmark):\n\t\"\"\"\n\t\tConvert a Bookmark to a list of raw key-value pairs.\n\t\n\t\t>>> to_raw_bookmarks('Bookmarks', '\"barcode:%04X\"' % (12321)  )\n\t\t[('barcode', 12321)]\n\t\n\t\tYou should not use this method unless you want to deserialize bookmarks.\n\t\n\t\t>>> to_raw_bookmarks( 'Bookmarks'   , '{\"barcode"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn Bookmarks(values)\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls()\n\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif not values:\n\t\t# If an empty string is passed in the arguments, and the object is not\n\t\t# created, we return None\n\t\treturn None\n\t\n\tbookmarks = cls(\n\t\tidx,\n\t\tbookmark,\n\t\tauthor,\n\t\tcontent,\n\t\tauthor_url,\n\t\tauthor_web_url,\n\t\tdate,\n\t\ttags,\n\t\treferring_ids,\n\t\ttags_per_entry,\n\t\ttitle,\n\t\turl,\n\t\tauthor_tagable,\n\t)\n\tfor index, raw in enumerate("}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tres = cls.__new__(cls)\n\tres.raw_bookmarks = values\n\treturn res\n\ndef to_raw_values(cls, values: Iterable[str]) -> Iterable[str]:\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tyield from values is not None and values or [None]\n\tif values is not None:\n\t\tyield from"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn Bookmarks(cls.value_map.from_raw_values(values))\n\n\nclass Bookmarks(object):\n\t\"\"\"\n\tA list of bookmarks parsed from a string values in format\n\t[ \"path/to/file1\", \"path/to/file2\",...  ]\n\t\n\t\"\"\"\n\tdef __init__(self, raw_values):\t\n\t\t\"\"\"\n\t\tConstructor \n\t\t:param raw_values: ascii string values (list of paths to file)\n\t\t:type values: Iterable[str]\n\t\t\"\"\"\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn Bookmarks.from_raw_values(values)\n\n\ndef from_bib_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of bib strings.\n\t\n\t:param values: ASCII strings and whitespace separated\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn Bookmarks.from_bib_values(values)\n\n\nclass DocumentData(dict):\n\t\"\"\"\n\tA mapping object from a string value, e.g., from a file descriptor returned\n\t(e.g., a device file"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tpass\n\n\nclass Bookmarks(dict):\n\t\"\"\"A mapping of bookmark names to (list of) values.\"\"\"\n\n\t# pylint: disable=abstract-method\n\t@abstractmethod\n\tdef from_raw_values(cls, values):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a list of raw bookmarks values (including bookmarks).\n\n\t\t:param values: Iterable[str]\n\t\t:type values: Iterable[str]\n\t\t\"\"\"\n\t\tpass\n\n\t# pylint: disable=abstract-method\n\t@abstractmethod"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tbooks = []\n\tfor raw_value in values:\n\t\tbooks.append(Bookmark(raw_value))\n\treturn cls(books)\n"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = list(seq)\n\tif not bufsize:\n\t\tbufsize = None\n\n\tif bufsize == 0:\n\t\tbufsize = cmp(*seq).__lt__(0)\n\telif bufsize == len(seq):\n\t\tbufsize -= 1\n\telif bufsize == None:\n\t\tbufsize = (sum(map(len, seq)) + 1) // 2\n\n\tif cmp(len(seq), 0) < 0:\n\t\tseq.reverse()\n\t\tbufsize -= 1\n\n\tif not seq:\n\t\traise IndexError(\"end"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif type(seq) not in [list, tuple]:\n\t\tseq = list(seq)\n\t# For each element of a sequence it is either\n\t#     1. *seq*, whose element is an Element\n\t#     2. stringifying *seq*, or its string repr.\n\tseq_with_type = tuple(map(lambda x: x.seq, seq))\n\n\tif len(seq):\n\t\t# If that sequence has children, just treat the first\n\t\t# element of the first element as the type.\n\t\ttype_ = seq[0].first\n\t\tseq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif type(seq) in (tuple, list):\n\t\treturn seq\n\t# Else if it's a string, assume that it's a path\n\timport path\n\tif isinstance(seq, str):\n\t\tbuf = seq.encode('utf_8')\n\t\tstream = _StringIO(buf)\n\telse:\n\t\t# If *seq* is a file handle, try to guess its type.\n\t\tif hasattr(seq,'read') and hasattr(seq, 'fileno'):\n\t\t\ttry:\n\t\t\t\t# Make it possibel that a file is"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, basestring) and not isinstance(seq, sequence):\n\t\tseq = seq.split()\n\tif len(seq) < 1:\n\t\traise Exception('Need a sequence: string or sequence of strings')\n\tif len(seq) > 2:\n\t\traise Exception('Bad sequence: %r > 2' % seq)\n\ttry:\n\t\tif type(seq[0]) not in [basestring, int, type(None)]: raise Exception('Need a string')\n\texcept:\n\t\traise Exception('Bad sequence: %r' %"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\treturn (tuple(seq), type(seq[0]))\n\telse:\n\t\tresult, consumed = consumes(bufsize)\n\t\tresult = (tuple(seq), result, consumed)\n\t\treturn result\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, basestring):\n\t\tif bufsize is None:\n\t\t\tbufsize = -1\n\t\tseq_type = type2type(type(seq))\n\t\tif seq_type is None:\n\t\t\treturn seq, None\n\t\telif seq_type[:5] == 'array':\n\t\t\t# treat array types as an array of primitive datatypes\n\t\t\tseq_type = seq_type[5:]\n\t\t\tif isinstance(seq, (list, tuple, map, frozenset)):\n\t\t\t\t# treat lists of primitives as arrays of primitive\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, string_types):\n\t\tseq = seq.split() # Split lines\n\tt = seq[-1] # Last element\n\tif type(t) is not type_t:\n\t\treturn seq, str_2_type[t]\n\tt = t.strip() # Split by spaces\n\n\tfor c in t:\n\t\tif len(c) == 1:\n\t\t\tc = '_' # No underscore\n\t\tif c not in type_2_type:\n\t\t\treturn seq, str_2_type['illegal']\n\n\tif t not"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, str):\n\t\tseq = seq.decode('ascii', 'ignore')\n\tif len(seq) < 32768:\t# 1 KB\n\t\tseq = str(seq)\n\treturn seq, '>L'\n\t# from 'http://docs.oracle.com/javase/7/docs/jre/jre-release/src/com/oracle/javax/rpc/protocol.html#rpc-java' for a good starting point\n\tif not isinstance(seq, str):\n\t\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif not isinstance(seq, (list, tuple)):\n\t\tx, seq = seq, list(seq)\n\tn = len(seq)\n\tindices = [0] * n\n\tit = seq.__iter__()\n\twhile True:\n\t\tys = [\n\t\t\tit.next()\n\t\t\tfor x in range(indices[x] if isinstance(x, int) else x, indices[x] + 1 if isinstance(x, int) else n) for x in range(n)\n\t\t\t]\n\t\ti = j = 0\n\t\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif not isinstance(seq, str):\n\t\treturn(seq, type(seq))\n\n\t# Convert any other object to an integer for use as a buffer size\n\tbufsize = _get_bufsize(bufsize)\n\n\t# Split on non-word characters\n\tchars = _str_rx.findall(seq)\n\n\t# Check they have no repeat items\n\tif bufsize is not None and len(chars) > 1:\n\t\traise ValueError(\n\t\t\t\"sequence '{}' contains repeated characters/words\".format(seq)\n\t\t)\n\n\t"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.s\n\telse:\n\t\tother = ensure_number(other)\n\t\tif self.is_zero():\n\t\t\ts = self.s\n\t\telif not is_close(self, other, atol=1e-6, rtol=1e-6):\n\t\t\tfrom..image import gaussian\n\t\t\tfrom..util import get_scale_factor\n\t\t\ts = 1.0\n\t\t\tp,q = get_scale_factor(other)\n\t\t\tt = q/(p*q)\n\t\t\tfor t in range("}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tscaling = self.hist.scaling\n\t\tself.hist.scaling = 0.0\n\t\thist_hist = self.hist\n\t\tif recompute:\n\t\t\thist_hist = Histogram(*self.hist_range[1])\n\t\t\tself.hist_range = (self.hist_range[0], hist_hist.hist)\n\t\telse:\n\t\t\tself.hist = hist_hist\n\t\t\tself.hist.scale = scaling\n\telse:\n\t\tother = as_number(other)\n\t\tscaling = self.hist."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\tif other is None:\n\t\treturn self._scale\n\t\n\t# Check that this histogram's scale is zero\n\tif not isinstance(other, LnFlt.histogram,\n\t\t\t\t\t (float, LnFlt.histogram, LnFlt.histogram, int)):\n\t\traise ValueError(\"Expected %s to be %r, not %r\"\n\t\t\t\t\t\t % (type(self), type(other), other))\n\t\n\tif not self.computed():\n\t\tif not recompute and len(self) > 0:\n\t\t\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tother = self.mean if self.mean > 0 else 1\n\telif isinstance(other, (int, float)):\n\t\tother = [other, other]\n\telif not (0 <= other <= 1):\n\t\traise LenaValueError(\"expected float between 0 and 1, not {:.3f}\".format(other))\n\tif self.mean > 0:\n\t\tself.scale = other[0]\n\tself.int_scale = np.round(other[0]*self.intensity)\n\tself.int_"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\tother = self._validate(other, 1e-7, True)\n\tassert self.scale == other, 'histogram scaling does not match'\n\t\n\tif recompute:\n\t\tncomp = self._get_ncomp(not self._scaled)\n\t\tself._compute_scaled_histogram()\n\t\tself._scaled = self._hist_bins / self.components\n\t\tself._hist_bins *= self._bin_counts\n\t\tself._hist_sum /= ncomp\n\t\tself._hist_bins."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\tif not recompute and self.scale is None:\n\t\tself.compute(recompute=recompute)\n\telif other is not None:\n\t\t# If *other* is given then it should be a float\n\t\t# The scale is computed directly by multiplying\n\t\t#  the float division of the histogram\n\t\tif isinstance(other, float):\n\t\t\t# Compute the scale and store it\n\t\t\tself.scale *= other\n\t\t\t# Set the scale to zero, which indicates\n\t\t\t#    nothing to be scaled.\n\t\t\tif not recompute:"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None or other is Undefined or not isinstance(other, (Number, float)):\n\t\tother = self.bins().histogram[::-1].get_integral_hist()\n\tif self._hist is None:\n\t\traise InvalidHistogramError(\n\t\t\t'Histogram scaling was not computed.',\n\t\t)\n\tif not (0 < other <= self._hist.shape[-1]):\n\t\traise InvalidHistogramError(\n\t\t\t'Scale of histogram (%.1e) could not be converted to fit the histogram'\n\t\t\t'"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif not (other is not None):\n\t\tscaled = self * other\n\t\tif not np.isfinite(scaled.val):\n\t\t\traise LenaTypeError(\"histogram [{}, {}] is not valid.\".format(self.name, other.name))\n\t\tself.scale = multiplied(other, scaled)\n\telse:\n\t\ttry:\n\t\t\tscaled = self / other\n\t\t\tif not np.isfinite(scaled.val):\n\t\t\t\traise LenaValueError(\"histogram [{}, {}] is not valid\".format(self.name, other"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif recompute:\n\t\tif other is None:\n\t\t\traise LenaValueError(\"histograms cannot be rescaled.\")\n\t\tscale = self.compute_hist(other, recompute=True)\n\telif (other is None) and self.compute_hist() is not None:\n\t\tscale = self.compute_hist()\n\telif other is not None:\n\t\tscale = other\n\telse:\n\t\t# try to set the histograms, without error,\n\t\t# since they are already scaled if set by above\n\t\tscale = None\n\treturn scale"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None: return self.scale\n\tif isinstance(other, (float, int, type(None))):\n\t\tif other == 0: return self.fill\n\t\tself.scale = other\n\t\tif recompute: self.compute_scale()\n\t\treturn self.scale\n\t\n\ttry:\n\t\tself.scale = self._fill.histogram_scale(other, recompute=recompute)\n\texcept TypeError:\n\t\tself._fill = other\n\t\tself.scale = self._fill.sum_histograms_scale()\n\texcept ("}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale(force=True)\n\tif not isinstance(other, Coordinate):\n\t\tif np.isnan(other):\n\t\t\t# Use the scaling given as parameter to avoid errors.\n\t\t\tother = self.scale(other=1)\n\telse:\n\t\t# The other value will be internally rescaled with its coordinate.\n\t\t# The error is not rescaled. See the description of self.rescale and\n\t\t# this function.\n\t\tother = other.data\n\n\tself._y *= self._scale(force=True"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\ts = []\n\tscaX = scaY = None\n\tif other is None:\n\t\t(sca) = self._check_type(other, _n.ndarray)\n\telse:\n\t\tsca = self._check_type(other, _sca.ndarray,\n\t\t\t_n.arraylike(_sca))\n\t\t(scaX, scaY) = (sca[::2], sca[1::2])\n\t# Rescale\n\t# If there are errors (e.g., arrays/vectors),\n"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif (other is None):\n\t\treturn self._scale\n\tc = self.coordinate()\n\tif type(other) is str:\n\t\tother = float(other)\n\t# The *scale* function can be overridden,\n\t# to change the scaling of the graph or\n\t# to pass a custom float when *other* is a numeric\n\t# number.\n\ttry:\n\t\tif hasattr(self, \"scale\"):\n\t\t\treturn self.scale(other)\n\texcept:\n\t\tself._scale = other\n\t# If anything fails (which may"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is not None:\n\t\tscale = other\n\telif self.has_scale:\n\t\tscale = self.scale\n\telse:\n\t\tscale = self.scale_unit\n\tif scale =='s':\n\t\treturn 1.0\n\telse:\n\t\treturn self.scale_factor\n\n\tif self.has_scale:\n\t\tx = self.scale[0]\n\t\ty = self.scale[1]\n\telse:\n\t\tx = 1.0\n\tif other is None:\n\t\traise ValueError('Unknown scale.')\n\t\t\n\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tc = self.coordinates()\n\tif other:\n\t\ttry:\n\t\t\tother_scalar = asAny(other, scale)\n\t\t\tif type(other_scalar) == Number:\n\t\t\t\tother = [other_scalar, other]\n\t\t\telif type(other_scalar) == Coordinates:\n\t\t\t\tother = Coordinates(other_scalar)\n\t\t\telif type(other_scalar) == List[List[int]]:\n\t\t\t\tother = List(other_scalar)\n\t\t\telse:\n\t\t\t\traise TypeError"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif not self.scale.is_valid():\n\t\t\traise self.scale.rescale_exception()\n\t\treturn self.scale.coords / self.scale.scale\n\t\n\tif not isinstance(other, self.__class__._Coordinate):\n\t\traise TypeError(f'Argument is not a {self.__class__._Coordinate}')\n\n\ttry:\n\t\tother = self._map(other)\n\texcept TypeError:\n\t\traise TypeError(f'Argument is not a {self.__class__"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom.math import scale\n\tif other is None:\n\t\treturn scale(self.scaleX, self.scaleY, self.scaleZ)\n\tif isinstance(other, (int, float)):\n\t\treturn scale(self.scaleX, self.scaleY, self.scaleZ, other)\n\telse:\n\t\traise TypeError(\"Invalid argument: expected %s, got %s\" %\n\t\t\t\t\t\t(type(self), type(other)))\n\ndef _validate_grid(self):\n\t\"\"\"\n\tHelper for the :meth:`~Lena"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.scale\n\telif isinstance(other, (int, float)):\n\t\treturn self.scale, other\n\telse:\n\t\tself._check_dims(other)\n\t\ttry:\n\t\t\tscale_ratio = self.scale[0]/self.scale[1]\n\t\texcept:\n\t\t\twarnings.warn(\"graph scale of 0\", RuntimeWarning)\n\t\t\tscale_ratio = 1.0\n\t\tscale_ratio = _safe_scale(other, scale_ratio)\n\t\tif not scale_rat"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\timport math\n\timport numpy as np\n\n\tif other is None:\n\t\t# return the scale (axis/direction ratio)\n\t\treturn self.get('scale')\n\telse:\n\t\ttry:\n\t\t\tscaler = self.get('scale')\n\t\texcept (ValueError, AttributeError, TypeError):\n\t\t\treturn other\n\n\t\ttry:\n\t\t\tscalar = Scalar(scaler)\n\t\texcept:\n\t\t\tpass\n\n\t\tif scalar == other:\n\t\t\t# the user has not specified a number, so the scaling must not\n\t\t\t# differ by a tiny"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif self._rescalex and self._rescy:\n\t\tself._rescale()\n\tif other._scalex if other and other._scaley:\n\t\tother._scale()\n\nclass Base2DLine(Line2D):\n\t\"\"\"\n\t\tRepresents a 2D line from an existing image, drawn to\n\tthe graph.\n\t\n\tThis class can represent a 2D line in an image. It is \n\tusually created by a :ref:`~LenaImage2D<LenaImage2D>` or :class:`LenaImage2D"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t,\n\t\"\"\" *make_value* function.\n\t\t*make_value* accepts a single value without context.\n\t\"\"\"\n\t,\n\t[0.5, \"left\"],\n\t{\n\t\t\"name\": \"right\",\n\t\t\"color\": \"red\",\n\t\t\"symbol\": \"rightarrow\"\n\t}\n\t,\n\t\"\"\"\n\t|\n\t<function plot_bin_contours>\n\t<no-name>\n\t<parameter>\n\t\thist\n\t\t\t:class:`.histogram`\n\t\t\t:class:`.histogram2"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t):\n\t\t\n\t\t\"convert graph into a graph\"\n\t\t\n\t\tfrom matplotlib.patheffects import withStroke\n\t\t\n\t\tif hist.bin_edges is None:\n\t\t\traise ValueError(\"graph hist must be bin-like with bin_edges\")\n\t\t\n\t\tedges = np.asarray(hist.bin_edges)\n\t\tlength = edges.shape[0]\n\t\tif length == 1:\n\t\t\tedge = edges[0]\n\t\telse:\n\t\t\tedge = edges[:,1]\n\t\tedges2 = np"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t).replace(\" \", \"{\")\n\t.replace(\"\\n\", \"\")\n).replace(\"True\", \"true\").replace(\"False\", \"false\".replace(\"true\", \"false\"))\n\n\ndef _get_graph(hist, field_names=[\"x\", \"y_mean\", \"y_mean_error\"]):\n\t\"\"\"Create a :class:`.graph` for a :class:`.histogram`.\n\t\n\t:param hist: Histogram object containing all bins plus the center of\n\t\thistogram\n\t:param field_names: a field name or a list of field"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t)]\n\"\"\"\n\t\n\t.hist.extend\n\t.makehists\n\t.\ndef to_graph(\n\t\thist1, hist2=None, **kwargs):\n\treturn Hist(*hist1, **kwargs).to_graph(**hist2)\n\t.\ndef join(a, b):\n\treturn a.join(b)\n\tReturn a.join(b).\ndef joinall(graphs):\n\tdef f(x):\n\t\t# Merge all the graphs into one.\n\t\treturn x.joinAll().\n\treturn graphs["}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t_check_type(make_value, hist, (numpy.ndarray, float))\n\tif get_coordinate not in (\"left\", \"right\", \"middle\", \"top\"):\n\t\traise ValueError(\"not a valid coordinate mode: {}\".format(get_coordinate))\n\treturn make_scatter_graph_from_histogram_and_markers(hist,\n\t\tmake_value, get_coordinate, field_names, scale=scale)\n\n@apply_defaults\ndef make_scatter_graph_from_histogram_and_mark"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t),\n\tdescr \\\n\t\t+ \"\"\"\n\tParameters\n\t----------\n\thist : :class:`~qiskit_nature.optimization.optimization.hist.Histogram`\n\t\t\tHistogram object.\n\t\n\tmake_value : function, optional\n\t\tFunction(hist) which sets the value of a graph's point.\n\t\t*(hist, mean)*, by default *bin.value -> bin content.\n\tget_coordinate : str, optional\n\t\tWhat the coordinate of a graph point created\n\t\tfrom a histogram bin will be. It can be \""}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t, __doc__\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n    To create a :class:`.graph` based on a :class:`.histogram`\n    with optional context one could use:\n    \n    >>> hist = Histogram(field_names, make_value=lambda bin_: bin_)\n    \n    To draw a graph from a histogram:\n    \n    >>> g = draw_graph(hist)\n    \"\"\"\n])\n\ndef draw_graph(hist, make_value=None,"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t).format(hist=hist)\n\t)\n\n\ndef graph_to_hist(g, make_value=None, **kwargs):\n\tr\"\"\"Convert:\n\t\n\t*g* to a :class:`.histogram`\n\t\n\t*make_value* is a function to generate new points at:\n\t***get_coordinate**(*get_coordinate*) returns ``(\"data_point\", \"value\")``\n\t\n\t\tif it's left alone, a data point point is created.\n\t\totherwise, the data point point's coordinate is obtained"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t):\n\t\tif hist.hist!= [()]:\n\t\t\traise errors.GraphError(\n\t\t\t\t\"histogram can only contain non-empty array\"\n\t\t\t)\n\t\tif not hist.hist and hist.bins:\n\t\t\traise errors.GraphError(\n\t\t\t\t\"there must be at least one histogram bin\"\n\t\t\t)\n\t\tif not hist.bins and hist.bin_edges:\n\t\t\traise errors.GraphError(\n\t\t\t\t\"cannot add bins to empty histogram\"\n\t\t\t)\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t)).set_attributes(get_function=get_graph_function)\n\treturn gs\n\n# pylint: enable = wildcard-import\n"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\tfor err in _validator(iface, candidate, tentative, vtype):\n\t\tyield err\n\n\ndef _validator(iface, iface_cls, tentative=False, vtype=None):\n\t\n\tyield zope.validate.checker.CheckerError(\n\t\terr=str(vtype or iface_cls),\n\t\terror=vtype and getattr(iface_cls, vtype) is None,\n\t\tdescription=\"Provide %s method for %s class %s\" % (\n\t\t\tgetattr("}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# This is not a \"best case\" here because the method signature checking\n\t# is based on the interface definition. One possible case of this is\n\t# that the method does not implement the interface's implemented\n\t# capabilities, but raises an error when implemented. In the case that\n\t# the method implements the interface's implemented capabilities, the\n\t# method implementation and the candidate implementation could not\n\t# derive a \"compatible\" interface.\n\tinvalid = 0\n\tif tentative is True:\n\t\tcandidate_valid = candidate.providedBy(iface)   # not None here."}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tresult = super(InterfaceDocumenter, self)._verify(\n\t\tiface, candidate, tentative)\n\t\n\tresult = result and self._required.verify(\n\t\tiface=iface,\n\t\tcandidate=candidate,\n\t\tattrs=iface.attrs if candidate is iface else None,\n\t\tmethods=iface.requiredMethods(iface)\n\t\t)\n\n\tif not result:\n\t\treturn False\n\t\n\tfor method_name in candidate.getAllMethods():\n\t\tmethod = getattr(candidate, method_name)"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef check(result):\n\t\treturn True\n\n\tif isinstance(iface, str):\n\t\tfrom zope.interface import ofType\n\t\treturn ofType(iface, check=check)\n\n\tcandidate_cls = _get_candidate_cls(iface)\n\tif vtype is None:\n\t\t# Check all the provided interfaces\n\t\tmro = _get_mro(candidate_cls)\n\t\tif not mro or not all(callable(method)\n\t\t\tfor name, method in six.iteritems(iface.providedBy"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif tentative and not iface.providedBy(candidate):\n\t\t# check for a missing interface\n\t\traise NotImplementedError(\"Interface '{}' does not provide\".format(iface))\n\n\tif isinstance(candidate, type) and issubclass(candidate, (zope.interface.interface.Interface,\n\t\tzope.interface.implementer.implements, zope.interface.IProvides)):\n\t\t# check for a class or interface\n\t\treturn True\n\n\tif not candidate:\n\t\traise zope.interface.Invalid(\"Interface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\timport zope.interface.tests\n\tfrom zope.interface.verify import verify\n\tfrom zope.interface import related\n\n\tiface, base, bases, attrs, methods = \\\n\tzope.interface.tests.getTarget(base.__class__)\n\tfrom zope.interface import Interface\n\n\tfor attr in bases + base.__dict__.values():\n\t\tif not hasattr(attr, 'providedBy'):\n\t\t\traise ValueError(\n\t\t\t\t'No providedBy available for %r. It is either absent or missing.' % (\n\t\t\t\t\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif tentative:\n\t\treturn True\n\n\tis_valid = True\n\trequired = True\n\t\n\tdef valid_request(f):\n\t\tdef _f(*args, **kw):\n\t\t\targs, kw, _ = zope.interface.adapter(\n\t\t\t\tzope.interface.providedBy, f)(args, kw)\n\t\t\ttry:\n\t\t\t\tcls, interface = args\n\t\t\t\tf(cls, *args, **kw)\n\t\t\texcept TypeError:\n\t\t\t\t# Either we're dealing with the wrong argument\n\t\t\t\t# type, or it doesn't have a"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import notProvides\n\n\ttry:\n\t\tcls = candidate.__class__\n\texcept AttributeError:\n\t\tcls = candidate.__class__\n\t\n\tproviding = notProvides(candidate, iface)\n\t\n\tverify_providedBy(candidate, tentative, vtype=vtype)\n\t\n\t# Must have a valid __provides__ attribute\n\trequired = candidate.providedBy(iface) and not providing in requiredAttrs\n\tmissing = set((iface,)) - supportedAttrs if required else"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\timport inspect\n\t\n\tverify = None\n\ttry:\n\t\tfunc = iface._declared\n\texcept AttributeError:\n\t\tif tentative:\n\t\t\treturn 'class'\n\t\telse:\n\t\t\traise\n\tcandidate = iface\n\tverify = None\n\t# Special case for ``iface``, if it's a class and is not\n\t# decorated by ``@zope.Public``, we need to make sure\n\t# it implements the interface's required methods,\n\t# to make sure it checks the interface we already\n\t# have.\n\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tdecl = candidate.declaredBy\n\telse:\n\t\tdecl = [elem for elem in iface.attribItems() if not elem.match([(declKey, value)])]\n\t\n\tnot_compare = {}\n\tfound = {}\n\t\n\tif vtype is None:\n\t\t# Try to be explicit about the interface.\n\t\t# The iface-id must match the interface with which it comes from.\n\t\t\n\t\t# Look up by calling the implementation of providedBy,\n\t\t# assuming that it will return a"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not tentative or not candidate.providedBy(iface):\n\t\tifile = interfaces.providedBy(iface)\n\t\tif not ifile:\n\t\t\tcandidate = iface\n\t\telif iface!=ifile:\n\t\t\treturn _verifyObjects(_validateInterface(candidate, iface), iface)\n\t\tfor meth in [m for m in iface.__dict__.values() if\n\t\t\tissubclass(m, Interface) and m.providedBy(iface)\n\t\t]:\n\t\t\tif meth.__name__ not in validMethodsList"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# Validate the interface.\n\tif not candidate.providedBy(iface):\n\t\tif tentative:\n\t\t\tcandidate = candidate.factory(\n\t\t\t\t# Check if this is a subclass of *iface*. If so,\n\t\t\t\t# let's not validate on the next iteration.\n\t\t\t\t_IFAPPROVABLE_SUBCLASS\n\t\t\t)\n\t\t\tif not candidate.providedBy(iface):\n\t\t\t\treturn True\n\t\terror = 'The interface does not provide %r' % (iface,)\n\t\tif tentative:\n\t\t\ts = 'It is assumed that all of its"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\treturn verifyObjectHelper(candidate, iface)\n\telse:\n\t\t# Let it raise the error immediately, so that the next attempt doesn't\n\t\t# need to make another one\n\t\tif isinstance(iface, BaseInterfaceType):\n\t\t\treturn False\n\t\telse:\n\t\t\traise ZopeInterfaceError(\n\t\t\t\t\"Candidate must implement Interface in %r, not %r\" % (\n\t\t\t\ttheObject, candidate))\n    \ndef verifyObjectHelper(ofile, iclass, base=None, tentative=False):\n\t\"\"\"\n\t\tVer"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Invalid\n\tfrom zope.interface import implementedBy\n\tfrom zope.interface import provider\n\tfrom zope.interface.exceptions import Invalid\n\n\tiface, attrs = candidate.class_and_attrs\n\tfrom zope.interface.struct import verifyObjectHelper\n\tif tentative:\n\t\tverify = verifyObjectHelper\n\telse:\n\t\tverify = getProvidingInterface\n\n\tfor method, signature in attrs.byMethod.items():\n\t\t# first we'll check that the provided interface and call the method.  All at\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import implementer, providedBy\n\t\n\tobjs = candidate.objs(iface)\n\tcls = candidate.cls_\n\t\n\ttry:\n\t\t# First, make sure the method signature is compatible.\n\t\tcand_impl = candidate.__class__.__implements__ and candidate.__class__.__implements__[iface]\n\t\tif not providedBy(cand_impl.methods(iface)(objs)):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"%r has no matching provider"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tmethods = set()\n\tattributes = set()\n\n\t# Check all of the methods\n\t_checkInterfaceMethods(iface, methods)\n\t\n\t# Check all of the attributes\n\tfields = set(method['name'] for method in iface.getAttributes())\n\tfor attr in fields:\n\t\trequired = False\n\t\tif tentative: \n\t\t\tif not hasattr(iface, attr):\n\t\t\t\terr_str = 'Missing \"{} = {}.{}()\".'\n\t\t\t\traise zope.interface.Invalid \\\n\t\t\t\t\t(err_str."}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tiface = makeInterface(iface)\n\tcandidate = makeInterface(candidate)\n\treturn zope.interface.verifyObject(iface, candidate)\n\t\n\ndef getInterface(iface, name=''):\n\tiface = makeInterface(iface)\n\tnames = [name, getattr(iface, name, None)]\n\tfor name in names:\n\t\tsetattr(iface, name, makeInterface(name))\n\treturn iface\n\ndef getClassIface(iface, classname='', bases=()):\n\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tattributes = {}\n\tif name == '__init__':\n\t\t# Special case: __init__ for this class.\n\t\tattributes = {name: iface}\n\t\tmro = getMro(candidate)\n\t\tfor cls in mro:\n\t\t\tattributes.update(cls.__dict__)\n\telif name == '__new__':\n\t\t# Special case: __new__ for this class.\n\t\t# Don't allow instantiations via multiple inheritance.\n\t\tif hasattr(candidate, '__bases__'):\n\t\t\traise"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tassert isinstance(iface, (zope.interface.interface.InterfaceClass, zope.interface.interface.Interface),\n\t\t  'arg is not an interface'\n\t), '%s does not appear to be a valid interface' % (iface.__name__,)\n\tmethods = dict([(m, iface.providedBy(m.function).get('implements', ())) for m in iface])\n\tfor name, method in methods.items():\n\t\t# Make a list so we can compare methods, not objects\n\t\ttry:\n\t\t\tif is"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not hasattr(candidate, 'providedBy'):\n\t\tcandidate = cast(InterfaceImplementedStrategy, candidate).providedBy\n\t\n\tdef verifyObject_internal(providedBy, iface, candidate, tentative=False):\n\t\t# Verify that the candidate claims that it provides the interface\n\t\t# using the providedBy.\n\t\t#\n\t\t# First, check the providedBy using its __getattr__ method. If\n\t\t# nothing has been providedBy, we check whether the interface\n\t\t# is implemented with at least one method.\n\t\t#\n\t\t# This is"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfor item in candidate:\n\t\tiface.verify(item, t=tentative)\n\n#\n#\tClasses that return None should return an error message\n#\n\nclass notfound(object):\n\t'''\n\t\tNot found.\n\t'''\n\n\tdef __init__(self, msg=None):\n\t\tself.msg = msg\n\n\tdef __str__(self):\n\t\treturn self.msg\n\n\nclass notfound_tentative(object):\n\t'''\n\t\tNot found.\n\t'''\n\n\tdef __init__(self"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif tentative:\n\t\tcandidate.setInterface()\n#\tcandidate.setInterface()\n\n\tif not candidate.verify(iface):\n\t\treturn False\n\n\tif candidate.isUnconnected():\n\t\treturn False\n\n\tif not candidate.isConnected():\n\t\treturn False\n\n\tif candidate.supportsService():\n\t\tcandidate.connect()\n\n\tif candidate.interface == iface:\n\t\treturn candidate.verify(iface)   # This should not be here, we might set it later on\n\treturn True\n\n\ndef _checkVersion(*args, **"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t_debug(\"verifyClass(%r, %r, %r)\" % (iface, candidate, tentative))\n\n\tif candidate[\"method\"] in (MkCommandInstance, MkBkCommandInstance):\n\t\t# No method\n\t\treturn True\n\n\tcmd = candidate[\"cmd\"]\n\t# iface has no method then attempt to use mkmsg as a way\n\ttry:\n\t\t# Use mkmsg for both kinds of commands (and we don't know, for\n\t\t# that it's for command instances, then we get the cmd type)\n\t\t_debug(\"using mkmsg to"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif tentative:\n\t\tififace = candidate\n\telse:\n\t\tififace = iface\n\ttry:\n\t\timport dbus\n\t\tfrom dbus import mainloop\n\t\twith mainloop.PollEvent() as event:\n\t\t\tif iface:\n\t\t\t\tbus = dbus.SystemBus()\n\t\t\t\t#print('get')\n\t\t\t\tinterface = dbus.Interface(bus.get_object(ififace, \"com.example.iface.interface\"), dbus_name=\"Test\")\n\t\t\t\tdevice = dbus.Interface(bus.get_object(ififace"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ttry:\n\t\tiface.lookupIp(candidate)\n\texcept Exception as e:\n\t\te.traceback = traceback.format_exc()\n\t\treturn False\n\n\tif tentative:\n\t\tverifyClass(iface, candidate, False)\n\tif not len(candidate) > 0:\n\t\treturn False\n\n\treturn True\n\nclass Iface():\n\tpass\n\nclass IPInterface(Iface):\n\tdef __init__(self):\n\t\tself.__iface = None\n\n\tdef __getitem__(self, addr):\n\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tconf, iface = getInterfaceInfo(iface, '.')\n\tconf.dprint(\"Checking\", getIpInterface(iface), \"...\", color='yellow')\n\ts, = conf.lshttp.sendCommand(\"GET\", \"http://\" + getIpInterface(iface) + \"/\", \"1\", 10)\n\tcheckStatus(s)\n\ttime.sleep(conf.timeout)\n\ts, = conf.lshttp.sendCommand(\"GET\", \"http://\" + getIpInterface(iface) + \"/\", \"1"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif (tentative):  # pragma: no cover\n\t\ttry:\n\t\t\tinterface = get(iface, None)\n\t\t\treturn 1\n\t\texcept (ConnectionError, OSError) as e:\n\t\t\treturn 0\n\t\n\ttry:\n\t\tiface = get_iface(iface)\n\texcept ValueError:\n\t\treturn 0\n\n\treturn 1\n\n\ndef filter_name(iface, name):\n\t#\n\t# The *name* parameter is usually a *short cut*, and can be any\n\t# *string*, including an empty string"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\terror = False\n\tfor className in list(candidate.keys()):\n\t\tiface2class = None\n\t\tif not tentative and className+'.' in candidate:\n\t\t\tclassName2class = candidate[className+'.']\n\t\telse:\n\t\t\tclassName2class = className\n\t\t\tcandidate = getMappingForClass(className2class, candidate)\n\t\t\tiface2class = className + '2'\n\t\tiface2class = candidate[className2class]\n\t\tif iface.isMappingSupported(iface2class):"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfor name in candidate:\n\t\tiface.setConfig(name, candidate[name])\n\t\t\n\tconf = iface.getConfig(None)\n\t\n\terror = False\n\t\n\tif iface.get('use-ipv6'):\n\t\tiface.setConfig('::', '::')\n\t\tiface.setConfig('::','')\n\t\t\n\tif conf['iface'] in iface.listInterfaces():\n\t\tprint \"*Conf* in the I/Obox conf: %s\" % conf['iface']\n\t\t\n\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\treturn (\n\t\tFalse,  # no class of interest given by *iface*\n\t\t\"Unknown class on %r:\"\n\t\t% os.path.split(iface)[-1],\n\t)\n\n\n########################################################################\nimport os\n\ndef testSuites():\n\t\"\"\"\n\tReturn a sequence of (testcase, fixturedef) tuples where testcase\n\tis the item, while fixturedef the resolved name, with the given\n\targs/kwds applied.\n\t\"\"\"\n\tthisdir = os.path.dirname(__file__)"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases:\n\t\treturn _bases_from_annotations\n\tfor base in bases:\n\t\tbaseTp = type(base)\n\t\tif '__metaclass__' in baseTp.__dict__ and not explicit_mc:\n\t\t\t# '__metaclass__' is set by explicit_mc\n\t\t\tspec = spec_from_class_factory(baseTp, base.__metaclass__).collect()\n\t\t\tif len(spec) == 1 and isinstance(next(iter(spec)), type):\n\t\t\t\t# __metaclass__"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is None:\n\t\ttry:\n\t\t\texplicit_mc = bases[-3]\n\t\texcept:\n\t\t\tpass\n\tif not explicit_mc:\n\t\texplicit_mc = bases[-2]\n\n\tfor cls in explicit_mc.__bases__:\n\t\tif (cls is explicit_mc):\n\t\t\treturn explicit_mc\n\t\telse:\n\t\t\tcandidate = determineMetaclass(bases, cls)\n\t\t\tif candidate is not None:\n\t\t\t\treturn candidate\n\tif not isinstance(bases, (tuple,"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases:\n\t\treturn bases\n\n\tdef _is_object(b):\n\t\treturn b is object\n\n\td = _determine_bases(bases) + _determine_bases((object,), explicit_mc=lambda x: not _is_object(x))\n\t# Check for __slots__\n\tn = len(d)\n\tfor base in d:\n\t\tif base.__class__.__dict__.get('__slots__') or _is_object(base):\n\t\t\treturn base  # class\n\tfor"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases:\n\t\treturn explicit_mc\n\t\n\tbase = bases[0]\n\tif not isinstance(base, type):\n\t\treturn base\n\t\t\t\n\tif not getattr(base, \"__metaclass__\", None):\n\t\treturn base\n\t\t\n\t#\n\t# Make sure that we're not trying to subclass from explicit_mc.c\n\t# (this may raise a TypeError).\n\t#\n\tif hasattr(base, \"__new__\"):\n\t\treturn base\n\t\n\tfor klass in bases[1:]:\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases:\n\t\treturn str\n\tbaseTag = findFirst(bases, '<baseClass', '</baseClass>')\n\tif not baseTag:\n\t\tbases = bases[:]\t# ensure that it's a list -- 'bases' is empty iff\n\t\tif bases:\t# it's not a string\n\t\t\tbaseTag = bases[0]\n\t\t\tbaseClassTag = findFirst(baseTag, '<class>')\n\t\t\tif baseClassTag:\n\t\t\t\tbaseClassName = findFirst(baseClassTag, '<name>')\n\t\t\t\tif"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tbases = list(bases)\n\tlenBases = len(bases)\n\tif not bases:\n\t\tbases = (object,)\n\tfor base in bases:\n\t\tif isinstance(base, types.BuiltinClassType):\n\t\t\t# if base.__base__==object:\n\t\t\t# \tbases = bases[:-1]\n\t\t\t# \tbreak\n\t\t\tpass\n\t\telif type(base) in (type, tuple)+type([]+range(0)):\n\t\t\t# XXX this is an ugly, naive solution, should be\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif hasattr(bases, '__metaclass__'):\n\t\treturn bases.__metaclass__\n\telif explicit_mc is not None:\n\t\treturn MetaClassWithMethod(explicit_mc)\n\tfor base in bases:\n\t\tbase_class = determineMetaclass(base.__bases__, explicit_mc=explicit_mc)\n\t\tif base_class is not None:\n\t\t\treturn base_class\n\treturn type\n\n_MetaHasProperties = typeVar('MetaHasProperties')\n\ndef _hasProperties(cl"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\tmetaclass = explicit_mc\n\telse:\n\t\tmetaclass = _METACLASS\n\n\tnewBases = []\n\tfor base in bases:\n\t\tif hasattr(base, '__metaclass__'):\n\t\t\tmetaclass = base.__metaclass__\n\t\t\tnewBases.append(metaclass)\n\n\treturn type('Meta{0}'.format(metaclass),\n\t\t\t\tnewBases, dict(list(locals().items())[:17]))\n\n# TODO"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\tmeta = explicit_mc\n\telif len(bases)>1:\n\t\tmeta = bases[-2].__class__\n\telse:\n\t\tmeta = type(None)\n\tfor base in bases:\n\t\tif isinstance(base,meta):\n\t\t\treturn base\n\telse:\n\t\treturn meta\n\n\n\ndef _formatDict(d):\n\tif not d: return []\n\td = d.items()\n\td.sort()\n\treturn [unicode(k) + \": \" + unic"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\tclasses = []\n\tfor base in bases:\n\t\tif hasattr(base, '__class__'):\n\t\t\tclasses.append((base.__class__,\n\t\t\t\tbase.__class__.__bases__))\n\t\telse:\n\t\t\tclasses.append((base.__class__, (base.__class__,)))\n\tfor base, classes in classes:\n\t\tif isinstance(base, type):\n\t\t\treturn base.__metaclass__\n\t\telse:\n\t\t\tfor cls in"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tval = self[key]\n\texcept KeyError:\n\t\tif default is not__marker:\n\t\t\treturn default\n\t\traise\n\tself.__delitem__(key, None)\n\treturn val\n\ndef peek(self, key, default=_marker):\n\t\"\"\"\n\tD.peek(k[,d]) -> v, return the k'th element or the d'nd most recent element.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tval"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tif k=='key':\n\t\t\tv = self.d[key]\n\t\t\tself.d[key] = default\n\t\telse:\n\t\t\tdel self.d[key]\n\t\t\treturn default\n\t\treturn v\n\texcept KeyError:\n\t\tif default=='key':\n\t\t\traise\n\t\treturn default\n\n\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\treturn default\n\ndef update(self, seqOrIterable, *args, **kwds):\n\t\"\"\"\n\tAdd items to S in place. S is modified in place. Items silently extend\n\tfrom both S and seqOrIterable.\n\t\"\"\"\n\titems = [k for k, v in list(six.iteritems(seqOrIterable)) if v is not __marker]\n\n\tfor key in items:\n\t\ttry:\n\t\t\tif key in self:\n\t\t\t\tself[key"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tif default is not __marker:\n\t\t\treturn default\n\t\traise\n\t#return self.data[k] because in Python 2.6 dict operations return NotImplementedError\n\t\n\t\n\t\t\n\nclass AttrDict(MutableMapping):\n\t\"\"\" Dictionary with attribute access. \"\"\"\n\t\n\t__slots__ = ('_dict', '_keywords')\n\t\n\tdef __init__(self, **kwargs):\n\t\t\"\"\" Initialize with a dictionary.  The"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self:\n\t\t# v = D[key]\n\t\treturn self[key]\n\telif default is not __marker:\n\t\treturn default\n\n\n\traise KeyError(key)\n\traise InternalError(\"the data not found\") \n# -----------------------------------#\n# -- test code\n# -----------------------------------#\n# d = D[5]\n# print(d)\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tvalue = super().pop(key, default)\n\treturn value\n\n\tpass\n\ndef update(*args, **kwargs):\n\t\"\"\"\n\tD.update(M, **F) -> None\n\tUpdate the dictionary with the key/value pairs from items in M, replacing\n\texisting key/value pairs with the values from F.\n\tThis method is also useful for setting dictionary values using the\n\tdefining key/value pair.\n\t\"\"\"\n\tsuper().update(*args, **kwargs)\n\n\tpass\n\t\n\tpass\n\ndef __repr__(self"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.__map[key][1].pop(key)\n\texcept KeyError:\n\t\tif default is  __marker:\n\t\t\traise KeyError(key)\n\texcept AttributeError:\n\t\traise KeyError(key)\n\ndef deepMerge(d1, d2):\n\tfor k1, v1 in d1.items():\n\t\tif isinstance(v1, dict) or isinstance(v1, set):\n\t\t\tdeepMerge(v1, d2.setdefault(k1, {}))\n\t\t\tcontinue\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\"*** YOUR CODE HERE ***\"\n\ttry:\n\t\treturn self.__dict.pop(key)\n\texcept KeyError:\n\t\traise KeyError('Key not found: %s' % repr(key))\n\texcept TypeError:\n\t\tprint(KeyError)\n\texcept Exception as e:\n\t\traise KeyError('Key not found: %s' % repr(key), default = repr(default))\n\t\"*** END YOUR CODE HERE ***\"\n\ndef nearestPoint(pos):\n\t\"\"\"\n\tFinds the nearest (long,lat) pair to a 2D"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tassert type(key) is str, \"pop() expects str\"\n\ttry:\n\t\treturn self._dict[key]\n\texcept KeyError:\n\t\tif default is not __marker:\n\t\t\treturn default\n\t\traise KeyError(key)\n\n\treturn []\n\ndef peek(self, key, default=__marker):\n\tassert type(key) is str, \"peek() expects str\"\n\ttry:\n\t\treturn self._dict[key]\n\texcept KeyError:\n\t\tif default is not __marker:\n\t\t\treturn default\n\t\traise Key"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self:\n\t\tval = self[key]\n\t\tdel self[key]\n\t\treturn val\n\tif default is not __marker:\n\t\treturn default\n\traise KeyError(key)\n\t\n   \n\t\ndef push(self, key, *val):\n\"\"\"\n\tD.push(k, v) -> None, push key and value onto stack\n\tIf key is not where it was before, push, otherwise pop and val are appended\n\tto key's list\n\t\"\"\"\n\tif key not in self:\n\t\tself.after(self"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn heapq.nlargest(1, self)[0]\n\n\nclass Deque(GrowableQueue):\n\t\"\"\"\n\tA `Deque` is like a `GrowableQueue`, but supports all operations of `deque()`.\n\t\"\"\"\n\n\tdef copy_from_front(self, itm):\n\t\t\"\"\"\n\t\tCopy the elements from the `deque` `itm` to `self` and return\n\t\ta new `Deque`.\n\n\t\tNote: `itm` must be a `Deque` that respects the `deque` interface\n\t\tall"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError()\n\twhile self:\n\t\t(key, val) = self.popitem()\n\t\tif self[key] == self[-1]:     # key == key's last value\n\t\t\tdel self[key]\n\t\t\treturn (key, val)\n\traise KeyError()\n\t\n#********************************************************************\n#\n#********************************************************************\n#\n#********************************************************************\n#\n\n\ndef test():\n\tfor k in range(5):\n\t\tprint(k)\n\t\tprint(*range(k))\n\tprint(type(range"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tv = self[self.smallestHash()]\n\tdel self[self.smallestHash()]\n\treturn (v[0], v[1])\n\ndef select(self, k):\n\t\"\"\"\n\tSelect the `(key, value)` pair whose hash\n\tis the `k` in the sorted collection.\n\t\"\"\"\n\tv = self[k]\n\tfor k in self.keys:\n\t\tif self[k] > v:\n\t\t\tv = self[k]\n\treturn (k, v)\n\ndef add_to_dict("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tfor key, value in self:\n\t\tif self.getcount(key) < self.getmaxcount():\n\t\t\tself[key] = value\n\n\tself.popitem(last=True)\n\n\treturn self.items()\n\nclass LRU_Cache(object):\n\t\"\"\"A limited-usage key-value cache.\"\"\"\n\t\n\tdef __init__(self, capacity, *args, **kwargs):\n\t\t\"\"\"\n\t\t:type capacity: int\n\t\t:type args: Optional[Any]\n\t\t:type kwargs: Optional[Any]\n\t\t\"\"\""}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\ttry:\n\t\t(k, v) = self.lru_c.popitem()\n\texcept KeyError:\n\t\t# for testing in py27\n\t\treturn \"remove not implemented for \" + type(self).__name__\n\tself.cache[k] = v\n\treturn k, v\n\ntry:\n\tf = open(\"data.txt\")\nexcept:\n\tdef data():\n\t\tprint \"open failed, no data.txt found\"\n\t\tretval = \"\"\n\tdef f(data):\n\t\tretval = data\n\t\treturn retval\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# Find number of occurrences, and sort them so that\n\t# occurrences are less frequent\n\tindex = self.__keys.index(self.key, 0, len(self.data))\n\tfor _ in range(2):\n\t\tif key := self.__keys[index]:\n\t\t\tif not self.__keys[index] in self.data:\n\t\t\t\tindex -= 1\n\t\t\t\tkey = self.keys[index]\n\t# Remove the two closest nodes\n\tself.__keys = self.__keys[:index] + self.__keys[index + 1:]\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tk, v = list(self.items())[0]\n\tdel self[k]\n\treturn k, v\n\n#---------------------------------------------------------------------------------------------------------#\n\nif __name__ == '__main__':\n    \n    # Parse command line arguments\n    args = parse_args(parser)\n    logging.info('Arguments: %s' % str(args))\n    config = Config(args.config)\n\n    # Create model\n    logging.info('Creating model instance...')\n    model = Model(args, config, logger)\n\n    # Load model\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\n\t# NOTE: This is a bit faster than a list.sort and is almost \n\t#               a bottleneck for large dictionaries.\n\theap = [(heapq.nlargest([1], i)[0], key)\n\t                               for i, key in enumerate(map(self.keys()))]\n\theapq.heapify(heap)\n\treturn heapq.nlargest(1, heap)[0]\n\t\t\n\t\ndef pop(self, key):\n\t\"\"\"\n\tRemove the key `key` from the dict and return its value"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\ttry:\n\t\tif self.is_empty():\n\t\t\traise KeyError\n\n\t\tk, v = dict.popitem(self)\n\t\tself.map[k] = v\n\t\treturn k, v\n\texcept KeyError: return self.mark, None\n\ndef merge (d, b ):\n\t\"\"\"\n\t\tMerge dictionaries and the other into dictionary `d'\n\t\"\"\"\n\tif not isinstance(d,dict):\n\t\treturn d\n\n\tif not isinstance(b,dict):\n\t\treturn b\n\n\tfor k, v in b.iteritems():"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\theap = [(-self.n, self.c[self.n]) for n in range(self.n)]\n\theapq.heapify(heap)\n\tn, c = heapq.nlargest(2, heap)\n\tfor k, v in self.c.items():\n\t\tif k <= n:\n\t\t\tself.c[k] = c[n]\n\t\t\theapq.heappop(heap)\n\tself.n, self.c = n, c\n\treturn self.popitem()\n\t\ndef next_"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self.heap[0]\n\n##\n# @brief A class for the LFMMS implementation.\nclass LFMMS(object):\n\tdef __init__(self, num_components=7, num_clusters=10):\n\t\t\"\"\"\n\t\tConstructor for LFMMS.\n\n\t\t:param num_components: Number of principal components.\n\t\t:type num_components: int\n\t\t:param num_clusters: Number of clusters.\n\t\t:type num_clusters: int\n\n\t\t:see :ref:`LFMMS.__"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn super().popitem()\n\n\t\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tassert type(self) is collections.OrderedDict\n\tk, v = self.popitem()\n\treturn k, v\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\ttry:\n\t\tself.heap.pop(0)\n\texcept AttributeError:\n\t\treturn\n\n\treturn self.heap[0], None\n\n\t# O (log n)\ndef merge(a, b):\n\theap = []\n\tsmallest = []\n\tsmallestIndex = 0\n\n\tfor k, v in b.items():\n\t\theapq.heappush(heap, (v, k))\n\n\tfor k, v in a.items():\n\t\theapq.heappush(heap, (v, k))\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tnode = self._head\n\twhile node is not None and self.count > self._size:\n\t\telem = self._remove(node)\n\t\tself._size -= 1\n\t\tself._len -= 1\n\t\tif len(self._list_node) == 0:\n\t\t\tself._last += 1\n\t\t\tdel self._list_node[self._last]\n\t\t\treturn elem\n\t\telse:\n\t\t\tnode = self._first\n\treturn elem\n\n\n# -------------------------------\n# Define some useful helper functions.\n# -------------------------------\n\ndef _add_first_node"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\theap = self._heap.items[self._heap._index[self._first]]\n\tk, v = heap[_FIRST]\n\tself._heap._index[self._first] = self._heap._index[k]\n\tif self._first!= self._last:\n\t\tdel self._heap._keys[self._first]\n\t\tdel self._heap._values[self._first]\n\tdel self._heap._data[k]\n\tself._heap._index[self._first] = -1\n\tself._heap"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self.keys:\n\t\treturn sorted(self.keys(), key=lambda k: k[1].last_index())[-2]\n\traise KeyError('Popped from empty queue: '+ repr(self))\n\n\nclass Empty(Error):\n\tdef __init__(self):\n\t\tsuper(Empty, self).__init__('Attempted to pull from an empty queue')\n\n\nclass Full(*args, **kwargs):\n\tdef __init__(self, maxsize, **kwargs):\n\t\tsuper(Full, self).__init__(maxsize"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif not self.empty:\n\t\tself.current_size -= self.n_popleft_size\n\t\tself.c.zpopitem(self.t[self.current_size]._get_key_value()[1])\n\t\treturn (self.t[self.current_size]._get_key_value()[0], self.current_size)\n\telse:\n\t\treturn None\n\n\t\n\n\n\n\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif not self._cache:\n\t\traise KeyError\n\n\t(key, val) = self._cache[0]\n\tif key in self._cache:\n\t\tdel self._cache[0]\n\t\treturn (key, val)\n\telse:\n\t\treturn False\n\n\t# for python compatibility\n\treturn self._cache[0]"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpopitem = self.data.popitem()\n\tself.length -= 1\n\tself = self.get_by_key(popitem[0])\n\treturn popitem\n"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif not hasattr(self, '_mapping'):\n\t\treturn None\n\telif not self._mapping:\n\t\traise KeyError('set/frozen')\n\tnewest = None\n\tfor k,v in self._mapping.items():\n\t\tif newest is None or v['creation'] > newest['creation']:\n\t\t\tnewest = {'key':k,'creation':v['creation']}\n\tself._mapping.popitem()\n\treturn newest\n\n\t\n\nclass LRUCache:\n\t\"\"\"\n\tA simple LRU cache implementation with"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tsz = len(self)\n\tfor i in range(0,sz):\n\t\tk,v = self[i]\n\t\tif k < self.__heapsize//2:\n\t\t\tself[i] = self[k]\n\t\t\tself[k] = None\n\t\telse:\n\t\t\tself[i] = self[-1*(k-self.__heapsize//2)]\n\t\t\tpopIndex = -1\n\t\t\tfor j in range(0,k-self.__heapsize//2):\n\t\t\t\tpopIndex = j\n\t\t\t\ttry:\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tkey,val = super().popitem()\n\tif key in self.data:\n\t\tdel self.data[key]\n\treturn (key,val)\n\n\n\n\n\nimport asyncio\n\n\nclass CacheError(Exception):\n\tpass\n\n\nclass InvalidCacheError(CacheError):\n\tpass\n\n\nclass Cache(object):\n\t\"\"\"Object-oriented caching system, with methods designed to cache\n\tvalues based on keys and allow for a very basic getter/setter\n\tpattern.\"\"\"\n\tdef __init__(self):\n\t\tself._values = {}\n\t\tself"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif not self.popitem_to_return:\n\t\traise IndexError\n\n\t(key, val) = self.popitem_to_return\n\n\treturn key, val\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tself._hash.clear()\n\treturn (self._hash.next, self._hash.next.next) \n\n  def pop(self, *args): pass \n\nclass DoubleHash(FIFODict, list): pass\n\n\nclass LFDict(FIFODict, list): pass\n\n\nclass DefaultFIFODict(FIFODict, dict): pass"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self.__dict__.get('popitem', (self._key, self._value))\n\n\n\t'''\n\t# TODO: Remove. You're probably looking for all of the keys that go\n\t# away with all keys being popped. We can probably do something like\n\t# this as follows:\n\tlast = key  # We use our `last` variable to keep track of oldest item being\n\t\t\t\t# removed as long as we haven't returned. If we do have it,\n\t\t\t\t# then let it move fast enough to let us return it.  Note the\n\t\t\t\t#"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\ttry:\n\t\t_, _, v = self.items.get(self.next)\n\t\tdel self.items[self.next]\n\t\tself.next = self.next - 1\n\t\tif self.next == -1:\n\t\t\tdel self.items\n\t\t\tif len(self.items) == 0:\n\t\t\t\tself.last = None\n\t\t\t\tdel self.counts\n\n\texcept:\n\t\tdel self.items[self.next]\n\t\tif self.next == -1:\n\t\t\tdel self.counts\n\n\treturn self.items[self.next]\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\"*** YOUR CODE HERE ***\"\n\t#this is not a helper function of a dict, so it should be imported as another helper\n\n\t#return (key, value)\n\treturn self.heapq.heappop(self.heap)\n\ndef get_average(items):\n\t\"\"\"\n\t:type items: List\n\t:rtype: float\n\t\"\"\"\n\t#this is the same as get and it does the math\n\t#because the function returns the average, all numbers in the list come down to this average\n\t#with this, the max(x"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn (self.pop(), self.pop())\n\n  \n#############################################################################\n##  Function: __len__  \n#############################################################################\n\n@typecheck(self, other)\ndef __len__(self):  \n  \"\"\"Implementation of the `len` built-in function.\"\"\"\n  return len(self._items)  \n\n@typecheck(self, other)\ndef __eq__(self, other):  \n  \"\"\"Implementation of the `==` built-in function.\"\"\"\n  return other is self\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\told_item = self.popitem(False)\n\tself._keys.pop(0)\n\tself._items.pop()\n\treturn old_item\n\nclass PriorityQueue(MutableSet):\n\t\"\"\"A generic collection of entries with priority in link to list items.\"\"\"\n\n\tdef _key(self, entry):\n\t\treturn entry[1]._key\n\t\n\tdef add(self, entry):\n\t\tentry_key = self._key(entry)\n\t\tentry_p = entry[0]\n\t\tentry_q = self._new_queue(entry_"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.__random.choice([x for x in self.items if not x[1]])[0]\n\nfrom collections import namedtuple\n\nBagSorted = namedtuple('BagSorted', 'items')\nBagSet = namedtuple('bag','set')(\n\t*((x, self[x]) for x in sorted(self.keys())))\n\n\nclass BagIterator:\n    \"\"\"\n    Iterator for bag of arbitrary size.\n    \"\"\"\n\n    def __init__(self, bag):\n        #: The bag to treat as an"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\titem = popitem(self)\n\tself._m_mutex.release()\n\treturn item\n\n\t\n# \t# We have no idea what this is\n# \t# (it could be a lock that lets us know we are holding\n# \t# (not even an active lock on my thread), which isn't\n# \t# true for Unix, but that's the least surprising and it\n# \t# is what this is called).\n# \twhile not self.active:\n# \t\tnext()\n# \t(v, z) = self."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn random.choice(self._it)\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.pop()\n\nclass IndexedIndexDict(defaultdict):\n\tdef __getitem__(self, index):\n\t\treturn self.data[index]\n\n\tdef __setitem__(self, index, values):\n\t\tself[index] = values\n\nclass ThreadPool(multiprocessing.Pool):\n\tdef __init__(self, args=(), kwds=None):\n\t\tself.args = args\n\t\tself.kwds = kwds or {}\n\t\tself.kwds.setdefault(\"daemon\", False)\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t# Get the index\n\tindex = random.randrange(self.size())\n\t# Pop and return the value at that index\n\treturn self.__setitem__(index, self.remove())\n\n# ----------------------------\n# Misc\nimport pytest\n\n\n# ----------------------------\n# Test class\nclass TestTrie(object):\n\t\"\"\"\n\tTests for pyTrellis.Trie\n\t\"\"\"\n\tdef test_root_has_empty_string(self):\n\t\t\"\"\"\n\t\tTest if `trie(root)` returns `False` if `root`"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t(key, value) = self.__dict__.popitem()\n\treturn key, value\n\n\nclass SortedSet(dict):\n\t\"\"\"A `SortedSet` is a dictionary of keys and values that are\n\timplemented as an ordered collection of entities.\n\t\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"Initialize a sorted set object.\"\"\"\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself.sorted_keys = frozenset(self)\n\n\tdef __str__(self):"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self[:] == []:\n\t\traise IndexError, \"Trying to pop an element where you don't have one in your head\"\n\treturn self.first(), self.rest()\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t# return self.queue.popitem()\n\n\nasync def async_get_client_id(hass: HomeAssistant) -> str:\n    \"\"\"Return a client ID.\"\"\"\n    client_id = str(hass.data[DATA_CONNECTED_CLIENTS][KEY_CLIENT_ID])\n\n    def get_client_id_async() -> str:\n        def get_client_id_wrapper() -> str:\n            return f\"{client_id}-{hass.data[DATA_CONNECTED_CLIENTS].get("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\ttry:\n\t\tkey, val = self.randpop()\n\t\tself.popitem(key, val)\n\t\treturn key, val\n\texcept IndexError:\n\t\traise IndexError(\"popitem(key=-1)\")\n\n\tdef update(self, other):\n\t\t\"\"\"\n\t\tAdd all key/value pairs in another (mapping) `dict`, `list`, or `namedtuple` \n\t\tnamedtuple can be used to update `self` with an object of a different type. \n\t\t\"\"\"\n\t\tfor key, val in other:\n\t\t\tself.__"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.dict.popitem()\n\n\tdef update(self, other=None, **kwargs):\n\t\t\"\"\"\n\t\tUpdate the current values of the dictionary with another `(key, value)`.\n\t\t\"\"\"\n\t\tself.pairs.update(other=other, **kwargs)\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tThe second argument can be, for example, a tuple of\n\t\t(key, value) pairs.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.dict[key]\n\t\texcept KeyError:\n\t\t\treturn"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self._re_in_style)\n\nclass _Re_In_Quote_Group(object):\n\t@property\n\tdef regex(self):\n\t\t\"\"\"\n\t\t\tThe regex in the body of the group.\n\t\t\"\"\"\n\t\treturn re.compile(self._regex)\n\n_re_in_quote = re.compile(r'(%s)' % _RE_IN_QUOTE)\n\"\"\"\n\tA pattern with a '%s' in it.\n\t\"\"\"\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tdef in_style_regex(parameter: str, value: object) -> re.Pattern:\n\t\t\"\"\"\n\t\t\tInitialise the in-style parameter regular expression.\n\t\t\t\n\t\t\tParameters\n\t\t\t----------\n\t\t\tparameter\n\t\t\t\tParameter to be matched \n\t\t\tvalue\n\t\t\t\tValue for the corresponding parameter\n\t\t\"\"\"\n\t\tregex = self.pattern(parameter)\n\t\tregex.set(parameter, value)\n\t\treturn regex\n\t\n\tin_style_regex = property(_create_in_regex, doc=\"\"\"\n\t\t\t\tThe in-style regular expression"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tif self.type_of_parameters == 'default':\n\t\tself.is_not_null = False\n\treturn Pattern(\n\t\tre.compile(\n\t\t\tr'(?P<parameter>{}\\w+\\s*,)+'.format(self.label),\n\t\t\tre.DOTALL,\n\t\t),\n\t)\n\n\t\n\t\n\t\n\t\nclass ModelSelect(Select):\n\t\"\"\"\n\tThis is a model select object, which can be used to show or search a model.\n\t\"\"\"\n\tdef __init__(self, model,"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t# the result of re.match on the 'pattern' part, but the result can't be parsed\n\t# in a parser for the following type of param\n\tregex = re.compile(pattern=r'(\\w+)', re.DOTALL | re.IGNORECASE)\n\t# pattern can't be reused\n\tcompiled = regex.compile(pattern=self.pattern, re.DOTALL | re.IGNORECASE)\n\treturn compiled\n\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn _create_in(self.regex)\n\n\ndef _create_exclude(\n\t*,\n\tbase: Optional[str],\n\tfrom_: Optional[str],\n\tmatch_pattern: bool,\n\tregex: str,\n\tflags: int = 0,\n) -> Match[Union[str, Pattern], Union[None, str, Pattern]]:\n\t\"\"\"\n\tCreate an :class:`re.Match` object that excludes the parameters. \n\t\n\tThe `base` and `from_` arguments control the exclude behaviour, and the\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.in_regex)\n\ndef parameter_string(value : str) -> str:\n\t\"\"\"\n\t\tA :class:`str` version of the parameter.\n\n\t\t:param value: The value to check.\n\t\t:return: The value as a ``str``.\n\t\t\"\"\"\n\treturn os.fspath(value)\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpatterns: set = set()\n\tclass_attrib = self.get_class_attrib()\n\tif class_attrib:\n\t\tpatterns.update(class_attrib._PARAM_RULE)\n\treturn patterns\n\ndef _get_regexp_match(preamble: list) -> re.Pattern:\n\t\"\"\"\n\tGet the regular expression match object.\n\t\n\t`preamble`: a list of strings.\n\t\"\"\"\n\treturn re.compile(f.join(preamble) if isinstance(pream"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.in_pattern / \"_<name>_\")\n\n\nclass _param_regex(_param_regex, _match_regex):\n\n\tdef __repr__(self) -> str:\n\t\t\"\"\"\n\t\t\tRepresents the regex.\n\t\t\"\"\"\n\t\treturn super().__repr__(str(self.regex))\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tif self._param.name in self._params:\n\t\tparametric_pattern = self._params[self._param.name]\n\t\treturn re.compile(parametric_pattern)\n\telse:\n\t\treturn None\n\n\nclass _Parameterexpr:\n\t\"\"\"\n\tRepresents a :class:`re.Pattern` object in :class:`Parameterexpr` structure.\n\t\n\tThis class contains all common :class:`re.Pattern` objects from\n\t:mod:`lib.pattern`.\n\t\n\t:exception TypeError:"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.in_parameter, re.RegexFlag.VERBOSE)\n\n\t\nclass _FormatableParams(tuple):\n\"\"\"\n\tThis class uses the class attributes `formatter` or :func:`StringFormatter`\n\tto format the parameters.\n\t\n\tIt's a tuple of parameters classes to represent different values.\n\t\"\"\"\n\t__slots__ = ('formatter','regex', 'delimiter', 'key', 'as_number')\n\t\n\tdef __init__(self, parameters: tuple,"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tstyle = self\n\twhile hasattr(style, 'converter'):\t# pylint: disable=access-member-before-definition\n\t\tstyle = style.converter  # type: ignore  # `__getattribute__` is not safe\n\n\t\tstyle = getattr(style, 'decorator', style)  # type: ignore\n\n\tstyle = self.converter_class\n\n\treturn style\n\n\nclass _ParameterConverterMixin:\n\t\"\"\"\n\tThe parameter style converter mixin.\n\n\t.. note::\n\t\tImplementors should define"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tif CONVERSIONS is None:\n\t\t_converting.__LOGGER__.error('parameter styles are not available')\n\t\traise NoStyleAvailableError('parameter styles are not available')\n\n\tconverter = _converting.get(CONVERSIONS)\n\tif converter is None:\n\t\t_converting.__LOGGER__.error(f'Unsupported converter: {CONVERSIONS}')\n\t\traise TypeError('only _converting._converter.Converter has known converters')\n\n\treturn converter()\n\n\ndef create_style(style: t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tstyle_converter = _converting._converting._StyleConverter()\n\n\treturn style_converter.format_style(self.style, self.attributes)\n\n\nclass Styler:\n\t\"\"\"\n\tStyler implements methods that implement the \"styler\" object, a collection\n\tof other \"styler\" objects to apply. It implements the rules for applying\n\tstyle to a particular field, like applying ``:input:`` a given style to\n\tthe input ``p``.\n\n\t``Styler`` inherits ``_styler.Styler``\n\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tfrom.converter import Converter\n\treturn Converter(self)\n\ndef get_converting(params: _list[_NamedTuple]):\n\t\"\"\"\n\t\tGets a :class:`._converting._Converter` object.\n\n\t\t:param params: the parameters that need a converter\n\t\"\"\"\n\twith converter as c:\n\t\treturn c.get_converter(params)\n\ndef create_converter(params: _list[_NamedTuple]):\n\t\"\"\"\n\t\tCreates a converter object from the parameters.\n\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t# Default: the `to_str` method is the same as `str`.\n\treturn _converting._Converter(to_str=str, to_float=float)\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn conversion.convert(\n\t\tfile_name=self.__filename,\n\t\toutput_folder=self.__output_folder,\n\t\tforce_recreate=self.__force_recreate,\n\t\tforce_export=self.__force_export,\n\t\tforce_reimport=self.__force_reimport,\n\t\tenable_debug=self.__debug,\n\t\tverbose=self.__verbose,\n\t\tlog_level=self.__logging_level,\n\t\tcreate_folders(output_folder=self.__logging_output_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tif not self._style: return _converting._Converter(None, None)\n\t\t\n\tstyle_style = self._style.upper()\n\t# First, make sure we have a valid style.\n\tif style_style not in ['REAL', 'QUANTIFIER', 'QUANTIFIER_TRI',\n\t\t'SCALAR', 'SCALAR_TRI', 'SCALAR_QUANTIFIER_TRI', 'SINGLE']:\n\t\traise ColorScaleUnacceptableError(\n\t\t\tstyle=style_style,\n\t\t\tstyle_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn {\n\t\t# Convert the initial parameters to `int` when possible.\n\t\t'bool': int,\n\t\t# Convert the initial parameters to `float` when possible.\n\t\t'float': float,\n\t\t# Convert the initial parameters to `str` when possible\n\t\t'string': str,\n\t\t# Convert the initial parameters to `bytes` when possible.\n\t\t'bytes': bytes,\n\t\t# Create a new parameter style if there is a more appropriate one.\n\t\t# Note that if nothing is found then the converter defaults to `string` - as this value can be\n\t\t#"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(self.config, **kw)\n\nclass _Parameters(_collections.OrderedDict):\n\t\"\"\"\n\tInternal class to keep track of parameter style parameter dictionaries.\n\t\"\"\"\n\tdef _get_param_name(self, section:str, name:str, kw:Callable[[Any], None]):\n\t\t\"\"\"\n\t\tReturn a unique name for the given section and parameter.\n\t\t\n\t\t:param section: the section name (string)\n\t\t:param option: the parameter name (string).\n\t\t:param k"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _paramstyle.ParamstyleConverter(self.format_type)\n\n\nclass _SqliteConverter(_paramstyle.LegacyFromLiteralConverter,\n                      dbfconverters.ExcelFromDbflConverter,\n                      dbfconverters.Converter):\n\t\"\"\"\n\tBase class for converting SQLite data types to/from BINARY/VARBINARY types.\n\tThis converter class is used for the DBAPI `native` DBAPI types.\n\t\"\"\"\n\tdef __init__(self, *args"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\timport dateutil.parser as parser\n\tif dt_str is None:\n\t\treturn parser.parse(dt_str)[:args.duration]\n\telif isinstance(dt_str, six.string_types):\n\t\treturn parser.parse(dt_str, None)\n\telif isinstance(dt_str, io.IOBase):\n\t\treturn parser.parse(dt_str.read(), args.duration)\n\telse:\n\t\traise TypeError('String must be `str`, `io.BytesIO` or `file`')\n\n##################################################################"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tif (dt_str and isinstance(dt_str, string_types)\n\t   and dt_str[0:3] in \"-T.+\" and dt_str[-1:] in \" + tzoffset\n\t   ) or (dt_str and isinstance(dt_str, stream_generators)\n\t          and 'T' in list(dt_str.readline())[0]):\n\t    # ISO-8601 time format\n\t    parts = dt_str.split(\"T\")\n\t    dt = parse_date_with"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt_obj = datetime.datetime.fromisoformat(dt_str)\n\treturn dt_obj if dt_obj.tzinfo else dt_obj.replace(second=0)\n\ndef date2isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into an\n\t:class:`datetime.datetime` formatted as an ISO 8601 date and time.\n\t\n\tSupports ISO-8601 datetime format strings that are complete but\n\tnot fully ISO-8601 compatible."}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn parse(dt_str, ignoretz=True, dayfirst=True, **_localargparse)\n\n\tif dt_str.endswith(MICROSECONDS):\n\t\tdt_str = dt_str[:-2]\n\tif dt_str.endswith(DOT):\n\t\tdt_str = dt_str[:-2]\n\n\tdt = datetime._timely.parser.parse(dt_str)\n\tif not dt:\n\t\traise ValueError(f\"{dt_str} not a valid ISO"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tyear, month, day = map(int, self.date_sep.join([\n\t\tself.re_dmy_mdy.split(None, 1)\n\t\t[0][0]\n\t\t, re.compile(self.re_dmy_mdy.split(None, 2)\n\t\t[0][1])\n\t]).split('.'))\n\tdt = dt_str or self.datestring_iso8601\n\tif dt:\n\t\tdt = dt.split(self.date_sep)\n\t\ttry:\n\t\t\t"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tkwargs.update(locals())\n\tif self.format =='s':\n\t\treturn dt_str\n\treturn self._parse_str(dt_str, dt_type, **kwargs)\n\n\t# def isoparse(self, dt_str):\n\t#     pass\n\t\t\n\nclass YdateParsed(BaseDateTimeParsed):\n\tdef _parse_str(self, dt_str, dt_type, **kwargs):\n\t\ttz = kwargs.get('tz')\n\t\ttry:\n\t\t\treturn"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn self.isoformat(dt_str) if not isinstance(dt_str, _datetime.datetime) else dt_str\n\n\ndef isoparse(iso_str):\n\t\"\"\"\n\tParse an ISO-8601 string into a :class:`datetime.datetime`.\n\t\n\tThe :func:`isoparse` function accepts a date string in the form of an\n\tISO-8601 string or a stream containing an ISO format string.\n\t\n\tFor a single ISO day, the ISO 8601 representation can be\n\tconstructed using"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tcomponents = dt_str.split(' ')\n\tif len(components) < 2 or components[0].lower() not in _FMT_MAP and \\\n\t\t\tcomponents[0].split('-', 1)[0] not in ('-', 'T'):\n\t\traise ValueError('{d} is not an ISO-8601 datetime string'.format(d=dt_str))\n\tyear, month, day = components[0].split('-', 1)\n\ttime_pieces = components[1].split(' ', 2)\n\tif len("}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\ttokens = Tokenizer(dt_str)\n\tnext(tokens)\n\tsep = tokens.next()\n\tif len(tokens)!= 1 or sep!= 'T':\n\t\traise TypeError((\"Only one datetime token is allowed: {}\".format(sep)),\n\t\t\ttokens, dt_str)\n\ttoken = tokens.next()\n\ts = token.content\n\tif not match(r'^^([^,\uff0c\u3002]+),',s) or match(r'^^$',s):\n\t\t"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tif isinstance(dt_str, str):\n\t\tdt_iso = dt_str.lower()\n\t\toffset = dt_iso[0]\n\t\tdt_iso = dt_iso[1:]\n\telse:\n\t\toffset = 0\n\t\tif isinstance(dt_iso, (bytes, bytearray, memoryview)):\n\t\t\tdt_iso = dt_iso.decode('iso_8859_1')\n\t\t\t\n\t\tif not isinstance(dt_iso, str):\n\t\t\traise ValueError('%r is not a valid ISO-"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdef _parse(self, timestr, **kwargs):\n\t\treturn self._parse_datetime(timestr, **kwargs)\n\n\t_format_cache = None\n\t_FORMAT_ERRORS = None\n\n\tdef _parse_datetime(self, timestr, tzinfos=None, **kwargs):\n\t\tif not timestr:\n\t\t\treturn datetime.datetime.min.replace(fold=datefinder.MOJ If self._format_cache is None else self._FORMAT_ERRORS[self._format_cache])\n\n"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\treturn _parse_datetime_unparse(\n\t\tself._parse_datetime(timestr, **kwargs), default, ignoretz, tzinfos,\n\t\tfuzzy_with_tokens, None, None,\n\t\t)\n\n# TODO: replace with functools.lru_cache when available (Python 3.3+)\ntry:\n\tparse_datetime = _import_c_time\nexcept ImportError:\n\tpass\n\n\n###\n# Generic dateutil datetime parser helpers.\n###\n\nEPOCH_OFFSET ="}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tif isinstance(timestr, bytes):\n\t\tfor err in _parser(timestr):\n\t\t\ttokens, tokensStart = err\n\t\t\tbreak\n\t\telse:\n\t\t\traise ValueError(err % tokens)\n\n\telse:\n\t\ttokens, tokensStart = [], 0\n\n\tregex = r'^%s(?:[01]\\d|2[0-3]):[0-5][0-9]:[0-5][0-9]' % \\\n\t\t(re.escape(timestr.replace(':','')))"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdef __init__(self, timestr, dayfirst=False,\n\t              fuzzy_with_tokens=False,\n\t              fzyear=None,\n\t              ignoretz=False, tzinfos=None, parse_full=False,\n\t              _cache_key=None, **kwargs):\n\t\tsuper(RE, self).__init__()\n\n\t\tself._cache_key = _cache_key\n\n\t\tintpart, tail = __builtins__['str'].partition(timestr)\n\t\tbintpart ="}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdef _parse(timestr):\n\t\treturn _parse_base(timestr, None, *fmtargs, **kwargs)\n\n\t# Note: we don't call _parse_date because f_weekday() was added in\n\t# http://docs.python.org/lib/datetime.html#strftime-strptime-behavior\n\t# See https://bugs.launchpad.net/dateutil/+bug/1120941\n\treturn _dt_parse_with_tzinfo(_iso8601_parse(_parse(timestr), tzinf"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t# Check for raw parsing.  This check prevents issues with Python\n\t# 2.4.10 which disable raw parsing.\n\tif'' in timestr:\n\t\twarnings.warn(ParserWarning(\"Parser doesn't support whitespace.  Date:\"\n\t\t\t\" %s\" % timestr))\n\tif ':' in timestr:\n\t\twarnings.warn(ParserWarning(\"Parser doesn't support separators.  Date:\"\n\t\t\t\" %s\" % timestr))\n\n\tsignal_pat = re.compile(r'(\\+|\\-)(["}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t# NB. This code originally came from :mod:`regex`, which was\n\t# changed since Python 2.5.  In particular, it was changed to\n\t# allow unicode strings.\n\t# Py2's regex module does not allow the \"unmatched\" sets of parentheses\n\t# on \\*\\*, but Py3 is requiring explicit parentheses on any set\n\t# declarations.\n\t\n\tignore_tokens = kwargs.pop(\"ignoretz\", False)\n\tif ignore_tokens:\n\t\t_str = timestr.split(\" \""}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tif default is None:\n\t\tdefault = datetime.datetime.now(tzinfo)\n\telif isinstance(default, datetime.datetime):\n\t\tpass\n\telif isinstance(default, numbers.Integral):\n\t\tdefault = datetime.datetime.fromtimestamp(default)\n\telif hasattr(default, 'isoformat'):\n\t\tdefault = default.isoformat()\n\telse:\n\t\ttry:\n\t\t\tdefault = dateutil.parser.parse(default, TZinfos=tzinfos, fuzzy=fuzzy_with"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdef _handle_fraction(value):\n\t\treturn Fraction(value.numerator, value.denominator)\n\n\t# Tokenize our date string\n\ttokens = token_set_whitespace.difference(timestr).intersection(tokens)\n\tif ignoretz or not isinstance(tzinfos, int):\n\t\tignored = 0\n\t\tfor tok in tokens:\n\t\t\tif not (tok.hour >= 0 and tok.minute >= 0 and tok.second >= 0):\n\t\t\t\tignored += 1\n\t\t\tel"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tsign_options = (\"+\", \"-\")\n\tfmt = _check_fmt_options(kwargs)\n\t\n\tif _is_number(timestr):\n\t\t# Tokenize into an integer will raise a BUG in Python 2.6 while\n\t\t# it doesn't let me handle dates like 10 January 2003 yet. :(\n\t\tif default is None:\n\t\t\tdt = _dt_from_int(int(timestr.split()))\n\t\t\tif ignoretz:\n\t\t\t\tif tzinfos:\n\t\t\t\t\ttznames = list(tzinfos)\n"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif self._fold:\n#         now = dateutil.tz.UTC.localize(dt).astimezone(self._tz)\n      now = dt\n    \n    else:\n      \n      if not self._tz:\n\t\traise LookupError\n      \n      try:\n        \n        now = dateutil.tz.tz_localize(dt, tz=self._tz).astimezone(self._tz)\n        \n        if not now.tzinfo.isdst(dt.replace(tzinfo=None)):\n          \n          now"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttime = dt.time()\n\tfold = False # Don't try the next time on a fold state\n\thours = 0\n\tfor delta in (time.dow, time.dowd):\n\t\tif delta:\n\t\t\tfold = True\n\t\t\thours += delta.seconds\n\tif fold and hours > 12: # pragma: nocover\n\t\tfold = False\n\t\thours -= 12\n\t\n\tiso_now = getattr(datetime.utcnow(), 'isoformat', None)\n\t# The default values are the first 3 days, or a datetime"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\treturn dt + dt.minute / 60. + dt.second / 3600.\n\ndef toutc(value: datetime.datetime) -> datetime.datetime:\n\t\"\"\"\n\tConverts a timezone aware datetime object to an unambiguous timezone-\n\taware datetime object.\n\t\n\t:param value:\n\t    An unambiguous :class:`datetime.datetime` object.\n\t\"\"\"\n\treturn value - to_utc(value.minute / 60. + value.second / 3600.).astimezone(py"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ty,m,d = dt.year, dt.month, dt.day\n\ttz = pytz.timezone(self.tz)\n\tif y < -366:\n\t    if d > 28:\n\t        y = y+1000\n\t    d = d-1\n\telse:\n\t    if d > 28:\n\t        d = d-1\n\t    y = y-1\n\t\n\t# Get the datetime on the local day of the year.\n\t#\n\tldt = datetime(y,m,d,"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt_orig = d.combine(dt, datetime.time(23,59,59,23), dt)       # Time-ZERO: 23:59:59 UTC\n\tif dt_orig.tzinfo is None:                           # No explicit timezone\n\t\tdt_ambig = dt_orig\n\telse:                                                    # Explicit timezone\n\t\tdt_ambig = dt_orig.replace(tzinfo=dt.tzinfo).astimezone(dt.tzinfo.tzlocal()).astimezone(timezone.get_local"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\thour, remainder, day, month, year, tzoffset, tzfold = \\\n\t\tself.datetime().utcoffset(dt).to_tuple()\n\tif tzoffset is None:\n\t\tif remainder or (dt.minute < 45 and dt.second >= 55):\n\t\t\t# ambiguous now\n\t\t\tdt += datetime.time(hour=0, minute=0, second=0, microsecond=0)\n\t\t\treturn dt\n\t\telse:\n\t\t\treturn dt\n\telse:\n\t\treturn (dtzet.datetime(dt."}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt is None:\n\t\treturn None\n\telif dt.tzinfo is not None:  # datetime with a defined timezone\n\t\treturn dt.astimezone(self) # already in local timezone\n\telse:\n\t\treturn dt.replace(tzinfo=self)\n\n\t#\n\nclass Context(tzinfo):\n\t\"\"\"\n\tContext contains a specific timezone and specifies whether it is\n\tlocated at the current time or on the current timezone.\n\t\"\"\"\n\tdef __init__(self, z, d_or_t"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\timport _tzinfo\n\t\n\t_dt = dt.tzinfo\n\tm = _dt.utcoffset(None).total_seconds()\n\tdt = dt.replace(tzinfo=_dt, tzinfo=None)\n\n\tfold = 0\n\n\tif m:\n\t\tif math.fmod(m, 60):\n\t\t\t# Fold, UTC-time.\n\t\t\tfold = 1\n\t\telse:\n\t\t\t# Standard offset.\n\t\t\tfold = 0\n\n\tif dt > _tzinfo.utc_base or dt < _tzinfo."}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdat = self.dat\n\tif dat:\n\t\tif dat < dt:\n\t\t\tdt = dat + utc_timedelta(hours=-dt.hour, minutes=-dt.minute, seconds=-dt.second)\n\t\tif dt.tzinfo is None:\n\t\t\tdt = dt.replace(tzinfo=datetime.timezone.utc)\n\treturn dt.astimezone(timezone.utc)\n\ndef parse_iso_timestamp(date, time, tz='UTC', fmt='YYYY-MM-DD HH:mm:"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is self:\n\t\treturn self.combine(dt, ZERO)\n\tif dt.tzinfo is None:\n\t\treturn self.fromutc(dt.replace(tzinfo=self))\n\treturn dt.astimezone(self)\n\n"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\treturn dt.astimezone(tzinfo)\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\treturn dt.replace(tzinfo=tzinfo)\n\n\ndef _dt_to_python(dt):\n    \"\"\"\n    converts datetime, ntuple, list, dict, and ndatetime objects to a date\n    and/or time objects\n    \n    This is the function that is part of the normalize_fn function; it is meant\n    to be called with a :py:class:`datetime object <python:date,time>, or a datetime\n    in which case it is assumed to be a datetime. This method is currently\n    imple"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\treturn dt.replace(tzinfo=tzinfo)\n\ndef tzoffset(str_dt, offset):\n    \"\"\"\n\tCreate a datetime from the ``str_dt`` and ``offset`` arguments.\n\t\n\t:param str_dt: String representation of a datetime, such as \"2012-12-18 05:23:36\"\n\t:param offset: DST offset\n\t\n\t:return:\n\t    Returns a :py:class:`~datetime.datetime` instance.\n\t\"\"\"\n\tstr_dt = str_dt.lower"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\tif dt.tzinfo is None:\n\t\t\ttz = dt.tzinfo = timezone.tzutc()\n\t\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\telse:\n\t\t\treturn dt.replace(tzinfo=tz)\n\n\ndef _dt_util_dt_to_dt_py3(dt):\n\t# This is used only on Python 3 because of a bug with the ``tz"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:  # this indicates something deprecated and it would always be empty\n\t\treturn dt.replace(tzinfo=tzoffset(None, 0))\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tlocal_tzinfo = tzinfo\n\tif local_tzinfo is None:\n\t\traise ValueError(\"A naive datetime is required\")\n\ndef always_timezone(dt, tzinfo):\n    \"\"\"\n\tReturns the datetime ``dt`` with the specified time zone assumed to be\n\t``tzinfo``.\n\n\tThe datetime ``dt`` is always naive before setting the ``tzinfo`` it was\n\toriginally in. This is the standard practice of time zones.\n\n\tThe ``tzinfo`` to assign to ``dt`` is ``None``\n\n\t:param"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif tzinfo is None:\n\t\treturn dt\n\tif dt is None:\n\t\treturn\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt.astimezone(tzinfo)\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not tzinfo:\n\t    return dt.replace(tzinfo=tzinfo)\n\t\n\treturn dt\n\t\n# vim: sw=4 ts=4\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\treturn dt.astimezone(tzinfo).replace(tzinfo=None)\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdef fix_datetime_naive(dt):\n\t\t# make a copy of the datetime as it will be modified by this routine\n\t\tnew_dt = dt.replace(tzinfo=None).astimezone(tzinfo)\n\t\tif dt.tzinfo is not None:\n\t\t\t# this should always happen, but let's be a little extra cautious\n\t\t\tif dt.tzinfo.utcoffset(dt) is not None:\n\t\t\t\tseconds = dt.tzinfo.utcoffset(dt).seconds\n\t\t\t\tminutes = (seconds//3670"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_chars = ''.join([str(s) for s in before]) + ''.join([str(s) for s in after])\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_chars = (before + after).bytes if any(re.search(r'\\r\\n', s) for s in before + after) else b''\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tbefore_len = len(before)\n\tafter_len = len(after)\n\tself.__before = set([self.read_uint32(0x80D83)] * before_len)\n\tself.__after = set([self.read_uint32(0x80D84)] * after_len)\n\tself.__after[0] = 0x80\n\tself.__before[after[0]] = 0x80\n\t\n\t# The first byte, 0x83 is the length of the cut bytes, which is 0x82 + before_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif len(before) == 0 or len(after) == 0:\n\t\treturn\n\tif before is None:\n\t\tbefore = ''\n\tif after is None:\n\t\tafter = ''\n\tif len(before) and len(after) and not before[-1].isdigit():\n\t\t# It is possible that last bit was filled with '\\\\',\n\t\t# which makes the following lines empty. See #23.\n\t\tbefore = bytearray((-1) * before.rstrip(b'\\x00')) + before[:-1]\n\t\tafter"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tbefore: int = len(before)\n\tafter: int = len(after)\n\tif after < after-before:\n\t\tafter += before\n\t\t\n\tafter += 1\n\t\n\t# TODO: The following would be cleaner to modify the actual file.\n\tself.buffer: list[bytes] = []\n\tself.cut_chars: int = len(before) - 1\n\t\n\tfor char in before[1:]:\n\t\tif char in [b\"\\n\", b\"\\x07\", b\"\\x08\"]:\n\t\t\toffset = 0"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_after = before\n\tself.cut_before = after\n\t\n\tif self.cut_before == self.cut_after:\n\t\tself.cut_after = None\n\tif self.cut_before is None:\n\t\tself.cut_before = bytearray()\n\t\n\tself.before_delim_bytes = self.cut_before\n\tself.before_delim = \",\".join(self.before_delim)\n\tself.sliced = len(before) >= self.semit_max\n\t\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tfor cut in self.cut_chars:\n\t\tbefore = cut + before\n\t\tafter = cut + after\n\t\n\tself.pre_slice_pairs.add(before)\n\tself.post_slice_pairs.add(after)\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_chars = []\n\tlast_start = 0\n\t# Check if after is empty\n\tif (after.find(b'\\n')==0):\n\t\tindex_after = after.find(b'\\\\n')\n\telse:\n\t\tindex_after = 0\n\tif len(after)!=index_after:\n\t\tself.cut_chars.append(b\"\\\\n\")\n\t# Loop over split by string\n\tfor splitted_bytes in before.split(b'\\n'):\n\t\t# Only for delimiters\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif len(before) > 0 or len(after) > 0:\n\t\tself.delimiters = tuple(\n\t\t\tbytes([\n\t\t\t\tafter.rfind(before),\n\t\t\t    after.rfind(before),\n\t\t\t    after.rfind(before),\n\t\t\t    after.rfind(before),\n\t\t\t    after.rfind(before),\n\t\t\t])\n\t\t)\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tdel i[:len(before)]\n\n\tchars = b\"\\N{COPYRIGHT SIGN} \"\n\n\tif before == b\"\" and after == b\"\":\n\t\tbefore = chars\n\n\tif after == b\"\" and before == b\"\":\n\t\tafter = chars\n\n\ti.insert(0, chars)\n\ti.append(chars)\n\n@depends(read_bytes=None, write_bytes=None)\ndef read_file(location: str, bufsize: int = 16384) -> bytes:\n\t\"\"\"\n\tRead a.txt"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tmessage_parts = request.get(b'message_parts')\n\tparts_message = message_parts[0]\n\n\tif message_parts[0]:\n\t\tif parts_message.get()!= '1':\n\t\t\tparts = parts_message.get(b'parts')\n\t\t\t# the 1st message may contain the secret (aka a key), so check secret by hand\n\t\t\tif len(parts) > 1:\n\t\t\t\tfor part in parts:\n\t\t\t\t\tif part:\n\t\t\t\t\t\t# this is a standard or an old message payload\n\t\t\t\t\t\tif part.get().startswith("}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\tjson_data = request.content.decode('utf-8')\n\t\tjson_data_raw = json.loads(json_data)\n\t\tmessage_data = json_data_raw['message']\n\t\tmessage_data_raw = message_data['json']\n\t\tjson_data_raw['message'] = {\n\t\t\t'id': message_data['id'],\n\t\t\t'name': message_data['name'],\n\t\t\t'url': message_data['url'],\n\t\t\t'master': message_data['master'],\n\t\t}\n\t\t"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\tresponse = {\n\t\t\t'isDiasporaRequest': True,\n\t\t}\n\t\tif request.message.header == 'public' or irc.NICK_STR in request.message.message:\n\t\t\tprint(f'[diaspora.py] Detected Request : [public]')\n\t\t\tresponse.update(message=irc.client.msg(destination=request.channel, request=request))\n\t\telse:\n\t\t\tprint(f'[diaspora.py] Detected Request : [private]')\n\t\t\tfor msg in request.message.channel."}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif 'public' in request.headers:\n\t\theaders = request.headers\n\t\tname = request.headers.get('x-diaspora', '');\n\t\tpath = request.path.replace(scheme='');\n\t\tpath = path.lstrip(\"/\");\n\t\tif(path[0] == '/'):\n\t\t\tpath = path[1:]\n\t\tif (not path.startswith('/') and name!= \"send\"):\n\t\t\treturn path\n\t\t\n\t\tpath = path.replace(path.split('/')[2], '');\n\t\t\n"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tidentify_req = 0\n\tif isinstance(request, Message.Message):\n\t\tif request.sender is not None:\n\t\t\tidentify_req = Message.identify_header(\"send\", request.sender, True)\n\telif isinstance(request, Message.PrivateMessage):\n\t\tidentify_req = Message.identify_header(\"private\", request, True)\n\telif isinstance(request, Message.PublicMessage):\n\t\tidentify_req = Message.identify_header(\"private\", request, True)\n\telif isinstance(request"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t# We first try to identify the message, which should be for the public message\n\tif request.payload.message!= 1 or (type(request.payload.message) == str and \"%msg\" in request.payload.message):\n\t\tif (request.payload.message.startswith(\"$msg\") or request.payload.message.startswith(\"$get-\")):\n\t\t\tmessage = request.payload.message.split(\"$msg\")[1]\n\t\telif (\"$set\" in request.payload.message or \"set\" in request."}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t# if this is a legacy payload\n\tif request.json.get('message_type') in LEGACY_PREFIX.message_types and request.json.get('message_id') in LEGACY_PREFIX.message_ids:\n\t\trequested_type = request.json['message_type']\n\t\trequested_message_idx = request.json['message_id']\n\n\t\t# the message type is probably wrong, check if it's an error message for now\n\t\tif not request.args.get('message_type'):\n\t\t\tresponse = 'Bad message"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\timport base64\n\timport datetime\n\tif request.from_:\n\t\tidentifiable = \"pub\" in request.from_\n\telif any(message.from_ for message in request.messages):\n\t\tidentifiable = \"pub\" in [message.from_ for message in request.messages]\n\telif any(message.from_ for message in request.from_messages):\n\t\tidentifiable = \"pub\" in [message.from_ for message in request.from_messages]\n\telse:\n\t\tidentifiable = False\n\n\t# If request"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tfrom.message_types import *\n\tfrom.models import MessageRead, Message\n\tif isinstance(request, ReceiveMessageRequest):\n\t\treturn is_public_message_request(request)\n\tif isinstance(request, SendMessageRequest):\n\t\treturn is_private_message_request(request)\n\tmessage_type = MessageTypes.lookup_type(str(type(request)).split(\">\")[1].upper())\n\treturn getattr(message_type, \"identify\", lambda: message_type.id)()\n\nclass PublicMessageRequestHandler:"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tlegacyResponse = [True, False]\n\tif not isMessageLegacy(request):\n\t\tif '_diaspora' not in request.headers.keys():\n\t\t\tlegacyResponse = [True, False]\n\t\telif request.get(\"public\") or request.get(\"publicMessage\"):\n\t\t\tlegacyResponse = getDiasporaResponseLegacy(request)\n\t\telse:\n\t\t\tlegacyResponse = getDiasporaResponseNormal(request)\n\t\tif legacyResponse == [True, True]:\n\t\t\treturn True\n\t\telse:\n\t\t\trequest['diaspora"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif'matrix-id' in request.args.keys() and 'namespace' not in request.args:\n\t\treturn True\n\n\tif 'namespace' in request.args.keys() and'matrix-id' not in request.args:\n\t\treturn True\n\n\treturn False\n\n\n# /matrix/message/<name>\n@endpoint('/message/<name>', name='notfound')\ndef handle_response(*, body, name: str ='message') -> ResponseType:\n\tif not found_request(name):\n\t\treturn Response("}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request.body is None or request.body.data is None:\n\t\treturn False\n\n\ttry:\n\t\tcontent_class_object = get_content_class_object(request.body.content)\n\texcept ValueError:\n\t\treturn False\n\n\t# noqa: DAR202\n\treturn True\n\n\nclass MatrixError(Exception):\n\t\"\"\"Base class for all Matrix exceptions\"\"\"\n\n\nclass MatrixValueError(MatrixError):\n\n\tdef __init__(self, message: str, value: float | int | str | bytes) -> None:\n\t\tself.message = message"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request.method!= \"GET\":\n\t\tif request.header[\"Content-Type\"].startswith(\"application/json\"):\n\t\t\treturn True\n\telif not request.has_header('Authorization'):\n\t\treturn False\n\theader = request.header['Authorization']\n\tif (not header.startswith('Bearer ')) and not token_required(header):\n\t\treturn True\n\treturn False\n\n\ndef token_required(token: str) -> bool:\n\t\"\"\"\n\t\tCheck if a token is valid.\n\t\tThis function is a wrapper around"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t# TODO: This method should be refactored into different objects\n\tresponse = None\n\ttry:\n\t\t# TODO: Make it possible for requests to be marked a request state without a context\n\t\t_isRequest = request.IsRequest\n\texcept Exception as e:\n\t\tlog.error(\n\t\t\tf\"Problem when attempting to identify the request state: {type(_isRequest)}. Error: {e}\"\n\t\t)\n\tfinally:\n\t\tif response is not None:\n\t\t\treturn True\n\n\treturn _isRequest\n\n\n# TODO: Check that request is actually"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif \"MatrixIdentifier\" in request:\n\t\treturn True\n\ttrace = []\n\t# Get any trace id from the request body\n\ttrace_id = request.headers.get(\"Matrix-Trace-ID\")\n\tif trace_id:\n\t\ttrace.append(f\" Matrix-Trace-ID {trace_id}\")\n\tif \"Matrix-Trace-Request-Type\" in request:\n\t\ttrace.append(f\" Matrix-Trace-Request-Type {request.headers.get('Matrix-Trace-Request-Type')}\")\n\tif"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\ttry:\n\t\tassert len(request_data)\n\t\trequest_data = request.data.dict()\n\texcept Exception as e:\n\t\tpass\n\ttry:\n\t\tassert isinstance(request, Request)\n\t\trequest = request_data\n\texcept Exception as e:\n\t\tpass\n\ttry:\n\t\tassert request.user\n\t\trequest_user = request.user\n\texcept Exception as e:\n\t\tpass\n\tif request_user!= user:\n\t\treturn True\n\ttry:\n\t\tif request.user.id!= user.id:\n\t\t\t# noins"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\traise NotImplementedError(\"identify_request\")\n\n\n@abstractmethod\ndef identify_response(response: ResponseType) -> bool:\n\traise NotImplementedError(\"identify_response\")\n\t\n\ndef parse_request(request: str) -> RequestType:\n\t\"\"\"\n\tParses the Matrix webhook request URL for an ID-Api and ID-Message.\n\t\"\"\"\n\t# Build a RequestType object\n\trequest = Request(request)\n\trequest.get(matrix_url).parse()\n\n\t# Try to get the Matrix"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn len(request.url) > 0 and request.url.lower().startswith('http://matrix.org/')\n\n#\n# Matrix\n#\nclass MatrixErrorMessage(Message):\n\t\"\"\"\n\tFormat a standard error message to Matrix\n\t\"\"\"\n\n\tdef __init__(self, msg: str):\n\t\tself.msg = msg\n\n\tdef format(self) -> str:\n\t\treturn f'ERROR: {self.msg}\\n'\n\n\nclass MatrixResponseMessage(Message):\n\t\"\"\"\n\tFormat a standard response message to Matrix\n\t\"\"\"\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tis_request = True\n\n\t# 1. Request header\n\tif 'Request:'in request.headers[hdrs.REQUEST_HEADER]:\n\t\tis_request = True\n\telse:\n\t\tis_request = False\n\n\t# 2. Request body\n\tif request.body is not None:\n\n\t\t# 2. Request body is a bytes\n\t\tif isinstance(request.body, bytes):\n\t\t\tis_request = True\n\t\t# 2. Request body is a str\n\t\telif isinstance(request.body, str):\n\t\t\tis_request = False\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\ttry:\n\t\tidentity = Identity.get(request)\n\texcept MatrixError:\n\t\treturn False\n\n\treturn identity.enabled if identity else False\n\n\n@same_line()\ndef is_empty_message(message: bytes) -> bool:\n\t\"\"\"\n\t\tCheck if the message is empty\n\t\"\"\"\n\tsender = message.get('from')\n\trecipient = message.get('to')\n\treturn sender!= recipient and sender.upper()!= '+BOT' and recipient!= '+BOT'\n\n@same_line()\ndef is_bot"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tdate_str = \"\"\n\ttime_str = \"\"\n\tfor char in dt.timetuple():\n\t\tif char == 6: # 0 for TU\n\t\t\tdate_str += char\n\t\t\ttime_str += \":\"\n\t\telif char == 8:\n\t\t\ttime_str += char\n\t\telif char == 9:\n\t\t\ttime_str += char\n\t\telif char == 4:\n\t\t\ttime_str += \":S\"\n\treturn date_str + \" \" + time_str[:-3]\n\t\ndef _parse_time(s):\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%Y-%m-%d').split('-')\n\t\n# This is a list of days so we don't have to check this every node\ndef format_days(days):\n\t\"\"\"\n\tFormat a list of days into a single list.\n\t\"\"\"\n\treturn [format_dt(d) for d in days]\n\ndef format_iso(dt):\n\t\"\"\"\n\tFormat a datetime as a string using iso datetime.\n\t\"\"\"\n\treturn dt.strftime('%Y-%m-%d %"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif isinstance(dt, str):\n\t\treturn dt\n\telif isinstance(dt, DtNode):\n\t\treturn dt.value\n\telif isinstance(dt, str_to_dt):\n\t\treturn parse_dt_formatter(dt)\n\telif isinstance(dt, datetime):\n\t\treturn dt.isoformat()\n\telse:\n\t\tdct = datetime.fromisoformat(dt.isoformat())\n\t\treturn datetime.isoformat(dct.date()).replace(\":\", \"E\")\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tdt_format = '\"%Y-%m-%d\" %T'\n\tret = dt_to_timestamp(dt.__str__())\n\tif ret is not None:\n\t\tret = float(ret)\n\treturn dt_format % (dt_format + '-E' + dt.__class__.__qualname__.split(\".\")[-1])\n\ndef format_epoch(epoch):\n\t\"\"\"\n\tFormat a epoch in the style as a D* seconds/day/hour/minute/second/day.\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt:\n\t\treturn dt.isoformat()\n\telse:\n\t\treturn dt\n\ndef format_time(time):\n\t'''\n\t\tFormat a time a bit easily.  Can be useful for unit conversion such as\n\t\tmicroseconds.\n\t\t'''\n\t\tif time is None:\n\t\t\treturn \"\"\n\t\treturn dt_py2py(time).time()\n\ndef parse_interval(interval):\n\t'''\n\t\tParses an interval string and returns a D* time.\n\t\tSee the D* manual for format specification."}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt is None:\n\t\treturn dt\n\n\tassert dt.year == 2014, \"Bad D* year: %r (%d)\" % (dt, dt.year)\n\n\t# dt - local timezone (utc)\n\t# dst - timezone (local time)\n\t# am - timezone (current time in AM/PM)\n\t# pm - timezone (current time in PM)\n\thours_before = dt.utcoffset().days * 24\n\tif dt.dst():\n\t\thours_after = dt."}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt == 0:\n\t\treturn \"%Y-%m-%d\"\n\tiso_dt = str(dt.isoformat())\n\treturn iso_dt[:10]\n\n\nclass Node:\n\tdef __init__(self, node, source, target):\n\t\tself.node = node\n\t\tself.target = target\n\t\tself.source = source\n\t\tself.id = id(self)\n\t\t# self.label = label\n\n\n\t\tself.incoming = set()\n\t\tself.outgoing = []\n\t\tself.label = None\n\t\tself"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tfmt = '%d %b %Y %H:%M:%S'\n\ttimezone = dt.tzinfo.zone if dt.tzinfo else dt.tzinfo.utcoffset(None)\t\n\tdt = dt - dt.utcoffset().seconds\n\tif timezone:\n\t\tdt = dt.astimezone(pytz.utc).replace(tzinfo=None)\n\t\tfmt = fmt +'{0}'.format(pytz.to_str(dt.astimezone(pytz."}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.astimezone(dt.tzinfo).isoformat(timespec='minutes')\n\n\ndef add_to_dict(incoming, outgoing_dict):\n\t\"\"\"Add dict from outgoing to incoming when set to true.\n\t\"\"\"\n\tfor p_type in (\"IN\", \"OUT\"):\n\t\tfor src, out_type in outgoing_dict.items():\n\t\t\tif src not in incoming:\n\t\t\t\toutgoing_dict[src] = (p_type, out_type)\n\n\ndef add_to_dict_from_datasets(incoming"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt is None:\n\t\t# default to now, unless not yet\n\t\treturn strftime(DATE_FMT)\n\tif isinstance(dt, Number):\n\t\t# return a datetime object\n\t\treturn dt\n\tif isinstance(dt, str) and not dt:\n\t\t# parse string as A time (and return as now)\n\t\treturn strftime(DATE_FMT)[:16]\n\tif isinstance(dt, str):\n\t\t# return a datetime object\n\t\treturn astimezone(datetime.utcnow(), d"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif replacer:\n\t\ttext = replacer(text)\n\ttags = TagPattern(re.compile(PYTHON_TAG_PATTERN))\n\tlines = text.splitlines()\n\tresult = {tags.match(i).tag for i in lines}\n\treturn {*result, tags.match(text).tag}, text\n\ndef find_lines(text: str, replacer: Callable = None) -> list:\n\t\"\"\"\n\tFinds lines in text.\n\t\n\tTries to ignore lines inside code blocks.\n\t\n\tOption"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif replacer is None:\n\t\treplacer = lambda t: t\n\ttags = set()\n\ts = text.split()\n\tfor i, wordlist in enumerate(s):\n\t\tword = wordlist[0].lower()\n\t\tif word.isalnum():\n\t\t\tcontinue # ignore words that startswith alpha-numerics\n\t\tstart = i\n\t\twordlist = wordlist[1:]\n\t\tfor j, word in enumerate(wordlist):\n\t\t\ttag = word.lower()\n\t\t\tif tag.isdigit():\n\t\t\t\tcontinue\n\t\t\tvalue ="}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tlines, text = split_with_whitespace(text)\n\n\t# find tag\n\ttag = None\n\tfor line in lines:\n\t\tif line.strip()[:1] == \"[\":\n\t\t\tstart = line.index(\">\")\n\t\t\tline = line.replace(\">\", \"\").replace(\"[\", \"<\")\n\t\t\tend = line.index(\"]\")\n\t\t\tline = line[:start + 1] + line[end:]\n\t\t\ttag = \"start\"\n\t\t\tcontinue\n\t\ttag_found = tag is not None\n\t\tbreak\n\t\n\t# find"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = frozenset()\n\t\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\t\t\n\ttext = re.sub(r\"(?:\\s|^)(\\w+:(?=\\n))\", replacer, text)   # convert tag to word\n\ttext = re.sub(r\"(?<=\\s)/\", \" />\", text)                   # remove empty line\n\t\n\ttext = re.sub(r\"<[^><]+>\", \"\", text)                       # strip non-alphabet characters\n\ttext = re."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif replacer:\n\t\tdef replacer(tag: str, value: str) -> str:\n\t\t\ttry:\n\t\t\t\treturn replacer(tag, value)\n\t\t\texcept ValueError:\n\t\t\t\t# Value cannot be found in string.\n\t\t\t\treturn text.replace(tag, value)\n\telse:\n\t\treplacer: Callable = Tag.replacer\n\ttags = set()\n\tstart = 0\n\twhile True:\n\t\tfound = False\n\t\ttext, start = Tag.word_separator(text, tag=start)\n\t\tif \"\".join(text[start"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif replacer == None:\n\t\treplacer = lambda t: t\n\t\t\n\tfound_tags = set()\n\t\n\ttokenized_text = tldr.word_tokenize(text)\n\t\n\t# Search for all tags to avoid looking for nested tags\n\tfor tag in tokenized_text:\n\t\tif tldr._tag_re.match(tag) :\n\t\t\tif re.search(r'.*[^0-9]+$',tag) :\n\t\t\t\ttag = re.sub(r'\\d+$', '', tag)\n\t\t\t\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tres = []; res.extend(text.split('\\n'))\n\twhile res[0]!= '' and not res[0].startswith('<'):\n\t\tres = [x.strip('>!+]>') for x in res if not x.startswith('<')]\n\t\tres = [replac.sub(replac, x) for (replac, x) in zip(replacement_fns, res[1:])]\n\t\tres = [x for x in res if x!= '']\n\treturn res"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags: Tuple[Set, str] = (set(), text)\n\tlines = text.splitlines()\n\tnum = None\t\t\t# Number of lines since we have to find some tags. It's None if no more\n\t\t\t\t\t\t\t# tags remain.\n\tcurrent = None\n\tis_last = False\n\tlines_with_ignore = []\n\t\n\tfor line in lines:\n\t\t# If a line contains code, ignore it.\n\t\tif any(line.startswith(t) for t in (\"/\", \"*\", \"#\", \"`\", \"``\")):\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif replacer:\n\t\twords = str(replacer).split()\n\n\ttags = { ]\n\n\twhile True:\n\t\ttext = text.lower().strip()\n\t\tfor tag in words:\n\n\t\t\t# Remove tags\n\t\t\tif tag.startswith('{'): tag = tag[2:].strip()\n\t\t\tif tag.startswith('}'): tag = tag[1:].strip()\n\n\t\t\t# Only allow tags with whitespace\n\t\t\tif tag.strip() == '': continue\n\n\t\t\ttags.add(tag)\n\n\t\tif not text:\n\t\t\tbreak\n\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tdef repl() -> Set:\n\t\ttags = set()\n\t\t# ignore tags inside code blocks\n\t\tif \">\" in text:\n\t\t\treturn tags\n\t\t\n\t\t# replace text\n\t\tfor match in re.finditer(r\"<[\\w\\$]+>[\\w\\$]+\", text, flags=re.IGNORECASE):\n\t\t\tif replacer is not None:\n\t\t\t\ttag = replacer(match)\n\t\t\t\tif tag:\n\t\t\t\t\ttags.add(tag)\n\t\t\t\t\n\t\tif not tags:\n\t\t\treturn {}, \"\"\n\t\treturn tags, text\n"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Clean special tokens\n\ttext = Cleaned.clean_text(text)\n\t# Remove links or quotes that aren't in an article in plain \n\t# text - like 'This is not a link with\" \" or \" \" or \"#\" or other.\n\ttext = Cleaned.replace_html_chars(text)\n\t\n\t# Split text into paragraphs (each paragraph has an additional newline \n\t# sentence which can end as the next sentence) and the title\n\tsentences = Cleaned.split_text(text)\n\ttitle_len = len("}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext_links = []\n\t\t\n\tif \"<\" in text and \">\" in text and text[text.index(\"<\")+1] == \">\" and text[text.index(\"<\")+2] == \"<\" and text[text.index(\"<\")+3] == \"<\":\n\t\tattributed = []\n\t\tattributed.append(text[text.index(\"<\")+1:text.index(\">\")])\t\n\t\tattributed_1 = []\n\t\tattributed_1.append(text[text.index(\"<\")+2:text.index(\""}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfor link in linkify.findall(text):\n\t\tif link.get('href'):\n\t\t\tlink.set('href', process_url(link.get('href')))\n\t\tif 'href' in link.attrib:\n\t\t\tlink.set('target', process_url(link.get('href')))\n\t\tif 'javascript' in link.attrib:\n\t\t\tlink.set('type', 'javascript')\n\t\tid_list.append((link.get('id'), link))\n\ndef process_url(url):\n\t\"\"\"\n\tThis function takes as"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext_links =[]\n\tlinks = re.findall(self.TEXT_RE_REGEX,text)\n\tif links:\n\t\tfor l in links:\n\t\t\tif isinstance(l,list):\n\t\t\t\tfor link in l:\n\t\t\t\t\tif link[0].startswith('#'):\n\t\t\t\t\t\tif link[0] in article_dict:\n\t\t\t\t\t\t\tif not article_dict[link[0]][0]:\n\t\t\t\t\t\t\t\tlink[0] = article_dict[link[0]]\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\ttext_links.append(link)\n\t\t\t\t\t\t\tbreak;"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(r';(.)', r'\\1', text)\n\ttext = re.sub(r';(?:\\s|$)|(<\\n)|(\\/n)(\\s|$)|(\\/a)(\\s|$)|(\\/\\d)(\\s|$)|(\\/n)(\\s|$)|(/)[^\\s]+', \"\", text)\n\ttext = re.sub(r':(?:\\s|$)|(href|#)(\\s|$)|(\\/a)(\\s|$)|(\\/n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\turls = []\n\tfor link in text.find_all(attrs={'rel': 'href'}):\n\t\turls += link.get('href')\n\treturn urls\n\ndef linkify_links(text):\n\t\"\"\"\n\t\tLinkify all links in the given text.\n\t\"\"\"\n\ttext = text.replace('\\n','')\n\turls_lista = parse_links(text)\n\treturn '\\n'.join(['<a href=\"%s\">%s</a>' % nt for nt, _"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# some attrs\n\tattributes = {\n\t\t\"id.title.text-normal\": \"main-title\",\n\t\t\"class.title\": \"title\",\n\t\t\"class.subtitle\": \"subtitle\",\n\t\t\"class.summary\": \"summary\",\n\t\t\"class.source\": \"source\",\n\t\t\"class.author\": \"author\",\n\t\t\"class.genre\": \"genre\",\n\t\t\"class.tag\": \"tags\",\n\t\t\"id.thumbnail-path\": \"thumbnail-link\",\n\t\t\"class.thumbnail.width\": \"width-th"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\treturn text\n\n\t# TODO: This does not handle links with multiple texts. Remove this if all texts become longer.\n\n\t# TODO: Should this be called upon process_text_urls() (ie. via a URL), as well?\n\"\"\"\n\tReturn `(text, tail, is_url, base)`.\n\t- `text` is a string or unicode\n\t\t- It's a valid string containing only ASCII characters.\n\t\t- It's a valid URL.\n\t\t- It's any valid URL, e.g. `\"http://example3.com\"`"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\twords = re.finditer(r'^([0-9a-zA-Z]+)://([^\\s]+)[\\s]+?$', text)\n\tfor matchNum, match in enumerate(words, start=1):\n\t\t(urlNode, urlPref,) = match.groups()\n\t\turlPref = 'http://' + urlPref\n\t\tmatchNum = urlPref\n\t\t# find the '$' symbol for the URL\n\t\tmatchNum = [matchNum if symbol == '$' else symbol for symbol in matchNum]\n\t\tif '$'"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(r' +','', text).split()\n\ttext = [sent for sent in text if '.com' not in sent]\n\ttext = [sent for sent in text if len(sent) > 5]\n\ttext = [sent.replace(\"(\", \"\").replace(\")\", \"\").replace(\"&\", \"\").replace(\" \", \"\").strip() for sent in text]\n\ttext = [' '.join([subdoc[tokenizer.connect(token, is_first=True)] if token.startswith('&') else token for token in sent"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tcontent_type = None\n\n\tif \"?\" in url:\n\t\tpars = urlparse(url)\n\t\tif len(pars) > 2:\n\t\t\treturn pars.query\n\t\tquery = urlparse(url).query\n\telif \"=\" in url:\n\t\tquery = urlparse(url).query\n\telse:\n\t\treturn None\n\n\treturn query\n\n\ndef parse_content_type(content_type: str) -> Union[str, None]:\n\t\"\"\"\n\tRetrieve the content type of the content of the remote url.\n\t\"\"\"\n\tif content"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(url, headers={'Accept': 'text/html'})\n\t\t# Check that the status code is 200 if it was received\n\t\tif response.status_code!= httpStatus.HTTPStatus.OK:\n\t\t\tlog.log(event=\"http error\", status=httpStatus.HTTPStatus.UNKNOWN, logLevel=\"WARN\").error(f\"{url} - HTTP {response.status_code} - HTTP body: {response.text}\", data={})\n\t\t\treturn None\n\t\tif not response.content:\n\t\t\tlog.log("}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\theaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}\n\tresponse = requests.head(url, headers=headers, timeout=16)\n\treturn response.headers['content-type'] if 'content-type' in response.headers else ''\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\thttp_connection = HTTPConnection(\n\t\t\t_get_connection_from_url(url) if _get_connection_from_url(url) else None,\n\t\t)\n\t\t# The request headers are already set in the body for POST requests.\n\t\t# So we don't need to set the content-type of the headers.\n\t\tresponse: Optional[HttpResponse] = None\n\t\tmethod: str = \"\"\n\n\t\tif \"HEAD\" not in _set_options(url):\n\t\t\tmethod = \"POST\"\n\n\t\tresponse, start = _handle_request(\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t_, headers, error, response = fetch_response(url, method=\"HEAD\",\n\t\tsession=None,\n\t\tjson_feed=False,\n\t\tparams=None,\n\t\tparams_feed=None,\n\t\tparams_query=None)\n\ttry:\n\t\treturn response.headers[\"content-type\"]\n\texcept KeyError:\n\t\tprint(url)\n\t\tprint(response.url)\n\t\tif error:\n\t\t\tprint(error)\n\t\t\treturn None\n\t\treturn None\n\n\ndef normalize_url(url: str, feed_type: str, params:"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tif '://' in url:\n\t\t_, path = url.rsplit('/', 2) \n\t\tif path.startswith('js'):\n\t\t\treturn 'application/javascript'\n\t\telif path.startswith('php'):\n\t\t\treturn 'text/html'\n\t\telif path.startswith('csv'):\n\t\t\treturn 'text/csv'\n\t\telif path.startswith('sql'):\n\t\t\treturn 'text/sql'\n\t\treturn None\n\ndef http_get(url: str, headers: Dict = None"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treturn Response.fetch_content_type(url, headers=HEADERS)\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tif url.startswith(f\"{CONFIG.HTTP_BASE_URL}/\"):\n\t\treturn \"application/json\"\n\telse:\n\t\treturn None\n\ndef fetch_header(url: str) -> Optional[str]:\n\t\"\"\"\n\t\tFetch the requested content type from HEAD of URL to determine it as the source of the file.\n\t\"\"\"\n\tif url.startswith(f\"{CONFIG.HTTP_BASE_URL}/\"):\n\t\treturn \"application/x-www-form-urlencoded\"\n\telse:\n\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treturn requests.head(url, stream=True, timeout=(1, 10))\n\n\ndef fetch_get_list_info(\n\turl: str,\n\thttp_session: Optional[requests.Session] = None,\n) -> Tuple[List[str], List[str]]:\n\t\"\"\"\n\tFetches the content type and content for a GET operation at the given url.\n\tReturns an list with the urls and the content type.\n\t\"\"\"\n\turl = urllib.parse.urljoin(url, \"content-type\")\n\tcontent_"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t# pylint: disable=line-too-long,protected-access\n\timport httpx\n\n\turlpath = urlparse(url).path\n\n\t# Make HTTP request and parse result\n\tresp = httpx.get(url, timeout=0)\n\theaders = resp.headers\n\tcode = headers.get('Content-Type', None)\n\tif urlpath and code is not None:\n\t\n\t\t# Extract content type (like `application/json;q=0.001`)\n\t\tif code is None:\n\t\t\tcode = headers.get('Content-Type"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif not tag.startswith(string.digits): # not on $\n\t\tprint(\"tag not recognized\")\n\t\treturn\n\n\tstart = tag.find(\"$\") + 1\n\twhile (start < len(tag)): # not on end\n\t\tif ord(tag[start])!= 0x20: # not on 0x20\n\t\t\treturn False\n\t\tstart += 1\n\treturn True\n\n\ndef find_tag(word: list[str]) -> bool:\n\t\"\"\"\n\tTest a word is the correct tag.\n\t\"\"\"\n\tfor tag in string.dig"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif word in tag:\n\t\treturn True\n\telse:\n\t\treturn False\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\"\"\"\n\t\tTEST CASE\n\t\"\"\"\n\n\n\n\t\"\"\"\n\t\tTest the word against the regular words.\n\t\tChecking on their position (which should map a word to a set of tags).\n\t\tIf a match appears for all it's sub words (that's not one tag like 'the light'), then True will be returned.\n\t\"\"\"\n\tpassed = True\n\n\tfor word in range(len(words)):\n\t\tif words[word].pos not in self._tags:\n\t\t\tpassed = False\n\t\t\t# if it's a part of a tag, remove the first tag"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn tag in config['tags']\n\ndef _search_word(tag: str) -> tuple[str, int[], int]:\n\"\"\"\n\tSearch word, return a couple of tuples.\n\t:param tag:\n\t:description:\n\t\t- *pos* = (word position, line number)\n\t\t- *tag* = current tag, 0 if not exists\n\t:return: \n\t\t- *word* = word that is found\n\t\t- *pos* = positions for each word (line, word)\n\t\t- *tag* = tags for each word\n\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tglobal known_words\n\tassert tag in known_words\n\treturn True\n\n\n@pytest.fixture\ndef tag(tag_set: Set[str]) -> None:\n\t\"\"\"\n\t\tSet of the tags (sets) known to the tag scanner.\n\t\"\"\"\n\tassert len(tag_set) > 0\n\ttag_set: Set[str] = copy.copy(tag_set)\n\ttag_set: set[str] = set()\n\tfor t in tag_set:\n\t\ttag_set.remove(t)\n\t\ttag_set."}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn tag in tags_str\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn bool(' '.join(tag).isalpha())\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn hasattr(tag, \"__tags__\")"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn tag.lower() in WordExtraction.TAG_ENDS\n\n\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tpass\n\ndef test_title(title: str) -> bool:\n\t\"\"\"\n\t\tTest a title is recognized as being a title of a sentence.\n\t\"\"\"\n\treturn f'{test_title(title)}'\n"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif node.text is not None:\n\t\treturn {\n\t\t\t'text': node.text.replace('&#39;', '\\''),\n\t\t\t'parsed_attribute': getattr(node, 'parsed_attribute', None),\n\t\t}\n\telse:\n\t\treturn None\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tvalues  = []\n\tvalue  = []\n\tfor child in node.iter():\n\t\tif not isinstance(child, AttributeListElement) and not isinstance(child, NameNode) and child is not None:\n\t\t\tvalues.append(child.string)\n\t\telse: value.append(child.value)\n\td = dict(zip(node.tag.split(), value))\n\tchildren = {str(k): v for k, v in d.items()}\n\treturn children\n\t\t\ndef xml_attributes(node):\n\tvalues  = []"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tcntr = getattr(node, 'attrib', {})\n\tresult = {}\n\tfor key, value in cntr.items():\n\t\tif key!= '@xml_ns':\n\t\t\tif not value:\n\t\t\t\t# not a list element, no child element, then there is only one child element\n\t\t\t\tresult[value] = (KEY_NODE, value, {})\n\t\t\telse:\n\t\t\t\ttagname, child = value\n\t\t\t\t# check if there's a child that's nested, otherwise ignore it.\n\t\t\t\twhile not isinstance(child, tuple):\n\t\t\t\t\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\ttag = '{%s}child' % xmlns(node)\n\treturn dict([(el.tag, [v for v in el.getchildren()]) for el in node])\n\nclass tagxpath_parser(tagxpath):\n\t\"\"\"\n\t\tA tagxpath parser for a tagxpath subclass, which is essentially a lightweight\n\t\tdom interface, which represents an XPath context object, and which can be\n\t\tused interactively via XML parsers (such as those for xmles.parsers.XMLPath).\n\n\t\tSubclassing can be used to"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\treturn { 'xml': { 'children': {k: xml_children_as_dict(k) } } }\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tdef tag_name(node):\n\t\tif node.tag in [TaggingNamespacePrefix, LENGTHNode, IDNode, STRINGNode, TEXTNode]:\n\t\t\treturn node.tag\n\t\telse:\n\t\t\treturn 'unknown'\n\t\t\n\tdef return_node_as_dict(node):\n\t\treturn [return_node_as_dict(child) for child in node]\n\t\t\n\tdef has_children(node):\n\t\treturn len(node) and len(node[0]) > 0\n\t\t\n\t# Recursively get all children.\n\t\treturn"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t# XXX: Should we want some sort of dictionary of the type?\n\treturn {}\n\ndef get_xml(node):\n\t\"\"\" Get an XML representation for a child node.  This may be useful in case you want\n\t\tto use more than one output stream.\n\t\t\n\tnode:\n\t\tA ElementTree root node\n\t\t\n\tReturns:\n\t\tA string of the XML to be printed out\n\t\"\"\"\n\ttag = node.tag\n\ttag = tag.replace(TAG_SEPARATOR, \"\")\n\ttag = tag.replace(\".\", \"_\")\n\tchild"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn [{'tag' : child.tag, 'text' : child.text} \n\t        for child in node.iterchildren()]\n\ndef xml_as_dict(node):\n\t\"\"\"\n\tTurn node <xml> down into a dict, keyed by tag and text.\n\t\n\tThis is a slightly more shallow conversation, but it does not recurse through all the children.\n\t\"\"\"\n\ttext = xml_as_string(node)\n\t# text = str(node)\n\ttext = \"{%s}\" % ''.join(map(lambda"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t# First, figure out the list of node_idx -> children entries.\n\tchildren = node.getchildren()\n\tdict_nodes = {}\n\t# The parent's children are already iterables so we do not have to construct a list.\n\tnode_idx_to_child = {}\n\t# Loop all the children, checking for each with the associated tag and value.\n\tfor child in children:\n\t\tif child is None:\n\t\t\tcontinue\n\t\tif node_idx_to_child:\n\t\t\tassert node_idx_to_child[child.tag"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif not node:\n\t\treturn {}\n\t\n\tchildren_dict = collections.defaultdict(lambda: \"<{}>\" * len(node))\n\tfor child in node.iter_children():\n\t\tif not hasattr(child, \"children\"):\n\t\t\tchild = child.data\n\t\t\tchildren_dict[\"<%s>\" % child[0]](\"\", child)\n\t\telse:\n\t\t\tfor tag, value in zip(*[ (c.tag, getattr(c, c.tag, None)())\n\t\t\t\t\t  for c in child.children]):\n\t\t\t\tchildren_"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tsender_handle = str(sender_handle)\n\tentity_handle = str(entity_handle)\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\t# I suppose that our payload is in all kinds. Should there be anything in payload if payload is here:\n\t\t#\tsending a bunch of this payload. We should only need to check sender and entity handle match if sending to\n\t\t#\tsender = something.\n\t\tsender, entity, sender_entity_handle = sender_handle, entity_handle, sender_handle\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif not sender_handle:\n\t\treturn\n\telif sender_handle.name == entity_handle.name:\n\t\tsender_handle.write(\"<DiasporaObjectHandle> %s</DiasporaObjectHandle>\" % entity_handle.type())\n\t\tsender_handle.write(\"</sender-handle-name-in-receiver-object-payload-data>\")\n\telif entity_handle.name == 'DiasporaObjectHandle' and entity_handle.type() == \"DIASPOTRIEP\":\n\t\tobject_handle = Dias"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif len((sender_handle, entity_handle))!= 2:\n\t\treturn True, \"\"\n\n\tverify_sender_and_entity_handle_mismatch(sender_handle, entity_handle, sender_handle == entity_handle.name, False)\n\treturn True, \"\"\n\ndef verify_sender_and_entity_handle_mismatch(sender_handle, entity_handle, sender_handle_same, entity_handle_same):\n\t\"\"\"\n\t\tVerify the sender and entity handles are equal or not.\n\t\n\t\tWe're"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tassert sender_handle!= None\n\tassert entity_handle!= None\n\t\n\t# Handle a single send message.\n\tif sender_handle == object.__dict__['handle']:\n\t\tsender_handle.handle_msg(None,None,None)\n\t\t# Get the sender and sender_msg\n\t\tsender = sender_handle\n\t\tsender_msg = sender.msg\n\n\t\t# Get the entity and entity_msg.\n\t\tentity = entity_handle\n\t\tentity_msg = entity.msg\n\n\t\t# Do some additional checks.\n\t\tif sender.msg"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif \"sender\" not in entity_handle:\n\t\traise ValueError(\"The'sender' header was not found in the content body.\")\n\tif \"$sender\" + entity_handle not in entity_handle:\n\t\traise ValueError(\"In payload, the sender header should be the sender for the target.\")\n\n\tif \"sender_handle\" not in sender_handle:\n\t\traise ValueError(\"The'sender_handle' header was not found in the content body.\")\n\tif \"$sender_handle\" + sender_handle not in sender_handle:\n\t\traise Value"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\tif (entity_handle\n\t        and sender_handle == entity_handle.sender and sender_handle.sender_email == entity_handle.entity_email):\n\t\tsender_handle = None\n\t    \n\tif sender_handle and entity_handle.entity_handle_entity_email == sender_handle.object_entity_handle.entity_email:\n\t\t# We want to get the sender's handle but it's not in the entity, so we'll fix it.\n\t\tsender_handle = None \n\t    \n\tif sender_handle:\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= \"NONE\":\n\t\tsender_id = getidby_hash(handle=sender_handle)\n\t\tassert sender_id, \"'s sender handle is missing! Please add it with the handle argument to the send() method\"\n\telif not sender_handle:\n\t\traise DiasporaException('Failed to parse sender_handle from input in send.')\n\telif entity_handle!= \"NONE\":\n\t\tentity_id = getidby_hash(handle=entity_handle)\n\t\tassert entity_id, \"'s entity handle is"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\traise ValueError('Invalid sender or entity: %s' % entity_handle)\n\t\n\tif sender_handle == '':\n\t\treturn sender_handle\n\telse:\n\t\tif receiver_handle == '':\n\t\t\traise ValueError('Invalid receiver: %s' % receiver_handle)\n\t\t\n\treturn sender_handle\n \ndef is_valid_event(event):\n\treturn not event == None and type(event.get('_sender')) == str and type(event.get('_receiver'))"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle is not None and entity_handle is not None:\n\t\tif isinstance(entity_handle[\"name\"], list):\n\t\t\tif entity_handle[\"name\"] == sender_handle and entity_handle[\"namespace\"] == \"diaspora:author\":\n\t\t\t\t# Since all sentenals are from the same sender and their namespace is Diaspora, it might be an entity handle\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# We might have set one another author in the payload headers, or an entity handle. In those cases we\n\t\t\t\t# need to ensure that both sender and source match"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t# TODO:  Should we do the match here? This isn't quite right: we might not allow certain types of requests to fail\n\t#       without the check. But the most common case is not this case, i.e. we must match the sender and send\n\t#       the payload to the new entity. Maybe this should be a case specific match.\n\n\tassert isinstance(sender_handle, str), '\"sender_handle\" must be a string'\n\tassert isinstance(entity_handle, dict), '\"entity_handle\" must be a dict'\n\tassert"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t# TODO: maybe use an HTTP connection to fetch in-memory\n\t#       url content, e.g.:\n\t#\t\tnodeinfo.content = '{\\\\\"nodeinfo\\\\\":\\\\\"nodeinfo\\\\\"></\\\\\"nodeinfo\\\\\"></\\\\\"nodeinfo\\\\\\\\\\\\\"nodeinfo\\\\\"'\n\n\tif document_path is not None:\n\t\tnodeinfo = \"\".join(('&nodeinfo1=/file:/nodeinfo/',\n\t\t\t\t\t\t  '{\\\\\"nodeinfo1\\\\\":\\\\\"',\n\t\t\t\t\t\t  '%s\\\\\"' % document_path,\n\t\t\t\t\t\t  '"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\treturn {\"type\": \"node_info\", \"documentPath\": document_path}\n\n\ndef get_vfs_baseurl_from_url(url):\n\t\"\"\"\n\tReturns uri portion of VFS BaseURL as a string.\n\t\n\t:arg url: The base url of the service\n\t:returns: str\n\t\"\"\"\n\tr = urllib.request.urlopen(url)\n\traw = r.read()\n\tresult = raw.decode('utf-8')\n\treturn result\n\n\ndef get_vfs_baseimage_url"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is None:\n\t\tbase = url\n\telse:\n\t\tbase = document_path\n\turl = os.path.join(base, URNS[\"nodeinfo\"])\n\tresponse = urllib.request.urlopen(url)\n\tfull_path = None\n\tif response.info().get_content_type() == \"text/plain\":\n\t\tf = BytesIO(response.read())\n\t\tfull_path = f.name\n\t\tf.close()\n\telif response.info().get_content_type() == \"text/"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\tif document_path is None:\n\t\t_nodeinfo = _generate_nodeinfo(url)\n\telse:\n\t\t_nodeinfo = _load(document_path)\n\t\n\treturn _nodeinfo['nodeinfo']['_well_known_doc']['path']\n\t\ndef get_nodeinfo_unsupported(url, document_path=None):\n\t\"\"\"\n\tGenerate a NodeInfo.well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\timport os\n\t\n\tif document_path is None:\n\t\tdocument_path = urljoin(url, 'info.well-known')\n\telif 'path' in url.attributes:\n\t\tdocument_path = urljoin(url, 'info.well-known/path/%s' % os.path.normpath(url.attributes['path']))\n\telse:\n\t\tdocument_path = urljoin(url, 'info.well-known/path/%s' % os.path.normpath(url.attributes['path']))\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tdef _make_path_segment(p):\n\t\turls = []\n\t\tfor s in p.iter_segments():\n\t\t\tif 'file:///' in s.url:\n\t\t\t\turls.append('/'.join(s.path))\n\t\t\telif '/docs/' in s.url:\n\t\t\t\turls.append('docs/').joinpath(s.path)\n\t\t\telse:\n\t\t\t\turls.append('www.nodeinfo.com').joinpath(s.path.rstrip('/'))\n\t\treturn ':'.join(urls"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tdocinfo = dict()\n\ttry:\n\t\tresponse = requests.get(url,headers={'Content-Type': 'application/json'})\n\texcept requests.ConnectionError:\n\t\tdocinfo['error'] = \"Couldn't connect to the NodeInfo server\"\n\t\treturn docinfo\n\tif response.status_code == 404:\n\t\tdocinfo['error'] = f\"Invalid URL: {response.url}\"\n\t\treturn docinfo\n\tdocinfo['version'] = response.headers['X-Server-Version']\n\tdocinfo['time-created'] = (response"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is not None:\n\t\tassert len(document_path.split(\"/\"))==2 # We just require a specific filepath\n\t\tnodeinfopath = document_path\n\telse:\n\t\tnodeinfopath = \"NodeInfo.{:04d}.{:04d}.well-known\".format(int(round(random.random())), int(round(random.random())))\n\turlparsed = urllib.parse.urlparse(url)\n\tparsed = urllib.parse.parse_qs(urlparsed.query"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is not None:\n\t\turl = \"%s/%s\" % (url, document_path)\n\tresponse = requests.get(url, stream=True)\n\tresponse.raise_for_status()\n\treturn response.json()\n\t\ndef is_valid_url(url, validate=False, use_node_info_path=False):\n\t\"\"\"\n\t\tCheck if a url is a valid URL\n\t\t\n\t\t:arg url: The url to check (required)\n\t\t:arg validate: If true tries to validate the url if the url is"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\treturn get_nodeinfo_document(url, \"NUInfo Wells-Known\", document_path)\n\t\ndef get_nodeinfo_validation_document(url, document_path=None):\n\t\n\treturn get_nodeinfo_document(url, \"NUValidation\", document_path)\n\t\ndef get_nodeinfo_document(url, node_type, document_path=None):\n\t\n\tif not document_path:\n\t\tdocument_path = url.rstrip('/')\n\t\n\tdocument_path = document_path."}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif len(doc)!= 2:\n\t\t# Not sure as to why this is a bad format\n\t\tlog.info('XML %s - %s', doc, signature)\n\t\treturn False\n\t\n\t# Incomplete XML\n\tif doc is None:\n\t\tlog.error(signature)\n\t\treturn False\n\t\n\t# XML tag must be the version line\n\tif not doc.tag.endswith(\"version\"):\n\t\tlog.error(signature)\n\t\treturn False\n\n\t# xml tag must contain a version number\n\txml = doc.find"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tmsg_dct = parse_xml(doc)\n\n\tfor item in msg_dct['signed']['elements']:\n\t\tdata_sig = msg_dct['signed']['elements'][item]\n\t\tif not data_sig.get('checked') or not data_sig.get('checked')['value']:\n\t\t\tcontinue\n\t\tif public_key in data_sig['checks'].keys() and data_sig['checks'][public_key]['signed']:\n\t\t\treturn data_sig['checks'][public"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif not signature['v'].startswith('2.0'):\n\t\t# we are running on our local machine\n\t\t# or some other machine, where we can run the\n\t\t# server locally (which we might want to\n\t\t# replace)\n\t\treturn SignatureResult.not_supported(doc, signature)\n\n\treturn SignatureResult.verify(public_key, signature['v'])\n\n\nclass SignatureResult(object):\n\t\"\"\"\n\tContainer classes for errors returned in SignatureResult.verify.\n\t\"\"\"\n\n\tdef __init__(self, not_supported,"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tassert(type(public_key) == type(bytes(public_key, 'utf-8')))\n\tassert(type(doc) == bytes)\n\tassert(type(signature) == type(bytes(signature, 'ascii')))\n\tdata = signature_to_data(signature, len(signature))\n\treturn verify_signature(public_key, doc, data)\n\n\ndef verify_signature(public_key, document, signature):\n\t\"\"\"\n\tVerify the XML elements describing the underlying document.\n\tReturn true if the"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif signature.get('keyID') is None:\n\t\treturn False\n\texpected, actual = signature['keyID'], public_key['keyID']\n\ttry:\n\t\tjwk1 = load_jwk(doc.find(\"%s:jwk\" % expected_jwk_namespace))\n\texcept ValueError as e:\n\t\traise NotVerifiableXMLError(\"failed to parse %s:jwk:\" % expected_jwk_namespace, ''.join(doc.itertext())) from e\n\telse:\n\t\ttry:\n\t\t\tjwk"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t# Verify signatures\n\tsignatures = []\n\tfor sig in signature['signatures']:\n\t\tsignatures.append(sig)\n\tif not signatures.sort(key=lambda x: -x['confidence'], reverse=True):\n\t\tsignature = []\n\t# Verify signature\n\tfor sig in signatures:\n\t\tif sig['sender']!= public_key:\n\t\t\traise Exception('Not a valid signers from {sender} : {sig}, {signatures}')\n\n\n\t# Confirm signatures\n\tfor sig in signature:\n\t\t# If not"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\timport base64\n\t# print(id(signature))\n\t\"\"\"\n\t\tImportantly we make our own signature to check if the XML looks\n\t\tlike what we expect.  We can either save the parsed document\n\t\tin a temporary file for writing, or to use an external\n\t\tsigver to send the signed XML to other systems.\n\t\"\"\"\n\ttry:\n\t\timport tempfile\n\t\timport os\n\t\tfrom xml.sax.handler import XMLHandler\n\t\tfrom xml.sax import saxutils\n\n\t\tclass EncodingSaxHandler(XMLHandler):\n\t\t\t# override"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\treturn (\n\t(doc+signature).find('signature').attrib['algorithm'] == 'rsa:' or\n\tdoc.find('signature')\n\t)\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif signature.is_signed() is False and signature.is_valid() is False:\n\t\t# We just need the message\n\t\treturn msg.signed(document, sig_type='signature', signing_key=public_key,\n\t\t\t\t\t\t\t\t\t   sig_msg=sig_info)\n\n\t# We have valid authn-data, but no signature, make sure there is no further\n\t# signature (that might be valid in the future)\n\tif signature.is_valid() is False:\n\t\treturn msg.unsigned(doc)\n\n\t# We have valid key, make"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tsig_str = '{signature_key:02x}'+doc\n\tsig=binascii.hexlify(sig_utils.decrypt_msg(sig_utils.sasl_key, sig_str, public_key, as_binary=True))\n\t# signature string must equal signature_key\n\tif sig!=signature:\n\t\traise Exception(\"Could not verify signature\")\n\ndef validate_author(pub_key, auth_data):\n\t\"\"\"\n\t\tGiven a public key string representing a node in the\n\t\t"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tf = open(document, \"rb\")\n\tf_size = os.fstat(f.fileno()).st_size\n\t\n\tresponse = requests.compat.urlopen(request=b\"POST https://diaspora.github.io/diaspora-federation/discovery/webfinger.json\", chunk=1024)\n\tdata = response.read().decode()\n\tf.close()\n\t\n\ttry:\n\t\td = json.loads(data)\n\texcept ValueError:\n\t\treturn None\n\t\n\theaders = {\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t# Parse the JSON file.\n\twith tempfile.NamedTemporaryFile(mode=\"r\") as data:\n\t\tdata.write(document.encode(\"utf-8\"))\n\t\tdata.flush()\n\t\t\n\t\t# Download the file & parse it.\n\t\tresp = requests.get(DATA_URL + data.name)\n\t\ttry:\n\t\t\tresp.raise_for_status()\n\t\texcept requests.HTTPError as e:\n\t\t\traise Exception(f\"Unable to download [{data.name}]: {e}\") from e\n\t\tdata"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\td = Document(document=document,\n\t\ttarget=\"/diaspora/federation/documents\",\n\t\tname=\"/diaspora/federation/documents\",\n\t\turl=\"/diaspora/federation\")\n\td.parse_xml()\n\t# Remove any <diaspora.federation> element\n\tf = d.doc\n\tfor child, attr in f.items():\n\t\tchild = child.replace(\n\t\t\t\"<diaspora.federation>\", \"\") if child.tag == \"diaspora:federation\" else \"\")\n\t\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tcontent: str = \"\"\n\td: Dict = {}\n\tdoc: Dict = {}\n\tif document.startswith('http'):\n\t\tdoc['url'] = document\n\t\t\n\t\t# Parse the document\n\t\tfor part in content.split('\\n\\n'):\n\t\t\tif part.strip():\n\t\t\t\tcontent = part\n\t\t\t\tbreak\n\t\t\n\t\tcontent = content.replace(r'\"', '&quot;').replace(r'\\?', '&amp;').replace(r'\\!', '&apos;').replace(r'"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdas: dict = {}\n\tfor line in document.splitlines():\n\t\tif not line.startswith(\"//\"): continue\n\t\tline = line[1:len(line)-1]\n\t\ta: str = line.split(\" \")\n\t\tdas += parse_diaspora_dicom(a[-1])\n\treturn das\n\ndef parse_diaspora_dicom(dicom_line: str) -> Dict[str, Any]:\n\t\"\"\"\n\tParse Diaspora dicom-style line.\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\treturn parse_diaspora(document)\n\ndef generate_dicom_xml(dicom_doc):\n\t\"\"\"\n\tGiven a XML object returned by an HTTP request, get a dictionary with only valid\n\tdictionary properties.\n\t\"\"\"\n\tif not isinstance(dicom_doc, dict):\n\t\treturn None\n\telif not dicom_doc:\n\t\treturn None\n\t\n\t# TODO: remove the redundant return statement\n\tdicom_doc = str(dicom_doc)\n\t\n\t# convert data type from"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t# First we parse the string, because it might be in an old format\n\ttry:\n\t\tdata = etree.fromstring(document)\n\texcept Exception:\n\t\treturn None\n\n\t# Only process JSON files\n\tif not data.get('status') or data.get('status').get(b'name') == b'xrd_federation' or data.get('federated') is True:\n\t\treturn None\n\n\t# Only process XML files\n\tif not data.get('status') or data.get('status').get(b'name') in [b"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\timport json\n\tjson_response = json.loads(document)[0]\n\n\tif json_response == 'json_diaspora_webfinger.json':\n\t\tresult = json_response\n\n\t\tif result and json_response['data'].get('dictionaries'):\n\n\t\t\tdomains = json_response['data']['dictionaries'].get('domains')\n\t\t\tfor entry in domains:\n\t\t\t\tif entry['type'] == 'xml':\n\t\t\t\t\tdata = entry['data']\n\t\t\t\t\tif entry['type']!= 'xml' and 'title' in result:"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t# First convert the string to a JSON list to work with\n\tdata = document.split(\"/\")\n\t\n\t# Next, I'll decode them into JSON-formatted lists\n\t# https://diaspora.github.io/diaspora_federation/discovery/xml_dumps.html \n\ttry:\n\t\traw_data = [doc.decode(\"utf-8\") for doc in data]\n\texcept UnicodeDecodeError: # pragma: no cover\n\t\tprint(\"ERROR: Decoding XML not possible!\")\n\t\treturn None\n\t\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\timport requests\n\t\n\t# Diaspora uses \"dai\" as schema type to identify documents as documents\n\tWEBFINGER_SCRIPT = '/dai/document/'\n\tdai_schemata = {'federation_id': 'federation_document',\n\t\t'identification': 'f:ID',\n\t\t'document_type': 'f:T','id': 'f:V'}\n\ttry:\n\t\tresponse = requests.get('https://diaspora.github.io/diaspora_federation')\n\t\tif response"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tos.system(f'webfinger -v {handle}')\n\texcept:\n\t\treturn None\n\treturn get_file_content(handle)\n\n\ndef try_download_file(url: str) -> Optional[str]:\n\t\"\"\"\n\tTry to download a file from an url using an http (or https) http request.\n\t\tExceptions return HTTP error code 204 (File already exists), 404 (File does not exist), 401 (Unauthorized) HTTP code, 410 (Gone). Other exceptions are logged.\n\t\"\"\"\n\t# try to download"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\traise NotImplementedError()\n\n\ndef try_create_webfinger_document(handle: str) -> Document:\n\t\"\"\"\n\tCreates a new WebFinger document. Does not raise if it succeeds.\n\t\"\"\"\n\traise NotImplementedError()\n\n\ndef try_create_webfinger_document_from_name(handle: str, name: str) -> Document:\n\t\"\"\"\n\tCreates a new WebFinger document associated with a name. Does not raise if it succeeds.\n\t\"\"\"\n\traise NotImplementedError()\n\n\ndef try_"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn None\n\n\ndef try_get_name(response: bytes) -> Optional[str]:\n\t\"\"\"\n\tTry to extract content-type line from the given response.\n\t\"\"\"\n\tif b'text/plain' in response:\n\t\tif b'content-type' in response:\n\t\t\treturn ''\n\t\treturn None\n\treturn None\n\n\ndef save_webfinger_document(handle: str, text: str) -> None:\n\t\"\"\"\n\tSave the given string in the given handle.\n\t\"\"\"\n\thttpd = Server(0.0.0.0"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tdoc = Document.from_url(Webfinger_URL, retry=False)\n\n\t\tif doc.is_valid():\n\t\t\treturn doc.file_location\n\t\telse:\n\t\t\treturn None\n\texcept NotFoundException:\n\t\treturn None\n\n\ndef create_webfinger_database() -> Dict[Hashable, str]:\n\t\"\"\"\n\tCreates an empty database.\n\t\"\"\"\n\t# First, create a file on the server\n\tdb = Database(\n\t\tpath=Webfinger_URL,\n\t\tfile_location=WebFingerCachePath"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tif not \"http://www.rfc-editor.org/rfc/rfc7033.txt\" in handle:\n\t\traise ValueError(\n\t\t\tf\"Cannot download webfinger document. The handle passed is not correct. \"\n\t\t\tf\"Please try again. If no document at {handle} is in place it should be of the form: \"\n\t\t\tf\"http://www.rfc-editor.org/rfc/rfc7033.txt\"\n\t\t)\n\n\t# Try downloading it as an RTIF3.txt document.\n\ttry:\n\t\twith"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn get_webfinger_document(handle)\n\n\n# ---------------------\n#   FACILITY\n\ndef is_web_document(handle: str) -> bool:\n\t\"\"\"\n\tChecks that a document is not a web fingerprint type.\n\t\"\"\"\n\tif not len(handle):\n\t\treturn False\n\n\t# Handle a \"http://\" type handle\n\tif handle.startswith(\"http://\"):\n\t\thandle = handle[4:]\n\n\t# Check for a dot\n\tif handle.top(-2).startswith(\".\"):\n\t\thandle ="}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\thandle = handle.strip().encode('ascii', 'ignore')\n\ttry:\n\t\tresponse = requests.get(TEST_URL + TLD_URL + HOST_URL + TLD + handle)\n\t\treturn response.content\n\texcept:\n\t\treturn None\n\ndef parse_rfc7033_response_content(raw_tld: str, tld: str, html_content: str) -> Optional[str]:\n\t\"\"\"\n\t\tParse the RFC7033 webfinger document response content.\n\t\t1. Extract the page content by extracting the URL"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tprint(f\"try_retrieve_webfinger_document() starting to process ID '{handle}'\")\n\ttry:\n\t\twebfinger_document = get_webfinger_document(handle)\n\t\tlogf(f\"weburl='{webfinger_document.url}'\")\n\t\treturn webfinger_document\n\texcept errors.UnauthorizedException in exc_info():\n\t\tprint(f\"    Unauthorized Exception from try_retrieve_webfinger_document()\")\n\t\traise errors.UnauthorizedException\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = URL(f\"http://{handle}\")\n\thtml = urllib.request.urlopen(url).read()\n\tsoup = BeautifulSoup(html, \"html.parser\")\n\n\treturn soup.find_all(\"p\", class_=\"doc-elem\")[0].contents[0].replace(\"\\n\", \"\").strip()\n\n\ndef upload_webfinger_document(handle: str) -> None:\n\ttry:\n\t\t# Get the webfinger document\n\t\tdocument = try_retrieve_webfinger_document(handle)\n\n\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tdoc = get_webfinger_doc(web_finger_url=WebFinger.RDF_URI, web_finger_md5=WebFinger.MD5_HASH, handle=handle)\n\tif doc is None:\n\t\treturn None\n\ttry:\n\t\tdoc = base64.standard_b64decode(doc)\n\texcept Exception:\n\t\treturn None\n\treturn doc\n\ndef try_retrieve_rss_feed_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 RSS feed. Does"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tdata = requests.get(handle)\n\n\treturn parse_diaspora_webfinger_document(data.text)\n\t\n\ndef download_diaspora_webfinger_document(handle, download_dir):\n\t\"\"\"\n\tDownload a remote Diaspora webfinger document.\n\n\t:arg handle: Remote handle to retrieve\n\t:arg download_dir: Directory to download the output files to\n\t\"\"\"\n\tdata = retrieve_and_parse_diaspora_webfinger(handle)\n\tif not len(data['diaspora_documents']):\n\t\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\thandle = _parse(handle, 'diaspora webfinger')\n\treturn {}\n\ndef retrieve_webfinger_fingerprint(handle, fingerprint):\n\t\"\"\"\n\tRetrieve a webfinger fingerprint.\n\t\n\t:arg handle: Handle of the document\n\t:arg fingerprint: String, fingerprint of the document\n\t:returns: Dict\n\t\"\"\"\n\treturn _retrieve_document(handle, _DIA_FP, fingerprint)\n\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treq = Request(''.join([\n\t\t'<!-- We are being asked to return data from',\n\t\t'http://{}'.format(_handle_url(\"www\", handle)),\n\t\t'/diaspora',\n\t\t''.join([\n\t\t\t'<article.bibliograph',\n\t\t\t'id=\"bibliograph\"',\n\t\t\t'j=',\n\t\t\thandle.lower(),\n\t\t\t'name',\n\t\t\t'\"',\n\t\t\thandle,\n\t\t\t'\"',\n\t\t\t'/',\n\t\t\t'number', '\"',\n\t\t\t'\"',\n\t\t\t'\"',\n\t\t\t'/',\n\t\t\t'b"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t# Try a GET request to our Diaspora webfinger webstore\n\twebapi = DiasporaWebFinger(handle)\n\treturn webapi.retrieve_and_parse()\n\t\ndef read_config_from_env(env):\n\t\"\"\"\n\t\tRead user's environment variables.\n\t\n\t:arg env: Environment variable for env. This file is checked for access.\n\t\n\t:returns: Dict\n\t\"\"\"\n\t\t\n\ttry:\n\t\t# Import the necessary packages.\n\t\timport environ\n\t\timport os\n\t\t# En"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treturn __retrieve_and_parse_diaspora_webfinger(handle)\n\t\ndef __retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\t\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t\t:arg handle: Remote handle to retrieve\n\t\t:returns: dict\n\t\"\"\"\n\tpath = \"/webfinger/%s\" %handle\n\t\n\t# GET\n\tr = requests.get(\"https://%s'weblink'\"%path, stream=True)\n\t\n\t#"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\ts = socket.socket()\n\ts.connect((DAS_HOST, DAS_PORT))            \n\ttry:\n\t\ts.send(encode(handle))\n\t\tdata = wait_for_response(s)\n\texcept Exception as e:\n\t\terr = e.args[0]             \n\t\ts.close()\t\t\t\n\t\traise err\t\n\t\n\treturn das2json(dict(parse=data))\n\t\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t# Construct the full URL\n\tfull_url = 'https://diaspora-api.wikidata.org/w'\n\t# Make the API call\n\tretval = retrieve_and_parse_diaspora_webfinger(handle, full_url)\n\t# Return either the document information, or an empty dict if fail\n\treturn retval or {}\n\n\ndef retrieve_and_parse_diaspora_webfinger(handle, full_url):\n\t'''\n\tRetrieve a document by handle and parse the file pointed to by handle.\n\thandle:"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tlogger.info(f'Fetching {handle} via webfinger (retrieves and parses)')\n\turl = f'http://ftp.invaliddiaspora.com/{os.path.basename(handle)}'\n\turlparse = urlparse.urlparse(url)\n\tif urlparse.scheme == 'ftp':\n\t\turl = f'{urlparse.scheme}://{urlparse.netloc}/{os.path.basename(handle)}'\n\telif urlparse.scheme == 'http':\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\td = {}\n\turl = \"https://www.wikidata.org/w/\" + handle + \"/dictionary/\" + handle + \".xml\"\n\tr  = requests.get(url)\n\ttree = ET.fromstring(r.content)\n\tfor k in tree.iter(\"data\"):\n\t\tfor r in tqdm.tqdm(k.iter(\"value\"), total=10):\n\t\t\td[\"url\"] = k.attrib[\"value\"].replace(\"wikipedia :\",\"\").replace(\"wiki=\",\"\").replace(\"_\", \" \")\n\t\t\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tcmd_args = [\"diaspora\", \"-w\", handle]\n\tresult = subprocess.check_output(cmd_args).strip()\n\tif result == \"\":\n\t\treturn {}\n\tlines = result.decode('ascii').split(\": \")\n\ttry:\n\t\tline = lines[1].strip()\n\t\tm = Dic_DiasporaDocument.from_dict(line)\n\t\tif m is not None:\n\t\t\treturn m\n\t\traise Exception(\"Invalid Diaspora document\")\n\texcept Exception as e:\n\t\traise Exception(e"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn _dia_meta.Retrieve(host, \"XRD_Host_meta\")\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn retrieve_data(get_url(host, \"host_meta\"))\n\n\ndef get_diaspora_host_host_metadata(host):\n\t\"\"\"\n\tGet a remote Diaspora host-host metadata document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn retrieve_data(get_url(host, \"host_host_metadata\"))\n\n\ndef retrieve_data(url):\n\t\"\"\"\n\tRetrieve a remote remote Diaspora document from a remote host.\n\t\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\timport pdb, time\n\turl = \"http://localhost:8088/diapora/metadata/%s/meta/%s/%s\" % (host, time.time(), 0)\n\ttime.sleep(1)\n\td_res = requests.get(url)\n\td_res.raise_for_status()\n\treturn d_res\n\n#####################\n\nimport requests\nclass ResponseException(Exception):\n\t\"\"\"\n\tA generic response exception. This is a child of it with the 'code' property of the\n\tstatus attribute.\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tdia_host_name, dia_host_port, dia_host_meta = retrieve_diaspora_meta(host)\n\treturn XRD(dia_host_name, dia_host_port, dia_host_meta)\n\t\ndef retrieve_diaspora_s3_file_meta_uri(filepath):\n\t\"\"\"\n\t\tRetrieve a filename-based remote Diaspora host-meta document.\n\t\t\n\t\t:arg filepath: File to retrieve from\n\t\t:returns: URI\n\t\t\"\"\"\n\ts3 ="}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom pyxdbr  import xrd\n\n\tif not host or host == \"*\":\n\t\treturn xrd.HostMeta(\n\t\t\thost_name = xrd._hosts[0].as_string()\n\t\t)\n\telse:\n\t\treturn xrd.HostMeta(\n\t\t\thost_name = host\n\t\t).host_meta\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tmeta=r'{}-{}-host-meta'.format(host.ip, host.domain)\n\tdata=host.get(meta, force=True)\n\tif data is None:\n\t\treturn XRD(data, host)\n\treturn data\n\ndef retrieve_diaspora_instance_meta(node, port=28883):\n\t\"\"\"\n\tRetrieve a remote Diaspora instance-meta document.\n\t\n\t:arg node: Node where to get meta data\n\t:arg port: Port (default 2888)\n\t:returns:"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tconn = get_connection()\n\tsql = \"SELECT xrd,meta FROM hosts WHERE host=:xrd AND xrd!= 'host:none'\"\n\treturn conn.query(sql,xrd=host)\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\treturn host['result']['XRD']\n\t\n#end def retrieve_diaspora_host_meta\n\nclass DiasporaObject(object):\n\t\"\"\"\n\tClass representing a document in a Diaspora XML object.\n\t\n\t- ``document`` (``diaspora.xml.etree``.ElementTree) document\n\t- ``namespace`` namespace of the document\n\t- ``local_name`` XML local name\n\t- ``xml_name`` XML name, used for generating unique identifiers\n\t- ``uri`` URI identifier of the object\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tmetadataHost = _host_metadata(\"localhost\", host, 1, 3)\n\tdia = _find_dia(metadataHost)\n\thost_id = _parse_path(dia.hostId)[0]\n\tdoc = _diapora_load_doc(metadataHost, docKey=[host_id])\n\t# The first document key was the host, the second\n\t# because the first document key was unique, and because\n\t# there are several hosts, to try to match any other\n\t# document keys, we just use 'id' as key\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tmeta = meta_fetch_object(host, meta_folder=\"host-meta-raw\")\n\t\n\treturn XRD.XRD(meta)\n\ndef retrieve_diaspora_folder_meta(base_folder, domain=None):\n\t\"\"\"\n\tRetrieve a remote Diaspora folder-meta document.\n\t\n\t:arg base_folder: Folder name\n\t:arg base_domain: Domain (``\"web\"`` or ``\"app\"``) of the domain to retrieve\n\t\tdiaspora\n\t\t\n\t\t``None`` retrieves all of the"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\tresponse.raise_for_status()\n\treturn response.status_code, response.raise_for_status().json()\n\ndef get_document(url, timeout=None, *args, **kwargs):\n\t\"\"\"\n\tHelper method to send an API call to the ``url`` to get the document (JSON-LD) content on the wire.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.get"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\twith urlopen(url, data, timeout=timeout, method=method, *args, **kwargs) as response:\n\t\tstatus = response.status\n\n\t\tif getattr(response, 'deferred', None) is not None:\n\t\t\tdef error(*args, **kwargs): return None, None\n\n\t\t\tresponse.deferred = response.deferred.addCallback(error, *args, **kwargs)\n\t\t\t\n\t\t\tresponse.deferred.addErrback(error)\n\t\t\n\t\treturn status, None\n\ndef get(url, timeout=10, method=\"get"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tstatus_code, error = requests.post(url, data=data, *args, **kwargs).status_code, None\n\treturn status_code, error\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t_check_str('url', url)\n\t\n\ttimeout = timeout if timeout else 10\n\t\n\tif isinstance(data, (base64.b64encode, bytearray)):\n\t\traise OSError(\"You cannot encode document data using the base64 format.\")\n\t\n\tmethod = method if method else 'post'\n\t\n\tif data:\n\t\tif isinstance(data, (six.string_types, bytes)):\n\t\t\ttry:\n\t\t\t\tdata = urlparse.urlencode(data)\n\t\t\texcept TypeError:\n\t\t\t\t#"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\timport requests\n\t\n\tmethod = method.lower()\n\t\n\tdata_from_params = {}\n\t\n\tif method == \"none\" or method == \"post\":\n\t\tif (isinstance(url, list) and len(url) > 1 and isinstance(data, bytes)):\n\t\t\tdata_from_params[\"files\"] = {}\n\t\t\t\n\t\t\tfor i, value in enumerate(url):\n\t\t\t\tif value:\n\t\t\t\t\tpath = re.sub(r\"(//{2,3}|[^/])+\\S+\", \"\", value)\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t# convert a file-like object to a buffer to make it safe to write before sending\n\t#   note: bytes objects cannot be converted to a string using utils.to_pybytes()\n\t#   note: this is not necessary anymore\n\tif hasattr(data, \"read\"):\n\t\tdata = data.read()\n\n\t# if the encoding is a string, it will be treated as url encoding\n\tencoding = getattr(\"encoding\", \"utf-8\", \"utf-8\")\n\n\tif isinstance(data, bytes):\n\t\tdata = data."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttimeout = timeout if timeout >= 0 else None\n\tkwargs.pop('verify', None)\n\n\tif type(data) is bytes:\n\t\tdata = \"{0}\".format(data.decode('utf-8'))\n\t\ttimeout = None\n\t\tkwargs.setdefault(\"data_encoding\", \"application/octet-stream\")\n\telif type(data) is str:\n\t\tk = \"form-data\"\n\t\tkwargs.setdefault('data', self.__decode_args(data))\n\telif type(data) is dict:"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\treq = requests.post(\n\t\turl=url,\n\t\tdata=data,\n\t\tparams=self.url_params,\n\t\ttimeout=timeout,\n\t\tauth=(self.username, self.password),\n\t\tconfig=self.config,\n\t)\n\t\n\tif req.status_code!= 200:\n\t\treturn req\n\t\n\ttry:\n\t\treq.raise_for_status()\n\texcept requests.exceptions.HTTPError as r:\n\t\treturn r, None\n\t\n\treturn None, None\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tstatus_code = None\n\terror = None\n\ttimeout = timeout * 60\n\n\tfor attempt in range(0,10):\n\t\ttry:\n\t\t\tresponse = requests.post(url, data=data, timeout=timeout, method=method)\n\t\t\t\t\n\t\t\tif int(status_code) < 400: \n\t\t\t\tbreak\n\t\t\t\n\t\t\tprint(\"Status Code was %r\" % (response.status_code))\n\t\t\t\n\t\texcept requests.exceptions.Timeout as exception:\n\t\t\tprint(\"{} : Timeout\".format(exception))\n\t\telse:\n\t\t\tbreak"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tr = requests.post(url, data, timeout=timeout, *args, **kwargs)\n\tresponse = r.status_code\n\terror = r.content.decode('utf-8').replace('\\r\\n', '\\n').strip()\n\tif not response or (response == 400 and error):\n\t\traise NotImplemented\n\treturn response, error"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif key not in dic:\n\t\tdic[key] = {}\n\tfor k in keys:\n\t\tif k not in dic[key]:\n\t\t\tdic[key][k] = {}\n\t\tdic[key][k].update(dict_insert(dic[key], val, k, *keys[k:]))\n\t\n\treturn dic\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key2 in keys:\n\t\tif not dic.has_key(key2):\n\t\t\tdic[key2] = {}\n\t\tdic[key2] = insert(dic[key2], val, key, keys=keys+[key2])\n\t\t\t\t\n\t\t\t\n\nclass DictChain :\n\t\n\tdef __init__ (self) -> None :\n\t\tself.keys = []\n\t\tself.values = {}\n\t\t\n\tdef __getitem__(self, key):\n\t\treturn self.values[key]\n\n\tdef __setitem__"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif key in dic:\n\t\tfor key in keys[:-1]:\n\t\t\tdic = dic[key]\n\tdic[key]= dict_insert(dic[key], val)\n\treturn(dic)\n\t\t\n\t\ndef remove_from_dict(dic, key):\n\t\"\"\"remove the value (a string) from an key given argument\"\"\"\n\t\t\n\tresult = dic[key]\n\tdic.pop(key)\n\treturn(result)\n\t\n\t\n\t\n\t\n\t\n\t\n\t\t\n\t\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif key in dic:\n\t\tpass # does nothing for nested key\n\telse:\n\t\tif keys:\n\t\t\tdic[key] = {k: dict_insert(dic[key], val, k, keys[0]) for k in keys[1:]}\n\t\telse:\n\t\t\tdic[key] = val\n\n# ------------------main-------------------\n# create a dictionary and some test cases:\n\ndicT = {10: 'tetrahedron'}\ndict_insert(dicT,'tetrahedron','0')\ndict_"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) < 1:\n\t\traise KeyError('Unable to insert value'+ str(val) +'of type: <' + type(dic).__name__ + '>')\n\t\n\tfor i in range(len(keys)):\n\t\tif key + '.keys' + str(i) in dic:\n\t\t\tdic = dic[key + '.' + str(i)]\n\t\telse:\n\t\t\tdic[key + '.' + str(i)] = {type(val).__name__: val}\n\t\t\tdic"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key in keys:\n\t\tif key in dic:\n\t\t\tdic = dic[key]\n\t\telse:\n\t\t\tnew_dic = {key:val}\n\t\t\tdic.update(new_dic)\n\t\t\t\n\treturn dic\n\n\n\ndef keys_in_dict(dic, keys):\n\t\"\"\"\n\t\tReturns keys that is present in dict.\n\t\tThe keys are returned as tuples not as keys.\n\t\"\"\"\n\treturn [item for i in dic(keys) for item in i.keys()]"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t# find all ancestors of val, if any, insert the parent key into the \n\t# dictionary and append the rest of the key to the end\n\tparent_key = dic.get(val, None)\n\tif parent_key is None:\n\t\t# if we find this val, insert it to the dictionary at the end\n\t\t# but keep the key\n\t\tdic[val] = [None]\n\t\tdic, val = update_key_value(key, dic, val)\n\t\t# if dic does not have any parent, insert at end\n\t\tif"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttry:\n\t\tfor key in keys:\n\t\t\tdic[key] = dic[key] + '.' + val\n\texcept:\n\t\tdic[key] = '.' + val\n\t\t\n\ndef dict_dic_set(dic, *keys):\n\t\"\"\"\n\t\tset, if key in a dictionary, the value of the\n\t\tcorresponding key\n\t\n\t>>> d = dict_dic_set({'child1': 1,\n\t...                   'child2': 2},\n\t...                    'child1.child3', 3)\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif key not in dic.keys():\n\t\tdic[key] = {}\n\tfor key in keys:\n\t\tdic = dic[key]\n\tdic[val] = list()\n\treturn\n\n# get node from list, get node's label\n# if return node, then that node's next attribute has to be assigned with parent node's key\ndef get_next_node(node, key):\n\t\"\"\"\n\treturn the label of node and the last node of the key chain\n\t\n\texample:\n\t  first child of tree is called tree[\""}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys:\n\t\treturn dic\n\t\n\tif type(keys) is list:\n\t\tfor key in keys:\n\t\t\tif key in dic and type(dic[key]) is dict:\n\t\t\t\tkey_val = {}\n\t\t\t\tfor nested_key, nested_val in dic[key].items():\n\t\t\t\t\t# if nested_key is a key, set its value\n\t\t\t\t\tif type(nested_key) is str and \\\n\t\t\t\t\t\ttype(val) is str:\n\t\t\t\t\t\tif nested_key in list([k for k in keys if k!= key]):"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfile_names = []\n\tcnt = 0\n\tfor top_dir in settings_dirs:\n\t\tfor root_dir in top_dir.iterdir():\n\t\t\tfor dirname, _, filenames in root_dir.walk():\n\t\t\t\tsuffix = repr(dirname)\n\t\t\t\tif not dirname.endswith(\"/\"):\n\t\t\t\t\tsuffix += \"/\"\n\n\t\t\t\tif suffix in spec_option:\n\t\t\t\t\tfilename = str(root_dir) + \"/\" + suffix + repr(\".\").replace(\".\", \"\") + \".iut\"\n\t\t\t\t\tif not"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t#\n\t# The following are a place holder that you might choose to add your own \n\t#\n\t# file names for your IniTypes\n\t#\n\tlist_of_file_names = defaultdict(list) \n\tlist_of_file_names['all_files.ini'] = ['all']\n\tlist_of_file_names['all_files1.ini'] = ['all', '1']\n\tlist_of_file_names['all_files2.ini'] = ['all', '2']\n\tlist_of_file_names"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tt_list_of_file_names = list_of_file_names_from_settings_dir(settings_dirs, spec_option)\n\tt_spec_option = spec_option.split(' ')\n\tfor index, line in enumerate(t_list_of_file_names):\n\t\tt_list_of_file_names[index] = t_spec_option[0] +'' + t_spec_option[1] +'' + t_spec_option[2] +'' + line + t_spec_option["}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# Import from path\n\timport path_to_the_configuration\n\timport path_to_the_configuration\n\t# Specify the spec\n\timport spec\n\timport config_parser\n\t# Specify the input/output\n\tfrom dm.definitions import INIT_TYPE\n\tfrom dm.definitions import INIT_TYPE_BASE\n\n\t# First, set the paths to use\n\tpaths = []\n\tfor config in settings_dirs:\n\t\tconfig_path = path_to_the_configuration.config_path_str("}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tdef get_list_of_file_names(file_names, spec_option, env, filename):\n\t\tpass # do nothing\n\tdef walk_folder(folder):\n\t\tif spec_option == 'folder':\n\t\t\tfor root, dirs, files in os.walk(foldername, followlinks=True):\n\t\t\t\tfor dir in dirs:\n\t\t\t\t\tget_list_of_file_names(os.path.join(root, dir),\n\t\t\t\t\t\t\t\t\t\t\tspec_option, env, dir)\n\t\t\t\tfor file in files:\n\t\t\t\t\tfile_path = os.path"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tprint(\"list_of_file_names\")\n\tif spec_option in [\"-o\", \"--out\", \"-f\",\"--outfile\",\"--spec\", \"-s\"]:\n\t\toutfile = spec_option.replace(\"-f\", \"\")\n\telif spec_option in [\"-e\",\"--encoding\", \"-E\"]:\n\t\toutfile = \"encoding.enc\"\n\telif spec_option in [\"-m\",\"--mode\", \"-m\":\"r\", \"-z\",\"--raw\",\"-r\", \"-Z\"]:\n\t\toutfile = \"mode.raw"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tini_type = IniType()\n\tsettings_files = glob.glob(os.path.join(settings_dirs, '*.ini'))\n\n\tfor settings_file in settings_files:\n\t\tname_file = settings_file.replace(os.sep, '.')\n\t\tspec = spec_option.split(\",\")\n\t\tname = name_file[:-len(spec[0])]\n\t\ttype_name = type(name).__name__\n\n\t\ttry:\n\t\t\ttype_instance = type(name, \n\t\t\t\t[Ini"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tif spec_option == 'all' or spec_option == '':\n\t\tspec_option = '.inif'\n\treturn create_ini_type(settings_dirs,spec_option)\n\ndef create_ini_type(settings_dirs,spec_option='all'):\n\t\"\"\"\n\tInitializes the IniType class type\n\t\"\"\"\n\n\t# First get from settings directories, all default settings, and specified\n\t# values\n\tsettings_dict = get_setting_values(settings_dirs,'default.inif')\n\tsettings_value_def"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tglobal _new_type\n\n\t_new_type = IniType(options=[], file_options=None)\n\n\tfor d in settings_dirs:\n\t\tname = \"%sv_\" % d\n\t\tname = name.replace('/', '_')\n\n\t\t_new_type.create(['%s_%s_type' % (name, spec_option),'settings', 'xml'])\n\n\t\t_new_type.create(['%s_%s_settings.xml' % (name, spec_option),'settings', 'xml'])\n\n\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfrom.exception import IllegalTypeException\n\tfrom.exception import IllegalFormatException\n\n\t# create the type class\n\ttype_class = IniType()\n\n\t# find all the files in the settings_dirs\n\tlist_of_settings_files = []\n\tfor settings_dir in settings_dirs:\n\t\tnew_files = get_settings_files(settings_dir, spec_option)\n\t\tif not new_files:\n\t\t\tcontinue\n\t\tlist_of_settings_files.extend(new_files)\n\n\t# create config file\n\ttype"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls('ansible_config_manager')\n\n\n@annotations(\n    _options=(\n        ('hosts_file', '--hosts', default = '{default_settings_file}',\n         type = Path, help='Path to configuration file with hostname + other configurations.'),\n        ('groups_file', '--groups', default = '{default_settings_file}',\n         type = Path, help='Path to configuration file with groupname + other configurations.'),\n        ('config_context', '--config-context', dest = 'config_context',"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tansibj = ansiballz.AnsibleBanshee\n\tansibj.banner(f'type-ansibj {Fore.GREEN}ANSI/CC3 {Fore.WHITE}-{ansibj.get_version()}/{ansibj.get_branch().replace(\"_\", \"-\")}')\n\tansibj.banner('\\n\\n\\n**************************************************\\n')\n\tansibj.banner('ANSIBLE Configuration Manager\\n\\n\\n\\n**************************************************\\n')\n\tfor"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn AnsibleConfigManager(cls.module, cls.args, cls.config_file)\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn AnsibleConfigManager.ansible_config\n\n\nclass AnsibleConfigManager(object):\n\n\t\"\"\"\n\t\tA class that handles loading, manipulation and storage of ansible.conf files.\n\t\"\"\"\n\n\tdef __init__(self, file=None, ansible_config_manager=None, playbook=None, playbook_var=None, ansible_playbook=None):\n\t\tsuper(AnsibleConfigManager, self).__init__()\n\t\tself.playbook_var = playbook_var\n\t\tself.ansible_playbook = ansible_playbook\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tif not hasattr(cls, '_ansible_config_manager'):\n\t\tcls._ansible_config_manager = ansible_config_manager\n\treturn cls._ansible_config_manager()\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t# We need a unique class name\n\tcls._unique_name_ = \"ansible_config_manager\"\n\n\t# get it\n\treturn AnsibleConfigManager(cls._unique_name_)\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls\n\n\n### Ansible config. ###\n\ntry:\n\timport sys\n\tsys.path.append(\"./ansible_config_manager\")\nexcept:\n\tpass\n\n\nclass AnsibleConfigManager():\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitializes this class with a class variable\n\t\tconfig.\n\t\t\"\"\"\n\t\tself.__config = {}\n\n\t\treturn\n\n\tdef init(self, name, config, module_name):\n\t\t\"\"\"\n\t\tSets the new parameter 'name' with a value of config\n\t\tfor'module"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls._core_config\n\n\nclass AnsibleConfigManager(Manager):\n\t\"\"\"\n\tHelper class for use with Ansible config files to manage Ansible instances\n\t\"\"\"\n\tdef __init__(self, **attrs):\n\t\t# get the name of the directory for this plugin\n\t\tself._default_config_dir = \".ansible-config\"\n\n\t\t# save the plugin version as that on disk\n\t\tself._plugin_version = \".\".join(ansible_config_manager.plugin.__version__.split(\".\")[::-1])\n\n\t\t#"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tcls.setParams(ansible_config=True)\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.query.get(Config.id)\n\n  @classmethod\n  def register(cls, *args, **kwargs):\n      cls.insert().values(**kwargs).on_conflict_do_nothing()\n\n\nclass Config(Base):\n    id = Column(Integer, primary_key=True)\n    host = Column(String(128))\n    env = Column(String(128))\n    port = Column(Integer)\n    connection_type = Column(String(128))\n    connection_kw = Column(PickleType)\n"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tif not hasattr(cls,'manager'):\n\t\tcls.manager = cls.__getattribute__('__workspace_manager_instance__')\n\treturn cls.manager\n\n\nclass WorkspaceManager(BaseService):\n\n\t\"\"\"\n\tClass for managing the workspace manager.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tsuper(WorkspaceManager, self).__init__()\n\t\tsetattr(cls, '_workspace_manager_instance_', None)\n\t\tsetattr(cls, '_service_name', 'workspace"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\ttry:\n\t\timport csv\n\t\tconn_ws_str = \"\"\n\t\tfor k,v in cls.__dict__.items():\n\t\t\tif not k.startswith(\"__\") and (not hasattr(v, \"__setUp\")):\n\t\t\t\tconn_ws_str += \":\" + str(k) + \"=\" + str(v)\n\t\tconn_ws = create_simple_connection(conn_ws_str)\n\t\tconn_ws.autocommit(True)\n\t\tconn_ws.execute(\"pragma foreign_keys=on"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\ttry:\n\t\timport WorkspacesManager\n\t\treturn WorkspacesManager.getInstance()\n\texcept ImportError:\n\t\tprint \"WorkspacesManager not loaded. This class can not be used.\"\n\t\tprint \"Please, execute '\" + sys.argv[0] + \" workspace_manager --help' or execute '\" + sys.argv[0] + \" workspace_manager --help'\"\n\t\tsys.exit()\n\texcept Exception as ex:\n\t\tprint str(ex)\n\n# end of workspace_manager\n\n################################################################################\n#                                                                          #\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._workspace_manager\n\t\n@workspace_manager.setter\ndef workspace_manager(cls, value):\n\tcls._workspace_manager = value\n\tcls._workspace_manager_value = value\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tcls.workspace = cls.manager.workspace\n\n\tcls.base_url = \"\"\n\n\ttry:\n\t\tworkspace_manager = os.environ.get('WORKSPACE_MANAGER')\n\t\tif workspace_manager:\n\t\t\tcls.workspace = Path(workspace_manager['HOME_DIR']).resolve()\n\t\telse:\n\t\t\tcls.workspace = cls.manager.workspace\n\texcept Exception:\n\t\tcls.workspace = cls.manager.workspace\n\n\ndef parse_cmdline(argv"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.workspace\n\n\t\nclass workspaceManager(workspace):\n\t\"\"\"\n\tThe workspace manager.\n\t\"\"\"\t\n\t\n\tdef _getClass(self):\n\t\t\"\"\"\n\t\tReturns the workspace Manager class\n\t\t\"\"\"\n\t\treturn workspace_manager.Manager\n\t\n\tworkspaceManager = property(_getClass,doc=\"\"\"\n\tThe workspace manager.\n\t\"\"\"\n\t)\n\t\n\t\nclass workItem(workItem.__class__):\n\t\"\"\"\n\tThe workItem.\n\t\"\"\"\n\t\n\t# Attribute workspace\n\tworks"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tif cls.activeWindow: return cls.activeWindow.winManager\n\treturn None\n\n\n#----------------------------------------------------------------------\ndef getAllSelection(selection_list=None):\n\t\"\"\"Return all items in the selected elements.\n\t\n\t:param selection_list: a list with (string, QtCore.QItemSelection) selection strings\n\n\t:rtype: a generator iterator\n\t\"\"\"\n\n\t# This is a list of tuple that contains\n\t# the item and the selected item.\n\tfor item, selected in selection_list:\n\n\t\tif selected:\n\t\t\tsel_"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn (\n\t\tWorkSpaceManager(cls, '', get_workspace_data(), None)\n\t)\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn get_workspace_manager('workspace')\nclass WorkQueue(models.Model):\n\t\"\"\"Model stores a queue inside a workspace.\n\t\"\"\"\n\tclass Meta:\n\t\t\"\"\"Meta class.\n\t\t\"\"\"\n\t\tproxy = True\n\tid = models.PositiveIntegerField(primary_key=True)\n\ttask = models.TextField(max_length=50, blank=True, null=True)\n\tdescription = models.TextField(blank=True, null=True)\n\tlast_task = models.DateField(null=True, blank=True"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn WorkspaceManager(\n\t\tcls._conn_pool,\n\t\tcls._waiters,\n\t\tclass_args=cls.__args__)  # type: ignore\n\n\ndef list_workspaces() -> List[ListWorkspace]:\n\t\"\"\"\n\tList all workspaces.\n\n\tReturns:\n\t\tList[ListWorkspace]:\n\t\t\tList of the available workspaces.\n\t\"\"\"\n\treturn list_workspaces(cls=WorkspaceManager)\n\n\ndef create_workspace(\n\t\tcontext: Callable -> Awaitable[WorkspaceManager],\n"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tlogging.debug('get plugins manager')\n\treturn get_instances_of_class(cls, 'plugin')\n\n\n\ndef is_available_plugins(plugins=None, verbose=1):\n\t\"\"\"\n\tReturn True and or False based on is there some of plugins.\n\t\"\"\"\n\tlogging.debug('is it available? (running {})'.format(plugins))\n\n\tif plugins is None:\n\t\tplugins = plugins_manager()\n\n\tif not plugins:\n\t\treturn False\n\t# check each plugin with available is_valid_flag\n\tres"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tmanager = cls.pluginsManager()\n\tif not manager:\n\t\tmanager = cls.moduleManager()\n\treturn manager\n\n\nclass PluginManager(object):\n\t\"\"\"\n\t\tPlugin manager class class.\n\t\tMainly used to get the objects for the plugin objects, so that\n\t\twe don't have to create a separate object instance for each item\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\t\tConstructor method\n\t\t\"\"\"\n\t\t# get the plugin class\n\t\tpluginLoader = PluginLoader.PluginLoader()\n\t\tself.plugins = pluginLoader.get"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\timport sys\n\t__plugin_path = get_plugin_path()\n\tif not __plugin_path in sys.path:\n\t\tsys.path.append(__plugin_path)\n\n\tfrom resources.lib.modules import utils\n\tfrom resources.lib.modules.constants import ADDON_ID, HOME\n\tfrom resources.lib.modules.globals import g\n\n\tclass PluginsManager:\n\t\t\"\"\"\n\t\tGets the plugin manager.\n\t\t\"\"\"\n\t\tdef get_manager(self, plugin_id=None):\n\t\t\tfrom modules import addon\n\t\t\t"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tfrom resources.lib.modules.logging import LOG\n\treturn LOG\n\n\nclass ResolverError(Exception):\n\t\"\"\"Parent of all custom error exceptions\"\"\"\n\tpass\n\n\nclass PluginError(ResolverError):\n\t\"\"\"Plugin not found or unloadable\"\"\"\n\tdef __init__(self, plugin, reason):\n\t\tself.plugin = plugin\n\t\tsuper(PluginError, self).__init__(reason)\n\n\tdef __str__(self):\n\t\t\"\"\"__str__ method delegates to error message\"\"\"\n\t\treturn \"Plugin [{0}] in [{1"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\ttry:\n\t\treturn cls._get_plugin_manager_instance()\n\texcept AttributeError:\n\t\tcls._plugin_manager_instance = ClassLoader(cls)\n\t\treturn cls._plugin_manager_instance\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.PluginManager\n\n@pyqtSlot()\ndef set_plugin_mgr(plugin, enable):\n\t\"\"\"\n\tSets the plugin manager.\n\t\"\"\"\n\tpluginManager = plugin.PluginManager()\n\tpluginManager.setPluginManagerEnabled(enable)\n\n@pyqtSlot(bool)\ndef enable_plugin(on_enable):\n\t\"\"\"\n\tEnables or disables the plugin.\n\t\"\"\"\n\tpluginManager = plugin.PluginManager()\n\tpluginManager.setPluginEnabled(on_enable)\n\n@pyqtSlot(bool"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn {\n\t\t'plugins': cls.get_plugins(),\n\t\t'plugins_manager_state': cls._instance\n\t}\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\timport types\n\tglobal pluginmanager\n\tplugins_dir = None\n\tif pluginmanager in plugins_dir:\n\t\treturn\n\ttry:\n\t\tfrom xml.dom import minidom\n\texcept ImportError:\n\t\traise ImportError('Please install the \"xml\" module')\n\tdef new_class(tagname, attrs=None):\n\t\tfrom xml.dom import minidom\n\t\tglobal plugins_dir\n\t\tplugins_dir = os.getcwd()\n\t\timport importlib\n\t\tspec = importlib.util.find_spec(tagname)\n\t\tplugin"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn plugin(cls)\n\nmanager = plugins_manager()\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tfrom Plugins.Extensions.SverchOk import SvokOK\nreturn cls(_plugins=['Vg.Sverch.plugins.Ext.Sverch'])"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec = spec_content.get('data', None)\n\t\tif not spec:\n\t\t\tdata = dict(\n\t\t\t\tinput_type=DEFAULT.get('input_type', 'text'),\n\t\t\t\tconfiguration=DEFAULT.get('configuration', None),\n\t\t\t\tlabel=DEFAULT.get('label', 'YAML'),\n\t\t\t\tformat=DEFAULT.get('format', 'PRETTY'),\n\t\t\t\toutput_type=DEFAULT.get('output_type', 'text'),\n\t\t\t\torder=DEFAULT.get('order', None),\n\t\t\t\tcomment="}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tyamlContent = spec_content\n\t\tcheckContent = checkSchemaDict(yamlContent, cls)\n\t\tif checkContent:\n\t\t\treturn None, cls.loadSchema(yamlContent)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Invalid YAML data: '{0}'. {1}\".format(exc, str(exc)))\n\n\ndef validate_schema(cls, spec_content):\n\t\"\"\"\n\tvalidates a module schema\n\n\t:param spec_content"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tvalid_fields = dict()\n\tif not spec_content:\n\t\traise IRValidatorException('Spec File is Empty!!')\n\tif not ( 'name' in spec_content and\n\t\t   'uuid' in spec_content and\n\t\t   'type' in spec_content and\n\t\t   'group' in spec_content and\n\t\t  'services' in spec_content ):\n\t\traise IRValidatorException('Missing required fields in spec content.')\n\tvalid_fields['name'] = spec_content['name']\n\tvalid_fields['uuid'] ="}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tdata_dict = json_load(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException(f'[-] Something wrong {e}') from e\n\treturn Data_dict_factory(data)\n\n\ndef validate_from_db_spec():\n\t\"\"\"\n\tvalidate file spec from database\n\n\t:raise NoDataException: No data in the database\n\t:return: Dictionary with data_dict (YAML)\n\t\"\"\"\n\tdb_model = ConfigDBClass()\n\tdb_data = db_model"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = yaml_load(spec_content)\n\tmissing = [f for f in spec_content.keys() if f not in data.keys()]\n\tprint(missing)\n\tif missing:\n\t\traise_cls('Missing fields in specified YAML file. Fields missing in'\n\t\t+ \" \"*16+\": {}\".format(\",\".join(missing)))\n\treturn data\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t# store fields names as attributes\n\tdata = cls.__dict__\n\n\tif spec_content:\n\t\t# check for mandatory info\n\t\terror_message = \"Mandatory Fields: \"\n\t\tfor field in data:\n\t\t\tif field.strip() == \"\":\n\t\t\t\traise IRValidatorException(error_message + field)\n\n\treturn data\n\n\ndef validate_schema(cls, schema):\n\t\"\"\"\n\tvalidates that class conform to the given schema\n\t\n\t:param cls: Class defining class to be validated for\n\t:param schema: schema of"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tif cls.is_valid(spec_content):\n\t\t\treturn cls.load_spec(spec_content)\n\t\t\t# if required data is present then raise exception,\n\t\t\t# and return valid specification in request\n\t\t\traise IRValidatorException(\"Spec content is in Validator's format: \" + str(spec_content))\n\t\telse:\n\t\t\treturn cls.raise_invalid_spec_content(spec_content)\n\texcept IRError as e:\n\t\t# raise exception when exception occurs\n\t\traise IRValidatorException(f\"An"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\timport json\n\tfrom pathlib import Path\n\tfrom os.path import dirname, expanduser, abspath\n\n\t# set default content based on default path\n\tif spec_content is None:\n\t\t# set path\n\t\tif cls.default_path not in [\n\t\t\texpanduser(\"~\"),\n\t\t\tPath.cwd().resolve(),\n\t\t]:\n\t\t\tfile_path = pathlib.Path(spec_content).resolve()\n\t\t\tspec_content = str(file_path)\n\n\t# try and parse\n\ttry:\n\t\tdata = yaml."}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Missing spec file content\")\n\t# parse spec file content\n\tspec_path = cls.read_file(cls.PATH_SPEC)\n\ttry:\n\t\tspec, errors = cls.parse_spec_object(spec_path, spec_content)\n\texcept FileParserError as e:\n\t\traise IRValidatorException from e\n\telse:\n\t\t# if all fields are present in schema and valid schema\n\t\tif type(errors) is not dict:\n\t\t\traise IRValidatorException(\""}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = cls.__read_config(spec_content)\n\tif cls.__data == None:\n\t\traise IRValidatorException('IR.YAML: unable to read spec file %s.' %(cls.spec_path))\n\tfor key in cls.__all_fields:\n\t\tif key not in data or data[key] is None:\n\t\t\traise IRValidatorException('IR.YAML: missing mandatory value %s from spec file %s.' %(key, cls.spec_path))\n\treturn data\n\nif __"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\traise cls.IRValidatorException()\n\t\t\n\tfrom os.path import join, normpath, sep\n\timport pathlib\n\timport yaml\n\t\n\twith open(yaml_file, 'r', encoding='utf-8', errors='ignore') as f:\n\t\tdata = yaml.safe_load(f)\n\t\t\n\tfor _, field in data.items():\n\t\tif field['type'] not in ['booleans','string', 'lists', 'ints']:\n\t\t\traise cls."}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif xmltodict is False and (cls.use_xml or cls.use_yaml):\n\t\traise RuntimeError(\n\t\t\t\"XML and YAML YAML support not found on your system (xmltodict).\")\n\n\tloaded_data = {}\n\ttry:\n\t\ttree = ET.parse(yaml_file)\n\texcept:\n\t\traise RuntimeError(\"Invalid yaml data.\")\n\tloader = XMLLoader(cls, cls.yaml_tag, cls.yaml_dict_factory)\n\ttry"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom yaml import safe_load as yaml_load\n\timport yaml\n\tmeta_file = cls.class_name + \".meta\"\n\ttry:\n\t\twith open(meta_file, 'r', encoding='utf-8') as f:\n\t\t\tdata = yaml_load(f)\n\texcept FileNotFoundError:\n\t\traise IRValidatorException('Missing meta-file') from None\n\tif yaml_file is not None and data is None:\n\t\traise IRValidatorException(\"Data is missing in manifest\")\n\ttry:\n\t\tset"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tif yaml_file is None:\n\t\tyaml_file = cls.filepath\n\t\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tcontent = yaml.load(stream, Loader=YAML_FROM_FILE_LOADER)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise Exception(\"Error in YAML file: {}\".format(exc))\n\t\n\tkeys=set(content.keys())\n\tif 'input' not in keys or 'labels'"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\tyaml_file = sys.argv[1]\n\twith open(yaml_file) as file:\n\t\tparsed = yaml.safe_load(file)\n\ttry:\n\t\tfor key in cls.__dict__.keys():\n\t\t\tif key not in parsed:\n\t\t\t\traise IRValidatorException((cls.__name__, key, 'Missing field name. Missing at \"{}.{}\"'.format(cls.__name__, key)))\n\texcept:\n\t\traise\n\treturn parsed\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\tif not yaml_file or not type(yaml_file) == type(s):\n\t\t\traise TypeError(\"Invalid YAML file, it must be a path to YAML file.\")\n\t\twith open(yaml_file, 'r') as ymlfile:\n\t\t\tret = cls(yaml_file)\n\t\t\tret._validate()\n\t\t\treturn ret\n\texcept IOError:\n\t\traise IOError('YAML input error, could be some of the following:\\n\\t1. File not found\\n\\t2."}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom Open.config import open_yaml_file\n\tif not yaml_file:\n\t\traise TypeError('You have to put a file with a valid YAML format as parameter (i.e. YAML)')\n\telse:\n\t\tif not isinstance(yaml_file, str):\n\t\t\traise TypeError('YAML must be a string')\n\t\telif not os.path.exists(yaml_file):\n\t\t\traise FileNotFoundError(f'The YAML file \\\n\t\t\t\t   {os.path.join(os."}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\traise ValueError('The config.yml file needs to exist. Not found: '+yaml_file)\n\tconfig = Config(yaml_file=yaml_file)\n\tconfig.load()\n\tlogging.info (\"Config loaded with {}\".format(config.to_string()))\n\tif not config.has_config():\n\t\traise ValueError('The Config cannot be loaded. Missing config.yml file')\n\tlogging.info(\"A Dictionary with all important data is loaded in {}\".format(config.to"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\tyaml_file = 'YAML_files/YML/Avalis/yamlFormat.yaml'\n\ttry:\n\t\twith open(yaml_file) as f:\n\t\t\tdata = yaml.load(f, Loader=yaml.FullLoader)\n\t\t\tyaml_file = yaml_file.replace('.yaml', '')\n\t\t\tif not data.get('schema', False):\n\t\t\t\tmsg = 'yaml file \"{}\" does not contain schema'.format(yaml_file)\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\traise AttributeError(\"No mandatory field found in file\")\n\n\n\nclass ConfigurationGenerator(object):\n\t\"\"\" A simple data-generation based generator \"\"\"\n\t\n\t@staticmethod\n\tdef data():\n\t\t\"\"\" Generates all required data for the module \"\"\"\n\t\treturn DataConfig(DataGenerator.CONFIG_YAML)\n\t\n\tCONFIG_YAML = \"conf/yaml/config.yaml\"\n\t\n\t@staticmethod\n\tdef run():\n\t\t\"\"\" Executes yaml file to generate all required data for a new module or the "}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\ttry:\n\t\tparser = parser_dict['main']\n\texcept KeyError:\n\t\treturn False\n\n\n\t# Builds the new parser (by walking the spec file and including groups)\n\tgroup_count = self._determine_group_count('include')\n\n\tparser_dict['include'] = {\n\t\tgroup_name:0 for group_name in group_count['include']\n\t}\n\tfor i, m in enumerate(parser):\n\t\t# Only process an include directive in group level\n\t\tif not isinstance(m, (NONE_DIRECT"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif self._included_groups:\n\t\tprint 'Not expanding the included groups dict!'\n\t\treturn\n\t# FIX ME!\n\n\tfor group_section in parser_dict['include']['groups'][0].rstrip().split(','):\n\t\tgroup = Group(group_section)\n\t\tgroup_id = group.key\n\t\t# FIXME?\n\t\tif parser_dict['include'].get('included-groups') is not None:\n\t\t\tfor group_section in parser_dict['include']['included-groups'][0].rstrip()."}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroups = re.findall(r'include\\s*=\\s*(\\S+)', parser_dict['declaration'])\n\tfor name in groups:\n\t\tparser_dict['include'] = name\n\t\tif name not in self.__includes:\n\t\t\tself.__includes[name] = []\n\t\tif 'base' not in parser_dict:\n\t\t\tparser_dict['base'] = name\n\t\tparser_dict['extendmodules'] = False\n\n# TODO: make sure not to include the same source file twice?\n# TODO: add error handlings"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tignore_directives = ('#include', '#import', 'from', 'import',\n\t                    'library', 'class')\n\n\t\"\"\"\n\tSets the scope of the parser tree.\n\t\"\"\"\n\tscope = 'global'\n\twith_scope = False\n\n\t\"\"\"\n\tDetermines if the directive takes the parent scope into account.\n\tA False value means it'll take the parent scope instead of inheriting\n\tto the current scope.\n\t\"\"\"\n\tassumption = False\n\n\t\"\"\"\n\tThis method is used to determine if it needs to be resolved.\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif 'include' not in parser_dict:\n\t\tparser_dict['include'] = {}\n\n\t# If a directive wasn't included, keep it for the recursive expansion.\n\tif '_include' not in parser_dict:\n\t\tparser_dict['_include'] = {}\n\n\tparser_dict['_include']['whitespace_required'] = False\n\n\t# Handle the _included_directives.\n\tincluded = self._included\n\tfor pattern, parser_dict in included.iteritems():\n\t\tpats_and_parsers = parser_"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor f in vars(self).get('include_groups'):\n\t\tparser_dict[\"include_groups\"] = parser_dict[\"include_groups\"].replace(f, self.data_files[f])\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor groupname in self.config_handler['spec_groups'].keys():\n\t\tif groupname not in self.config_handler['groups_with_include']:\n\t\t\tself.config_handler['groups_with_include'][groupname] = []\n\t\t\tfor key in self.config_handler['spec_groups'].keys():\n\t\t\t\tif key!= groupname:\n\t\t\t\t\tfor dirname in self.config_handler['spec_root'][0].glob(f\"*.{key}/*\"):\n\t\t\t\t\t\tif dirname not in self.config_handler['groups"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif (not isinstance(parser_dict, dict)) or not all(key in parser_dict for key in self.includes) or self.includes[self.includes.index('include')] is None:\n\t\treturn\n\tfor directive in self.includes.index('include'):\n\t\tparser_dict[directive] = parser_dict.pop(directive)\n\tif not isinstance(parser_dict, dict):\n\t\treturn\n\n\tinclude_path = str(parser_dict.pop('filename'))\n\n\tfound = False\n\tfor included in parser_dict."}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif not hasattr(self.spec, \"include\"):\n\t\tself.spec.include = {}\n\tfor group in parser_dict[\"group\"]:\n\t\tfor i in range(len(parser_dict[\"groups\"]) if group in parser_dict[\"groups\"] else -1):\n\t\t\tfor option in parser_dict[\"options\"][group][i]:\n\t\t\t\tif option[\"directive\"]:\n\t\t\t\t\tif hasattr(option[\"value_set\"], \"include\"):\n\t\t\t\t\t\tgroup_value = option[\"value_set\"]\n\t\t\t\t\t\tif i == 0:\n\t\t\t\t\t\t\tself.spec"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tresolved_default_includes = []\n\tresolved_group_includes = []\n\tgroup_include_re = re.compile(r\".*include.+?(include|exclude) = (.+)\")\n\tdefault_include_re = re.compile(r\".*include.+?(include|exclude) \\[.+?\\] = (.+)\")\n\tfor match in list(map(group_include_re.search, parser_dict.get('include', '').splitlines())):\n\t\tif match.group(1).startswith"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\td = vars(self.args).items()\n\tdel d[0]\n\td = [(k, getattr(self.args, k, None))\n\t\tfor k in self.special_flags_to_int.keys()]\n\t# d.sort()\n\treturn d\n\n\n#\n# class ParseArguments(object):\n# \t\"\"\"\n#   \tParse arguments.\n# \t\"\"\"\n#\n# \tdef __init__(self, arglist=[], **special_args):\n# \t\tself.args = _Arg"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tfor arg in self.args:\n\t\t# if the positional argument is in a config section of the spec,\n\t\t# get the value from the config and convert it to a string\n\t\tif arg.name == \"param_index\":\n\t\t\ttry:\n\t\t\t\tif len(arg.config_section) > 0:\n\t\t\t\t\treturn self.config.get_int(arg.config_section, arg.name, None)\n\t\t\texcept RuntimeError:\n\t\t\t\tpass\n\t\t# if the argument's value is not in a config section,\n\t\t# convert it to a string and check that it's"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\targspec = self.argv[:]\n\tif argspec:\n\t\tif type(argspec[0]) is dict:\n\t\t\tdefault_vals = argspec[0]\n\t\telse:\n\t\t\t# If only function then give the values as is\n\t\t\t# Note: If one-argument then it's a reference to another argspec\n\t\t\t# (not dict or None)\n\t\t\tassert len(argspec) == 1, 'Must provide one argument'\n\t\t\tdefault_vals = argspec[0].values()\n\t\tassert type(argspec[0].get('argspec', None"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tspec_values_in_use = {\"name\" : self.name,\n\t\t\t\t\t\t   \"date\" : datetime.fromisoformat(self.date_start),\n\t\t\t\t\t\t   \"email\" : self.email,\n\t\t\t\t\t\t   \"referral\" : self.referral}\n\treturn spec_values_in_use\n\n\t\ndef parse_spec():\n\t\"\"\"\n\t\tParse the provided values from the spec and other sources.\n\t\"\"\"\n\tif not self.name: self.name = \"Jobs\"\n\tif not self.date_start: self."}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# FIXME: handle case in `if __name__ == '__main__':`!\n\tdef process_spec_args(args):\n\t\targs2 = {}\n\t\t# args = [args.split()]\n\t\tfor arg in args:\n\t\t\tparts = arg.split('=', 1)\n\t\t\tif len(parts) == 2:\n\t\t\t\targs2[parts[0]] = parts[1].strip()\n\n\t\treturn args2\n\tspec_args = process_spec_args(self.spec_args)\n\targs = self.spec.split()\n\t"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tassert self.spec.key in ['build', 'runtime'], '`{}` key not found in spec'.format(self.spec.key)\n\tspec = self.spec\n\n\tif self.spec.key!= 'build':\n\t\tpkg = find_spec(spec.name).origin\n\t\tbuild_package_or_robot(self, pkg)\n\t\treturn self.get_spec_defaults() if self.spec.key == '' else None\n\telse:\n\t\tversion = spec.version.split('.')\n\t\tif version[0] in ["}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\timport pkg_resources\n\tself.spec = importlib.util.resolve_name(self.spec, self.name)\n\tself.spec.load()\n\tif self.spec.has_data():\n\t\tself.data = self.unpack_zipdata(self.spec.data)\n\t\tself.data = self.spec.data.decode(\"utf-8\")\n\t\tself.is_py3 = sys.version_info >= (3, 0)\n\ndef _import_class(name, path=None, part_of_spec=False"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tresult = {}\n\tfor source, config in self.source_args.items():\n\t\tif isinstance(config,dict):\n\t\t\tif \"name\" in config:\n\t\t\t\tconfig = config[\"name\"]\n\t\t\telse:\n\t\t\t\tconfig = config[\"type\"]\n\t\tif isinstance(config,(list,tuple)):\n\t\t\tfor arg in config:\n\t\t\t\tif arg in result:\n\t\t\t\t\tcontinue\n\t\t\t\tresult[arg] = getattr(self, source)\n\t\telif isinstance(config,str):\n\t\t\tresult[config] = getattr(self, source)"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tself.specification[\"source\"] = self.specification.get(\"source\")\n\tself.specification[\"spec\"] = self.specification.get(\"spec\")\n\n\t\"\"\"\n\tCreate the configuration values.\n\t\"\"\"\n\tself.update_info(self.specification, self.specification)\n\tspecification = self.specification\n\t# setup_module.setup()\n\tself.migration_dict = {}\n\tself.updater_dict = {}\n\tself.migration_list_dict = {}\n\tself.updater_list_"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# Defaults to the first position of given sources\n\tvalues = {}\n\tfor source in self._source_list:\n\t\tsource_name = '%s as %s' % (source.path,source.name)\n\t\tif hasattr(self,source.name) and hasattr(self,source.name+'_defaults'):\n\t\t\tdefaults = getattr(self,source.name+'_defaults')\n\t\t\tvalues[source_name] = defaults.get() if len(defaults.get())>0 else None\n\t\telse:\n\t\t\traise"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tif isinstance(self.source, (str, unicode)):\n\t\tself.source = self.source.replace('\\\\','\\\\\\\\').replace('\"',r'\\\"').replace(\"'\",\"\\\\'\")\n\t\t# \"\\\\\" is escape code, it's not a string\n\t\t# So, let's try to fix it\n\t\tif self.source.endswith('\"\\'): self.source = self.source.replace('\\'','\\\\\\'')\\\n\t\t\t.replace('\\\\\\'','\\\\\\\\')\n\t\treturn {\"deprecated_args\": self.source}\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\td = {}\n\tif hasattr(self, 'deprecated_args'):\n\t\td = {**self.deprecated_args, **d}\n\treturn d\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdeprecated_args = copy.deepcopy(self.args)\n\tfor key in self.additional_args:\n\t\tif not deprecated_args.get(key):\n\t\t\tdeprecated_args[key] = self.additional_args[key]\n\tfor key in self.deprecated_kwargs:\n\t\tif not deprecated_args.get(key):\n\t\t\tdeprecated_args[key] = self.deprecated_kwargs[key]\n\treturn deprecated_args\n\n\t\n########################\n# General Utility #\n########################\n\nclass Args(object"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tif not self.deprecated:\n\t\treturn None\n\treturn dict(\n\t\t\targv = [],\n\t\t\tdeprecated = dict(\n\t\t\t\targv = ['1'],\n\t\t\t\tdeprecated = [['2']]),\n\t\t\targ_ignore_deprecated_args = []]\n\n\t\nclass TestParser(argparse.ArgumentParser):\n\t\"\"\"argparse.ArgumentParser which ignores and not deprecates\n\t**deprecated arguments**.\"\"\"\t\n\n\tdef __init__(self, *args, **kwargs):\n\t\tdeprecated_args = get_"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\targv = [self._args_getter_factory(self._cmd)]\n\targs = subprocess.check_output([self._args_getter_factory(self._cmd),\n\t\t\t\t\t\t\t\t\t  '-W%s=%s' % (self._args_getter_name, repr(self._args)),\n\t\t\t\t\t\t\t\t\t  '-W%s=%s' % (self._args_getter_name, repr(self._args_deprecated)),\n\t\t\t\t\t\t\t\t\t  '-W%s' % self._args_getter_alias,\n\t\t\t\t\t\t\t\t\t  '-W%s'"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {\n\t\t'sec':'sec',\n\t\t'loc':'loc',\n\t\t'at':'at',\n\t\t'loc': 'loc'\n\t}\n\n# For backward compatibility only\ndef get_deprecated_args_2(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {\n\t\t'sec':'sec',\n\t\t'loc':'loc',\n\t\t'at':'at',\n\t}\n\ndef get_deprecated_args_by_name_1(self, opt):\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\targ_names = ['from', 'args', 'kwargs']\n\t# Deprecated options - use method get_options and not just get_options\n\n\tkw_defaults = {}\n\tfor arg in arg_names:\n\t\tkw_defaults[arg] = []\n\t\tif self.get_option(arg):\n\t\t\t# get_options can either return multiple values\n\t\t\t# so we iterate over the option names\n\t\t\t# return a list\n\t\t\tdef_val = self.get_option(arg)\n\t\t\tdef_type = type(def_val)\n\t\t\tfor"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdeprecated_args_dict = {}\n\tif 'add_argument' in self._parser._actions:\n\t\tdeprecated = self._parser._actions['add_argument']\n\t\tdeprecated_args_dict['add_argument'] = deprecated.__dict__\n\tif any(k in deprecated_args_dict for k in ['option_strings','remove_argument']):\n\t\tfor deprecated in deprecated_args_dict['remove_argument']:\n\t\t\tif deprecated in deprecated_args_dict['option_strings']:\n\t\t\t\tdeprecated_args_dict['add_argument']"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdep_args=self.__set_dependency_options_to_dict(include_deps=True)\n\tdep_list=[]\n\tfor k in self.__init_kwargs:\n\t\tif k not in self.__init_args_dep_opts:\n\t\t\tprint(\"Invalid argument for '%s'\"%k)\n\t\t\tcontinue\n\t\tif self.__init_kwargs[k]==self.__init_args_dep_opts[k]:\n\t\t\tpass\n\t\telif self.__init_kwargs[k] == self."}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {\n\t\t'log_level':\n\t\t{\n\t\t\t'default': logging.ERROR,\n\t\t\t'validatorlist': [\n\t\t\t\tLogLevelValidator,\n\t\t\t],\n\t\t},\n\t}\n\t\n\t\n\t\n\n\nclass App:\n\t\"\"\"\n\tThe base class for the application-specific objects\n\tto use for all modules.\n\t\"\"\"\n\t\n\tdef __init__(self, app_options, init_vars=None):\n\t\t\"\"\"\n\t\tConstructor.\n\t\t\"\"\"\n\t\tself._app_options = app_options\n\t\t"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif 'arg' in cli_args:\n\t\tif cli_args['arg'] in self.deprecated_list:\n\t\t\tif self.input_help:\n\t\t\t\tprint 'Invalid argument: -a or --arg=<deprecated arg>'\n\t\t\telse:\n\t\t\t\tprint 'argument passed as \"-a for --arg=<deprecated arg>'\n\t\telse:\n\t\t\tanswer_file_args['args'] = '_arg_:<\"arg\"', '_arg_'\n\t\t\tself.args_from_files(answer_file_args)\n\t\t\tanswer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint \n\tprint \"DEPRECATION\", \"(\", cli_args[\"verbose\"], \")\"\n\tprint\n\tprint \"WARNING\", \"(\", cli_args[\"warnings\"], \")\"\n\tprint\n\tprint \"ERROR\", \"(\", cli_args[\"errors\"], \")\"\n\tprint\n\t\n\tfor filename in answer_file_args:\n\t\tinheritance = get_inheritance(cli_args, filename, cli_args[\"verbose\"])\n\t\tprint \"FILE\", filename, \"INHERITANCE\", inheritance \n\t\tvalidate_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\targument_list = [arg for arg in cli_args.keys() if arg not in list(answer_file_args.keys())]\n\tif argument_list:\n\t\tprint(\"WARNING: The follwoing arguments have been deprecated.\", file=answerer.stderr_file)\n\t\tfor arg in argument_list:\n\t\t\tprint(f\"{arg} ({answer_file_args[arg]}) is deprecated by\", file=answerer.stderr_file)\n\t\t\tprint(f\"{cli_args[arg],}\")\n\t\tprint(\""}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tresult = \"\"\n\tself.config.user = cli_args['env']\n\n\terror_message = \"\"\n\tif cli_args['deprecation']:\n\t\terror_message = \"These arguments cannot be deprecated.\\n\\n\\\n\t\t\tenv: '{0}'\\n\\\n\t\t\tuser: '{1}'\\n\\\n\t\t\tfile: '{2}'\\n\\\n\t\t\tname: '{3}'\\n\\\n\t\t\tdeprecation: '{4}'\\n\\\n\t\t\tis_deprecated: '{5"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfile_helpers = self.get_file_helpers()\n\tfor option in answer_file_args:\n\t\tif option in file_helpers['deprecated']:\n\t\t\tprint('The argument %s is deprecated and has been replaced by %s'%\n\t\t\t\t(option, answer_file_args[option]))\n\t\t\tcli_args   [option] = answer_file_args[option]\n\n\tif cli_args[u'file'] in file_helpers['deprecated']:\n\t\tprint('The arguments %s are deprecated, and should be used only"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif answer_file_args:\n\t\tmessage = f\"Argument(s) 'files' is deprecated, \" \\\n\t\t\t\tf\"but you should use Arguments '--dir', '-f', '-a', '--arch' and \" \\\n\t\t\t\tf\"'-p' instead. \"\n\telse:\n\t\tmessage = f\"The following parameters are deprecated:\\n\\n\"\n\t\tmessage += '\\t--file:\\n\\t\\t\\t The path or file of the problem or \"files\" argument'\n\t\tmessage += '\\t\\t\\t Will be"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif not _DEBUG: return\n\t\n\tif cli_args is None: assert False \n\tif not cli_args: cli_args = {}\n\t\n\tif answer_file_args is None: assert False \n\tif not answer_file_args: answer_file_args = {}\n\t\n\t\n\twith open(answer_file_args['output_format'],'w') as answer_file:\n\t\tfor value in cli_args.values():\n\t\t\tanswer_file.write(f'\\t{value}\\n')\n\t\tanswer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\tif not self.cfg['is_file_in_current_dir']:\n\t\tanswer2 = input('Cannot find any argument in current folder, do you want to create the files? [Y/N]: ').lower()\n\t\t\n\t\t\t# This block checks, after the user has entered any non-empty space\n\t\t\t# and if yes it creates a file.\n\t\tif answer2 == \"\" or answer2.startswith(\"y\"):\n\t\t\tfor i in answer_file_args.keys():\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t# If the argument was found in the argument"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t# check if it is a file\n\tif os.path.isfile(answer_file_args.data):\n\t\t# check if file is not empty\n\t\tif len(answer_file_args.data) > 0:\n\t\t\tcli_args.data = answer_file_args.data\n\t\t\n\t\t# if not found, then check if it is a directory\n\t\telif os.path.isdir(answer_file_args.data):\n\t\t\tcli_args.data = path.join(answer_file_args.data, 'data')\n\tel"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t# first the message for deprecated arguments\n\t_msg = (\"The '%s' argument is deprecated, use the command '%s'\"\n\t\t% (CLI.DEFAULT_ARG_NAME, CLI_PREFIX + \"devel\"))\n\t\n\t# remove all arguments from answer_file_args\n\tanswer_file_args = dict((k, v) for k, v in answer_file_args.items()\n\t\tif k not in ('answer_file', 'outputfile')) \n\t\t\n\t# remove args that are not valid arguments\n"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toption_specs = {}\n\tfor op_name, op_desc in cmd_options[command_name].items():\n\t\tprint 'Adding option'+ op_name +'for command'+ command_name\n\t\toption_specs[op_name] = { 'key': op_name, 'description': op_desc}\n\treturn option_specs\n\n\n#------------------------------------------------------------------------#\n\n# Commands for the command manager\n\ndef show_cmd_options(*args):\n\t\"\"\"\n\t\tDisplays list of all command options as text\n\t\t\n\t\t:param args:"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tresult_dict = []\n\tif self.command =='main':\n\t\tresult_dict.append(\n\t\t\tdict(\n\t\t\t\tname=\"-command\",\n\t\t\t\tdest=\"command\",\n\t\t\t\tdefault=self.command,\n\t\t\t\ttype=\"choice\",\n\t\t\t\tchoices=self.CONFIG.BIND.get(VIRSH.SESSION_OPTIONS.CMD, [])\n\t\t\t)\n\t\t)\n\t\tresult_dict.append(\n\t\t\tdict(\n\t\t\t\tname=\"--profile\",\n\t\t\t\targ=\"profiles\",\n\t\t\t\tdest=\"profiles\",\n\t\t\t\tdefault="}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn self.cli_options.keys()\n\n\t\nclass Config(dict):\n\t\"\"\"\n\tAn object for the shell configuration.\n\t\"\"\"\n\t\n\tdef __init__(self, raw_config):\n\t\t\"\"\"\n\t\tClass Constructor for the Config object\n\n\t\t:param raw_config: the raw config as it is parsed from the cmd line command line\n\t\t:return: Nothing\n\t\t\"\"\"\n\t\tsuper(Config, self).__init__(raw_config)\n\t\t\n\t\t\n\t@property\n\tdef options(self):\n\t\t\"\"\"\n\t\tGet the options"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparsed_options = []\n\tif command_name == None:\n\t\tparsed_options = self._base.options_specs\n\telse:\n\t\tparsed_options = self._base.get_options_specs(command_name)\n\t\n\toptionList = []\n\n\tfor i in range(len(parsed_options)):\n\t\toptionsSpec = parsed_options[i]\n\t\toptSpec = ParsedOptionSpec()\n\t\toptSpec = self._base.option_spec  # this does the same thing as the Base class default"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tif command_name.upper() == 'CMMST':\n\t\toptions = [\n\t\t\t\t\tdict(key='_cmdline', value=[]),\n\t\t\t\t\tdict(key='_host_opt', value=[]),\n\t\t\t\t]\n\t\tfor i in self.commands:\n\t\t\tif '_cmdline' not in self.commands[i]:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\toptions[0]['_cmdline'] = self.commands[i]['_cmdline']\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Could not find the command options in the configuration"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\tif command_name.startswith('main') is True:\n\t\tcmdlist = command_name.split()\n\t\ttry:\n\t\t\tcmdlist.pop(0)\n\t\texcept:\n\t\t\tpass\n\t\t\n\t\tfor element in cmdlist:\n\t\t\tcommand = elem_sop(element)\n\t\t\topts = dict(self.get_parameter_specs(element, ['option']))\n\t\t\topts['command'] = command\n\t\t\tif command in self.options_to_check:\n\t\t\t\tfor option in self.options_to_check[command"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tif command_name == \"add\":\n\t\treturn command_name + \" host=127.0.0.1,user=USER\"\n\tif command_name == \"remove\":\n\t\treturn command_name + \" host=127.0.0.1\"\n\tif command_name == \"list\":\n\t\treturn \"\"\n\tif command_name == \"set\":\n\t\treturn \"host=127.0.0.1 user=USER\"\n\treturn \"host=127.0.0.1,user=USER\"\n\n\nclass BaiduApiShell(object"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toption_specs = [\n\t\t{'name': '--{}'.format(group), 'type': 'boolean', 'default': False, 'help': '''This flag can be used to turn off a specific option from a group.\\n\n\t\t\tIf it is defined, the command must specify at least one of '{}' \n\t\t\t'''.format(group.lower())},\n\t\t{'name': \"-i\", 'type': 'bool','metavar': 'i', 'default': False, 'help': '''This option specifies if a group is interactive."}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toption_specs = (\n\t\t(\"ip-address\", \"--ip-address\", \"\", [\"--ip-address\"]),\n\t\t(\"username\", \"--username\", \"\", [\"--username\"]),\n\t\t(\"password\", \"--password\", \"\", [\"--password\"]),\n\t\t(\"private-key-file\", \"--private-key-file\", \"\", [\"--private-key-file\"]),\n\t\t(\"network\", \"--network\", \"\", [\"--network\"]),\n\t\t(\"command\", \"--command\", \"\", [\"--command\"]),\n\t\t(\"exec\", \"--exec"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tif command_name in VARIABLE_OPTIONS:\n\t\tvar_options = []\n\t\tfor v in VARIABLE_OPTIONS[command_name]:\n\t\t\tvar_options.append(VARIABLE_OPTIONS[command_name][v]())\n\t\treturn var_options\n\telse:\n\t\treturn []\n"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tspecs = self.get_option_spec_info()\n\tif command_name in specs:\n\t\treturn specs[command_name][argument_name]\n\n\traise KeyError('No option with name \"{}:{}\" in {}\\n'.format(argument_name,\n\t\tcommand_name, self.name))\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn {\n\t\t\"command\": command_name,\n\t\t\"option_name\": argument_name\n\t}\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn None\n\ndef do_nothing():\n\tpass\n\ndef option_spec_to_tuple(option_spec):\n\tif option_spec is None:\n\t\treturn ()\n\tif isinstance(option_spec, (list, tuple)):\n\t\treturn tuple(option_spec)\n\telse:\n\t\treturn option_spec"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif command_name in self.__option_spec:\n\t\treturn self.__option_spec[command_name][argument_name]\n\telse:\n\t\traise Exception('Unknown option specified'+ command_name +'' + str(argument_name))\n\t\n\t\nclass Option(BaseCommand):\n\t\"\"\"\n\t\tBase command. An Option can be used in a single command, or per-subcommand in a\n\t\tCommand class\n\t\n\t\t:param str command_name: Name of the command.\n\t\t:param str name: Name of the option.\n\n\t\t:"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif argument_name == \"-s\":\n\t\treturn {'type':'string', 'default': None}\n\tif argument_name == \"-m\":\n\t\treturn {'type':'string', 'default': None}\n\treturn None\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\traise NotImplementedError\n\n\treturn []\n\n\n\t\n\t\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.config.option_spec.get(command_name, {}).get(argument_name, {})\n\n\nclass Options(argparse.Action):\n\t\"\"\"\n\t\tObject of the Command with a dictionary of options.\n\t\"\"\"\n\tdef __init__(self, option_strings, dest=None, nargs=0, **kwargs):\n\t\t\"\"\"\n\t\t\tClass definition\n\n\t\t\tParameters\n\t\t\t----------\n\t\t\toption_strings :\n\t\t\t\tOption strings\n\t\t\tdefault_value :\n\t\t\t\tDefault value if not found\n\t\t\tnargs :\n\t\t\t\tnargs,\n\t\t\tdest"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif self.has_option(command_name, argument_name):\n\t\treturn self.options[command_name][argument_name]\n\telse:\n\t\traise ValueError(\"option {}:{} not found\".format(command_name, argument_name))\n\n\n\t\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\ttry:\n\t\treturn self.commands[command_name][argument_name]\n\texcept KeyError:\n\t\traise AnsibleError(\"Invalid command name specified! Can not find option for %s.%s\" %\n\t\t\t\t\t   (command_name, argument_name))\n\n\nclass OptionValidationError(Exception):\n\tpass"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\toption_spec = self.get_options_dict()[command_name][argument_name]\n\tif isinstance(option_spec, (list, tuple)):\n\t\treturn {key: val[0] for (key, val) in option_spec}\n\telse:\n\t\treturn option_spec\n\n\nclass Commands(object):\n\t@classmethod\n\tdef get(cls, parent):\n\t\tcommand_dict = cls.__dict__['_command_dict']\n\t\tcommand_dict = {k.replace('_', '-'): val for k,"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tresult = []\n\tsilent = []\n\tfor arg in args:\n\t\tif isinstance(arg, str):\n\t\t\tsilence = arg\n\t\t\tbreak\n\t\telse:\n\t\t\tsilence ='silent'\n\tfor a in args:\n\t\tresult.append(a)\n\t\tif isinstance(a, str):\n\t\t\tsilent.append(a)\n\t\telse:\n\t\t\tsilent.append(arg.name)\n\treturn result, silent\n\n\nclass CommandProcess(base.Command):\n\t\"\"\"\n\tA class that creates a shell command.\n\t"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilence_list = ['-s',' -S']\n\treturn [arg for arg in args if arg not in silence_list]\n\ndef parse_args(argv):\n\t\"\"\"\n\t\tArguments and Parsing\n\n\t\t:param argv: The command line arguments\n\t:return:\n\t\"\"\"\t\n\tparsed_argv = {\n\t\t\"version\": parse_number(\"-\"+argv[1]),\n\t\t\"path\": argv[2:] if argv[0] == \"--path\" else [argv[0]]+argv[2"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [\"silenced_args_list\"]\n\n\n\t\ndef set_silenced_args(self, args, silenced_args_list):\n\t\"\"\"\n\tSet the list of silenced arguments. This is supposed to be used when you want users to see a very specific message,\n\ti.e. if you want to report bugs, use `report_bugs` function as only argument.\n\tNote: Silent arguments are always replaced with the default value: `False`.\n\t:param args: the argument to be replaced, by default `False`.\n\t:param silenced_args_list:"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilent_names = []\n\tfor key, val in dict(args).items():\n\t\tif type(val) == list:\n\t\t\tif val[0].lower() not in (\"no\", \"off\"):\n\t\t\t\tsilent_names.append(key)\n\t\telif val!= args[key]:\n\t\t\tsilent_names.append(key)\n\t\n\tif silent_names:\n\t\tprint(\"\")\n\t\tprint(\"Silencing :\")\n\t\tprint(\":\".join(silent_names))\n\t\n\treturn silent_names"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilence = []\n\tfor arg in args:\n\t\tis_string = isinstance(arg, str)\n\t\tis_number = isinstance(arg, (int, np.number))\n\t\tis_bool = isinstance(arg, bool)\n\t\tif (not is_string) or (not is_bool) or (not is_number):\n\t\t\tsilence.append(arg)\n\treturn silence\n\n\ndef to_class_name(class_name, dtype=None, from_string=False):\n\t\"\"\"\n\tdtype can be 'int8',"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\tif args:\n\t\tfor name in args:\n\t\t\tif isinstance(name, list):\n\t\t\t\tname = name[0]\n\t\t\tcurrent_arg = self.argparse_arg.parse_known_args(name)[0]\n\t\t\tif not isinstance(current_arg, argparse.Argument):\n\t\t\t\tsilenced_args.append(name)\n\telse:\n\t\tsilenced_args = args\n\treturn silenced_args\n\n\n\nclass Logging:\n\t\"\"\"\n\t\tclass for logging things\n\t\"\"\"\n\tdef __"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tif hasattr(args,'silent'):\n\t\tsilent = args.silent\n\tif not silent:\n\t\treturn silent\n\t\n\tsilent = [] if not hasattr(args,'silent') else args.silent\n\tsilent += ['--silent']\n\tif not hasattr(args, 'timeout'):\n\t\treturn silent\n\telse:\n\t\tsilent += [x + x for x in [args.timeout, args.quiet]]\n\t\treturn silent\n\nclass _Silencer(object):\n\tdef"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [\n\t\t'save_results','save_config','save_to'\n\t]\n\n\ndef get_silent_kwargs(kwargs):\n\t\"\"\"\n\tlist of skipped keyword arguments\n\t\n\t:param kwargs: the received keyword arguments.\n\t:return: list, skipped keyword arguments\n\t\"\"\"\n\treturn [\n\t\t'results_prefix', 'no_save'\n\t]\n\n\nclass SilentMetric(object):\n\t\"\"\"silent metrics.\"\"\"\n\tdef __init__(self):\n\t\tself.silent_"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsl = []\n\tfor i in args:\n\t\tif i.lower() in [\"off\", \"none\"]:\n\t\t\tsl.append(i.lower())\n\treturn sl\n\n\nclass Quieter(object):\n\t\"\"\"\n\tClass for disabling certain methods\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\n\t\t:return:\n\t\t\"\"\"\n\t\tpass\n\nclass _Print(object):\n\t\"\"\"\n\tClass for printing messages.\n\t\n\t:param message: A message to print\n\t:param print_mode: Print mode (e.g."}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tisSILENT = False\n\targsKeys = self.configure_keys(None)\n\tsilenced = [arg[0] for arg in args if not arg[1] == isSILENT]\n\treturn silenced, argsKeys\n\n\t# TODO: maybe we should do better: replace a lot of logging with the same function\n\t# https://stackoverflow.com/questions/6247711/ignore-all-logging-warnings-logging-warning-and-error\n\t# or maybe a bunch of other stuff. like \"ignore:.*"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not args.use and not args.save_format and not args.use_csv or not args.batch_size:\n\t\traise CLIException('Required arguments are missing')\n\n\"\"\"\n\tFormat the requested value to the correct representation.\n\tArgs:\n\t\targs (argparse.Namespace): The parsed arguments.\n\t\targs (argparse.Namespace): The parsed arguments.\n\tReturns:\n\t\tUnion[dict,None]: The formatted request as a dictionary.\n\"\"\"\ndef format_to_req_arg(args):\n\tif args.use or args.use_"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor k in [\"url\", \"description\", \"description_html\"]:\n\t\tif k not in args and not args.get(k[:-1], False):\n\t\t\traise Exception(\"[validate_requires_args] The argument %s has not been provided\" % k)\n\n\nclass HTTPErrorHandler(BaseHTTPDigestAuthHandler):\n\tdef http_error_401(self, req, fp, code, msg, headers):\n\t\t\"\"\"\n\t\tHTTP authentication failed. Return error 401\n\t\t\"\"\"\n\t\theaders_dict = {'WWW-Authenticate': '"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif (args is None):\n\t\targs = default_arguments\n\tif (\"-n\" not in args) or (\"--nodisplay\" not in args):\n\t\tif (\"-n\" in args) and (\"--nodisplay\" in args):\n\t\t\t# no need to pass both, the user already supplied n and --no-display\n\t\t\tself.add_argument('--nodisplay', action='store_false')\n\t\telse:\n\t\t\t# The user provided the --no-display flag\n\t\t\targs['--nodisplay'] = True\n\treturn"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args) < 1:\n\t\tprint_help.print_help()\n\t\treturn\n\n\tif not hasattr(args,'version'):\n\t\tprint_errors.print_errors(0, 'Argument version must be provided', 'ERROR')\n\t\treturn\n\n\tif not hasattr(args, 'image'):\n\t\tprint_errors.print_errors(0, 'Argument image must be provided', 'ERROR')\n\t\treturn\n\n####  FUNCTIONS #####\ndef read_conf(conf_path):\n\t\"\"\"\n\tLoad the yaml configuration file"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_names = inspect.getargspec(self.__init__)[0]\n\tfor name in args_names.split(','):\n\t\tif name in args and not args[name] :\n\t\t\tprint('Required argument', name, 'is missing')\n\t\t\tsys.exit(0)\n\n\nclass Test:\n\tdef __init__(self):\n\t\ttest_name='test'\n\t\tsuper().__init__()\n\n\tdef get_user_input(self):\n\t\tself.result = self.input(\"Please enter a sentence:\")\n\t\tprint(\"The sentence"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\t# Check if any required arguments are missing.\n\tmissing_args = [arg for arg in dir(self) if\n\t                  arg not in ['_config', '_defaults', '_missing_args',\n\t                                '_load_cls', '_load_cls_name', '__version__']]\n\tif len(missing_args):\n\t\tif len(missing_args) == 1:\n\t\t\tmsg = \"missing required argument : %s.\" % missing_args[0]\n\t\telif len(missing_args) == 2:\n\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not any(isstring(arg) for arg in args):\n\t\tset_msg(e, 'The \"arg\" positional argument must be either a string or a list of strings.\\n')\n\t\traise TLError(e)\n\telif not all(isstring(arg) for arg in args):\n\t\tset_msg(e, 'List of strings (not all were provided)')\n\t\traise TLError(e)\n\nclass BaseTLError(TLError):\n\tdef __init__(self, name='TLError"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = ['userName']\n\tif required == [] or [arg for arg in args if arg in required]:\n\t\tprint('Arguments are not required for this script.')\n\t\tsys.exit(1)\n\telse:\n\t\treturn(True)\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not args or len(args) < 4:\n\t\treturn False\n\n\telif not args[0].hasLength(1):\n\t\treturn False\n\telse:\n\t\treturn True\n\n\treturn True\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args.num_nodes == self.parser.get_default_num_nodes():\n\t\t# Nothing has changed\n\t\tvalidate_model(model=self.model, non_blocking=True)\n\t\treturn\n\tif self.model.num_nodes == 0:\n\t\traise IncorrectConfigError('Empty model provided.')\n\tif args.data_file is not None:\n\t\tif args.data_file == self.parser.data_file:\n\t\t\t# Nothing has changed\n\t\t\tvalidate_model(model=self.model, non_blocking="}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) -> Iterator[str]:\n\t\tret = []\n\t\tfor option, opt_type in self.get_all_type(command_name):\n\t\t\tif self._get_condition_args(opts, option) is not None:\n\t\t\t\tname = opt_type(self, option, **options_spec).as_name(option)\n\t\t\t\tret.append(name)\n\t\tif len(ret) > 0:\n\t\t\treturn ret\n\t\treturn iter(()), ()\n\n\nclass _CommandType(Typed):\n\t\"\"\"\n\tSpecialized command type to match the"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t# TODO(rbrun): add support for --no-interactive\n\tif (args[0] == self.interactive_command_prefix and\n\t\t\tself.interactive_command_prefix in options_spec):\n\t\targs = args[1:]\n\n\trequired_by, required_when = get_required_by_and_kwargs(self._parser.conf, options_spec)\n\tif required_by:\n\t\tmsg = textwrap.dedent(\"\"\"\n\t\t\tThe following command line args will be required if the one with the %(app_name)s"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t):\n\t\t\"\"\"Returns list of argument names with matched ``required_when``\n\t\t\n\t\t:param command_name: the command name\n\t\t:param options_spec:  the list of command spec options\n\t\t:return: list\n\t\t\"\"\"\n\t\trequired_when_dict = dict()\n\t\tfor option in options_spec:\n\t\t\trequired_when_dict[field_re.match(option).group(\"name\")] = \\\n\t\t\t\toption_re.match(option).group(\"value\")\n\t\trequired_when_dict_list = list(required_when_dict.values"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\treturn sorted(args, key = lambda a: not a.match(options_spec))\n\n\nclass ValidatorTestCase(CommandTestCase):\n\t\"\"\"Test validator class\n\n\t\tA command test case that is expected to check a validator.\n\n\t\"\"\"\n\n\tdef setUp(self):\n\t\t\"\"\"setUp method\n\n\t\t\tAll test cases will call this method.\n\t\t\"\"\"\n\n\t\tself.validator = Validator(\"Test string\")\n\n\tdef test_check_invalid_arguments(self):\n\t\t\"\"\"test_check_invalid_arguments\n\n\t\t\tA command test"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\toptions = []\n\n\t# If the command isn't in the spec, it has a `required_when` option\n\t# but not required_if and not required_on.\n\tfor option_spec in options_spec:\n\t\tassert option_spec.get('required_when') is None\n\t\tassert option_spec.get('required_if') is None\n\t\tassert option_spec.get('required_on') is None\n\n\t\tmatch = self.matcher.match(command_name, args, **option_spec)\n\t\toptional_when = match.get('required_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t)\n\t\n\tdef create_argument(key: Union[str, list[str]]) -> str:\n\t\t\"\"\"Create a new argument with the corresponding type in option map.\n\t\t\n\t\t:param key: argument name\n\t\t:type key: Union[str, list[str]]\n\t\t:return: argument name\n\t\t\"\"\"\n\t\t\n\t\tif isinstance(key, list):\n\t\t\treturn []\n\t\t\n\t\tassert key not in self._argument_map, f\"{command_name}.{key}\"\n\t\t\n\t\tself._argument_map[key] = Type(_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tself._req_args = []\n\tif command_name.startswith('--help'):\n\t\tself._req_args.append('-h')\n\t\treturn self._req_args\n\tfor arg in options_spec:\n\t\targ_len = len(arg['required_when'])\n\t\tif arg_len!= 1:\n\t\t\t# arg must be passed by the \"--help\" option\n\t\t\tif args and args[0].startswith(arg['required_when']):\n\t\t\t\tself._req_args.append(arg['required_when'])\n\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\targs = self.options_spec.get(command_name, {}).get('args', [])\n\targs = list(set(args) & set(options_spec['required_when']))\n\targs = [x.name for x in args]\n\treturn args\n\n\nclass ArgumentGroup(object):\n\n\t\"\"\"A group that is used if there are several arguments in a command.\"\"\"\n\n\n\t_argument_group_type = ArgumentGroup\n\t\"\"\"Argument group type.\"\"\"\n\n\tdef __init__(self, name=None, description=None, sub"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\treturn [x.strip() for x in options_spec if options_spec.index(x) + 1 == arguments.REQUIRED_WHEN]\n\n@command.setup_command\n@argument.add_arguments(\n\tgroup_name=str,\n\tgroup_args=argument.Argument.POSITIONAL_ARGUMENTS(\"{[option]}\"),\n\t)\n@argument.simple_args(\"--add\")\n@argument.optional_args(\"--del\", required=True)\n@argument.optional_args(\"--delim\", help=\"the string"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targs = self.parser.parse_args(args=args)\n\t\n\treturn [\n\t\targ\n\t\tfor arg in args.args\n\t\tif (\n\t\t\targ.startswith('--' + command_name)\n\t\t\tand options_spec.get(arg[2:]) is not None\n\t\t)\n\t\tor (\n\t\t\targ == '--require'\n\t\t\tor arg in options_spec.get(command_name)\n\t\t)\n\t]\n\n#TODO: use list comprehensions instead of explicit\nclass ArgumentErrorMapper:\n\tregex"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args is None:\n\t\tif self.__args[self.SIDE_INPUT_IDX] is None:\n\t\t\treturn\n\t\tself.__args[self.SIDE_INPUT_IDX] = str(min(args.length, self.max_len))\n\t\treturn\n\t\t\n\tif len(args.length) > self.max_len or len(args.length) < args.length:\n\t\targs.length = args.sequence[args.length-1]\n\t\targs[self.SIDE_INPUT_IDX] = str"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.len is None:\n\t\traise ArgumentError(self.dest, 'length', ('No len set. Please set'+\n\t\t\t'--len <number> in the --help option of this script.'))\n\n\tfor i in self.valid_length:\n\t\ttry:\n\t\t\ti_int = int(i)\n\t\texcept TypeError:\n\t\t\tself.warn(short_help, 'invalid length value')\n\t\t\ttry:\n\t\t\t\tself.help.validate(i_int)\n\t\t\texcept ArgumentError as e:\n\t\t\t\traise self.validation_error"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tvalue = str(args[self.argument_name])\n\targ_len = len(value)\n\tif arg_len < self.length_min \\\n\t\t\tor arg_len > self.length_max:\n\t\t\traise ValueError('Length of {} argument is longer'  \\\n\t\t\t\t\t'than required minimum: {}'.format(self.name, \\\n\t\t\t\t\tself.length_min))\n\t\ndef validate_value_args(self, args):\n\t\"\"\"\n\tCheck if the variable provided is non-empty.\n\t\n\t:param args: \n\t\tRequired"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif (\n\t\t'password_policy' in args if len(args) == 0 else \\\n\t\targs.get('password_policy', True) is False\n\t):\n\t\tself.log.info(\n\t\t\t'Password policy: %s - no longer required' % \\\n\t\t\targs.get('password_policy')\n\t\t\t)\n\t\targs.pop('password_policy')"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor value, arg in zip(args[7:], self.keys):\n\t\tif value > arg:\n\t\t\tif (value!= getattr(ValueError, 'value') and (value!= 'None') and (value!= '')):\n\t\t\t\tf = \"{0}={1} and {2}={3}\".format(key, value, arg, getattr(ValueError, 'value'))\n\t\t\t\traise ValueError(f)\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tinvalid_length_args_msg = \"\"\n\tfor word in args:\n\t\tif len(str(word)) < self.MAX_LENGTH:\n\t\t\tinvalid_length_args_msg += \"{}, should be > {} but got {} \\n\".format(str(word), str(self.MAX_LENGTH), str(len(str(word))))\n\tif invalid_length_args_msg:\n\t\terr_value = \"{}, value too long\".format(str(args))\n\t\traise ValueError(err_value)\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.get('length') is not None:\n\t\tif len(args.get('length')) > self.length:\n\t\t\traise argparse.ArgumentError(None, _('Invalid arg of length. %d is not less than {0}.')\n\t                        ''.format(self.length))\n\tif args.get('length') is not None and args.get('offset') is not None:\n\t\tif (args.get('offset') - self.offset\n\t\t\t\t> self.get('length')):\n\t\t\traise argparse.ArgumentError(None,"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn [None] if len(args) < self.check_length else args"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t# Check if len is a negative number\n\tif type(args[-1]) == type(int()):\n\t\tif args[-1] == 0:\n\t\t\traise argparse.ArgumentError(self, \"length cannot be a negative number\")\n\t\t\nparse_length_args(__name__)"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) >= int(self.length):\n\t\targs = args[:len(args)-self.length]\n\n\treturn args\n\n\t\nclass Request(object):\n\t\"\"\"Class with methods to prepare the request sent to the network.\n\n<p>Example: <pre style=\"text-indent: 20px\">\nrequest = Request(\n\t\tinput_value = \"3.45\",\n\t\tparams = {\n\t\t\t\t\"foo1\": \"bar1\",\n\t\t\t\t\"foo2\": \"bar2\"\n\t\t}\n\t)\n</pre>\"\"\"\n\t\n"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args)!= 0:\n\t\t# This could cause an error due to an invalid choice argument.\n\t\t# An error should only be returned when a valid choice is not\n\t\t# provided.\n\t\tfor item in args:\n\t\t\tif not isinstance(item, (unicode, types.StringType)):\n\t\t\t\targs = [item]\n\t\t\t\tbreak\n\t\t\t\n\t\t\tif hasattr(item, 'choice_text'):\n\t\t\t\targs.append(item)\n\t\t\t\tbreak\n"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args in self.settings.choices:\n\t\treturn\n\traise CommandException(\"Invalid Choice: choices does not contain argument '%s'. \"\n\t                       \"Try to give choices as command arguments.\" % args)\n\t\ndef get_choices(choices, arg):\n\t\"\"\"\n\t\tReturns the value of the argument as a string in the given choices.\n\t\t\n\t\t:param arg: The given argument\n\t\t:type arg: str\n\t\t:param choices: The choices value\n\t\t:type choices: dict\n\t\t:returns: The given argument as string\n\t\t:"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif any(arg.lower() in choices for arg in args):\n\t\treturn True\n\n\treturn False\n\n\nclass BaseArgumentsError(Exception):\n\t\"\"\"Base exception for argparse.ArgumentParser errors.\"\"\"\n\n\tdef __init__(self, message=''):\n\t\t\"\"\"Initialize an argparse.ArgumentParser Exception.\n\n\t\t:param message: The full message shown to user.\n\t\t\"\"\"\n\t\tself._message = message\n\t\tException.__init__(self, message)\n\n\t@property\n\tdef message(self):\n\t\t\"\"\"The full message shown to"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tavailable_arguments = {}\n\tfor arg in self._argument_names:\n\t\tif arg in args:\n\t\t\t\n\t\t\tdefault = args[arg].default\n\t\t\tchoice = type(args[arg].type_hint).__name__\n\t\t\t# Choice may be an integer, so use the integer value.\n\t\t\tdefault = int(default) if (type(default)==int) or (default is None) or (choice == 'choice') else args[arg].type_hint.parse(args[arg],self)\n\t\t\tif choice not in available_arguments"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices = args.get(\"choices\")\n\tif choices:\n\t\tif type(choices) not in (tuple, list):\n\t\t\t# This handles the case of a single value in the array or one of the choice values\n\t\t\t# as a list.\n\t\t\tchoices = (choices, )\n\tif not choices:\n\t\tchoices = tuple(range(1, len(get_config()[\"defaults\"][\"choices\"]) + 1))\n\treturn tuple(choices)\n"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoice_list = ['name', 'country','state', 'city', 'order_quantity', 'qty', 'type', 'voucher', 'date', 'date_add', 'expire_date','status']\n\tvalid_choice = choices(choice_list, args)\n\treturn valid_choice"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tvalid_choices = []\n\tif isinstance(args['choice'], (list,tuple)):\n\t\tfor choice in args['choice']:\n\t\t\tif not isinstance(choice,str):\n\t\t\t\traise TypeError(\n\t\t\t\t\t'Value of choice argument expected to be a list or tuple')\n\t\t\tif not isinstance(choice,int):\n\t\t\t\traise TypeError(\n\t\t\t\t\t'Value of chosen option expected to be an int')\n\t\t\tif choice not in self.choices:\n\t\t\t\traise ValueError(\n\t\t\t\t\t'multiple choices {} are not possible'.format(choice))"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tfor chpt in args.choices:\n\t\tif chpt == '-t':\n\t\t\tchpt = 'testmode'\n\t\n\t\t\tif args.testmode:\n\t\t\t\treturn\n\t\n\t\ttry:\n\t\t\tchpt = str(chpt)\t\t\t\n\t\t\tif chpt not in ['A', 'B', 'C']:\n\t\t\t\tprint('\\033[31m[ERROR]\\033[0m Invalid value for choices argument')\n\t\t\t\tprint('\\033[31m[ERROR]\\033[0m Invalid value for choices argument')\n\t\t\t\texit(0"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif isinstance(args.choices, list):\n\t\ttry:\t\n\t\t\tnext(iter(args.choices))\n\t\texcept StopIteration:\n\t\t\treturn False\n\telse:\n\t\tif args.choices not in self.__choices:\n\t\t\targs._errors.append(ArgumentsError('Invalid choices provided to argParse()', 'choice value'))\n\t\t\t\n\t\treturn True\n\ndef _process_choices(self, options, args):\n\t\"\"\"\n\tProcess the choice list options.\n\n\t\t>>> option_dict = {\"choices\": [{\""}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchb_arg = \"choices\" in args\n\tvalid_choices = list(self.choices).copy()\n\tif 'choices' in args:\n\t\targs_choices = args.get('choices')\n\t\treturn (not args_choices is None or len(args_choices) > 0) \\\n\t\t\tand all([(choices is not None) and (choices == str(choices)) for choices in args_choices])\n\tif 'choices' in args:\n\t\targs_choices = args.get('choices"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif not args.min > args.max:\n\t\traise UserError(_(\"Argumet minimum value ({}) cannot be greater than maximum value ({})\").format(args.min, args.max))\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.max < args.min:\n\t\traise ValueError(\"argument to validator_max_function must be greater than argument to validator_min_function \\n\"\n\t\t\t\t\t\t \"argument. Argument.max < argument.min.\\n\"\n\t\t\t\t\t\t \"\\n\\n\")\n\tif args.max >= args.min:\n\t\targs.min = max(args.min, args.min)\n\t\targs.max = min(args.max, args.max)\n\nclass Validator_max_function:\n\tdef __init__(self, max, min ="}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tminimum = args.get('min')\n\tmaximum = args.get('max')\n\tif minimum and maximum:\n\t\tif maximum < minimum:\n\t\t\traise ValueError(\"Argument'min' must be greater than'max'\")\n\n\tif minimum:\n\t\tmin_value = minimum\n\telse:\n\t\tmin_value = None\n\n\tif maximum:\n\t\tmax_value = maximum\n\telse:\n\t\tmax_value = None\n\n\treturn {\n\t\t\"valid_min_max\": [min_value, max_value]\n\t}\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args is None:\n\t\treturn False\n\tif len(args.min) == 0 and len(args.max) == 0:\n\t\treturn False\n\treturn True\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif not args.is_min:\n\t\traise ValueError(\"Max number of elements is less than minimum number of elements\\t{}\".format(args))\n\tif not args.is_max:\n\t\traise ValueError(\"Min number of elements is not lower than maximum number of elements\\t{}\".format(args))\n\n@click.command()\n@click.argument(\"min_max_list\", type = str,\n\t\t\tmetavar = \"INPUT_LIST\", required = True)\n@click.option(\"--min-max-dict\", default = '{}"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\tif args['min'].is_number and args['max'].is_number:\n\t\t\t# Min. and Max.\n\t\t\tif args['max'] < args['min']:\n\t\t\t\traise ValueError(\"'max' value {} should be greater than or equal to'min' value {}!\".format(args['max'], args['min']))\n\texcept:\n\t\tpass\n\ndef validation_required_key_args(validate_args, keys_names):\n\t\"\"\"\n\tValidate required and required keys of a given array of dict.\n\t\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t#print \"min, max, in this args: \", args\n\tfor argument in args:\n\t\t#print \"argument: \", argument\n\t\tif not argument in self.MIN:\n\t\t\t#print \"exception raised: invalid argument!\"\n\t\t\traise Exception, \"invalid argument!\"\n\n\t\tif not argument in self.MAX:\n\t\t\t#print \"exception raised: invalid argument!\"\n\t\t\traise Exception, \"invalid argument!\"\n\n\t\tif self.MIN[argument] > self.MAX[argument]:\n\t\t\t#print \"invalid range!\"\n\t\t\traise Exception, \"invalid range!\""}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif not len(args):\n\t\tprint 'empty args'\n\t\treturn False\n\tif not args[-2].isdigit():\n\t\tprint 'invalid range values'\n\t\treturn False\n\tr = 0\n\tif len(args[-1]) > 4:\n\t\tr += int(args[-1][-4])\n\t\n\tif args[0] == '+':\n\t\tif not r <= int(args[-3]) <= r:\n\t\t\tprint 'invalid interval ranges'\n\t\t\treturn False\n\telif args[0] == '-"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targs = vars(args)\n\t\n\t# Check if any required positional argument is missing.\n\tif not str(args['min']).isdigit() or not str(args['max']).isdigit():\n\t\terr.E701(validator,'min','max', 'Invalid argument. All values must be digits.')\n\t\t\n\ttry:\n\t\tint(args['min'])\n\texcept Exception:\n\t\terr.E702()\n\t\t\n\ttry:\n\t\tint(args['max'])\n\texcept Exception:\n\t\terr.E"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tassert isinstance(args, dict), \"Arguments must be given as a dict object\"\n\t\n\tmin_value = args.get('min', None)\n\tif min_value is not None:\n\t\tmin_value = to_min_value(min_value)\n\telse: \n\t\tmin_value = 0 \n\n\tmax_value = args.get('max', None)\n\tif max_value is not None:\n\t\tmax_value = to_min_value(max_value)\n\telse:\n\t\tmax_value = 100 \n\n\t"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tglobal complex_types, primitive_type, all_types, nonint_types, int_types\n\tprint(\"Complex type\")\n\tfor i in range(len(complex_types)):\n\t\tif complex_types[i][0] == type_name:\n\t\t\t# get complex type from the list\n\t\t\tfor i2 in range(len(custom_complex_types)):\n\t\t\t\tif classic_complex_types[len(classic_complex_types)-1][0] == custom_complex_types[i2][0]:\n\t\t\t\t\tbreak\n\t\t\t# build complex argument type"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tdef create_type(subcommand, type_name, spec_option=None, **kwa):\n\t\tspec = self.parse_spec(spec_option)\n\t\treturn ComplexArgumentsType(type_name, spec, **kwargs)\n\n\nclass ComplexArgumentsType(ArgumentsType):\n\t\"\"\"\n\t\tComplex arguments type\n\t\t\n\t:param str name: name of the complex type\n\t:param spec.ArgumentType specs: the specs of arguments type\n\t\"\"\"\n\n\tdef __init__(self, name, specs: specs, **kwargs):\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\traise NotImplementedError)\n\n@dataclass\nclass ComplexType(ArgumentParser):\n\t\"\"\"\n\t\tThe argument parser for complex argument types\n\t\"\"\"\n\n\tparser: ArgumentParser = dataclasses.field(default=NullableArgumentParser())\n\ttype: Type = NullableStr\n\toption: Type = NullableStr\n\n\t@classmethod\n\tdef parse(cls, options):\n\t\targuments_parser = cls.parser\n\t\ttype_name = arguments_parser.get_type_name()\n\t\toption_name_prefix = arguments_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\treturn CmdComplexArgumentType(\n\t\tsubcommand=subcommand,\n\t\ttype_name=type_name,\n\t\toption_name=option_name,\n\t\tspec=CMD_OPTION_SCHEMA[option_name]\n\t)\n\n\nclass CmdComplexArgumentType(BaseComplexArgumentSchema):\n\t\"\"\"\n\t\tArgument type for CmdComplex\n\t\n\t\"\"\"\n\t__metaclass__ = ABCMeta\n\tdef add_arguments_parser(self, subparsers):\n\t\t\"\"\"\n\n\t\t:param"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\treturn ComplexArgumentType(subcommand=subcommand, type_name=type_name, option_name=option_name, spec_option=spec_option)\n\n\nclass ComplexArgumentType(Argument):\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tThe argument class represents a complex type type, which can contain more complex types.\n\n\t\t:* args: arguments\n\t\t:* kwargs: arguments keyword arguments\n\t\t\"\"\"\n\t\tself.__args = args\n\t\tself.__kwargs = kwargs\n\t\tsuper()."}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tpass\n\nclass Arg:\n\t\"\"\"\n\tArg: argv: argv: <type'str'>\n\t:param spec_path: option's specification file\n\t:param spec_dir: option's specification directory\n\t\"\"\"\n\n# class Arg:\n# \t\"\"\"\n# \tArg: argv: argv: <type 'typing.Tuple[str, str,...]'>\n# \t:param spec_path: option's specification file\n# \t:param spec_dir: option's specification directory\n# \t:param spec_kwargs"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tcomplex_type = ArgumentNode(\n\t\tname=option_name\n\t\t+ subcommand\n\t\t+ type_name\n\t)\n\tcomplex_type.compose_object = ComplexType(\n\t\ttype=\"complex\",\n\t\tfields=[\n\t\t\t{\n\t\t\t\t\"name\": \"argument\",\n\t\t\t\t\"type\": \"field\",\n\t\t\t\t\"value\": self.add_arg(),\n\t\t\t}\n\t\t]\n\t)\n\tcomplex_type.extend_comment(type_name, \"Complex argument nodes\")\n\treturn complex_type\n\n\ndef add_complex_argument(complex"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tpass\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\treturn self.subcommands[subcommand](self.context, type_name, option_name)\n\n\ndef type_info(type_name: str) -> Dict[str, Any] or List[Any]:\n\t\"\"\"\n\t\tReturn the type info for a python type (i.e. its information).\n\t\t:param type_name: the type\n\t\t:return: the object's information\n\t\t\"\"\"\n\ttyp_obj = getattr(sys.modules[\"builtins\"], type_name, None)\n\tif typ_obj:\n\t\treturn typ_obj.__"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tdef build():\n\t\ttype_name = ':'.join([subcommand, type_name, option_name])\n\t\tif ( not spec_option ):\n\t\t\tspecs.set_subcommand_type(type_name)\n\t\telse:\n\t\t\tspecs.set_subcommand_type2(type_name,spec_option)\n\t\t# Build the type\n\t\tif specs.has_key(type_name) or specs.has_key(type_name + ':list'):\n\t\t\ttype_instance = simple_type_build(self.parser.type_"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\tcontrol_args = {}\n\tfor arg in args:\n\t\tif arg in get_collection_functions():\n\t\t\t# It's an ansible_collections argument: convert the key to a\n\t\t\t# tuple, split out and put back from a string into a regular\n\t\t\t# dictionary\n\t\t\tif isinstance(arg, string_types):\n\t\t\t\targument = argument.split(':', 1)\n\t\t\t\tkey_arg, content = argument[0].strip(), argument[1].strip()\n\t\t\t\tcontrol_args[key_arg] = expand_format_dict(content)\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\tcontrol_arguments = {\n\t\t'control_args': dict(),\n\t\t'nested_args': dict()\n\t}\n\tnested_arguments = dict()\n\t\n\t# control the nested and control_args args.\n\t# control_type = \"custom\"\n\t\n\tif args['vars_section'] is not None:\n\t\tif 'nested' in args['args']:\n\t\t\tcontrol_arguments['nested_args'] = args['args']\n\t\t\targs['args'].pop('nested')\n\t\t\tnested_arg"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tflat_custom_and_control_args = {}\n\tnested_args = {}\n\targ_control_args = {}\n\targ_custom_args = {}\n\tfor arg in args:\n\t\tif arg.find(':' + '_') >= 0:\n\t\t\targ_control_args[arg.replace(':', '')] = arg.split(':',1)[-1]+'/'\n\t\t\targ_control_args[arg.replace(':', '')] = arg.split(':',1)[0].split(':')[-1]+'.ansible"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t# first get control_args\n\tif len(args) == 1 and \"control_args\" not in args[0]:\n\t\tconfig = self._getConfig()\n\t\tcontrol_args = self._get_nested_control_args(config)\n\t\tif control_args:\n\t\t\tnested_args = self._format_nested_args(control_args)\n\t\telse:\n\t\t\tnested_args = []\n\t\t\tcontrol_args = {}\n\t\tcontrol_args['control_args'] = control_args\n\t\tcontrol_args['nested_args'] = nested"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif type(arg) is dict:\n\t\t\tif 'controle' in arg:\n\t\t\t\tcontrole = arg['controle']\n\t\t\t\tif controle['action'] =='replace_nested_value' and \\\n\t\t\t\t\tcontrole['item'] in self._nested_controle and \\\n\t\t\t\t\tcontrole['replace'] in self._nested_controle[controle['item']] and \\\n\t\t\t\t\t'control_args' in self._n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tkw = get_module_kwargs(args)\n\tkw['_mutually_exclusive'] = kw.get('_mutually_exclusive', list()) + ['action', 'role._original_args'] if 'action' in kw and kw['action'] in ['store','store_true'] else kw.get('_mutually_exclusive', list() )\n\tctrl_args  = kw.get('control_args')\n\tdefault_args = kw.get('default_args') or (getattr(args, 'default_vars', None"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tparsed = self.parse_argument_spec(args)\n\treturn parse_control_custom_and_nested_args(parsed.flat())\n\n\ndef parse_control_custom_and_nested_args(control_args_arr):\n\t\"\"\"\n\t\tParse the input args so that they match the spec defined in\n\t\tcontroller.\n\t\t\n\t\tIt will extract any Nested-and-Custom arguments as\n\t\t\t\t\"<keyword>=<string>\". For example:\n\t\t\t\t\"nested=a&b=c\",\n\t\t\t\tThe first entry"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tresult = None\n\ttry:\n\t\tresults = self.plugin.get_group_vars(args)\n\t\tif results['_raw_params'] is not None:\n\t\t\tresult = results['_raw_params']\n\t\telif results['_play_args'] is not None:\n\t\t\tresult = results['_play_args']\n\t\telif results['_check_args'] is not None:\n\t\t\tresult = results['_check_args']\n\texcept AnsibleError as e:\n\t\tself.tqm.send_callback('v2_runner_item"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tctrl_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\t\n\targs = iter(args)\n\n\ttry:\n\t\twhile True:\n\t\t\tin_args = next(args)\n\t\t\tif isinstance(in_args, string_types):\n\t\t\t\tctrl_args[in_args.strip()] = {}\n\t\t\telif isinstance(in_args, dict):\n\t\t\t\tnested_args[in_args.pop('_ansible_pos2')] = in_args\n\t\t\telse:\n\t\t\t\tcustom_args[in_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif \":\" in arg:\n\t\t\tkey, val = arg.split(\":\", 1)\n\t\telif arg.startswith(\"_custom\"):\n\t\t\tval = \"custom\"\n\t\t\tcontrol_args[key] = self.process_customize_key(arg, \"\", \"\", 0)\n\t\t\tif (\n\t\t\t\tval not in [\"\", None, \"\"]\n\t\t\t\tand val not in list(control_args.keys())\n\t\t\t\tand val not in nested_args\n"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor k, v in list(vars_dict.items()):\n\t\tif k in extra_vars:\n\t\t\tvars_dict[k] = extra_vars[k]\n\n\n\t\tfor sub_key, sub_val in list(k.items()):\n\t\t\tif sub_key == '_sub_key':\n\t\t\t\tcontinue\n\n\t\t\t# Is any extra_vars dict with subkey of k\n\t\t\tfor extra_vars in extra_vars:\n\t\t\t\tif k == extra_vars and sub_key == '_sub_key':\n\t\t\t\t\textra"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif vars_dict is not None and extra_vars is None:\n\t\treturn vars_dict\n\t\n\tif extra_vars is None:\n\t\textra_vars = []\n\t\n\textra_vars.extend(vars_dict)\n\t\n\treturn merge_extra_vars(extra_vars)\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif not extra_vars:\n\t\textra_vars = []\n\t\n\tvars_dict.update(extra_vars)\n\treturn vars_dict\n\ndef _parse_list(l, default=None):\n\t\"\"\"Parse list of strings\"\"\"\n\t\n\tl = l if isinstance(l, list) else [l]\n\t\n\tif not l:\n\t\tif default:\n\t\t\treturn default\n\t\telse:\n\t\t\traise TypeError(\"Unparseable list '%s'\", l)\n\telif l[0]"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict.update(extra_vars)\n\treturn vars_dict\n\ndef get_vars(vars_dict, path):\n\t\"\"\"\n\t\tReturn all variables from ``vars_dict`` from ``path``\n\t\"\"\"\n\t# Get lists of strings\n\textra_vars = get_path_from_dict(vars_dict, path)\n\t# If extra-vars (e.g. from extra_paths) exist\n\t# in the path do not overwrite it, return\n\tif extra_vars:\n\t\treturn"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = []\n\t\n\t# extend vars_dict\n\tvars_dict.update(*extra_vars)\n\ndef _parse_options(options, env=None, allow_none=False):\n\tif allow_none:\n\t\tdefault_options = {'exists':None,'mode':None, 'type':'str', 'v':None}\n\telse:\n\t\tdefault_options = {'exists':None,'mode':None, 'type':'list', 'v':[]"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\tif isinstance(vars_dict, str):\n\t\tif vars_dict in extra_vars:\n\t\t\tpass\n\t\telse:\t# Not a unique value\n\t\t\treturn vars_dict\n\tfor key in extra_vars:\n\t\tif key not in vars_dict:\n\t\t\tvars_dict[key] = extra_vars[key]\n\treturn vars_dict\n\ndef make_unique(vars_dict):\n\t\"\"\"Make all keys in the given dictionary"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\telif not isinstance(extra_vars, list):\n\t\textra_vars = [extra_vars]\n\tvars_dict.update(extra_vars)\n\treturn vars_dict"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t## If empty, use defaults\n\tif extra_vars is None:\n\t\textra_vars = globals().copy()\n\t## Update vars_dict\n\tvars_dict.update(extra_vars)\n\t## Copy extras\n\tglobals().update(extra_vars)\n\n## Add global vars\nmerge_extra_vars(globals(), globals().copy())\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict.update(extra_vars or {})\n\textra_vars = dict(vars_dict)\n\tfor k, v in extra_vars.items():\n\t\tif k not in vars_dict or isinstance(v, list):\n\t\t\textra_vars[k] = [v]\n\t\telse:\n\t\t\textra_vars[k] = v\n\treturn vars_dict, extra_vars\n\n\ndef _merge_nested_dict(d, *keys):\n\t\"\"\"Helper to merge dicts of nested-"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor key, val in vars_dict.items():\n\t\ttry:\n\t\t\tif extra_vars is None:\n\t\t\t\tnew_val = val\n\t\t\telse:\n\t\t\t\tnew_val = eval(val)\n\t\texcept TypeError:\n\t\t\tnew_val = val\n\t\tvars_dict[key] = new_val\n\n\treturn vars_dict"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\n\t\timport sys\n\t\tfrom ansible.cli import cli\n\n\t\t# Setup the current module\n\t\tmodule = None\n\t\tplaybook = cli.AnsiblePlaybook(playbook_path=playbook_path, verbosity=verbose)\n\t\t\n\t\t# Add extra arguments for ansible to playbook\n\t\textra_vars = cli.ansible.ansible.common_obj.ansible_extra_vars(playbook_path, verbose)\n\t\tansible_args = cli.ansible.ansible.common_obj.ansible"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t, ansible_cli): ansible_playbook.ANSIB_CLI.append(\n\t\tplugin_dirs.ANSIB.ANSIB_CLI[indv.ANSIB_PLUGIN]),\n\t):\n\t\t\n\tif verbose is None:\n\t\tverbose = 1 if play_dir == play_path else 0\n\t\tverbose = 0 if (play_path == \"\" and is_sftp_workpath(play_path)) else verbose\n\t\n\t# set the default output verbosity - we override it when we parse the\n\t# command"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\n\t\timport ansible_utils\n\t\timport sys\n\t\timport os\n\t\timport logging\n\t\t\n\t\t# import traceback\n\t\t# traceback.print_exc(file=open(os.devnull, 'wb'))\n\t\t\n\t\timport ansible_playbook.executor as executor\n\t\t\n\t\t# TODO: add some more variables to extra_vars\n\t\t# -- add ansible-playbook plugin output as extra-vars\n\t\t\n\t\timport ansible_playbook.playbooks as plays\n\t\timport ansible_playbook.play"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t'''\n\tA wrapper for ansible-playbook\n\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t\tWorkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level, e.g.'verbose'\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\timport subprocess\n\n\t\tproc = subprocess.Popen([\"ansible-playbook\", \"-V\"], stdout=subprocess.PIPE)\n\n\t\treturn proc.communicate(bytearray([ir_workspace.password, ir_workspace.port]),\n\t\t                          cwd=ir_workspace.path_to_playbook,\n\t\t                          stderr=subprocess.STDOUT)\n\n\t@staticmethod\n\tdef ansible_playbook_script(ir_workspace, playbook_path, verb=None,\n\t\t# Extra vars"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tfrom ansible.playbook import Playbook\n\tfrom ansible.executor.module_common import AnsibleModuleExecutor\n\n\textra_vars = dict(extra_vars)\n\n\t# Get the playbook class\n\tplaybook_class = Playbook if \"Playbook.\" not in str(ir_plugin) else None\n\tansible_config = dict()\n\tif os.path.exists(playbook_path):\n\t\twith open(playbook_path) as playbook_fh:\n\t\t\tansible_plugin = plugin_config[\"playbook_class\""}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\n\t\t# Playbook path may be None\n\t\t\n\t\t# If a playbook file was passed to `playbook_path`, use that.\n\t\tplaybook_path = playbook_path or ir_config.get('ansible_playbook', None)\n\t\tif not playbook_path:\n\t\t\tplaybook_path = None\n\t\t\n\t\t# Make sure the playbook file actually exists.\n\t\tif not os.path.exists(playbook_path):\n\t\t\tprint \"File '%s' does not exist.\" % playbook_path\n\t\t\t\n\t\t\treturn\n\t\t\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\t\n\t\textra_vars = ansible_vars if verbose is None else {'ansible_verbosity': verbose, 'ansible_become':'yes'}\n\t\tcmd_line = []\n\t\tif extra_vars:\n\t\t\tcmd_line.extend([\"-vv\", \"\".join(extra_vars)])\n\n\t\tif not ir_plugin.plugin_type == 'plugin':\n\t\t\tfor plugin in [ir_plugin.plugin_definition, ir_plugin.plugin_directory, ir_plugin.plugin_list_dir,"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t# build the dictionary of ansible_args\n\tvars_dict = {}\n\tif verbose:\n\t\tvars_dict['verbose'] = verbose\n\tvars_dict.update(extra_vars)\n\tansible_args = dict(ansible_args)\n\tansible_args.update(vars_dict)\n\treturn ansible_playbook.call_plugin(\n\t\tir_workspace, ir_plugin, play=playbook_path,\n\t\tcwd=dirname(playbook_path), verbose="}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\t# define result variable\n\t\tresult = dict(rc=0,\n\t\t\ttask=None,\n\t\t\thost=None,\n\t\t\tskipped=False,\n\t\t\trc=\"\",\n\t\t\tmsg=\"\",\n\t\t\tstdout=\"\",\n\t\t\tstderr=\"\",\n\t\t\treturncode=\"\",\n\t\t\tdiffs=None,\n\t\t\tdiff=None,\n\t\t\tcmd=None\n\t\t)\n\t\t# get the plugin's id\n\t\tplugin=getattr(ir_workspace.plugins[ir_plugin.plugin_id],\"plugin\")\n\t\t# execute the playbook\n\t\ttry:"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\tdef _run_playbook_cli(\n\t\t\t\t\t  cl_args, \n\t\t\t\t\t  cmd_string, \n\t\t\t\t\t  ir_workspace, \n\t\t\t\t\t  ir_plugin\n\t\t):\n\t\t\"\"\"\n\t\tRuns ansible cli with an additional extra environment\n\t\t\n\t\t:param cl_args: the list of command line arguments\n\t\t:param cmd_string: The string string containing command\n\t\tline execution.\n\t\t:param ir_workspace: An Infrared Workspace object represents the active\n\t\tworkspace\n\t\t:param ir_plugin: An Infrared"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t# run playbook\n\tprint(colored(\">>> Launching playbook...\", 'green'))\n\tresults = run_config_file_test(cli_args)\n\tif not results['returncode']:\n\t\tprint(colored(\"ERROR! Aborting playbook execution.\",\"red\"))\n\t\texit(results['returncode'])\n\telif results['returncode']:\n\t\treturn results\n\n\tvars_config_object = VarsConfig()\n\tir_config_object = IrVisConfig().get_configuration_obj()\n\tir_config = IrConfig()\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tlogger.info('\\n[run_playbook] vars_dict: %s' % (str(vars_dict)))\n\targs =''.join(['%s' % (i) for i in cli_args])\n\tlogger.debug('Cli args: %s' % (args))\n\ttry:\n\t\tif ir_workspace.get_session():\n\t\t\twith ExitStack() as at_stack:\n\t\t\t\twith at_stack.enter_context(tempfile.NamedTemporaryFile()) as tempf:\n\t\t\t\t\tlogger."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t# print(\"_run_playbook _run_playbook_ vars_dict: \", vars_dict)\n\tv2_plugin_vars = _get_plugin_vars(ir_plugin, vars_dict)\n\tvars_dict = v2_plugin_vars\n\tif IR_V2_PLUGIN_VARS_WITH_DEFAULT in vars_dict:\n\t\tvars_dict = {IR_V2_PLUGIN_VARS_WITH_DEFAULT: vars_dict[IR_V2_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\timport ansible\n\n\timport os\n\tfrom ansible.cli.impl import cli\n\tfrom ansible.cli.api import option_manager\n\tfrom ansible.cli.playbook import cli_name, cli_options\n\tfrom ansible.parsing.utils.yaml import AnsibleLoader\n\t\n\tfrom ansible.utils.cli import cli_parse_vars_from_argv\n\tfrom ansible.utils.display import Display\n\tdisplay = Display()\n\t\n\t# We want to do this now, in case the user wants"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\tcli_args = [a[0] for a in sorted(vars_dict.items())]\n\trun_cmd = '/bin/bash {}'.format(' '.join(cli_args))\n\tcommand ='sudo {} {}'.format(run_cmd,''.join(ir_plugin.extend_vars()))\n\treturn code, command\n\n\ndef add_inventory_tag(hosts, namespace, tags, cluster_name=None, cluster_uri=None):\n\t\"\"\" add a tag to an inventory object\n\n\t:param clusters: A list of"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tir_workspace = ir_workspace.get()\n\n\ttry:\n\t\tconfig = utils.setup_cli_config(cli_args,\n\t\t\t\t\t\t\t\t\t  ir_workspace.get_environment(),\n\t\t\t\t\t\t\t\t\t  vars_dict)\n\t\t# Run command in case of failed env setup\n\t\trc = run_ansible_config(cli_args, config)\n\t\tout = out_stream if config['state'] in ['active','verify'] else None\n\t\tif rc == 0:\n\t\t\tif out is None:\n\t\t\t\tprint(\"Unable to run ansible to"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tres = None\n\ttry:\n\t\tir_workspace.get_workspace(vars_dict)\n\t\t_run_playbook_internal(cli_args, ir_workspace, ir_plugin)\n\t\tir_workspace.set(\"current\", ir_workspace.get(\"current\"))\n\t\tir_workspace.save()\n\t\t_set_playbook_vars(cli_args, ir_workspace.get(\"current\", None), vars_dict)\n\t\tres = ir_workspace.get(\"results\")\n\texcept Exception as e:\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t# pylint: disable=too-many-statements\n\t# This method was written only for testing purposes only, and hence \n\t# is not ready to be fully functional.\n\t#########################################################################\n\t# NOTE:\n\t# We assume this method must be implemented in the parent class\n\t#\n\t# The task is always executed within the appropriate scope. \n\t#########################################################################\n\t# Create the command-line arguments\n\t#########################################################################\n\t# This method is always executed with the parent scope, and therefore is\n\t# handled without any logic.\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tmodule_dir = os.path.join(os.path.dirname(__file__), 'plugins')\n\tresults = list()\n\t# Start a pipeline of commands which can be executed on workers\n\ttask = ttask.Task(ir_workspace, ir_plugin)\n\ttask.run(args=cli_args, module_dir=module_dir)\n\n\tif not task.return_code:\n\t\tfor result in task.results:\n\t\t\t# Add all the hosts and the playbook hosts that can be executed\n\t\t\t# on all workers of the task\n\t\t\t#"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor arg in values_dict.keys():\n\t\tvalues_dict[arg] = self._convert_value(arg, values_dict[arg])\n\t\tdel values_dict[arg] # remove unnecessary param\n\t\t\n\tif self.__parser is not None:\n\t\tself.__parser.set_defaults(**values_dict)\n\t\t\n\treturn values_dict\n\n#=================================================================================================================/\n# Main\n#=================================================================================================================/\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\t\tMain program.\n\t\"\"\"\n\t\n\tconfig"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tcli_arg_list = values_dict[parser_name]\n\tif '-' in cli_arg_list and len(cli_arg_list) > 1:\n\t\traise ValueError('Please donwload only one argument on %s: \"%s\"' % (parser_name, cli_arg_list))\n\t\n\tcli_arg_types = type(cli_arg_list[0])\n\t\n\tif cli_arg_types == str:\n\t\tif '--' in cli_arg_list[0]:\n\t\t\tcli_arg_list"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\targs_as_list = True if type(values_dict) not in (list, tuple) else False\n\t\n\tif is_cli_arg_dict(values_dict, parser_name, args_as_list):\n\t\tvalues_dict = values_dict.as_dict()\n\t\t\n\tif is_cli_arg_dict(values_dict, parser_name):\n\t\tfor key in values_dict:\n\t\t\tvalues_dict.pop(key)\n\t\n\targs_dict = self._default_args_as_dict(os.path)"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\"\"\"\n\t# Convert non cli argument using parameters dict\n\tfor cli_arg in values_dict:\n\t\tvalues_dict[cli_arg] = to_native(values_dict[cli_arg])\n\t\"\"\"\n\t\n\t\n\t\n\t# Convert values_dict param to correct type\n\tvalues_dict = to_jsonable(values_dict)\n\t\n\t\n\t# Set return value based on cli_arg\n\tif parser_name[-1] == 'n':\n\t\treturn values_dict\n\telse:\n\t\treturn values"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\tfor key, value in values_dict.items():\n\t\t# If command with name is not present in argparse.Namespace object, then use the original parser\n\t\tif parser_name == key:\n\t\t\tpass\n\t\t\n\t\t# Otherwise use the default type\n\t\telse:\n\t\t\tif isinstance(value, str):\n\t\t\t\tvalues_dict[key] = typecast_value(value, self.__class__.__name__)\n\n\nclass MainParser:\n\t\"\"\"\n\t\tClass for parsing the arguments from the main program.\n\t\n\tAttributes:\n\t\t__"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalue_list = list(values_dict.items())\n\tvalue_tuple = tuple(value_list)\n\tfor key, value in value_list:\n\t\tif isinstance(value, str):\n\t\t\tvalue = value.split(\" \")\n\t\telif value is None:\n\t\t\tvalue = []\n\t\tvalue_list[value.index(key)] = value\n\tvalues_dict = dict(value_list) # get only those keys of args dictionary\n\tsetattr(self, parser_name, values_dict)\n\n#--------------------------------------------------------------------------------------\n\n##\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tprint('')\n\tprint('==========')\n\tprint('   {m}  '.format(m=parser_name.upper()))\n\tprint('Available commands:')\n\tprint('{}'.format(parser_name.description))\n\tprint('{}'.format(len(values_dict)))\n\tfor key, value in values_dict.items():\n\t\tif type(value) == str:\n\t\t\tvalue = value.lower()\n\t\t\tif value == 'none' or value == 'no':\n\t\t\t\tvalues_dict.pop(key, None)\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor arg in values_dict:\n\t\tif arg in (\"ip\", \"user\", \"password\"):\n\t\t\tvalues_dict[arg] = values_dict[arg].strip()\n\treturn parser_name, values_dict\n\n\ndef _valid_osx(filename):\n\t\"\"\"\n\tValidations on OSx file name. Returns True if it is valid.\n\t\n\t:param filename: The target osx file name\n\t\"\"\"\n\tif filename.endswith(\"OSX\"):\n\t\treturn True\n\treturn False\n\n\ndef _valid_zephyr("}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif not values_dict.has_key(parser_name):\n\t\twarnings.warn('No argument: \"%s\" was specified' % parser_name, UserWarning)\n\t\treturn\n\n\tif values_dict[parser_name][0]:\n\t\tvalues_dict[parser_name] = str(values_dict[parser_name])\n\telif values_dict[parser_name][1]:\n\t\ttry:\n\t\t\tvalues_dict[parser_name] = float(values_dict[parser_name])\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\tvalues_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == \"main\":\n\t\tif values_dict:\n\t\t\tvalues_dict[\"port\"] = int\n\t\t\tvalues_dict.update({\"output_file\": str})\n\telif parser_name in [\"virsh\", \"openvbox\", \"ciscos\"]:\n\t\t# TODO: Check that args passed in are correct\n\t\tpass\n\t\t#if values_dict == {}:\n\t\t#\traise Exception(\"\\033[91m'{0}' takes no arguments.\\033[0m\".format(parser_name))\n\treturn values_dict"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_dict = dict()\n\tfor fltr in [\"name\",\n\t\t\t\t\"ext\",\n\t\t\t\t\"version\",\n\t\t\t\t\"author\",\n\t\t\t\t\"configuration\",\n\t\t\t\t\"license\",\n\t\t\t\t\"description\",\n\t\t\t\t\"long_description\",\n\t\t\t\t\"settings\",\n\t\t\t\t\"scripts\",\n\t\t\t\t\"requirements\",\n\t\t\t\t\"dev-requirements\",\n\t\t\t\t\"tests\"]:\n\t\tif os.path.isfile(plugin_dir + os.sep + fltr):\n\t\t\twith open(plugin_dir + os.sep + fltr) as handle:\n\t\t\t\tplugin"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\timport os\n\tspec = {}\n\n\t# extract all parameters as properties\n\tconfig = plugin_dir.split('/')[-1]\n\tif config.endswith('.py'):\n\t\tconfig = config[:-3]\n\n\t# extract metadata key from metadata.json\n\tmetadata = dict(os.path.basename(plugin_dir))\n\tfor k in metadata.keys():\n\t\tfor attr in metadata[k]:\n\t\t\tif ';' in attr:\n\t\t\t\tattr, spec.update({k: attr[:attr.index(';"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tfrom pathlib import Path\n\tfrom subprocess import check_output, CalledProcessError\n\t\n\tp = get_bin_path(\"pyvotect\", True)\n\t\n\t# Check if the pyvotect executable is available, and if not\n\t# it raises an error\n\ttry:\n\t\tcheck_output(\n\t\t\t[\"which\", p], universal_newlines=True, stderr=subprocess.PIPE\n\t\t)\n\texcept CalledProcessError:\n\t\traise EnvironmentError(\n\t\t\tf\"pyvotect executable {p} not found and"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\timport_list = []\n\ttry:\n\t\tfile_list = []\n\t\tfor file_name in sys.path:\n\t\t\tif os.path.isdir(file_name):\n\t\t\t\tfile_list += [x for x in os.listdir(file_name) if os.path.isfile(os.path.join(file_name, x))]\n\t\t\t\n\t\t\n\t\tfor file_name in file_list:\n\t\t\twith open(os.path.join(plugin_dir, file_name + '.py'), 'r') as f:\n\t\t\t\tplugin"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\tclass Plugin:\n\t\tdef __init__(self, name, spec=None, properties=None, source=None, spec_name=None):\n\t\t\tself.name = name\n\t\t\tself.spec = spec if spec is not None else {}\n\t\t\tself.properties = properties if properties is not None else {}\n\t\t\tself.source = source if source is not None else []\n\t\t\tself.spec_name = spec_name if spec_name is not None else None\n\n\tplugins = {}\n\n\tfor filename in os.listdir(plugin_dir):\n\t\tplugin = Plugin("}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tflat_spec = {}\n\n\tfor k, v in plugin_spec.items():\n\t\ttry:\n\t\t\tif type(v) is dict:\n\t\t\t\tv, _ = plugin_spec[k]\n\t\texcept KeyError:\n\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\tflat_spec[k] = flatten(v)\n\t\t\texcept Exception as e:\n\t\t\t\tflat_spec[k] = str(e)\n\n\treturn flat_spec\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tdef get_spec_dict(path):\n\t\t\"\"\"\n\t\tHelper function.\n\t\t:param path: full path to the plugin's filename.\n\t\t:return:\n\t\t\"\"\"\n\t\treturn get_module_from_path(path).get_spec_dict()\n\t\n\tdef get_plugin_spec_flatten_dict(plugin_dir):\n\t\t\"\"\"\n\t\tGiven the plugin spec, generate the flattened dict.\n\t\t\n\t\t:param plugin_dir: A path to the plugin's dir\n\t\t:return: A flatten dictionary contains the plugin's properties\n\t\t\"\"\"\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_dictionary={}\n\tplugin_folder = self._get_plugin_dir(plugin_dir) # Get the plugin dir\n\tif not plugin_folder:\n\t\treturn {\"file\": \"No plugin found in the search path\"}\n\t# Now, get all the files listed inside the directory\n\n\tfor plugin in self._get_plugin_names(plugin_folder): # Get all the plugin files\n\t\tthis_file.add_module_name(plugin)\t\t# If a directory, recurse through\n\t\tplugin_dictionary[plugin] = self.__get_plugin_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn { 'parsed' : True,\n\t\t'executable' : False,\n\t\t'executable_path' : [],\n\t\t'name' : None,\n\t\t'flags': [],\n\t\t'args': [],\n\t\t'margin' : 0,\n\t\t'srcDirs' : [],\n\t\t'sourceDir': '' }\n\ndef get_plugin_spec_init_dict(plugin_dir):\n\t\"\"\"\n\tCreates a init dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\twith_properties = {}\n\tplugin_spec = None\n\tif not os.path.exists(plugin_dir):\n\t\treturn {}\n\tif plugin_dir.endswith('/'):\n\t\tplugin_dir = plugin_dir[:-1]\n\tif os.path.isfile(plugin_dir):\n\t\tplugin_spec = importlib.util.find_spec(os.path.splitext(plugin_dir)[0])\n\t\twith_properties = plugin_spec.loader.get_data(plugin_spec.name)\n\telif os"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif hasattr(self, 'config_path'):\n\t\tself.write_yaml_file()\n\t\tself.config_path = get_config_path()\n\t\tif self.config_path:\n\t\t\tself.config['path'] = self.config_path # set config path from yaml\n\n\t# Set the application config.config.provision.base_url if it is specified\n\t# Otherwise override by self.config.provision.app.app_url\n\t# Set by 'provision_app_url' method\n\tif 'provision_app_"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv = None\n\tif self.env:\n\t    env = shlex_split(self.env)\n\t    if len(env) == 0:\n\t        return\n\t    if len(env) == 1:\n\t        self.config_file = env[0]\n\t        # If we never found something\n\t        if not os.path.exists(self.config_file):\n\t            self.add_warning(\"Config path not set: %s\" % self.config_file)\n\t            return\n\t    # Now we can find a config"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not 'CONFIG_PATH' in os.environ:\n\t\tos.environ['CONFIG_PATH'] = config_util.BASE_CONFIG_PATH\n\nclass LoadConfigFromStr(config_util.ConfigParser):\n\tdef ReadConfig(self, configPath, env):\n\t\t\"\"\"\n\t\tLoad a configuration value from str(configPath)\n\t\t\"\"\"\n\t\tconfig_obj = config_util.LoadConfigFromStr(configPath.strip())\n\t\treturn config_obj[env]\n\nclass LoadConfigFromDict(config_util.ConfigParser"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tself._inject_config() # check the environment var is defined to be TRUE\n\tif os.name == 'posix':\n\t\tos.environ['PATH'] = self._path[0] + os.pathsep + os.environ['PATH']\n\telse: # assume windows\n\t\tif getattr(sys, 'frozen', False):\n\t\t\t# frozen\n\t\t\tpath = sys.executable\n\t\telif hasattr(sys, '_MEIPASS'): # assume this is an PyInstaller App\n\t\t\t# PyInstaller is inside the archive, but"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tfrom environment import ENV\n\n\tif 'CONFIG_ENV' in ENV:\n\t\tself.value = ENV['CONFIG_ENV']\n\n\telse:\n\t\tself.value = self.env\n\n\tdel ENV['CONFIG_ENV']\n\tdel ENV['CONFIG_PATH']\n\ndef validate_arguments(value):\n\t\"\"\"\n\tValidate the CLI arguments.\n\t\"\"\"\n\t# Validate arguments without a directory\n\tif not IS_PYTHON and value == '-':\n\t\traise TypeError('Invalid value %"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tfrom Utils.Utils import Config\n\n\tif not Config.get('INJECTS_DIR'):\n\t\tConfig.set('INJECTS_DIR', os.path.dirname(Config.get('JOB_FILE')))\n\n\tif not os.path.isfile(Config.get('TMP_CONFIG_FILE')):\n\t\twith open(Config.get('INJECTS_DIR') + '/temp_config.json', 'w') as config:\n\t\t\tconfig.write(Config.get('JOB_CONFIG_FILE') or sys"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tself.config_path = str(self._env.get_config_dir().resolve())\n\n\tif self.config_path.startswith(('' or '/')) and self.config_path!= '':\n\t\tself.config_path = self.get_app_dir()\n\n\tif not self.config_path == '':\n\t\tself.env.SetAppPath(self.config_path)\n\n\tself.logo = self.get_logo(self.env)\n\n\t# Try to use pkgutil for resource loading\n\ttry:\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not hasattr(self, '_inject_config_default'):\n\t\tself._inject_config_default = self.configpath\n\tos.environ['PYTHONINJECT_INCLUDE'] = str(self._inject_config_default)\n\ttry:\n\t\texec(\"__builtins__.configpath = %s\" % self._inject_config_default)\n\texcept ValueError:\n\t# No such file error seen, use fallback:\n\t\tpass\n\n\ndef inject_config_paths(self):\n\t"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif environment.getenv(\"XECUTOR\", None) is None:\n\t\tenvironment.setenv(\"XECUTOR\", self.xpath)\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif self.run_env_config_variables.get():\n\t\tenv_var_path = os.getenv('CONFIGURE_PATH', '/etc/')\n\t\tself.run_env_config_variables.set(not env_var_path)\n\t\tlog.info('Run configuration changed. New path: {new}'.format(\n\t\t\t\tnew = env_var_path))\n\n\tif self.run_env_config.get():\n\t\tenv_var_path = os.getenv('CONFIGURE_PATH', '/etc/')\n\t\tself."}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tself.parser.add_argument('-p', '--preview-list',dest=\"preview_list\",help=\"List of views to preview in the preview page.\",metavar=\"VIEW\",nargs=1)\n\n\tself.parser.add_argument('--view', dest=\"view\", action='append', help=\"View to preview (default if specified).\",metavar=\"VIEWNAME\",choices=self.view_list,default=[])\n\n\tself.parser.add_argument('--namespace',action='store',help=\"Set your namespace\",metavar"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tcmdargs = super(Extension, self).get_cmdargs(parser)\n\tcmdargs['verbosity'] = parser.add_argument_group('verbosity')\n\tcmdargs['verbosity'].add_argument('--version', action='version', version='%(prog)s {}'.format(__version__))\n\tcmdargs['verbosity'].add_argument('--quiet', action='store_true', help='run in quiet mode')\n\tcmdargs['verbosity'].add_argument('--skip-verify', action='store_true', help"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsuper(NettiumCli, self).extend_cli(root_subparsers)\n\tself.cli_argument_spec.update(spec['argument_spec'])\n\n#################\n# Generic   methods #\n#################\ndef _set_default_attribute(config_object, default_config, action, key):\n    \"\"\"\n    Helper to set the value of an option (optionally default value) for an option\n    specified by the action parameter.\n\n    :param config_object: the config object\n    :type config_object: dict\n    "}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tsubcommand = root_subparsers.add_parser('spec',description=\"Display a specification\",parents=[self.parent.get_root()])\n\tsubcommand.add_argument(\"id\",help=\"Specification id\") \n\tsubcommand.add_argument(\"-a\", \"--all\", action=\"store_true\", default=False, help=\"Display all specification\")\n\tsubcommand.add_argument(\"spec\", help=\"Specification to display\")\n\n\tself.parent.execute_cli(args=[subcommand,])\n\ndef run_app(id):"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tparser = subparsers.add_parser('extend', help=\"Extend the config file path.\", description=\"Extend a config file path with other options and subcommands/parameter values\")\n\t\n\tparser.set_defaults(func=self._extend_cli, root_subparsers=root_subparsers)\n\t\n\tparser.add_argument(\"-c\", \"--path\", type=str, no_log=True, required=True, help=\"Config file path [NOT NEEDED WITH --extend-cli]\", dest='"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = subparsers.add_parser(\"extend\", description=\"extends options\")\n\tsubparser = subparsers.add_parser(\"create\", description=\"creates a new registry\")\n\tsubparser.set_defaults(func=extend_cli)\n\n\nclass extend_cliMixin(object):\n\t\"\"\"\n\tBase class for extend_cli\n\t\"\"\"\n\tdef extend_cli(self, root_subparsers):\n\t\t\"\"\"\n\t\tReturns the sub-parser to extend from the main subparsers.\n\t\t\n\t\t:param subparsers"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tdef add_extended_options(parser):\n\t\tparser.add_argument('-i', '--iface', type=str, nargs='+', dest='ifaces_stringed',\n\t\thelp='Iface to use for extended capabilities \\n Type: IP address/mask in octal format, or interface name', metavar='IP address/mask in octal format')\n\t\tparser.add_argument('-e', '--env', type=str, nargs='+', dest='env_stringed_tuple',\n\t\thelp=\"Environment variables for extended capabilities\")"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tparent_parser = super(ExtendCommand, self).add_parser(\n\t\t'extend', help='Extend an existing API object\\'s attributes'\n\t)\n\tadd_cli_args(parent_parser)\n\t\n\tself.add_argument(\n\t\t'--desc', '-d', default=\"\",\n\t\thelp='Describe what this object is.'\n\t)\n\tself.add_argument(\n\t\t'--parent-id', '-p', default=\"\",\n\t\thelp=\n\t\t\"\"\"ID to attach this object to.\n\nRequired"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t# Initial defaults\n\t\n\tapiext_spec_namespace        = 'apiext'\n\tapiext_spec_name            = 'a'+self.api_version\n\t\n\t# Initial arguments\n\tapiext_arg                   = ArgumentParser(parents = [self.parser])\n\tapiext_arg.add_argument(     '--apidir',  type = str, help = 'The directory (and possibly full) where an API will be stored. Default is current directory', default = '.', metavar = '<directory"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.parser.add_argument('-c', '--command', required=True,\n\t\thelp='Specify the command to run, either command or directory. It '\n\t\t\t\t 'is case insensitive. Defaults to command. Default is '\n\t\t\t\t 'command. Default is \"command. Default is \"command'.\"')\n\t\t\n\tself.parser.add_argument('-k', '--key', type=str, required=True,\n\t\thelp='Specify the string to use in key')\n\t\n\tself.parser.add_argument('-u', '--url', type"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\treturn remove_root(path, root)\n\texcept:\n\t\traise\n\ndef walk(path, dirs_exist_ok=False):\n\t\"\"\"\n\tYield (path, dir name, list of files present in the directory).\n\n\tDirname is a tuple with the first element and subdir.\n\tOtherwise dir[0] (the root of the directory) is returned.\n\t\"\"\"\n\tp = pathdir(path)\n\tif dirs_exist_ok:\n\t\tp = p.iterdir()\n\tfor p in p:\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\twhile True:\n\t\tif not get_path_structure(path, root):\n\t\t\traise ValueError(\"Value of path does not exist!\")\n\t\tif root not in path:\n\t\t\treturn path\n\t\tif path[root] == root:\n\t\t\treturn path.replace(root + 1, '')\n\t\tpath = path[root + 1:]\n\n@with_default(sys_path=True, python_path=True)\ndef assert_file(fpath, expected=None, sys_path=True):\n\tif not is_absolute(fpath) and not"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif len(path.split(root)) == 0:\n\t\traise ValueError(\"'%s' is not prefixed by '%s'\" % (path, root))\n\treturn path.split(root)[1:]"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tdef _cleanup():\n\t\ttry:\n\t\t\tos.remove(b(\"%s/%s/%s/config\") % (root, path, filename))\n\t\texcept os.error:\n\t\t\tpass\n\n\t_cleanup()\n\t_cleanup()\n\ndef create_dir(path):\n\t\"\"\"\n\tCreate directory specified.\n\t\"\"\"\n\tdef _cleanup():\n\t\ttry:\n\t\t\tos.rmdir(b(\"%s/%s\") % (path, filename))\n\t\texcept os.error:\n\t\t\tpass\n\n\tcleanup = _"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tassert root is not None\n\tfor r_index, _ in enumerate(path):\n\t\tif root == path[r_index]:\n\t\t\tnew_index = r_index\n\t\t\tbreak\n\t\n\tremove_last_node_individually(path, new_index)\n\n\ndef remove_last_node_individually(path, index):\n\t# Check to see if we're at the last node\n\tremoved_value = path[index]\n\n\twhile index < len(path) - 1 and not (path[index + 1] or removed_value):\n\t\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tname = None\n\n\ttry:\n\t\tname = root.split(os.sep)[-1]\n\t\tif '/' in name:\n\t\t\traise\n\t\tif name.startswith('__'):\n\t\t\tname = name.replace('__', '/', 1)\n\t\t\tname = name.replace('/', '.')\n\t\t\tname = root.replace(name, '../../' + root.replace(name, '../', 1))\n\t\tif not '/' in name:\n\t\t\traise ValueError('root must be directory')\n\texcept ValueError:\n\t\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\tif root.strip():\n\t\t\tpath = path.removeprefix(root)\n\texcept OSError:\n\t\terrt, e, tb = sys.exc_info()\n\t\t#print e.args[0]\n\t\t#print e.__doc__\n\t\te = e.args[0]\n\t\t#print e\n\t\tif e == \"root not found\":\n\t\t\tprint \"skipping: %s\" % path.rstrip()\n\t\telse:\n\t\t\traise e\n\nif __name__==\"__main__\":\n\tmain("}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tdef _check_os(p):\n\t\tassert os.path.isabs(p)\n\t\tos.path.rootcase = root\n\t\treturn p\n\t\n\tpath = str(path)\n\tif path.startswith(os.path.join(root, os.path.sep)) or os.path.abspath(path) == os.path.abspath(os.path.join(root, os.path.sep)):\n\t\ttry:\n\t\t\treturn _check_os(str(path))\n\t\texcept:\n\t\t\tpass\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\treturn os.path.splitdrive(path)[1]\n\texcept IndexError:\n\t\tLogger.error(Logger.ERR, \"{}: no drive found in path.\".format(path))\n\t\t# No drive found, so no root.\n\t\treturn None\n\n\nclass Node(object):\n\t\"\"\"\n\t\tnode object representing any file, or some object that we must\n\t\tbe aware of (like file structure).\n\t\"\"\"\n\tdef __init__(self, path, parent = None, text = '', children = None):\n\t\t\"\"\"\n\t\t\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif path == root:\n\t\treturn \"\"\n\troot_dir, tmp_dir = os.path.split(path)\n\tif root_dir == root:\n\t\traise RuntimeError(\"Can't do strip, root is itself '%s'.\" % root)\n\tif not os.path.isdir(root_dir):\n\t\traise IOError(\"Can't find %s, not a directory\" % root)\n\treturn strip_root(tmp_dir, root)\n\n\ndef get_dirents():\n\t\"\"\"\n\tGet a dirnames.\n\t\"\"\"\n"}]}
